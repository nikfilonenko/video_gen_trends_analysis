title,abstract,published,authors,url
Video to Video Generative Adversarial Network for Few-shot Learning Based on Policy Gradient,"The development of sophisticated models for video-to-video synthesis has been
facilitated by recent advances in deep reinforcement learning and generative
adversarial networks (GANs). In this paper, we propose RL-V2V-GAN, a new deep
neural network approach based on reinforcement learning for unsupervised
conditional video-to-video synthesis. While preserving the unique style of the
source video domain, our approach aims to learn a mapping from a source video
domain to a target video domain. We train the model using policy gradient and
employ ConvLSTM layers to capture the spatial and temporal information by
designing a fine-grained GAN architecture and incorporating spatio-temporal
adversarial goals. The adversarial losses aid in content translation while
preserving style. Unlike traditional video-to-video synthesis methods requiring
paired inputs, our proposed approach is more general because it does not
require paired inputs. Thus, when dealing with limited videos in the target
domain, i.e., few-shot learning, it is particularly effective. Our experiments
show that RL-V2V-GAN can produce temporally coherent video results. These
results highlight the potential of our approach for further advances in
video-to-video synthesis.",2024-10-28 01:35:10+00:00,"['Yintai Ma', 'Diego Klabjan', 'Jean Utke']",http://arxiv.org/abs/2410.20657v1
Contrastive Sequential-Diffusion Learning: Non-linear and Multi-Scene Instructional Video Synthesis,"Generated video scenes for action-centric sequence descriptions, such as
recipe instructions and do-it-yourself projects, often include non-linear
patterns, where the next video may need to be visually consistent not with the
immediately preceding video but with earlier ones. Current multi-scene video
synthesis approaches fail to meet these consistency requirements. To address
this, we propose a contrastive sequential video diffusion method that selects
the most suitable previously generated scene to guide and condition the
denoising process of the next scene. The result is a multi-scene video that is
grounded in the scene descriptions and coherent w.r.t. the scenes that require
visual consistency. Experiments with action-centered data from the real world
demonstrate the practicality and improved consistency of our model compared to
previous work.",2024-07-16 15:03:05+00:00,"['Vasco Ramos', 'Yonatan Bitton', 'Michal Yarom', 'Idan Szpektor', 'Joao Magalhaes']",http://arxiv.org/abs/2407.11814v3
ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning,"Recently, breakthroughs in video modeling have allowed for controllable
camera trajectories in generated videos. However, these methods cannot be
directly applied to user-provided videos that are not generated by a video
model. In this paper, we present ReCapture, a method for generating new videos
with novel camera trajectories from a single user-provided video. Our method
allows us to re-generate the reference video, with all its existing scene
motion, from vastly different angles and with cinematic camera motion. Notably,
using our method we can also plausibly hallucinate parts of the scene that were
not observable in the reference video. Our method works by (1) generating a
noisy anchor video with a new camera trajectory using multiview diffusion
models or depth-based point cloud rendering and then (2) regenerating the
anchor video into a clean and temporally consistent reangled video using our
proposed masked video fine-tuning technique.",2024-11-07 18:59:45+00:00,"['David Junhao Zhang', 'Roni Paiss', 'Shiran Zada', 'Nikhil Karnad', 'David E. Jacobs', 'Yael Pritch', 'Inbar Mosseri', 'Mike Zheng Shou', 'Neal Wadhwa', 'Nataniel Ruiz']",http://arxiv.org/abs/2411.05003v1
"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text","Text-to-video diffusion models enable the generation of high-quality videos
that follow text instructions, making it easy to create diverse and individual
content. However, existing approaches mostly focus on high-quality short video
generation (typically 16 or 24 frames), ending up with hard-cuts when naively
extended to the case of long video synthesis. To overcome these limitations, we
introduce StreamingT2V, an autoregressive approach for long video generation of
80, 240, 600, 1200 or more frames with smooth transitions. The key components
are:(i) a short-term memory block called conditional attention module (CAM),
which conditions the current generation on the features extracted from the
previous chunk via an attentional mechanism, leading to consistent chunk
transitions, (ii) a long-term memory block called appearance preservation
module, which extracts high-level scene and object features from the first
video chunk to prevent the model from forgetting the initial scene, and (iii) a
randomized blending approach that enables to apply a video enhancer
autoregressively for infinitely long videos without inconsistencies between
chunks. Experiments show that StreamingT2V generates high motion amount. In
contrast, all competing image-to-video methods are prone to video stagnation
when applied naively in an autoregressive manner. Thus, we propose with
StreamingT2V a high-quality seamless text-to-long video generator that
outperforms competitors with consistency and motion. Our code will be available
at: https://github.com/Picsart-AI-Research/StreamingT2V",2024-03-21 18:27:29+00:00,"['Roberto Henschel', 'Levon Khachatryan', 'Daniil Hayrapetyan', 'Hayk Poghosyan', 'Vahram Tadevosyan', 'Zhangyang Wang', 'Shant Navasardyan', 'Humphrey Shi']",http://arxiv.org/abs/2403.14773v1
FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention,"Video diffusion models have made substantial progress in various video
generation applications. However, training models for long video generation
tasks require significant computational and data resources, posing a challenge
to developing long video diffusion models. This paper investigates a
straightforward and training-free approach to extend an existing short video
diffusion model (e.g. pre-trained on 16-frame videos) for consistent long video
generation (e.g. 128 frames). Our preliminary observation has found that
directly applying the short video diffusion model to generate long videos can
lead to severe video quality degradation. Further investigation reveals that
this degradation is primarily due to the distortion of high-frequency
components in long videos, characterized by a decrease in spatial
high-frequency components and an increase in temporal high-frequency
components. Motivated by this, we propose a novel solution named FreeLong to
balance the frequency distribution of long video features during the denoising
process. FreeLong blends the low-frequency components of global video features,
which encapsulate the entire video sequence, with the high-frequency components
of local video features that focus on shorter subsequences of frames. This
approach maintains global consistency while incorporating diverse and
high-quality spatiotemporal details from local videos, enhancing both the
consistency and fidelity of long video generation. We evaluated FreeLong on
multiple base video diffusion models and observed significant improvements.
Additionally, our method supports coherent multi-prompt generation, ensuring
both visual coherence and seamless transitions between scenes.",2024-07-29 11:52:07+00:00,"['Yu Lu', 'Yuanzhi Liang', 'Linchao Zhu', 'Yi Yang']",http://arxiv.org/abs/2407.19918v1
SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix,"Video generation models have demonstrated great capabilities of producing
impressive monocular videos, however, the generation of 3D stereoscopic video
remains under-explored. We propose a pose-free and training-free approach for
generating 3D stereoscopic videos using an off-the-shelf monocular video
generation model. Our method warps a generated monocular video into camera
views on stereoscopic baseline using estimated video depth, and employs a novel
frame matrix video inpainting framework. The framework leverages the video
generation model to inpaint frames observed from different timestamps and
views. This effective approach generates consistent and semantically coherent
stereoscopic videos without scene optimization or model fine-tuning. Moreover,
we develop a disocclusion boundary re-injection scheme that further improves
the quality of video inpainting by alleviating the negative effects propagated
from disoccluded areas in the latent space. We validate the efficacy of our
proposed method by conducting experiments on videos from various generative
models, including Sora [4 ], Lumiere [2], WALT [8 ], and Zeroscope [ 42]. The
experiments demonstrate that our method has a significant improvement over
previous methods. The code will be released at
\url{https://daipengwa.github.io/SVG_ProjectPage}.",2024-06-29 08:33:55+00:00,"['Peng Dai', 'Feitong Tan', 'Qiangeng Xu', 'David Futschik', 'Ruofei Du', 'Sean Fanello', 'Xiaojuan Qi', 'Yinda Zhang']",http://arxiv.org/abs/2407.00367v1
Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control,"Research on video generation has recently made tremendous progress, enabling
high-quality videos to be generated from text prompts or images. Adding control
to the video generation process is an important goal moving forward and recent
approaches that condition video generation models on camera trajectories make
strides towards it. Yet, it remains challenging to generate a video of the same
scene from multiple different camera trajectories. Solutions to this
multi-video generation problem could enable large-scale 3D scene generation
with editable camera trajectories, among other applications. We introduce
collaborative video diffusion (CVD) as an important step towards this vision.
The CVD framework includes a novel cross-video synchronization module that
promotes consistency between corresponding frames of the same video rendered
from different camera poses using an epipolar attention mechanism. Trained on
top of a state-of-the-art camera-control module for video generation, CVD
generates multiple videos rendered from different camera trajectories with
significantly better consistency than baselines, as shown in extensive
experiments. Project page: https://collaborativevideodiffusion.github.io/.",2024-05-27 17:58:01+00:00,"['Zhengfei Kuang', 'Shengqu Cai', 'Hao He', 'Yinghao Xu', 'Hongsheng Li', 'Leonidas Guibas', 'Gordon Wetzstein']",http://arxiv.org/abs/2405.17414v1
DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes,"The increasing demand for immersive AR/VR applications and spatial
intelligence has heightened the need to generate high-quality scene-level and
360{\deg} panoramic video. However, most video diffusion models are constrained
by limited resolution and aspect ratio, which restricts their applicability to
scene-level dynamic content synthesis. In this work, we propose the
DynamicScaler, addressing these challenges by enabling spatially scalable and
panoramic dynamic scene synthesis that preserves coherence across panoramic
scenes of arbitrary size. Specifically, we introduce a Offset Shifting
Denoiser, facilitating efficient, synchronous, and coherent denoising panoramic
dynamic scenes via a diffusion model with fixed resolution through a seamless
rotating Window, which ensures seamless boundary transitions and consistency
across the entire panoramic space, accommodating varying resolutions and aspect
ratios. Additionally, we employ a Global Motion Guidance mechanism to ensure
both local detail fidelity and global motion continuity. Extensive experiments
demonstrate our method achieves superior content and motion quality in
panoramic scene-level video generation, offering a training-free, efficient,
and scalable solution for immersive dynamic scene creation with constant VRAM
consumption regardless of the output video resolution. Our project page is
available at \url{https://dynamic-scaler.pages.dev/}.",2024-12-15 07:42:26+00:00,"['Jinxiu Liu', 'Shaoheng Lin', 'Yinxiao Li', 'Ming-Hsuan Yang']",http://arxiv.org/abs/2412.11100v1
Accelerating Video Diffusion Models via Distribution Matching,"Generative models, particularly diffusion models, have made significant
success in data synthesis across various modalities, including images, videos,
and 3D assets. However, current diffusion models are computationally intensive,
often requiring numerous sampling steps that limit their practical application,
especially in video generation. This work introduces a novel framework for
diffusion distillation and distribution matching that dramatically reduces the
number of inference steps while maintaining-and potentially
improving-generation quality. Our approach focuses on distilling pre-trained
diffusion models into a more efficient few-step generator, specifically
targeting video generation. By leveraging a combination of video GAN loss and a
novel 2D score distribution matching loss, we demonstrate the potential to
generate high-quality video frames with substantially fewer sampling steps. To
be specific, the proposed method incorporates a denoising GAN discriminator to
distil from the real data and a pre-trained image diffusion model to enhance
the frame quality and the prompt-following capabilities. Experimental results
using AnimateDiff as the teacher model showcase the method's effectiveness,
achieving superior performance in just four sampling steps compared to existing
techniques.",2024-12-08 11:36:32+00:00,"['Yuanzhi Zhu', 'Hanshu Yan', 'Huan Yang', 'Kai Zhang', 'Junnan Li']",http://arxiv.org/abs/2412.05899v1
Generative Adversarial Synthesis of Radar Point Cloud Scenes,"For the validation and verification of automotive radars, datasets of
realistic traffic scenarios are required, which, how ever, are laborious to
acquire. In this paper, we introduce radar scene synthesis using GANs as an
alternative to the real dataset acquisition and simulation-based approaches. We
train a PointNet++ based GAN model to generate realistic radar point cloud
scenes and use a binary classifier to evaluate the performance of scenes
generated using this model against a test set of real scenes. We demonstrate
that our GAN model achieves similar performance (~87%) to the real scenes test
set.",2024-10-17 13:14:25+00:00,"['Muhammad Saad Nawaz', 'Thomas Dallmann', 'Torsten Schoen', 'Dirk Heberling']",http://arxiv.org/abs/2410.13526v1
MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling,"Character video synthesis aims to produce realistic videos of animatable
characters within lifelike scenes. As a fundamental problem in the computer
vision and graphics community, 3D works typically require multi-view captures
for per-case training, which severely limits their applicability of modeling
arbitrary characters in a short time. Recent 2D methods break this limitation
via pre-trained diffusion models, but they struggle for pose generality and
scene interaction. To this end, we propose MIMO, a novel framework which can
not only synthesize character videos with controllable attributes (i.e.,
character, motion and scene) provided by simple user inputs, but also
simultaneously achieve advanced scalability to arbitrary characters, generality
to novel 3D motions, and applicability to interactive real-world scenes in a
unified framework. The core idea is to encode the 2D video to compact spatial
codes, considering the inherent 3D nature of video occurrence. Concretely, we
lift the 2D frame pixels into 3D using monocular depth estimators, and
decompose the video clip to three spatial components (i.e., main human,
underlying scene, and floating occlusion) in hierarchical layers based on the
3D depth. These components are further encoded to canonical identity code,
structured motion code and full scene code, which are utilized as control
signals of synthesis process. The design of spatial decomposed modeling enables
flexible user control, complex motion expression, as well as 3D-aware synthesis
for scene interactions. Experimental results demonstrate effectiveness and
robustness of the proposed method.",2024-09-24 15:00:07+00:00,"['Yifang Men', 'Yuan Yao', 'Miaomiao Cui', 'Liefeng Bo']",http://arxiv.org/abs/2409.16160v1
Multi-sentence Video Grounding for Long Video Generation,"Video generation has witnessed great success recently, but their application
in generating long videos still remains challenging due to the difficulty in
maintaining the temporal consistency of generated videos and the high memory
cost during generation. To tackle the problems, in this paper, we propose a
brave and new idea of Multi-sentence Video Grounding for Long Video Generation,
connecting the massive video moment retrieval to the video generation task for
the first time, providing a new paradigm for long video generation. The method
of our work can be summarized as three steps: (i) We design sequential scene
text prompts as the queries for video grounding, utilizing the massive video
moment retrieval to search for video moment segments that meet the text
requirements in the video database. (ii) Based on the source frames of
retrieved video moment segments, we adopt video editing methods to create new
video content while preserving the temporal consistency of the retrieved video.
Since the editing can be conducted segment by segment, and even frame by frame,
it largely reduces the memory cost. (iii) We also attempt video morphing and
personalized generation methods to improve the subject consistency of long
video generation, providing ablation experimental results for the subtasks of
long video generation. Our approach seamlessly extends the development in
image/video editing, video morphing and personalized generation, and video
grounding to the long video generation, offering effective solutions for
generating long videos at low memory cost.",2024-07-18 07:05:05+00:00,"['Wei Feng', 'Xin Wang', 'Hong Chen', 'Zeyang Zhang', 'Wenwu Zhu']",http://arxiv.org/abs/2407.13219v1
I4VGen: Image as Free Stepping Stone for Text-to-Video Generation,"Text-to-video generation has trailed behind text-to-image generation in terms
of quality and diversity, primarily due to the inherent complexities of
spatio-temporal modeling and the limited availability of video-text datasets.
Recent text-to-video diffusion models employ the image as an intermediate step,
significantly enhancing overall performance but incurring high training costs.
In this paper, we present I4VGen, a novel video diffusion inference pipeline to
leverage advanced image techniques to enhance pre-trained text-to-video
diffusion models, which requires no additional training. Instead of the vanilla
text-to-video inference pipeline, I4VGen consists of two stages: anchor image
synthesis and anchor image-augmented text-to-video synthesis. Correspondingly,
a simple yet effective generation-selection strategy is employed to achieve
visually-realistic and semantically-faithful anchor image, and an innovative
noise-invariant video score distillation sampling (NI-VSDS) is developed to
animate the image to a dynamic video by distilling motion knowledge from video
diffusion models, followed by a video regeneration process to refine the video.
Extensive experiments show that the proposed method produces videos with higher
visual realism and textual fidelity. Furthermore, I4VGen also supports being
seamlessly integrated into existing image-to-video diffusion models, thereby
improving overall video quality.",2024-06-04 11:48:44+00:00,"['Xiefan Guo', 'Jinlin Liu', 'Miaomiao Cui', 'Liefeng Bo', 'Di Huang']",http://arxiv.org/abs/2406.02230v2
ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning,"Recently, advancements in video synthesis have attracted significant
attention. Video synthesis models such as AnimateDiff and Stable Video
Diffusion have demonstrated the practical applicability of diffusion models in
creating dynamic visual content. The emergence of SORA has further spotlighted
the potential of video generation technologies. Nonetheless, the extension of
video lengths has been constrained by the limitations in computational
resources. Most existing video synthesis models can only generate short video
clips. In this paper, we propose a novel post-tuning methodology for video
synthesis models, called ExVideo. This approach is designed to enhance the
capability of current video synthesis models, allowing them to produce content
over extended temporal durations while incurring lower training expenditures.
In particular, we design extension strategies across common temporal model
architectures respectively, including 3D convolution, temporal attention, and
positional embedding. To evaluate the efficacy of our proposed post-tuning
approach, we conduct extension training on the Stable Video Diffusion model.
Our approach augments the model's capacity to generate up to $5\times$ its
original number of frames, requiring only 1.5k GPU hours of training on a
dataset comprising 40k videos. Importantly, the substantial increase in video
length doesn't compromise the model's innate generalization capabilities, and
the model showcases its advantages in generating videos of diverse styles and
resolutions. We will release the source code and the enhanced model publicly.",2024-06-20 09:18:54+00:00,"['Zhongjie Duan', 'Wenmeng Zhou', 'Cen Chen', 'Yaliang Li', 'Weining Qian']",http://arxiv.org/abs/2406.14130v1
Turns Out I'm Not Real: Towards Robust Detection of AI-Generated Videos,"The impressive achievements of generative models in creating high-quality
videos have raised concerns about digital integrity and privacy
vulnerabilities. Recent works to combat Deepfakes videos have developed
detectors that are highly accurate at identifying GAN-generated samples.
However, the robustness of these detectors on diffusion-generated videos
generated from video creation tools (e.g., SORA by OpenAI, Runway Gen-2, and
Pika, etc.) is still unexplored. In this paper, we propose a novel framework
for detecting videos synthesized from multiple state-of-the-art (SOTA)
generative models, such as Stable Video Diffusion. We find that the SOTA
methods for detecting diffusion-generated images lack robustness in identifying
diffusion-generated videos. Our analysis reveals that the effectiveness of
these detectors diminishes when applied to out-of-domain videos, primarily
because they struggle to track the temporal features and dynamic variations
between frames. To address the above-mentioned challenge, we collect a new
benchmark video dataset for diffusion-generated videos using SOTA video
creation tools. We extract representation within explicit knowledge from the
diffusion model for video frames and train our detector with a CNN + LSTM
architecture. The evaluation shows that our framework can well capture the
temporal features between frames, achieves 93.7% detection accuracy for
in-domain videos, and improves the accuracy of out-domain videos by up to 16
points.",2024-06-13 21:52:49+00:00,"['Qingyuan Liu', 'Pengyuan Shi', 'Yun-Yun Tsai', 'Chengzhi Mao', 'Junfeng Yang']",http://arxiv.org/abs/2406.09601v1
CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models,"We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular
video. CAT4D leverages a multi-view video diffusion model trained on a diverse
combination of datasets to enable novel view synthesis at any specified camera
poses and timestamps. Combined with a novel sampling approach, this model can
transform a single monocular video into a multi-view video, enabling robust 4D
reconstruction via optimization of a deformable 3D Gaussian representation. We
demonstrate competitive performance on novel view synthesis and dynamic scene
reconstruction benchmarks, and highlight the creative capabilities for 4D scene
generation from real or generated videos. See our project page for results and
interactive demos: https://cat-4d.github.io/.",2024-11-27 18:57:16+00:00,"['Rundi Wu', 'Ruiqi Gao', 'Ben Poole', 'Alex Trevithick', 'Changxi Zheng', 'Jonathan T. Barron', 'Aleksander Holynski']",http://arxiv.org/abs/2411.18613v2
CoNo: Consistency Noise Injection for Tuning-free Long Video Diffusion,"Tuning-free long video diffusion has been proposed to generate
extended-duration videos with enriched content by reusing the knowledge from
pre-trained short video diffusion model without retraining. However, most works
overlook the fine-grained long-term video consistency modeling, resulting in
limited scene consistency (i.e., unreasonable object or background
transitions), especially with multiple text inputs. To mitigate this, we
propose the Consistency Noise Injection, dubbed CoNo, which introduces the
""look-back"" mechanism to enhance the fine-grained scene transition between
different video clips, and designs the long-term consistency regularization to
eliminate the content shifts when extending video contents through noise
prediction. In particular, the ""look-back"" mechanism breaks the noise
scheduling process into three essential parts, where one internal noise
prediction part is injected into two video-extending parts, intending to
achieve a fine-grained transition between two video clips. The long-term
consistency regularization focuses on explicitly minimizing the pixel-wise
distance between the predicted noises of the extended video clip and the
original one, thereby preventing abrupt scene transitions. Extensive
experiments have shown the effectiveness of the above strategies by performing
long-video generation under both single- and multi-text prompt conditions. The
project has been available in https://wxrui182.github.io/CoNo.github.io/.",2024-06-07 16:56:42+00:00,"['Xingrui Wang', 'Xin Li', 'Zhibo Chen']",http://arxiv.org/abs/2406.05082v1
V3D: Video Diffusion Models are Effective 3D Generators,"Automatic 3D generation has recently attracted widespread attention. Recent
methods have greatly accelerated the generation speed, but usually produce
less-detailed objects due to limited model capacity or 3D data. Motivated by
recent advancements in video diffusion models, we introduce V3D, which
leverages the world simulation capacity of pre-trained video diffusion models
to facilitate 3D generation. To fully unleash the potential of video diffusion
to perceive the 3D world, we further introduce geometrical consistency prior
and extend the video diffusion model to a multi-view consistent 3D generator.
Benefiting from this, the state-of-the-art video diffusion model could be
fine-tuned to generate 360degree orbit frames surrounding an object given a
single image. With our tailored reconstruction pipelines, we can generate
high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method
can be extended to scene-level novel view synthesis, achieving precise control
over the camera path with sparse input views. Extensive experiments demonstrate
the superior performance of the proposed approach, especially in terms of
generation quality and multi-view consistency. Our code is available at
https://github.com/heheyas/V3D",2024-03-11 14:03:36+00:00,"['Zilong Chen', 'Yikai Wang', 'Feng Wang', 'Zhengyi Wang', 'Huaping Liu']",http://arxiv.org/abs/2403.06738v1
Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation,"An image may convey a thousand words, but a video composed of hundreds or
thousands of image frames tells a more intricate story. Despite significant
progress in multimodal large language models (MLLMs), generating extended
videos remains a formidable challenge. As of this writing, OpenAI's Sora, the
current state-of-the-art system, is still limited to producing videos that are
up to one minute in length. This limitation stems from the complexity of long
video generation, which requires more than generative AI techniques for
approximating density functions essential aspects such as planning, story
development, and maintaining spatial and temporal consistency present
additional hurdles. Integrating generative AI with a divide-and-conquer
approach could improve scalability for longer videos while offering greater
control. In this survey, we examine the current landscape of long video
generation, covering foundational techniques like GANs and diffusion models,
video generation strategies, large-scale training datasets, quality metrics for
evaluating long videos, and future research areas to address the limitations of
the existing video generation capabilities. We believe it would serve as a
comprehensive foundation, offering extensive information to guide future
advancements and research in the field of long video generation.",2024-12-24 21:24:41+00:00,"['Faraz Waseem', 'Muhammad Shahzad']",http://arxiv.org/abs/2412.18688v1
VideoStudio: Generating Consistent-Content and Multi-Scene Videos,"The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoStudio, for consistent-content and multi-scene video
generation. Technically, VideoStudio leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoStudio identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoStudio outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoStudio outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference. Source code
is available at \url{https://github.com/FuchenUSTC/VideoStudio}.",2024-01-02 15:56:48+00:00,"['Fuchen Long', 'Zhaofan Qiu', 'Ting Yao', 'Tao Mei']",http://arxiv.org/abs/2401.01256v2
Move-in-2D: 2D-Conditioned Human Motion Generation,"Generating realistic human videos remains a challenging task, with the most
effective methods currently relying on a human motion sequence as a control
signal. Existing approaches often use existing motion extracted from other
videos, which restricts applications to specific motion types and global scene
matching. We propose Move-in-2D, a novel approach to generate human motion
sequences conditioned on a scene image, allowing for diverse motion that adapts
to different scenes. Our approach utilizes a diffusion model that accepts both
a scene image and text prompt as inputs, producing a motion sequence tailored
to the scene. To train this model, we collect a large-scale video dataset
featuring single-human activities, annotating each video with the corresponding
human motion as the target output. Experiments demonstrate that our method
effectively predicts human motion that aligns with the scene image after
projection. Furthermore, we show that the generated motion sequence improves
human motion quality in video synthesis tasks.",2024-12-17 18:58:07+00:00,"['Hsin-Ping Huang', 'Yang Zhou', 'Jui-Hsien Wang', 'Difan Liu', 'Feng Liu', 'Ming-Hsuan Yang', 'Zhan Xu']",http://arxiv.org/abs/2412.13185v1
Vid3D: Synthesis of Dynamic 3D Scenes using 2D Video Diffusion,"A recent frontier in computer vision has been the task of 3D video
generation, which consists of generating a time-varying 3D representation of a
scene. To generate dynamic 3D scenes, current methods explicitly model 3D
temporal dynamics by jointly optimizing for consistency across both time and
views of the scene. In this paper, we instead investigate whether it is
necessary to explicitly enforce multiview consistency over time, as current
approaches do, or if it is sufficient for a model to generate 3D
representations of each timestep independently. We hence propose a model,
Vid3D, that leverages 2D video diffusion to generate 3D videos by first
generating a 2D ""seed"" of the video's temporal dynamics and then independently
generating a 3D representation for each timestep in the seed video. We evaluate
Vid3D against two state-of-the-art 3D video generation methods and find that
Vid3D is achieves comparable results despite not explicitly modeling 3D
temporal dynamics. We further ablate how the quality of Vid3D depends on the
number of views generated per frame. While we observe some degradation with
fewer views, performance degradation remains minor. Our results thus suggest
that 3D temporal knowledge may not be necessary to generate high-quality
dynamic 3D scenes, potentially enabling simpler generative algorithms for this
task.",2024-06-17 04:09:04+00:00,"['Rishab Parthasarathy', 'Zachary Ankner', 'Aaron Gokaslan']",http://arxiv.org/abs/2406.11196v3
SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input,"Stereo video synthesis from a monocular input is a demanding task in the
fields of spatial computing and virtual reality. The main challenges of this
task lie on the insufficiency of high-quality paired stereo videos for training
and the difficulty of maintaining the spatio-temporal consistency between
frames. Existing methods primarily address these issues by directly applying
novel view synthesis (NVS) techniques to video, while facing limitations such
as the inability to effectively represent dynamic scenes and the requirement
for large amounts of training data. In this paper, we introduce a novel
self-supervised stereo video synthesis paradigm via a video diffusion model,
termed SpatialDreamer, which meets the challenges head-on. Firstly, to address
the stereo video data insufficiency, we propose a Depth based Video Generation
module DVG, which employs a forward-backward rendering mechanism to generate
paired videos with geometric and temporal priors. Leveraging data generated by
DVG, we propose RefinerNet along with a self-supervised synthetic framework
designed to facilitate efficient and dedicated training. More importantly, we
devise a consistency control module, which consists of a metric of stereo
deviation strength and a Temporal Interaction Learning module TIL for geometric
and temporal consistency ensurance respectively. We evaluated the proposed
method against various benchmark methods, with the results showcasing its
superior performance.",2024-11-18 15:12:59+00:00,"['Zhen Lv', 'Yangqi Long', 'Congzhentao Huang', 'Cao Li', 'Chengfei Lv', 'Hao Ren', 'Dian Zheng']",http://arxiv.org/abs/2411.11934v1
VidPanos: Generative Panoramic Videos from Casual Panning Videos,"Panoramic image stitching provides a unified, wide-angle view of a scene that
extends beyond the camera's field of view. Stitching frames of a panning video
into a panoramic photograph is a well-understood problem for stationary scenes,
but when objects are moving, a still panorama cannot capture the scene. We
present a method for synthesizing a panoramic video from a casually-captured
panning video, as if the original video were captured with a wide-angle camera.
We pose panorama synthesis as a space-time outpainting problem, where we aim to
create a full panoramic video of the same length as the input video. Consistent
completion of the space-time volume requires a powerful, realistic prior over
video content and motion, for which we adapt generative video models. Existing
generative models do not, however, immediately extend to panorama completion,
as we show. We instead apply video generation as a component of our panorama
synthesis system, and demonstrate how to exploit the strengths of the models
while minimizing their limitations. Our system can create video panoramas for a
range of in-the-wild scenes including people, vehicles, and flowing water, as
well as stationary background features.",2024-10-17 17:53:24+00:00,"['Jingwei Ma', 'Erika Lu', 'Roni Paiss', 'Shiran Zada', 'Aleksander Holynski', 'Tali Dekel', 'Brian Curless', 'Michael Rubinstein', 'Forrester Cole']",http://arxiv.org/abs/2410.13832v2
Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis,"Accurate reconstruction of complex dynamic scenes from just a single
viewpoint continues to be a challenging task in computer vision. Current
dynamic novel view synthesis methods typically require videos from many
different camera viewpoints, necessitating careful recording setups, and
significantly restricting their utility in the wild as well as in terms of
embodied AI applications. In this paper, we propose $\textbf{GCD}$, a
controllable monocular dynamic view synthesis pipeline that leverages
large-scale diffusion priors to, given a video of any scene, generate a
synchronous video from any other chosen perspective, conditioned on a set of
relative camera pose parameters. Our model does not require depth as input, and
does not explicitly model 3D scene geometry, instead performing end-to-end
video-to-video translation in order to achieve its goal efficiently. Despite
being trained on synthetic multi-view video data only, zero-shot real-world
generalization experiments show promising results in multiple domains,
including robotics, object permanence, and driving environments. We believe our
framework can potentially unlock powerful applications in rich dynamic scene
understanding, perception for robotics, and interactive 3D video viewing
experiences for virtual reality.",2024-05-23 17:59:52+00:00,"['Basile Van Hoorick', 'Rundi Wu', 'Ege Ozguroglu', 'Kyle Sargent', 'Ruoshi Liu', 'Pavel Tokmakov', 'Achal Dave', 'Changxi Zheng', 'Carl Vondrick']",http://arxiv.org/abs/2405.14868v2
LVD-2M: A Long-take Video Dataset with Temporally Dense Captions,"The efficacy of video generation models heavily depends on the quality of
their training datasets. Most previous video generation models are trained on
short video clips, while recently there has been increasing interest in
training long video generation models directly on longer videos. However, the
lack of such high-quality long videos impedes the advancement of long video
generation. To promote research in long video generation, we desire a new
dataset with four key features essential for training long video generation
models: (1) long videos covering at least 10 seconds, (2) long-take videos
without cuts, (3) large motion and diverse contents, and (4) temporally dense
captions. To achieve this, we introduce a new pipeline for selecting
high-quality long-take videos and generating temporally dense captions.
Specifically, we define a set of metrics to quantitatively assess video quality
including scene cuts, dynamic degrees, and semantic-level quality, enabling us
to filter high-quality long-take videos from a large amount of source videos.
Subsequently, we develop a hierarchical video captioning pipeline to annotate
long videos with temporally-dense captions. With this pipeline, we curate the
first long-take video dataset, LVD-2M, comprising 2 million long-take videos,
each covering more than 10 seconds and annotated with temporally dense
captions. We further validate the effectiveness of LVD-2M by fine-tuning video
generation models to generate long videos with dynamic motions. We believe our
work will significantly contribute to future research in long video generation.",2024-10-14 17:59:56+00:00,"['Tianwei Xiong', 'Yuqing Wang', 'Daquan Zhou', 'Zhijie Lin', 'Jiashi Feng', 'Xihui Liu']",http://arxiv.org/abs/2410.10816v1
SVS-GAN: Leveraging GANs for Semantic Video Synthesis,"In recent years, there has been a growing interest in Semantic Image
Synthesis (SIS) through the use of Generative Adversarial Networks (GANs) and
diffusion models. This field has seen innovations such as the implementation of
specialized loss functions tailored for this task, diverging from the more
general approaches in Image-to-Image (I2I) translation. While the concept of
Semantic Video Synthesis (SVS)$\unicode{x2013}$the generation of temporally
coherent, realistic sequences of images from semantic maps$\unicode{x2013}$is
newly formalized in this paper, some existing methods have already explored
aspects of this field. Most of these approaches rely on generic loss functions
designed for video-to-video translation or require additional data to achieve
temporal coherence. In this paper, we introduce the SVS-GAN, a framework
specifically designed for SVS, featuring a custom architecture and loss
functions. Our approach includes a triple-pyramid generator that utilizes SPADE
blocks. Additionally, we employ a U-Net-based network for the image
discriminator, which performs semantic segmentation for the OASIS loss. Through
this combination of tailored architecture and objective engineering, our
framework aims to bridge the existing gap between SIS and SVS, outperforming
current state-of-the-art models on datasets like Cityscapes and KITTI-360.",2024-09-09 21:14:44+00:00,"['Khaled M. Seyam', 'Julian Wiederer', 'Markus Braun', 'Bin Yang']",http://arxiv.org/abs/2409.06074v1
Fine-gained Zero-shot Video Sampling,"Incorporating a temporal dimension into pretrained image diffusion models for
video generation is a prevalent approach. However, this method is
computationally demanding and necessitates large-scale video datasets. More
critically, the heterogeneity between image and video datasets often results in
catastrophic forgetting of the image expertise. Recent attempts to directly
extract video snippets from image diffusion models have somewhat mitigated
these problems. Nevertheless, these methods can only generate brief video clips
with simple movements and fail to capture fine-grained motion or non-grid
deformation. In this paper, we propose a novel Zero-Shot video Sampling
algorithm, denoted as $\mathcal{ZS}^2$, capable of directly sampling
high-quality video clips from existing image synthesis methods, such as Stable
Diffusion, without any training or optimization. Specifically, $\mathcal{ZS}^2$
utilizes the dependency noise model and temporal momentum attention to ensure
content consistency and animation coherence, respectively. This ability enables
it to excel in related tasks, such as conditional and context-specialized video
generation and instruction-guided video editing. Experimental results
demonstrate that $\mathcal{ZS}^2$ achieves state-of-the-art performance in
zero-shot video generation, occasionally outperforming recent supervised
methods.
  Homepage: \url{https://densechen.github.io/zss/}.",2024-07-31 09:36:58+00:00,"['Dengsheng Chen', 'Jie Hu', 'Xiaoming Wei', 'Enhua Wu']",http://arxiv.org/abs/2407.21475v1
Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes,"State-of-the-art novel view synthesis methods achieve impressive results for
multi-view captures of static 3D scenes. However, the reconstructed scenes
still lack ""liveliness,"" a key component for creating engaging 3D experiences.
Recently, novel video diffusion models generate realistic videos with complex
motion and enable animations of 2D images, however they cannot naively be used
to animate 3D scenes as they lack multi-view consistency. To breathe life into
the static world, we propose Gaussians2Life, a method for animating parts of
high-quality 3D scenes in a Gaussian Splatting representation. Our key idea is
to leverage powerful video diffusion models as the generative component of our
model and to combine these with a robust technique to lift 2D videos into
meaningful 3D motion. We find that, in contrast to prior work, this enables
realistic animations of complex, pre-existing 3D scenes and further enables the
animation of a large variety of object classes, while related work is mostly
focused on prior-based character animation, or single 3D objects. Our model
enables the creation of consistent, immersive 3D experiences for arbitrary
scenes.",2024-11-28 16:01:58+00:00,"['Thomas Wimmer', 'Michael Oechsle', 'Michael Niemeyer', 'Federico Tombari']",http://arxiv.org/abs/2411.19233v2
Driving Scene Synthesis on Free-form Trajectories with Generative Prior,"Driving scene synthesis along free-form trajectories is essential for driving
simulations to enable closed-loop evaluation of end-to-end driving policies.
While existing methods excel at novel view synthesis on recorded trajectories,
they face challenges with novel trajectories due to limited views of driving
videos and the vastness of driving environments. To tackle this challenge, we
propose a novel free-form driving view synthesis approach, dubbed DriveX, by
leveraging video generative prior to optimize a 3D model across a variety of
trajectories. Concretely, we crafted an inverse problem that enables a video
diffusion model to be utilized as a prior for many-trajectory optimization of a
parametric 3D model (e.g., Gaussian splatting). To seamlessly use the
generative prior, we iteratively conduct this process during optimization. Our
resulting model can produce high-fidelity virtual driving environments outside
the recorded trajectory, enabling free-form trajectory driving simulation.
Beyond real driving scenes, DriveX can also be utilized to simulate virtual
driving worlds from AI-generated videos.",2024-12-02 17:07:53+00:00,"['Zeyu Yang', 'Zijie Pan', 'Yuankun Yang', 'Xiatian Zhu', 'Li Zhang']",http://arxiv.org/abs/2412.01717v1
xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations,"We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of
producing realistic scenes from textual descriptions. Building on recent
advancements, such as OpenAI's Sora, we explore the latent diffusion model
(LDM) architecture and introduce a video variational autoencoder (VidVAE).
VidVAE compresses video data both spatially and temporally, significantly
reducing the length of visual tokens and the computational demands associated
with generating long-sequence videos. To further address the computational
costs, we propose a divide-and-merge strategy that maintains temporal
consistency across video segments. Our Diffusion Transformer (DiT) model
incorporates spatial and temporal self-attention layers, enabling robust
generalization across different timeframes and aspect ratios. We have devised a
data processing pipeline from the very beginning and collected over 13M
high-quality video-text pairs. The pipeline includes multiple steps such as
clipping, text detection, motion estimation, aesthetics scoring, and dense
captioning based on our in-house video-LLM model. Training the VidVAE and DiT
models required approximately 40 and 642 H100 days, respectively. Our model
supports over 14-second 720p video generation in an end-to-end way and
demonstrates competitive performance against state-of-the-art T2V models.",2024-08-22 17:55:22+00:00,"['Can Qin', 'Congying Xia', 'Krithika Ramakrishnan', 'Michael Ryoo', 'Lifu Tu', 'Yihao Feng', 'Manli Shu', 'Honglu Zhou', 'Anas Awadalla', 'Jun Wang', 'Senthil Purushwalkam', 'Le Xue', 'Yingbo Zhou', 'Huan Wang', 'Silvio Savarese', 'Juan Carlos Niebles', 'Zeyuan Chen', 'Ran Xu', 'Caiming Xiong']",http://arxiv.org/abs/2408.12590v2
Progressive Autoregressive Video Diffusion Models,"Current frontier video diffusion models have demonstrated remarkable results
at generating high-quality videos. However, they can only generate short video
clips, normally around 10 seconds or 240 frames, due to computation limitations
during training. In this work, we show that existing models can be naturally
extended to autoregressive video diffusion models without changing the
architectures. Our key idea is to assign the latent frames with progressively
increasing noise levels rather than a single noise level, which allows for
fine-grained condition among the latents and large overlaps between the
attention windows. Such progressive video denoising allows our models to
autoregressively generate video frames without quality degradation or abrupt
scene changes. We present state-of-the-art results on long video generation at
1 minute (1440 frames at 24 FPS). Videos from this paper are available at
https://desaixie.github.io/pa-vdm/.",2024-10-10 17:36:15+00:00,"['Desai Xie', 'Zhan Xu', 'Yicong Hong', 'Hao Tan', 'Difan Liu', 'Feng Liu', 'Arie Kaufman', 'Yang Zhou']",http://arxiv.org/abs/2410.08151v1
DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving,"The advancement of autonomous driving technologies necessitates increasingly
sophisticated methods for understanding and predicting real-world scenarios.
Vision language models (VLMs) are emerging as revolutionary tools with
significant potential to influence autonomous driving. In this paper, we
propose the DriveGenVLM framework to generate driving videos and use VLMs to
understand them. To achieve this, we employ a video generation framework
grounded in denoising diffusion probabilistic models (DDPM) aimed at predicting
real-world video sequences. We then explore the adequacy of our generated
videos for use in VLMs by employing a pre-trained model known as Efficient
In-context Learning on Egocentric Videos (EILEV). The diffusion model is
trained with the Waymo open dataset and evaluated using the Fr\'echet Video
Distance (FVD) score to ensure the quality and realism of the generated videos.
Corresponding narrations are provided by EILEV for these generated videos,
which may be beneficial in the autonomous driving domain. These narrations can
enhance traffic scene understanding, aid in navigation, and improve planning
capabilities. The integration of video generation with VLMs in the DriveGenVLM
framework represents a significant step forward in leveraging advanced AI
models to address complex challenges in autonomous driving.",2024-08-29 15:52:56+00:00,"['Yongjie Fu', 'Anmol Jain', 'Xuan Di', 'Xu Chen', 'Zhaobin Mo']",http://arxiv.org/abs/2408.16647v1
RACCooN: A Versatile Instructional Video Editing Framework with Auto-Generated Narratives,"Recent video generative models primarily rely on carefully written text
prompts for specific tasks, like inpainting or style editing. They require
labor-intensive textual descriptions for input videos, hindering their
flexibility to adapt personal/raw videos to user specifications. This paper
proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video
generative framework that supports multiple video editing capabilities such as
removal, addition, and modification, through a unified pipeline. RACCooN
consists of two principal stages: Video-to-Paragraph (V2P) and
Paragraph-to-Video (P2V). In the V2P stage, we automatically describe video
scenes in well-structured natural language, capturing both the holistic context
and focused object details. Subsequently, in the P2V stage, users can
optionally refine these descriptions to guide the video diffusion model,
enabling various modifications to the input video, such as removing, changing
subjects, and/or adding new objects. The proposed approach stands out from
other methods through several significant contributions: (1) RACCooN suggests a
multi-granular spatiotemporal pooling strategy to generate well-structured
video descriptions, capturing both the broad context and object details without
requiring complex human annotations, simplifying precise video content editing
based on text for users. (2) Our video generative model incorporates
auto-generated narratives or instructions to enhance the quality and accuracy
of the generated content. (3) RACCooN also plans to imagine new objects in a
given video, so users simply prompt the model to receive a detailed video
editing plan for complex video editing. The proposed framework demonstrates
impressive versatile capabilities in video-to-paragraph generation, video
content editing, and can be incorporated into other SoTA video generative
models for further enhancement.",2024-05-28 17:46:36+00:00,"['Jaehong Yoon', 'Shoubin Yu', 'Mohit Bansal']",http://arxiv.org/abs/2405.18406v3
Illumination Histogram Consistency Metric for Quantitative Assessment of Video Sequences,"The advances in deep generative models have greatly accelerate the process of
video procession such as video enhancement and synthesis. Learning
spatio-temporal video models requires to capture the temporal dynamics of a
scene, in addition to the visual appearance of individual frames. Illumination
consistency, which reflects the variations of illumination in the dynamic video
sequences, play a vital role in video processing. Unfortunately, to date, no
well-accepted quantitative metric has been proposed for video illumination
consistency evaluation. In this paper, we propose a illumination histogram
consistency (IHC) metric to quantitatively and automatically evaluate the
illumination consistency of the video sequences. IHC measures the illumination
variation of any video sequence based on the illumination histogram
discrepancies across all the frames in the video sequence. Specifically, given
a video sequence, we first estimate the illumination map of each individual
frame using the Retinex model; Then, using the illumination maps, the mean
illumination histogram of the video sequence is computed by the mean operation
across all the frames; Next, we compute the illumination histogram discrepancy
between each individual frame and the mean illumination histogram and sum up
all the illumination histogram discrepancies to represent the illumination
variations of the video sequence. Finally, we obtain the IHC score from the
illumination histogram discrepancies via normalization and subtraction
operations. Experiments are conducted to illustrate the performance of the
proposed IHC metric and its capability to measure the illumination variations
in video sequences. The source code is available on
\url{https://github.com/LongChenCV/IHC-Metric}.",2024-05-15 22:11:52+00:00,"['Long Chen', 'Mobarakol Islam', 'Matt Clarkson', 'Thomas Dowrick']",http://arxiv.org/abs/2405.09716v1
SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency,"We present Stable Video 4D (SV4D), a latent video diffusion model for
multi-frame and multi-view consistent dynamic 3D content generation. Unlike
previous methods that rely on separately trained generative models for video
generation and novel view synthesis, we design a unified diffusion model to
generate novel view videos of dynamic 3D objects. Specifically, given a
monocular reference video, SV4D generates novel views for each video frame that
are temporally consistent. We then use the generated novel view videos to
optimize an implicit 4D representation (dynamic NeRF) efficiently, without the
need for cumbersome SDS-based optimization used in most prior works. To train
our unified novel view video generation model, we curate a dynamic 3D object
dataset from the existing Objaverse dataset. Extensive experimental results on
multiple datasets and user studies demonstrate SV4D's state-of-the-art
performance on novel-view video synthesis as well as 4D generation compared to
prior works.",2024-07-24 17:59:43+00:00,"['Yiming Xie', 'Chun-Han Yao', 'Vikram Voleti', 'Huaizu Jiang', 'Varun Jampani']",http://arxiv.org/abs/2407.17470v2
AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers,"Numerous works have recently integrated 3D camera control into foundational
text-to-video models, but the resulting camera control is often imprecise, and
video generation quality suffers. In this work, we analyze camera motion from a
first principles perspective, uncovering insights that enable precise 3D camera
manipulation without compromising synthesis quality. First, we determine that
motion induced by camera movements in videos is low-frequency in nature. This
motivates us to adjust train and test pose conditioning schedules, accelerating
training convergence while improving visual and motion quality. Then, by
probing the representations of an unconditional video diffusion transformer, we
observe that they implicitly perform camera pose estimation under the hood, and
only a sub-portion of their layers contain the camera information. This
suggested us to limit the injection of camera conditioning to a subset of the
architecture to prevent interference with other video features, leading to a 4x
reduction of training parameters, improved training speed, and 10% higher
visual quality. Finally, we complement the typical dataset for camera control
learning with a curated dataset of 20K diverse, dynamic videos with stationary
cameras. This helps the model distinguish between camera and scene motion and
improves the dynamics of generated pose-conditioned videos. We compound these
findings to design the Advanced 3D Camera Control (AC3D) architecture, the new
state-of-the-art model for generative video modeling with camera control.",2024-11-27 18:49:13+00:00,"['Sherwin Bahmani', 'Ivan Skorokhodov', 'Guocheng Qian', 'Aliaksandr Siarohin', 'Willi Menapace', 'Andrea Tagliasacchi', 'David B. Lindell', 'Sergey Tulyakov']",http://arxiv.org/abs/2411.18673v3
Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach,"Diffusion models have revolutionized image generation, and their extension to
video generation has shown promise. However, current video diffusion
models~(VDMs) rely on a scalar timestep variable applied at the clip level,
which limits their ability to model complex temporal dependencies needed for
various tasks like image-to-video generation. To address this limitation, we
propose a frame-aware video diffusion model~(FVDM), which introduces a novel
vectorized timestep variable~(VTV). Unlike conventional VDMs, our approach
allows each frame to follow an independent noise schedule, enhancing the
model's capacity to capture fine-grained temporal dependencies. FVDM's
flexibility is demonstrated across multiple tasks, including standard video
generation, image-to-video generation, video interpolation, and long video
synthesis. Through a diverse set of VTV configurations, we achieve superior
quality in generated videos, overcoming challenges such as catastrophic
forgetting during fine-tuning and limited generalizability in zero-shot
methods.Our empirical evaluations show that FVDM outperforms state-of-the-art
methods in video generation quality, while also excelling in extended tasks. By
addressing fundamental shortcomings in existing VDMs, FVDM sets a new paradigm
in video synthesis, offering a robust framework with significant implications
for generative modeling and multimedia applications.",2024-10-04 05:47:39+00:00,"['Yaofang Liu', 'Yumeng Ren', 'Xiaodong Cun', 'Aitor Artola', 'Yang Liu', 'Tieyong Zeng', 'Raymond H. Chan', 'Jean-michel Morel']",http://arxiv.org/abs/2410.03160v1
NVS-Solver: Video Diffusion Model as Zero-Shot Novel View Synthesizer,"By harnessing the potent generative capabilities of pre-trained large video
diffusion models, we propose NVS-Solver, a new novel view synthesis (NVS)
paradigm that operates \textit{without} the need for training. NVS-Solver
adaptively modulates the diffusion sampling process with the given views to
enable the creation of remarkable visual experiences from single or multiple
views of static scenes or monocular videos of dynamic scenes. Specifically,
built upon our theoretical modeling, we iteratively modulate the score function
with the given scene priors represented with warped input views to control the
video diffusion process. Moreover, by theoretically exploring the boundary of
the estimation error, we achieve the modulation in an adaptive fashion
according to the view pose and the number of diffusion steps. Extensive
evaluations on both static and dynamic scenes substantiate the significant
superiority of our NVS-Solver over state-of-the-art methods both quantitatively
and qualitatively. \textit{ Source code in }
\href{https://github.com/ZHU-Zhiyu/NVS_Solver}{https://github.com/ZHU-Zhiyu/NVS$\_$Solver}.",2024-05-24 08:56:19+00:00,"['Meng You', 'Zhiyu Zhu', 'Hui Liu', 'Junhui Hou']",http://arxiv.org/abs/2405.15364v1
Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models,"3D meshes are widely used in computer vision and graphics for their
efficiency in animation and minimal memory use, playing a crucial role in
movies, games, AR, and VR. However, creating temporally consistent and
realistic textures for mesh sequences remains labor-intensive for professional
artists. On the other hand, while video diffusion models excel at text-driven
video generation, they often lack 3D geometry awareness and struggle with
achieving multi-view consistent texturing for 3D meshes. In this work, we
present Tex4D, a zero-shot approach that integrates inherent 3D geometry
knowledge from mesh sequences with the expressiveness of video diffusion models
to produce multi-view and temporally consistent 4D textures. Given an
untextured mesh sequence and a text prompt as inputs, our method enhances
multi-view consistency by synchronizing the diffusion process across different
views through latent aggregation in the UV space. To ensure temporal
consistency, we leverage prior knowledge from a conditional video generation
model for texture synthesis. However, straightforwardly combining the video
diffusion model and the UV texture aggregation leads to blurry results. We
analyze the underlying causes and propose a simple yet effective modification
to the DDIM sampling process to address this issue. Additionally, we introduce
a reference latent texture to strengthen the correlation between frames during
the denoising process. To the best of our knowledge, Tex4D is the first method
specifically designed for 4D scene texturing. Extensive experiments demonstrate
its superiority in producing multi-view and multi-frame consistent videos based
on untextured mesh sequences.",2024-10-14 17:59:59+00:00,"['Jingzhi Bao', 'Xueting Li', 'Ming-Hsuan Yang']",http://arxiv.org/abs/2410.10821v2
Video Diffusion Transformers are In-Context Learners,"This paper investigates a solution for enabling in-context capabilities of
video diffusion transformers, with minimal tuning required for activation.
Specifically, we propose a simple pipeline to leverage in-context generation:
($\textbf{i}$) concatenate videos along spacial or time dimension,
($\textbf{ii}$) jointly caption multi-scene video clips from one source, and
($\textbf{iii}$) apply task-specific fine-tuning using carefully curated small
datasets. Through a series of diverse controllable tasks, we demonstrate
qualitatively that existing advanced text-to-video models can effectively
perform in-context generation. Notably, it allows for the creation of
consistent multi-scene videos exceeding 30 seconds in duration, without
additional computational overhead. Importantly, this method requires no
modifications to the original models, results in high-fidelity video outputs
that better align with prompt specifications and maintain role consistency. Our
framework presents a valuable tool for the research community and offers
critical insights for advancing product-level controllable video generation
systems. The data, code, and model weights are publicly available at:
https://github.com/feizc/Video-In-Context.",2024-12-14 10:39:55+00:00,"['Zhengcong Fei', 'Di Qiu', 'Debang Li', 'Changqian Yu', 'Mingyuan Fan']",http://arxiv.org/abs/2412.10783v3
360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model,"Panorama video recently attracts more interest in both study and application,
courtesy of its immersive experience. Due to the expensive cost of capturing
360-degree panoramic videos, generating desirable panorama videos by prompts is
urgently required. Lately, the emerging text-to-video (T2V) diffusion methods
demonstrate notable effectiveness in standard video generation. However, due to
the significant gap in content and motion patterns between panoramic and
standard videos, these methods encounter challenges in yielding satisfactory
360-degree panoramic videos. In this paper, we propose a pipeline named
360-Degree Video Diffusion model (360DVD) for generating 360-degree panoramic
videos based on the given prompts and motion conditions. Specifically, we
introduce a lightweight 360-Adapter accompanied by 360 Enhancement Techniques
to transform pre-trained T2V models for panorama video generation. We further
propose a new panorama dataset named WEB360 consisting of panoramic video-text
pairs for training 360DVD, addressing the absence of captioned panoramic video
datasets. Extensive experiments demonstrate the superiority and effectiveness
of 360DVD for panorama video generation. Our project page is at
https://akaneqwq.github.io/360DVD/.",2024-01-12 13:52:29+00:00,"['Qian Wang', 'Weiqi Li', 'Chong Mou', 'Xinhua Cheng', 'Jian Zhang']",http://arxiv.org/abs/2401.06578v2
Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation,"In recent years, there has been a significant surge of interest in unifying
image comprehension and generation within Large Language Models (LLMs). This
growing interest has prompted us to explore extending this unification to
videos. The core challenge lies in developing a versatile video tokenizer that
captures both the spatial characteristics and temporal dynamics of videos to
obtain representations for LLMs, and the representations can be further decoded
into realistic video clips to enable video generation. In this work, we
introduce Divot, a Diffusion-Powered Video Tokenizer, which leverages the
diffusion process for self-supervised video representation learning. We posit
that if a video diffusion model can effectively de-noise video clips by taking
the features of a video tokenizer as the condition, then the tokenizer has
successfully captured robust spatial and temporal information. Additionally,
the video diffusion model inherently functions as a de-tokenizer, decoding
videos from their representations. Building upon the Divot tokenizer, we
present Divot-Vicuna through video-to-text autoregression and text-to-video
generation by modeling the distributions of continuous-valued Divot features
with a Gaussian Mixture Model. Experimental results demonstrate that our
diffusion-based video tokenizer, when integrated with a pre-trained LLM,
achieves competitive performance across various video comprehension and
generation benchmarks. The instruction tuned Divot-Vicuna also excels in video
storytelling, generating interleaved narratives and corresponding videos.",2024-12-05 18:53:04+00:00,"['Yuying Ge', 'Yizhuo Li', 'Yixiao Ge', 'Ying Shan']",http://arxiv.org/abs/2412.04432v1
DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion,"In this paper, we introduce \textbf{DimensionX}, a framework designed to
generate photorealistic 3D and 4D scenes from just a single image with video
diffusion. Our approach begins with the insight that both the spatial structure
of a 3D scene and the temporal evolution of a 4D scene can be effectively
represented through sequences of video frames. While recent video diffusion
models have shown remarkable success in producing vivid visuals, they face
limitations in directly recovering 3D/4D scenes due to limited spatial and
temporal controllability during generation. To overcome this, we propose
ST-Director, which decouples spatial and temporal factors in video diffusion by
learning dimension-aware LoRAs from dimension-variant data. This controllable
video diffusion approach enables precise manipulation of spatial structure and
temporal dynamics, allowing us to reconstruct both 3D and 4D representations
from sequential frames with the combination of spatial and temporal dimensions.
Additionally, to bridge the gap between generated videos and real-world scenes,
we introduce a trajectory-aware mechanism for 3D generation and an
identity-preserving denoising strategy for 4D generation. Extensive experiments
on various real-world and synthetic datasets demonstrate that DimensionX
achieves superior results in controllable video generation, as well as in 3D
and 4D scene generation, compared with previous methods.",2024-11-07 18:07:31+00:00,"['Wenqiang Sun', 'Shuo Chen', 'Fangfu Liu', 'Zilong Chen', 'Yueqi Duan', 'Jun Zhang', 'Yikai Wang']",http://arxiv.org/abs/2411.04928v1
OSV: One Step is Enough for High-Quality Image to Video Generation,"Video diffusion models have shown great potential in generating high-quality
videos, making them an increasingly popular focus. However, their inherent
iterative nature leads to substantial computational and time costs. While
efforts have been made to accelerate video diffusion by reducing inference
steps (through techniques like consistency distillation) and GAN training
(these approaches often fall short in either performance or training
stability). In this work, we introduce a two-stage training framework that
effectively combines consistency distillation with GAN training to address
these challenges. Additionally, we propose a novel video discriminator design,
which eliminates the need for decoding the video latents and improves the final
performance. Our model is capable of producing high-quality videos in merely
one-step, with the flexibility to perform multi-step refinement for further
performance enhancement. Our quantitative evaluation on the OpenWebVid-1M
benchmark shows that our model significantly outperforms existing methods.
Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of
the consistency distillation based method, AnimateLCM (FVD 184.79), and
approaches the 25-step performance of advanced Stable Video Diffusion (FVD
156.94).",2024-09-17 17:16:37+00:00,"['Xiaofeng Mao', 'Zhengkai Jiang', 'Fu-Yun Wang', 'Wenbing Zhu', 'Jiangning Zhang', 'Hao Chen', 'Mingmin Chi', 'Yabiao Wang']",http://arxiv.org/abs/2409.11367v1
LVCD: Reference-based Lineart Video Colorization with Diffusion Models,"We propose the first video diffusion framework for reference-based lineart
video colorization. Unlike previous works that rely solely on image generative
models to colorize lineart frame by frame, our approach leverages a large-scale
pretrained video diffusion model to generate colorized animation videos. This
approach leads to more temporally consistent results and is better equipped to
handle large motions. Firstly, we introduce Sketch-guided ControlNet which
provides additional control to finetune an image-to-video diffusion model for
controllable video synthesis, enabling the generation of animation videos
conditioned on lineart. We then propose Reference Attention to facilitate the
transfer of colors from the reference frame to other frames containing fast and
expansive motions. Finally, we present a novel scheme for sequential sampling,
incorporating the Overlapped Blending Module and Prev-Reference Attention, to
extend the video diffusion model beyond its original fixed-length limitation
for long video colorization. Both qualitative and quantitative results
demonstrate that our method significantly outperforms state-of-the-art
techniques in terms of frame and video quality, as well as temporal
consistency. Moreover, our method is capable of generating high-quality, long
temporal-consistent animation videos with large motions, which is not
achievable in previous works. Our code and model are available at
https://luckyhzt.github.io/lvcd.",2024-09-19 17:59:48+00:00,"['Zhitong Huang', 'Mohan Zhang', 'Jing Liao']",http://arxiv.org/abs/2409.12960v1
MagicDrive-V2: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control,"The rapid advancement of diffusion models has greatly improved video
synthesis, especially in controllable video generation, which is vital for
applications like autonomous driving. Although DiT with 3D VAE has become a
standard framework for video generation, it introduces challenges in
controllable driving video generation, especially for geometry control,
rendering existing control methods ineffective. To address these issues, we
propose MagicDrive-V2, a novel approach that integrates the MVDiT block and
spatial-temporal conditional encoding to enable multi-view video generation and
precise geometric control. Additionally, we introduce an efficient method for
obtaining contextual descriptions for videos to support diverse textual
control, along with a progressive training strategy using mixed video data to
enhance training efficiency and generalizability. Consequently, MagicDrive-V2
enables multi-view driving video synthesis with $3.3\times$ resolution and
$4\times$ frame count (compared to current SOTA), rich contextual control, and
geometric controls. Extensive experiments demonstrate MagicDrive-V2's ability,
unlocking broader applications in autonomous driving.",2024-11-21 03:13:30+00:00,"['Ruiyuan Gao', 'Kai Chen', 'Bo Xiao', 'Lanqing Hong', 'Zhenguo Li', 'Qiang Xu']",http://arxiv.org/abs/2411.13807v3
VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis,"Despite tremendous progress in the field of text-to-video (T2V) synthesis,
open-sourced T2V diffusion models struggle to generate longer videos with
dynamically varying and evolving content. They tend to synthesize quasi-static
videos, ignoring the necessary visual change-over-time implied in the text
prompt. At the same time, scaling these models to enable longer, more dynamic
video synthesis often remains computationally intractable. To address this
challenge, we introduce the concept of Generative Temporal Nursing (GTN), where
we aim to alter the generative process on the fly during inference to improve
control over the temporal dynamics and enable generation of longer videos. We
propose a method for GTN, dubbed VSTAR, which consists of two key ingredients:
1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis
based on the original single prompt leveraging LLMs, which gives accurate
textual guidance to different visual states of longer videos, and 2) Temporal
Attention Regularization (TAR) - a regularization technique to refine the
temporal attention units of the pre-trained T2V diffusion models, which enables
control over the video dynamics. We experimentally showcase the superiority of
the proposed approach in generating longer, visually appealing videos over
existing open-sourced T2V models. We additionally analyze the temporal
attention maps realized with and without VSTAR, demonstrating the importance of
applying our method to mitigate neglect of the desired visual change over time.",2024-03-20 10:58:58+00:00,"['Yumeng Li', 'William Beluch', 'Margret Keuper', 'Dan Zhang', 'Anna Khoreva']",http://arxiv.org/abs/2403.13501v2
Ctrl-V: Higher Fidelity Video Generation with Bounding-Box Controlled Object Motion,"Controllable video generation has attracted significant attention, largely
due to advances in video diffusion models. In domains such as autonomous
driving, it is essential to develop highly accurate predictions for object
motions. This paper tackles a crucial challenge of how to exert precise control
over object motion for realistic video synthesis. To accomplish this, we 1)
control object movements using bounding boxes and extend this control to the
renderings of 2D or 3D boxes in pixel space, 2) employ a distinct, specialized
model to forecast the trajectories of object bounding boxes based on their
previous and, if desired, future positions, and 3) adapt and enhance a separate
video diffusion network to create video content based on these high quality
trajectory forecasts. Our method, Ctrl-V, leverages modified and fine-tuned
Stable Video Diffusion (SVD) models to solve both trajectory and video
generation. Extensive experiments conducted on the KITTI, Virtual-KITTI 2,
BDD100k, and nuScenes datasets validate the effectiveness of our approach in
producing realistic and controllable video generation.",2024-06-09 03:44:35+00:00,"['Ge Ya Luo', 'Zhi Hao Luo', 'Anthony Gosselin', 'Alexia Jolicoeur-Martineau', 'Christopher Pal']",http://arxiv.org/abs/2406.05630v3
"Read, Watch and Scream! Sound Generation from Text and Video","Despite the impressive progress of multimodal generative models,
video-to-audio generation still suffers from limited performance and limits the
flexibility to prioritize sound synthesis for specific objects within the
scene. Conversely, text-to-audio generation methods generate high-quality audio
but pose challenges in ensuring comprehensive scene depiction and time-varying
control. To tackle these challenges, we propose a novel video-and-text-to-audio
generation method, called \ours, where video serves as a conditional control
for a text-to-audio generation model. Especially, our method estimates the
structural information of sound (namely, energy) from the video while receiving
key content cues from a user prompt. We employ a well-performing text-to-audio
model to consolidate the video control, which is much more efficient for
training multimodal diffusion models with massive triplet-paired
(audio-video-text) data. In addition, by separating the generative components
of audio, it becomes a more flexible system that allows users to freely adjust
the energy, surrounding environment, and primary sound source according to
their preferences. Experimental results demonstrate that our method shows
superiority in terms of quality, controllability, and training efficiency. Code
and demo are available at https://naver-ai.github.io/rewas.",2024-07-08 01:59:17+00:00,"['Yujin Jeong', 'Yunji Kim', 'Sanghyuk Chun', 'Jiyoung Lee']",http://arxiv.org/abs/2407.05551v2
SF-V: Single Forward Video Generation Model,"Diffusion-based video generation models have demonstrated remarkable success
in obtaining high-fidelity videos through the iterative denoising process.
However, these models require multiple denoising steps during sampling,
resulting in high computational costs. In this work, we propose a novel
approach to obtain single-step video generation models by leveraging
adversarial training to fine-tune pre-trained video diffusion models. We show
that, through the adversarial training, the multi-steps video diffusion model,
i.e., Stable Video Diffusion (SVD), can be trained to perform single forward
pass to synthesize high-quality videos, capturing both temporal and spatial
dependencies in the video data. Extensive experiments demonstrate that our
method achieves competitive generation quality of synthesized videos with
significantly reduced computational overhead for the denoising process (i.e.,
around $23\times$ speedup compared with SVD and $6\times$ speedup compared with
existing works, with even better generation quality), paving the way for
real-time video synthesis and editing. More visualization results are made
publicly available at https://snap-research.github.io/SF-V.",2024-06-06 17:58:27+00:00,"['Zhixing Zhang', 'Yanyu Li', 'Yushu Wu', 'Yanwu Xu', 'Anil Kag', 'Ivan Skorokhodov', 'Willi Menapace', 'Aliaksandr Siarohin', 'Junli Cao', 'Dimitris Metaxas', 'Sergey Tulyakov', 'Jian Ren']",http://arxiv.org/abs/2406.04324v2
4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models,"Existing dynamic scene generation methods mostly rely on distilling knowledge
from pre-trained 3D generative models, which are typically fine-tuned on
synthetic object datasets. As a result, the generated scenes are often
object-centric and lack photorealism. To address these limitations, we
introduce a novel pipeline designed for photorealistic text-to-4D scene
generation, discarding the dependency on multi-view generative models and
instead fully utilizing video generative models trained on diverse real-world
datasets. Our method begins by generating a reference video using the video
generation model. We then learn the canonical 3D representation of the video
using a freeze-time video, delicately generated from the reference video. To
handle inconsistencies in the freeze-time video, we jointly learn a per-frame
deformation to model these imperfections. We then learn the temporal
deformation based on the canonical representation to capture dynamic
interactions in the reference video. The pipeline facilitates the generation of
dynamic scenes with enhanced photorealism and structural integrity, viewable
from multiple perspectives, thereby setting a new standard in 4D scene
generation.",2024-06-11 17:19:26+00:00,"['Heng Yu', 'Chaoyang Wang', 'Peiye Zhuang', 'Willi Menapace', 'Aliaksandr Siarohin', 'Junli Cao', 'Laszlo A Jeni', 'Sergey Tulyakov', 'Hsin-Ying Lee']",http://arxiv.org/abs/2406.07472v2
SNED: Superposition Network Architecture Search for Efficient Video Diffusion Model,"While AI-generated content has garnered significant attention, achieving
photo-realistic video synthesis remains a formidable challenge. Despite the
promising advances in diffusion models for video generation quality, the
complex model architecture and substantial computational demands for both
training and inference create a significant gap between these models and
real-world applications. This paper presents SNED, a superposition network
architecture search method for efficient video diffusion model. Our method
employs a supernet training paradigm that targets various model cost and
resolution options using a weight-sharing method. Moreover, we propose the
supernet training sampling warm-up for fast training optimization. To showcase
the flexibility of our method, we conduct experiments involving both
pixel-space and latent-space video diffusion models. The results demonstrate
that our framework consistently produces comparable results across different
model options with high efficiency. According to the experiment for the
pixel-space video diffusion model, we can achieve consistent video generation
results simultaneously across 64 x 64 to 256 x 256 resolutions with a large
range of model sizes from 640M to 1.6B number of parameters for pixel-space
video diffusion models.",2024-05-31 21:12:30+00:00,"['Zhengang Li', 'Yan Kang', 'Yuchen Liu', 'Difan Liu', 'Tobias Hinz', 'Feng Liu', 'Yanzhi Wang']",http://arxiv.org/abs/2406.00195v1
VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement,"Recent text-to-video (T2V) diffusion models have demonstrated impressive
generation capabilities across various domains. However, these models often
generate videos that have misalignments with text prompts, especially when the
prompts describe complex scenes with multiple objects and attributes. To
address this, we introduce VideoRepair, a novel model-agnostic, training-free
video refinement framework that automatically identifies fine-grained
text-video misalignments and generates explicit spatial and textual feedback,
enabling a T2V diffusion model to perform targeted, localized refinements.
VideoRepair consists of two stages: In (1) video refinement planning, we first
detect misalignments by generating fine-grained evaluation questions and
answering them using an MLLM. Based on video evaluation outputs, we identify
accurately generated objects and construct localized prompts to precisely
refine misaligned regions. In (2) localized refinement, we enhance video
alignment by 'repairing' the misaligned regions from the original video while
preserving the correctly generated areas. This is achieved by frame-wise region
decomposition using our Region-Preserving Segmentation (RPS) module. On two
popular video generation benchmarks (EvalCrafter and T2V-CompBench),
VideoRepair substantially outperforms recent baselines across various
text-video alignment metrics. We provide a comprehensive analysis of
VideoRepair components and qualitative examples.",2024-11-22 18:31:47+00:00,"['Daeun Lee', 'Jaehong Yoon', 'Jaemin Cho', 'Mohit Bansal']",http://arxiv.org/abs/2411.15115v2
Lumiere: A Space-Time Diffusion Model for Video Generation,"We introduce Lumiere -- a text-to-video diffusion model designed for
synthesizing videos that portray realistic, diverse and coherent motion -- a
pivotal challenge in video synthesis. To this end, we introduce a Space-Time
U-Net architecture that generates the entire temporal duration of the video at
once, through a single pass in the model. This is in contrast to existing video
models which synthesize distant keyframes followed by temporal super-resolution
-- an approach that inherently makes global temporal consistency difficult to
achieve. By deploying both spatial and (importantly) temporal down- and
up-sampling and leveraging a pre-trained text-to-image diffusion model, our
model learns to directly generate a full-frame-rate, low-resolution video by
processing it in multiple space-time scales. We demonstrate state-of-the-art
text-to-video generation results, and show that our design easily facilitates a
wide range of content creation tasks and video editing applications, including
image-to-video, video inpainting, and stylized generation.",2024-01-23 18:05:25+00:00,"['Omer Bar-Tal', 'Hila Chefer', 'Omer Tov', 'Charles Herrmann', 'Roni Paiss', 'Shiran Zada', 'Ariel Ephrat', 'Junhwa Hur', 'Guanghui Liu', 'Amit Raj', 'Yuanzhen Li', 'Michael Rubinstein', 'Tomer Michaeli', 'Oliver Wang', 'Deqing Sun', 'Tali Dekel', 'Inbar Mosseri']",http://arxiv.org/abs/2401.12945v2
"Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion","This paper examines three major generative modelling frameworks: Variational
Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Stable
Diffusion models. VAEs are effective at learning latent representations but
frequently yield blurry results. GANs can generate realistic images but face
issues such as mode collapse. Stable Diffusion models, while producing
high-quality images with strong semantic coherence, are demanding in terms of
computational resources. Additionally, the paper explores how incorporating
Grounding DINO and Grounded SAM with Stable Diffusion improves image accuracy
by utilising sophisticated segmentation and inpainting techniques. The analysis
guides on selecting suitable models for various applications and highlights
areas for further research.",2024-08-16 13:50:50+00:00,['Sanchayan Vivekananthan'],http://arxiv.org/abs/2408.08751v1
Searching Priors Makes Text-to-Video Synthesis Better,"Significant advancements in video diffusion models have brought substantial
progress to the field of text-to-video (T2V) synthesis. However, existing T2V
synthesis model struggle to accurately generate complex motion dynamics,
leading to a reduction in video realism. One possible solution is to collect
massive data and train the model on it, but this would be extremely expensive.
To alleviate this problem, in this paper, we reformulate the typical T2V
generation process as a search-based generation pipeline. Instead of scaling up
the model training, we employ existing videos as the motion prior database.
Specifically, we divide T2V generation process into two steps: (i) For a given
prompt input, we search existing text-video datasets to find videos with text
labels that closely match the prompt motions. We propose a tailored search
algorithm that emphasizes object motion features. (ii) Retrieved videos are
processed and distilled into motion priors to fine-tune a pre-trained base T2V
model, followed by generating desired videos using input prompt. By utilizing
the priors gleaned from the searched videos, we enhance the realism of the
generated videos' motion. All operations can be finished on a single NVIDIA RTX
4090 GPU. We validate our method against state-of-the-art T2V models across
diverse prompt inputs. The code will be public.",2024-06-05 12:53:28+00:00,"['Haoran Cheng', 'Liang Peng', 'Linxuan Xia', 'Yuepeng Hu', 'Hengjia Li', 'Qinglin Lu', 'Xiaofei He', 'Boxi Wu']",http://arxiv.org/abs/2406.03215v1
Multimodal Semantic Communication for Generative Audio-Driven Video Conferencing,"This paper studies an efficient multimodal data communication scheme for
video conferencing. In our considered system, a speaker gives a talk to the
audiences, with talking head video and audio being transmitted. Since the
speaker does not frequently change posture and high-fidelity transmission of
audio (speech and music) is required, redundant visual video data exists and
can be removed by generating the video from the audio. To this end, we propose
a wave-to-video (Wav2Vid) system, an efficient video transmission framework
that reduces transmitted data by generating talking head video from audio. In
particular, full-duration audio and short-duration video data are synchronously
transmitted through a wireless channel, with neural networks (NNs) extracting
and encoding audio and video semantics. The receiver then combines the decoded
audio and video data, as well as uses a generative adversarial network (GAN)
based model to generate the lip movement videos of the speaker. Simulation
results show that the proposed Wav2Vid system can reduce the amount of
transmitted data by up to 83% while maintaining the perceptual quality of the
generated conferencing video.",2024-10-29 15:11:45+00:00,"['Haonan Tong', 'Haopeng Li', 'Hongyang Du', 'Zhaohui Yang', 'Changchuan Yin', 'Dusit Niyato']",http://arxiv.org/abs/2410.22112v1
Matten: Video Generation with Mamba-Attention,"In this paper, we introduce Matten, a cutting-edge latent diffusion model
with Mamba-Attention architecture for video generation. With minimal
computational cost, Matten employs spatial-temporal attention for local video
content modeling and bidirectional Mamba for global video content modeling. Our
comprehensive experimental evaluation demonstrates that Matten has competitive
performance with the current Transformer-based and GAN-based models in
benchmark performance, achieving superior FVD scores and efficiency.
Additionally, we observe a direct positive correlation between the complexity
of our designed model and the improvement in video quality, indicating the
excellent scalability of Matten.",2024-05-05 18:36:45+00:00,"['Yu Gao', 'Jiancheng Huang', 'Xiaopeng Sun', 'Zequn Jie', 'Yujie Zhong', 'Lin Ma']",http://arxiv.org/abs/2405.03025v2
Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis,"Recent advances in diffusion models have demonstrated exceptional
capabilities in image and video generation, further improving the effectiveness
of 4D synthesis. Existing 4D generation methods can generate high-quality 4D
objects or scenes based on user-friendly conditions, benefiting the gaming and
video industries. However, these methods struggle to synthesize significant
object deformation of complex 4D transitions and interactions within scenes. To
address this challenge, we propose Trans4D, a novel text-to-4D synthesis
framework that enables realistic complex scene transitions. Specifically, we
first use multi-modal large language models (MLLMs) to produce a physic-aware
scene description for 4D scene initialization and effective transition timing
planning. Then we propose a geometry-aware 4D transition network to realize a
complex scene-level 4D transition based on the plan, which involves expressive
geometrical object deformation. Extensive experiments demonstrate that Trans4D
consistently outperforms existing state-of-the-art methods in generating 4D
scenes with accurate and high-quality transitions, validating its
effectiveness. Code: https://github.com/YangLing0818/Trans4D",2024-10-09 17:56:03+00:00,"['Bohan Zeng', 'Ling Yang', 'Siyu Li', 'Jiaming Liu', 'Zixiang Zhang', 'Juanxi Tian', 'Kaixin Zhu', 'Yongzhen Guo', 'Fu-Yun Wang', 'Minkai Xu', 'Stefano Ermon', 'Wentao Zhang']",http://arxiv.org/abs/2410.07155v1
ZeroSmooth: Training-free Diffuser Adaptation for High Frame Rate Video Generation,"Video generation has made remarkable progress in recent years, especially
since the advent of the video diffusion models. Many video generation models
can produce plausible synthetic videos, e.g., Stable Video Diffusion (SVD).
However, most video models can only generate low frame rate videos due to the
limited GPU memory as well as the difficulty of modeling a large set of frames.
The training videos are always uniformly sampled at a specified interval for
temporal compression. Previous methods promote the frame rate by either
training a video interpolation model in pixel space as a postprocessing stage
or training an interpolation model in latent space for a specific base video
model. In this paper, we propose a training-free video interpolation method for
generative video diffusion models, which is generalizable to different models
in a plug-and-play manner. We investigate the non-linearity in the feature
space of video diffusion models and transform a video model into a
self-cascaded video diffusion model with incorporating the designed hidden
state correction modules. The self-cascaded architecture and the correction
module are proposed to retain the temporal consistency between key frames and
the interpolated frames. Extensive evaluations are preformed on multiple
popular video models to demonstrate the effectiveness of the propose method,
especially that our training-free method is even comparable to trained
interpolation models supported by huge compute resources and large-scale
datasets.",2024-06-03 00:31:13+00:00,"['Shaoshu Yang', 'Yong Zhang', 'Xiaodong Cun', 'Ying Shan', 'Ran He']",http://arxiv.org/abs/2406.00908v1
AMG: Avatar Motion Guided Video Generation,"Human video generation task has gained significant attention with the
advancement of deep generative models. Generating realistic videos with human
movements is challenging in nature, due to the intricacies of human body
topology and sensitivity to visual artifacts. The extensively studied 2D media
generation methods take advantage of massive human media datasets, but struggle
with 3D-aware control; whereas 3D avatar-based approaches, while offering more
freedom in control, lack photorealism and cannot be harmonized seamlessly with
background scene. We propose AMG, a method that combines the 2D photorealism
and 3D controllability by conditioning video diffusion models on controlled
rendering of 3D avatars. We additionally introduce a novel data processing
pipeline that reconstructs and renders human avatar movements from dynamic
camera videos. AMG is the first method that enables multi-person diffusion
video generation with precise control over camera positions, human motions, and
background style. We also demonstrate through extensive evaluation that it
outperforms existing human video generation methods conditioned on pose
sequences or driving videos in terms of realism and adaptability.",2024-09-02 23:59:01+00:00,"['Zhangsihao Yang', 'Mengyi Shan', 'Mohammad Farazi', 'Wenhui Zhu', 'Yanxi Chen', 'Xuanzhao Dong', 'Yalin Wang']",http://arxiv.org/abs/2409.01502v1
Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation,"Generating high-quality videos that synthesize desired realistic content is a
challenging task due to their intricate high-dimensionality and complexity of
videos. Several recent diffusion-based methods have shown comparable
performance by compressing videos to a lower-dimensional latent space, using
traditional video autoencoder architecture. However, such method that employ
standard frame-wise 2D and 3D convolution fail to fully exploit the
spatio-temporal nature of videos. To address this issue, we propose a novel
hybrid video diffusion model, called HVDM, which can capture spatio-temporal
dependencies more effectively. The HVDM is trained by a hybrid video
autoencoder which extracts a disentangled representation of the video
including: (i) a global context information captured by a 2D projected latent
(ii) a local volume information captured by 3D convolutions with wavelet
decomposition (iii) a frequency information for improving the video
reconstruction. Based on this disentangled representation, our hybrid
autoencoder provide a more comprehensive video latent enriching the generated
videos with fine structures and details. Experiments on video generation
benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed
approach achieves state-of-the-art video generation quality, showing a wide
range of video applications (e.g., long video generation, image-to-video, and
video dynamics control).",2024-02-21 11:46:16+00:00,"['Kihong Kim', 'Haneol Lee', 'Jihye Park', 'Seyeon Kim', 'Kwanghee Lee', 'Seungryong Kim', 'Jaejun Yoo']",http://arxiv.org/abs/2402.13729v4
MovieBench: A Hierarchical Movie Level Dataset for Long Video Generation,"Recent advancements in video generation models, like Stable Video Diffusion,
show promising results, but primarily focus on short, single-scene videos.
These models struggle with generating long videos that involve multiple scenes,
coherent narratives, and consistent characters. Furthermore, there is no
publicly available dataset tailored for the analysis, evaluation, and training
of long video generation models. In this paper, we present MovieBench: A
Hierarchical Movie-Level Dataset for Long Video Generation, which addresses
these challenges by providing unique contributions: (1) movie-length videos
featuring rich, coherent storylines and multi-scene narratives, (2) consistency
of character appearance and audio across scenes, and (3) hierarchical data
structure contains high-level movie information and detailed shot-level
descriptions. Experiments demonstrate that MovieBench brings some new insights
and challenges, such as maintaining character ID consistency across multiple
scenes for various characters. The dataset will be public and continuously
maintained, aiming to advance the field of long video generation. Data can be
found at: https://weijiawu.github.io/MovieBench/.",2024-11-22 10:25:08+00:00,"['Weijia Wu', 'Mingyu Liu', 'Zeyu Zhu', 'Xi Xia', 'Haoen Feng', 'Wen Wang', 'Kevin Qinghong Lin', 'Chunhua Shen', 'Mike Zheng Shou']",http://arxiv.org/abs/2411.15262v1
DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation,"Storytelling video generation (SVG) aims to produce coherent and visually
rich multi-scene videos that follow a structured narrative. Existing methods
primarily employ LLM for high-level planning to decompose a story into
scene-level descriptions, which are then independently generated and stitched
together. However, these approaches struggle with generating high-quality
videos aligned with the complex single-scene description, as visualizing such
complex description involves coherent composition of multiple characters and
events, complex motion synthesis and muti-character customization. To address
these challenges, we propose DreamRunner, a novel story-to-video generation
method: First, we structure the input script using a large language model (LLM)
to facilitate both coarse-grained scene planning as well as fine-grained
object-level layout and motion planning. Next, DreamRunner presents
retrieval-augmented test-time adaptation to capture target motion priors for
objects in each scene, supporting diverse motion customization based on
retrieved videos, thus facilitating the generation of new videos with complex,
scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D
attention and prior injection module SR3AI for fine-grained object-motion
binding and frame-by-frame semantic control. We compare DreamRunner with
various SVG baselines, demonstrating state-of-the-art performance in character
consistency, text alignment, and smooth transitions. Additionally, DreamRunner
exhibits strong fine-grained condition-following ability in compositional
text-to-video generation, significantly outperforming baselines on
T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate
multi-object interactions with qualitative examples.",2024-11-25 18:41:56+00:00,"['Zun Wang', 'Jialu Li', 'Han Lin', 'Jaehong Yoon', 'Mohit Bansal']",http://arxiv.org/abs/2411.16657v3
DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation,"Sora-like video generation models have achieved remarkable progress with a
Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current
video generation models predominantly focus on single-prompt, struggling to
generate coherent scenes with multiple sequential prompts that better reflect
real-world dynamic scenarios. While some pioneering works have explored
multi-prompt video generation, they face significant challenges including
strict training data requirements, weak prompt following, and unnatural
transitions. To address these problems, we propose DiTCtrl, a training-free
multi-prompt video generation method under MM-DiT architectures for the first
time. Our key idea is to take the multi-prompt video generation task as
temporal video editing with smooth transitions. To achieve this goal, we first
analyze MM-DiT's attention mechanism, finding that the 3D full attention
behaves similarly to that of the cross/self-attention blocks in the UNet-like
diffusion models, enabling mask-guided precise semantic control across
different prompts with attention sharing for multi-prompt video generation.
Based on our careful design, the video generated by DiTCtrl achieves smooth
transitions and consistent object motion given multiple sequential prompts
without additional training. Besides, we also present MPVBench, a new benchmark
specially designed for multi-prompt video generation to evaluate the
performance of multi-prompt generation. Extensive experiments demonstrate that
our method achieves state-of-the-art performance without additional training.",2024-12-24 18:51:19+00:00,"['Minghong Cai', 'Xiaodong Cun', 'Xiaoyu Li', 'Wenze Liu', 'Zhaoyang Zhang', 'Yong Zhang', 'Ying Shan', 'Xiangyu Yue']",http://arxiv.org/abs/2412.18597v2
SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion,"We present Stable Video 3D (SV3D) -- a latent video diffusion model for
high-resolution, image-to-multi-view generation of orbital videos around a 3D
object. Recent work on 3D generation propose techniques to adapt 2D generative
models for novel view synthesis (NVS) and 3D optimization. However, these
methods have several disadvantages due to either limited views or inconsistent
NVS, thereby affecting the performance of 3D object generation. In this work,
we propose SV3D that adapts image-to-video diffusion model for novel multi-view
synthesis and 3D generation, thereby leveraging the generalization and
multi-view consistency of the video models, while further adding explicit
camera control for NVS. We also propose improved 3D optimization techniques to
use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental
results on multiple datasets with 2D and 3D metrics as well as user study
demonstrate SV3D's state-of-the-art performance on NVS as well as 3D
reconstruction compared to prior works.",2024-03-18 17:46:06+00:00,"['Vikram Voleti', 'Chun-Han Yao', 'Mark Boss', 'Adam Letts', 'David Pankratz', 'Dmitry Tochilkin', 'Christian Laforte', 'Robin Rombach', 'Varun Jampani']",http://arxiv.org/abs/2403.12008v1
Anything in Any Scene: Photorealistic Video Object Insertion,"Realistic video simulation has shown significant potential across diverse
applications, from virtual reality to film production. This is particularly
true for scenarios where capturing videos in real-world settings is either
impractical or expensive. Existing approaches in video simulation often fail to
accurately model the lighting environment, represent the object geometry, or
achieve high levels of photorealism. In this paper, we propose Anything in Any
Scene, a novel and generic framework for realistic video simulation that
seamlessly inserts any object into an existing dynamic video with a strong
emphasis on physical realism. Our proposed general framework encompasses three
key processes: 1) integrating a realistic object into a given scene video with
proper placement to ensure geometric realism; 2) estimating the sky and
environmental lighting distribution and simulating realistic shadows to enhance
the light realism; 3) employing a style transfer network that refines the final
video output to maximize photorealism. We experimentally demonstrate that
Anything in Any Scene framework produces simulated videos of great geometric
realism, lighting realism, and photorealism. By significantly mitigating the
challenges associated with video data generation, our framework offers an
efficient and cost-effective solution for acquiring high-quality videos.
Furthermore, its applications extend well beyond video data augmentation,
showing promising potential in virtual reality, video editing, and various
other video-centric applications. Please check our project website
https://anythinginanyscene.github.io for access to our project code and more
high-resolution video results.",2024-01-30 23:54:43+00:00,"['Chen Bai', 'Zeman Shao', 'Guoxiang Zhang', 'Di Liang', 'Jie Yang', 'Zhuorui Zhang', 'Yujian Guo', 'Chengzhang Zhong', 'Yiqiao Qiu', 'Zhendong Wang', 'Yichen Guan', 'Xiaoyin Zheng', 'Tao Wang', 'Cheng Lu']",http://arxiv.org/abs/2401.17509v1
MotionCraft: Physics-based Zero-Shot Video Generation,"Generating videos with realistic and physically plausible motion is one of
the main recent challenges in computer vision. While diffusion models are
achieving compelling results in image generation, video diffusion models are
limited by heavy training and huge models, resulting in videos that are still
biased to the training dataset. In this work we propose MotionCraft, a new
zero-shot video generator to craft physics-based and realistic videos.
MotionCraft is able to warp the noise latent space of an image diffusion model,
such as Stable Diffusion, by applying an optical flow derived from a physics
simulation. We show that warping the noise latent space results in coherent
application of the desired motion while allowing the model to generate missing
elements consistent with the scene evolution, which would otherwise result in
artefacts or missing content if the flow was applied in the pixel space. We
compare our method with the state-of-the-art Text2Video-Zero reporting
qualitative and quantitative improvements, demonstrating the effectiveness of
our approach to generate videos with finely-prescribed complex motion dynamics.
Project page: https://mezzelfo.github.io/MotionCraft/",2024-05-22 11:44:57+00:00,"['Luca Savant Aira', 'Antonio Montanaro', 'Emanuele Aiello', 'Diego Valsesia', 'Enrico Magli']",http://arxiv.org/abs/2405.13557v2
COMUNI: Decomposing Common and Unique Video Signals for Diffusion-based Video Generation,"Since videos record objects moving coherently, adjacent video frames have
commonness (similar object appearances) and uniqueness (slightly changed
postures). To prevent redundant modeling of common video signals, we propose a
novel diffusion-based framework, named COMUNI, which decomposes the COMmon and
UNIque video signals to enable efficient video generation. Our approach
separates the decomposition of video signals from the task of video generation,
thus reducing the computation complexity of generative models. In particular,
we introduce CU-VAE to decompose video signals and encode them into latent
features. To train CU-VAE in a self-supervised manner, we employ a cascading
merge module to reconstitute video signals and a time-agnostic video decoder to
reconstruct video frames. Then we propose CU-LDM to model latent features for
video generation, which adopts two specific diffusion streams to simultaneously
model the common and unique latent features. We further utilize additional
joint modules for cross modeling of the common and unique latent features, and
a novel position embedding method to ensure the content consistency and motion
coherence of generated videos. The position embedding method incorporates
spatial and temporal absolute position information into the joint modules.
Extensive experiments demonstrate the necessity of decomposing common and
unique video signals for video generation and the effectiveness and efficiency
of our proposed method.",2024-10-02 16:30:08+00:00,"['Mingzhen Sun', 'Weining Wang', 'Xinxin Zhu', 'Jing Liu']",http://arxiv.org/abs/2410.01718v1
Align3R: Aligned Monocular Depth Estimation for Dynamic Videos,"Recent developments in monocular depth estimation methods enable high-quality
depth estimation of single-view images but fail to estimate consistent video
depth across different frames. Recent works address this problem by applying a
video diffusion model to generate video depth conditioned on the input video,
which is training-expensive and can only produce scale-invariant depth values
without camera poses. In this paper, we propose a novel video-depth estimation
method called Align3R to estimate temporal consistent depth maps for a dynamic
video. Our key idea is to utilize the recent DUSt3R model to align estimated
monocular depth maps of different timesteps. First, we fine-tune the DUSt3R
model with additional estimated monocular depth as inputs for the dynamic
scenes. Then, we apply optimization to reconstruct both depth maps and camera
poses. Extensive experiments demonstrate that Align3R estimates consistent
video depth and camera poses for a monocular video with superior performance
than baseline methods.",2024-12-04 07:09:59+00:00,"['Jiahao Lu', 'Tianyu Huang', 'Peng Li', 'Zhiyang Dou', 'Cheng Lin', 'Zhiming Cui', 'Zhen Dong', 'Sai-Kit Yeung', 'Wenping Wang', 'Yuan Liu']",http://arxiv.org/abs/2412.03079v2
MSG score: A Comprehensive Evaluation for Multi-Scene Video Generation,"This paper addresses the metrics required for generating multi-scene videos
based on a continuous scenario, as opposed to traditional short video
generation. Scenario-based videos require a comprehensive evaluation that
considers multiple factors such as character consistency, artistic coherence,
aesthetic quality, and the alignment of the generated content with the intended
prompt. Additionally, in video generation, unlike single images, the movement
of characters across frames introduces potential issues like distortion or
unintended changes, which must be effectively evaluated and corrected. In the
context of probabilistic models like diffusion, generating the desired scene
requires repeated sampling and manual selection, akin to how a film director
chooses the best shots from numerous takes. We propose a score-based evaluation
benchmark that automates this process, enabling a more objective and efficient
assessment of these complexities. This approach allows for the generation of
high-quality multi-scene videos by selecting the best outcomes based on
automated scoring rather than manual inspection.",2024-11-28 13:11:50+00:00,"['Daewon Yoon', 'Hyungsuk Lee', 'Wonsik Shin']",http://arxiv.org/abs/2411.19121v1
Continuous Video Process: Modeling Videos as Continuous Multi-Dimensional Processes for Video Prediction,"Diffusion models have made significant strides in image generation, mastering
tasks such as unconditional image synthesis, text-image translation, and
image-to-image conversions. However, their capability falls short in the realm
of video prediction, mainly because they treat videos as a collection of
independent images, relying on external constraints such as temporal attention
mechanisms to enforce temporal coherence. In our paper, we introduce a novel
model class, that treats video as a continuous multi-dimensional process rather
than a series of discrete frames. We also report a reduction of 75\% sampling
steps required to sample a new frame thus making our framework more efficient
during the inference time. Through extensive experimentation, we establish
state-of-the-art performance in video prediction, validated on benchmark
datasets including KTH, BAIR, Human3.6M, and UCF101. Navigate to the project
page https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html for video results.",2024-12-06 10:34:50+00:00,"['Gaurav Shrivastava', 'Abhinav Shrivastava']",http://arxiv.org/abs/2412.04929v2
StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models,"This paper aims to tackle the problem of photorealistic view synthesis from
vehicle sensor data. Recent advancements in neural scene representation have
achieved notable success in rendering high-quality autonomous driving scenes,
but the performance significantly degrades as the viewpoint deviates from the
training trajectory. To mitigate this problem, we introduce StreetCrafter, a
novel controllable video diffusion model that utilizes LiDAR point cloud
renderings as pixel-level conditions, which fully exploits the generative prior
for novel view synthesis, while preserving precise camera control. Moreover,
the utilization of pixel-level LiDAR conditions allows us to make accurate
pixel-level edits to target scenes. In addition, the generative prior of
StreetCrafter can be effectively incorporated into dynamic scene
representations to achieve real-time rendering. Experiments on Waymo Open
Dataset and PandaSet demonstrate that our model enables flexible control over
viewpoint changes, enlarging the view synthesis regions for satisfying
rendering, which outperforms existing methods.",2024-12-17 18:58:55+00:00,"['Yunzhi Yan', 'Zhen Xu', 'Haotong Lin', 'Haian Jin', 'Haoyu Guo', 'Yida Wang', 'Kun Zhan', 'Xianpeng Lang', 'Hujun Bao', 'Xiaowei Zhou', 'Sida Peng']",http://arxiv.org/abs/2412.13188v1
SpatialMe: Stereo Video Conversion Using Depth-Warping and Blend-Inpainting,"Stereo video conversion aims to transform monocular videos into immersive
stereo format. Despite the advancements in novel view synthesis, it still
remains two major challenges: i) difficulty of achieving high-fidelity and
stable results, and ii) insufficiency of high-quality stereo video data. In
this paper, we introduce SpatialMe, a novel stereo video conversion framework
based on depth-warping and blend-inpainting. Specifically, we propose a
mask-based hierarchy feature update (MHFU) refiner, which integrate and refine
the outputs from designed multi-branch inpainting module, using feature update
unit (FUU) and mask mechanism. We also propose a disparity expansion strategy
to address the problem of foreground bleeding. Furthermore, we conduct a
high-quality real-world stereo video dataset -- StereoV1K, to alleviate the
data shortage. It contains 1000 stereo videos captured in real-world at a
resolution of 1180 x 1180, covering various indoor and outdoor scenes.
Extensive experiments demonstrate the superiority of our approach in generating
stereo videos over state-of-the-art methods.",2024-12-16 07:42:49+00:00,"['Jiale Zhang', 'Qianxi Jia', 'Yang Liu', 'Wei Zhang', 'Wei Wei', 'Xin Tian']",http://arxiv.org/abs/2412.11512v1
HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation,"Multimodal LLMs have advanced vision-language tasks but still struggle with
understanding video scenes. To bridge this gap, Video Scene Graph Generation
(VidSGG) has emerged to capture multi-object relationships across video frames.
However, prior methods rely on pairwise connections, limiting their ability to
handle complex multi-object interactions and reasoning. To this end, we propose
Multimodal LLMs on a Scene HyperGraph (HyperGLM), promoting reasoning about
multi-way interactions and higher-order relationships. Our approach uniquely
integrates entity scene graphs, which capture spatial relationships between
objects, with a procedural graph that models their causal transitions, forming
a unified HyperGraph. Significantly, HyperGLM enables reasoning by injecting
this unified HyperGraph into LLMs. Additionally, we introduce a new Video Scene
Graph Reasoning (VSGR) dataset featuring 1.9M frames from third-person,
egocentric, and drone views and supports five tasks: Scene Graph Generation,
Scene Graph Anticipation, Video Question Answering, Video Captioning, and
Relation Reasoning. Empirically, HyperGLM consistently outperforms
state-of-the-art methods across five tasks, effectively modeling and reasoning
complex relationships in diverse video scenes.",2024-11-27 04:24:39+00:00,"['Trong-Thuan Nguyen', 'Pha Nguyen', 'Jackson Cothren', 'Alper Yilmaz', 'Khoa Luu']",http://arxiv.org/abs/2411.18042v1
VISAGE: Video Synthesis using Action Graphs for Surgery,"Surgical data science (SDS) is a field that analyzes patient data before,
during, and after surgery to improve surgical outcomes and skills. However,
surgical data is scarce, heterogeneous, and complex, which limits the
applicability of existing machine learning methods. In this work, we introduce
the novel task of future video generation in laparoscopic surgery. This task
can augment and enrich the existing surgical data and enable various
applications, such as simulation, analysis, and robot-aided surgery.
Ultimately, it involves not only understanding the current state of the
operation but also accurately predicting the dynamic and often unpredictable
nature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis
using Action Graphs for Surgery), leverages the power of action scene graphs to
capture the sequential nature of laparoscopic procedures and utilizes diffusion
models to synthesize temporally coherent video sequences. VISAGE predicts the
future frames given only a single initial frame, and the action graph triplets.
By incorporating domain-specific knowledge through the action graph, VISAGE
ensures the generated videos adhere to the expected visual and motion patterns
observed in real laparoscopic procedures. The results of our experiments
demonstrate high-fidelity video generation for laparoscopy procedures, which
enables various applications in SDS.",2024-10-23 10:28:17+00:00,"['Yousef Yeganeh', 'Rachmadio Lazuardi', 'Amir Shamseddin', 'Emine Dari', 'Yash Thirani', 'Nassir Navab', 'Azade Farshad']",http://arxiv.org/abs/2410.17751v2
CFSynthesis: Controllable and Free-view 3D Human Video Synthesis,"Human video synthesis aims to create lifelike characters in various
environments, with wide applications in VR, storytelling, and content creation.
While 2D diffusion-based methods have made significant progress, they struggle
to generalize to complex 3D poses and varying scene backgrounds. To address
these limitations, we introduce CFSynthesis, a novel framework for generating
high-quality human videos with customizable attributes, including identity,
motion, and scene configurations. Our method leverages a texture-SMPL-based
representation to ensure consistent and stable character appearances across
free viewpoints. Additionally, we introduce a novel foreground-background
separation strategy that effectively decomposes the scene as foreground and
background, enabling seamless integration of user-defined backgrounds.
Experimental results on multiple datasets show that CFSynthesis not only
achieves state-of-the-art performance in complex human animations but also
adapts effectively to 3D motions in free-view and user-specified scenarios.",2024-12-15 05:57:36+00:00,"['Liyuan Cui', 'Xiaogang Xu', 'Wenqi Dong', 'Zesong Yang', 'Hujun Bao', 'Zhaopeng Cui']",http://arxiv.org/abs/2412.11067v3
Compressing Scene Dynamics: A Generative Approach,"This paper proposes to learn generative priors from the motion patterns
instead of video contents for generative video compression. The priors are
derived from small motion dynamics in common scenes such as swinging trees in
the wind and floating boat on the sea. Utilizing such compact motion priors, a
novel generative scene dynamics compression framework is built to realize
ultra-low bit-rate communication and high-quality reconstruction for diverse
scene contents. At the encoder side, motion priors are characterized into
compact representations in a dense-to-sparse manner. At the decoder side, the
decoded motion priors serve as the trajectory hints for scene dynamics
reconstruction via a diffusion-based flow-driven generator. The experimental
results illustrate that the proposed method can achieve superior
rate-distortion performance and outperform the state-of-the-art conventional
video codec Versatile Video Coding (VVC) on scene dynamics sequences. The
project page can be found at https://github.com/xyzysz/GNVDC.",2024-10-13 07:54:02+00:00,"['Shanzhi Yin', 'Zihan Zhang', 'Bolin Chen', 'Shiqi Wang', 'Yan Ye']",http://arxiv.org/abs/2410.09768v1
DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View Driving Scenes,"Recent advances in diffusion models have improved controllable streetscape
generation and supported downstream perception and planning tasks. However,
challenges remain in accurately modeling driving scenes and generating long
videos. To alleviate these issues, we propose DreamForge, an advanced
diffusion-based autoregressive video generation model tailored for
3D-controllable long-term generation. To enhance the lane and foreground
generation, we introduce perspective guidance and integrate object-wise
position encoding to incorporate local 3D correlation and improve foreground
object modeling. We also propose motion-aware temporal attention to capture
motion cues and appearance changes in videos. By leveraging motion frames and
an autoregressive generation paradigm,we can autoregressively generate long
videos (over 200 frames) using a model trained in short sequences, achieving
superior quality compared to the baseline in 16-frame video evaluations.
Finally, we integrate our method with the realistic simulator DriveArena to
provide more reliable open-loop and closed-loop evaluations for vision-based
driving agents. Project Page:
https://pjlab-adg.github.io/DriveArena/dreamforge.",2024-09-06 03:09:58+00:00,"['Jianbiao Mei', 'Tao Hu', 'Xuemeng Yang', 'Licheng Wen', 'Yu Yang', 'Tiantian Wei', 'Yukai Ma', 'Min Dou', 'Botian Shi', 'Yong Liu']",http://arxiv.org/abs/2409.04003v3
ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis,"Despite recent advancements in neural 3D reconstruction, the dependence on
dense multi-view captures restricts their broader applicability. In this work,
we propose \textbf{ViewCrafter}, a novel method for synthesizing high-fidelity
novel views of generic scenes from single or sparse images with the prior of
video diffusion model. Our method takes advantage of the powerful generation
capabilities of video diffusion model and the coarse 3D clues offered by
point-based representation to generate high-quality video frames with precise
camera pose control. To further enlarge the generation range of novel views, we
tailored an iterative view synthesis strategy together with a camera trajectory
planning algorithm to progressively extend the 3D clues and the areas covered
by the novel views. With ViewCrafter, we can facilitate various applications,
such as immersive experiences with real-time rendering by efficiently
optimizing a 3D-GS representation using the reconstructed 3D points and the
generated novel views, and scene-level text-to-3D generation for more
imaginative content creation. Extensive experiments on diverse datasets
demonstrate the strong generalization capability and superior performance of
our method in synthesizing high-fidelity and consistent novel views.",2024-09-03 16:53:19+00:00,"['Wangbo Yu', 'Jinbo Xing', 'Li Yuan', 'Wenbo Hu', 'Xiaoyu Li', 'Zhipeng Huang', 'Xiangjun Gao', 'Tien-Tsin Wong', 'Ying Shan', 'Yonghong Tian']",http://arxiv.org/abs/2409.02048v1
Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation,"Image diffusion distillation achieves high-fidelity generation with very few
sampling steps. However, applying these techniques directly to video diffusion
often results in unsatisfactory frame quality due to the limited visual quality
in public video datasets. This affects the performance of both teacher and
student video diffusion models. Our study aims to improve video diffusion
distillation while improving frame appearance using abundant high-quality image
data. We propose motion consistency model (MCM), a single-stage video diffusion
distillation method that disentangles motion and appearance learning.
Specifically, MCM includes a video consistency model that distills motion from
the video teacher model, and an image discriminator that enhances frame
appearance to match high-quality image data. This combination presents two
challenges: (1) conflicting frame learning objectives, as video distillation
learns from low-quality video frames while the image discriminator targets
high-quality images; and (2) training-inference discrepancies due to the
differing quality of video samples used during training and inference. To
address these challenges, we introduce disentangled motion distillation and
mixed trajectory distillation. The former applies the distillation objective
solely to the motion representation, while the latter mitigates
training-inference discrepancies by mixing distillation trajectories from both
the low- and high-quality video domains. Extensive experiments show that our
MCM achieves the state-of-the-art video diffusion distillation performance.
Additionally, our method can enhance frame quality in video diffusion models,
producing frames with high aesthetic scores or specific styles without
corresponding video data.",2024-06-11 02:09:46+00:00,"['Yuanhao Zhai', 'Kevin Lin', 'Zhengyuan Yang', 'Linjie Li', 'Jianfeng Wang', 'Chung-Ching Lin', 'David Doermann', 'Junsong Yuan', 'Lijuan Wang']",http://arxiv.org/abs/2406.06890v2
Fashion-VDM: Video Diffusion Model for Virtual Try-On,"We present Fashion-VDM, a video diffusion model (VDM) for generating virtual
try-on videos. Given an input garment image and person video, our method aims
to generate a high-quality try-on video of the person wearing the given
garment, while preserving the person's identity and motion. Image-based virtual
try-on has shown impressive results; however, existing video virtual try-on
(VVT) methods are still lacking garment details and temporal consistency. To
address these issues, we propose a diffusion-based architecture for video
virtual try-on, split classifier-free guidance for increased control over the
conditioning inputs, and a progressive temporal training strategy for
single-pass 64-frame, 512px video generation. We also demonstrate the
effectiveness of joint image-video training for video try-on, especially when
video data is limited. Our qualitative and quantitative experiments show that
our approach sets the new state-of-the-art for video virtual try-on. For
additional results, visit our project page:
https://johannakarras.github.io/Fashion-VDM.",2024-10-31 21:52:33+00:00,"['Johanna Karras', 'Yingwei Li', 'Nan Liu', 'Luyang Zhu', 'Innfarn Yoo', 'Andreas Lugmayr', 'Chris Lee', 'Ira Kemelmacher-Shlizerman']",http://arxiv.org/abs/2411.00225v2
TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation,"We present TANGO, a framework for generating co-speech body-gesture videos.
Given a few-minute, single-speaker reference video and target speech audio,
TANGO produces high-fidelity videos with synchronized body gestures. TANGO
builds on Gesture Video Reenactment (GVR), which splits and retrieves video
clips using a directed graph structure - representing video frames as nodes and
valid transitions as edges. We address two key limitations of GVR: audio-motion
misalignment and visual artifacts in GAN-generated transition frames. In
particular, (i) we propose retrieving gestures using latent feature distance to
improve cross-modal alignment. To ensure the latent features could effectively
model the relationship between speech audio and gesture motion, we implement a
hierarchical joint embedding space (AuMoCLIP); (ii) we introduce the
diffusion-based model to generate high-quality transition frames. Our diffusion
model, Appearance Consistent Interpolation (ACInterp), is built upon
AnimateAnyone and includes a reference motion module and homography background
flow to preserve appearance consistency between generated and reference videos.
By integrating these components into the graph-based retrieval framework, TANGO
reliably produces realistic, audio-synchronized videos and outperforms all
existing generative and retrieval methods. Our codes and pretrained models are
available: \url{https://pantomatrix.github.io/TANGO/}",2024-10-05 16:30:46+00:00,"['Haiyang Liu', 'Xingchao Yang', 'Tomoya Akiyama', 'Yuantian Huang', 'Qiaoge Li', 'Shigeru Kuriyama', 'Takafumi Taketomi']",http://arxiv.org/abs/2410.04221v1
DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos,"View-predictive generative models provide strong priors for lifting
object-centric images and videos into 3D and 4D through rendering and score
distillation objectives. A question then remains: what about lifting complete
multi-object dynamic scenes? There are two challenges in this direction: First,
rendering error gradients are often insufficient to recover fast object motion,
and second, view predictive generative models work much better for objects than
whole scenes, so, score distillation objectives cannot currently be applied at
the scene level directly. We present DreamScene4D, the first approach to
generate 3D dynamic scenes of multiple objects from monocular videos via
360-degree novel view synthesis. Our key insight is a ""decompose-recompose""
approach that factorizes the video scene into the background and object tracks,
while also factorizing object motion into 3 components: object-centric
deformation, object-to-world-frame transformation, and camera motion. Such
decomposition permits rendering error gradients and object view-predictive
models to recover object 3D completions and deformations while bounding box
tracks guide the large object movements in the scene. We show extensive results
on challenging DAVIS, Kubric, and self-captured videos with quantitative
comparisons and a user preference study. Besides 4D scene generation,
DreamScene4D obtains accurate 2D persistent point track by projecting the
inferred 3D trajectories to 2D. We will release our code and hope our work will
stimulate more research on fine-grained 4D understanding from videos.",2024-05-03 17:55:34+00:00,"['Wen-Hsuan Chu', 'Lei Ke', 'Katerina Fragkiadaki']",http://arxiv.org/abs/2405.02280v2
From Slow Bidirectional to Fast Autoregressive Video Diffusion Models,"Current video diffusion models achieve impressive generation quality but
struggle in interactive applications due to bidirectional attention
dependencies. The generation of a single frame requires the model to process
the entire sequence, including the future. We address this limitation by
adapting a pretrained bidirectional diffusion transformer to an autoregressive
transformer that generates frames on-the-fly. To further reduce latency, we
extend distribution matching distillation (DMD) to videos, distilling 50-step
diffusion model into a 4-step generator. To enable stable and high-quality
distillation, we introduce a student initialization scheme based on teacher's
ODE trajectories, as well as an asymmetric distillation strategy that
supervises a causal student model with a bidirectional teacher. This approach
effectively mitigates error accumulation in autoregressive generation, allowing
long-duration video synthesis despite training on short clips. Our model
achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all
previous video generation models. It enables fast streaming generation of
high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our
approach also enables streaming video-to-video translation, image-to-video, and
dynamic prompting in a zero-shot manner. We will release the code based on an
open-source model in the future.",2024-12-10 18:59:50+00:00,"['Tianwei Yin', 'Qiang Zhang', 'Richard Zhang', 'William T. Freeman', 'Fredo Durand', 'Eli Shechtman', 'Xun Huang']",http://arxiv.org/abs/2412.07772v2
MotionCom: Automatic and Motion-Aware Image Composition with LLM and Video Diffusion Prior,"This work presents MotionCom, a training-free motion-aware diffusion based
image composition, enabling automatic and seamless integration of target
objects into new scenes with dynamically coherent results without finetuning or
optimization. Traditional approaches in this area suffer from two significant
limitations: they require manual planning for object placement and often
generate static compositions lacking motion realism. MotionCom addresses these
issues by utilizing a Large Vision Language Model (LVLM) for intelligent
planning, and a Video Diffusion prior for motion-infused image synthesis,
streamlining the composition process. Our multi-modal Chain-of-Thought (CoT)
prompting with LVLM automates the strategic placement planning of foreground
objects, considering their potential motion and interaction within the scenes.
Complementing this, we propose a novel method MotionPaint to distill
motion-aware information from pretrained video diffusion models in the
generation phase, ensuring that these objects are not only seamlessly
integrated but also endowed with realistic motion. Extensive quantitative and
qualitative results highlight MotionCom's superiority, showcasing its
efficiency in streamlining the planning process and its capability to produce
compositions that authentically depict motion and interaction.",2024-09-16 08:44:17+00:00,"['Weijing Tao', 'Xiaofeng Yang', 'Miaomiao Cui', 'Guosheng Lin']",http://arxiv.org/abs/2409.10090v1
Imagine360: Immersive 360 Video Generation from Perspective Anchor,"$360^\circ$ videos offer a hyper-immersive experience that allows the viewers
to explore a dynamic scene from full 360 degrees. To achieve more user-friendly
and personalized content creation in $360^\circ$ video format, we seek to lift
standard perspective videos into $360^\circ$ equirectangular videos. To this
end, we introduce Imagine360, the first perspective-to-$360^\circ$ video
generation framework that creates high-quality $360^\circ$ videos with rich and
diverse motion patterns from video anchors. Imagine360 learns fine-grained
spherical visual and motion patterns from limited $360^\circ$ video data with
several key designs. 1) Firstly we adopt the dual-branch design, including a
perspective and a panorama video denoising branch to provide local and global
constraints for $360^\circ$ video generation, with motion module and spatial
LoRA layers fine-tuned on extended web $360^\circ$ videos. 2) Additionally, an
antipodal mask is devised to capture long-range motion dependencies, enhancing
the reversed camera motion between antipodal pixels across hemispheres. 3) To
handle diverse perspective video inputs, we propose elevation-aware designs
that adapt to varying video masking due to changing elevations across frames.
Extensive experiments show Imagine360 achieves superior graphics quality and
motion coherence among state-of-the-art $360^\circ$ video generation methods.
We believe Imagine360 holds promise for advancing personalized, immersive
$360^\circ$ video creation.",2024-12-04 18:50:08+00:00,"['Jing Tan', 'Shuai Yang', 'Tong Wu', 'Jingwen He', 'Yuwei Guo', 'Ziwei Liu', 'Dahua Lin']",http://arxiv.org/abs/2412.03552v1
GameGen-X: Interactive Open-world Game Video Generation,"We introduce GameGen-X, the first diffusion transformer model specifically
designed for both generating and interactively controlling open-world game
videos. This model facilitates high-quality, open-domain generation by
simulating an extensive array of game engine features, such as innovative
characters, dynamic environments, complex actions, and diverse events.
Additionally, it provides interactive controllability, predicting and altering
future content based on the current clip, thus allowing for gameplay
simulation. To realize this vision, we first collected and built an Open-World
Video Game Dataset from scratch. It is the first and largest dataset for
open-world game video generation and control, which comprises over a million
diverse gameplay video clips sampling from over 150 games with informative
captions from GPT-4o. GameGen-X undergoes a two-stage training process,
consisting of foundation model pre-training and instruction tuning. Firstly,
the model was pre-trained via text-to-video generation and video continuation,
endowing it with the capability for long-sequence, high-quality open-domain
game video generation. Further, to achieve interactive controllability, we
designed InstructNet to incorporate game-related multi-modal control signal
experts. This allows the model to adjust latent representations based on user
inputs, unifying character interaction and scene content control for the first
time in video generation. During instruction tuning, only the InstructNet is
updated while the pre-trained foundation model is frozen, enabling the
integration of interactive controllability without loss of diversity and
quality of generated video content.",2024-11-01 17:59:17+00:00,"['Haoxuan Che', 'Xuanhua He', 'Quande Liu', 'Cheng Jin', 'Hao Chen']",http://arxiv.org/abs/2411.00769v3
VEnhancer: Generative Space-Time Enhancement for Video Generation,"We present VEnhancer, a generative space-time enhancement framework that
improves the existing text-to-video results by adding more details in spatial
domain and synthetic detailed motion in temporal domain. Given a generated
low-quality video, our approach can increase its spatial and temporal
resolution simultaneously with arbitrary up-sampling space and time scales
through a unified video diffusion model. Furthermore, VEnhancer effectively
removes generated spatial artifacts and temporal flickering of generated
videos. To achieve this, basing on a pretrained video diffusion model, we train
a video ControlNet and inject it to the diffusion model as a condition on low
frame-rate and low-resolution videos. To effectively train this video
ControlNet, we design space-time data augmentation as well as video-aware
conditioning. Benefiting from the above designs, VEnhancer yields to be stable
during training and shares an elegant end-to-end training manner. Extensive
experiments show that VEnhancer surpasses existing state-of-the-art video
super-resolution and space-time super-resolution methods in enhancing
AI-generated videos. Moreover, with VEnhancer, exisiting open-source
state-of-the-art text-to-video method, VideoCrafter-2, reaches the top one in
video generation benchmark -- VBench.",2024-07-10 13:46:08+00:00,"['Jingwen He', 'Tianfan Xue', 'Dongyang Liu', 'Xinqi Lin', 'Peng Gao', 'Dahua Lin', 'Yu Qiao', 'Wanli Ouyang', 'Ziwei Liu']",http://arxiv.org/abs/2407.07667v1
Towards Chunk-Wise Generation for Long Videos,"Generating long-duration videos has always been a significant challenge due
to the inherent complexity of spatio-temporal domain and the substantial GPU
memory demands required to calculate huge size tensors. While diffusion based
generative models achieve state-of-the-art performance in video generation
task, they are typically trained with predefined video resolutions and lengths.
During inference, a noise tensor with specific resolution and length should be
specified at first, and the model will perform denoising on the entire video
tensor simultaneously, all the frames together. Such approach will easily raise
an out-of-memory (OOM) problem when the specified resolution and/or length
exceed a certain limit. One of the solutions to this problem is to generate
many short video chunks autoregressively with strong inter-chunk
spatio-temporal relation and then concatenate them together to form a long
video. In this approach, a long video generation task is divided into multiple
short video generation subtasks, and the cost of each subtask is reduced to a
feasible level. In this paper, we conduct a detailed survey on long video
generation with the autoregressive chunk-by-chunk strategy. We address common
problems caused by applying short image-to-video models to long video tasks and
design an efficient $k$-step search solution to mitigate these problems.",2024-11-27 16:13:26+00:00,"['Siyang Zhang', 'Ser-Nam Lim']",http://arxiv.org/abs/2411.18668v1
Representing Long Volumetric Video with Temporal Gaussian Hierarchy,"This paper aims to address the challenge of reconstructing long volumetric
videos from multi-view RGB videos. Recent dynamic view synthesis methods
leverage powerful 4D representations, like feature grids or point cloud
sequences, to achieve high-quality rendering results. However, they are
typically limited to short (1~2s) video clips and often suffer from large
memory footprints when dealing with longer videos. To solve this issue, we
propose a novel 4D representation, named Temporal Gaussian Hierarchy, to
compactly model long volumetric videos. Our key observation is that there are
generally various degrees of temporal redundancy in dynamic scenes, which
consist of areas changing at different speeds. Motivated by this, our approach
builds a multi-level hierarchy of 4D Gaussian primitives, where each level
separately describes scene regions with different degrees of content change,
and adaptively shares Gaussian primitives to represent unchanged scene content
over different temporal segments, thus effectively reducing the number of
Gaussian primitives. In addition, the tree-like structure of the Gaussian
hierarchy allows us to efficiently represent the scene at a particular moment
with a subset of Gaussian primitives, leading to nearly constant GPU memory
usage during the training or rendering regardless of the video length.
Extensive experimental results demonstrate the superiority of our method over
alternative methods in terms of training cost, rendering speed, and storage
usage. To our knowledge, this work is the first approach capable of efficiently
handling minutes of volumetric video data while maintaining state-of-the-art
rendering quality. Our project page is available at:
https://zju3dv.github.io/longvolcap.",2024-12-12 18:59:34+00:00,"['Zhen Xu', 'Yinghao Xu', 'Zhiyuan Yu', 'Sida Peng', 'Jiaming Sun', 'Hujun Bao', 'Xiaowei Zhou']",http://arxiv.org/abs/2412.09608v1
Tri-Ergon: Fine-grained Video-to-Audio Generation with Multi-modal Conditions and LUFS Control,"Video-to-audio (V2A) generation utilizes visual-only video features to
produce realistic sounds that correspond to the scene. However, current V2A
models often lack fine-grained control over the generated audio, especially in
terms of loudness variation and the incorporation of multi-modal conditions. To
overcome these limitations, we introduce Tri-Ergon, a diffusion-based V2A model
that incorporates textual, auditory, and pixel-level visual prompts to enable
detailed and semantically rich audio synthesis. Additionally, we introduce
Loudness Units relative to Full Scale (LUFS) embedding, which allows for
precise manual control of the loudness changes over time for individual audio
channels, enabling our model to effectively address the intricate correlation
of video and audio in real-world Foley workflows. Tri-Ergon is capable of
creating 44.1 kHz high-fidelity stereo audio clips of varying lengths up to 60
seconds, which significantly outperforms existing state-of-the-art V2A methods
that typically generate mono audio for a fixed duration.",2024-12-29 06:46:24+00:00,"['Bingliang Li', 'Fengyu Yang', 'Yuxin Mao', 'Qingwen Ye', 'Hongkai Chen', 'Yiran Zhong']",http://arxiv.org/abs/2412.20378v1
TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation,"Most of these text-to-video (T2V) generative models often produce
single-scene video clips that depict an entity performing a particular action
(e.g., 'a red panda climbing a tree'). However, it is pertinent to generate
multi-scene videos since they are ubiquitous in the real-world (e.g., 'a red
panda climbing a tree' followed by 'the red panda sleeps on the top of the
tree'). To generate multi-scene videos from the pretrained T2V model, we
introduce a simple and effective Time-Aligned Captions (TALC) framework.
Specifically, we enhance the text-conditioning mechanism in the T2V
architecture to recognize the temporal alignment between the video scenes and
scene descriptions. For instance, we condition the visual features of the
earlier and later scenes of the generated video with the representations of the
first scene description (e.g., 'a red panda climbing a tree') and second scene
description (e.g., 'the red panda sleeps on the top of the tree'),
respectively. As a result, we show that the T2V model can generate multi-scene
videos that adhere to the multi-scene text descriptions and be visually
consistent (e.g., entity and background). Further, we finetune the pretrained
T2V model with multi-scene video-text data using the TALC framework. We show
that the TALC-finetuned model outperforms the baseline by achieving a relative
gain of 29% in the overall score, which averages visual consistency and text
adherence using human evaluation.",2024-05-07 21:52:39+00:00,"['Hritik Bansal', 'Yonatan Bitton', 'Michal Yarom', 'Idan Szpektor', 'Aditya Grover', 'Kai-Wei Chang']",http://arxiv.org/abs/2405.04682v4
Video ReCap: Recursive Captioning of Hour-Long Videos,"Most video captioning models are designed to process short video clips of few
seconds and output text describing low-level visual concepts (e.g., objects,
scenes, atomic actions). However, most real-world videos last for minutes or
hours and have a complex hierarchical structure spanning different temporal
granularities. We propose Video ReCap, a recursive video captioning model that
can process video inputs of dramatically different lengths (from 1 second to 2
hours) and output video captions at multiple hierarchy levels. The recursive
video-language architecture exploits the synergy between different video
hierarchies and can process hour-long videos efficiently. We utilize a
curriculum learning training scheme to learn the hierarchical structure of
videos, starting from clip-level captions describing atomic actions, then
focusing on segment-level descriptions, and concluding with generating
summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by
augmenting Ego4D with 8,267 manually collected long-range video summaries. Our
recursive model can flexibly generate captions at different hierarchy levels
while also being useful for other complex video understanding tasks, such as
VideoQA on EgoSchema. Data, code, and models are available at:
https://sites.google.com/view/vidrecap",2024-02-20 18:58:54+00:00,"['Md Mohaiminul Islam', 'Ngan Ho', 'Xitong Yang', 'Tushar Nagarajan', 'Lorenzo Torresani', 'Gedas Bertasius']",http://arxiv.org/abs/2402.13250v6
DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures,"Audio-driven talking video generation has advanced significantly, but
existing methods often depend on video-to-video translation techniques and
traditional generative networks like GANs and they typically generate taking
heads and co-speech gestures separately, leading to less coherent outputs.
Furthermore, the gestures produced by these methods often appear overly smooth
or subdued, lacking in diversity, and many gesture-centric approaches do not
integrate talking head generation. To address these limitations, we introduce
DiffTED, a new approach for one-shot audio-driven TED-style talking video
generation from a single image. Specifically, we leverage a diffusion model to
generate sequences of keypoints for a Thin-Plate Spline motion model, precisely
controlling the avatar's animation while ensuring temporally coherent and
diverse gestures. This innovative approach utilizes classifier-free guidance,
empowering the gestures to flow naturally with the audio input without relying
on pre-trained classifiers. Experiments demonstrate that DiffTED generates
temporally coherent talking videos with diverse co-speech gestures.",2024-09-11 22:31:55+00:00,"['Steven Hogue', 'Chenxu Zhang', 'Hamza Daruger', 'Yapeng Tian', 'Xiaohu Guo']",http://arxiv.org/abs/2409.07649v1
Latte: Latent Diffusion Transformer for Video Generation,"We propose a novel Latent Diffusion Transformer, namely Latte, for video
generation. Latte first extracts spatio-temporal tokens from input videos and
then adopts a series of Transformer blocks to model video distribution in the
latent space. In order to model a substantial number of tokens extracted from
videos, four efficient variants are introduced from the perspective of
decomposing the spatial and temporal dimensions of input videos. To improve the
quality of generated videos, we determine the best practices of Latte through
rigorous experimental analysis, including video clip patch embedding, model
variants, timestep-class information injection, temporal positional embedding,
and learning strategies. Our comprehensive evaluation demonstrates that Latte
achieves state-of-the-art performance across four standard video generation
datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In
addition, we extend Latte to text-to-video generation (T2V) task, where Latte
achieves comparable results compared to recent T2V models. We strongly believe
that Latte provides valuable insights for future research on incorporating
Transformers into diffusion models for video generation.",2024-01-05 19:55:15+00:00,"['Xin Ma', 'Yaohui Wang', 'Gengyun Jia', 'Xinyuan Chen', 'Ziwei Liu', 'Yuan-Fang Li', 'Cunjian Chen', 'Yu Qiao']",http://arxiv.org/abs/2401.03048v1
ViViD: Video Virtual Try-on using Diffusion Models,"Video virtual try-on aims to transfer a clothing item onto the video of a
target person. Directly applying the technique of image-based try-on to the
video domain in a frame-wise manner will cause temporal-inconsistent outcomes
while previous video-based try-on solutions can only generate low visual
quality and blurring results. In this work, we present ViViD, a novel framework
employing powerful diffusion models to tackle the task of video virtual try-on.
Specifically, we design the Garment Encoder to extract fine-grained clothing
semantic features, guiding the model to capture garment details and inject them
into the target video through the proposed attention feature fusion mechanism.
To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder
to encode pose signals, enabling the model to learn the interactions between
clothing and human posture and insert hierarchical Temporal Modules into the
text-to-image stable diffusion model for more coherent and lifelike video
synthesis. Furthermore, we collect a new dataset, which is the largest, with
the most diverse types of garments and the highest resolution for the task of
video virtual try-on to date. Extensive experiments demonstrate that our
approach is able to yield satisfactory video try-on results. The dataset,
codes, and weights will be publicly available. Project page:
https://becauseimbatman0.github.io/ViViD.",2024-05-20 05:28:22+00:00,"['Zixun Fang', 'Wei Zhai', 'Aimin Su', 'Hongliang Song', 'Kai Zhu', 'Mao Wang', 'Yu Chen', 'Zhiheng Liu', 'Yang Cao', 'Zheng-Jun Zha']",http://arxiv.org/abs/2405.11794v2
Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and Semantic Controls,"Sound designers and Foley artists usually sonorize a scene, such as from a
movie or video game, by manually annotating and sonorizing each action of
interest in the video. In our case, the intent is to leave full creative
control to sound designers with a tool that allows them to bypass the more
repetitive parts of their work, thus being able to focus on the creative
aspects of sound production. We achieve this presenting Stable-V2A, a two-stage
model consisting of: an RMS-Mapper that estimates an envelope representative of
the audio characteristics associated with the input video; and Stable-Foley, a
diffusion model based on Stable Audio Open that generates audio semantically
and temporally aligned with the target video. Temporal alignment is guaranteed
by the use of the envelope as a ControlNet input, while semantic alignment is
achieved through the use of sound representations chosen by the designer as
cross-attention conditioning of the diffusion process. We train and test our
model on Greatest Hits, a dataset commonly used to evaluate V2A models. In
addition, to test our model on a case study of interest, we introduce Walking
The Maps, a dataset of videos extracted from video games depicting animated
characters walking in different locations. Samples and code available on our
demo page at https://ispamm.github.io/Stable-V2A.",2024-12-19 16:37:19+00:00,"['Riccardo Fosco Gramaccioni', 'Christian Marinoni', 'Emilian Postolache', 'Marco Comunit', 'Luca Cosmo', 'Joshua D. Reiss', 'Danilo Comminiello']",http://arxiv.org/abs/2412.15023v2
Scene123: One Prompt to 3D Scene Generation via Video-Assisted and Consistency-Enhanced MAE,"As Artificial Intelligence Generated Content (AIGC) advances, a variety of
methods have been developed to generate text, images, videos, and 3D objects
from single or multimodal inputs, contributing efforts to emulate human-like
cognitive content creation. However, generating realistic large-scale scenes
from a single input presents a challenge due to the complexities involved in
ensuring consistency across extrapolated views generated by models. Benefiting
from recent video generation models and implicit neural representations, we
propose Scene123, a 3D scene generation model, that not only ensures realism
and diversity through the video generation framework but also uses implicit
neural fields combined with Masked Autoencoders (MAE) to effectively ensures
the consistency of unseen areas across views. Specifically, we initially warp
the input image (or an image generated from text) to simulate adjacent views,
filling the invisible areas with the MAE model. However, these filled images
usually fail to maintain view consistency, thus we utilize the produced views
to optimize a neural radiance field, enhancing geometric consistency.
  Moreover, to further enhance the details and texture fidelity of generated
views, we employ a GAN-based Loss against images derived from the input image
through the video generation model. Extensive experiments demonstrate that our
method can generate realistic and consistent scenes from a single prompt. Both
qualitative and quantitative results indicate that our approach surpasses
existing state-of-the-art methods. We show encourage video examples at
https://yiyingyang12.github.io/Scene123.github.io/.",2024-08-10 08:09:57+00:00,"['Yiying Yang', 'Fukun Yin', 'Jiayuan Fan', 'Xin Chen', 'Wanzhang Li', 'Gang Yu']",http://arxiv.org/abs/2408.05477v2
V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data,"Diffusion-based generative models have recently shown remarkable image and
video editing capabilities. However, local video editing, particularly removal
of small attributes like glasses, remains a challenge. Existing methods either
alter the videos excessively, generate unrealistic artifacts, or fail to
perform the requested edit consistently throughout the video. In this work, we
focus on consistent and identity-preserving removal of glasses in videos, using
it as a case study for consistent local attribute removal in videos. Due to the
lack of paired data, we adopt a weakly supervised approach and generate
synthetic imperfect data, using an adjusted pretrained diffusion model. We show
that despite data imperfection, by learning from our generated data and
leveraging the prior of pretrained diffusion models, our model is able to
perform the desired edit consistently while preserving the original video
content. Furthermore, we exemplify the generalization ability of our method to
other local video editing tasks by applying it successfully to facial
sticker-removal. Our approach demonstrates significant improvement over
existing methods, showcasing the potential of leveraging synthetic data and
strong video priors for local video editing tasks.",2024-06-20 17:14:43+00:00,"['Rotem Shalev-Arkushin', 'Aharon Azulay', 'Tavi Halperin', 'Eitan Richardson', 'Amit H. Bermano', 'Ohad Fried']",http://arxiv.org/abs/2406.14510v1
Video Creation by Demonstration,"We explore a novel video creation experience, namely Video Creation by
Demonstration. Given a demonstration video and a context image from a different
scene, we generate a physically plausible video that continues naturally from
the context image and carries out the action concepts from the demonstration.
To enable this capability, we present $\delta$-Diffusion, a self-supervised
training approach that learns from unlabeled videos by conditional future frame
prediction. Unlike most existing video generation controls that are based on
explicit signals, we adopts the form of implicit latent control for maximal
flexibility and expressiveness required by general videos. By leveraging a
video foundation model with an appearance bottleneck design on top, we extract
action latents from demonstration videos for conditioning the generation
process with minimal appearance leakage. Empirically, $\delta$-Diffusion
outperforms related baselines in terms of both human preference and large-scale
machine evaluations, and demonstrates potentials towards interactive world
simulation. Sampled video generation results are available at
https://delta-diffusion.github.io/.",2024-12-12 18:41:20+00:00,"['Yihong Sun', 'Hao Zhou', 'Liangzhe Yuan', 'Jennifer J. Sun', 'Yandong Li', 'Xuhui Jia', 'Hartwig Adam', 'Bharath Hariharan', 'Long Zhao', 'Ting Liu']",http://arxiv.org/abs/2412.09551v1
EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos,"We introduce EgoSonics, a method to generate semantically meaningful and
synchronized audio tracks conditioned on silent egocentric videos. Generating
audio for silent egocentric videos could open new applications in virtual
reality, assistive technologies, or for augmenting existing datasets. Existing
work has been limited to domains like speech, music, or impact sounds and
cannot capture the broad range of audio frequencies found in egocentric videos.
EgoSonics addresses these limitations by building on the strengths of latent
diffusion models for conditioned audio synthesis. We first encode and process
paired audio-video data to make them suitable for generation. The encoded data
is then used to train a model that can generate an audio track that captures
the semantics of the input video. Our proposed SyncroNet builds on top of
ControlNet to provide control signals that enables generation of temporally
synchronized audio. Extensive evaluations and a comprehensive user study show
that our model outperforms existing work in audio quality, and in our proposed
synchronization evaluation method. Furthermore, we demonstrate downstream
applications of our model in improving video summarization.",2024-07-30 06:57:00+00:00,"['Aashish Rai', 'Srinath Sridhar']",http://arxiv.org/abs/2407.20592v2
MVOC: a training-free multiple video object composition method with diffusion models,"Video composition is the core task of video editing. Although image
composition based on diffusion models has been highly successful, it is not
straightforward to extend the achievement to video object composition tasks,
which not only exhibit corresponding interaction effects but also ensure that
the objects in the composited video maintain motion and identity consistency,
which is necessary to composite a physical harmony video. To address this
challenge, we propose a Multiple Video Object Composition (MVOC) method based
on diffusion models. Specifically, we first perform DDIM inversion on each
video object to obtain the corresponding noise features. Secondly, we combine
and edit each object by image editing methods to obtain the first frame of the
composited video. Finally, we use the image-to-video generation model to
composite the video with feature and attention injections in the Video Object
Dependence Module, which is a training-free conditional guidance operation for
video generation, and enables the coordination of features and attention maps
between various objects that can be non-independent in the composited video.
The final generative model not only constrains the objects in the generated
video to be consistent with the original object motion and identity, but also
introduces interaction effects between objects. Extensive experiments have
demonstrated that the proposed method outperforms existing state-of-the-art
approaches. Project page: https://sobeymil.github.io/mvoc.com.",2024-06-22 12:18:46+00:00,"['Wei Wang', 'Yaosen Chen', 'Yuegen Liu', 'Qi Yuan', 'Shubin Yang', 'Yanru Zhang']",http://arxiv.org/abs/2406.15829v1
Leveraging Compressed Frame Sizes For Ultra-Fast Video Classification,"Classifying videos into distinct categories, such as Sport and Music Video,
is crucial for multimedia understanding and retrieval, especially when an
immense volume of video content is being constantly generated. Traditional
methods require video decompression to extract pixel-level features like color,
texture, and motion, thereby increasing computational and storage demands.
Moreover, these methods often suffer from performance degradation in
low-quality videos. We present a novel approach that examines only the
post-compression bitstream of a video to perform classification, eliminating
the need for bitstream decoding. To validate our approach, we built a
comprehensive data set comprising over 29,000 YouTube video clips, totaling
6,000 hours and spanning 11 distinct categories. Our evaluations indicate
precision, accuracy, and recall rates consistently above 80%, many exceeding
90%, and some reaching 99%. The algorithm operates approximately 15,000 times
faster than real-time for 30fps videos, outperforming traditional Dynamic Time
Warping (DTW) algorithm by seven orders of magnitude.",2024-03-13 14:35:13+00:00,"['Yuxing Han', 'Yunan Ding', 'Chen Ye Gan', 'Jiangtao Wen']",http://arxiv.org/abs/2403.08580v1
WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens,"World models play a crucial role in understanding and predicting the dynamics
of the world, which is essential for video generation. However, existing world
models are confined to specific scenarios such as gaming or driving, limiting
their ability to capture the complexity of general world dynamic environments.
Therefore, we introduce WorldDreamer, a pioneering world model to foster a
comprehensive comprehension of general world physics and motions, which
significantly enhances the capabilities of video generation. Drawing
inspiration from the success of large language models, WorldDreamer frames
world modeling as an unsupervised visual sequence modeling challenge. This is
achieved by mapping visual inputs to discrete tokens and predicting the masked
ones. During this process, we incorporate multi-modal prompts to facilitate
interaction within the world model. Our experiments show that WorldDreamer
excels in generating videos across different scenarios, including natural
scenes and driving environments. WorldDreamer showcases versatility in
executing tasks such as text-to-video conversion, image-tovideo synthesis, and
video editing. These results underscore WorldDreamer's effectiveness in
capturing dynamic elements within diverse general world environments.",2024-01-18 14:01:20+00:00,"['Xiaofeng Wang', 'Zheng Zhu', 'Guan Huang', 'Boyuan Wang', 'Xinze Chen', 'Jiwen Lu']",http://arxiv.org/abs/2401.09985v1
ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model,"Advancements in 3D scene reconstruction have transformed 2D images from the
real world into 3D models, producing realistic 3D results from hundreds of
input photos. Despite great success in dense-view reconstruction scenarios,
rendering a detailed scene from insufficient captured views is still an
ill-posed optimization problem, often resulting in artifacts and distortions in
unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction
paradigm that reframes the ambiguous reconstruction challenge as a temporal
generation task. The key insight is to unleash the strong generative prior of
large pre-trained video diffusion models for sparse-view reconstruction.
However, 3D view consistency struggles to be accurately preserved in directly
generated video frames from pre-trained models. To address this, given limited
input views, the proposed ReconX first constructs a global point cloud and
encodes it into a contextual space as the 3D structure condition. Guided by the
condition, the video diffusion model then synthesizes video frames that are
both detail-preserved and exhibit a high degree of 3D consistency, ensuring the
coherence of the scene from various perspectives. Finally, we recover the 3D
scene from the generated video through a confidence-aware 3D Gaussian Splatting
optimization scheme. Extensive experiments on various real-world datasets show
the superiority of our ReconX over state-of-the-art methods in terms of quality
and generalizability.",2024-08-29 17:59:40+00:00,"['Fangfu Liu', 'Wenqiang Sun', 'Hanyang Wang', 'Yikai Wang', 'Haowen Sun', 'Junliang Ye', 'Jun Zhang', 'Yueqi Duan']",http://arxiv.org/abs/2408.16767v2
3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors,"Novel-view synthesis aims to generate novel views of a scene from multiple
input images or videos, and recent advancements like 3D Gaussian splatting
(3DGS) have achieved notable success in producing photorealistic renderings
with efficient pipelines. However, generating high-quality novel views under
challenging settings, such as sparse input views, remains difficult due to
insufficient information in under-sampled areas, often resulting in noticeable
artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing
the representation quality of 3DGS representations. We leverage 2D video
diffusion priors to address the challenging 3D view consistency problem,
reformulating it as achieving temporal consistency within a video generation
process. 3DGS-Enhancer restores view-consistent latent features of rendered
novel views and integrates them with the input views through a spatial-temporal
decoder. The enhanced views are then used to fine-tune the initial 3DGS model,
significantly improving its rendering performance. Extensive experiments on
large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields
superior reconstruction performance and high-fidelity rendering results
compared to state-of-the-art methods. The project webpage is
https://xiliu8006.github.io/3DGS-Enhancer-project .",2024-10-21 17:59:09+00:00,"['Xi Liu', 'Chaoyi Zhou', 'Siyu Huang']",http://arxiv.org/abs/2410.16266v1
UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation,"Recent diffusion-based human image animation techniques have demonstrated
impressive success in synthesizing videos that faithfully follow a given
reference identity and a sequence of desired movement poses. Despite this,
there are still two limitations: i) an extra reference model is required to
align the identity image with the main video branch, which significantly
increases the optimization burden and model parameters; ii) the generated video
is usually short in time (e.g., 24 frames), hampering practical applications.
To address these shortcomings, we present a UniAnimate framework to enable
efficient and long-term human video generation. First, to reduce the
optimization difficulty and ensure temporal coherence, we map the reference
image along with the posture guidance and noise video into a common feature
space by incorporating a unified video diffusion model. Second, we propose a
unified noise input that supports random noised input as well as first frame
conditioned input, which enhances the ability to generate long-term video.
Finally, to further efficiently handle long sequences, we explore an
alternative temporal modeling architecture based on state space model to
replace the original computation-consuming temporal Transformer. Extensive
experimental results indicate that UniAnimate achieves superior synthesis
results over existing state-of-the-art counterparts in both quantitative and
qualitative evaluations. Notably, UniAnimate can even generate highly
consistent one-minute videos by iteratively employing the first frame
conditioning strategy. Code and models will be publicly available. Project
page: https://unianimate.github.io/.",2024-06-03 10:51:10+00:00,"['Xiang Wang', 'Shiwei Zhang', 'Changxin Gao', 'Jiayu Wang', 'Xiaoqiang Zhou', 'Yingya Zhang', 'Luxin Yan', 'Nong Sang']",http://arxiv.org/abs/2406.01188v1
SurGen: Text-Guided Diffusion Model for Surgical Video Generation,"Diffusion-based video generation models have made significant strides,
producing outputs with improved visual fidelity, temporal coherence, and user
control. These advancements hold great promise for improving surgical education
by enabling more realistic, diverse, and interactive simulation environments.
In this study, we introduce SurGen, a text-guided diffusion model tailored for
surgical video synthesis. SurGen produces videos with the highest resolution
and longest duration among existing surgical video generation models. We
validate the visual and temporal quality of the outputs using standard image
and video generation metrics. Additionally, we assess their alignment to the
corresponding text prompts through a deep learning classifier trained on
surgical data. Our results demonstrate the potential of diffusion models to
serve as valuable educational tools for surgical trainees.",2024-08-26 05:38:27+00:00,"['Joseph Cho', 'Samuel Schmidgall', 'Cyril Zakka', 'Mrudang Mathur', 'Dhamanpreet Kaur', 'Rohan Shad', 'William Hiesinger']",http://arxiv.org/abs/2408.14028v3
Investigating the Effectiveness of Cross-Attention to Unlock Zero-Shot Editing of Text-to-Video Diffusion Models,"With recent advances in image and video diffusion models for content
creation, a plethora of techniques have been proposed for customizing their
generated content. In particular, manipulating the cross-attention layers of
Text-to-Image (T2I) diffusion models has shown great promise in controlling the
shape and location of objects in the scene. Transferring image-editing
techniques to the video domain, however, is extremely challenging as object
motion and temporal consistency are difficult to capture accurately. In this
work, we take a first look at the role of cross-attention in Text-to-Video
(T2V) diffusion models for zero-shot video editing. While one-shot models have
shown potential in controlling motion and camera movement, we demonstrate
zero-shot control over object shape, position and movement in T2V models. We
show that despite the limitations of current T2V models, cross-attention
guidance can be a promising approach for editing videos.",2024-04-08 13:40:01+00:00,"['Saman Motamed', 'Wouter Van Gansbeke', 'Luc Van Gool']",http://arxiv.org/abs/2404.05519v1
Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation,"While recent foundational video generators produce visually rich output, they
still struggle with appearance drift, where objects gradually degrade or change
inconsistently across frames, breaking visual coherence. We hypothesize that
this is because there is no explicit supervision in terms of spatial tracking
at the feature level. We propose Track4Gen, a spatially aware video generator
that combines video diffusion loss with point tracking across frames, providing
enhanced spatial supervision on the diffusion features. Track4Gen merges the
video generation and point tracking tasks into a single network by making
minimal changes to existing video generation architectures. Using Stable Video
Diffusion as a backbone, Track4Gen demonstrates that it is possible to unify
video generation and point tracking, which are typically handled as separate
tasks. Our extensive evaluations show that Track4Gen effectively reduces
appearance drift, resulting in temporally stable and visually coherent video
generation. Project page: hyeonho99.github.io/track4gen",2024-12-08 18:21:00+00:00,"['Hyeonho Jeong', 'Chun-Hao Paul Huang', 'Jong Chul Ye', 'Niloy Mitra', 'Duygu Ceylan']",http://arxiv.org/abs/2412.06016v2
Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices,"Text-to-image (T2I) diffusion models achieve state-of-the-art results in
image synthesis and editing. However, leveraging such pretrained models for
video editing is considered a major challenge. Many existing works attempt to
enforce temporal consistency in the edited video through explicit
correspondence mechanisms, either in pixel space or between deep features.
These methods, however, struggle with strong nonrigid motion. In this paper, we
introduce a fundamentally different approach, which is based on the observation
that spatiotemporal slices of natural videos exhibit similar characteristics to
natural images. Thus, the same T2I diffusion model that is normally used only
as a prior on video frames, can also serve as a strong prior for enhancing
temporal consistency by applying it on spatiotemporal slices. Based on this
observation, we present Slicedit, a method for text-based video editing that
utilizes a pretrained T2I diffusion model to process both spatial and
spatiotemporal slices. Our method generates videos that retain the structure
and motion of the original video while adhering to the target text. Through
extensive experiments, we demonstrate Slicedit's ability to edit a wide range
of real-world videos, confirming its clear advantages compared to existing
competing methods. Webpage: https://matankleiner.github.io/slicedit/",2024-05-20 17:55:56+00:00,"['Nathaniel Cohen', 'Vladimir Kulikov', 'Matan Kleiner', 'Inbar Huberman-Spiegelglas', 'Tomer Michaeli']",http://arxiv.org/abs/2405.12211v1
Scene Co-pilot: Procedural Text to Video Generation with Human in the Loop,"Video generation has achieved impressive quality, but it still suffers from
artifacts such as temporal inconsistency and violation of physical laws.
Leveraging 3D scenes can fundamentally resolve these issues by providing
precise control over scene entities. To facilitate the easy generation of
diverse photorealistic scenes, we propose Scene Copilot, a framework combining
large language models (LLMs) with a procedural 3D scene generator.
Specifically, Scene Copilot consists of Scene Codex, BlenderGPT, and Human in
the loop. Scene Codex is designed to translate textual user input into commands
understandable by the 3D scene generator. BlenderGPT provides users with an
intuitive and direct way to precisely control the generated 3D scene and the
final output video. Furthermore, users can utilize Blender UI to receive
instant visual feedback. Additionally, we have curated a procedural dataset of
objects in code format to further enhance our system's capabilities. Each
component works seamlessly together to support users in generating desired 3D
scenes. Extensive experiments demonstrate the capability of our framework in
customizing 3D scenes and video generation.",2024-11-26 19:21:57+00:00,"['Zhaofang Qian', 'Abolfazl Sharifi', 'Tucker Carroll', 'Ser-Nam Lim']",http://arxiv.org/abs/2411.18644v1
Animate Your Motion: Turning Still Images into Dynamic Videos,"In recent years, diffusion models have made remarkable strides in
text-to-video generation, sparking a quest for enhanced control over video
outputs to more accurately reflect user intentions. Traditional efforts
predominantly focus on employing either semantic cues, like images or depth
maps, or motion-based conditions, like moving sketches or object bounding
boxes. Semantic inputs offer a rich scene context but lack detailed motion
specificity; conversely, motion inputs provide precise trajectory information
but miss the broader semantic narrative. For the first time, we integrate both
semantic and motion cues within a diffusion model for video generation, as
demonstrated in Fig 1. To this end, we introduce the Scene and Motion
Conditional Diffusion (SMCD), a novel methodology for managing multimodal
inputs. It incorporates a recognized motion conditioning module and
investigates various approaches to integrate scene conditions, promoting
synergy between different modalities. For model training, we separate the
conditions for the two modalities, introducing a two-stage training pipeline.
Experimental results demonstrate that our design significantly enhances video
quality, motion precision, and semantic coherence.",2024-03-15 10:36:24+00:00,"['Mingxiao Li', 'Bo Wan', 'Marie-Francine Moens', 'Tinne Tuytelaars']",http://arxiv.org/abs/2403.10179v3
VividDream: Generating 3D Scene with Ambient Dynamics,"We introduce VividDream, a method for generating explorable 4D scenes with
ambient dynamics from a single input image or text prompt. VividDream first
expands an input image into a static 3D point cloud through iterative
inpainting and geometry merging. An ensemble of animated videos is then
generated using video diffusion models with quality refinement techniques and
conditioned on renderings of the static 3D scene from the sampled camera
trajectories. We then optimize a canonical 4D scene representation using an
animated video ensemble, with per-video motion embeddings and visibility masks
to mitigate inconsistencies. The resulting 4D scene enables free-view
exploration of a 3D scene with plausible ambient scene dynamics. Experiments
demonstrate that VividDream can provide human viewers with compelling 4D
experiences generated based on diverse real images and text prompts.",2024-05-30 17:59:24+00:00,"['Yao-Chih Lee', 'Yi-Ting Chen', 'Andrew Wang', 'Ting-Hsuan Liao', 'Brandon Y. Feng', 'Jia-Bin Huang']",http://arxiv.org/abs/2405.20334v1
Replace Anyone in Videos,"Recent advancements in controllable human-centric video generation,
particularly with the rise of diffusion models, have demonstrated considerable
progress. However, achieving precise and localized control over human motion,
e.g., replacing or inserting individuals into videos while exhibiting desired
motion patterns, still remains challenging. In this work, we propose the
ReplaceAnyone framework, which focuses on localizing and manipulating human
motion in videos with diverse and intricate backgrounds. Specifically, we
formulate this task as an image-conditioned pose-driven video inpainting
paradigm, employing a unified video diffusion architecture that facilitates
image-conditioned pose-driven video generation and inpainting within masked
video regions. Moreover, we introduce diverse mask forms involving regular and
irregular shapes to avoid shape leakage and allow granular local control.
Additionally, we implement a two-stage training methodology, initially training
an image-conditioned pose driven video generation model, followed by joint
training of the video inpainting within masked areas. In this way, our approach
enables seamless replacement or insertion of characters while maintaining the
desired pose motion and reference appearance within a single framework.
Experimental results demonstrate the effectiveness of our method in generating
realistic and coherent video content.",2024-09-30 03:27:33+00:00,"['Xiang Wang', 'Changxin Gao', 'Yuehuan Wang', 'Nong Sang']",http://arxiv.org/abs/2409.19911v1
DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework,"Current video generation models excel at creating short, realistic clips, but
struggle with longer, multi-scene videos. We introduce \texttt{DreamFactory},
an LLM-based framework that tackles this challenge. \texttt{DreamFactory}
leverages multi-agent collaboration principles and a Key Frames Iteration
Design Method to ensure consistency and style across long videos. It utilizes
Chain of Thought (COT) to address uncertainties inherent in large language
models. \texttt{DreamFactory} generates long, stylistically coherent, and
complex videos. Evaluating these long-form videos presents a challenge. We
propose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene
Style Consistency Score. To further research in this area, we contribute the
Multi-Scene Videos Dataset containing over 150 human-rated videos.",2024-08-21 17:21:13+00:00,"['Zhifei Xie', 'Daniel Tang', 'Dingwei Tan', 'Jacques Klein', 'Tegawend F. Bissyand', 'Saad Ezzini']",http://arxiv.org/abs/2408.11788v1
Enhanced Creativity and Ideation through Stable Video Synthesis,"This paper explores the innovative application of Stable Video Diffusion
(SVD), a diffusion model that revolutionizes the creation of dynamic video
content from static images. As digital media and design industries accelerate,
SVD emerges as a powerful generative tool that enhances productivity and
introduces novel creative possibilities. The paper examines the technical
underpinnings of diffusion models, their practical effectiveness, and potential
future developments, particularly in the context of video generation. SVD
operates on a probabilistic framework, employing a gradual denoising process to
transform random noise into coherent video frames. It addresses the challenges
of visual consistency, natural movement, and stylistic reflection in generated
videos, showcasing high generalization capabilities. The integration of SVD in
design tasks promises enhanced creativity, rapid prototyping, and significant
time and cost efficiencies. It is particularly impactful in areas requiring
frame-to-frame consistency, natural motion capture, and creative diversity,
such as animation, visual effects, advertising, and educational content
creation. The paper concludes that SVD is a catalyst for design innovation,
offering a wide array of applications and a promising avenue for future
research and development in the field of digital media and design.",2024-05-22 05:23:14+00:00,"['Elijah Miller', 'Thomas Dupont', 'Mingming Wang']",http://arxiv.org/abs/2405.13357v1
What Matters in Detecting AI-Generated Videos like Sora?,"Recent advancements in diffusion-based video generation have showcased
remarkable results, yet the gap between synthetic and real-world videos remains
under-explored. In this study, we examine this gap from three fundamental
perspectives: appearance, motion, and geometry, comparing real-world videos
with those generated by a state-of-the-art AI model, Stable Video Diffusion. To
achieve this, we train three classifiers using 3D convolutional networks, each
targeting distinct aspects: vision foundation model features for appearance,
optical flow for motion, and monocular depth for geometry. Each classifier
exhibits strong performance in fake video detection, both qualitatively and
quantitatively. This indicates that AI-generated videos are still easily
detectable, and a significant gap between real and fake videos persists.
Furthermore, utilizing the Grad-CAM, we pinpoint systematic failures of
AI-generated videos in appearance, motion, and geometry. Finally, we propose an
Ensemble-of-Experts model that integrates appearance, optical flow, and depth
information for fake video detection, resulting in enhanced robustness and
generalization ability. Our model is capable of detecting videos generated by
Sora with high accuracy, even without exposure to any Sora videos during
training. This suggests that the gap between real and fake videos can be
generalized across various video generative models. Project page:
https://justin-crchang.github.io/3DCNNDetection.github.io/",2024-06-27 23:03:58+00:00,"['Chirui Chang', 'Zhengzhe Liu', 'Xiaoyang Lyu', 'Xiaojuan Qi']",http://arxiv.org/abs/2406.19568v1
AnimateLCM: Computation-Efficient Personalized Style Video Generation without Personalized Video Data,"This paper introduces an effective method for computation-efficient
personalized style video generation without requiring access to any
personalized video data. It reduces the necessary generation time of similarly
sized video diffusion models from 25 seconds to around 1 second while
maintaining the same level of performance. The method's effectiveness lies in
its dual-level decoupling learning approach: 1) separating the learning of
video style from video generation acceleration, which allows for personalized
style video generation without any personalized style video data, and 2)
separating the acceleration of image generation from the acceleration of video
motion generation, enhancing training efficiency and mitigating the negative
effects of low-quality video data.",2024-02-01 16:58:11+00:00,"['Fu-Yun Wang', 'Zhaoyang Huang', 'Weikang Bian', 'Xiaoyu Shi', 'Keqiang Sun', 'Guanglu Song', 'Yu Liu', 'Hongsheng Li']",http://arxiv.org/abs/2402.00769v3
I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models,"The remarkable generative capabilities of diffusion models have motivated
extensive research in both image and video editing. Compared to video editing
which faces additional challenges in the time dimension, image editing has
witnessed the development of more diverse, high-quality approaches and more
capable software like Photoshop. In light of this gap, we introduce a novel and
generic solution that extends the applicability of image editing tools to
videos by propagating edits from a single frame to the entire video using a
pre-trained image-to-video model. Our method, dubbed I2VEdit, adaptively
preserves the visual and motion integrity of the source video depending on the
extent of the edits, effectively handling global edits, local edits, and
moderate shape changes, which existing methods cannot fully achieve. At the
core of our method are two main processes: Coarse Motion Extraction to align
basic motion patterns with the original video, and Appearance Refinement for
precise adjustments using fine-grained attention matching. We also incorporate
a skip-interval strategy to mitigate quality degradation from auto-regressive
generation across multiple video clips. Experimental results demonstrate our
framework's superior performance in fine-grained video editing, proving its
capability to produce high-quality, temporally consistent outputs.",2024-05-26 11:47:40+00:00,"['Wenqi Ouyang', 'Yi Dong', 'Lei Yang', 'Jianlou Si', 'Xingang Pan']",http://arxiv.org/abs/2405.16537v1
Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers,"The quality of the data and annotation upper-bounds the quality of a
downstream model. While there exist large text corpora and image-text pairs,
high-quality video-text data is much harder to collect. First of all, manual
labeling is more time-consuming, as it requires an annotator to watch an entire
video. Second, videos have a temporal dimension, consisting of several scenes
stacked together, and showing multiple actions. Accordingly, to establish a
video dataset with high-quality captions, we propose an automatic approach
leveraging multimodal inputs, such as textual video description, subtitles, and
individual video frames. Specifically, we curate 3.8M high-resolution videos
from the publicly available HD-VILA-100M dataset. We then split them into
semantically consistent video clips, and apply multiple cross-modality teacher
models to obtain captions for each video. Next, we finetune a retrieval model
on a small subset where the best caption of each video is manually selected and
then employ the model in the whole dataset to select the best caption as the
annotation. In this way, we get 70M videos paired with high-quality text
captions. We dub the dataset as Panda-70M. We show the value of the proposed
dataset on three downstream tasks: video captioning, video and text retrieval,
and text-driven video generation. The models trained on the proposed data score
substantially better on the majority of metrics across all the tasks.",2024-02-29 18:59:50+00:00,"['Tsai-Shien Chen', 'Aliaksandr Siarohin', 'Willi Menapace', 'Ekaterina Deyneka', 'Hsiang-wei Chao', 'Byung Eun Jeon', 'Yuwei Fang', 'Hsin-Ying Lee', 'Jian Ren', 'Ming-Hsuan Yang', 'Sergey Tulyakov']",http://arxiv.org/abs/2402.19479v1
L3DG: Latent 3D Gaussian Diffusion,"We propose L3DG, the first approach for generative 3D modeling of 3D
Gaussians through a latent 3D Gaussian diffusion formulation. This enables
effective generative 3D modeling, scaling to generation of entire room-scale
scenes which can be very efficiently rendered. To enable effective synthesis of
3D Gaussians, we propose a latent diffusion formulation, operating in a
compressed latent space of 3D Gaussians. This compressed latent space is
learned by a vector-quantized variational autoencoder (VQ-VAE), for which we
employ a sparse convolutional architecture to efficiently operate on room-scale
scenes. This way, the complexity of the costly generation process via diffusion
is substantially reduced, allowing higher detail on object-level generation, as
well as scalability to large scenes. By leveraging the 3D Gaussian
representation, the generated scenes can be rendered from arbitrary viewpoints
in real-time. We demonstrate that our approach significantly improves visual
quality over prior work on unconditional object-level radiance field synthesis
and showcase its applicability to room-scale scene generation.",2024-10-17 13:19:32+00:00,"['Barbara Roessle', 'Norman Mller', 'Lorenzo Porzi', 'Samuel Rota Bul', 'Peter Kontschieder', 'Angela Dai', 'Matthias Niener']",http://arxiv.org/abs/2410.13530v1
PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible Pose Control,"In this paper, we introduce PoseCrafter, a one-shot method for personalized
video generation following the control of flexible poses. Built upon Stable
Diffusion and ControlNet, we carefully design an inference process to produce
high-quality videos without the corresponding ground-truth frames. First, we
select an appropriate reference frame from the training video and invert it to
initialize all latent variables for generation. Then, we insert the
corresponding training pose into the target pose sequences to enhance
faithfulness through a trained temporal attention module. Furthermore, to
alleviate the face and hand degradation resulting from discrepancies between
poses of training videos and inference poses, we implement simple latent
editing through an affine transformation matrix involving facial and hand
landmarks. Extensive experiments on several datasets demonstrate that
PoseCrafter achieves superior results to baselines pre-trained on a vast
collection of videos under 8 commonly used metrics. Besides, PoseCrafter can
follow poses from different individuals or artificial edits and simultaneously
retain the human identity in an open-domain training video. Our project page is
available at https://ml-gsai.github.io/PoseCrafter-demo/.",2024-05-23 13:53:50+00:00,"['Yong Zhong', 'Min Zhao', 'Zebin You', 'Xiaofeng Yu', 'Changwang Zhang', 'Chongxuan Li']",http://arxiv.org/abs/2405.14582v3
FrameBridge: Improving Image-to-Video Generation with Bridge Models,"Image-to-video (I2V) generation is gaining increasing attention with its wide
application in video synthesis. Recently, diffusion-based I2V models have
achieved remarkable progress given their novel design on network architecture,
cascaded framework, and motion representation. However, restricted by their
noise-to-data generation process, diffusion-based methods inevitably suffer the
difficulty to generate video samples with both appearance consistency and
temporal coherence from an uninformative Gaussian noise, which may limit their
synthesis quality. In this work, we present FrameBridge, taking the given
static image as the prior of video target and establishing a tractable bridge
model between them. By formulating I2V synthesis as a frames-to-frames
generation task and modelling it with a data-to-data process, we fully exploit
the information in input image and facilitate the generative model to learn the
image animation process. In two popular settings of training I2V models, namely
fine-tuning a pre-trained text-to-video (T2V) model or training from scratch,
we further propose two techniques, SNR-Aligned Fine-tuning (SAF) and neural
prior, which improve the fine-tuning efficiency of diffusion-based T2V models
to FrameBridge and the synthesis quality of bridge-based I2V models
respectively. Experiments conducted on WebVid-2M and UCF-101 demonstrate that:
(1) our FrameBridge achieves superior I2V quality in comparison with the
diffusion counterpart (zero-shot FVD 83 vs. 176 on MSR-VTT and non-zero-shot
FVD 122 vs. 171 on UCF-101); (2) our proposed SAF and neural prior effectively
enhance the ability of bridge-based I2V models in the scenarios of fine-tuning
and training from scratch. Demo samples can be visited at:
https://framebridge-demo.github.io/.",2024-10-20 12:10:24+00:00,"['Yuji Wang', 'Zehua Chen', 'Xiaoyu Chen', 'Jun Zhu', 'Jianfei Chen']",http://arxiv.org/abs/2410.15371v1
VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control,"Modern text-to-video synthesis models demonstrate coherent, photorealistic
generation of complex videos from a text description. However, most existing
models lack fine-grained control over camera movement, which is critical for
downstream applications related to content creation, visual effects, and 3D
vision. Recently, new methods demonstrate the ability to generate videos with
controllable camera poses these techniques leverage pre-trained U-Net-based
diffusion models that explicitly disentangle spatial and temporal generation.
Still, no existing approach enables camera control for new, transformer-based
video diffusion models that process spatial and temporal information jointly.
Here, we propose to tame video transformers for 3D camera control using a
ControlNet-like conditioning mechanism that incorporates spatiotemporal camera
embeddings based on Pl\""ucker coordinates. The approach demonstrates
state-of-the-art performance for controllable video generation after
fine-tuning on the RealEstate10K dataset. To the best of our knowledge, our
work is the first to enable camera control for transformer-based video
diffusion models.",2024-07-17 17:59:05+00:00,"['Sherwin Bahmani', 'Ivan Skorokhodov', 'Aliaksandr Siarohin', 'Willi Menapace', 'Guocheng Qian', 'Michael Vasilkovsky', 'Hsin-Ying Lee', 'Chaoyang Wang', 'Jiaxu Zou', 'Andrea Tagliasacchi', 'David B. Lindell', 'Sergey Tulyakov']",http://arxiv.org/abs/2407.12781v3
Wonderland: Navigating 3D Scenes from a Single Image,"This paper addresses a challenging question: How can we efficiently create
high-quality, wide-scope 3D scenes from a single arbitrary image? Existing
methods face several constraints, such as requiring multi-view data,
time-consuming per-scene optimization, low visual quality in backgrounds, and
distorted reconstructions in unseen areas. We propose a novel pipeline to
overcome these limitations. Specifically, we introduce a large-scale
reconstruction model that uses latents from a video diffusion model to predict
3D Gaussian Splattings for the scenes in a feed-forward manner. The video
diffusion model is designed to create videos precisely following specified
camera trajectories, allowing it to generate compressed video latents that
contain multi-view information while maintaining 3D consistency. We train the
3D reconstruction model to operate on the video latent space with a progressive
training strategy, enabling the efficient generation of high-quality,
wide-scope, and generic 3D scenes. Extensive evaluations across various
datasets demonstrate that our model significantly outperforms existing methods
for single-view 3D scene generation, particularly with out-of-domain images.
For the first time, we demonstrate that a 3D reconstruction model can be
effectively built upon the latent space of a diffusion model to realize
efficient 3D scene generation.",2024-12-16 18:58:17+00:00,"['Hanwen Liang', 'Junli Cao', 'Vidit Goel', 'Guocheng Qian', 'Sergei Korolev', 'Demetri Terzopoulos', 'Konstantinos N. Plataniotis', 'Sergey Tulyakov', 'Jian Ren']",http://arxiv.org/abs/2412.12091v1
Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion,"We present a method for generating Streetscapes-long sequences of views
through an on-the-fly synthesized city-scale scene. Our generation is
conditioned by language input (e.g., city name, weather), as well as an
underlying map/layout hosting the desired trajectory. Compared to recent models
for video generation or 3D view synthesis, our method can scale to much
longer-range camera trajectories, spanning several city blocks, while
maintaining visual quality and consistency. To achieve this goal, we build on
recent work on video diffusion, used within an autoregressive framework that
can easily scale to long sequences. In particular, we introduce a new temporal
imputation method that prevents our autoregressive approach from drifting from
the distribution of realistic city imagery. We train our Streetscapes system on
a compelling source of data-posed imagery from Google Street View, along with
contextual map data-which allows users to generate city views conditioned on
any desired city layout, with controllable camera poses. Please see more
results at our project page at https://boyangdeng.com/streetscapes.",2024-07-18 17:56:30+00:00,"['Boyang Deng', 'Richard Tucker', 'Zhengqi Li', 'Leonidas Guibas', 'Noah Snavely', 'Gordon Wetzstein']",http://arxiv.org/abs/2407.13759v2
Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention,"In recent years there have been remarkable breakthroughs in image-to-video
generation. However, the 3D consistency and camera controllability of generated
frames have remained unsolved. Recent studies have attempted to incorporate
camera control into the generation process, but their results are often limited
to simple trajectories or lack the ability to generate consistent videos from
multiple distinct camera paths for the same scene. To address these
limitations, we introduce Cavia, a novel framework for camera-controllable,
multi-view video generation, capable of converting an input image into multiple
spatiotemporally consistent videos. Our framework extends the spatial and
temporal attention modules into view-integrated attention modules, improving
both viewpoint and temporal consistency. This flexible design allows for joint
training with diverse curated data sources, including scene-level static
videos, object-level synthetic multi-view dynamic videos, and real-world
monocular dynamic videos. To our best knowledge, Cavia is the first of its kind
that allows the user to precisely specify camera motion while obtaining object
motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art
methods in terms of geometric consistency and perceptual quality. Project Page:
https://ir1d.github.io/Cavia/",2024-10-14 17:46:32+00:00,"['Dejia Xu', 'Yifan Jiang', 'Chen Huang', 'Liangchen Song', 'Thorsten Gernoth', 'Liangliang Cao', 'Zhangyang Wang', 'Hao Tang']",http://arxiv.org/abs/2410.10774v1
Unlearning Concepts from Text-to-Video Diffusion Models,"With the advancement of computer vision and natural language processing,
text-to-video generation, enabled by text-to-video diffusion models, has become
more prevalent. These models are trained using a large amount of data from the
internet. However, the training data often contain copyrighted content,
including cartoon character icons and artist styles, private portraits, and
unsafe videos. Since filtering the data and retraining the model is
challenging, methods for unlearning specific concepts from text-to-video
diffusion models have been investigated. However, due to the high computational
complexity and relative large optimization scale, there is little work on
unlearning methods for text-to-video diffusion models. We propose a novel
concept-unlearning method by transferring the unlearning capability of the text
encoder of text-to-image diffusion models to text-to-video diffusion models.
Specifically, the method optimizes the text encoder using few-shot unlearning,
where several generated images are used. We then use the optimized text encoder
in text-to-video diffusion models to generate videos. Our method costs low
computation resources and has small optimization scale. We discuss the
generated videos after unlearning a concept. The experiments demonstrates that
our method can unlearn copyrighted cartoon characters, artist styles, objects
and people's facial characteristics. Our method can unlearn a concept within
about 100 seconds on an RTX 3070. Since there was no concept unlearning method
for text-to-video diffusion models before, we make concept unlearning feasible
and more accessible in the text-to-video domain.",2024-07-19 11:15:02+00:00,"['Shiqi Liu', 'Yihua Tan']",http://arxiv.org/abs/2407.14209v1
Q-Bench-Video: Benchmarking the Video Quality Understanding of LMMs,"With the rising interest in research on Large Multi-modal Models (LMMs) for
video understanding, many studies have emphasized general video comprehension
capabilities, neglecting the systematic exploration into video quality
understanding. To address this oversight, we introduce Q-Bench-Video in this
paper, a new benchmark specifically designed to evaluate LMMs' proficiency in
discerning video quality. a) To ensure video source diversity, Q-Bench-Video
encompasses videos from natural scenes, AI-generated Content (AIGC), and
Computer Graphics (CG). b) Building on the traditional multiple-choice
questions format with the Yes-or-No and What-How categories, we include
Open-ended questions to better evaluate complex scenarios. Additionally, we
incorporate the video pair quality comparison question to enhance
comprehensiveness. c) Beyond the traditional Technical, Aesthetic, and Temporal
distortions, we have expanded our evaluation aspects to include the dimension
of AIGC distortions, which addresses the increasing demand for video
generation. Finally, we collect a total of 2,378 question-answer pairs and test
them on 12 open-source & 5 proprietary LMMs. Our findings indicate that while
LMMs have a foundational understanding of video quality, their performance
remains incomplete and imprecise, with a notable discrepancy compared to human
performance. Through Q-Bench-Video, we seek to catalyze community interest,
stimulate further research, and unlock the untapped potential of LMMs to close
the gap in video quality understanding.",2024-09-30 08:05:00+00:00,"['Zicheng Zhang', 'Ziheng Jia', 'Haoning Wu', 'Chunyi Li', 'Zijian Chen', 'Yingjie Zhou', 'Wei Sun', 'Xiaohong Liu', 'Xiongkuo Min', 'Weisi Lin', 'Guangtao Zhai']",http://arxiv.org/abs/2409.20063v2
Decoupled Video Generation with Chain of Training-free Diffusion Model Experts,"Video generation models hold substantial potential in areas such as
filmmaking. However, current video diffusion models need high computational
costs and produce suboptimal results due to extreme complexity of video
generation task. In this paper, we propose \textbf{ConFiner}, an efficient
video generation framework that decouples video generation into easier
subtasks: structure \textbf{con}trol and spatial-temporal re\textbf{fine}ment.
It can generate high-quality videos with chain of off-the-shelf diffusion model
experts, each expert responsible for a decoupled subtask. During the
refinement, we introduce coordinated denoising, which can merge multiple
diffusion experts' capabilities into a single sampling. Furthermore, we design
ConFiner-Long framework, which can generate long coherent video with three
constraint strategies on ConFiner. Experimental results indicate that with only
10\% of the inference cost, our ConFiner surpasses representative models like
Lavie and Modelscope across all objective and subjective metrics. And
ConFiner-Long can generate high-quality and coherent videos with up to 600
frames.",2024-08-24 01:33:28+00:00,"['Wenhao Li', 'Yichao Cao', 'Xiu Su', 'Xi Lin', 'Shan You', 'Mingkai Zheng', 'Yi Chen', 'Chang Xu']",http://arxiv.org/abs/2408.13423v4
StereoCrafter-Zero: Zero-Shot Stereo Video Generation with Noisy Restart,"Generating high-quality stereo videos that mimic human binocular vision
requires consistent depth perception and temporal coherence across frames.
Despite advances in image and video synthesis using diffusion models, producing
high-quality stereo videos remains a challenging task due to the difficulty of
maintaining consistent temporal and spatial coherence between left and right
views. We introduce StereoCrafter-Zero, a novel framework for zero-shot stereo
video generation that leverages video diffusion priors without requiring paired
training data. Our key innovations include a noisy restart strategy to
initialize stereo-aware latent representations and an iterative refinement
process that progressively harmonizes the latent space, addressing issues like
temporal flickering and view inconsistencies. In addition, we propose the use
of dissolved depth maps to streamline latent space operations by reducing
high-frequency depth information. Our comprehensive evaluations, including
quantitative metrics and user studies, demonstrate that StereoCrafter-Zero
produces high-quality stereo videos with enhanced depth consistency and
temporal smoothness, even when depth estimations are imperfect. Our framework
is robust and adaptable across various diffusion models, setting a new
benchmark for zero-shot stereo video generation and enabling more immersive
visual experiences. Our code is in
https://github.com/shijianjian/StereoCrafter-Zero.",2024-11-21 16:41:55+00:00,"['Jian Shi', 'Qian Wang', 'Zhenyu Li', 'Ramzi Idoughi', 'Peter Wonka']",http://arxiv.org/abs/2411.14295v2
Open-Sora: Democratizing Efficient Video Production for All,"Vision and language are the two foundational senses for humans, and they
build up our cognitive ability and intelligence. While significant
breakthroughs have been made in AI language ability, artificial visual
intelligence, especially the ability to generate and simulate the world we see,
is far lagging behind. To facilitate the development and accessibility of
artificial visual intelligence, we created Open-Sora, an open-source video
generation model designed to produce high-fidelity video content. Open-Sora
supports a wide spectrum of visual generation tasks, including text-to-image
generation, text-to-video generation, and image-to-video generation. The model
leverages advanced deep learning architectures and training/inference
techniques to enable flexible video synthesis, which could generate video
content of up to 15 seconds, up to 720p resolution, and arbitrary aspect
ratios. Specifically, we introduce Spatial-Temporal Diffusion Transformer
(STDiT), an efficient diffusion framework for videos that decouples spatial and
temporal attention. We also introduce a highly compressive 3D autoencoder to
make representations compact and further accelerate training with an ad hoc
training strategy. Through this initiative, we aim to foster innovation,
creativity, and inclusivity within the community of AI content creation. By
embracing the open-source principle, Open-Sora democratizes full access to all
the training/inference/data preparation codes as well as model weights. All
resources are publicly available at: https://github.com/hpcaitech/Open-Sora.",2024-12-29 08:52:49+00:00,"['Zangwei Zheng', 'Xiangyu Peng', 'Tianji Yang', 'Chenhui Shen', 'Shenggui Li', 'Hongxin Liu', 'Yukun Zhou', 'Tianyi Li', 'Yang You']",http://arxiv.org/abs/2412.20404v1
The Effects of Short Video-Sharing Services on Video Copy Detection,"The short video-sharing services that allow users to post 10-30 second videos
(e.g., YouTube Shorts and TikTok) have attracted a lot of attention in recent
years. However, conventional video copy detection (VCD) methods mainly focus on
general video-sharing services (e.g., YouTube and Bilibili), and the effects of
short video-sharing services on video copy detection are still unclear.
Considering that illegally copied videos in short video-sharing services have
service-distinctive characteristics, especially in those time lengths, the pros
and cons of VCD in those services are required to be analyzed. In this paper,
we examine the effects of short video-sharing services on VCD by constructing a
dataset that has short video-sharing service characteristics. Our novel dataset
is automatically constructed from the publicly available dataset to have
reference videos and fixed short-time-length query videos, and such automation
procedures assure the reproducibility and data privacy preservation of this
paper. From the experimental results focusing on segment-level and video-level
situations, we can see that three effects: ""Segment-level VCD in short
video-sharing services is more difficult than those in general video-sharing
services"", ""Video-level VCD in short video-sharing services is easier than
those in general video-sharing services"", ""The video alignment component mainly
suppress the detection performance in short video-sharing services"".",2024-03-26 23:47:17+00:00,"['Rintaro Yanagi', 'Yamato Okamoto', 'Shuhei Yokoo', ""Shin'ichi Satoh""]",http://arxiv.org/abs/2403.18158v1
MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models,"Text-to-video models have demonstrated impressive capabilities in producing
diverse and captivating video content, showcasing a notable advancement in
generative AI. However, these models generally lack fine-grained control over
motion patterns, limiting their practical applicability. We introduce
MotionFlow, a novel framework designed for motion transfer in video diffusion
models. Our method utilizes cross-attention maps to accurately capture and
manipulate spatial and temporal dynamics, enabling seamless motion transfers
across various contexts. Our approach does not require training and works on
test-time by leveraging the inherent capabilities of pre-trained video
diffusion models. In contrast to traditional approaches, which struggle with
comprehensive scene changes while maintaining consistent motion, MotionFlow
successfully handles such complex transformations through its attention-based
mechanism. Our qualitative and quantitative experiments demonstrate that
MotionFlow significantly outperforms existing models in both fidelity and
versatility even during drastic scene alterations.",2024-12-06 18:59:12+00:00,"['Tuna Han Salih Meral', 'Hidir Yesiltepe', 'Connor Dunlop', 'Pinar Yanardag']",http://arxiv.org/abs/2412.05275v1
Generative Omnimatte: Learning to Decompose Video into Layers,"Given a video and a set of input object masks, an omnimatte method aims to
decompose the video into semantically meaningful layers containing individual
objects along with their associated effects, such as shadows and reflections.
Existing omnimatte methods assume a static background or accurate pose and
depth estimation and produce poor decompositions when these assumptions are
violated. Furthermore, due to the lack of generative prior on natural videos,
existing methods cannot complete dynamic occluded regions. We present a novel
generative layered video decomposition framework to address the omnimatte
problem. Our method does not assume a stationary scene or require camera pose
or depth information and produces clean, complete layers, including convincing
completions of occluded dynamic regions. Our core idea is to train a video
diffusion model to identify and remove scene effects caused by a specific
object. We show that this model can be finetuned from an existing video
inpainting model with a small, carefully curated dataset, and demonstrate
high-quality decompositions and editing results for a wide range of casually
captured videos containing soft shadows, glossy reflections, splashing water,
and more.",2024-11-25 18:59:57+00:00,"['Yao-Chih Lee', 'Erika Lu', 'Sarah Rumbley', 'Michal Geyer', 'Jia-Bin Huang', 'Tali Dekel', 'Forrester Cole']",http://arxiv.org/abs/2411.16683v2
LetsTalk: Latent Diffusion Transformer for Talking Video Synthesis,"Portrait image animation using audio has rapidly advanced, enabling the
creation of increasingly realistic and expressive animated faces. The
challenges of this multimodality-guided video generation task involve fusing
various modalities while ensuring consistency in timing and portrait. We
further seek to produce vivid talking heads. To address these challenges, we
present LetsTalk (LatEnt Diffusion TranSformer for Talking Video Synthesis), a
diffusion transformer that incorporates modular temporal and spatial attention
mechanisms to merge multimodality and enhance spatial-temporal consistency. To
handle multimodal conditions, we first summarize three fusion schemes, ranging
from shallow to deep fusion compactness, and thoroughly explore their impact
and applicability. Then we propose a suitable solution according to the
modality differences of image, audio, and video generation. For portrait, we
utilize a deep fusion scheme (Symbiotic Fusion) to ensure portrait consistency.
For audio, we implement a shallow fusion scheme (Direct Fusion) to achieve
audio-animation alignment while preserving diversity. Our extensive experiments
demonstrate that our approach generates temporally coherent and realistic
videos with enhanced diversity and liveliness.",2024-11-24 04:46:00+00:00,"['Haojie Zhang', 'Zhihao Liang', 'Ruibo Fu', 'Zhengqi Wen', 'Xuefei Liu', 'Chenxing Li', 'Jianhua Tao', 'Yaling Liang']",http://arxiv.org/abs/2411.16748v1
Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision,"The performance of Large Vision Language Models (LVLMs) is dependent on the
size and quality of their training datasets. Existing video instruction tuning
datasets lack diversity as they are derived by prompting large language models
with video captions to generate question-answer pairs, and are therefore mostly
descriptive. Meanwhile, many labeled video datasets with diverse labels and
supervision exist - however, we find that their integration into LVLMs is
non-trivial. Herein, we present Video Self-Training with augmented Reasoning
(Video-STaR), the first video self-training approach. Video-STaR allows the
utilization of any labeled video dataset for video instruction tuning. In
Video-STaR, an LVLM cycles between instruction generation and finetuning, which
we show (I) improves general video understanding and (II) adapts LVLMs to novel
downstream tasks with existing supervision. During generation, an LVLM is
prompted to propose an answer. The answers are then filtered only to those that
contain the original video labels, and the LVLM is then re-trained on the
generated dataset. By only training on generated answers that contain the
correct video labels, Video-STaR utilizes these existing video labels as weak
supervision for video instruction tuning. Our results demonstrate that
Video-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,
where TempCompass performance improved by 10%, and (II) on downstream tasks,
where Video-STaR improved Kinetics700-QA accuracy by 20% and action quality
assessment on FineDiving by 15%.",2024-07-08 17:59:42+00:00,"['Orr Zohar', 'Xiaohan Wang', 'Yonatan Bitton', 'Idan Szpektor', 'Serena Yeung-Levy']",http://arxiv.org/abs/2407.06189v1
World-consistent Video Diffusion with Explicit 3D Modeling,"Recent advancements in diffusion models have set new benchmarks in image and
video generation, enabling realistic visual synthesis across single- and
multi-frame contexts. However, these models still struggle with efficiently and
explicitly generating 3D-consistent content. To address this, we propose
World-consistent Video Diffusion (WVD), a novel framework that incorporates
explicit 3D supervision using XYZ images, which encode global 3D coordinates
for each image pixel. More specifically, we train a diffusion transformer to
learn the joint distribution of RGB and XYZ frames. This approach supports
multi-task adaptability via a flexible inpainting strategy. For example, WVD
can estimate XYZ frames from ground-truth RGB or generate novel RGB frames
using XYZ projections along a specified camera trajectory. In doing so, WVD
unifies tasks like single-image-to-3D generation, multi-view stereo, and
camera-controlled video generation. Our approach demonstrates competitive
performance across multiple benchmarks, providing a scalable solution for
3D-consistent video and image generation with a single pretrained model.",2024-12-02 18:58:23+00:00,"['Qihang Zhang', 'Shuangfei Zhai', 'Miguel Angel Bautista', 'Kevin Miao', 'Alexander Toshev', 'Joshua Susskind', 'Jiatao Gu']",http://arxiv.org/abs/2412.01821v1
HiFiVFS: High Fidelity Video Face Swapping,"Face swapping aims to generate results that combine the identity from the
source with attributes from the target. Existing methods primarily focus on
image-based face swapping. When processing videos, each frame is handled
independently, making it difficult to ensure temporal stability. From a model
perspective, face swapping is gradually shifting from generative adversarial
networks (GANs) to diffusion models (DMs), as DMs have been shown to possess
stronger generative capabilities. Current diffusion-based approaches often
employ inpainting techniques, which struggle to preserve fine-grained
attributes like lighting and makeup. To address these challenges, we propose a
high fidelity video face swapping (HiFiVFS) framework, which leverages the
strong generative capability and temporal prior of Stable Video Diffusion
(SVD). We build a fine-grained attribute module to extract
identity-disentangled and fine-grained attribute features through identity
desensitization and adversarial learning. Additionally, We introduce detailed
identity injection to further enhance identity similarity. Extensive
experiments demonstrate that our method achieves state-of-the-art (SOTA) in
video face swapping, both qualitatively and quantitatively.",2024-11-27 12:30:24+00:00,"['Xu Chen', 'Keke He', 'Junwei Zhu', 'Yanhao Ge', 'Wei Li', 'Chengjie Wang']",http://arxiv.org/abs/2411.18293v2
Controlling Space and Time with Diffusion Models,"We present 4DiM, a cascaded diffusion model for 4D novel view synthesis
(NVS), conditioned on one or more images of a general scene, and a set of
camera poses and timestamps. To overcome challenges due to limited availability
of 4D training data, we advocate joint training on 3D (with camera pose), 4D
(pose+time) and video (time but no pose) data and propose a new architecture
that enables the same. We further advocate the calibration of SfM posed data
using monocular metric depth estimators for metric scale camera control. For
model evaluation, we introduce new metrics to enrich and overcome shortcomings
of current evaluation schemes, demonstrating state-of-the-art results in both
fidelity and pose control compared to existing diffusion models for 3D NVS,
while at the same time adding the ability to handle temporal dynamics. 4DiM is
also used for improved panorama stitching, pose-conditioned video to video
translation, and several other tasks. For an overview see
https://4d-diffusion.github.io",2024-07-10 17:23:33+00:00,"['Daniel Watson', 'Saurabh Saxena', 'Lala Li', 'Andrea Tagliasacchi', 'David J. Fleet']",http://arxiv.org/abs/2407.07860v1
Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models,"Despite having tremendous progress in image-to-3D generation, existing
methods still struggle to produce multi-view consistent images with
high-resolution textures in detail, especially in the paradigm of 2D diffusion
that lacks 3D awareness. In this work, we present High-resolution Image-to-3D
model (Hi3D), a new video diffusion based paradigm that redefines a single
image to multi-view images as 3D-aware sequential image generation (i.e.,
orbital video generation). This methodology delves into the underlying temporal
consistency knowledge in video diffusion model that generalizes well to
geometry consistency across multiple views in 3D generation. Technically, Hi3D
first empowers the pre-trained video diffusion model with 3D-aware prior
(camera pose condition), yielding multi-view images with low-resolution texture
details. A 3D-aware video-to-video refiner is learnt to further scale up the
multi-view images with high-resolution texture details. Such high-resolution
multi-view images are further augmented with novel views through 3D Gaussian
Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D
reconstruction. Extensive experiments on both novel view synthesis and single
view reconstruction demonstrate that our Hi3D manages to produce superior
multi-view consistency images with highly-detailed textures. Source code and
data are available at \url{https://github.com/yanghb22-fdu/Hi3D-Official}.",2024-09-11 17:58:57+00:00,"['Haibo Yang', 'Yang Chen', 'Yingwei Pan', 'Ting Yao', 'Zhineng Chen', 'Chong-Wah Ngo', 'Tao Mei']",http://arxiv.org/abs/2409.07452v1
MultiDiff: Consistent Novel View Synthesis from a Single Image,"We introduce MultiDiff, a novel approach for consistent novel view synthesis
of scenes from a single RGB image. The task of synthesizing novel views from a
single reference image is highly ill-posed by nature, as there exist multiple,
plausible explanations for unobserved areas. To address this issue, we
incorporate strong priors in form of monocular depth predictors and
video-diffusion models. Monocular depth enables us to condition our model on
warped reference images for the target views, increasing geometric stability.
The video-diffusion prior provides a strong proxy for 3D scenes, allowing the
model to learn continuous and pixel-accurate correspondences across generated
images. In contrast to approaches relying on autoregressive image generation
that are prone to drifts and error accumulation, MultiDiff jointly synthesizes
a sequence of frames yielding high-quality and multi-view consistent results --
even for long-term scene generation with large camera movements, while reducing
inference time by an order of magnitude. For additional consistency and image
quality improvements, we introduce a novel, structured noise distribution. Our
experimental results demonstrate that MultiDiff outperforms state-of-the-art
methods on the challenging, real-world datasets RealEstate10K and ScanNet.
Finally, our model naturally supports multi-view consistent editing without the
need for further tuning.",2024-06-26 17:53:51+00:00,"['Norman Mller', 'Katja Schwarz', 'Barbara Roessle', 'Lorenzo Porzi', 'Samuel Rota Bul', 'Matthias Niener', 'Peter Kontschieder']",http://arxiv.org/abs/2406.18524v1
Transformer-based Image and Video Inpainting: Current Challenges and Future Directions,"Image inpainting is currently a hot topic within the field of computer
vision. It offers a viable solution for various applications, including
photographic restoration, video editing, and medical imaging. Deep learning
advancements, notably convolutional neural networks (CNNs) and generative
adversarial networks (GANs), have significantly enhanced the inpainting task
with an improved capability to fill missing or damaged regions in an image or
video through the incorporation of contextually appropriate details. These
advancements have improved other aspects, including efficiency, information
preservation, and achieving both realistic textures and structures. Recently,
visual transformers have been exploited and offer some improvements to image or
video inpainting. The advent of transformer-based architectures, which were
initially designed for natural language processing, has also been integrated
into computer vision tasks. These methods utilize self-attention mechanisms
that excel in capturing long-range dependencies within data; therefore, they
are particularly effective for tasks requiring a comprehensive understanding of
the global context of an image or video. In this paper, we provide a
comprehensive review of the current image or video inpainting approaches, with
a specific focus on transformer-based techniques, with the goal to highlight
the significant improvements and provide a guideline for new researchers in the
field of image or video inpainting using visual transformers. We categorized
the transformer-based techniques by their architectural configurations, types
of damage, and performance metrics. Furthermore, we present an organized
synthesis of the current challenges, and suggest directions for future research
in the field of image or video inpainting.",2024-06-28 20:42:36+00:00,"['Omar Elharrouss', 'Rafat Damseh', 'Abdelkader Nasreddine Belkacem', 'Elarbi Badidi', 'Abderrahmane Lakas']",http://arxiv.org/abs/2407.00226v1
Video Diffusion Models are Strong Video Inpainter,"Propagation-based video inpainting using optical flow at the pixel or feature
level has recently garnered significant attention. However, it has limitations
such as the inaccuracy of optical flow prediction and the propagation of noise
over time. These issues result in non-uniform noise and time consistency
problems throughout the video, which are particularly pronounced when the
removed area is large and involves substantial movement. To address these
issues, we propose a novel First Frame Filling Video Diffusion Inpainting model
(FFF-VDI). We design FFF-VDI inspired by the capabilities of pre-trained
image-to-video diffusion models that can transform the first frame image into a
highly natural video. To apply this to the video inpainting task, we propagate
the noise latent information of future frames to fill the masked areas of the
first frame's noise latent code. Next, we fine-tune the pre-trained
image-to-video diffusion model to generate the inpainted video. The proposed
model addresses the limitations of existing methods that rely on optical flow
quality, producing much more natural and temporally consistent videos. This
proposed approach is the first to effectively integrate image-to-video
diffusion models into video inpainting tasks. Through various comparative
experiments, we demonstrate that the proposed model can robustly handle diverse
inpainting types with high quality.",2024-08-21 08:01:00+00:00,"['Minhyeok Lee', 'Suhwan Cho', 'Chajin Shin', 'Jungho Lee', 'Sunghun Yang', 'Sangyoun Lee']",http://arxiv.org/abs/2408.11402v3
Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting,"Recent advancements in zero-shot video diffusion models have shown promise
for text-driven video editing, but challenges remain in achieving high temporal
consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting
(3DGS)-based video refiner designed to enhance temporal consistency in
zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian
optimizing process tailored for editing dynamic monocular videos. In the first
stage, Video-3DGS employs an improved version of COLMAP, referred to as
MC-COLMAP, which processes original videos using a Masked and Clipped approach.
For each video clip, MC-COLMAP generates the point clouds for dynamic
foreground objects and complex backgrounds. These point clouds are utilized to
initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent
foreground and background views. Both foreground and background views are then
merged with a 2D learnable parameter map to reconstruct full views. In the
second stage, we leverage the reconstruction ability developed in the first
stage to impose the temporal constraints on the video diffusion model. To
demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive
experiments across two related tasks: Video Reconstruction and Video Editing.
Video-3DGS trained with 3k iterations significantly improves video
reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency
(x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods
on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring
temporal consistency across 58 dynamic monocular videos.",2024-06-04 17:57:37+00:00,"['Inkyu Shin', 'Qihang Yu', 'Xiaohui Shen', 'In So Kweon', 'Kuk-Jin Yoon', 'Liang-Chieh Chen']",http://arxiv.org/abs/2406.02541v3
Context-aware Talking Face Video Generation,"In this paper, we consider a novel and practical case for talking face video
generation. Specifically, we focus on the scenarios involving multi-people
interactions, where the talking context, such as audience or surroundings, is
present. In these situations, the video generation should take the context into
consideration in order to generate video content naturally aligned with driving
audios and spatially coherent to the context. To achieve this, we provide a
two-stage and cross-modal controllable video generation pipeline, taking facial
landmarks as an explicit and compact control signal to bridge the driving
audio, talking context and generated videos. Inside this pipeline, we devise a
3D video diffusion model, allowing for efficient contort of both spatial
conditions (landmarks and context video), as well as audio condition for
temporally coherent generation. The experimental results verify the advantage
of the proposed method over other baselines in terms of audio-video
synchronization, video fidelity and frame consistency.",2024-02-28 06:25:50+00:00,"['Meidai Xuanyuan', 'Yuwang Wang', 'Honglei Guo', 'Qionghai Dai']",http://arxiv.org/abs/2402.18092v1
Vlogger: Make Your Dream A Vlog,"In this work, we present Vlogger, a generic AI system for generating a
minute-level video blog (i.e., vlog) of user descriptions. Different from short
videos with a few seconds, vlog often contains a complex storyline with
diversified scenes, which is challenging for most existing video generation
approaches. To break through this bottleneck, our Vlogger smartly leverages
Large Language Model (LLM) as Director and decomposes a long video generation
task of vlog into four key stages, where we invoke various foundation models to
play the critical roles of vlog professionals, including (1) Script, (2) Actor,
(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,
our Vlogger can generate vlogs through explainable cooperation of top-down
planning and bottom-up shooting. Moreover, we introduce a novel video diffusion
model, ShowMaker, which serves as a videographer in our Vlogger for generating
the video snippet of each shooting scene. By incorporating Script and Actor
attentively as textual and visual prompts, it can effectively enhance
spatial-temporal coherence in the snippet. Besides, we design a concise mixed
training paradigm for ShowMaker, boosting its capacity for both T2V generation
and prediction. Finally, the extensive experiments show that our method
achieves state-of-the-art performance on zero-shot T2V generation and
prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs
from open-world descriptions, without loss of video coherence on script and
actor. The code and model is all available at
https://github.com/zhuangshaobin/Vlogger.",2024-01-17 18:55:12+00:00,"['Shaobin Zhuang', 'Kunchang Li', 'Xinyuan Chen', 'Yaohui Wang', 'Ziwei Liu', 'Yu Qiao', 'Yali Wang']",http://arxiv.org/abs/2401.09414v1
EffiVED:Efficient Video Editing via Text-instruction Diffusion Models,"Large-scale text-to-video models have shown remarkable abilities, but their
direct application in video editing remains challenging due to limited
available datasets. Current video editing methods commonly require per-video
fine-tuning of diffusion models or specific inversion optimization to ensure
high-fidelity edits. In this paper, we introduce EffiVED, an efficient
diffusion-based model that directly supports instruction-guided video editing.
To achieve this, we present two efficient workflows to gather video editing
pairs, utilizing augmentation and fundamental vision-language techniques. These
workflows transform vast image editing datasets and open-world videos into a
high-quality dataset for training EffiVED. Experimental results reveal that
EffiVED not only generates high-quality editing videos but also executes
rapidly. Finally, we demonstrate that our data collection method significantly
improves editing performance and can potentially tackle the scarcity of video
editing data. Code can be found at https://github.com/alibaba/EffiVED.",2024-03-18 08:42:08+00:00,"['Zhenghao Zhang', 'Zuozhuo Dai', 'Long Qin', 'Weizhi Wang']",http://arxiv.org/abs/2403.11568v2
Efficient Video to Audio Mapper with Visual Scene Detection,"Video-to-audio (V2A) generation aims to produce corresponding audio given
silent video inputs. This task is particularly challenging due to the
cross-modality and sequential nature of the audio-visual features involved.
Recent works have made significant progress in bridging the domain gap between
video and audio, generating audio that is semantically aligned with the video
content. However, a critical limitation of these approaches is their inability
to effectively recognize and handle multiple scenes within a video, often
leading to suboptimal audio generation in such cases. In this paper, we first
reimplement a state-of-the-art V2A model with a slightly modified light-weight
architecture, achieving results that outperform the baseline. We then propose
an improved V2A model that incorporates a scene detector to address the
challenge of switching between multiple visual scenes. Results on VGGSound show
that our model can recognize and handle multiple scenes within a video and
achieve superior performance against the baseline for both fidelity and
relevance.",2024-09-15 18:51:18+00:00,"['Mingjing Yi', 'Ming Li']",http://arxiv.org/abs/2409.09823v1
OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model,"Variational Autoencoder (VAE), compressing videos into latent
representations, is a crucial preceding component of Latent Video Diffusion
Models (LVDMs). With the same reconstruction quality, the more sufficient the
VAE's compression for videos is, the more efficient the LVDMs are. However,
most LVDMs utilize 2D image VAE, whose compression for videos is only in the
spatial dimension and often ignored in the temporal dimension. How to conduct
temporal compression for videos in a VAE to obtain more concise latent
representations while promising accurate reconstruction is seldom explored. To
fill this gap, we propose an omni-dimension compression VAE, named OD-VAE,
which can temporally and spatially compress videos. Although OD-VAE's more
sufficient compression brings a great challenge to video reconstruction, it can
still achieve high reconstructed accuracy by our fine design. To obtain a
better trade-off between video reconstruction quality and compression speed,
four variants of OD-VAE are introduced and analyzed. In addition, a novel tail
initialization is designed to train OD-VAE more efficiently, and a novel
inference strategy is proposed to enable OD-VAE to handle videos of arbitrary
length with limited GPU memory. Comprehensive experiments on video
reconstruction and LVDM-based video generation demonstrate the effectiveness
and efficiency of our proposed methods.",2024-09-02 12:20:42+00:00,"['Liuhan Chen', 'Zongjian Li', 'Bin Lin', 'Bin Zhu', 'Qian Wang', 'Shenghai Yuan', 'Xing Zhou', 'Xinhua Cheng', 'Li Yuan']",http://arxiv.org/abs/2409.01199v2
MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence,"Recent advancements in video generation have primarily leveraged diffusion
models for short-duration content. However, these approaches often fall short
in modeling complex narratives and maintaining character consistency over
extended periods, which is essential for long-form video production like
movies. We propose MovieDreamer, a novel hierarchical framework that integrates
the strengths of autoregressive models with diffusion-based rendering to
pioneer long-duration video generation with intricate plot progressions and
high visual fidelity. Our approach utilizes autoregressive models for global
narrative coherence, predicting sequences of visual tokens that are
subsequently transformed into high-quality video frames through diffusion
rendering. This method is akin to traditional movie production processes, where
complex stories are factorized down into manageable scene capturing. Further,
we employ a multimodal script that enriches scene descriptions with detailed
character information and visual style, enhancing continuity and character
identity across scenes. We present extensive experiments across various movie
genres, demonstrating that our approach not only achieves superior visual and
narrative quality but also effectively extends the duration of generated
content significantly beyond current capabilities. Homepage:
https://aim-uofa.github.io/MovieDreamer/.",2024-07-23 17:17:05+00:00,"['Canyu Zhao', 'Mingyu Liu', 'Wen Wang', 'Weihua Chen', 'Fan Wang', 'Hao Chen', 'Bo Zhang', 'Chunhua Shen']",http://arxiv.org/abs/2407.16655v2
Transforming Static Images Using Generative Models for Video Salient Object Detection,"In many video processing tasks, leveraging large-scale image datasets is a
common strategy, as image data is more abundant and facilitates comprehensive
knowledge transfer. A typical approach for simulating video from static images
involves applying spatial transformations, such as affine transformations and
spline warping, to create sequences that mimic temporal progression. However,
in tasks like video salient object detection, where both appearance and motion
cues are critical, these basic image-to-video techniques fail to produce
realistic optical flows that capture the independent motion properties of each
object. In this study, we show that image-to-video diffusion models can
generate realistic transformations of static images while understanding the
contextual relationships between image components. This ability allows the
model to generate plausible optical flows, preserving semantic integrity while
reflecting the independent motion of scene elements. By augmenting individual
images in this way, we create large-scale image-flow pairs that significantly
enhance model training. Our approach achieves state-of-the-art performance
across all public benchmark datasets, outperforming existing approaches.",2024-11-21 09:41:33+00:00,"['Suhwan Cho', 'Minhyeok Lee', 'Jungho Lee', 'Sangyoun Lee']",http://arxiv.org/abs/2411.13975v1
Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained Image Denoisers,"Recent advancements in deep learning have shown impressive results in image
and video denoising, leveraging extensive pairs of noisy and noise-free data
for supervision. However, the challenge of acquiring paired videos for dynamic
scenes hampers the practical deployment of deep video denoising techniques. In
contrast, this obstacle is less pronounced in image denoising, where paired
data is more readily available. Thus, a well-trained image denoiser could serve
as a reliable spatial prior for video denoising. In this paper, we propose a
novel unsupervised video denoising framework, named ``Temporal As a Plugin''
(TAP), which integrates tunable temporal modules into a pre-trained image
denoiser. By incorporating temporal modules, our method can harness temporal
information across noisy frames, complementing its power of spatial denoising.
Furthermore, we introduce a progressive fine-tuning strategy that refines each
temporal module using the generated pseudo clean video frames, progressively
enhancing the network's denoising performance. Compared to other unsupervised
video denoising methods, our framework demonstrates superior performance on
both sRGB and raw video denoising datasets.",2024-09-17 15:05:33+00:00,"['Zixuan Fu', 'Lanqing Guo', 'Chong Wang', 'Yufei Wang', 'Zhihao Li', 'Bihan Wen']",http://arxiv.org/abs/2409.11256v1
Predicting Long-horizon Futures by Conditioning on Geometry and Time,"Our work explores the task of generating future sensor observations
conditioned on the past. We are motivated by `predictive coding' concepts from
neuroscience as well as robotic applications such as self-driving vehicles.
Predictive video modeling is challenging because the future may be multi-modal
and learning at scale remains computationally expensive for video processing.
To address both challenges, our key insight is to leverage the large-scale
pretraining of image diffusion models which can handle multi-modality. We
repurpose image models for video prediction by conditioning on new frame
timestamps. Such models can be trained with videos of both static and dynamic
scenes. To allow them to be trained with modestly-sized datasets, we introduce
invariances by factoring out illumination and texture by forcing the model to
predict (pseudo) depth, readily obtained for in-the-wild videos via
off-the-shelf monocular depth networks. In fact, we show that simply modifying
networks to predict grayscale pixels already improves the accuracy of video
prediction. Given the extra controllability with timestamp conditioning, we
propose sampling schedules that work better than the traditional autoregressive
and hierarchical sampling strategies. Motivated by probabilistic metrics from
the object forecasting literature, we create a benchmark for video prediction
on a diverse set of videos spanning indoor and outdoor scenes and a large
vocabulary of objects. Our experiments illustrate the effectiveness of learning
to condition on timestamps, and show the importance of predicting the future
with invariant modalities.",2024-04-17 16:56:31+00:00,"['Tarasha Khurana', 'Deva Ramanan']",http://arxiv.org/abs/2404.11554v1
Optical-Flow Guided Prompt Optimization for Coherent Video Generation,"While text-to-video diffusion models have made significant strides, many
still face challenges in generating videos with temporal consistency. Within
diffusion frameworks, guidance techniques have proven effective in enhancing
output quality during inference; however, applying these methods to video
diffusion models introduces additional complexity of handling computations
across entire sequences. To address this, we propose a novel framework called
MotionPrompt that guides the video generation process via optical flow.
Specifically, we train a discriminator to distinguish optical flow between
random pairs of frames from real videos and generated ones. Given that prompts
can influence the entire video, we optimize learnable token embeddings during
reverse sampling steps by using gradients from a trained discriminator applied
to random frame pairs. This approach allows our method to generate visually
coherent video sequences that closely reflect natural motion dynamics, without
compromising the fidelity of the generated content. We demonstrate the
effectiveness of our approach across various models.",2024-11-23 12:26:52+00:00,"['Hyelin Nam', 'Jaemin Kim', 'Dohun Lee', 'Jong Chul Ye']",http://arxiv.org/abs/2411.15540v2
FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality,"In this paper, we present \textbf{\textit{FasterCache}}, a novel
training-free strategy designed to accelerate the inference of video diffusion
models with high-quality generation. By analyzing existing cache-based methods,
we observe that \textit{directly reusing adjacent-step features degrades video
quality due to the loss of subtle variations}. We further perform a pioneering
investigation of the acceleration potential of classifier-free guidance (CFG)
and reveal significant redundancy between conditional and unconditional
features within the same timestep. Capitalizing on these observations, we
introduce FasterCache to substantially accelerate diffusion-based video
generation. Our key contributions include a dynamic feature reuse strategy that
preserves both feature distinction and temporal continuity, and CFG-Cache which
optimizes the reuse of conditional and unconditional outputs to further enhance
inference speed without compromising video quality. We empirically evaluate
FasterCache on recent video diffusion models. Experimental results show that
FasterCache can significantly accelerate video generation (\eg 1.67$\times$
speedup on Vchitect-2.0) while keeping video quality comparable to the
baseline, and consistently outperform existing methods in both inference speed
and video quality.",2024-10-25 07:24:38+00:00,"['Zhengyao Lv', 'Chenyang Si', 'Junhao Song', 'Zhenyu Yang', 'Yu Qiao', 'Ziwei Liu', 'Kwan-Yee K. Wong']",http://arxiv.org/abs/2410.19355v2
Diff-BGM: A Diffusion Model for Video Background Music Generation,"When editing a video, a piece of attractive background music is
indispensable. However, video background music generation tasks face several
challenges, for example, the lack of suitable training datasets, and the
difficulties in flexibly controlling the music generation process and
sequentially aligning the video and music. In this work, we first propose a
high-quality music-video dataset BGM909 with detailed annotation and shot
detection to provide multi-modal information about the video and music. We then
present evaluation metrics to assess music quality, including music diversity
and alignment between music and video with retrieval precision metrics.
Finally, we propose the Diff-BGM framework to automatically generate the
background music for a given video, which uses different signals to control
different aspects of the music during the generation process, i.e., uses
dynamic video features to control music rhythm and semantic features to control
the melody and atmosphere. We propose to align the video and music sequentially
by introducing a segment-aware cross-attention layer. Experiments verify the
effectiveness of our proposed method. The code and models are available at
https://github.com/sizhelee/Diff-BGM.",2024-05-20 09:48:36+00:00,"['Sizhe Li', 'Yiming Qin', 'Minghang Zheng', 'Xin Jin', 'Yang Liu']",http://arxiv.org/abs/2405.11913v1
VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models,"Text-to-video generation aims to produce a video based on a given prompt.
Recently, several commercial video models have been able to generate plausible
videos with minimal noise, excellent details, and high aesthetic scores.
However, these models rely on large-scale, well-filtered, high-quality videos
that are not accessible to the community. Many existing research works, which
train models using the low-quality WebVid-10M dataset, struggle to generate
high-quality videos because the models are optimized to fit WebVid-10M. In this
work, we explore the training scheme of video models extended from Stable
Diffusion and investigate the feasibility of leveraging low-quality videos and
synthesized high-quality images to obtain a high-quality video model. We first
analyze the connection between the spatial and temporal modules of video models
and the distribution shift to low-quality videos. We observe that full training
of all modules results in a stronger coupling between spatial and temporal
modules than only training temporal modules. Based on this stronger coupling,
we shift the distribution to higher quality without motion degradation by
finetuning spatial modules with high-quality images, resulting in a generic
high-quality video model. Evaluations are conducted to demonstrate the
superiority of the proposed method, particularly in picture quality, motion,
and concept composition.",2024-01-17 08:30:32+00:00,"['Haoxin Chen', 'Yong Zhang', 'Xiaodong Cun', 'Menghan Xia', 'Xintao Wang', 'Chao Weng', 'Ying Shan']",http://arxiv.org/abs/2401.09047v1
SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis,"Despite advances in Large Multi-modal Models, applying them to long and
untrimmed video content remains challenging due to limitations in context
length and substantial memory overhead. These constraints often lead to
significant information loss and reduced relevance in the model responses. With
the exponential growth of video data across web platforms, understanding
long-form video is crucial for advancing generalized intelligence. In this
paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel
video-LLM framework designed to enhance the comprehension of lengthy video
content through targeted retrieval process. We address two main challenges to
achieve it: (i) We present the SceneWalk dataset, a high-quality collection of
87.8K long videos, each densely captioned at the segment level to enable models
to capture scene continuity and maintain rich descriptive context. (ii) We
develop robust architectural designs integrating dynamic routing mechanism and
spatio-temporal projector to efficiently retrieve and process relevant video
segments based on user queries. Our framework mitigates the limitations of
current video-LMMs by allowing for precise identification and retrieval of
relevant video segments in response to queries, thereby improving the
contextual relevance of the generated responses. Through extensive experiments,
SALOVA demonstrates enhanced capability in processing complex long-form videos,
showing significant capability to maintain contextual integrity across extended
sequences.",2024-11-25 08:04:47+00:00,"['Junho Kim', 'Hyunjun Kim', 'Hosu Lee', 'Yong Man Ro']",http://arxiv.org/abs/2411.16173v2
Disentangling Foreground and Background Motion for Enhanced Realism in Human Video Generation,"Recent advancements in human video synthesis have enabled the generation of
high-quality videos through the application of stable diffusion models.
However, existing methods predominantly concentrate on animating solely the
human element (the foreground) guided by pose information, while leaving the
background entirely static. Contrary to this, in authentic, high-quality
videos, backgrounds often dynamically adjust in harmony with foreground
movements, eschewing stagnancy. We introduce a technique that concurrently
learns both foreground and background dynamics by segregating their movements
using distinct motion representations. Human figures are animated leveraging
pose-based motion, capturing intricate actions. Conversely, for backgrounds, we
employ sparse tracking points to model motion, thereby reflecting the natural
interaction between foreground activity and environmental changes. Training on
real-world videos enhanced with this innovative motion depiction approach, our
model generates videos exhibiting coherent movement in both foreground subjects
and their surrounding contexts. To further extend video generation to longer
sequences without accumulating errors, we adopt a clip-by-clip generation
strategy, introducing global features at each step. To ensure seamless
continuity across these segments, we ingeniously link the final frame of a
produced clip with input noise to spawn the succeeding one, maintaining
narrative flow. Throughout the sequential generation process, we infuse the
feature representation of the initial reference image into the network,
effectively curtailing any cumulative color inconsistencies that may otherwise
arise. Empirical evaluations attest to the superiority of our method in
producing videos that exhibit harmonious interplay between foreground actions
and responsive background dynamics, surpassing prior methodologies in this
regard.",2024-05-26 00:53:26+00:00,"['Jinlin Liu', 'Kai Yu', 'Mengyang Feng', 'Xiefan Guo', 'Miaomiao Cui']",http://arxiv.org/abs/2405.16393v2
"Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models","We introduce Vidu, a high-performance text-to-video generator that is capable
of producing 1080p videos up to 16 seconds in a single generation. Vidu is a
diffusion model with U-ViT as its backbone, which unlocks the scalability and
the capability for handling long videos. Vidu exhibits strong coherence and
dynamism, and is capable of generating both realistic and imaginative videos,
as well as understanding some professional photography techniques, on par with
Sora -- the most powerful reported text-to-video generator. Finally, we perform
initial experiments on other controllable video generation, including
canny-to-video generation, video prediction and subject-driven generation,
which demonstrate promising results.",2024-05-07 11:52:49+00:00,"['Fan Bao', 'Chendong Xiang', 'Gang Yue', 'Guande He', 'Hongzhou Zhu', 'Kaiwen Zheng', 'Min Zhao', 'Shilong Liu', 'Yaole Wang', 'Jun Zhu']",http://arxiv.org/abs/2405.04233v1
From an Image to a Scene: Learning to Imagine the World from a Million 360 Videos,"Three-dimensional (3D) understanding of objects and scenes play a key role in
humans' ability to interact with the world and has been an active area of
research in computer vision, graphics, and robotics. Large scale synthetic and
object-centric 3D datasets have shown to be effective in training models that
have 3D understanding of objects. However, applying a similar approach to
real-world objects and scenes is difficult due to a lack of large-scale data.
Videos are a potential source for real-world 3D data, but finding diverse yet
corresponding views of the same content has shown to be difficult at scale.
Furthermore, standard videos come with fixed viewpoints, determined at the time
of capture. This restricts the ability to access scenes from a variety of more
diverse and potentially useful perspectives. We argue that large scale 360
videos can address these limitations to provide: scalable corresponding frames
from diverse views. In this paper, we introduce 360-1M, a 360 video dataset,
and a process for efficiently finding corresponding frames from diverse
viewpoints at scale. We train our diffusion-based model, Odin, on 360-1M.
Empowered by the largest real-world, multi-view dataset to date, Odin is able
to freely generate novel views of real-world scenes. Unlike previous methods,
Odin can move the camera through the environment, enabling the model to infer
the geometry and layout of the scene. Additionally, we show improved
performance on standard novel view synthesis and 3D reconstruction benchmarks.",2024-12-10 18:59:44+00:00,"['Matthew Wallingford', 'Anand Bhattad', 'Aditya Kusupati', 'Vivek Ramanujan', 'Matt Deitke', 'Sham Kakade', 'Aniruddha Kembhavi', 'Roozbeh Mottaghi', 'Wei-Chiu Ma', 'Ali Farhadi']",http://arxiv.org/abs/2412.07770v1
VAST 1.0: A Unified Framework for Controllable and Consistent Video Generation,"Generating high-quality videos from textual descriptions poses challenges in
maintaining temporal coherence and control over subject motion. We propose VAST
(Video As Storyboard from Text), a two-stage framework to address these
challenges and enable high-quality video generation. In the first stage,
StoryForge transforms textual descriptions into detailed storyboards, capturing
human poses and object layouts to represent the structural essence of the
scene. In the second stage, VisionForge generates videos from these
storyboards, producing high-quality videos with smooth motion, temporal
consistency, and spatial coherence. By decoupling text understanding from video
generation, VAST enables precise control over subject dynamics and scene
composition. Experiments on the VBench benchmark demonstrate that VAST
outperforms existing methods in both visual quality and semantic expression,
setting a new standard for dynamic and coherent video generation.",2024-12-21 15:59:07+00:00,"['Chi Zhang', 'Yuanzhi Liang', 'Xi Qiu', 'Fangqiu Yi', 'Xuelong Li']",http://arxiv.org/abs/2412.16677v1
ActAnywhere: Subject-Aware Video Background Generation,"Generating video background that tailors to foreground subject motion is an
important problem for the movie industry and visual effects community. This
task involves synthesizing background that aligns with the motion and
appearance of the foreground subject, while also complies with the artist's
creative intention. We introduce ActAnywhere, a generative model that automates
this process which traditionally requires tedious manual efforts. Our model
leverages the power of large-scale video diffusion models, and is specifically
tailored for this task. ActAnywhere takes a sequence of foreground subject
segmentation as input and an image that describes the desired scene as
condition, to produce a coherent video with realistic foreground-background
interactions while adhering to the condition frame. We train our model on a
large-scale dataset of human-scene interaction videos. Extensive evaluations
demonstrate the superior performance of our model, significantly outperforming
baselines. Moreover, we show that ActAnywhere generalizes to diverse
out-of-distribution samples, including non-human subjects. Please visit our
project webpage at https://actanywhere.github.io.",2024-01-19 17:16:16+00:00,"['Boxiao Pan', 'Zhan Xu', 'Chun-Hao Paul Huang', 'Krishna Kumar Singh', 'Yang Zhou', 'Leonidas J. Guibas', 'Jimei Yang']",http://arxiv.org/abs/2401.10822v1
OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization,"In recent years, the field of text-to-video (T2V) generation has made
significant strides. Despite this progress, there is still a gap between
theoretical advancements and practical application, amplified by issues like
degraded image quality and flickering artifacts. Recent advancements in
enhancing the video diffusion model (VDM) through feedback learning have shown
promising results. However, these methods still exhibit notable limitations,
such as misaligned feedback and inferior scalability. To tackle these issues,
we introduce OnlineVPO, a more efficient preference learning approach tailored
specifically for video diffusion models. Our method features two novel designs,
firstly, instead of directly using image-based reward feedback, we leverage the
video quality assessment (VQA) model trained on synthetic data as the reward
model to provide distribution and modality-aligned feedback on the video
diffusion model. Additionally, we introduce an online DPO algorithm to address
the off-policy optimization and scalability issue in existing video preference
learning frameworks. By employing the video reward model to offer concise video
feedback on the fly, OnlineVPO offers effective and efficient preference
guidance. Extensive experiments on the open-source video-diffusion model
demonstrate OnlineVPO as a simple yet effective and more importantly scalable
preference learning algorithm for video diffusion models, offering valuable
insights for future advancements in this domain.",2024-12-19 18:34:50+00:00,"['Jiacheng Zhang', 'Jie Wu', 'Weifeng Chen', 'Yatai Ji', 'Xuefeng Xiao', 'Weilin Huang', 'Kai Han']",http://arxiv.org/abs/2412.15159v1
From Covert Hiding to Visual Editing: Robust Generative Video Steganography,"Traditional video steganography methods are based on modifying the covert
space for embedding, whereas we propose an innovative approach that embeds
secret message within semantic feature for steganography during the video
editing process. Although existing traditional video steganography methods
display a certain level of security and embedding capacity, they lack adequate
robustness against common distortions in online social networks (OSNs). In this
paper, we introduce an end-to-end robust generative video steganography network
(RoGVS), which achieves visual editing by modifying semantic feature of videos
to embed secret message. We employ face-swapping scenario to showcase the
visual editing effects. We first design a secret message embedding module to
adaptively hide secret message into the semantic feature of videos. Extensive
experiments display that the proposed RoGVS method applied to facial video
datasets demonstrate its superiority over existing video and image
steganography techniques in terms of both robustness and capacity.",2024-01-01 03:40:07+00:00,"['Xueying Mao', 'Xiaoxiao Hu', 'Wanli Peng', 'Zhenliang Gan', 'Qichao Ying', 'Zhenxing Qian', 'Sheng Li', 'Xinpeng Zhang']",http://arxiv.org/abs/2401.00652v1
ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation,"Human-scene interaction (HSI) generation is crucial for applications in
embodied AI, virtual reality, and robotics. Yet, existing methods cannot
synthesize interactions in unseen environments such as in-the-wild scenes or
reconstructed scenes, as they rely on paired 3D scenes and captured human
motion data for training, which are unavailable for unseen environments. We
present ZeroHSI, a novel approach that enables zero-shot 4D human-scene
interaction synthesis, eliminating the need for training on any MoCap data. Our
key insight is to distill human-scene interactions from state-of-the-art video
generation models, which have been trained on vast amounts of natural human
movements and interactions, and use differentiable rendering to reconstruct
human-scene interactions. ZeroHSI can synthesize realistic human motions in
both static scenes and environments with dynamic objects, without requiring any
ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different
types of various indoor and outdoor scenes with different interaction prompts,
demonstrating its ability to generate diverse and contextually appropriate
human-scene interactions.",2024-12-24 18:55:38+00:00,"['Hongjie Li', 'Hong-Xing Yu', 'Jiaman Li', 'Jiajun Wu']",http://arxiv.org/abs/2412.18600v2
AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation,"We propose AV-Link, a unified framework for Video-to-Audio (A2V) and
Audio-to-Video (A2V) generation that leverages the activations of frozen video
and audio diffusion models for temporally-aligned cross-modal conditioning. The
key to our framework is a Fusion Block that facilitates bidirectional
information exchange between video and audio diffusion models through
temporally-aligned self attention operations. Unlike prior work that uses
dedicated models for A2V and V2A tasks and relies on pretrained feature
extractors, AV-Link achieves both tasks in a single self-contained framework,
directly leveraging features obtained by the complementary modality (i.e. video
features to generate audio, or audio features to generate video). Extensive
automatic and subjective evaluations demonstrate that our method achieves a
substantial improvement in audio-video synchronization, outperforming more
expensive baselines such as the MovieGen video-to-audio model.",2024-12-19 18:57:21+00:00,"['Moayed Haji-Ali', 'Willi Menapace', 'Aliaksandr Siarohin', 'Ivan Skorokhodov', 'Alper Canberk', 'Kwot Sin Lee', 'Vicente Ordonez', 'Sergey Tulyakov']",http://arxiv.org/abs/2412.15191v2
Promptus: Can Prompts Streaming Replace Video Streaming with Stable Diffusion,"With the exponential growth of video traffic, traditional video streaming
systems are approaching their limits in compression efficiency and
communication capacity. To further reduce bitrate while maintaining quality, we
propose Promptus, a disruptive novel system that streaming prompts instead of
video content with Stable Diffusion, which converts video frames into a series
of ""prompts"" for delivery. To ensure pixel alignment, a gradient descent-based
prompt fitting framework is proposed. To achieve adaptive bitrate for prompts,
a low-rank decomposition-based bitrate control algorithm is introduced. For
inter-frame compression of prompts, a temporal smoothing-based prompt
interpolation algorithm is proposed. Evaluations across various video domains
and real network traces demonstrate Promptus can enhance the perceptual quality
by 0.111 and 0.092 (in LPIPS) compared to VAE and H.265, respectively, and
decreases the ratio of severely distorted frames by 89.3% and 91.7%. Moreover,
Promptus achieves real-time video generation from prompts at over 150 FPS. To
the best of our knowledge, Promptus is the first attempt to replace video
codecs with prompt inversion and the first to use prompt streaming instead of
video streaming. Our work opens up a new paradigm for efficient video
communication beyond the Shannon limit.",2024-05-30 13:16:48+00:00,"['Jiangkai Wu', 'Liming Liu', 'Yunpeng Tan', 'Junlin Hao', 'Xinggong Zhang']",http://arxiv.org/abs/2405.20032v1
DeCo: Decoupled Human-Centered Diffusion Video Editing with Motion Consistency,"Diffusion models usher a new era of video editing, flexibly manipulating the
video contents with text prompts. Despite the widespread application demand in
editing human-centered videos, these models face significant challenges in
handling complex objects like humans. In this paper, we introduce DeCo, a novel
video editing framework specifically designed to treat humans and the
background as separate editable targets, ensuring global spatial-temporal
consistency by maintaining the coherence of each individual component.
Specifically, we propose a decoupled dynamic human representation that utilizes
a parametric human body prior to generate tailored humans while preserving the
consistent motions as the original video. In addition, we consider the
background as a layered atlas to apply text-guided image editing approaches on
it. To further enhance the geometry and texture of humans during the
optimization, we extend the calculation of score distillation sampling into
normal space and image space. Moreover, we tackle inconsistent lighting between
the edited targets by leveraging a lighting-aware video harmonizer, a problem
previously overlooked in decompose-edit-combine approaches. Extensive
qualitative and numerical experiments demonstrate that DeCo outperforms prior
video editing methods in human-centered videos, especially in longer videos.",2024-08-14 11:53:40+00:00,"['Xiaojing Zhong', 'Xinyi Huang', 'Xiaofeng Yang', 'Guosheng Lin', 'Qingyao Wu']",http://arxiv.org/abs/2408.07481v1
Towards motion from video diffusion models,"Text-conditioned video diffusion models have emerged as a powerful tool in
the realm of video generation and editing. But their ability to capture the
nuances of human movement remains under-explored. Indeed the ability of these
models to faithfully model an array of text prompts can lead to a wide host of
applications in human and character animation. In this work, we take initial
steps to investigate whether these models can effectively guide the synthesis
of realistic human body animations. Specifically we propose to synthesize human
motion by deforming an SMPL-X body representation guided by Score distillation
sampling (SDS) calculated using a video diffusion model. By analyzing the
fidelity of the resulting animations, we gain insights into the extent to which
we can obtain motion using publicly available text-to-video diffusion models
using SDS. Our findings shed light on the potential and limitations of these
models for generating diverse and plausible human motions, paving the way for
further research in this exciting area.",2024-11-19 19:35:28+00:00,"['Paul Janson', 'Tiberiu Popa', 'Eugene Belilovsky']",http://arxiv.org/abs/2411.12831v1
V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning,"Video summarization aims to create short, accurate, and cohesive summaries of
longer videos. Despite the existence of various video summarization datasets, a
notable limitation is their limited amount of source videos, which hampers the
effective training of advanced large vision-language models (VLMs).
Additionally, most existing datasets are created for video-to-video
summarization, overlooking the contemporary need for multimodal video content
summarization. Recent efforts have been made to expand from unimodal to
multimodal video summarization, categorizing the task into three sub-tasks
based on the summary's modality: video-to-video (V2V), video-to-text (V2T), and
a combination of video and text summarization (V2VT). However, the textual
summaries in previous multimodal datasets are inadequate. To address these
issues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset
featuring 30,000 diverse videos sourced from YouTube, with lengths ranging from
40 to 940 seconds and an average summarization ratio of 16.39%. Each video
summary in Instruct-V2Xum is paired with a textual summary that references
specific frame indexes, facilitating the generation of aligned video and
textual summaries. In addition, we propose a new video summarization framework
named V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the
first framework that unifies different video summarization tasks into one large
language model's (LLM) text decoder and achieves task-controllable video
summarization with temporal prompts and task instructions. Experiments show
that V2Xum-LLaMA outperforms strong baseline models on multiple video
summarization tasks. Furthermore, we propose an enhanced evaluation metric for
V2V and V2VT summarization tasks.",2024-04-18 17:32:46+00:00,"['Hang Hua', 'Yunlong Tang', 'Chenliang Xu', 'Jiebo Luo']",http://arxiv.org/abs/2404.12353v2
GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model,"Autonomous driving training requires a diverse range of datasets encompassing
various traffic conditions, weather scenarios, and road types. Traditional data
augmentation methods often struggle to generate datasets that represent rare
occurrences. To address this challenge, we propose GenDDS, a novel approach for
generating driving scenarios generation by leveraging the capabilities of
Stable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology
involves the use of descriptive prompts to guide the synthesis process, aimed
at producing realistic and diverse driving scenarios. With the power of the
latest computer vision techniques, such as ControlNet and Hotshot-XL, we have
built a complete pipeline for video generation together with SDXL. We employ
the KITTI dataset, which includes real-world driving videos, to train the
model. Through a series of experiments, we demonstrate that our model can
generate high-quality driving videos that closely replicate the complexity and
variability of real-world driving scenarios. This research contributes to the
development of sophisticated training data for autonomous driving systems and
opens new avenues for creating virtual environments for simulation and
validation purposes.",2024-08-28 15:37:44+00:00,"['Yongjie Fu', 'Yunlong Li', 'Xuan Di']",http://arxiv.org/abs/2408.15868v1
EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions,"In this work, we tackle the challenge of enhancing the realism and
expressiveness in talking head video generation by focusing on the dynamic and
nuanced relationship between audio cues and facial movements. We identify the
limitations of traditional techniques that often fail to capture the full
spectrum of human expressions and the uniqueness of individual facial styles.
To address these issues, we propose EMO, a novel framework that utilizes a
direct audio-to-video synthesis approach, bypassing the need for intermediate
3D models or facial landmarks. Our method ensures seamless frame transitions
and consistent identity preservation throughout the video, resulting in highly
expressive and lifelike animations. Experimental results demonsrate that EMO is
able to produce not only convincing speaking videos but also singing videos in
various styles, significantly outperforming existing state-of-the-art
methodologies in terms of expressiveness and realism.",2024-02-27 13:10:11+00:00,"['Linrui Tian', 'Qi Wang', 'Bang Zhang', 'Liefeng Bo']",http://arxiv.org/abs/2402.17485v3
DIVD: Deblurring with Improved Video Diffusion Model,"Video deblurring presents a considerable challenge owing to the complexity of
blur, which frequently results from a combination of camera shakes, and object
motions. In the field of video deblurring, many previous works have primarily
concentrated on distortion-based metrics, such as PSNR. However, this approach
often results in a weak correlation with human perception and yields
reconstructions that lack realism. Diffusion models and video diffusion models
have respectively excelled in the fields of image and video generation,
particularly achieving remarkable results in terms of image authenticity and
realistic perception. However, due to the computational complexity and
challenges inherent in adapting diffusion models, there is still uncertainty
regarding the potential of video diffusion models in video deblurring tasks. To
explore the viability of video diffusion models in the task of video
deblurring, we introduce a diffusion model specifically for this purpose. In
this field, leveraging highly correlated information between adjacent frames
and addressing the challenge of temporal misalignment are crucial research
directions. To tackle these challenges, many improvements based on the video
diffusion model are introduced in this work. As a result, our model outperforms
existing models and achieves state-of-the-art results on a range of perceptual
metrics. Our model preserves a significant amount of detail in the images while
maintaining competitive distortion metrics. Furthermore, to the best of our
knowledge, this is the first time the diffusion model has been applied in video
deblurring to overcome the limitations mentioned above.",2024-12-01 11:39:02+00:00,"['Haoyang Long', 'Yan Wang', 'Wendong Wang']",http://arxiv.org/abs/2412.00773v1
Consistent Human Image and Video Generation with Spatially Conditioned Diffusion,"Consistent human-centric image and video synthesis aims to generate images or
videos with new poses while preserving appearance consistency with a given
reference image, which is crucial for low-cost visual content creation. Recent
advances based on diffusion models typically rely on separate networks for
reference appearance feature extraction and target visual generation, leading
to inconsistent domain gaps between references and targets. In this paper, we
frame the task as a spatially-conditioned inpainting problem, where the target
image is inpainted to maintain appearance consistency with the reference. This
approach enables the reference features to guide the generation of
pose-compliant targets within a unified denoising network, thereby mitigating
domain gaps. Additionally, to better maintain the reference appearance
information, we impose a causal feature interaction framework, in which
reference features can only query from themselves, while target features can
query appearance information from both the reference and the target. To further
enhance computational efficiency and flexibility, in practical implementation,
we decompose the spatially-conditioned generation process into two stages:
reference appearance extraction and conditioned target generation. Both stages
share a single denoising network, with interactions restricted to
self-attention layers. This proposed method ensures flexible control over the
appearance of generated human images and videos. By fine-tuning existing base
diffusion models on human video data, our method demonstrates strong
generalization to unseen human identities and poses without requiring
additional per-instance fine-tuning. Experimental results validate the
effectiveness of our approach, showing competitive performance compared to
existing methods for consistent human image and video synthesis.",2024-12-19 05:02:30+00:00,"['Mingdeng Cao', 'Chong Mou', 'Ziyang Yuan', 'Xintao Wang', 'Zhaoyang Zhang', 'Ying Shan', 'Yinqiang Zheng']",http://arxiv.org/abs/2412.14531v1
Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition,"Video diffusion models have recently made great progress in generation
quality, but are still limited by the high memory and computational
requirements. This is because current video diffusion models often attempt to
process high-dimensional videos directly. To tackle this issue, we propose
content-motion latent diffusion model (CMD), a novel efficient extension of
pretrained image diffusion models for video generation. Specifically, we
propose an autoencoder that succinctly encodes a video as a combination of a
content frame (like an image) and a low-dimensional motion latent
representation. The former represents the common content, and the latter
represents the underlying motion in the video, respectively. We generate the
content frame by fine-tuning a pretrained image diffusion model, and we
generate the motion latent representation by training a new lightweight
diffusion model. A key innovation here is the design of a compact latent space
that can directly utilizes a pretrained image diffusion model, which has not
been done in previous latent video diffusion models. This leads to considerably
better quality generation and reduced computational costs. For instance, CMD
can sample a video 7.7$\times$ faster than prior approaches by generating a
video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD
achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous
state-of-the-art of 292.4.",2024-03-21 05:48:48+00:00,"['Sihyun Yu', 'Weili Nie', 'De-An Huang', 'Boyi Li', 'Jinwoo Shin', 'Anima Anandkumar']",http://arxiv.org/abs/2403.14148v1
CV-VAE: A Compatible Video VAE for Latent Generative Video Models,"Spatio-temporal compression of videos, utilizing networks such as Variational
Autoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other
video generative models. For instance, many LLM-like video models learn the
distribution of discrete tokens derived from 3D VAEs within the VQVAE
framework, while most diffusion-based video models capture the distribution of
continuous latent extracted by 2D VAEs without quantization. The temporal
compression is simply realized by uniform frame sampling which results in
unsmooth motion between consecutive frames. Currently, there lacks of a
commonly used continuous video (3D) VAE for latent diffusion-based video models
in the research community. Moreover, since current diffusion-based approaches
are often implemented using pre-trained text-to-image (T2I) models, directly
training a video VAE without considering the compatibility with existing T2I
models will result in a latent space gap between them, which will take huge
computational resources for training to bridge the gap even with the T2I models
as initialization. To address this issue, we propose a method for training a
video VAE of latent video models, namely CV-VAE, whose latent space is
compatible with that of a given image VAE, e.g., image VAE of Stable Diffusion
(SD). The compatibility is achieved by the proposed novel latent space
regularization, which involves formulating a regularization loss using the
image VAE. Benefiting from the latent space compatibility, video models can be
trained seamlessly from pre-trained T2I or video models in a truly
spatio-temporally compressed latent space, rather than simply sampling video
frames at equal intervals. With our CV-VAE, existing video models can generate
four times more frames with minimal finetuning. Extensive experiments are
conducted to demonstrate the effectiveness of the proposed video VAE.",2024-05-30 17:33:10+00:00,"['Sijie Zhao', 'Yong Zhang', 'Xiaodong Cun', 'Shaoshu Yang', 'Muyao Niu', 'Xiaoyu Li', 'Wenbo Hu', 'Ying Shan']",http://arxiv.org/abs/2405.20279v2
InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models,"We introduce InVi, an approach for inserting or replacing objects within
videos (referred to as inpainting) using off-the-shelf, text-to-image latent
diffusion models. InVi targets controlled manipulation of objects and blending
them seamlessly into a background video unlike existing video editing methods
that focus on comprehensive re-styling or entire scene alterations. To achieve
this goal, we tackle two key challenges. Firstly, for high quality control and
blending, we employ a two-step process involving inpainting and matching. This
process begins with inserting the object into a single frame using a
ControlNet-based inpainting diffusion model, and then generating subsequent
frames conditioned on features from an inpainted frame as an anchor to minimize
the domain gap between the background and the object. Secondly, to ensure
temporal coherence, we replace the diffusion model's self-attention layers with
extended-attention layers. The anchor frame features serve as the keys and
values for these layers, enhancing consistency across frames. Our approach
removes the need for video-specific fine-tuning, presenting an efficient and
adaptable solution. Experimental results demonstrate that InVi achieves
realistic object insertion with consistent blending and coherence across
frames, outperforming existing methods.",2024-07-15 17:55:09+00:00,"['Nirat Saini', 'Navaneeth Bodla', 'Ashish Shrivastava', 'Avinash Ravichandran', 'Xiao Zhang', 'Abhinav Shrivastava', 'Bharat Singh']",http://arxiv.org/abs/2407.10958v1
Track the Answer: Extending TextVQA from Image to Video with Spatio-Temporal Clues,"Video text-based visual question answering (Video TextVQA) is a practical
task that aims to answer questions by jointly reasoning textual and visual
information in a given video. Inspired by the development of TextVQA in image
domain, existing Video TextVQA approaches leverage a language model (e.g. T5)
to process text-rich multiple frames and generate answers auto-regressively.
Nevertheless, the spatio-temporal relationships among visual entities
(including scene text and objects) will be disrupted and models are susceptible
to interference from unrelated information, resulting in irrational reasoning
and inaccurate answering. To tackle these challenges, we propose the TEA
(stands for ``\textbf{T}rack th\textbf{E} \textbf{A}nswer'') method that better
extends the generative TextVQA framework from image to video. TEA recovers the
spatio-temporal relationships in a complementary way and incorporates OCR-aware
clues to enhance the quality of reasoning questions. Extensive experiments on
several public Video TextVQA datasets validate the effectiveness and
generalization of our framework. TEA outperforms existing TextVQA methods,
video-language pretraining methods and video large language models by great
margins.",2024-12-17 03:06:12+00:00,"['Yan Zhang', 'Gangyan Zeng', 'Huawen Shen', 'Daiqing Wu', 'Yu Zhou', 'Can Ma']",http://arxiv.org/abs/2412.12502v1
JVID: Joint Video-Image Diffusion for Visual-Quality and Temporal-Consistency in Video Generation,"We introduce the Joint Video-Image Diffusion model (JVID), a novel approach
to generating high-quality and temporally coherent videos. We achieve this by
integrating two diffusion models: a Latent Image Diffusion Model (LIDM) trained
on images and a Latent Video Diffusion Model (LVDM) trained on video data. Our
method combines these models in the reverse diffusion process, where the LIDM
enhances image quality and the LVDM ensures temporal consistency. This unique
combination allows us to effectively handle the complex spatio-temporal
dynamics in video generation. Our results demonstrate quantitative and
qualitative improvements in producing realistic and coherent videos.",2024-09-21 13:59:50+00:00,"['Hadrien Reynaud', 'Matthew Baugh', 'Mischa Dombrowski', 'Sarah Cechnicka', 'Qingjie Meng', 'Bernhard Kainz']",http://arxiv.org/abs/2409.14149v2
Frame by Familiar Frame: Understanding Replication in Video Diffusion Models,"Building on the momentum of image generation diffusion models, there is an
increasing interest in video-based diffusion models. However, video generation
poses greater challenges due to its higher-dimensional nature, the scarcity of
training data, and the complex spatiotemporal relationships involved. Image
generation models, due to their extensive data requirements, have already
strained computational resources to their limits. There have been instances of
these models reproducing elements from the training samples, leading to
concerns and even legal disputes over sample replication. Video diffusion
models, which operate with even more constrained datasets and are tasked with
generating both spatial and temporal content, may be more prone to replicating
samples from their training sets. Compounding the issue, these models are often
evaluated using metrics that inadvertently reward replication. In our paper, we
present a systematic investigation into the phenomenon of sample replication in
video diffusion models. We scrutinize various recent diffusion models for video
synthesis, assessing their tendency to replicate spatial and temporal content
in both unconditional and conditional generation scenarios. Our study
identifies strategies that are less likely to lead to replication. Furthermore,
we propose new evaluation strategies that take replication into account,
offering a more accurate measure of a model's ability to generate the original
content.",2024-03-28 17:15:23+00:00,"['Aimon Rahman', 'Malsha V. Perera', 'Vishal M. Patel']",http://arxiv.org/abs/2403.19593v2
Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation,"We introduce Presto, a novel video diffusion model designed to generate
15-second videos with long-range coherence and rich content. Extending video
generation methods to maintain scenario diversity over long durations presents
significant challenges. To address this, we propose a Segmented Cross-Attention
(SCA) strategy, which splits hidden states into segments along the temporal
dimension, allowing each segment to cross-attend to a corresponding
sub-caption. SCA requires no additional parameters, enabling seamless
incorporation into current DiT-based architectures. To facilitate high-quality
long video generation, we build the LongTake-HD dataset, consisting of 261k
content-rich videos with scenario coherence, annotated with an overall video
caption and five progressive sub-captions. Experiments show that our Presto
achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree,
outperforming existing state-of-the-art video generation methods. This
demonstrates that our proposed Presto significantly enhances content richness,
maintains long-range coherence, and captures intricate textual details. More
details are displayed on our project page: https://presto-video.github.io/.",2024-12-02 09:32:36+00:00,"['Xin Yan', 'Yuxuan Cai', 'Qiuyue Wang', 'Yuan Zhou', 'Wenhao Huang', 'Huan Yang']",http://arxiv.org/abs/2412.01316v1
VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models,"The arrival of Sora marks a new era for text-to-video diffusion models,
bringing significant advancements in video generation and potential
applications. However, Sora, along with other text-to-video diffusion models,
is highly reliant on prompts, and there is no publicly available dataset that
features a study of text-to-video prompts. In this paper, we introduce VidProM,
the first large-scale dataset comprising 1.67 Million unique text-to-Video
Prompts from real users. Additionally, this dataset includes 6.69 million
videos generated by four state-of-the-art diffusion models, alongside some
related data. We initially discuss the curation of this large-scale dataset, a
process that is both time-consuming and costly. Subsequently, we underscore the
need for a new prompt dataset specifically designed for text-to-video
generation by illustrating how VidProM differs from DiffusionDB, a large-scale
prompt-gallery dataset for image generation. Our extensive and diverse dataset
also opens up many exciting new research areas. For instance, we suggest
exploring text-to-video prompt engineering, efficient video generation, and
video copy detection for diffusion models to develop better, more efficient,
and safer models. The project (including the collected dataset VidProM and
related code) is publicly available at https://vidprom.github.io under the
CC-BY-NC 4.0 License.",2024-03-10 05:40:12+00:00,"['Wenhao Wang', 'Yi Yang']",http://arxiv.org/abs/2403.06098v4
Extreme Video Compression with Pre-trained Diffusion Models,"Diffusion models have achieved remarkable success in generating high quality
image and video data. More recently, they have also been used for image
compression with high perceptual quality. In this paper, we present a novel
approach to extreme video compression leveraging the predictive power of
diffusion-based generative models at the decoder. The conditional diffusion
model takes several neural compressed frames and generates subsequent frames.
When the reconstruction quality drops below the desired level, new frames are
encoded to restart prediction. The entire video is sequentially encoded to
achieve a visually pleasing reconstruction, considering perceptual quality
metrics such as the learned perceptual image patch similarity (LPIPS) and the
Frechet video distance (FVD), at bit rates as low as 0.02 bits per pixel (bpp).
Experimental results demonstrate the effectiveness of the proposed scheme
compared to standard codecs such as H.264 and H.265 in the low bpp regime. The
results showcase the potential of exploiting the temporal relations in video
data using generative models. Code is available at:
https://github.com/ElesionKyrie/Extreme-Video-Compression-With-Prediction-Using-Pre-trainded-Diffusion-Models-",2024-02-14 04:23:05+00:00,"['Bohan Li', 'Yiming Liu', 'Xueyan Niu', 'Bo Bai', 'Lei Deng', 'Deniz Gndz']",http://arxiv.org/abs/2402.08934v1
Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models,"Image customization has been extensively studied in text-to-image (T2I)
diffusion models, leading to impressive outcomes and applications. With the
emergence of text-to-video (T2V) diffusion models, its temporal counterpart,
motion customization, has not yet been well investigated. To address the
challenge of one-shot video motion customization, we propose Customize-A-Video
that models the motion from a single reference video and adapts it to new
subjects and scenes with both spatial and temporal varieties. It leverages
low-rank adaptation (LoRA) on temporal attention layers to tailor the
pre-trained T2V diffusion model for specific motion modeling. To disentangle
the spatial and temporal information during training, we introduce a novel
concept of appearance absorbers that detach the original appearance from the
reference video prior to motion learning. The proposed modules are trained in a
staged pipeline and inferred in a plug-and-play fashion, enabling easy
extensions to various downstream tasks such as custom video generation and
editing, video appearance customization and multiple motion combination. Our
project page can be found at https://customize-a-video.github.io.",2024-02-22 18:38:48+00:00,"['Yixuan Ren', 'Yang Zhou', 'Jimei Yang', 'Jing Shi', 'Difan Liu', 'Feng Liu', 'Mingi Kwon', 'Abhinav Shrivastava']",http://arxiv.org/abs/2402.14780v3
Grid Diffusion Models for Text-to-Video Generation,"Recent advances in the diffusion models have significantly improved
text-to-image generation. However, generating videos from text is a more
challenging task than generating images from text, due to the much larger
dataset and higher computational cost required. Most existing video generation
methods use either a 3D U-Net architecture that considers the temporal
dimension or autoregressive generation. These methods require large datasets
and are limited in terms of computational costs compared to text-to-image
generation. To tackle these challenges, we propose a simple but effective novel
grid diffusion for text-to-video generation without temporal dimension in
architecture and a large text-video paired dataset. We can generate a
high-quality video using a fixed amount of GPU memory regardless of the number
of frames by representing the video as a grid image. Additionally, since our
method reduces the dimensions of the video to the dimensions of the image,
various image-based methods can be applied to videos, such as text-guided video
manipulation from image manipulation. Our proposed method outperforms the
existing methods in both quantitative and qualitative evaluations,
demonstrating the suitability of our model for real-world video generation.",2024-03-30 03:50:43+00:00,"['Taegyeong Lee', 'Soyeong Kwon', 'Taehwan Kim']",http://arxiv.org/abs/2404.00234v2
Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models,"Using image models naively for solving inverse video problems often suffers
from flickering, texture-sticking, and temporal inconsistency in generated
videos. To tackle these problems, in this paper, we view frames as continuous
functions in the 2D space, and videos as a sequence of continuous warping
transformations between different frames. This perspective allows us to train
function space diffusion models only on images and utilize them to solve
temporally correlated inverse problems. The function space diffusion models
need to be equivariant with respect to the underlying spatial transformations.
To ensure temporal consistency, we introduce a simple post-hoc test-time
guidance towards (self)-equivariant solutions. Our method allows us to deploy
state-of-the-art latent diffusion models such as Stable Diffusion XL to solve
video inverse problems. We demonstrate the effectiveness of our method for
video inpainting and $8\times$ video super-resolution, outperforming existing
techniques based on noise transformations. We provide generated video results:
https://giannisdaras.github.io/warped_diffusion.github.io/.",2024-10-21 16:19:34+00:00,"['Giannis Daras', 'Weili Nie', 'Karsten Kreis', 'Alex Dimakis', 'Morteza Mardani', 'Nikola Borislavov Kovachki', 'Arash Vahdat']",http://arxiv.org/abs/2410.16152v2
Towards Multi-Task Multi-Modal Models: A Video Generative Perspective,"Advancements in language foundation models have primarily fueled the recent
surge in artificial intelligence. In contrast, generative learning of
non-textual modalities, especially videos, significantly trails behind language
modeling. This thesis chronicles our endeavor to build multi-task models for
generating videos and other modalities under diverse conditions, as well as for
understanding and compression applications. Given the high dimensionality of
visual data, we pursue concise and accurate latent representations. Our
video-native spatial-temporal tokenizers preserve high fidelity. We unveil a
novel approach to mapping bidirectionally between visual observation and
interpretable lexical terms. Furthermore, our scalable visual token
representation proves beneficial across generation, compression, and
understanding tasks. This achievement marks the first instances of language
models surpassing diffusion models in visual synthesis and a video tokenizer
outperforming industry-standard codecs. Within these multi-modal latent spaces,
we study the design of multi-task generative models. Our masked multi-task
transformer excels at the quality, efficiency, and flexibility of video
generation. We enable a frozen language model, trained solely on text, to
generate visual content. Finally, we build a scalable generative multi-modal
transformer trained from scratch, enabling the generation of videos containing
high-fidelity motion with the corresponding audio given diverse conditions.
Throughout the course, we have shown the effectiveness of integrating multiple
tasks, crafting high-fidelity latent representation, and generating multiple
modalities. This work suggests intriguing potential for future exploration in
generating non-textual data and enabling real-time, interactive experiences
across various media forms.",2024-05-26 23:56:45+00:00,['Lijun Yu'],http://arxiv.org/abs/2405.16728v1
Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual Inversion,"Recent years have seen a tremendous improvement in the quality of video
generation and editing approaches. While several techniques focus on editing
appearance, few address motion. Current approaches using text, trajectories, or
bounding boxes are limited to simple motions, so we specify motions with a
single motion reference video instead. We further propose to use a pre-trained
image-to-video model rather than a text-to-video model. This approach allows us
to preserve the exact appearance and position of a target object or scene and
helps disentangle appearance from motion. Our method, called motion-textual
inversion, leverages our observation that image-to-video models extract
appearance mainly from the (latent) image input, while the text/image embedding
injected via cross-attention predominantly controls motion. We thus represent
motion using text/image embedding tokens. By operating on an inflated
motion-text embedding containing multiple text/image embedding tokens per
frame, we achieve a high temporal motion granularity. Once optimized on the
motion reference video, this embedding can be applied to various target images
to generate videos with semantically similar motions. Our approach does not
require spatial alignment between the motion reference video and target image,
generalizes across various domains, and can be applied to various tasks such as
full-body and face reenactment, as well as controlling the motion of inanimate
objects and the camera. We empirically demonstrate the effectiveness of our
method in the semantic video motion transfer task, significantly outperforming
existing methods in this context.",2024-08-01 10:55:20+00:00,"['Manuel Kansy', 'Jacek Naruniec', 'Christopher Schroers', 'Markus Gross', 'Romann M. Weber']",http://arxiv.org/abs/2408.00458v1
Generative Video Diffusion for Unseen Novel Semantic Video Moment Retrieval,"Video moment retrieval (VMR) aims to locate the most likely video moment(s)
corresponding to a text query in untrimmed videos. Training of existing methods
is limited by the lack of diverse and generalisable VMR datasets, hindering
their ability to generalise moment-text associations to queries containing
novel semantic concepts (unseen both visually and textually in a training
source domain). For model generalisation to novel semantics, existing methods
rely heavily on assuming to have access to both video and text sentence pairs
from a target domain in addition to the source domain pair-wise training data.
This is neither practical nor scalable. In this work, we introduce a more
generalisable approach by assuming only text sentences describing new semantics
are available in model training without having seen any videos from a target
domain. To that end, we propose a Fine-grained Video Editing framework, termed
FVE, that explores generative video diffusion to facilitate fine-grained video
editing from the seen source concepts to the unseen target sentences consisting
of new concepts. This enables generative hypotheses of unseen video moments
corresponding to the novel concepts in the target domain. This fine-grained
generative video diffusion retains the original video structure and subject
specifics from the source domain while introducing semantic distinctions of
unseen novel vocabularies in the target domain. A critical challenge is how to
enable this generative fine-grained diffusion process to be meaningful in
optimising VMR, more than just synthesising visually pleasing videos. We solve
this problem by introducing a hybrid selection mechanism that integrates three
quantitative metrics to selectively incorporate synthetic video moments (novel
video hypotheses) as enlarged additions to the original source training data,
whilst minimising potential ...",2024-01-24 09:45:40+00:00,"['Dezhao Luo', 'Shaogang Gong', 'Jiabo Huang', 'Hailin Jin', 'Yang Liu']",http://arxiv.org/abs/2401.13329v3
Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition,"Building upon the impressive success of CLIP (Contrastive Language-Image
Pretraining), recent pioneer works have proposed to adapt the powerful CLIP to
video data, leading to efficient and effective video learners for
open-vocabulary action recognition. Inspired by that humans perform actions in
diverse environments, our work delves into an intriguing question: Can
CLIP-based video learners effectively generalize to video domains they have not
encountered during training? To answer this, we establish a CROSS-domain
Open-Vocabulary Action recognition benchmark named XOV-Action, and conduct a
comprehensive evaluation of five state-of-the-art CLIP-based video learners
under various types of domain gaps. The evaluation demonstrates that previous
methods exhibit limited action recognition performance in unseen video domains,
revealing potential challenges of the cross-domain open-vocabulary action
recognition task. In this paper, we focus on one critical challenge of the
task, namely scene bias, and accordingly contribute a novel scene-aware
video-text alignment method. Our key idea is to distinguish video
representations apart from scene-encoded text representations, aiming to learn
scene-agnostic video representations for recognizing actions across domains.
Extensive experiments demonstrate the effectiveness of our method. The
benchmark and code will be available at
https://github.com/KunyuLin/XOV-Action/.",2024-03-03 16:48:16+00:00,"['Kun-Yu Lin', 'Henghui Ding', 'Jiaming Zhou', 'Yu-Ming Tang', 'Yi-Xing Peng', 'Zhilin Zhao', 'Chen Change Loy', 'Wei-Shi Zheng']",http://arxiv.org/abs/2403.01560v2
Optical Flow Representation Alignment Mamba Diffusion Model for Medical Video Generation,"Medical video generation models are expected to have a profound impact on the
healthcare industry, including but not limited to medical education and
training, surgical planning, and simulation. Current video diffusion models
typically build on image diffusion architecture by incorporating temporal
operations (such as 3D convolution and temporal attention). Although this
approach is effective, its oversimplification limits spatio-temporal
performance and consumes substantial computational resources. To counter this,
we propose Medical Simulation Video Generator (MedSora), which incorporates
three key elements: i) a video diffusion framework integrates the advantages of
attention and Mamba, balancing low computational load with high-quality video
generation, ii) an optical flow representation alignment method that implicitly
enhances attention to inter-frame pixels, and iii) a video variational
autoencoder (VAE) with frequency compensation addresses the information loss of
medical features that occurs when transforming pixel space into latent features
and then back to pixel frames. Extensive experiments and applications
demonstrate that MedSora exhibits superior visual quality in generating medical
videos, outperforming the most advanced baseline methods. Further results and
code are available at https://wongzbb.github.io/MedSora",2024-11-03 17:57:00+00:00,"['Zhenbin Wang', 'Lei Zhang', 'Lituan Wang', 'Minjuan Zhu', 'Zhenwei Zhang']",http://arxiv.org/abs/2411.01647v1
Controllable Longer Image Animation with Diffusion Models,"Generating realistic animated videos from static images is an important area
of research in computer vision. Methods based on physical simulation and motion
prediction have achieved notable advances, but they are often limited to
specific object textures and motion trajectories, failing to exhibit highly
complex environments and physical dynamics. In this paper, we introduce an
open-domain controllable image animation method using motion priors with video
diffusion models. Our method achieves precise control over the direction and
speed of motion in the movable region by extracting the motion field
information from videos and learning moving trajectories and strengths. Current
pretrained video generation models are typically limited to producing very
short videos, typically less than 30 frames. In contrast, we propose an
efficient long-duration video generation method based on noise reschedule
specifically tailored for image animation tasks, facilitating the creation of
videos over 100 frames in length while maintaining consistency in content
scenery and motion coordination. Specifically, we decompose the denoise process
into two distinct phases: the shaping of scene contours and the refining of
motion details. Then we reschedule the noise to control the generated frame
sequences maintaining long-distance noise correlation. We conducted extensive
experiments with 10 baselines, encompassing both commercial tools and academic
methodologies, which demonstrate the superiority of our method. Our project
page: https://wangqiang9.github.io/Controllable.github.io/",2024-05-27 16:08:00+00:00,"['Qiang Wang', 'Minghua Liu', 'Junjun Hu', 'Fan Jiang', 'Mu Xu']",http://arxiv.org/abs/2405.17306v2
TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation,"Video generation has many unique challenges beyond those of image generation.
The temporal dimension introduces extensive possible variations across frames,
over which consistency and continuity may be violated. In this study, we move
beyond evaluating simple actions and argue that generated videos should
incorporate the emergence of new concepts and their relation transitions like
in real-world videos as time progresses. To assess the Temporal
Compositionality of video generation models, we propose TC-Bench, a benchmark
of meticulously crafted text prompts, corresponding ground truth videos, and
robust evaluation metrics. The prompts articulate the initial and final states
of scenes, effectively reducing ambiguities for frame development and
simplifying the assessment of transition completion. In addition, by collecting
aligned real-world videos corresponding to the prompts, we expand TC-Bench's
applicability from text-conditional models to image-conditional ones that can
perform generative frame interpolation. We also develop new metrics to measure
the completeness of component transitions in generated videos, which
demonstrate significantly higher correlations with human judgments than
existing metrics. Our comprehensive experimental results reveal that most video
generators achieve less than 20% of the compositional changes, highlighting
enormous space for future improvement. Our analysis indicates that current
video generation models struggle to interpret descriptions of compositional
changes and synthesize various components across different time steps.",2024-06-12 21:41:32+00:00,"['Weixi Feng', 'Jiachen Li', 'Michael Saxon', 'Tsu-jui Fu', 'Wenhu Chen', 'William Yang Wang']",http://arxiv.org/abs/2406.08656v1
FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing,"Diffusion models have demonstrated remarkable capabilities in text-to-image
and text-to-video generation, opening up possibilities for video editing based
on textual input. However, the computational cost associated with sequential
sampling in diffusion models poses challenges for efficient video editing.
Existing approaches relying on image generation models for video editing suffer
from time-consuming one-shot fine-tuning, additional condition extraction, or
DDIM inversion, making real-time applications impractical. In this work, we
propose FastVideoEdit, an efficient zero-shot video editing approach inspired
by Consistency Models (CMs). By leveraging the self-consistency property of
CMs, we eliminate the need for time-consuming inversion or additional condition
extraction, reducing editing time. Our method enables direct mapping from
source video to target video with strong preservation ability utilizing a
special variance schedule. This results in improved speed advantages, as fewer
sampling steps can be used while maintaining comparable generation quality.
Experimental results validate the state-of-the-art performance and speed
advantages of FastVideoEdit across evaluation metrics encompassing editing
speed, temporal consistency, and text-video alignment.",2024-03-10 17:12:01+00:00,"['Youyuan Zhang', 'Xuan Ju', 'James J. Clark']",http://arxiv.org/abs/2403.06269v2
Towards Retrieval Augmented Generation over Large Video Libraries,"Video content creators need efficient tools to repurpose content, a task that
often requires complex manual or automated searches. Crafting a new video from
large video libraries remains a challenge. In this paper we introduce the task
of Video Library Question Answering (VLQA) through an interoperable
architecture that applies Retrieval Augmented Generation (RAG) to video
libraries. We propose a system that uses large language models (LLMs) to
generate search queries, retrieving relevant video moments indexed by speech
and visual metadata. An answer generation module then integrates user queries
with this metadata to produce responses with specific video timestamps. This
approach shows promise in multimedia content retrieval, and AI-assisted video
content creation.",2024-06-21 07:52:01+00:00,"['Yannis Tevissen', 'Khalil Guetari', 'Frdric Petitpont']",http://arxiv.org/abs/2406.14938v1
Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data,"Text-to-video (T2V) generation has gained significant attention due to its
wide applications to video generation, editing, enhancement and translation,
\etc. However, high-quality (HQ) video synthesis is extremely challenging
because of the diverse and complex motions existed in real world. Most existing
works struggle to address this problem by collecting large-scale HQ videos,
which are inaccessible to the community. In this work, we show that publicly
available limited and low-quality (LQ) data are sufficient to train a HQ video
generator without recaptioning or finetuning. We factorize the whole T2V
generation process into two steps: generating an image conditioned on a highly
descriptive caption, and synthesizing the video conditioned on the generated
image and a concise caption of motion details. Specifically, we present
\emph{Factorized-Dreamer}, a factorized spatiotemporal framework with several
critical designs for T2V generation, including an adapter to combine text and
image embeddings, a pixel-aware cross attention module to capture pixel-level
image information, a T5 text encoder to better understand motion description,
and a PredictNet to supervise optical flows. We further present a noise
schedule, which plays a key role in ensuring the quality and stability of video
generation. Our model lowers the requirements in detailed captions and HQ
videos, and can be directly trained on limited LQ datasets with noisy and brief
captions such as WebVid-10M, largely alleviating the cost to collect
large-scale HQ video-text pairs. Extensive experiments in a variety of T2V and
image-to-video generation tasks demonstrate the effectiveness of our proposed
Factorized-Dreamer. Our source codes are available at
\url{https://github.com/yangxy/Factorized-Dreamer/}.",2024-08-19 16:08:00+00:00,"['Tao Yang', 'Yangming Shi', 'Yunwen Huang', 'Feng Chen', 'Yin Zheng', 'Lei Zhang']",http://arxiv.org/abs/2408.10119v1
DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis,"Audio-driven talking head synthesis strives to generate lifelike video
portraits from provided audio. The diffusion model, recognized for its superior
quality and robust generalization, has been explored for this task. However,
establishing a robust correspondence between temporal audio cues and
corresponding spatial facial expressions with diffusion models remains a
significant challenge in talking head generation. To bridge this gap, we
present DreamHead, a hierarchical diffusion framework that learns
spatial-temporal correspondences in talking head synthesis without compromising
the model's intrinsic quality and adaptability.~DreamHead learns to predict
dense facial landmarks from audios as intermediate signals to model the spatial
and temporal correspondences.~Specifically, a first hierarchy of
audio-to-landmark diffusion is first designed to predict temporally smooth and
accurate landmark sequences given audio sequence signals. Then, a second
hierarchy of landmark-to-image diffusion is further proposed to produce
spatially consistent facial portrait videos, by modeling spatial
correspondences between the dense facial landmark and appearance. Extensive
experiments show that proposed DreamHead can effectively learn spatial-temporal
consistency with the designed hierarchical diffusion and produce high-fidelity
audio-driven talking head videos for multiple identities.",2024-09-16 13:44:20+00:00,"['Fa-Ting Hong', 'Yunfei Liu', 'Yu Li', 'Changyin Zhou', 'Fei Yu', 'Dan Xu']",http://arxiv.org/abs/2409.10281v1
LVMark: Robust Watermark for latent video diffusion models,"Rapid advancements in generative models have made it possible to create
hyper-realistic videos. As their applicability increases, their unauthorized
use has raised significant concerns, leading to the growing demand for
techniques to protect the ownership of the generative model itself. While
existing watermarking methods effectively embed watermarks into
image-generative models, they fail to account for temporal information,
resulting in poor performance when applied to video-generative models. To
address this issue, we introduce a novel watermarking method called LVMark,
which embeds watermarks into video diffusion models. A key component of LVMark
is a selective weight modulation strategy that efficiently embeds watermark
messages into the video diffusion model while preserving the quality of the
generated videos. To accurately decode messages in the presence of malicious
attacks, we design a watermark decoder that leverages spatio-temporal
information in the 3D wavelet domain through a cross-attention module. To the
best of our knowledge, our approach is the first to highlight the potential of
video-generative model watermarking as a valuable tool for enhancing the
effectiveness of ownership protection in video-generative models.",2024-12-12 09:57:20+00:00,"['MinHyuk Jang', 'Youngdong Jang', 'JaeHyeok Lee', 'Kodai Kawamura', 'Feng Yang', 'Sangpil Kim']",http://arxiv.org/abs/2412.09122v2
Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction,"Efficient tokenization of videos remains a challenge in training vision
models that can process long videos. One promising direction is to develop a
tokenizer that can encode long video clips, as it would enable the tokenizer to
leverage the temporal coherence of videos better for tokenization. However,
training existing tokenizers on long videos often incurs a huge training cost
as they are trained to reconstruct all the frames at once. In this paper, we
introduce CoordTok, a video tokenizer that learns a mapping from
coordinate-based representations to the corresponding patches of input videos,
inspired by recent advances in 3D generative models. In particular, CoordTok
encodes a video into factorized triplane representations and reconstructs
patches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows
for training large tokenizer models directly on long videos without requiring
excessive training resources. Our experiments show that CoordTok can
drastically reduce the number of tokens for encoding long video clips. For
instance, CoordTok can encode a 128-frame video with 128$\times$128 resolution
into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar
reconstruction quality. We further show that this efficient video tokenization
enables memory-efficient training of a diffusion transformer that can generate
128 frames at once.",2024-11-22 06:50:44+00:00,"['Huiwon Jang', 'Sihyun Yu', 'Jinwoo Shin', 'Pieter Abbeel', 'Younggyo Seo']",http://arxiv.org/abs/2411.14762v3
DirectorLLM for Human-Centric Video Generation,"In this paper, we introduce DirectorLLM, a novel video generation model that
employs a large language model (LLM) to orchestrate human poses within videos.
As foundational text-to-video models rapidly evolve, the demand for
high-quality human motion and interaction grows. To address this need and
enhance the authenticity of human motions, we extend the LLM from a text
generator to a video director and human motion simulator. Utilizing open-source
resources from Llama 3, we train the DirectorLLM to generate detailed
instructional signals, such as human poses, to guide video generation. This
approach offloads the simulation of human motion from the video generator to
the LLM, effectively creating informative outlines for human-centric scenes.
These signals are used as conditions by the video renderer, facilitating more
realistic and prompt-following video generation. As an independent LLM module,
it can be applied to different video renderers, including UNet and DiT, with
minimal effort. Experiments on automatic evaluation benchmarks and human
evaluations show that our model outperforms existing ones in generating videos
with higher human motion fidelity, improved prompt faithfulness, and enhanced
rendered subject naturalness.",2024-12-19 03:10:26+00:00,"['Kunpeng Song', 'Tingbo Hou', 'Zecheng He', 'Haoyu Ma', 'Jialiang Wang', 'Animesh Sinha', 'Sam Tsai', 'Yaqiao Luo', 'Xiaoliang Dai', 'Li Chen', 'Xide Xia', 'Peizhao Zhang', 'Peter Vajda', 'Ahmed Elgammal', 'Felix Juefei-Xu']",http://arxiv.org/abs/2412.14484v1
Vivid-ZOO: Multi-View Video Generation with Diffusion Model,"While diffusion models have shown impressive performance in 2D image/video
generation, diffusion-based Text-to-Multi-view-Video (T2MVid) generation
remains underexplored. The new challenges posed by T2MVid generation lie in the
lack of massive captioned multi-view videos and the complexity of modeling such
multi-dimensional distribution. To this end, we propose a novel diffusion-based
pipeline that generates high-quality multi-view videos centered around a
dynamic 3D object from text. Specifically, we factor the T2MVid problem into
viewpoint-space and time components. Such factorization allows us to combine
and reuse layers of advanced pre-trained multi-view image and 2D video
diffusion models to ensure multi-view consistency as well as temporal coherence
for the generated multi-view videos, largely reducing the training cost. We
further introduce alignment modules to align the latent spaces of layers from
the pre-trained multi-view and the 2D video diffusion models, addressing the
reused layers' incompatibility that arises from the domain gap between 2D and
multi-view data. In support of this and future research, we further contribute
a captioned multi-view video dataset. Experimental results demonstrate that our
method generates high-quality multi-view videos, exhibiting vivid motions,
temporal coherence, and multi-view consistency, given a variety of text
prompts.",2024-06-12 21:44:04+00:00,"['Bing Li', 'Cheng Zheng', 'Wenxuan Zhu', 'Jinjie Mai', 'Biao Zhang', 'Peter Wonka', 'Bernard Ghanem']",http://arxiv.org/abs/2406.08659v1
Synchronized Video Storytelling: Generating Video Narrations with Structured Storyline,"Video storytelling is engaging multimedia content that utilizes video and its
accompanying narration to attract the audience, where a key challenge is
creating narrations for recorded visual scenes. Previous studies on dense video
captioning and video story generation have made some progress. However, in
practical applications, we typically require synchronized narrations for
ongoing visual scenes. In this work, we introduce a new task of Synchronized
Video Storytelling, which aims to generate synchronous and informative
narrations for videos. These narrations, associated with each video clip,
should relate to the visual content, integrate relevant knowledge, and have an
appropriate word count corresponding to the clip's duration. Specifically, a
structured storyline is beneficial to guide the generation process, ensuring
coherence and integrity. To support the exploration of this task, we introduce
a new benchmark dataset E-SyncVidStory with rich annotations. Since existing
Multimodal LLMs are not effective in addressing this task in one-shot or
few-shot settings, we propose a framework named VideoNarrator that can generate
a storyline for input videos and simultaneously generate narrations with the
guidance of the generated or predefined storyline. We further introduce a set
of evaluation metrics to thoroughly assess the generation. Both automatic and
human evaluations validate the effectiveness of our approach. Our dataset,
codes, and evaluations will be released.",2024-05-22 22:22:26+00:00,"['Dingyi Yang', 'Chunru Zhan', 'Ziheng Wang', 'Biao Wang', 'Tiezheng Ge', 'Bo Zheng', 'Qin Jin']",http://arxiv.org/abs/2405.14040v2
Video Diffusion Alignment via Reward Gradients,"We have made significant progress towards building foundational video
diffusion models. As these models are trained using large-scale unsupervised
data, it has become crucial to adapt these models to specific downstream tasks.
Adapting these models via supervised fine-tuning requires collecting target
datasets of videos, which is challenging and tedious. In this work, we utilize
pre-trained reward models that are learned via preferences on top of powerful
vision discriminative models to adapt video diffusion models. These models
contain dense gradient information with respect to generated RGB pixels, which
is critical to efficient learning in complex search spaces, such as videos. We
show that backpropagating gradients from these reward models to a video
diffusion model can allow for compute and sample efficient alignment of the
video diffusion model. We show results across a variety of reward models and
video diffusion models, demonstrating that our approach can learn much more
efficiently in terms of reward queries and computation than prior gradient-free
approaches. Our code, model weights,and more visualization are available at
https://vader-vid.github.io.",2024-07-11 17:59:45+00:00,"['Mihir Prabhudesai', 'Russell Mendonca', 'Zheyang Qin', 'Katerina Fragkiadaki', 'Deepak Pathak']",http://arxiv.org/abs/2407.08737v1
GenLit: Reformulating Single-Image Relighting as Video Generation,"Manipulating the illumination within a single image represents a fundamental
challenge in computer vision and graphics. This problem has been traditionally
addressed using inverse rendering techniques, which require explicit 3D asset
reconstruction and costly ray tracing simulations. Meanwhile, recent
advancements in visual foundation models suggest that a new paradigm could soon
be practical and possible -- one that replaces explicit physical models with
networks that are trained on massive amounts of image and video data. In this
paper, we explore the potential of exploiting video diffusion models, and in
particular Stable Video Diffusion (SVD), in understanding the physical world to
perform relighting tasks given a single image. Specifically, we introduce
GenLit, a framework that distills the ability of a graphics engine to perform
light manipulation into a video generation model, enabling users to directly
insert and manipulate a point light in the 3D world within a given image and
generate the results directly as a video sequence. We find that a model
fine-tuned on only a small synthetic dataset (270 objects) is able to
generalize to real images, enabling single-image relighting with realistic ray
tracing effects and cast shadows. These results reveal the ability of video
foundation models to capture rich information about lighting, material, and
shape. Our findings suggest that such models, with minimal training, can be
used for physically-based rendering without explicit physically asset
reconstruction and complex ray tracing. This further suggests the potential of
such models for controllable and physically accurate image synthesis tasks.",2024-12-15 15:40:40+00:00,"['Shrisha Bharadwaj', 'Haiwen Feng', 'Victoria Abrevaya', 'Michael J. Black']",http://arxiv.org/abs/2412.11224v1
Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Video Diffusion Transformer,"Existing methodologies for animating portrait images face significant
challenges, particularly in handling non-frontal perspectives, rendering
dynamic objects around the portrait, and generating immersive, realistic
backgrounds. In this paper, we introduce the first application of a pretrained
transformer-based video generative model that demonstrates strong
generalization capabilities and generates highly dynamic, realistic videos for
portrait animation, effectively addressing these challenges. The adoption of a
new video backbone model makes previous U-Net-based methods for identity
maintenance, audio conditioning, and video extrapolation inapplicable. To
address this limitation, we design an identity reference network consisting of
a causal 3D VAE combined with a stacked series of transformer layers, ensuring
consistent facial identity across video sequences. Additionally, we investigate
various speech audio conditioning and motion frame mechanisms to enable the
generation of continuous video driven by speech audio. Our method is validated
through experiments on benchmark and newly proposed wild datasets,
demonstrating substantial improvements over prior methods in generating
realistic portraits characterized by diverse orientations within dynamic and
immersive scenes. Further visualizations and the source code are available at:
https://fudan-generative-vision.github.io/hallo3/.",2024-12-01 08:54:30+00:00,"['Jiahao Cui', 'Hui Li', 'Yun Zhan', 'Hanlin Shang', 'Kaihui Cheng', 'Yuqi Ma', 'Shan Mu', 'Hang Zhou', 'Jingdong Wang', 'Siyu Zhu']",http://arxiv.org/abs/2412.00733v4
Anchored Diffusion for Video Face Reenactment,"Video generation has drawn significant interest recently, pushing the
development of large-scale models capable of producing realistic videos with
coherent motion. Due to memory constraints, these models typically generate
short video segments that are then combined into long videos. The merging
process poses a significant challenge, as it requires ensuring smooth
transitions and overall consistency. In this paper, we introduce Anchored
Diffusion, a novel method for synthesizing relatively long and seamless videos.
We extend Diffusion Transformers (DiTs) to incorporate temporal information,
creating our sequence-DiT (sDiT) model for generating short video segments.
Unlike previous works, we train our model on video sequences with random
non-uniform temporal spacing and incorporate temporal information via external
guidance, increasing flexibility and allowing it to capture both short and
long-term relationships. Furthermore, during inference, we leverage the
transformer architecture to modify the diffusion process, generating a batch of
non-uniform sequences anchored to a common frame, ensuring consistency
regardless of temporal distance. To demonstrate our method, we focus on face
reenactment, the task of creating a video from a source image that replicates
the facial expressions and movements from a driving video. Through
comprehensive experiments, we show our approach outperforms current techniques
in producing longer consistent high-quality videos while offering editing
capabilities.",2024-07-21 13:14:17+00:00,"['Idan Kligvasser', 'Regev Cohen', 'George Leifman', 'Ehud Rivlin', 'Michael Elad']",http://arxiv.org/abs/2407.15153v1
Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation,"Text-to-video generation has been dominated by end-to-end diffusion-based or
autoregressive models. On one hand, those novel models provide plausible
versatility, but they are criticized for physical correctness, shading and
illumination, camera motion, and temporal consistency. On the other hand, film
industry relies on manually-edited Computer-Generated Imagery (CGI) using 3D
modeling software. Human-directed 3D synthetic videos and animations address
the aforementioned shortcomings, but it is extremely tedious and requires tight
collaboration between movie makers and 3D rendering experts. In this paper, we
introduce an automatic synthetic video generation pipeline based on Vision
Large Language Model (VLM) agent collaborations. Given a natural language
description of a video, multiple VLM agents auto-direct various processes of
the generation pipeline. They cooperate to create Blender scripts which render
a video that best aligns with the given description. Based on film making
inspiration and augmented with Blender-based movie making knowledge, the
Director agent decomposes the input text-based video description into
sub-processes. For each sub-process, the Programmer agent produces Python-based
Blender scripts based on customized function composing and API calling. Then,
the Reviewer agent, augmented with knowledge of video reviewing, character
motion coordinates, and intermediate screenshots uses its compositional
reasoning ability to provide feedback to the Programmer agent. The Programmer
agent iteratively improves the scripts to yield the best overall video outcome.
Our generated videos show better quality than commercial video generation
models in 5 metrics on video quality and instruction-following performance.
Moreover, our framework outperforms other approaches in a comprehensive user
study on quality, consistency, and rationality.",2024-08-19 23:31:02+00:00,"['Liu He', 'Yizhi Song', 'Hejun Huang', 'Daniel Aliaga', 'Xin Zhou']",http://arxiv.org/abs/2408.10453v1
LoVA: Long-form Video-to-Audio Generation,"Video-to-audio (V2A) generation is important for video editing and
post-processing, enabling the creation of semantics-aligned audio for silent
video. However, most existing methods focus on generating short-form audio for
short video segment (less than 10 seconds), while giving little attention to
the scenario of long-form video inputs. For current UNet-based diffusion V2A
models, an inevitable problem when handling long-form audio generation is the
inconsistencies within the final concatenated audio. In this paper, we first
highlight the importance of long-form V2A problem. Besides, we propose LoVA, a
novel model for Long-form Video-to-Audio generation. Based on the Diffusion
Transformer (DiT) architecture, LoVA proves to be more effective at generating
long-form audio compared to existing autoregressive models and UNet-based
diffusion models. Extensive objective and subjective experiments demonstrate
that LoVA achieves comparable performance on 10-second V2A benchmark and
outperforms all other baselines on a benchmark with long-form video input.",2024-09-23 16:04:50+00:00,"['Xin Cheng', 'Xihua Wang', 'Yihan Wu', 'Yuyue Wang', 'Ruihua Song']",http://arxiv.org/abs/2409.15157v2
Individual Content and Motion Dynamics Preserved Pruning for Video Diffusion Models,"The high computational cost and slow inference time are major obstacles to
deploying the video diffusion model (VDM) in practical applications. To
overcome this, we introduce a new Video Diffusion Model Compression approach
using individual content and motion dynamics preserved pruning and consistency
loss. First, we empirically observe that deeper VDM layers are crucial for
maintaining the quality of \textbf{motion dynamics} e.g., coherence of the
entire video, while shallower layers are more focused on \textbf{individual
content} e.g., individual frames. Therefore, we prune redundant blocks from the
shallower layers while preserving more of the deeper layers, resulting in a
lightweight VDM variant called VDMini. Additionally, we propose an
\textbf{Individual Content and Motion Dynamics (ICMD)} Consistency Loss to gain
comparable generation performance as larger VDM, i.e., the teacher to VDMini
i.e., the student. Particularly, we first use the Individual Content
Distillation (ICD) Loss to ensure consistency in the features of each generated
frame between the teacher and student models. Next, we introduce a Multi-frame
Content Adversarial (MCA) Loss to enhance the motion dynamics across the
generated video as a whole. This method significantly accelerates inference
time while maintaining high-quality video generation. Extensive experiments
demonstrate the effectiveness of our VDMini on two important video generation
tasks, Text-to-Video (T2V) and Image-to-Video (I2V), where we respectively
achieve an average 2.5 $\times$ and 1.4 $\times$ speed up for the I2V method
SF-V and the T2V method T2V-Turbo-v2, while maintaining the quality of the
generated videos on two benchmarks, i.e., UCF101 and VBench.",2024-11-27 14:22:13+00:00,"['Yiming Wu', 'Huan Wang', 'Zhenghao Chen', 'Dong Xu']",http://arxiv.org/abs/2411.18375v1
TAVGBench: Benchmarking Text to Audible-Video Generation,"The Text to Audible-Video Generation (TAVG) task involves generating videos
with accompanying audio based on text descriptions. Achieving this requires
skillful alignment of both audio and video elements. To support research in
this field, we have developed a comprehensive Text to Audible-Video Generation
Benchmark (TAVGBench), which contains over 1.7 million clips with a total
duration of 11.8 thousand hours. We propose an automatic annotation pipeline to
ensure each audible video has detailed descriptions for both its audio and
video contents. We also introduce the Audio-Visual Harmoni score (AVHScore) to
provide a quantitative measure of the alignment between the generated audio and
video modalities. Additionally, we present a baseline model for TAVG called
TAVDiffusion, which uses a two-stream latent diffusion model to provide a
fundamental starting point for further research in this area. We achieve the
alignment of audio and video by employing cross-attention and contrastive
learning. Through extensive experiments and evaluations on TAVGBench, we
demonstrate the effectiveness of our proposed model under both conventional
metrics and our proposed metrics.",2024-04-22 17:36:03+00:00,"['Yuxin Mao', 'Xuyang Shen', 'Jing Zhang', 'Zhen Qin', 'Jinxing Zhou', 'Mochu Xiang', 'Yiran Zhong', 'Yuchao Dai']",http://arxiv.org/abs/2404.14381v1
LT3SD: Latent Trees for 3D Scene Diffusion,"We present LT3SD, a novel latent diffusion model for large-scale 3D scene
generation. Recent advances in diffusion models have shown impressive results
in 3D object generation, but are limited in spatial extent and quality when
extended to 3D scenes. To generate complex and diverse 3D scene structures, we
introduce a latent tree representation to effectively encode both
lower-frequency geometry and higher-frequency detail in a coarse-to-fine
hierarchy. We can then learn a generative diffusion process in this latent 3D
scene space, modeling the latent components of a scene at each resolution
level. To synthesize large-scale scenes with varying sizes, we train our
diffusion model on scene patches and synthesize arbitrary-sized output 3D
scenes through shared diffusion generation across multiple scene patches.
Through extensive experiments, we demonstrate the efficacy and benefits of
LT3SD for large-scale, high-quality unconditional 3D scene generation and for
probabilistic completion for partial scene observations.",2024-09-12 16:55:51+00:00,"['Quan Meng', 'Lei Li', 'Matthias Niener', 'Angela Dai']",http://arxiv.org/abs/2409.08215v1
Exploring the Interplay Between Video Generation and World Models in Autonomous Driving: A Survey,"World models and video generation are pivotal technologies in the domain of
autonomous driving, each playing a critical role in enhancing the robustness
and reliability of autonomous systems. World models, which simulate the
dynamics of real-world environments, and video generation models, which produce
realistic video sequences, are increasingly being integrated to improve
situational awareness and decision-making capabilities in autonomous vehicles.
This paper investigates the relationship between these two technologies,
focusing on how their structural parallels, particularly in diffusion-based
models, contribute to more accurate and coherent simulations of driving
scenarios. We examine leading works such as JEPA, Genie, and Sora, which
exemplify different approaches to world model design, thereby highlighting the
lack of a universally accepted definition of world models. These diverse
interpretations underscore the field's evolving understanding of how world
models can be optimized for various autonomous driving tasks. Furthermore, this
paper discusses the key evaluation metrics employed in this domain, such as
Chamfer distance for 3D scene reconstruction and Fr\'echet Inception Distance
(FID) for assessing the quality of generated video content. By analyzing the
interplay between video generation and world models, this survey identifies
critical challenges and future research directions, emphasizing the potential
of these technologies to jointly advance the performance of autonomous driving
systems. The findings presented in this paper aim to provide a comprehensive
understanding of how the integration of video generation and world models can
drive innovation in the development of safer and more reliable autonomous
vehicles.",2024-11-05 08:58:35+00:00,"['Ao Fu', 'Yi Zhou', 'Tao Zhou', 'Yi Yang', 'Bojun Gao', 'Qun Li', 'Guobin Wu', 'Ling Shao']",http://arxiv.org/abs/2411.02914v1
Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy,"The accurate segmentation of guidewires in interventional cardiac fluoroscopy
videos is crucial for computer-aided navigation tasks. Although deep learning
methods have demonstrated high accuracy and robustness in wire segmentation,
they require substantial annotated datasets for generalizability, underscoring
the need for extensive labeled data to enhance model performance. To address
this challenge, we propose the Segmentation-guided Frame-consistency Video
Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy
videos, augmenting the training data for wire segmentation networks. SF-VD
leverages videos with limited annotations by independently modeling scene
distribution and motion distribution. It first samples the scene distribution
by generating 2D fluoroscopy images with wires positioned according to a
specified input mask, and then samples the motion distribution by progressively
generating subsequent frames, ensuring frame-to-frame coherence through a
frame-consistency strategy. A segmentation-guided mechanism further refines the
process by adjusting wire contrast, ensuring a diverse range of visibility in
the synthesized image. Evaluation on a fluoroscopy dataset confirms the
superior quality of the generated videos and shows significant improvements in
guidewire segmentation.",2024-12-20 16:52:11+00:00,"['Shaoyan Pan', 'Yikang Liu', 'Lin Zhao', 'Eric Z. Chen', 'Xiao Chen', 'Terrence Chen', 'Shanhui Sun']",http://arxiv.org/abs/2412.16050v4
"VJT: A Video Transformer on Joint Tasks of Deblurring, Low-light Enhancement and Denoising","Video restoration task aims to recover high-quality videos from low-quality
observations. This contains various important sub-tasks, such as video
denoising, deblurring and low-light enhancement, since video often faces
different types of degradation, such as blur, low light, and noise. Even worse,
these kinds of degradation could happen simultaneously when taking videos in
extreme environments. This poses significant challenges if one wants to remove
these artifacts at the same time. In this paper, to the best of our knowledge,
we are the first to propose an efficient end-to-end video transformer approach
for the joint task of video deblurring, low-light enhancement, and denoising.
This work builds a novel multi-tier transformer where each tier uses a
different level of degraded video as a target to learn the features of video
effectively. Moreover, we carefully design a new tier-to-tier feature fusion
scheme to learn video features incrementally and accelerate the training
process with a suitable adaptive weighting scheme. We also provide a new
Multiscene-Lowlight-Blur-Noise (MLBN) dataset, which is generated according to
the characteristics of the joint task based on the RealBlur dataset and YouTube
videos to simulate realistic scenes as far as possible. We have conducted
extensive experiments, compared with many previous state-of-the-art methods, to
show the effectiveness of our approach clearly.",2024-01-26 10:27:56+00:00,"['Yuxiang Hui', 'Yang Liu', 'Yaofang Liu', 'Fan Jia', 'Jinshan Pan', 'Raymond Chan', 'Tieyong Zeng']",http://arxiv.org/abs/2401.14754v1
Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis,"Contemporary models for generating images show remarkable quality and
versatility. Swayed by these advantages, the research community repurposes them
to generate videos. Since video content is highly redundant, we argue that
naively bringing advances of image models to the video generation domain
reduces motion fidelity, visual quality and impairs scalability. In this work,
we build Snap Video, a video-first model that systematically addresses these
challenges. To do that, we first extend the EDM framework to take into account
spatially and temporally redundant pixels and naturally support video
generation. Second, we show that a U-Net - a workhorse behind image generation
- scales poorly when generating videos, requiring significant computational
overhead. Hence, we propose a new transformer-based architecture that trains
3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us
to efficiently train a text-to-video model with billions of parameters for the
first time, reach state-of-the-art results on a number of benchmarks, and
generate videos with substantially higher quality, temporal consistency, and
motion complexity. The user studies showed that our model was favored by a
large margin over the most recent methods. See our website at
https://snap-research.github.io/snapvideo/.",2024-02-22 18:55:08+00:00,"['Willi Menapace', 'Aliaksandr Siarohin', 'Ivan Skorokhodov', 'Ekaterina Deyneka', 'Tsai-Shien Chen', 'Anil Kag', 'Yuwei Fang', 'Aleksei Stoliar', 'Elisa Ricci', 'Jian Ren', 'Sergey Tulyakov']",http://arxiv.org/abs/2402.14797v1
Compositional Video Generation as Flow Equalization,"Large-scale Text-to-Video (T2V) diffusion models have recently demonstrated
unprecedented capability to transform natural language descriptions into
stunning and photorealistic videos. Despite the promising results, a
significant challenge remains: these models struggle to fully grasp complex
compositional interactions between multiple concepts and actions. This issue
arises when some words dominantly influence the final video, overshadowing
other concepts.To tackle this problem, we introduce \textbf{Vico}, a generic
framework for compositional video generation that explicitly ensures all
concepts are represented properly. At its core, Vico analyzes how input tokens
influence the generated video, and adjusts the model to prevent any single
concept from dominating. Specifically, Vico extracts attention weights from all
layers to build a spatial-temporal attention graph, and then estimates the
influence as the \emph{max-flow} from the source text token to the video target
token. Although the direct computation of attention flow in diffusion models is
typically infeasible, we devise an efficient approximation based on subgraph
flows and employ a fast and vectorized implementation, which in turn makes the
flow computation manageable and differentiable. By updating the noisy latent to
balance these flows, Vico captures complex interactions and consequently
produces videos that closely adhere to textual descriptions. We apply our
method to multiple diffusion-based video models for compositional T2V and video
editing. Empirical results demonstrate that our framework significantly
enhances the compositional richness and accuracy of the generated videos. Visit
our website
at~\href{https://adamdad.github.io/vico/}{\url{https://adamdad.github.io/vico/}}.",2024-06-10 16:27:47+00:00,"['Xingyi Yang', 'Xinchao Wang']",http://arxiv.org/abs/2407.06182v1
VideoAgent: Self-Improving Video Generation,"Video generation has been used to generate visual plans for controlling
robotic systems. Given an image observation and a language instruction,
previous work has generated video plans which are then converted to robot
controls to be executed. However, a major bottleneck in leveraging video
generation for control lies in the quality of the generated videos, which often
suffer from hallucinatory content and unrealistic physics, resulting in low
task success when control actions are extracted from the generated videos.
While scaling up dataset and model size provides a partial solution,
integrating external feedback is both natural and essential for grounding video
generation in the real world. With this observation, we propose VideoAgent for
self-improving generated video plans based on external feedback. Instead of
directly executing the generated video plan, VideoAgent first refines the
generated video plans using a novel procedure which we call self-conditioning
consistency, allowing inference-time compute to be turned into better generated
video plans. As the refined video plan is being executed, VideoAgent can
collect additional data from the environment to further improve video plan
generation. Experiments in simulated robotic manipulation from MetaWorld and
iTHOR show that VideoAgent drastically reduces hallucination, thereby boosting
success rate of downstream manipulation tasks. We further illustrate that
VideoAgent can effectively refine real-robot videos, providing an early
indicator that robots can be an effective tool in grounding video generation in
the physical world. Video demos and code can be found at
https://video-as-agent.github.io.",2024-10-14 01:39:56+00:00,"['Achint Soni', 'Sreyas Venkataraman', 'Abhranil Chandra', 'Sebastian Fischmeister', 'Percy Liang', 'Bo Dai', 'Sherry Yang']",http://arxiv.org/abs/2410.10076v3
DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam Videos,"We present DC-Gaussian, a new method for generating novel views from
in-vehicle dash cam videos. While neural rendering techniques have made
significant strides in driving scenarios, existing methods are primarily
designed for videos collected by autonomous vehicles. However, these videos are
limited in both quantity and diversity compared to dash cam videos, which are
more widely used across various types of vehicles and capture a broader range
of scenarios. Dash cam videos often suffer from severe obstructions such as
reflections and occlusions on the windshields, which significantly impede the
application of neural rendering techniques. To address this challenge, we
develop DC-Gaussian based on the recent real-time neural rendering technique 3D
Gaussian Splatting (3DGS). Our approach includes an adaptive image
decomposition module to model reflections and occlusions in a unified manner.
Additionally, we introduce illumination-aware obstruction modeling to manage
reflections and occlusions under varying lighting conditions. Lastly, we employ
a geometry-guided Gaussian enhancement strategy to improve rendering details by
incorporating additional geometry priors. Experiments on self-captured and
public dash cam videos show that our method not only achieves state-of-the-art
performance in novel view synthesis, but also accurately reconstructing
captured scenes getting rid of obstructions. See the project page for code,
data: https://linhanwang.github.io/dcgaussian/.",2024-05-27 23:38:10+00:00,"['Linhan Wang', 'Kai Cheng', 'Shuo Lei', 'Shengkun Wang', 'Wei Yin', 'Chenyang Lei', 'Xiaoxiao Long', 'Chang-Tien Lu']",http://arxiv.org/abs/2405.17705v3
"Enhancing Multi-Text Long Video Generation Consistency without Tuning: Time-Frequency Analysis, Prompt Alignment, and Theory","Despite the considerable progress achieved in the long video generation
problem, there is still significant room to improve the consistency of the
videos, particularly in terms of smoothness and transitions between scenes. We
address these issues to enhance the consistency and coherence of videos
generated with either single or multiple prompts. We propose the Time-frequency
based temporal Attention Reweighting Algorithm (TiARA), which meticulously
edits the attention score matrix based on the Discrete Short-Time Fourier
Transform. Our method is supported by a theoretical guarantee, the
first-of-its-kind for frequency-based methods in diffusion models. For videos
generated by multiple prompts, we further investigate key factors affecting
prompt interpolation quality and propose PromptBlend, an advanced prompt
interpolation pipeline. The efficacy of our proposed method is validated via
extensive experimental results, exhibiting consistent and impressive
improvements over baseline methods. The code will be released upon acceptance.",2024-12-23 03:56:27+00:00,"['Xingyao Li', 'Fengzhuo Zhang', 'Jiachun Pan', 'Yunlong Hou', 'Vincent Y. F. Tan', 'Zhuoran Yang']",http://arxiv.org/abs/2412.17254v1
Four-Plane Factorized Video Autoencoders,"Latent variable generative models have emerged as powerful tools for
generative tasks including image and video synthesis. These models are enabled
by pretrained autoencoders that map high resolution data into a compressed
lower dimensional latent space, where the generative models can subsequently be
developed while requiring fewer computational resources. Despite their
effectiveness, the direct application of latent variable models to higher
dimensional domains such as videos continues to pose challenges for efficient
training and inference. In this paper, we propose an autoencoder that projects
volumetric data onto a four-plane factorized latent space that grows
sublinearly with the input size, making it ideal for higher dimensional data
like videos. The design of our factorized model supports straightforward
adoption in a number of conditional generation tasks with latent diffusion
models (LDMs), such as class-conditional generation, frame prediction, and
video interpolation. Our results show that the proposed four-plane latent space
retains a rich representation needed for high-fidelity reconstructions despite
the heavy compression, while simultaneously enabling LDMs to operate with
significant improvements in speed and memory.",2024-12-05 18:58:17+00:00,"['Mohammed Suhail', 'Carlos Esteves', 'Leonid Sigal', 'Ameesh Makadia']",http://arxiv.org/abs/2412.04452v1
Re-Attentional Controllable Video Diffusion Editing,"Editing videos with textual guidance has garnered popularity due to its
streamlined process which mandates users to solely edit the text prompt
corresponding to the source video. Recent studies have explored and exploited
large-scale text-to-image diffusion models for text-guided video editing,
resulting in remarkable video editing capabilities. However, they may still
suffer from some limitations such as mislocated objects, incorrect number of
objects. Therefore, the controllability of video editing remains a formidable
challenge. In this paper, we aim to challenge the above limitations by
proposing a Re-Attentional Controllable Video Diffusion Editing (ReAtCo)
method. Specially, to align the spatial placement of the target objects with
the edited text prompt in a training-free manner, we propose a Re-Attentional
Diffusion (RAD) to refocus the cross-attention activation responses between the
edited text prompt and the target video during the denoising stage, resulting
in a spatially location-aligned and semantically high-fidelity manipulated
video. In particular, to faithfully preserve the invariant region content with
less border artifacts, we propose an Invariant Region-guided Joint Sampling
(IRJS) strategy to mitigate the intrinsic sampling errors w.r.t the invariant
regions at each denoising timestep and constrain the generated content to be
harmonized with the invariant region content. Experimental results verify that
ReAtCo consistently improves the controllability of video diffusion editing and
achieves superior video editing performance.",2024-12-16 12:32:21+00:00,"['Yuanzhi Wang', 'Yong Li', 'Mengyi Liu', 'Xiaoya Zhang', 'Xin Liu', 'Zhen Cui', 'Antoni B. Chan']",http://arxiv.org/abs/2412.11710v1
Video Diffusion Models: A Survey,"Diffusion generative models have recently become a powerful technique for
creating and modifying high-quality, coherent video content. This survey
provides a comprehensive overview of the critical components of diffusion
models for video generation, including their applications, architectural
design, and temporal dynamics modeling. The paper begins by discussing the core
principles and mathematical formulations, then explores various architectural
choices and methods for maintaining temporal consistency. A taxonomy of
applications is presented, categorizing models based on input modalities such
as text prompts, images, videos, and audio signals. Advancements in
text-to-video generation are discussed to illustrate the state-of-the-art
capabilities and limitations of current approaches. Additionally, the survey
summarizes recent developments in training and evaluation practices, including
the use of diverse video and image datasets and the adoption of various
evaluation metrics to assess model performance. The survey concludes with an
examination of ongoing challenges, such as generating longer videos and
managing computational costs, and offers insights into potential future
directions for the field. By consolidating the latest research and
developments, this survey aims to serve as a valuable resource for researchers
and practitioners working with video diffusion models. Website:
https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models",2024-05-06 04:01:42+00:00,"['Andrew Melnik', 'Michal Ljubljanac', 'Cong Lu', 'Qi Yan', 'Weiming Ren', 'Helge Ritter']",http://arxiv.org/abs/2405.03150v2
CAGE: Unsupervised Visual Composition and Animation for Controllable Video Generation,"The field of video generation has expanded significantly in recent years,
with controllable and compositional video generation garnering considerable
interest. Most methods rely on leveraging annotations such as text, objects'
bounding boxes, and motion cues, which require substantial human effort and
thus limit their scalability. In contrast, we address the challenge of
controllable and compositional video generation without any annotations by
introducing a novel unsupervised approach. Our model is trained from scratch on
a dataset of unannotated videos. At inference time, it can compose plausible
novel scenes and animate objects by placing object parts at the desired
locations in space and time. The core innovation of our method lies in the
unified control format and the training process, where video generation is
conditioned on a randomly selected subset of pre-trained self-supervised local
features. This conditioning compels the model to learn how to inpaint the
missing information in the video both spatially and temporally, thereby
learning the inherent compositionality of a scene and the dynamics of moving
objects. The abstraction level and the imposed invariance of the conditioning
input to minor visual perturbations enable control over object motion by simply
using the same features at all the desired future locations. We call our model
CAGE, which stands for visual Composition and Animation for video GEneration.
We conduct extensive experiments to validate the effectiveness of CAGE across
various scenarios, demonstrating its capability to accurately follow the
control and to generate high-quality videos that exhibit coherent scene
composition and realistic animation.",2024-03-21 12:50:15+00:00,"['Aram Davtyan', 'Sepehr Sameni', 'Bjrn Ommer', 'Paolo Favaro']",http://arxiv.org/abs/2403.14368v2
ARDuP: Active Region Video Diffusion for Universal Policies,"Sequential decision-making can be formulated as a text-conditioned video
generation problem, where a video planner, guided by a text-defined goal,
generates future frames visualizing planned actions, from which control actions
are subsequently derived. In this work, we introduce Active Region Video
Diffusion for Universal Policies (ARDuP), a novel framework for video-based
policy learning that emphasizes the generation of active regions, i.e.
potential interaction areas, enhancing the conditional policy's focus on
interactive areas critical for task execution. This innovative framework
integrates active region conditioning with latent diffusion models for video
planning and employs latent representations for direct action decoding during
inverse dynamic modeling. By utilizing motion cues in videos for automatic
active region discovery, our method eliminates the need for manual annotations
of active regions. We validate ARDuP's efficacy via extensive experiments on
simulator CLIPort and the real-world dataset BridgeData v2, achieving notable
improvements in success rates and generating convincingly realistic video
plans.",2024-06-19 07:42:02+00:00,"['Shuaiyi Huang', 'Mara Levy', 'Zhenyu Jiang', 'Anima Anandkumar', 'Yuke Zhu', 'Linxi Fan', 'De-An Huang', 'Abhinav Shrivastava']",http://arxiv.org/abs/2406.13301v2
Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation,"Video outpainting is a challenging task, aiming at generating video content
outside the viewport of the input video while maintaining inter-frame and
intra-frame consistency. Existing methods fall short in either generation
quality or flexibility. We introduce MOTIA Mastering Video Outpainting Through
Input-Specific Adaptation, a diffusion-based pipeline that leverages both the
intrinsic data-specific patterns of the source video and the image/video
generative prior for effective outpainting. MOTIA comprises two main phases:
input-specific adaptation and pattern-aware outpainting. The input-specific
adaptation phase involves conducting efficient and effective pseudo outpainting
learning on the single-shot source video. This process encourages the model to
identify and learn patterns within the source video, as well as bridging the
gap between standard generative processes and outpainting. The subsequent
phase, pattern-aware outpainting, is dedicated to the generalization of these
learned patterns to generate outpainting outcomes. Additional strategies
including spatial-aware insertion and noise travel are proposed to better
leverage the diffusion model's generative prior and the acquired video patterns
from source videos. Extensive evaluations underscore MOTIA's superiority,
outperforming existing state-of-the-art methods in widely recognized
benchmarks. Notably, these advancements are achieved without necessitating
extensive, task-specific tuning.",2024-03-20 16:53:45+00:00,"['Fu-Yun Wang', 'Xiaoshi Wu', 'Zhaoyang Huang', 'Xiaoyu Shi', 'Dazhong Shen', 'Guanglu Song', 'Yu Liu', 'Hongsheng Li']",http://arxiv.org/abs/2403.13745v1
Video-Infinity: Distributed Long Video Generation,"Diffusion models have recently achieved remarkable results for video
generation. Despite the encouraging performances, the generated videos are
typically constrained to a small number of frames, resulting in clips lasting
merely a few seconds. The primary challenges in producing longer videos include
the substantial memory requirements and the extended processing time required
on a single GPU. A straightforward solution would be to split the workload
across multiple GPUs, which, however, leads to two issues: (1) ensuring all
GPUs communicate effectively to share timing and context information, and (2)
modifying existing video diffusion models, which are usually trained on short
sequences, to create longer videos without additional training. To tackle
these, in this paper we introduce Video-Infinity, a distributed inference
pipeline that enables parallel processing across multiple GPUs for long-form
video generation. Specifically, we propose two coherent mechanisms: Clip
parallelism and Dual-scope attention. Clip parallelism optimizes the gathering
and sharing of context information across GPUs which minimizes communication
overhead, while Dual-scope attention modulates the temporal self-attention to
balance local and global contexts efficiently across the devices. Together, the
two mechanisms join forces to distribute the workload and enable the fast
generation of long videos. Under an 8 x Nvidia 6000 Ada GPU (48G) setup, our
method generates videos up to 2,300 frames in approximately 5 minutes, enabling
long video generation at a speed 100 times faster than the prior methods.",2024-06-24 01:56:12+00:00,"['Zhenxiong Tan', 'Xingyi Yang', 'Songhua Liu', 'Xinchao Wang']",http://arxiv.org/abs/2406.16260v1
IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint Video-Depth Generation,"Significant advances have been made in human-centric video generation, yet
the joint video-depth generation problem remains underexplored. Most existing
monocular depth estimation methods may not generalize well to synthesized
images or videos, and multi-view-based methods have difficulty controlling the
human appearance and motion. In this work, we present IDOL (unIfied Dual-mOdal
Latent diffusion) for high-quality human-centric joint video-depth generation.
Our IDOL consists of two novel designs. First, to enable dual-modal generation
and maximize the information exchange between video and depth generation, we
propose a unified dual-modal U-Net, a parameter-sharing framework for joint
video and depth denoising, wherein a modality label guides the denoising
target, and cross-modal attention enables the mutual information flow. Second,
to ensure a precise video-depth spatial alignment, we propose a motion
consistency loss that enforces consistency between the video and depth feature
motion fields, leading to harmonized outputs. Additionally, a cross-attention
map consistency loss is applied to align the cross-attention map of the video
denoising with that of the depth denoising, further facilitating spatial
alignment. Extensive experiments on the TikTok and NTU120 datasets show our
superior performance, significantly surpassing existing methods in terms of
video FVD and depth accuracy.",2024-07-15 17:36:54+00:00,"['Yuanhao Zhai', 'Kevin Lin', 'Linjie Li', 'Chung-Ching Lin', 'Jianfeng Wang', 'Zhengyuan Yang', 'David Doermann', 'Junsong Yuan', 'Zicheng Liu', 'Lijuan Wang']",http://arxiv.org/abs/2407.10937v1
OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models,"We consider the problem of text-to-video generation tasks with precise
control for various applications such as camera movement control and
video-to-video editing. Most methods tacking this problem rely on providing
user-defined controls, such as binary masks or camera movement embeddings. In
our approach we propose OnlyFlow, an approach leveraging the optical flow
firstly extracted from an input video to condition the motion of generated
videos. Using a text prompt and an input video, OnlyFlow allows the user to
generate videos that respect the motion of the input video as well as the text
prompt. This is implemented through an optical flow estimation model applied on
the input video, which is then fed to a trainable optical flow encoder. The
output feature maps are then injected into the text-to-video backbone model. We
perform quantitative, qualitative and user preference studies to show that
OnlyFlow positively compares to state-of-the-art methods on a wide range of
tasks, even though OnlyFlow was not specifically trained for such tasks.
OnlyFlow thus constitutes a versatile, lightweight yet efficient method for
controlling motion in text-to-video generation. Models and code will be made
available on GitHub and HuggingFace.",2024-11-15 11:19:25+00:00,"['Mathis Koroglu', 'Hugo Caselles-Dupr', 'Guillaume Jeanneret Sanmiguel', 'Matthieu Cord']",http://arxiv.org/abs/2411.10501v1
PaintScene4D: Consistent 4D Scene Generation from Text Prompts,"Recent advances in diffusion models have revolutionized 2D and 3D content
creation, yet generating photorealistic dynamic 4D scenes remains a significant
challenge. Existing dynamic 4D generation methods typically rely on distilling
knowledge from pre-trained 3D generative models, often fine-tuned on synthetic
object datasets. Consequently, the resulting scenes tend to be object-centric
and lack photorealism. While text-to-video models can generate more realistic
scenes with motion, they often struggle with spatial understanding and provide
limited control over camera viewpoints during rendering. To address these
limitations, we present PaintScene4D, a novel text-to-4D scene generation
framework that departs from conventional multi-view generative models in favor
of a streamlined architecture that harnesses video generative models trained on
diverse real-world datasets. Our method first generates a reference video using
a video generation model, and then employs a strategic camera array selection
for rendering. We apply a progressive warping and inpainting technique to
ensure both spatial and temporal consistency across multiple viewpoints.
Finally, we optimize multi-view images using a dynamic renderer, enabling
flexible camera control based on user preferences. Adopting a training-free
architecture, our PaintScene4D efficiently produces realistic 4D scenes that
can be viewed from arbitrary trajectories. The code will be made publicly
available. Our project page is at https://paintscene4d.github.io/",2024-12-05 18:59:57+00:00,"['Vinayak Gupta', 'Yunze Man', 'Yu-Xiong Wang']",http://arxiv.org/abs/2412.04471v1
TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment,"Recent advancements in image understanding have benefited from the extensive
use of web image-text pairs. However, video understanding remains a challenge
despite the availability of substantial web video-text data. This difficulty
primarily arises from the inherent complexity of videos and the inefficient
language supervision in recent web-collected video-text datasets. In this
paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend
large language models (LLMs) for video understanding, without the need for
pre-training on real video data. Specifically, we first employ an advanced LLM
to automatically generate Textual Videos comprising continuous textual frames,
along with corresponding annotations to simulate real video-text data. Then,
these annotated textual videos are used to pre-align a language-only LLM with
the video modality. To bridge the gap between textual and real videos, we
employ the CLIP model as the feature extractor to align image and text
modalities. During text-only pre-alignment, the continuous textual frames,
encoded as a sequence of CLIP text features, are analogous to continuous CLIP
image features, thus aligning the LLM with real video representation. Extensive
experiments, including zero-shot evaluation and finetuning on various video
understanding tasks, demonstrate that TOPA is an effective and efficient
framework for aligning video content with LLMs. In particular, without training
on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%
on the challenging long-form video understanding benchmark, Egoschema. This
performance surpasses previous video-text pre-training approaches and proves
competitive with recent GPT-3.5-based video agents.",2024-05-22 18:35:10+00:00,"['Wei Li', 'Hehe Fan', 'Yongkang Wong', 'Mohan Kankanhalli', 'Yi Yang']",http://arxiv.org/abs/2405.13911v2
RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion,"We introduce RealmDreamer, a technique for generating forward-facing 3D
scenes from text descriptions. Our method optimizes a 3D Gaussian Splatting
representation to match complex text prompts using pretrained diffusion models.
Our key insight is to leverage 2D inpainting diffusion models conditioned on an
initial scene estimate to provide low variance supervision for unknown regions
during 3D distillation. In conjunction, we imbue high-fidelity geometry with
geometric distillation from a depth diffusion model, conditioned on samples
from the inpainting model. We find that the initialization of the optimization
is crucial, and provide a principled methodology for doing so. Notably, our
technique doesn't require video or multi-view data and can synthesize various
high-quality 3D scenes in different styles with complex layouts. Further, the
generality of our method allows 3D synthesis from a single image. As measured
by a comprehensive user study, our method outperforms all existing approaches,
preferred by 88-95%. Project Page: https://realmdreamer.github.io/",2024-04-10 17:57:41+00:00,"['Jaidev Shriram', 'Alex Trevithick', 'Lingjie Liu', 'Ravi Ramamoorthi']",http://arxiv.org/abs/2404.07199v2
WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled Diffusion Models,"Video virtual try-on aims to generate realistic sequences that maintain
garment identity and adapt to a person's pose and body shape in source videos.
Traditional image-based methods, relying on warping and blending, struggle with
complex human movements and occlusions, limiting their effectiveness in video
try-on applications. Moreover, video-based models require extensive,
high-quality data and substantial computational resources. To tackle these
issues, we reconceptualize video try-on as a process of generating videos
conditioned on garment descriptions and human motion. Our solution, WildVidFit,
employs image-based controlled diffusion models for a streamlined, one-stage
approach. This model, conditioned on specific garments and individuals, is
trained on still images rather than videos. It leverages diffusion guidance
from pre-trained models including a video masked autoencoder for segment
smoothness improvement and a self-supervised model for feature alignment of
adjacent frame in the latent space. This integration markedly boosts the
model's ability to maintain temporal coherence, enabling more effective video
try-on within an image-based framework. Our experiments on the VITON-HD and
DressCode datasets, along with tests on the VVT and TikTok datasets,
demonstrate WildVidFit's capability to generate fluid and coherent videos. The
project page website is at wildvidfit-project.github.io.",2024-07-15 11:21:03+00:00,"['Zijian He', 'Peixin Chen', 'Guangrun Wang', 'Guanbin Li', 'Philip H. S. Torr', 'Liang Lin']",http://arxiv.org/abs/2407.10625v1
UniVG: Towards UNIfied-modal Video Generation,"Diffusion based video generation has received extensive attention and
achieved considerable success within both the academic and industrial
communities. However, current efforts are mainly concentrated on
single-objective or single-task video generation, such as generation driven by
text, by image, or by a combination of text and image. This cannot fully meet
the needs of real-world application scenarios, as users are likely to input
images and text conditions in a flexible manner, either individually or in
combination. To address this, we propose a Unified-modal Video Genearation
system that is capable of handling multiple video generation tasks across text
and image modalities. To this end, we revisit the various video generation
tasks within our system from the perspective of generative freedom, and
classify them into high-freedom and low-freedom video generation categories.
For high-freedom video generation, we employ Multi-condition Cross Attention to
generate videos that align with the semantics of the input images or text. For
low-freedom video generation, we introduce Biased Gaussian Noise to replace the
pure random Gaussian Noise, which helps to better preserve the content of the
input conditions. Our method achieves the lowest Fr\'echet Video Distance (FVD)
on the public academic benchmark MSR-VTT, surpasses the current open-source
methods in human evaluations, and is on par with the current close-source
method Gen2. For more samples, visit https://univg-baidu.github.io.",2024-01-17 09:46:13+00:00,"['Ludan Ruan', 'Lei Tian', 'Chuanwei Huang', 'Xu Zhang', 'Xinyan Xiao']",http://arxiv.org/abs/2401.09084v1
Training-Free Condition Video Diffusion Models for single frame Spatial-Semantic Echocardiogram Synthesis,"Conditional video diffusion models (CDM) have shown promising results for
video synthesis, potentially enabling the generation of realistic
echocardiograms to address the problem of data scarcity. However, current CDMs
require a paired segmentation map and echocardiogram dataset. We present a new
method called Free-Echo for generating realistic echocardiograms from a single
end-diastolic segmentation map without additional training data. Our method is
based on the 3D-Unet with Temporal Attention Layers model and is conditioned on
the segmentation map using a training-free conditioning method based on SDEdit.
We evaluate our model on two public echocardiogram datasets, CAMUS and
EchoNet-Dynamic. We show that our model can generate plausible echocardiograms
that are spatially aligned with the input segmentation map, achieving
performance comparable to training-based CDMs. Our work opens up new
possibilities for generating echocardiograms from a single segmentation map,
which can be used for data augmentation, domain adaptation, and other
applications in medical imaging. Our code is available at
\url{https://github.com/gungui98/echo-free}",2024-08-06 08:31:34+00:00,"['Van Phi Nguyen', 'Tri Nhan Luong Ha', 'Huy Hieu Pham', 'Quoc Long Tran']",http://arxiv.org/abs/2408.03035v2
Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution,"We propose an efficient diffusion-based text-to-video super-resolution (SR)
tuning approach that leverages the readily learned capacity of pixel level
image diffusion model to capture spatial information for video generation. To
accomplish this goal, we design an efficient architecture by inflating the
weightings of the text-to-image SR model into our video generation framework.
Additionally, we incorporate a temporal adapter to ensure temporal coherence
across video frames. We investigate different tuning approaches based on our
inflated architecture and report trade-offs between computational costs and
super-resolution quality. Empirical evaluation, both quantitative and
qualitative, on the Shutterstock video dataset, demonstrates that our approach
is able to perform text-to-video SR generation with good visual quality and
temporal consistency. To evaluate temporal coherence, we also present
visualizations in video format in
https://drive.google.com/drive/folders/1YVc-KMSJqOrEUdQWVaI-Yfu8Vsfu_1aO?usp=sharing .",2024-01-18 22:25:16+00:00,"['Xin Yuan', 'Jinoo Baek', 'Keyang Xu', 'Omer Tov', 'Hongliang Fei']",http://arxiv.org/abs/2401.10404v1
MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views,"We introduce MVSplat360, a feed-forward approach for 360{\deg} novel view
synthesis (NVS) of diverse real-world scenes, using only sparse observations.
This setting is inherently ill-posed due to minimal overlap among input views
and insufficient visual information provided, making it challenging for
conventional methods to achieve high-quality results. Our MVSplat360 addresses
this by effectively combining geometry-aware 3D reconstruction with temporally
consistent video generation. Specifically, it refactors a feed-forward 3D
Gaussian Splatting (3DGS) model to render features directly into the latent
space of a pre-trained Stable Video Diffusion (SVD) model, where these features
then act as pose and visual cues to guide the denoising process and produce
photorealistic 3D-consistent views. Our model is end-to-end trainable and
supports rendering arbitrary views with as few as 5 sparse input views. To
evaluate MVSplat360's performance, we introduce a new benchmark using the
challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual
quality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}
NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the
effectiveness of our model. The video results are available on our project
page: https://donydchen.github.io/mvsplat360.",2024-11-07 17:59:31+00:00,"['Yuedong Chen', 'Chuanxia Zheng', 'Haofei Xu', 'Bohan Zhuang', 'Andrea Vedaldi', 'Tat-Jen Cham', 'Jianfei Cai']",http://arxiv.org/abs/2411.04924v1
VCA: Video Curious Agent for Long Video Understanding,"Long video understanding poses unique challenges due to their temporal
complexity and low information density. Recent works address this task by
sampling numerous frames or incorporating auxiliary tools using LLMs, both of
which result in high computational costs. In this work, we introduce a
curiosity-driven video agent with self-exploration capability, dubbed as VCA.
Built upon VLMs, VCA autonomously navigates video segments and efficiently
builds a comprehensive understanding of complex video sequences. Instead of
directly sampling frames, VCA employs a tree-search structure to explore video
segments and collect frames. Rather than relying on external feedback or
reward, VCA leverages VLM's self-generated intrinsic reward to guide its
exploration, enabling it to capture the most crucial information for reasoning.
Experimental results on multiple long video benchmarks demonstrate our
approach's superior effectiveness and efficiency.",2024-12-12 23:39:54+00:00,"['Zeyuan Yang', 'Delin Chen', 'Xueyang Yu', 'Maohao Shen', 'Chuang Gan']",http://arxiv.org/abs/2412.10471v2
Harnessing Meta-Learning for Improving Full-Frame Video Stabilization,"Video stabilization is a longstanding computer vision problem, particularly
pixel-level synthesis solutions for video stabilization which synthesize full
frames add to the complexity of this task. These techniques aim to stabilize
videos by synthesizing full frames while enhancing the stability of the
considered video. This intensifies the complexity of the task due to the
distinct mix of unique motion profiles and visual content present in each video
sequence, making robust generalization with fixed parameters difficult. In our
study, we introduce a novel approach to enhance the performance of pixel-level
synthesis solutions for video stabilization by adapting these models to
individual input video sequences. The proposed adaptation exploits low-level
visual cues accessible during test-time to improve both the stability and
quality of resulting videos. We highlight the efficacy of our methodology of
""test-time adaptation"" through simple fine-tuning of one of these models,
followed by significant stability gain via the integration of meta-learning
techniques. Notably, significant improvement is achieved with only a single
adaptation step. The versatility of the proposed algorithm is demonstrated by
consistently improving the performance of various pixel-level synthesis models
for video stabilization in real-world scenarios.",2024-03-06 12:31:02+00:00,"['Muhammad Kashif Ali', 'Eun Woo Im', 'Dongjin Kim', 'Tae Hyun Kim']",http://arxiv.org/abs/2403.03662v2
Rethinking Video Deblurring with Wavelet-Aware Dynamic Transformer and Diffusion Model,"Current video deblurring methods have limitations in recovering
high-frequency information since the regression losses are conservative with
high-frequency details. Since Diffusion Models (DMs) have strong capabilities
in generating high-frequency details, we consider introducing DMs into the
video deblurring task. However, we found that directly applying DMs to the
video deblurring task has the following problems: (1) DMs require many
iteration steps to generate videos from Gaussian noise, which consumes many
computational resources. (2) DMs are easily misled by the blurry artifacts in
the video, resulting in irrational content and distortion of the deblurred
video. To address the above issues, we propose a novel video deblurring
framework VD-Diff that integrates the diffusion model into the Wavelet-Aware
Dynamic Transformer (WADT). Specifically, we perform the diffusion model in a
highly compact latent space to generate prior features containing
high-frequency information that conforms to the ground truth distribution. We
design the WADT to preserve and recover the low-frequency information in the
video while utilizing the high-frequency information generated by the diffusion
model. Extensive experiments show that our proposed VD-Diff outperforms SOTA
methods on GoPro, DVD, BSD, and Real-World Video datasets.",2024-08-24 04:13:47+00:00,"['Chen Rao', 'Guangyuan Li', 'Zehua Lan', 'Jiakai Sun', 'Junsheng Luan', 'Wei Xing', 'Lei Zhao', 'Huaizhong Lin', 'Jianfeng Dong', 'Dalong Zhang']",http://arxiv.org/abs/2408.13459v1
Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit regularization,"Compositional 3D scene synthesis has diverse applications across a spectrum
of industries such as robotics, films, and video games, as it closely mirrors
the complexity of real-world multi-object environments. Conventional works
typically employ shape retrieval based frameworks which naturally suffer from
limited shape diversity. Recent progresses have been made in object shape
generation with generative models such as diffusion models, which increases the
shape fidelity. However, these approaches separately treat 3D shape generation
and layout generation. The synthesized scenes are usually hampered by layout
collision, which suggests that the scene-level fidelity is still
under-explored. In this paper, we aim at generating realistic and reasonable 3D
indoor scenes from scene graph. To enrich the priors of the given scene graph
inputs, large language model is utilized to aggregate the global-wise features
with local node-wise and edge-wise features. With a unified graph encoder,
graph features are extracted to guide joint layout-shape generation. Additional
regularization is introduced to explicitly constrain the produced 3D layouts.
Benchmarked on the SG-FRONT dataset, our method achieves better 3D scene
synthesis, especially in terms of scene-level fidelity. The source code will be
released after publication.",2024-03-19 15:54:48+00:00,"['Yao Wei', 'Martin Renqiang Min', 'George Vosselman', 'Li Erran Li', 'Michael Ying Yang']",http://arxiv.org/abs/2403.12848v2
SAR Image Synthesis with Diffusion Models,"In recent years, diffusion models (DMs) have become a popular method for
generating synthetic data. By achieving samples of higher quality, they quickly
became superior to generative adversarial networks (GANs) and the current
state-of-the-art method in generative modeling. However, their potential has
not yet been exploited in radar, where the lack of available training data is a
long-standing problem. In this work, a specific type of DMs, namely denoising
diffusion probabilistic model (DDPM) is adapted to the SAR domain. We
investigate the network choice and specific diffusion parameters for
conditional and unconditional SAR image generation. In our experiments, we show
that DDPM qualitatively and quantitatively outperforms state-of-the-art
GAN-based methods for SAR image generation. Finally, we show that DDPM profits
from pretraining on largescale clutter data, generating SAR images of even
higher quality.",2024-05-13 14:21:18+00:00,"['Denisa Qosja', 'Simon Wagner', ""Daniel O'Hagan""]",http://arxiv.org/abs/2405.07776v1
Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos,"Recent advancements in static feed-forward scene reconstruction have
demonstrated significant progress in high-quality novel view synthesis.
However, these models often struggle with generalizability across diverse
environments and fail to effectively handle dynamic content. We present BTimer
(short for BulletTimer), the first motion-aware feed-forward model for
real-time reconstruction and novel view synthesis of dynamic scenes. Our
approach reconstructs the full scene in a 3D Gaussian Splatting representation
at a given target ('bullet') timestamp by aggregating information from all the
context frames. Such a formulation allows BTimer to gain scalability and
generalization by leveraging both static and dynamic scene datasets. Given a
casual monocular dynamic video, BTimer reconstructs a bullet-time scene within
150ms while reaching state-of-the-art performance on both static and dynamic
scene datasets, even compared with optimization-based approaches.",2024-12-04 18:15:06+00:00,"['Hanxue Liang', 'Jiawei Ren', 'Ashkan Mirzaei', 'Antonio Torralba', 'Ziwei Liu', 'Igor Gilitschenski', 'Sanja Fidler', 'Cengiz Oztireli', 'Huan Ling', 'Zan Gojcic', 'Jiahui Huang']",http://arxiv.org/abs/2412.03526v1
"Exploring AIGC Video Quality: A Focus on Visual Harmony, Video-Text Consistency and Domain Distribution Gap","The recent advancements in Text-to-Video Artificial Intelligence Generated
Content (AIGC) have been remarkable. Compared with traditional videos, the
assessment of AIGC videos encounters various challenges: visual inconsistency
that defy common sense, discrepancies between content and the textual prompt,
and distribution gap between various generative models, etc. Target at these
challenges, in this work, we categorize the assessment of AIGC video quality
into three dimensions: visual harmony, video-text consistency, and domain
distribution gap. For each dimension, we design specific modules to provide a
comprehensive quality assessment of AIGC videos. Furthermore, our research
identifies significant variations in visual quality, fluidity, and style among
videos generated by different text-to-video models. Predicting the source
generative model can make the AIGC video features more discriminative, which
enhances the quality assessment performance. The proposed method was used in
the third-place winner of the NTIRE 2024 Quality Assessment for AI-Generated
Content - Track 2 Video, demonstrating its effectiveness. Code will be
available at https://github.com/Coobiw/TriVQA.",2024-04-21 08:27:20+00:00,"['Bowen Qu', 'Xiaoyu Liang', 'Shangkun Sun', 'Wei Gao']",http://arxiv.org/abs/2404.13573v2
VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition,"Recent advancements in Large Video-Language Models (LVLMs) have driven the
development of benchmarks designed to assess cognitive abilities in video-based
tasks. However, most existing benchmarks heavily rely on web-collected videos
paired with human annotations or model-generated questions, which limit control
over the video content and fall short in evaluating advanced cognitive
abilities involving symbolic elements and abstract concepts. To address these
limitations, we introduce VCBench, a controllable benchmark to assess LVLMs'
cognitive abilities, involving symbolic and abstract concepts at varying
difficulty levels. By generating video data with the Python-based engine,
VCBench allows for precise control over the video content, creating dynamic,
task-oriented videos that feature complex scenes and abstract concepts. Each
task pairs with tailored question templates that target specific cognitive
challenges, providing a rigorous evaluation test. Our evaluation reveals that
even state-of-the-art (SOTA) models, such as Qwen2-VL-72B, struggle with simple
video cognition tasks involving abstract concepts, with performance sharply
dropping by 19% as video complexity rises. These findings reveal the current
limitations of LVLMs in advanced cognitive tasks and highlight the critical
role of VCBench in driving research toward more robust LVLMs for complex video
cognition challenges.",2024-11-14 00:26:26+00:00,"['Chenglin Li', 'Qianglong Chen', 'Zhi Li', 'Feng Tao', 'Yin Zhang']",http://arxiv.org/abs/2411.09105v1
ConCLVD: Controllable Chinese Landscape Video Generation via Diffusion Model,"Chinese landscape painting is a gem of Chinese cultural and artistic heritage
that showcases the splendor of nature through the deep observations and
imaginations of its painters. Limited by traditional techniques, these artworks
were confined to static imagery in ancient times, leaving the dynamism of
landscapes and the subtleties of artistic sentiment to the viewer's
imagination. Recently, emerging text-to-video (T2V) diffusion methods have
shown significant promise in video generation, providing hope for the creation
of dynamic Chinese landscape paintings. However, challenges such as the lack of
specific datasets, the intricacy of artistic styles, and the creation of
extensive, high-quality videos pose difficulties for these models in generating
Chinese landscape painting videos. In this paper, we propose CLV-HD (Chinese
Landscape Video-High Definition), a novel T2V dataset for Chinese landscape
painting videos, and ConCLVD (Controllable Chinese Landscape Video Diffusion),
a T2V model that utilizes Stable Diffusion. Specifically, we present a motion
module featuring a dual attention mechanism to capture the dynamic
transformations of landscape imageries, alongside a noise adapter to leverage
unsupervised contrastive learning in the latent space. Following the generation
of keyframes, we employ optical flow for frame interpolation to enhance video
smoothness. Our method not only retains the essence of the landscape painting
imageries but also achieves dynamic transitions, significantly advancing the
field of artistic video generation. The source code and dataset are available
at https://anonymous.4open.science/r/ConCLVD-EFE3.",2024-04-19 14:19:13+00:00,"['Dingming Liu', 'Shaowei Li', 'Ruoyan Zhou', 'Lili Liang', 'Yongguan Hong', 'Fei Chao', 'Rongrong Ji']",http://arxiv.org/abs/2404.12903v1
Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event Condition For Foley Sound,"Foley sound synthesis is crucial for multimedia production, enhancing user
experience by synchronizing audio and video both temporally and semantically.
Recent studies on automating this labor-intensive process through
video-to-sound generation face significant challenges. Systems lacking explicit
temporal features suffer from poor alignment and controllability, while
timestamp-based models require costly and subjective human annotation. We
propose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an
intuitive condition with semantic timbre prompts (audio or text). RMS, a
frame-level intensity envelope closely related to audio semantics, acts as a
temporal event feature to guide audio generation from video. The
annotation-free self-supervised learning framework consists of two stages,
Video2RMS and RMS2Sound, incorporating novel ideas including RMS discretization
and RMS-ControlNet with a pretrained text-to-audio model. Our extensive
evaluation shows that Video-Foley achieves state-of-the-art performance in
audio-visual alignment and controllability for sound timing, intensity, timbre,
and nuance. Source code, model weights and demos are available on our companion
website. (https://jnwnlee.github.io/video-foley-demo)",2024-08-21 18:06:15+00:00,"['Junwon Lee', 'Jaekwon Im', 'Dabin Kim', 'Juhan Nam']",http://arxiv.org/abs/2408.11915v2
PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework with Correlated Differential Privacy,"Online video streaming has evolved into an integral component of the
contemporary Internet landscape. Yet, the disclosure of user requests presents
formidable privacy challenges. As users stream their preferred online videos,
their requests are automatically seized by video content providers, potentially
leaking users' privacy.
  Unfortunately, current protection methods are not well-suited to preserving
user request privacy from content providers while maintaining high-quality
online video services. To tackle this challenge, we introduce a novel
Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge
devices to pre-fetch and cache videos, ensuring the privacy of users' requests
while optimizing the efficiency of edge caching. More specifically, we design
PPVF with three core components: (1) \textit{Online privacy budget scheduler},
which employs a theoretically guaranteed online algorithm to select
non-requested videos as candidates with assigned privacy budgets. Alternative
videos are chosen by an online algorithm that is theoretically guaranteed to
consider both video utilities and available privacy budgets. (2) \textit{Noisy
video request generator}, which generates redundant video requests (in addition
to original ones) utilizing correlated differential privacy to obfuscate
request privacy. (3) \textit{Online video utility predictor}, which leverages
federated learning to collaboratively evaluate video utility in an online
fashion, aiding in video selection in (1) and noise generation in (2). Finally,
we conduct extensive experiments using real-world video request traces from
Tencent Video. The results demonstrate that PPVF effectively safeguards user
request privacy while upholding high video caching performance.",2024-08-27 02:03:36+00:00,"['Xianzhi Zhang', 'Yipeng Zhou', 'Di Wu', 'Quan Z. Sheng', 'Miao Hu', 'Linchang Xiao']",http://arxiv.org/abs/2408.14735v1
Owl-1: Omni World Model for Consistent Long Video Generation,"Video generation models (VGMs) have received extensive attention recently and
serve as promising candidates for general-purpose large vision models. While
they can only generate short videos each time, existing methods achieve long
video generation by iteratively calling the VGMs, using the last-frame output
as the condition for the next-round generation. However, the last frame only
contains short-term fine-grained information about the scene, resulting in
inconsistency in the long horizon. To address this, we propose an Omni World
modeL (Owl-1) to produce long-term coherent and comprehensive conditions for
consistent long video generation. As videos are observations of the underlying
evolving world, we propose to model the long-term developments in a latent
space and use VGMs to film them into videos. Specifically, we represent the
world with a latent state variable which can be decoded into explicit video
observations. These observations serve as a basis for anticipating temporal
dynamics which in turn update the state variable. The interaction between
evolving dynamics and persistent state enhances the diversity and consistency
of the long videos. Extensive experiments show that Owl-1 achieves comparable
performance with SOTA methods on VBench-I2V and VBench-Long, validating its
ability to generate high-quality video observations. Code:
https://github.com/huang-yh/Owl.",2024-12-12 18:59:01+00:00,"['Yuanhui Huang', 'Wenzhao Zheng', 'Yuan Gao', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Jie Zhou', 'Jiwen Lu']",http://arxiv.org/abs/2412.09600v1
HeartBeat: Towards Controllable Echocardiography Video Synthesis with Multimodal Conditions-Guided Diffusion Models,"Echocardiography (ECHO) video is widely used for cardiac examination. In
clinical, this procedure heavily relies on operator experience, which needs
years of training and maybe the assistance of deep learning-based systems for
enhanced accuracy and efficiency. However, it is challenging since acquiring
sufficient customized data (e.g., abnormal cases) for novice training and deep
model development is clinically unrealistic. Hence, controllable ECHO video
synthesis is highly desirable. In this paper, we propose a novel
diffusion-based framework named HeartBeat towards controllable and
high-fidelity ECHO video synthesis. Our highlight is three-fold. First,
HeartBeat serves as a unified framework that enables perceiving multimodal
conditions simultaneously to guide controllable generation. Second, we
factorize the multimodal conditions into local and global ones, with two
insertion strategies separately provided fine- and coarse-grained controls in a
composable and flexible manner. In this way, users can synthesize ECHO videos
that conform to their mental imagery by combining multimodal control signals.
Third, we propose to decouple the visual concepts and temporal dynamics
learning using a two-stage training scheme for simplifying the model training.
One more interesting thing is that HeartBeat can easily generalize to
mask-guided cardiac MRI synthesis in a few shots, showcasing its scalability to
broader applications. Extensive experiments on two public datasets show the
efficacy of the proposed HeartBeat.",2024-06-20 08:24:28+00:00,"['Xinrui Zhou', 'Yuhao Huang', 'Wufeng Xue', 'Haoran Dou', 'Jun Cheng', 'Han Zhou', 'Dong Ni']",http://arxiv.org/abs/2406.14098v2
Training-free Camera Control for Video Generation,"We propose a training-free and robust solution to offer camera movement
control for off-the-shelf video diffusion models. Unlike previous work, our
method does not require any supervised finetuning on camera-annotated datasets
or self-supervised training via data augmentation. Instead, it can be
plug-and-play with most pretrained video diffusion models and generate
camera-controllable videos with a single image or text prompt as input. The
inspiration for our work comes from the layout prior that intermediate latents
encode for the generated results, thus rearranging noisy pixels in them will
cause the output content to relocate as well. As camera moving could also be
seen as a type of pixel rearrangement caused by perspective change, videos can
be reorganized following specific camera motion if their noisy latents change
accordingly. Building on this, we propose CamTrol, which enables robust camera
control for video diffusion models. It is achieved by a two-stage process.
First, we model image layout rearrangement through explicit camera movement in
3D point cloud space. Second, we generate videos with camera motion by
leveraging the layout prior of noisy latents formed by a series of rearranged
images. Extensive experiments have demonstrated its superior performance in
both video generation and camera motion alignment compared with other finetuned
methods. Furthermore, we show the capability of CamTrol to generalize to
various base models, as well as its impressive applications in scalable motion
control, dealing with complicated trajectories and unsupervised 3D video
generation. Videos available at https://lifedecoder.github.io/CamTrol/.",2024-06-14 15:33:00+00:00,"['Chen Hou', 'Zhibo Chen']",http://arxiv.org/abs/2406.10126v4
Image Conductor: Precision Control for Interactive Video Synthesis,"Filmmaking and animation production often require sophisticated techniques
for coordinating camera transitions and object movements, typically involving
labor-intensive real-world capturing. Despite advancements in generative AI for
video creation, achieving precise control over motion for interactive video
asset generation remains challenging. To this end, we propose Image Conductor,
a method for precise control of camera transitions and object movements to
generate video assets from a single image. An well-cultivated training strategy
is proposed to separate distinct camera and object motion by camera LoRA
weights and object LoRA weights. To further address cinematographic variations
from ill-posed trajectories, we introduce a camera-free guidance technique
during inference, enhancing object movements while eliminating camera
transitions. Additionally, we develop a trajectory-oriented video motion data
curation pipeline for training. Quantitative and qualitative experiments
demonstrate our method's precision and fine-grained control in generating
motion-controllable videos from images, advancing the practical application of
interactive video synthesis. Project webpage available at
https://liyaowei-stu.github.io/project/ImageConductor/",2024-06-21 17:55:05+00:00,"['Yaowei Li', 'Xintao Wang', 'Zhaoyang Zhang', 'Zhouxia Wang', 'Ziyang Yuan', 'Liangbin Xie', 'Yuexian Zou', 'Ying Shan']",http://arxiv.org/abs/2406.15339v1
VIMI: Grounding Video Generation through Multi-modal Instruction,"Existing text-to-video diffusion models rely solely on text-only encoders for
their pretraining. This limitation stems from the absence of large-scale
multimodal prompt video datasets, resulting in a lack of visual grounding and
restricting their versatility and application in multimodal integration. To
address this, we construct a large-scale multimodal prompt dataset by employing
retrieval methods to pair in-context examples with the given text prompts and
then utilize a two-stage training strategy to enable diverse video generation
tasks within the same model. In the first stage, we propose a multimodal
conditional video generation framework for pretraining on these augmented
datasets, establishing a foundational model for grounded video generation.
Secondly, we finetune the model from the first stage on three video generation
tasks, incorporating multi-modal instructions. This process further refines the
model's ability to handle diverse inputs and tasks, ensuring seamless
integration of multi-modal information. After this two-stage train-ing process,
VIMI demonstrates multimodal understanding capabilities, producing contextually
rich and personalized videos grounded in the provided inputs, as shown in
Figure 1. Compared to previous visual grounded video generation methods, VIMI
can synthesize consistent and temporally coherent videos with large motion
while retaining the semantic control. Lastly, VIMI also achieves
state-of-the-art text-to-video generation results on UCF101 benchmark.",2024-07-08 18:12:49+00:00,"['Yuwei Fang', 'Willi Menapace', 'Aliaksandr Siarohin', 'Tsai-Shien Chen', 'Kuan-Chien Wang', 'Ivan Skorokhodov', 'Graham Neubig', 'Sergey Tulyakov']",http://arxiv.org/abs/2407.06304v1
The Tug-of-War Between Deepfake Generation and Detection,"Multimodal generative models are rapidly evolving, leading to a surge in the
generation of realistic video and audio that offers exciting possibilities but
also serious risks. Deepfake videos, which can convincingly impersonate
individuals, have particularly garnered attention due to their potential misuse
in spreading misinformation and creating fraudulent content. This survey paper
examines the dual landscape of deepfake video generation and detection,
emphasizing the need for effective countermeasures against potential abuses. We
provide a comprehensive overview of current deepfake generation techniques,
including face swapping, reenactment, and audio-driven animation, which
leverage cutting-edge technologies like GANs and diffusion models to produce
highly realistic fake videos. Additionally, we analyze various detection
approaches designed to differentiate authentic from altered videos, from
detecting visual artifacts to deploying advanced algorithms that pinpoint
inconsistencies across video and audio signals.
  The effectiveness of these detection methods heavily relies on the diversity
and quality of datasets used for training and evaluation. We discuss the
evolution of deepfake datasets, highlighting the importance of robust, diverse,
and frequently updated collections to enhance the detection accuracy and
generalizability. As deepfakes become increasingly indistinguishable from
authentic content, developing advanced detection techniques that can keep pace
with generation technologies is crucial. We advocate for a proactive approach
in the ""tug-of-war"" between deepfake creators and detectors, emphasizing the
need for continuous research collaboration, standardization of evaluation
metrics, and the creation of comprehensive benchmarks.",2024-07-08 17:49:41+00:00,"['Hannah Lee', 'Changyeon Lee', 'Kevin Farhat', 'Lin Qiu', 'Steve Geluso', 'Aerin Kim', 'Oren Etzioni']",http://arxiv.org/abs/2407.06174v4
VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers,"Video try-on stands as a promising area for its tremendous real-world
potential. Prior works are limited to transferring product clothing images onto
person videos with simple poses and backgrounds, while underperforming on
casually captured videos. Recently, Sora revealed the scalability of Diffusion
Transformer (DiT) in generating lifelike videos featuring real-world scenarios.
Inspired by this, we explore and propose the first DiT-based video try-on
framework for practical in-the-wild applications, named VITON-DiT.
Specifically, VITON-DiT consists of a garment extractor, a Spatial-Temporal
denoising DiT, and an identity preservation ControlNet. To faithfully recover
the clothing details, the extracted garment features are fused with the
self-attention outputs of the denoising DiT and the ControlNet. We also
introduce novel random selection strategies during training and an Interpolated
Auto-Regressive (IAR) technique at inference to facilitate long video
generation. Unlike existing attempts that require the laborious and restrictive
construction of a paired training dataset, severely limiting their scalability,
VITON-DiT alleviates this by relying solely on unpaired human dance videos and
a carefully designed multi-stage training strategy. Furthermore, we curate a
challenging benchmark dataset to evaluate the performance of casual video
try-on. Extensive experiments demonstrate the superiority of VITON-DiT in
generating spatio-temporal consistent try-on results for in-the-wild videos
with complicated human poses.",2024-05-28 16:21:03+00:00,"['Jun Zheng', 'Fuwei Zhao', 'Youjiang Xu', 'Xin Dong', 'Xiaodan Liang']",http://arxiv.org/abs/2405.18326v2
TC4D: Trajectory-Conditioned Text-to-4D Generation,"Recent techniques for text-to-4D generation synthesize dynamic 3D scenes
using supervision from pre-trained text-to-video models. However, existing
representations for motion, such as deformation models or time-dependent neural
representations, are limited in the amount of motion they can generate-they
cannot synthesize motion extending far beyond the bounding box used for volume
rendering. The lack of a more flexible motion model contributes to the gap in
realism between 4D generation methods and recent, near-photorealistic video
generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D
generation, which factors motion into global and local components. We represent
the global motion of a scene's bounding box using rigid transformation along a
trajectory parameterized by a spline. We learn local deformations that conform
to the global trajectory using supervision from a text-to-video model. Our
approach enables the synthesis of scenes animated along arbitrary trajectories,
compositional scene generation, and significant improvements to the realism and
amount of generated motion, which we evaluate qualitatively and through a user
study. Video results can be viewed on our website:
https://sherwinbahmani.github.io/tc4d.",2024-03-26 17:55:11+00:00,"['Sherwin Bahmani', 'Xian Liu', 'Wang Yifan', 'Ivan Skorokhodov', 'Victor Rong', 'Ziwei Liu', 'Xihui Liu', 'Jeong Joon Park', 'Sergey Tulyakov', 'Gordon Wetzstein', 'Andrea Tagliasacchi', 'David B. Lindell']",http://arxiv.org/abs/2403.17920v3
Generative Video Propagation,"Large-scale video generation models have the inherent ability to
realistically model natural scenes. In this paper, we demonstrate that through
a careful design of a generative video propagation framework, various video
tasks can be addressed in a unified way by leveraging the generative power of
such models. Specifically, our framework, GenProp, encodes the original video
with a selective content encoder and propagates the changes made to the first
frame using an image-to-video generation model. We propose a data generation
scheme to cover multiple video tasks based on instance-level video segmentation
datasets. Our model is trained by incorporating a mask prediction decoder head
and optimizing a region-aware loss to aid the encoder to preserve the original
content while the generation model propagates the modified region. This novel
design opens up new possibilities: In editing scenarios, GenProp allows
substantial changes to an object's shape; for insertion, the inserted objects
can exhibit independent motion; for removal, GenProp effectively removes
effects like shadows and reflections from the whole video; for tracking,
GenProp is capable of tracking objects and their associated effects together.
Experiment results demonstrate the leading performance of our model in various
video tasks, and we further provide in-depth analyses of the proposed
framework.",2024-12-27 17:42:29+00:00,"['Shaoteng Liu', 'Tianyu Wang', 'Jui-Hsien Wang', 'Qing Liu', 'Zhifei Zhang', 'Joon-Young Lee', 'Yijun Li', 'Bei Yu', 'Zhe Lin', 'Soo Ye Kim', 'Jiaya Jia']",http://arxiv.org/abs/2412.19761v1
OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation,"Tokenizer, serving as a translator to map the intricate visual data into a
compact latent space, lies at the core of visual generative models. Based on
the finding that existing tokenizers are tailored to image or video inputs,
this paper presents OmniTokenizer, a transformer-based tokenizer for joint
image and video tokenization. OmniTokenizer is designed with a spatial-temporal
decoupled architecture, which integrates window and causal attention for
spatial and temporal modeling. To exploit the complementary nature of image and
video data, we further propose a progressive training strategy, where
OmniTokenizer is first trained on image data on a fixed resolution to develop
the spatial encoding capacity and then jointly trained on image and video data
on multiple resolutions to learn the temporal dynamics. OmniTokenizer, for the
first time, handles both image and video inputs within a unified framework and
proves the possibility of realizing their synergy. Extensive experiments
demonstrate that OmniTokenizer achieves state-of-the-art (SOTA) reconstruction
performance on various image and video datasets, e.g., 1.11 reconstruction FID
on ImageNet and 42 reconstruction FVD on UCF-101, beating the previous SOTA
methods by 13% and 26%, respectively. Additionally, we also show that when
integrated with OmniTokenizer, both language model-based approaches and
diffusion models can realize advanced visual synthesis performance,
underscoring the superiority and versatility of our method. Code is available
at https://github.com/FoundationVision/OmniTokenizer.",2024-06-13 17:59:26+00:00,"['Junke Wang', 'Yi Jiang', 'Zehuan Yuan', 'Binyue Peng', 'Zuxuan Wu', 'Yu-Gang Jiang']",http://arxiv.org/abs/2406.09399v1
MAVIN: Multi-Action Video Generation with Diffusion Models via Transition Video Infilling,"Diffusion-based video generation has achieved significant progress, yet
generating multiple actions that occur sequentially remains a formidable task.
Directly generating a video with sequential actions can be extremely
challenging due to the scarcity of fine-grained action annotations and the
difficulty in establishing temporal semantic correspondences and maintaining
long-term consistency. To tackle this, we propose an intuitive and
straightforward solution: splicing multiple single-action video segments
sequentially. The core challenge lies in generating smooth and natural
transitions between these segments given the inherent complexity and
variability of action transitions. We introduce MAVIN (Multi-Action Video
INfilling model), designed to generate transition videos that seamlessly
connect two given videos, forming a cohesive integrated sequence. MAVIN
incorporates several innovative techniques to address challenges in the
transition video infilling task. Firstly, a consecutive noising strategy
coupled with variable-length sampling is employed to handle large infilling
gaps and varied generation lengths. Secondly, boundary frame guidance (BFG) is
proposed to address the lack of semantic guidance during transition generation.
Lastly, a Gaussian filter mixer (GFM) dynamically manages noise initialization
during inference, mitigating train-test discrepancy while preserving generation
flexibility. Additionally, we introduce a new metric, CLIP-RS (CLIP Relative
Smoothness), to evaluate temporal coherence and smoothness, complementing
traditional quality-based metrics. Experimental results on horse and tiger
scenarios demonstrate MAVIN's superior performance in generating smooth and
coherent video transitions compared to existing methods.",2024-05-28 09:46:09+00:00,"['Bowen Zhang', 'Xiaofei Xie', 'Haotian Lu', 'Na Ma', 'Tianlin Li', 'Qing Guo']",http://arxiv.org/abs/2405.18003v1
Bora: Biomedical Generalist Video Generation Model,"Generative models hold promise for revolutionizing medical education,
robot-assisted surgery, and data augmentation for medical AI development.
Diffusion models can now generate realistic images from text prompts, while
recent advancements have demonstrated their ability to create diverse,
high-quality videos. However, these models often struggle with generating
accurate representations of medical procedures and detailed anatomical
structures. This paper introduces Bora, the first spatio-temporal diffusion
probabilistic model designed for text-guided biomedical video generation. Bora
leverages Transformer architecture and is pre-trained on general-purpose video
generation tasks. It is fine-tuned through model alignment and instruction
tuning using a newly established medical video corpus, which includes paired
text-video data from various biomedical fields. To the best of our knowledge,
this is the first attempt to establish such a comprehensive annotated
biomedical video dataset. Bora is capable of generating high-quality video data
across four distinct biomedical domains, adhering to medical expert standards
and demonstrating consistency and diversity. This generalist video generative
model holds significant potential for enhancing medical consultation and
decision-making, particularly in resource-limited settings. Additionally, Bora
could pave the way for immersive medical training and procedure planning.
Extensive experiments on distinct medical modalities such as endoscopy,
ultrasound, MRI, and cell tracking validate the effectiveness of our model in
understanding biomedical instructions and its superior performance across
subjects compared to state-of-the-art generation models.",2024-07-12 03:00:25+00:00,"['Weixiang Sun', 'Xiaocao You', 'Ruizhe Zheng', 'Zhengqing Yuan', 'Xiang Li', 'Lifang He', 'Quanzheng Li', 'Lichao Sun']",http://arxiv.org/abs/2407.08944v2
DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models,"Videos are inherently temporal sequences by their very nature. In this work,
we explore the potential of modeling videos in a chronological and scalable
manner with autoregressive (AR) language models, inspired by their success in
natural language processing. We introduce DiCoDe, a novel approach that
leverages Diffusion-Compressed Deep Tokens to generate videos with a language
model in an autoregressive manner. Unlike existing methods that employ
low-level representations with limited compression rates, DiCoDe utilizes deep
tokens with a considerable compression rate (a 1000x reduction in token count).
This significant compression is made possible by a tokenizer trained through
leveraging the prior knowledge of video diffusion models. Deep tokens enable
DiCoDe to employ vanilla AR language models for video generation, akin to
translating one visual ""language"" into another. By treating videos as temporal
sequences, DiCoDe fully harnesses the capabilities of language models for
autoregressive generation. DiCoDe is scalable using readily available AR
architectures, and is capable of generating videos ranging from a few seconds
to one minute using only 4 A100 GPUs for training. We evaluate DiCoDe both
quantitatively and qualitatively, demonstrating that it performs comparably to
existing methods in terms of quality while ensuring efficient training. To
showcase its scalability, we release a series of DiCoDe configurations with
varying parameter sizes and observe a consistent improvement in performance as
the model size increases from 100M to 3B. We believe that DiCoDe's exploration
in academia represents a promising initial step toward scalable video modeling
with AR language models, paving the way for the development of larger and more
powerful video generation models.",2024-12-05 18:57:06+00:00,"['Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ping Luo', 'Ying Shan']",http://arxiv.org/abs/2412.04446v1
CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation,"Recently video diffusion models have emerged as expressive generative tools
for high-quality video content creation readily available to general users.
However, these models often do not offer precise control over camera poses for
video generation, limiting the expression of cinematic language and user
control. To address this issue, we introduce CamCo, which allows fine-grained
Camera pose Control for image-to-video generation. We equip a pre-trained
image-to-video generator with accurately parameterized camera pose input using
Pl\""ucker coordinates. To enhance 3D consistency in the videos produced, we
integrate an epipolar attention module in each attention block that enforces
epipolar constraints to the feature maps. Additionally, we fine-tune CamCo on
real-world videos with camera poses estimated through structure-from-motion
algorithms to better synthesize object motion. Our experiments show that CamCo
significantly improves 3D consistency and camera control capabilities compared
to previous models while effectively generating plausible object motion.
Project page: https://ir1d.github.io/CamCo/",2024-06-04 17:27:19+00:00,"['Dejia Xu', 'Weili Nie', 'Chao Liu', 'Sifei Liu', 'Jan Kautz', 'Zhangyang Wang', 'Arash Vahdat']",http://arxiv.org/abs/2406.02509v1
LumiSculpt: A Consistency Lighting Control Network for Video Generation,"Lighting plays a pivotal role in ensuring the naturalness of video
generation, significantly influencing the aesthetic quality of the generated
content. However, due to the deep coupling between lighting and the temporal
features of videos, it remains challenging to disentangle and model independent
and coherent lighting attributes, limiting the ability to control lighting in
video generation. In this paper, inspired by the established controllable T2I
models, we propose LumiSculpt, which, for the first time, enables precise and
consistent lighting control in T2V generation models.LumiSculpt equips the
video generation with strong interactive capabilities, allowing the input of
custom lighting reference image sequences. Furthermore, the core learnable
plug-and-play module of LumiSculpt facilitates remarkable control over lighting
intensity, position, and trajectory in latent video diffusion models based on
the advanced DiT backbone.Additionally, to effectively train LumiSculpt and
address the issue of insufficient lighting data, we construct LumiHuman, a new
lightweight and flexible dataset for portrait lighting of images and videos.
Experimental results demonstrate that LumiSculpt achieves precise and
high-quality lighting control in video generation.",2024-10-30 12:44:08+00:00,"['Yuxin Zhang', 'Dandan Zheng', 'Biao Gong', 'Jingdong Chen', 'Ming Yang', 'Weiming Dong', 'Changsheng Xu']",http://arxiv.org/abs/2410.22979v1
Exposing AI-generated Videos: A Benchmark Dataset and a Local-and-Global Temporal Defect Based Detection Method,"The generative model has made significant advancements in the creation of
realistic videos, which causes security issues. However, this emerging risk has
not been adequately addressed due to the absence of a benchmark dataset for
AI-generated videos. In this paper, we first construct a video dataset using
advanced diffusion-based video generation algorithms with various semantic
contents. Besides, typical video lossy operations over network transmission are
adopted to generate degraded samples. Then, by analyzing local and global
temporal defects of current AI-generated videos, a novel detection framework by
adaptively learning local motion information and global appearance variation is
constructed to expose fake videos. Finally, experiments are conducted to
evaluate the generalization and robustness of different spatial and temporal
domain detection methods, where the results can serve as the baseline and
demonstrate the research challenge for future studies.",2024-05-07 09:00:09+00:00,"['Peisong He', 'Leyao Zhu', 'Jiaxing Li', 'Shiqi Wang', 'Haoliang Li']",http://arxiv.org/abs/2405.04133v1
Shaping a Stabilized Video by Mitigating Unintended Changes for Concept-Augmented Video Editing,"Text-driven video editing utilizing generative diffusion models has garnered
significant attention due to their potential applications. However, existing
approaches are constrained by the limited word embeddings provided in
pre-training, which hinders nuanced editing targeting open concepts with
specific attributes. Directly altering the keywords in target prompts often
results in unintended disruptions to the attention mechanisms. To achieve more
flexible editing easily, this work proposes an improved concept-augmented video
editing approach that generates diverse and stable target videos flexibly by
devising abstract conceptual pairs. Specifically, the framework involves
concept-augmented textual inversion and a dual prior supervision mechanism. The
former enables plug-and-play guidance of stable diffusion for video editing,
effectively capturing target attributes for more stylized results. The dual
prior supervision mechanism significantly enhances video stability and
fidelity. Comprehensive evaluations demonstrate that our approach generates
more stable and lifelike videos, outperforming state-of-the-art methods.",2024-10-16 13:03:15+00:00,"['Mingce Guo', 'Jingxuan He', 'Shengeng Tang', 'Zhangye Wang', 'Lechao Cheng']",http://arxiv.org/abs/2410.12526v1
ImmersePro: End-to-End Stereo Video Synthesis Via Implicit Disparity Learning,"We introduce \textit{ImmersePro}, an innovative framework specifically
designed to transform single-view videos into stereo videos. This framework
utilizes a novel dual-branch architecture comprising a disparity branch and a
context branch on video data by leveraging spatial-temporal attention
mechanisms. \textit{ImmersePro} employs implicit disparity guidance, enabling
the generation of stereo pairs from video sequences without the need for
explicit disparity maps, thus reducing potential errors associated with
disparity estimation models. In addition to the technical advancements, we
introduce the YouTube-SBS dataset, a comprehensive collection of 423 stereo
videos sourced from YouTube. This dataset is unprecedented in its scale,
featuring over 7 million stereo pairs, and is designed to facilitate training
and benchmarking of stereo video generation models. Our experiments demonstrate
the effectiveness of \textit{ImmersePro} in producing high-quality stereo
videos, offering significant improvements over existing methods. Compared to
the best competitor stereo-from-mono we quantitatively improve the results by
11.76\% (L1), 6.39\% (SSIM), and 5.10\% (PSNR).",2024-09-30 22:19:32+00:00,"['Jian Shi', 'Zhenyu Li', 'Peter Wonka']",http://arxiv.org/abs/2410.00262v1
SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos,"Video-based visual relation detection tasks, such as video scene graph
generation, play important roles in fine-grained video understanding. However,
current video visual relation detection datasets have two main limitations that
hinder the progress of research in this area. First, they do not explore
complex human-human interactions in multi-person scenarios. Second, the
relation types of existing datasets have relatively low-level semantics and can
be often recognized by appearance or simple prior information, without the need
for detailed spatio-temporal context reasoning. Nevertheless, comprehending
high-level interactions between humans is crucial for understanding complex
multi-person videos, such as sports and surveillance videos. To address this
issue, we propose a new video visual relation detection task: video human-human
interaction detection, and build a dataset named SportsHHI for it. SportsHHI
contains 34 high-level interaction classes from basketball and volleyball
sports. 118,075 human bounding boxes and 50,649 interaction instances are
annotated on 11,398 keyframes. To benchmark this, we propose a two-stage
baseline method and conduct extensive experiments to reveal the key factors for
a successful human-human interaction detector. We hope that SportsHHI can
stimulate research on human interaction understanding in videos and promote the
development of spatio-temporal context modeling techniques in video visual
relation detection.",2024-04-06 09:13:03+00:00,"['Tao Wu', 'Runyu He', 'Gangshan Wu', 'Limin Wang']",http://arxiv.org/abs/2404.04565v1
Zero-Shot Surgical Tool Segmentation in Monocular Video Using Segment Anything Model 2,"The Segment Anything Model 2 (SAM 2) is the latest generation foundation
model for image and video segmentation. Trained on the expansive Segment
Anything Video (SA-V) dataset, which comprises 35.5 million masks across 50.9K
videos, SAM 2 advances its predecessor's capabilities by supporting zero-shot
segmentation through various prompts (e.g., points, boxes, and masks). Its
robust zero-shot performance and efficient memory usage make SAM 2 particularly
appealing for surgical tool segmentation in videos, especially given the
scarcity of labeled data and the diversity of surgical procedures. In this
study, we evaluate the zero-shot video segmentation performance of the SAM 2
model across different types of surgeries, including endoscopy and microscopy.
We also assess its performance on videos featuring single and multiple tools of
varying lengths to demonstrate SAM 2's applicability and effectiveness in the
surgical domain. We found that: 1) SAM 2 demonstrates a strong capability for
segmenting various surgical videos; 2) When new tools enter the scene,
additional prompts are necessary to maintain segmentation accuracy; and 3)
Specific challenges inherent to surgical videos can impact the robustness of
SAM 2.",2024-08-03 03:19:56+00:00,"['Ange Lou', 'Yamin Li', 'Yike Zhang', 'Robert F. Labadie', 'Jack Noble']",http://arxiv.org/abs/2408.01648v1
Noise Crystallization and Liquid Noise: Zero-shot Video Generation using Image Diffusion Models,"Although powerful for image generation, consistent and controllable video is
a longstanding problem for diffusion models. Video models require extensive
training and computational resources, leading to high costs and large
environmental impacts. Moreover, video models currently offer limited control
of the output motion. This paper introduces a novel approach to video
generation by augmenting image diffusion models to create sequential animation
frames while maintaining fine detail. These techniques can be applied to
existing image models without training any video parameters (zero-shot) by
altering the input noise in a latent diffusion model. Two complementary methods
are presented. Noise crystallization ensures consistency but is limited to
large movements due to reduced latent embedding sizes. Liquid noise trades
consistency for greater flexibility without resolution limitations. The core
concepts also allow other applications such as relighting, seamless upscaling,
and improved video style transfer. Furthermore, an exploration of the VAE
embedding used for latent diffusion models is performed, resulting in
interesting theoretical insights such as a method for human-interpretable
latent spaces.",2024-10-05 12:53:05+00:00,"['Muhammad Haaris Khan', 'Hadrien Reynaud', 'Bernhard Kainz']",http://arxiv.org/abs/2410.05322v1
LatentColorization: Latent Diffusion-Based Speaker Video Colorization,"While current research predominantly focuses on image-based colorization, the
domain of video-based colorization remains relatively unexplored. Most existing
video colorization techniques operate on a frame-by-frame basis, often
overlooking the critical aspect of temporal coherence between successive
frames. This approach can result in inconsistencies across frames, leading to
undesirable effects like flickering or abrupt color transitions between frames.
To address these challenges, we harness the generative capabilities of a
fine-tuned latent diffusion model designed specifically for video colorization,
introducing a novel solution for achieving temporal consistency in video
colorization, as well as demonstrating strong improvements on established image
quality metrics compared to other existing methods. Furthermore, we perform a
subjective study, where users preferred our approach to the existing state of
the art. Our dataset encompasses a combination of conventional datasets and
videos from television/movies. In short, by leveraging the power of a
fine-tuned latent diffusion-based colorization system with a temporal
consistency mechanism, we can improve the performance of automatic video
colorization by addressing the challenges of temporal inconsistency. A short
demonstration of our results can be seen in some example videos available at
https://youtu.be/vDbzsZdFuxM.",2024-05-09 12:06:06+00:00,"['Rory Ward', 'Dan Bigioi', 'Shubhajit Basak', 'John G. Breslin', 'Peter Corcoran']",http://arxiv.org/abs/2405.05707v1
Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models,"Large Language Models have shown remarkable efficacy in generating streaming
data such as text and audio, thanks to their temporally uni-directional
attention mechanism, which models correlations between the current token and
previous tokens. However, video streaming remains much less explored, despite a
growing need for live video processing. State-of-the-art video diffusion models
leverage bi-directional temporal attention to model the correlations between
the current frame and all the surrounding (i.e. including future) frames, which
hinders them from processing streaming videos. To address this problem, we
present Live2Diff, the first attempt at designing a video diffusion model with
uni-directional temporal attention, specifically targeting live streaming video
translation. Compared to previous works, our approach ensures temporal
consistency and smoothness by correlating the current frame with its
predecessors and a few initial warmup frames, without any future frames.
Additionally, we use a highly efficient denoising scheme featuring a KV-cache
mechanism and pipelining, to facilitate streaming video translation at
interactive framerates. Extensive experiments demonstrate the effectiveness of
the proposed attention mechanism and pipeline, outperforming previous methods
in terms of temporal smoothness and/or efficiency.",2024-07-11 17:34:51+00:00,"['Zhening Xing', 'Gereon Fox', 'Yanhong Zeng', 'Xingang Pan', 'Mohamed Elgharib', 'Christian Theobalt', 'Kai Chen']",http://arxiv.org/abs/2407.08701v1
Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models,"Generating lifelike 3D humans from a single RGB image remains a challenging
task in computer vision, as it requires accurate modeling of geometry,
high-quality texture, and plausible unseen parts. Existing methods typically
use multi-view diffusion models for 3D generation, but they often face
inconsistent view issues, which hinder high-quality 3D human generation. To
address this, we propose Human-VDM, a novel method for generating 3D human from
a single RGB image using Video Diffusion Models. Human-VDM provides temporally
consistent views for 3D human generation using Gaussian Splatting. It consists
of three modules: a view-consistent human video diffusion module, a video
augmentation module, and a Gaussian Splatting module. First, a single image is
fed into a human video diffusion module to generate a coherent human video.
Next, the video augmentation module applies super-resolution and video
interpolation to enhance the textures and geometric smoothness of the generated
video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans
under the guidance of these high-resolution and view-consistent images.
Experiments demonstrate that Human-VDM achieves high-quality 3D human from a
single image, outperforming state-of-the-art methods in both generation quality
and quantity. Project page: https://human-vdm.github.io/Human-VDM/",2024-09-04 16:21:33+00:00,"['Zhibin Liu', 'Haoye Dong', 'Aviral Chharia', 'Hefeng Wu']",http://arxiv.org/abs/2409.02851v1
Video2BEV: Transforming Drone Videos to BEVs for Video-based Geo-localization,"Existing approaches to drone visual geo-localization predominantly adopt the
image-based setting, where a single drone-view snapshot is matched with images
from other platforms. Such task formulation, however, underutilizes the
inherent video output of the drone and is sensitive to occlusions and viewpoint
disparity. To address these limitations, we formulate a new video-based drone
geo-localization task and propose the Video2BEV paradigm. This paradigm
transforms the video into a Bird's Eye View (BEV), simplifying the subsequent
\textbf{inter-platform} matching process. In particular, we employ Gaussian
Splatting to reconstruct a 3D scene and obtain the BEV projection. Different
from the existing transform methods, \eg, polar transform, our BEVs preserve
more fine-grained details without significant distortion. To facilitate the
discriminative \textbf{intra-platform} representation learning, our Video2BEV
paradigm also incorporates a diffusion-based module for generating hard
negative samples. To validate our approach, we introduce UniV, a new
video-based geo-localization dataset that extends the image-based
University-1652 dataset. UniV features flight paths at $30^\circ$ and
$45^\circ$ elevation angles with increased frame rates of up to 10 frames per
second (FPS). Extensive experiments on the UniV dataset show that our Video2BEV
paradigm achieves competitive recall rates and outperforms conventional
video-based methods. Compared to other competitive methods, our proposed
approach exhibits robustness at lower elevations with more occlusions.",2024-11-20 01:52:49+00:00,"['Hao Ju', 'Shaofei Huang', 'Si Liu', 'Zhedong Zheng']",http://arxiv.org/abs/2411.13610v2
EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture,"This paper presents EasyAnimate, an advanced method for video generation that
leverages the power of transformer architecture for high-performance outcomes.
We have expanded the DiT framework originally designed for 2D image synthesis
to accommodate the complexities of 3D video generation by incorporating a
motion module block. It is used to capture temporal dynamics, thereby ensuring
the production of consistent frames and seamless motion transitions. The motion
module can be adapted to various DiT baseline methods to generate video with
different styles. It can also generate videos with different frame rates and
resolutions during both training and inference phases, suitable for both images
and videos. Moreover, we introduce slice VAE, a novel approach to condense the
temporal axis, facilitating the generation of long duration videos. Currently,
EasyAnimate exhibits the proficiency to generate videos with 144 frames. We
provide a holistic ecosystem for video production based on DiT, encompassing
aspects such as data pre-processing, VAE training, DiT models training (both
the baseline model and LoRA model), and end-to-end video inference. Code is
available at: https://github.com/aigc-apps/EasyAnimate. We are continuously
working to enhance the performance of our method.",2024-05-29 11:11:07+00:00,"['Jiaqi Xu', 'Xinyi Zou', 'Kunzhe Huang', 'Yunkuo Chen', 'Bo Liu', 'MengLi Cheng', 'Xing Shi', 'Jun Huang']",http://arxiv.org/abs/2405.18991v2
Hierarchical Patch Diffusion Models for High-Resolution Video Generation,"Diffusion models have demonstrated remarkable performance in image and video
synthesis. However, scaling them to high-resolution inputs is challenging and
requires restructuring the diffusion pipeline into multiple independent
components, limiting scalability and complicating downstream applications. This
makes it very efficient during training and unlocks end-to-end optimization on
high-resolution videos. We improve PDMs in two principled ways. First, to
enforce consistency between patches, we develop deep context fusion -- an
architectural technique that propagates the context information from low-scale
to high-scale patches in a hierarchical manner. Second, to accelerate training
and inference, we propose adaptive computation, which allocates more network
capacity and computation towards coarse image details. The resulting model sets
a new state-of-the-art FVD score of 66.32 and Inception Score of 87.68 in
class-conditional video generation on UCF-101 $256^2$, surpassing recent
methods by more than 100%. Then, we show that it can be rapidly fine-tuned from
a base $36\times 64$ low-resolution generator for high-resolution $64 \times
288 \times 512$ text-to-video synthesis. To the best of our knowledge, our
model is the first diffusion-based architecture which is trained on such high
resolutions entirely end-to-end. Project webpage:
https://snap-research.github.io/hpdm.",2024-06-12 01:12:53+00:00,"['Ivan Skorokhodov', 'Willi Menapace', 'Aliaksandr Siarohin', 'Sergey Tulyakov']",http://arxiv.org/abs/2406.07792v1
Diving Deep into the Motion Representation of Video-Text Models,"Videos are more informative than images because they capture the dynamics of
the scene. By representing motion in videos, we can capture dynamic activities.
In this work, we introduce GPT-4 generated motion descriptions that capture
fine-grained motion descriptions of activities and apply them to three action
datasets. We evaluated several video-text models on the task of retrieval of
motion descriptions. We found that they fall far behind human expert
performance on two action datasets, raising the question of whether video-text
models understand motion in videos. To address it, we introduce a method of
improving motion understanding in video-text models by utilizing motion
descriptions. This method proves to be effective on two action datasets for the
motion description retrieval task. The results draw attention to the need for
quality captions involving fine-grained motion information in existing datasets
and demonstrate the effectiveness of the proposed pipeline in understanding
fine-grained motion during video-text retrieval.",2024-06-07 16:46:10+00:00,"['Chinmaya Devaraj', 'Cornelia Fermuller', 'Yiannis Aloimonos']",http://arxiv.org/abs/2406.05075v1
A Toolchain for Comprehensive Audio/Video Analysis Using Deep Learning Based Multimodal Approach (A use case of riot or violent context detection),"In this paper, we present a toolchain for a comprehensive audio/video
analysis by leveraging deep learning based multimodal approach. To this end,
different specific tasks of Speech to Text (S2T), Acoustic Scene Classification
(ASC), Acoustic Event Detection (AED), Visual Object Detection (VOD), Image
Captioning (IC), and Video Captioning (VC) are conducted and integrated into
the toolchain. By combining individual tasks and analyzing both audio \& visual
data extracted from input video, the toolchain offers various audio/video-based
applications: Two general applications of audio/video clustering, comprehensive
audio/video summary and a specific application of riot or violent context
detection. Furthermore, the toolchain presents a flexible and adaptable
architecture that is effective to integrate new models for further
audio/video-based applications.",2024-05-02 07:34:31+00:00,"['Lam Pham', 'Phat Lam', 'Tin Nguyen', 'Hieu Tang', 'Alexander Schindler']",http://arxiv.org/abs/2407.03110v1
Multi-Granularity Video Object Segmentation,"Current benchmarks for video segmentation are limited to annotating only
salient objects (i.e., foreground instances). Despite their impressive
architectural designs, previous works trained on these benchmarks have
struggled to adapt to real-world scenarios. Thus, developing a new video
segmentation dataset aimed at tracking multi-granularity segmentation target in
the video scene is necessary. In this work, we aim to generate
multi-granularity video segmentation dataset that is annotated for both salient
and non-salient masks. To achieve this, we propose a large-scale, densely
annotated multi-granularity video object segmentation (MUG-VOS) dataset that
includes various types and granularities of mask annotations. We automatically
collected a training set that assists in tracking both salient and non-salient
objects, and we also curated a human-annotated test set for reliable
evaluation. In addition, we present memory-based mask propagation model (MMPM),
trained and evaluated on MUG-VOS dataset, which leads to the best performance
among the existing video object segmentation methods and Segment SAM-based
video segmentation methods. Project page is available at
https://cvlab-kaist.github.io/MUG-VOS.",2024-12-02 13:17:41+00:00,"['Sangbeom Lim', 'Seongchan Kim', 'Seungjun An', 'Seokju Cho', 'Paul Hongsuck Seo', 'Seungryong Kim']",http://arxiv.org/abs/2412.01471v2
Motion Control for Enhanced Complex Action Video Generation,"Existing text-to-video (T2V) models often struggle with generating videos
with sufficiently pronounced or complex actions. A key limitation lies in the
text prompt's inability to precisely convey intricate motion details. To
address this, we propose a novel framework, MVideo, designed to produce
long-duration videos with precise, fluid actions. MVideo overcomes the
limitations of text prompts by incorporating mask sequences as an additional
motion condition input, providing a clearer, more accurate representation of
intended actions. Leveraging foundational vision models such as GroundingDINO
and SAM2, MVideo automatically generates mask sequences, enhancing both
efficiency and robustness. Our results demonstrate that, after training, MVideo
effectively aligns text prompts with motion conditions to produce videos that
simultaneously meet both criteria. This dual control mechanism allows for more
dynamic video generation by enabling alterations to either the text prompt or
motion condition independently, or both in tandem. Furthermore, MVideo supports
motion condition editing and composition, facilitating the generation of videos
with more complex actions. MVideo thus advances T2V motion generation, setting
a strong benchmark for improved action depiction in current video diffusion
models. Our project page is available at https://mvideo-v1.github.io/.",2024-11-13 04:20:45+00:00,"['Qiang Zhou', 'Shaofeng Zhang', 'Nianzu Yang', 'Ye Qian', 'Hao Li']",http://arxiv.org/abs/2411.08328v1
MIMOSA: Human-AI Co-Creation of Computational Spatial Audio Effects on Videos,"Spatial audio offers more immersive video consumption experiences to viewers;
however, creating and editing spatial audio often expensive and requires
specialized equipment and skills, posing a high barrier for amateur video
creators. We present MIMOSA, a human-AI co-creation tool that enables amateur
users to computationally generate and manipulate spatial audio effects. For a
video with only monaural or stereo audio, MIMOSA automatically grounds each
sound source to the corresponding sounding object in the visual scene and
enables users to further validate and fix the errors in the locations of
sounding objects. Users can also augment the spatial audio effect by flexibly
manipulating the sounding source positions and creatively customizing the audio
effect. The design of MIMOSA exemplifies a human-AI collaboration approach
that, instead of utilizing state-of art end-to-end ""black-box"" ML models, uses
a multistep pipeline that aligns its interpretable intermediate results with
the user's workflow. A lab user study with 15 participants demonstrates
MIMOSA's usability, usefulness, expressiveness, and capability in creating
immersive spatial audio effects in collaboration with users.",2024-04-23 15:01:36+00:00,"['Zheng Ning', 'Zheng Zhang', 'Jerrick Ban', 'Kaiwen Jiang', 'Ruohong Gan', 'Yapeng Tian', 'Toby Jia-Jun Li']",http://arxiv.org/abs/2404.15107v1
Improving Multi-Center Generalizability of GAN-Based Fat Suppression using Federated Learning,"Generative Adversarial Network (GAN)-based synthesis of fat suppressed (FS)
MRIs from non-FS proton density sequences has the potential to accelerate
acquisition of knee MRIs. However, GANs trained on single-site data have poor
generalizability to external data. We show that federated learning can improve
multi-center generalizability of GANs for synthesizing FS MRIs, while
facilitating privacy-preserving multi-institutional collaborations.",2024-04-10 22:16:20+00:00,"['Pranav Kulkarni', 'Adway Kanhere', 'Harshita Kukreja', 'Vivian Zhang', 'Paul H. Yi', 'Vishwa S. Parekh']",http://arxiv.org/abs/2404.07374v1
Splatter a Video: Video Gaussian Representation for Versatile Processing,"Video representation is a long-standing problem that is crucial for various
down-stream tasks, such as tracking,depth prediction,segmentation,view
synthesis,and editing. However, current methods either struggle to model
complex motions due to the absence of 3D structure or rely on implicit 3D
representations that are ill-suited for manipulation tasks. To address these
challenges, we introduce a novel explicit 3D representation-video Gaussian
representation -- that embeds a video into 3D Gaussians. Our proposed
representation models video appearance in a 3D canonical space using explicit
Gaussians as proxies and associates each Gaussian with 3D motions for video
motion. This approach offers a more intrinsic and explicit representation than
layered atlas or volumetric pixel matrices. To obtain such a representation, we
distill 2D priors, such as optical flow and depth, from foundation models to
regularize learning in this ill-posed setting. Extensive applications
demonstrate the versatility of our new video representation. It has been proven
effective in numerous video processing tasks, including tracking, consistent
video depth and feature refinement, motion and appearance editing, and
stereoscopic video generation. Project page:
https://sunyangtian.github.io/spatter_a_video_web/",2024-06-19 22:20:03+00:00,"['Yang-Tian Sun', 'Yi-Hua Huang', 'Lin Ma', 'Xiaoyang Lyu', 'Yan-Pei Cao', 'Xiaojuan Qi']",http://arxiv.org/abs/2406.13870v2
Intention-driven Ego-to-Exo Video Generation,"Ego-to-exo video generation refers to generating the corresponding exocentric
video according to the egocentric video, providing valuable applications in
AR/VR and embodied AI. Benefiting from advancements in diffusion model
techniques, notable progress has been achieved in video generation. However,
existing methods build upon the spatiotemporal consistency assumptions between
adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to
drastic changes in views. To this end, this paper proposes an Intention-Driven
Ego-to-exo video generation framework (IDE) that leverages action intention
consisting of human movement and action description as view-independent
representation to guide video generation, preserving the consistency of content
and motion. Specifically, the egocentric head trajectory is first estimated
through multi-view stereo matching. Then, cross-view feature perception module
is introduced to establish correspondences between exo- and ego- views, guiding
the trajectory transformation module to infer human full-body movement from the
head trajectory. Meanwhile, we present an action description unit that maps the
action semantics into the feature space consistent with the exocentric image.
Finally, the inferred human movement and high-level action descriptions jointly
guide the generation of exocentric motion and interaction content (i.e.,
corresponding optical flow and occlusion maps) in the backward process of the
diffusion model, ultimately warping them into the corresponding exocentric
video. We conduct extensive experiments on the relevant dataset with diverse
exo-ego video pairs, and our IDE outperforms state-of-the-art models in both
subjective and objective assessments, demonstrating its efficacy in ego-to-exo
video generation.",2024-03-14 09:07:31+00:00,"['Hongchen Luo', 'Kai Zhu', 'Wei Zhai', 'Yang Cao']",http://arxiv.org/abs/2403.09194v2
Video Editing via Factorized Diffusion Distillation,"We introduce Emu Video Edit (EVE), a model that establishes a new
state-of-the art in video editing without relying on any supervised video
editing data. To develop EVE we separately train an image editing adapter and a
video generation adapter, and attach both to the same text-to-image model.
Then, to align the adapters towards video editing we introduce a new
unsupervised distillation procedure, Factorized Diffusion Distillation. This
procedure distills knowledge from one or more teachers simultaneously, without
any supervised data. We utilize this procedure to teach EVE to edit videos by
jointly distilling knowledge to (i) precisely edit each individual frame from
the image editing adapter, and (ii) ensure temporal consistency among the
edited frames using the video generation adapter. Finally, to demonstrate the
potential of our approach in unlocking other capabilities, we align additional
combinations of adapters",2024-03-14 12:22:54+00:00,"['Uriel Singer', 'Amit Zohar', 'Yuval Kirstain', 'Shelly Sheynin', 'Adam Polyak', 'Devi Parikh', 'Yaniv Taigman']",http://arxiv.org/abs/2403.09334v2
Tell What You Hear From What You See -- Video to Audio Generation Through Text,"The content of visual and audio scenes is multi-faceted such that a video can
be paired with various audio and vice-versa. Thereby, in video-to-audio
generation task, it is imperative to introduce steering approaches for
controlling the generated audio. While Video-to-Audio generation is a
well-established generative task, existing methods lack such controllability.
In this work, we propose VATT, a multi-modal generative framework that takes a
video and an optional text prompt as input, and generates audio and optional
textual description of the audio. Such a framework has two advantages: i)
Video-to-Audio generation process can be refined and controlled via text which
complements the context of visual information, and ii) The model can suggest
what audio to generate for the video by generating audio captions. VATT
consists of two key modules: VATT Converter, a LLM that is fine-tuned for
instructions and includes a projection layer that maps video features to the
LLM vector space; and VATT Audio, a transformer that generates audio tokens
from visual frames and from optional text prompt using iterative parallel
decoding. The audio tokens are converted to a waveform by pretrained neural
codec. Experiments show that when VATT is compared to existing video-to-audio
generation methods in objective metrics, it achieves competitive performance
when the audio caption is not provided. When the audio caption is provided as a
prompt, VATT achieves even more refined performance (lowest KLD score of 1.41).
Furthermore, subjective studies show that VATT Audio has been chosen as
preferred generated audio than audio generated by existing methods. VATT
enables controllable video-to-audio generation through text as well as
suggesting text prompts for videos through audio captions, unlocking novel
applications such as text-guided video-to-audio generation and video-to-audio
captioning.",2024-11-08 16:29:07+00:00,"['Xiulong Liu', 'Kun Su', 'Eli Shlizerman']",http://arxiv.org/abs/2411.05679v2
Dynamic Try-On: Taming Video Virtual Try-on with Dynamic Attention Mechanism,"Video try-on stands as a promising area for its tremendous real-world
potential. Previous research on video try-on has primarily focused on
transferring product clothing images to videos with simple human poses, while
performing poorly with complex movements. To better preserve clothing details,
those approaches are armed with an additional garment encoder, resulting in
higher computational resource consumption. The primary challenges in this
domain are twofold: (1) leveraging the garment encoder's capabilities in video
try-on while lowering computational requirements; (2) ensuring temporal
consistency in the synthesis of human body parts, especially during rapid
movements. To tackle these issues, we propose a novel video try-on framework
based on Diffusion Transformer(DiT), named Dynamic Try-On.
  To reduce computational overhead, we adopt a straightforward approach by
utilizing the DiT backbone itself as the garment encoder and employing a
dynamic feature fusion module to store and integrate garment features. To
ensure temporal consistency of human body parts, we introduce a limb-aware
dynamic attention module that enforces the DiT backbone to focus on the regions
of human limbs during the denoising process. Extensive experiments demonstrate
the superiority of Dynamic Try-On in generating stable and smooth try-on
results, even for videos featuring complicated human postures.",2024-12-13 03:20:53+00:00,"['Jun Zheng', 'Jing Wang', 'Fuwei Zhao', 'Xujie Zhang', 'Xiaodan Liang']",http://arxiv.org/abs/2412.09822v1
EVA: An Embodied World Model for Future Video Anticipation,"World models integrate raw data from various modalities, such as images and
language to simulate comprehensive interactions in the world, thereby
displaying crucial roles in fields like mixed reality and robotics. Yet,
applying the world model for accurate video prediction is quite challenging due
to the complex and dynamic intentions of the various scenes in practice. In
this paper, inspired by the human rethinking process, we decompose the complex
video prediction into four meta-tasks that enable the world model to handle
this issue in a more fine-grained manner. Alongside these tasks, we introduce a
new benchmark named Embodied Video Anticipation Benchmark (EVA-Bench) to
provide a well-rounded evaluation. EVA-Bench focused on evaluating the video
prediction ability of human and robot actions, presenting significant
challenges for both the language model and the generation model. Targeting
embodied video prediction, we propose the Embodied Video Anticipator (EVA), a
unified framework aiming at video understanding and generation. EVA integrates
a video generation model with a visual language model, effectively combining
reasoning capabilities with high-quality generation. Moreover, to enhance the
generalization of our framework, we tailor-designed a multi-stage pretraining
paradigm that adaptatively ensembles LoRA to produce high-fidelity results.
Extensive experiments on EVA-Bench highlight the potential of EVA to
significantly improve performance in embodied scenes, paving the way for
large-scale pre-trained models in real-world prediction tasks.",2024-10-20 18:24:00+00:00,"['Xiaowei Chi', 'Hengyuan Zhang', 'Chun-Kai Fan', 'Xingqun Qi', 'Rongyu Zhang', 'Anthony Chen', 'Chi-min Chan', 'Wei Xue', 'Wenhan Luo', 'Shanghang Zhang', 'Yike Guo']",http://arxiv.org/abs/2410.15461v1
FlexiFilm: Long Video Generation with Flexible Conditions,"Generating long and consistent videos has emerged as a significant yet
challenging problem. While most existing diffusion-based video generation
models, derived from image generation models, demonstrate promising performance
in generating short videos, their simple conditioning mechanism and sampling
strategy-originally designed for image generation-cause severe performance
degradation when adapted to long video generation. This results in prominent
temporal inconsistency and overexposure. Thus, in this work, we introduce
FlexiFilm, a new diffusion model tailored for long video generation. Our
framework incorporates a temporal conditioner to establish a more consistent
relationship between generation and multi-modal conditions, and a resampling
strategy to tackle overexposure. Empirical results demonstrate FlexiFilm
generates long and consistent videos, each over 30 seconds in length,
outperforming competitors in qualitative and quantitative analyses. Project
page: https://y-ichen.github.io/FlexiFilm-Page/",2024-04-29 11:41:34+00:00,"['Yichen Ouyang', 'jianhao Yuan', 'Hao Zhao', 'Gaoang Wang', 'Bo zhao']",http://arxiv.org/abs/2404.18620v1
Video Interpolation with Diffusion Models,"We present VIDIM, a generative model for video interpolation, which creates
short videos given a start and end frame. In order to achieve high fidelity and
generate motions unseen in the input data, VIDIM uses cascaded diffusion models
to first generate the target video at low resolution, and then generate the
high-resolution video conditioned on the low-resolution generated video. We
compare VIDIM to previous state-of-the-art methods on video interpolation, and
demonstrate how such works fail in most settings where the underlying motion is
complex, nonlinear, or ambiguous while VIDIM can easily handle such cases. We
additionally demonstrate how classifier-free guidance on the start and end
frame and conditioning the super-resolution model on the original
high-resolution frames without additional parameters unlocks high-fidelity
results. VIDIM is fast to sample from as it jointly denoises all the frames to
be generated, requires less than a billion parameters per diffusion model to
produce compelling results, and still enjoys scalability and improved quality
at larger parameter counts.",2024-04-01 15:59:32+00:00,"['Siddhant Jain', 'Daniel Watson', 'Eric Tabellion', 'Aleksander Hoyski', 'Ben Poole', 'Janne Kontkanen']",http://arxiv.org/abs/2404.01203v1
UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame Organizer,"Recently, diffusion-based video generation models have achieved significant
success. However, existing models often suffer from issues like weak
consistency and declining image quality over time. To overcome these
challenges, inspired by aesthetic principles, we propose a non-invasive plug-in
called Uniform Frame Organizer (UFO), which is compatible with any
diffusion-based video generation model. The UFO comprises a series of adaptive
adapters with adjustable intensities, which can significantly enhance the
consistency between the foreground and background of videos and improve image
quality without altering the original model parameters when integrated. The
training for UFO is simple, efficient, requires minimal resources, and supports
stylized training. Its modular design allows for the combination of multiple
UFOs, enabling the customization of personalized video generation models.
Furthermore, the UFO also supports direct transferability across different
models of the same specification without the need for specific retraining. The
experimental results indicate that UFO effectively enhances video generation
quality and demonstrates its superiority in public video generation benchmarks.
The code will be publicly available at https://github.com/Delong-liu-bupt/UFO.",2024-12-12 15:56:26+00:00,"['Delong Liu', 'Zhaohui Hou', 'Mingjie Zhan', 'Shihao Han', 'Zhicheng Zhao', 'Fei Su']",http://arxiv.org/abs/2412.09389v1
Real-time One-Step Diffusion-based Expressive Portrait Videos Generation,"Latent diffusion models have made great strides in generating expressive
portrait videos with accurate lip-sync and natural motion from a single
reference image and audio input. However, these models are far from real-time,
often requiring many sampling steps that take minutes to generate even one
second of video-significantly limiting practical use. We introduce OSA-LCM
(One-Step Avatar Latent Consistency Model), paving the way for real-time
diffusion-based avatars. Our method achieves comparable video quality to
existing methods but requires only one sampling step, making it more than 10x
faster. To accomplish this, we propose a novel avatar discriminator design that
guides lip-audio consistency and motion expressiveness to enhance video quality
in limited sampling steps. Additionally, we employ a second-stage training
architecture using an editing fine-tuned method (EFT), transforming video
generation into an editing task during training to effectively address the
temporal gap challenge in single-step generation. Experiments demonstrate that
OSA-LCM outperforms existing open-source portrait video generation models while
operating more efficiently with a single sampling step.",2024-12-18 03:42:42+00:00,"['Hanzhong Guo', 'Hongwei Yi', 'Daquan Zhou', 'Alexander William Bergman', 'Michael Lingelbach', 'Yizhou Yu']",http://arxiv.org/abs/2412.13479v1
Perceptual Video Quality Assessment: A Survey,"Perceptual video quality assessment plays a vital role in the field of video
processing due to the existence of quality degradations introduced in various
stages of video signal acquisition, compression, transmission and display. With
the advancement of internet communication and cloud service technology, video
content and traffic are growing exponentially, which further emphasizes the
requirement for accurate and rapid assessment of video quality. Therefore,
numerous subjective and objective video quality assessment studies have been
conducted over the past two decades for both generic videos and specific videos
such as streaming, user-generated content (UGC), 3D, virtual and augmented
reality (VR and AR), high frame rate (HFR), audio-visual, etc. This survey
provides an up-to-date and comprehensive review of these video quality
assessment studies. Specifically, we first review the subjective video quality
assessment methodologies and databases, which are necessary for validating the
performance of video quality metrics. Second, the objective video quality
assessment algorithms for general purposes are surveyed and concluded according
to the methodologies utilized in the quality measures. Third, we overview the
objective video quality assessment measures for specific applications and
emerging topics. Finally, the performances of the state-of-the-art video
quality assessment measures are compared and analyzed. This survey provides a
systematic overview of both classical works and recent progresses in the realm
of video quality assessment, which can help other researchers quickly access
the field and conduct relevant research.",2024-02-05 16:13:52+00:00,"['Xiongkuo Min', 'Huiyu Duan', 'Wei Sun', 'Yucheng Zhu', 'Guangtao Zhai']",http://arxiv.org/abs/2402.03413v1
Faster Projected GAN: Towards Faster Few-Shot Image Generation,"In order to solve the problems of long training time, large consumption of
computing resources and huge parameter amount of GAN network in image
generation, this paper proposes an improved GAN network model, which is named
Faster Projected GAN, based on Projected GAN. The proposed network is mainly
focuses on the improvement of generator of Projected GAN. By introducing depth
separable convolution (DSC), the number of parameters of the Projected GAN is
reduced, the training speed is accelerated, and memory is saved. Experimental
results show that on ffhq-1k, art-painting, Landscape and other few-shot image
datasets, a 20% speed increase and a 15% memory saving are achieved. At the
same time, FID loss is less or no loss, and the amount of model parameters is
better controlled. At the same time, significant training speed improvement has
been achieved in the small sample image generation task of special scenes such
as earthquake scenes with few public datasets.",2024-01-23 07:55:27+00:00,"['Chuang Wang', 'Zhengping Li', 'Yuwen Hao', 'Lijun Wang', 'Xiaoxue Li']",http://arxiv.org/abs/2403.08778v1
ID-Animator: Zero-Shot Identity-Preserving Human Video Generation,"Generating high-fidelity human video with specified identities has attracted
significant attention in the content generation community. However, existing
techniques struggle to strike a balance between training efficiency and
identity preservation, either requiring tedious case-by-case fine-tuning or
usually missing identity details in the video generation process. In this
study, we present \textbf{ID-Animator}, a zero-shot human-video generation
approach that can perform personalized video generation given a single
reference facial image without further training. ID-Animator inherits existing
diffusion-based video generation backbones with a face adapter to encode the
ID-relevant embeddings from learnable facial latent queries. To facilitate the
extraction of identity information in video generation, we introduce an
ID-oriented dataset construction pipeline that incorporates unified human
attributes and action captioning techniques from a constructed facial image
pool. Based on this pipeline, a random reference training strategy is further
devised to precisely capture the ID-relevant embeddings with an ID-preserving
loss, thus improving the fidelity and generalization capacity of our model for
ID-specific video generation. Extensive experiments demonstrate the superiority
of ID-Animator to generate personalized human videos over previous models.
Moreover, our method is highly compatible with popular pre-trained T2V models
like animatediff and various community backbone models, showing high
extendability in real-world applications for video generation where identity
preservation is highly desired. Our codes and checkpoints are released at
https://github.com/ID-Animator/ID-Animator.",2024-04-23 17:59:43+00:00,"['Xuanhua He', 'Quande Liu', 'Shengju Qian', 'Xin Wang', 'Tao Hu', 'Ke Cao', 'Keyu Yan', 'Jie Zhang']",http://arxiv.org/abs/2404.15275v3
WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model,"Video Variational Autoencoder (VAE) encodes videos into a low-dimensional
latent space, becoming a key component of most Latent Video Diffusion Models
(LVDMs) to reduce model training costs. However, as the resolution and duration
of generated videos increase, the encoding cost of Video VAEs becomes a
limiting bottleneck in training LVDMs. Moreover, the block-wise inference
method adopted by most LVDMs can lead to discontinuities of latent space when
processing long-duration videos. The key to addressing the computational
bottleneck lies in decomposing videos into distinct components and efficiently
encoding the critical information. Wavelet transform can decompose videos into
multiple frequency-domain components and improve the efficiency significantly,
we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages
multi-level wavelet transform to facilitate low-frequency energy flow into
latent representation. Furthermore, we introduce a method called Causal Cache,
which maintains the integrity of latent space during block-wise inference.
Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior
performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and
4x lower memory consumption while maintaining competitive reconstruction
quality. Our code and models are available at
https://github.com/PKU-YuanGroup/WF-VAE.",2024-11-26 14:23:53+00:00,"['Zongjian Li', 'Bin Lin', 'Yang Ye', 'Liuhan Chen', 'Xinhua Cheng', 'Shenghai Yuan', 'Li Yuan']",http://arxiv.org/abs/2411.17459v2
VideoQA in the Era of LLMs: An Empirical Study,"Video Large Language Models (Video-LLMs) are flourishing and has advanced
many video-language tasks. As a golden testbed, Video Question Answering
(VideoQA) plays pivotal role in Video-LLM developing. This work conducts a
timely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to
elucidate their success and failure modes, and provide insights towards more
human-like video understanding and question answering. Our analyses demonstrate
that Video-LLMs excel in VideoQA; they can correlate contextual cues and
generate plausible responses to questions about varied video contents. However,
models falter in handling video temporality, both in reasoning about temporal
content ordering and grounding QA-relevant temporal moments. Moreover, the
models behave unintuitively - they are unresponsive to adversarial video
perturbations while being sensitive to simple variations of candidate answers
and questions. Also, they do not necessarily generalize better. The findings
demonstrate Video-LLMs' QA capability in standard condition yet highlight their
severe deficiency in robustness and interpretability, suggesting the urgent
need on rationales in Video-LLM developing.",2024-08-08 05:14:07+00:00,"['Junbin Xiao', 'Nanxin Huang', 'Hangyu Qin', 'Dongyang Li', 'Yicong Li', 'Fengbin Zhu', 'Zhulin Tao', 'Jianxing Yu', 'Liang Lin', 'Tat-Seng Chua', 'Angela Yao']",http://arxiv.org/abs/2408.04223v1
Towards Long Video Understanding via Fine-detailed Video Story Generation,"Long video understanding has become a critical task in computer vision,
driving advancements across numerous applications from surveillance to content
retrieval. Existing video understanding methods suffer from two challenges when
dealing with long video understanding: intricate long-context relationship
modeling and interference from redundancy. To tackle these challenges, we
introduce Fine-Detailed Video Story generation (FDVS), which interprets long
videos into detailed textual representations. Specifically, to achieve
fine-grained modeling of long-temporal content, we propose a Bottom-up Video
Interpretation Mechanism that progressively interprets video content from clips
to video. To avoid interference from redundant information in videos, we
introduce a Semantic Redundancy Reduction mechanism that removes redundancy at
both the visual and textual levels. Our method transforms long videos into
hierarchical textual representations that contain multi-granularity information
of the video. With these representations, FDVS is applicable to various tasks
without any fine-tuning. We evaluate the proposed method across eight datasets
spanning three tasks. The performance demonstrates the effectiveness and
versatility of our method.",2024-12-09 03:41:28+00:00,"['Zeng You', 'Zhiquan Wen', 'Yaofo Chen', 'Xin Li', 'Runhao Zeng', 'Yaowei Wang', 'Mingkui Tan']",http://arxiv.org/abs/2412.06182v2
Subjective and Objective Quality Assessment Methods of Stereoscopic Videos with Visibility Affecting Distortions,"We present two major contributions in this work: 1) we create a full HD
resolution stereoscopic (S3D) video dataset comprised of 12 reference and 360
distorted videos. The test stimuli are produced by simulating the five levels
of fog and haze ambiances on the pristine left and right video sequences. We
perform subjective analysis on the created video dataset with 24 viewers and
compute Difference Mean Opinion Scores (DMOS) as quality representative of the
dataset, 2) an Opinion Unaware (OU) and Distortion Unaware (DU) video quality
assessment model is developed for S3D videos. We construct cyclopean frames
from the individual views of an S3D video and partition them into
nonoverlapping blocks. We analyze the Natural Scene Statistics (NSS) of all
patches of pristine and test videos, and empirically model the NSS features
with Univariate Generalized Gaussian Distribution (UGGD). We compute UGGD model
parameters ({\alpha}, \b{eta}) at multiple spatial scales and multiple
orientations of spherical steerable pyramid decomposition and show that the
UGGD parameters are distortion discriminable. Further, we perform Multivariate
Gaussian (MVG) modeling on the pristine and distorted video feature sets and
compute the corresponding mean vectors and covariance matrices of MVG fits. We
compute the Bhattacharyya distance measure between mean vectors and covariance
matrices to estimate the perceptual deviation of a test video from pristine
video set. Finally, we pool both distance measures to estimate the overall
quality score of an S3D video. The performance of the proposed objective
algorithm is verified on the popular S3D video datasets such as IRCCYN,
LFOVIAS3DPh1, LFOVIAS3DPh2 and the proposed VAD stereo dataset. The algorithm
delivers consistent performance across all datasets and shows competitive
performance against off-the-shelf 2D and 3D image and video quality assessment
algorithms.",2024-11-29 07:40:58+00:00,"['Sria Biswas', 'Balasubramanyam Appina', 'Priyanka Kokil', 'Sumohana S Channappayya']",http://arxiv.org/abs/2411.19522v1
Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models,"Diffusion models have achieved impressive results in generative tasks like
text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving
accurate text alignment in T2V generation remains challenging due to the
complex temporal dependency across frames. Existing reinforcement learning
(RL)-based approaches to enhance text alignment often require differentiable
reward functions or are constrained to limited prompts, hindering their
scalability and applicability. In this paper, we propose Free$^2$Guide, a novel
gradient-free framework for aligning generated videos with text prompts without
requiring additional model training. Leveraging principles from path integral
control, Free$^2$Guide approximates guidance for diffusion models using
non-differentiable reward functions, thereby enabling the integration of
powerful black-box Large Vision-Language Models (LVLMs) as reward model.
Additionally, our framework supports the flexible ensembling of multiple reward
models, including large-scale image-based models, to synergistically enhance
alignment without incurring substantial computational overhead. We demonstrate
that Free$^2$Guide significantly improves text alignment across various
dimensions and enhances the overall quality of generated videos.",2024-11-26 02:14:47+00:00,"['Jaemin Kim', 'Bryan S Kim', 'Jong Chul Ye']",http://arxiv.org/abs/2411.17041v1
Boximator: Generating Rich and Controllable Motions for Video Synthesis,"Generating rich and controllable motion is a pivotal challenge in video
synthesis. We propose Boximator, a new approach for fine-grained motion
control. Boximator introduces two constraint types: hard box and soft box.
Users select objects in the conditional frame using hard boxes and then use
either type of boxes to roughly or rigorously define the object's position,
shape, or motion path in future frames. Boximator functions as a plug-in for
existing video diffusion models. Its training process preserves the base
model's knowledge by freezing the original weights and training only the
control module. To address training challenges, we introduce a novel
self-tracking technique that greatly simplifies the learning of box-object
correlations. Empirically, Boximator achieves state-of-the-art video quality
(FVD) scores, improving on two base models, and further enhanced after
incorporating box constraints. Its robust motion controllability is validated
by drastic increases in the bounding box alignment metric. Human evaluation
also shows that users favor Boximator generation results over the base model.",2024-02-02 16:59:48+00:00,"['Jiawei Wang', 'Yuchen Zhang', 'Jiaxin Zou', 'Yan Zeng', 'Guoqiang Wei', 'Liping Yuan', 'Hang Li']",http://arxiv.org/abs/2402.01566v1
Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation,"In this paper, we explore the visual representations produced from a
pre-trained text-to-video (T2V) diffusion model for video understanding tasks.
We hypothesize that the latent representation learned from a pretrained
generative T2V model encapsulates rich semantics and coherent temporal
correspondences, thereby naturally facilitating video understanding. Our
hypothesis is validated through the classic referring video object segmentation
(R-VOS) task. We introduce a novel framework, termed ""VD-IT"", tailored with
dedicatedly designed components built upon a fixed pretrained T2V model.
Specifically, VD-IT uses textual information as a conditional input, ensuring
semantic consistency across time for precise temporal instance matching. It
further incorporates image tokens as supplementary textual inputs, enriching
the feature set to generate detailed and nuanced masks. Besides, instead of
using the standard Gaussian noise, we propose to predict the video-specific
noise with an extra noise prediction module, which can help preserve the
feature fidelity and elevates segmentation quality. Through extensive
experiments, we surprisingly observe that fixed generative T2V diffusion
models, unlike commonly used video backbones (e.g., Video Swin Transformer)
pretrained with discriminative image/video pre-tasks, exhibit better potential
to maintain semantic alignment and temporal consistency. On existing standard
benchmarks, our VD-IT achieves highly competitive results, surpassing many
existing state-of-the-art methods. The code is available at
https://github.com/buxiangzhiren/VD-IT.",2024-03-18 17:59:58+00:00,"['Zixin Zhu', 'Xuelu Feng', 'Dongdong Chen', 'Junsong Yuan', 'Chunming Qiao', 'Gang Hua']",http://arxiv.org/abs/2403.12042v2
MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation,"Recent advances in video diffusion models have unlocked new potential for
realistic audio-driven talking video generation. However, achieving seamless
audio-lip synchronization, maintaining long-term identity consistency, and
producing natural, audio-aligned expressions in generated talking videos remain
significant challenges. To address these challenges, we propose Memory-guided
EMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animation
approach to generate identity-consistent and expressive talking videos. Our
approach is built around two key modules: (1) a memory-guided temporal module,
which enhances long-term identity consistency and motion smoothness by
developing memory states to store information from a longer past context to
guide temporal modeling via linear attention; and (2) an emotion-aware audio
module, which replaces traditional cross attention with multi-modal attention
to enhance audio-video interaction, while detecting emotions from audio to
refine facial expressions via emotion adaptive layer norm. Extensive
quantitative and qualitative results demonstrate that MEMO generates more
realistic talking videos across diverse image and audio types, outperforming
state-of-the-art methods in overall quality, audio-lip synchronization,
identity consistency, and expression-emotion alignment.",2024-12-05 18:57:26+00:00,"['Longtao Zheng', 'Yifan Zhang', 'Hanzhong Guo', 'Jiachun Pan', 'Zhenxiong Tan', 'Jiahao Lu', 'Chuanxin Tang', 'Bo An', 'Shuicheng Yan']",http://arxiv.org/abs/2412.04448v1
UnDIVE: Generalized Underwater Video Enhancement Using Generative Priors,"With the rise of marine exploration, underwater imaging has gained
significant attention as a research topic. Underwater video enhancement has
become crucial for real-time computer vision tasks in marine exploration.
However, most existing methods focus on enhancing individual frames and neglect
video temporal dynamics, leading to visually poor enhancements. Furthermore,
the lack of ground-truth references limits the use of abundant available
underwater video data in many applications. To address these issues, we propose
a two-stage framework for enhancing underwater videos. The first stage uses a
denoising diffusion probabilistic model to learn a generative prior from
unlabeled data, capturing robust and descriptive feature representations. In
the second stage, this prior is incorporated into a physics-based image
formulation for spatial enhancement, while also enforcing temporal consistency
between video frames. Our method enables real-time and
computationally-efficient processing of high-resolution underwater videos at
lower resolutions, and offers efficient enhancement in the presence of diverse
water-types. Extensive experiments on four datasets show that our approach
generalizes well and outperforms existing enhancement methods. Our code is
available at github.com/suhas-srinath/undive.",2024-11-08 11:16:36+00:00,"['Suhas Srinath', 'Aditya Chandrasekar', 'Hemang Jamadagni', 'Rajiv Soundararajan', 'Prathosh A P']",http://arxiv.org/abs/2411.05886v1
Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning Scenarios,"In the Massive Open Online Courses (MOOC) learning scenario, the semantic
information of instructional videos has a crucial impact on learners' emotional
state. Learners mainly acquire knowledge by watching instructional videos, and
the semantic information in the videos directly affects learners' emotional
states. However, few studies have paid attention to the potential influence of
the semantic information of instructional videos on learners' emotional states.
To deeply explore the impact of video semantic information on learners'
emotions, this paper innovatively proposes a multimodal emotion recognition
method by fusing video semantic information and physiological signals. We
generate video descriptions through a pre-trained large language model (LLM) to
obtain high-level semantic information about instructional videos. Using the
cross-attention mechanism for modal interaction, the semantic information is
fused with the eye movement and PhotoPlethysmoGraphy (PPG) signals to obtain
the features containing the critical information of the three modes. The
accurate recognition of learners' emotional states is realized through the
emotion classifier. The experimental results show that our method has
significantly improved emotion recognition performance, providing a new
perspective and efficient method for emotion recognition research in MOOC
learning scenarios. The method proposed in this paper not only contributes to a
deeper understanding of the impact of instructional videos on learners'
emotional states but also provides a beneficial reference for future research
on emotion recognition in MOOC learning scenarios.",2024-04-11 05:44:27+00:00,"['Yuan Zhang', 'Xiaomei Tao', 'Hanxu Ai', 'Tao Chen', 'Yanling Gan']",http://arxiv.org/abs/2404.07484v1
Reanimating Images using Neural Representations of Dynamic Stimuli,"While computer vision models have made incredible strides in static image
recognition, they still do not match human performance in tasks that require
the understanding of complex, dynamic motion. This is notably true for
real-world scenarios where embodied agents face complex and motion-rich
environments. Our approach, BrainNRDS (Brain-Neural Representations of Dynamic
Stimuli), leverages state-of-the-art video diffusion models to decouple static
image representation from motion generation, enabling us to utilize fMRI brain
activity for a deeper understanding of human responses to dynamic visual
stimuli. Conversely, we also demonstrate that information about the brain's
representation of motion can enhance the prediction of optical flow in
artificial systems. Our novel approach leads to four main findings: (1) Visual
motion, represented as fine-grained, object-level resolution optical flow, can
be decoded from brain activity generated by participants viewing video stimuli;
(2) Video encoders outperform image-based models in predicting video-driven
brain activity; (3) Brain-decoded motion signals enable realistic video
reanimation based only on the initial frame of the video; and (4) We extend
prior work to achieve full video decoding from video-driven brain activity.
BrainNRDS advances our understanding of how the brain represents spatial and
temporal information in dynamic visual scenes. Our findings demonstrate the
potential of combining brain imaging with video diffusion models for developing
more robust and biologically-inspired computer vision systems. We show
additional decoding and encoding examples on this site:
https://brain-nrds.github.io/.",2024-06-04 17:59:49+00:00,"['Jacob Yeung', 'Andrew F. Luo', 'Gabriel Sarch', 'Margaret M. Henderson', 'Deva Ramanan', 'Michael J. Tarr']",http://arxiv.org/abs/2406.02659v3
MarDini: Masked Autoregressive Diffusion for Video Generation at Scale,"We introduce MarDini, a new family of video diffusion models that integrate
the advantages of masked auto-regression (MAR) into a unified diffusion model
(DM) framework. Here, MAR handles temporal planning, while DM focuses on
spatial generation in an asymmetric network design: i) a MAR-based planning
model containing most of the parameters generates planning signals for each
masked frame using low-resolution input; ii) a lightweight generation model
uses these signals to produce high-resolution frames via diffusion de-noising.
MarDini's MAR enables video generation conditioned on any number of masked
frames at any frame positions: a single model can handle video interpolation
(e.g., masking middle frames), image-to-video generation (e.g., masking from
the second frame onward), and video expansion (e.g., masking half the frames).
The efficient design allocates most of the computational resources to the
low-resolution planning model, making computationally expensive but important
spatio-temporal attention feasible at scale. MarDini sets a new
state-of-the-art for video interpolation; meanwhile, within few inference
steps, it efficiently generates videos on par with those of much more expensive
advanced image-to-video models.",2024-10-26 21:12:32+00:00,"['Haozhe Liu', 'Shikun Liu', 'Zijian Zhou', 'Mengmeng Xu', 'Yanping Xie', 'Xiao Han', 'Juan C. Prez', 'Ding Liu', 'Kumara Kahatapitiya', 'Menglin Jia', 'Jui-Chieh Wu', 'Sen He', 'Tao Xiang', 'Jrgen Schmidhuber', 'Juan-Manuel Prez-Ra']",http://arxiv.org/abs/2410.20280v1
MatchDiffusion: Training-free Generation of Match-cuts,"Match-cuts are powerful cinematic tools that create seamless transitions
between scenes, delivering strong visual and metaphorical connections. However,
crafting match-cuts is a challenging, resource-intensive process requiring
deliberate artistic planning. In MatchDiffusion, we present the first
training-free method for match-cut generation using text-to-video diffusion
models. MatchDiffusion leverages a key property of diffusion models: early
denoising steps define the scene's broad structure, while later steps add
details. Guided by this insight, MatchDiffusion employs ""Joint Diffusion"" to
initialize generation for two prompts from shared noise, aligning structure and
motion. It then applies ""Disjoint Diffusion"", allowing the videos to diverge
and introduce unique details. This approach produces visually coherent videos
suited for match-cuts. User studies and metrics demonstrate MatchDiffusion's
effectiveness and potential to democratize match-cut creation.",2024-11-27 18:59:59+00:00,"['Alejandro Pardo', 'Fabio Pizzati', 'Tong Zhang', 'Alexander Pondaven', 'Philip Torr', 'Juan Camilo Perez', 'Bernard Ghanem']",http://arxiv.org/abs/2411.18677v1
Endora: Video Generation Models as Endoscopy Simulators,"Generative models hold promise for revolutionizing medical education,
robot-assisted surgery, and data augmentation for machine learning. Despite
progress in generating 2D medical images, the complex domain of clinical video
generation has largely remained untapped.This paper introduces \model, an
innovative approach to generate medical videos that simulate clinical endoscopy
scenes. We present a novel generative model design that integrates a
meticulously crafted spatial-temporal video transformer with advanced 2D vision
foundation model priors, explicitly modeling spatial-temporal dynamics during
video generation. We also pioneer the first public benchmark for endoscopy
simulation with video generation models, adapting existing state-of-the-art
methods for this endeavor.Endora demonstrates exceptional visual quality in
generating endoscopy videos, surpassing state-of-the-art methods in extensive
testing. Moreover, we explore how this endoscopy simulator can empower
downstream video analysis tasks and even generate 3D medical scenes with
multi-view consistency. In a nutshell, Endora marks a notable breakthrough in
the deployment of generative AI for clinical endoscopy research, setting a
substantial stage for further advances in medical content generation. For more
details, please visit our project page: https://endora-medvidgen.github.io/.",2024-03-17 00:51:59+00:00,"['Chenxin Li', 'Hengyu Liu', 'Yifan Liu', 'Brandon Y. Feng', 'Wuyang Li', 'Xinyu Liu', 'Zhen Chen', 'Jing Shao', 'Yixuan Yuan']",http://arxiv.org/abs/2403.11050v1
Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis,"Foley is a term commonly used in filmmaking, referring to the addition of
daily sound effects to silent films or videos to enhance the auditory
experience. Video-to-Audio (V2A), as a particular type of automatic foley task,
presents inherent challenges related to audio-visual synchronization. These
challenges encompass maintaining the content consistency between the input
video and the generated audio, as well as the alignment of temporal and
loudness properties within the video. To address these issues, we construct a
controllable video-to-audio synthesis model, termed Draw an Audio, which
supports multiple input instructions through drawn masks and loudness signals.
To ensure content consistency between the synthesized audio and target video,
we introduce the Mask-Attention Module (MAM), which employs masked video
instruction to enable the model to focus on regions of interest. Additionally,
we implement the Time-Loudness Module (TLM), which uses an auxiliary loudness
signal to ensure the synthesis of sound that aligns with the video in both
loudness and temporal dimensions. Furthermore, we have extended a large-scale
V2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive
experiments on challenging benchmarks across two large-scale V2A datasets
verify Draw an Audio achieves the state-of-the-art. Project page:
https://yannqi.github.io/Draw-an-Audio/.",2024-09-10 01:07:20+00:00,"['Qi Yang', 'Binjie Mao', 'Zili Wang', 'Xing Nie', 'Pengfei Gao', 'Ying Guo', 'Cheng Zhen', 'Pengfei Yan', 'Shiming Xiang']",http://arxiv.org/abs/2409.06135v1
Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation,"The evolution of video generation from text, starting with animating MNIST
numbers to simulating the physical world with Sora, has progressed at a
breakneck speed over the past seven years. While often seen as a superficial
expansion of the predecessor text-to-image generation model, text-to-video
generation models are developed upon carefully engineered constituents. Here,
we systematically discuss these elements consisting of but not limited to core
building blocks (vision, language, and temporal) and supporting features from
the perspective of their contributions to achieving a world model. We employ
the PRISMA framework to curate 97 impactful research articles from renowned
scientific databases primarily studying video synthesis using text conditions.
Upon minute exploration of these manuscripts, we observe that text-to-video
generation involves more intricate technologies beyond the plain extension of
text-to-image generation. Our additional review into the shortcomings of
Sora-generated videos pinpoints the call for more in-depth studies in various
enabling aspects of video generation such as dataset, evaluation metric,
efficient architecture, and human-controlled generation. Finally, we conclude
that the study of the text-to-video generation may still be in its infancy,
requiring contribution from the cross-discipline research community towards its
advancement as the first step to realize artificial general intelligence (AGI).",2024-03-08 07:58:13+00:00,"['Joseph Cho', 'Fachrina Dewi Puspitasari', 'Sheng Zheng', 'Jingyao Zheng', 'Lik-Hang Lee', 'Tae-Ho Kim', 'Choong Seon Hong', 'Chaoning Zhang']",http://arxiv.org/abs/2403.05131v2
Video Summarization using Denoising Diffusion Probabilistic Model,"Video summarization aims to eliminate visual redundancy while retaining key
parts of video to construct concise and comprehensive synopses. Most existing
methods use discriminative models to predict the importance scores of video
frames. However, these methods are susceptible to annotation inconsistency
caused by the inherent subjectivity of different annotators when annotating the
same video. In this paper, we introduce a generative framework for video
summarization that learns how to generate summaries from a probability
distribution perspective, effectively reducing the interference of subjective
annotation noise. Specifically, we propose a novel diffusion summarization
method based on the Denoising Diffusion Probabilistic Model (DDPM), which
learns the probability distribution of training data through noise prediction,
and generates summaries by iterative denoising. Our method is more resistant to
subjective annotation noise, and is less prone to overfitting the training data
than discriminative methods, with strong generalization ability. Moreover, to
facilitate training DDPM with limited data, we employ an unsupervised video
summarization model to implement the earlier denoising process. Extensive
experiments on various datasets (TVSum, SumMe, and FPVSum) demonstrate the
effectiveness of our method.",2024-12-11 13:02:09+00:00,"['Zirui Shang', 'Yubo Zhu', 'Hongxi Li', 'Shuo Yang', 'Xinxiao Wu']",http://arxiv.org/abs/2412.08357v2
Generating 3D-Consistent Videos from Unposed Internet Photos,"We address the problem of generating videos from unposed internet photos. A
handful of input images serve as keyframes, and our model interpolates between
them to simulate a path moving between the cameras. Given random images, a
model's ability to capture underlying geometry, recognize scene identity, and
relate frames in terms of camera position and orientation reflects a
fundamental understanding of 3D structure and scene layout. However, existing
video models such as Luma Dream Machine fail at this task. We design a
self-supervised method that takes advantage of the consistency of videos and
variability of multiview internet photos to train a scalable, 3D-aware video
model without any 3D annotations such as camera parameters. We validate that
our method outperforms all baselines in terms of geometric and appearance
consistency. We also show our model benefits applications that enable camera
control, such as 3D Gaussian Splatting. Our results suggest that we can scale
up scene-level 3D learning using only 2D data such as videos and multiview
internet photos.",2024-11-20 18:58:31+00:00,"['Gene Chou', 'Kai Zhang', 'Sai Bi', 'Hao Tan', 'Zexiang Xu', 'Fujun Luan', 'Bharath Hariharan', 'Noah Snavely']",http://arxiv.org/abs/2411.13549v1
Text-Animator: Controllable Visual Text Video Generation,"Video generation is a challenging yet pivotal task in various industries,
such as gaming, e-commerce, and advertising. One significant unresolved aspect
within T2V is the effective visualization of text within generated videos.
Despite the progress achieved in Text-to-Video~(T2V) generation, current
methods still cannot effectively visualize texts in videos directly, as they
mainly focus on summarizing semantic scene information, understanding, and
depicting actions. While recent advances in image-level visual text generation
show promise, transitioning these techniques into the video domain faces
problems, notably in preserving textual fidelity and motion coherence. In this
paper, we propose an innovative approach termed Text-Animator for visual text
video generation. Text-Animator contains a text embedding injection module to
precisely depict the structures of visual text in generated videos. Besides, we
develop a camera control module and a text refinement module to improve the
stability of generated visual text by controlling the camera movement as well
as the motion of visualized text. Quantitative and qualitative experimental
results demonstrate the superiority of our approach to the accuracy of
generated visual text over state-of-the-art video generation methods. The
project page can be found at https://laulampaul.github.io/text-animator.html.",2024-06-25 17:59:41+00:00,"['Lin Liu', 'Quande Liu', 'Shengju Qian', 'Yuan Zhou', 'Wengang Zhou', 'Houqiang Li', 'Lingxi Xie', 'Qi Tian']",http://arxiv.org/abs/2406.17777v1
UVCG: Leveraging Temporal Consistency for Universal Video Protection,"The security risks of AI-driven video editing have garnered significant
attention. Although recent studies indicate that adding perturbations to images
can protect them from malicious edits, directly applying image-based methods to
perturb each frame in a video becomes ineffective, as video editing techniques
leverage the consistency of inter-frame information to restore individually
perturbed content. To address this challenge, we leverage the temporal
consistency of video content to propose a straightforward and efficient, yet
highly effective and broadly applicable approach, Universal Video Consistency
Guard (UVCG). UVCG embeds the content of another video(target video) within a
protected video by introducing continuous, imperceptible perturbations which
has the ability to force the encoder of editing models to map continuous inputs
to misaligned continuous outputs, thereby inhibiting the generation of videos
consistent with the intended textual prompts. Additionally leveraging
similarity in perturbations between adjacent frames, we improve the
computational efficiency of perturbation generation by employing a
perturbation-reuse strategy. We applied UVCG across various versions of Latent
Diffusion Models (LDM) and assessed its effectiveness and generalizability
across multiple LDM-based editing pipelines. The results confirm the
effectiveness, transferability, and efficiency of our approach in safeguarding
video content from unauthorized modifications.",2024-11-25 08:48:54+00:00,"['KaiZhou Li', 'Jindong Gu', 'Xinchun Yu', 'Junjie Cao', 'Yansong Tang', 'Xiao-Ping Zhang']",http://arxiv.org/abs/2411.17746v1
Towards A Better Metric for Text-to-Video Generation,"Generative models have demonstrated remarkable capability in synthesizing
high-quality text, images, and videos. For video generation, contemporary
text-to-video models exhibit impressive capabilities, crafting visually
stunning videos. Nonetheless, evaluating such videos poses significant
challenges. Current research predominantly employs automated metrics such as
FVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis,
particularly in the temporal assessment of video content, thus rendering them
unreliable indicators of true video quality. Furthermore, while user studies
have the potential to reflect human perception accurately, they are hampered by
their time-intensive and laborious nature, with outcomes that are often tainted
by subjective bias. In this paper, we investigate the limitations inherent in
existing metrics and introduce a novel evaluation pipeline, the Text-to-Video
Score (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video
Alignment, which scrutinizes the fidelity of the video in representing the
given text description, and (2) Video Quality, which evaluates the video's
overall production caliber with a mixture of experts. Moreover, to evaluate the
proposed metrics and facilitate future improvements on them, we present the
TVGE dataset, collecting human judgements of 2,543 text-to-video generated
videos on the two criteria. Experiments on the TVGE dataset demonstrate the
superiority of the proposed T2VScore on offering a better metric for
text-to-video generation.",2024-01-15 15:42:39+00:00,"['Jay Zhangjie Wu', 'Guian Fang', 'Haoning Wu', 'Xintao Wang', 'Yixiao Ge', 'Xiaodong Cun', 'David Junhao Zhang', 'Jia-Wei Liu', 'Yuchao Gu', 'Rui Zhao', 'Weisi Lin', 'Wynne Hsu', 'Ying Shan', 'Mike Zheng Shou']",http://arxiv.org/abs/2401.07781v1
VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding,"Building on the advances of language models, Large Multimodal Models (LMMs)
have contributed significant improvements in video understanding. While the
current video LMMs utilize advanced Large Language Models (LLMs), they rely on
either image or video encoders to process visual inputs, each of which has its
own limitations. Image encoders excel at capturing rich spatial details from
frame sequences but lack explicit temporal context, which can be important in
videos with intricate action sequences. On the other hand, video encoders
provide temporal context but are often limited by computational constraints
that lead to processing only sparse frames at lower resolutions, resulting in
reduced contextual and spatial understanding. To this end, we introduce
VideoGPT+, which combines the complementary benefits of the image encoder (for
detailed spatial understanding) and the video encoder (for global temporal
context modeling). The model processes videos by dividing them into smaller
segments and applies an adaptive pooling strategy on features extracted by both
image and video encoders. Our architecture showcases improved performance
across multiple video benchmarks, including VCGBench, MVBench and Zero-shot
question-answering. Further, we develop 112K video-instruction set using a
novel semi-automatic annotation pipeline which further improves the model
performance. Additionally, to comprehensively evaluate video LMMs, we present
VCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports,
science, gaming, and surveillance videos. This benchmark with 4,354
question-answer pairs evaluates the generalization of existing LMMs on dense
video captioning, spatial and temporal understanding, and complex reasoning,
ensuring comprehensive assessment across diverse video types and dynamics.
Code: https://github.com/mbzuai-oryx/VideoGPT-plus.",2024-06-13 17:59:59+00:00,"['Muhammad Maaz', 'Hanoona Rasheed', 'Salman Khan', 'Fahad Khan']",http://arxiv.org/abs/2406.09418v1
MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion,"The spatio-temporal complexity of video data presents significant challenges
in tasks such as compression, generation, and inpainting. We present four key
contributions to address the challenges of spatiotemporal video processing.
First, we introduce the 3D Mobile Inverted Vector-Quantization Variational
Autoencoder (3D-MBQ-VAE), which combines Variational Autoencoders (VAEs) with
masked token modeling to enhance spatiotemporal video compression. The model
achieves superior temporal consistency and state-of-the-art (SOTA)
reconstruction quality by employing a novel training strategy with full frame
masking. Second, we present MotionAura, a text-to-video generation framework
that utilizes vector-quantized diffusion models to discretize the latent space
and capture complex motion dynamics, producing temporally coherent videos
aligned with text prompts. Third, we propose a spectral transformer-based
denoising network that processes video data in the frequency domain using the
Fourier Transform. This method effectively captures global context and
long-range dependencies for high-quality video generation and denoising.
Lastly, we introduce a downstream task of Sketch Guided Video Inpainting. This
task leverages Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning.
Our models achieve SOTA performance on a range of benchmarks. Our work offers
robust frameworks for spatiotemporal modeling and user-driven video content
manipulation. We will release the code, datasets, and models in open-source.",2024-10-10 07:07:56+00:00,"['Onkar Susladkar', 'Jishu Sen Gupta', 'Chirag Sehgal', 'Sparsh Mittal', 'Rekha Singhal']",http://arxiv.org/abs/2410.07659v2
Towards Understanding Unsafe Video Generation,"Video generation models (VGMs) have demonstrated the capability to synthesize
high-quality output. It is important to understand their potential to produce
unsafe content, such as violent or terrifying videos. In this work, we provide
a comprehensive understanding of unsafe video generation.
  First, to confirm the possibility that these models could indeed generate
unsafe videos, we choose unsafe content generation prompts collected from 4chan
and Lexica, and three open-source SOTA VGMs to generate unsafe videos. After
filtering out duplicates and poorly generated content, we created an initial
set of 2112 unsafe videos from an original pool of 5607 videos. Through
clustering and thematic coding analysis of these generated videos, we identify
5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic,
Violent/Bloody, and Political. With IRB approval, we then recruit online
participants to help label the generated videos. Based on the annotations
submitted by 403 participants, we identified 937 unsafe videos from the initial
video set. With the labeled information and the corresponding prompts, we
created the first dataset of unsafe videos generated by VGMs.
  We then study possible defense mechanisms to prevent the generation of unsafe
videos. Existing defense methods in image generation focus on filtering either
input prompt or output results. We propose a new approach called Latent
Variable Defense (LVD), which works within the model's internal sampling
process. LVD can achieve 0.90 defense accuracy while reducing time and
computing resources by 10x when sampling a large number of unsafe prompts.",2024-07-17 14:07:22+00:00,"['Yan Pang', 'Aiping Xiong', 'Yang Zhang', 'Tianhao Wang']",http://arxiv.org/abs/2407.12581v1
EasyControl: Transfer ControlNet to Video Diffusion for Controllable Generation and Interpolation,"Following the advancements in text-guided image generation technology
exemplified by Stable Diffusion, video generation is gaining increased
attention in the academic community. However, relying solely on text guidance
for video generation has serious limitations, as videos contain much richer
content than images, especially in terms of motion. This information can hardly
be adequately described with plain text. Fortunately, in computer vision,
various visual representations can serve as additional control signals to guide
generation. With the help of these signals, video generation can be controlled
in finer detail, allowing for greater flexibility for different applications.
Integrating various controls, however, is nontrivial. In this paper, we propose
a universal framework called EasyControl. By propagating and injecting
condition features through condition adapters, our method enables users to
control video generation with a single condition map. With our framework,
various conditions including raw pixels, depth, HED, etc., can be integrated
into different Unet-based pre-trained video diffusion models at a low practical
cost. We conduct comprehensive experiments on public datasets, and both
quantitative and qualitative results indicate that our method outperforms
state-of-the-art methods. EasyControl significantly improves various evaluation
metrics across multiple validation datasets compared to previous works.
Specifically, for the sketch-to-video generation task, EasyControl achieves an
improvement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 compared
with VideoComposer. For fidelity, our model demonstrates powerful image
retention ability, resulting in high FVD and IS in UCF101 and MSR-VTT compared
to other image-to-video models.",2024-08-23 11:48:29+00:00,"['Cong Wang', 'Jiaxi Gu', 'Panwen Hu', 'Haoyu Zhao', 'Yuanfan Guo', 'Jianhua Han', 'Hang Xu', 'Xiaodan Liang']",http://arxiv.org/abs/2408.13005v2
Dreamitate: Real-World Visuomotor Policy Learning via Video Generation,"A key challenge in manipulation is learning a policy that can robustly
generalize to diverse visual environments. A promising mechanism for learning
robust policies is to leverage video generative models, which are pretrained on
large-scale datasets of internet videos. In this paper, we propose a visuomotor
policy learning framework that fine-tunes a video diffusion model on human
demonstrations of a given task. At test time, we generate an example of an
execution of the task conditioned on images of a novel scene, and use this
synthesized execution directly to control the robot. Our key insight is that
using common tools allows us to effortlessly bridge the embodiment gap between
the human hand and the robot manipulator. We evaluate our approach on four
tasks of increasing complexity and demonstrate that harnessing internet-scale
generative models allows the learned policy to achieve a significantly higher
degree of generalization than existing behavior cloning approaches.",2024-06-24 17:59:45+00:00,"['Junbang Liang', 'Ruoshi Liu', 'Ege Ozguroglu', 'Sruthi Sudhakar', 'Achal Dave', 'Pavel Tokmakov', 'Shuran Song', 'Carl Vondrick']",http://arxiv.org/abs/2406.16862v1
TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models,"Text-conditioned image-to-video generation (TI2V) aims to synthesize a
realistic video starting from a given image (e.g., a woman's photo) and a text
description (e.g., ""a woman is drinking water.""). Existing TI2V frameworks
often require costly training on video-text datasets and specific model designs
for text and image conditioning. In this paper, we propose TI2V-Zero, a
zero-shot, tuning-free method that empowers a pretrained text-to-video (T2V)
diffusion model to be conditioned on a provided image, enabling TI2V generation
without any optimization, fine-tuning, or introducing external modules. Our
approach leverages a pretrained T2V diffusion foundation model as the
generative prior. To guide video generation with the additional image input, we
propose a ""repeat-and-slide"" strategy that modulates the reverse denoising
process, allowing the frozen diffusion model to synthesize a video
frame-by-frame starting from the provided image. To ensure temporal continuity,
we employ a DDPM inversion strategy to initialize Gaussian noise for each newly
synthesized frame and a resampling technique to help preserve visual details.
We conduct comprehensive experiments on both domain-specific and open-domain
datasets, where TI2V-Zero consistently outperforms a recent open-domain TI2V
model. Furthermore, we show that TI2V-Zero can seamlessly extend to other tasks
such as video infilling and prediction when provided with more images. Its
autoregressive design also supports long video generation.",2024-04-25 03:21:11+00:00,"['Haomiao Ni', 'Bernhard Egger', 'Suhas Lohit', 'Anoop Cherian', 'Ye Wang', 'Toshiaki Koike-Akino', 'Sharon X. Huang', 'Tim K. Marks']",http://arxiv.org/abs/2404.16306v1
AICL: Action In-Context Learning for Video Diffusion Model,"The open-domain video generation models are constrained by the scale of the
training video datasets, and some less common actions still cannot be
generated. Some researchers explore video editing methods and achieve action
generation by editing the spatial information of the same action video.
However, this method mechanically generates identical actions without
understanding, which does not align with the characteristics of open-domain
scenarios. In this paper, we propose AICL, which empowers the generative model
with the ability to understand action information in reference videos, similar
to how humans do, through in-context learning. Extensive experiments
demonstrate that AICL effectively captures the action and achieves
state-of-the-art generation performance across three typical video diffusion
models on five metrics when using randomly selected categories from
non-training datasets.",2024-03-18 07:41:19+00:00,"['Jianzhi Liu', 'Junchen Zhu', 'Lianli Gao', 'Heng Tao Shen', 'Jingkuan Song']",http://arxiv.org/abs/2403.11535v2
MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators,"Recent advances in Text-to-Video generation (T2V) have achieved remarkable
success in synthesizing high-quality general videos from textual descriptions.
A largely overlooked problem in T2V is that existing models have not adequately
encoded physical knowledge of the real world, thus generated videos tend to
have limited motion and poor variations. In this paper, we propose
\textbf{MagicTime}, a metamorphic time-lapse video generation model, which
learns real-world physics knowledge from time-lapse videos and implements
metamorphic generation. First, we design a MagicAdapter scheme to decouple
spatial and temporal training, encode more physical knowledge from metamorphic
videos, and transform pre-trained T2V models to generate metamorphic videos.
Second, we introduce a Dynamic Frames Extraction strategy to adapt to
metamorphic time-lapse videos, which have a wider variation range and cover
dramatic object metamorphic processes, thus embodying more physical knowledge
than general videos. Finally, we introduce a Magic Text-Encoder to improve the
understanding of metamorphic video prompts. Furthermore, we create a time-lapse
video-text dataset called \textbf{ChronoMagic}, specifically curated to unlock
the metamorphic video generation ability. Extensive experiments demonstrate the
superiority and effectiveness of MagicTime for generating high-quality and
dynamic metamorphic videos, suggesting time-lapse video generation is a
promising path toward building metamorphic simulators of the physical world.",2024-04-07 16:49:07+00:00,"['Shenghai Yuan', 'Jinfa Huang', 'Yujun Shi', 'Yongqi Xu', 'Ruijie Zhu', 'Bin Lin', 'Xinhua Cheng', 'Li Yuan', 'Jiebo Luo']",http://arxiv.org/abs/2404.05014v1
Semantically Consistent Video Inpainting with Conditional Diffusion Models,"Current state-of-the-art methods for video inpainting typically rely on
optical flow or attention-based approaches to inpaint masked regions by
propagating visual information across frames. While such approaches have led to
significant progress on standard benchmarks, they struggle with tasks that
require the synthesis of novel content that is not present in other frames. In
this paper, we reframe video inpainting as a conditional generative modeling
problem and present a framework for solving such problems with conditional
video diffusion models. We introduce inpainting-specific sampling schemes which
capture crucial long-range dependencies in the context, and devise a novel
method for conditioning on the known pixels in incomplete frames. We highlight
the advantages of using a generative approach for this task, showing that our
method is capable of generating diverse, high-quality inpaintings and
synthesizing new content that is spatially, temporally, and semantically
consistent with the provided context.",2024-04-30 23:49:26+00:00,"['Dylan Green', 'William Harvey', 'Saeid Naderiparizi', 'Matthew Niedoba', 'Yunpeng Liu', 'Xiaoxuan Liang', 'Jonathan Lavington', 'Ke Zhang', 'Vasileios Lioutas', 'Setareh Dabiri', 'Adam Scibior', 'Berend Zwartsenberg', 'Frank Wood']",http://arxiv.org/abs/2405.00251v2
VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models,"Text-to-image diffusion models (T2I) have demonstrated unprecedented
capabilities in creating realistic and aesthetic images. On the contrary,
text-to-video diffusion models (T2V) still lag far behind in frame quality and
text alignment, owing to insufficient quality and quantity of training videos.
In this paper, we introduce VideoElevator, a training-free and plug-and-play
method, which elevates the performance of T2V using superior capabilities of
T2I. Different from conventional T2V sampling (i.e., temporal and spatial
modeling), VideoElevator explicitly decomposes each sampling step into temporal
motion refining and spatial quality elevating. Specifically, temporal motion
refining uses encapsulated T2V to enhance temporal consistency, followed by
inverting to the noise distribution required by T2I. Then, spatial quality
elevating harnesses inflated T2I to directly predict less noisy latent, adding
more photo-realistic details. We have conducted experiments in extensive
prompts under the combination of various T2V and T2I. The results show that
VideoElevator not only improves the performance of T2V baselines with
foundational T2I, but also facilitates stylistic video synthesis with
personalized T2I. Our code is available at
https://github.com/YBYBZhang/VideoElevator.",2024-03-08 16:44:54+00:00,"['Yabo Zhang', 'Yuxiang Wei', 'Xianhui Lin', 'Zheng Hui', 'Peiran Ren', 'Xuansong Xie', 'Xiangyang Ji', 'Wangmeng Zuo']",http://arxiv.org/abs/2403.05438v1
MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model,"We present MOFA-Video, an advanced controllable image animation method that
generates video from the given image using various additional controllable
signals (such as human landmarks reference, manual trajectories, and another
even provided video) or their combinations. This is different from previous
methods which only can work on a specific motion domain or show weak control
abilities with diffusion prior. To achieve our goal, we design several
domain-aware motion field adapters (\ie, MOFA-Adapters) to control the
generated motions in the video generation pipeline. For MOFA-Adapters, we
consider the temporal motion consistency of the video and generate the dense
motion flow from the given sparse control conditions first, and then, the
multi-scale features of the given image are wrapped as a guided feature for
stable video diffusion generation. We naively train two motion adapters for the
manual trajectories and the human landmarks individually since they both
contain sparse information about the control. After training, the MOFA-Adapters
in different domains can also work together for more controllable video
generation. Project Page: https://myniuuu.github.io/MOFA_Video/",2024-05-30 16:22:22+00:00,"['Muyao Niu', 'Xiaodong Cun', 'Xintao Wang', 'Yong Zhang', 'Ying Shan', 'Yinqiang Zheng']",http://arxiv.org/abs/2405.20222v3
Loong: Generating Minute-level Long Videos with Autoregressive Language Models,"It is desirable but challenging to generate content-rich long videos in the
scale of minutes. Autoregressive large language models (LLMs) have achieved
great success in generating coherent and long sequences of tokens in the domain
of natural language processing, while the exploration of autoregressive LLMs
for video generation is limited to generating short videos of several seconds.
In this work, we conduct a deep analysis of the challenges that prevent
autoregressive LLM-based video generators from generating long videos. Based on
the observations and analysis, we propose Loong, a new autoregressive LLM-based
video generator that can generate minute-long videos. Specifically, we model
the text tokens and video tokens as a unified sequence for autoregressive LLMs
and train the model from scratch. We propose progressive short-to-long training
with a loss re-weighting scheme to mitigate the loss imbalance problem for long
video training. We further investigate inference strategies, including video
token re-encoding and sampling strategies, to diminish error accumulation
during inference. Our proposed Loong can be trained on 10-second videos and be
extended to generate minute-level long videos conditioned on text prompts, as
demonstrated by the results. More samples are available at:
https://epiphqny.github.io/Loong-video.",2024-10-03 17:59:02+00:00,"['Yuqing Wang', 'Tianwei Xiong', 'Daquan Zhou', 'Zhijie Lin', 'Yang Zhao', 'Bingyi Kang', 'Jiashi Feng', 'Xihui Liu']",http://arxiv.org/abs/2410.02757v1
Generate Any Scene: Evaluating and Improving Text-to-Vision Generation with Scene Graph Programming,"DALL-E and Sora have gained attention by producing implausible images, such
as ""astronauts riding a horse in space."" Despite the proliferation of
text-to-vision models that have inundated the internet with synthetic visuals,
from images to 3D assets, current benchmarks predominantly evaluate these
models on real-world scenes paired with captions. We introduce Generate Any
Scene, a framework that systematically enumerates scene graphs representing a
vast array of visual scenes, spanning realistic to imaginative compositions.
Generate Any Scene leverages 'scene graph programming', a method for
dynamically constructing scene graphs of varying complexity from a structured
taxonomy of visual elements. This taxonomy includes numerous objects,
attributes, and relations, enabling the synthesis of an almost infinite variety
of scene graphs. Using these structured representations, Generate Any Scene
translates each scene graph into a caption, enabling scalable evaluation of
text-to-vision models through standard metrics. We conduct extensive
evaluations across multiple text-to-image, text-to-video, and text-to-3D
models, presenting key findings on model performance. We find that DiT-backbone
text-to-image models align more closely with input captions than UNet-backbone
models. Text-to-video models struggle with balancing dynamics and consistency,
while both text-to-video and text-to-3D models show notable gaps in human
preference alignment. We demonstrate the effectiveness of Generate Any Scene by
conducting three practical applications leveraging captions generated by
Generate Any Scene: 1) a self-improving framework where models iteratively
enhance their performance using generated data, 2) a distillation process to
transfer specific strengths from proprietary models to open-source
counterparts, and 3) improvements in content moderation by identifying and
generating challenging synthetic data.",2024-12-11 09:17:39+00:00,"['Ziqi Gao', 'Weikai Huang', 'Jieyu Zhang', 'Aniruddha Kembhavi', 'Ranjay Krishna']",http://arxiv.org/abs/2412.08221v2
OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation,"As virtual reality gains popularity, the demand for controllable creation of
immersive and dynamic omnidirectional videos (ODVs) is increasing. While
previous text-to-ODV generation methods achieve impressive results, they
struggle with content inaccuracies and inconsistencies due to reliance solely
on textual inputs. Although recent motion control techniques provide
fine-grained control for video generation, directly applying these methods to
ODVs often results in spatial distortion and unsatisfactory performance,
especially with complex spherical motions. To tackle these challenges, we
propose OmniDrag, the first approach enabling both scene- and object-level
motion control for accurate, high-quality omnidirectional image-to-video
generation. Building on pretrained video diffusion models, we introduce an
omnidirectional control module, which is jointly fine-tuned with temporal
attention layers to effectively handle complex spherical motion. In addition,
we develop a novel spherical motion estimator that accurately extracts
motion-control signals and allows users to perform drag-style ODV generation by
simply drawing handle and target points. We also present a new dataset, named
Move360, addressing the scarcity of ODV data with large scene and object
motions. Experiments demonstrate the significant superiority of OmniDrag in
achieving holistic scene-level and fine-grained object-level control for ODV
generation. The project page is available at
https://lwq20020127.github.io/OmniDrag.",2024-12-12 18:59:56+00:00,"['Weiqi Li', 'Shijie Zhao', 'Chong Mou', 'Xuhan Sheng', 'Zhenyu Zhang', 'Qian Wang', 'Junlin Li', 'Li Zhang', 'Jian Zhang']",http://arxiv.org/abs/2412.09623v1
Multi-scale 2D Temporal Map Diffusion Models for Natural Language Video Localization,"Natural Language Video Localization (NLVL), grounding phrases from natural
language descriptions to corresponding video segments, is a complex yet
critical task in video understanding. Despite ongoing advancements, many
existing solutions lack the capability to globally capture temporal dynamics of
the video data. In this study, we present a novel approach to NLVL that aims to
address this issue. Our method involves the direct generation of a global 2D
temporal map via a conditional denoising diffusion process, based on the input
video and language query. The main challenges are the inherent sparsity and
discontinuity of a 2D temporal map in devising the diffusion decoder. To
address these challenges, we introduce a multi-scale technique and develop an
innovative diffusion decoder. Our approach effectively encapsulates the
interaction between the query and video data across various time scales.
Experiments on the Charades and DiDeMo datasets underscore the potency of our
design.",2024-01-16 09:33:29+00:00,"['Chongzhi Zhang', 'Mingyuan Zhang', 'Zhiyang Teng', 'Jiayi Li', 'Xizhou Zhu', 'Lewei Lu', 'Ziwei Liu', 'Aixin Sun']",http://arxiv.org/abs/2401.08232v1
EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation,"Video generation has emerged as a promising tool for world simulation,
leveraging visual data to replicate real-world environments. Within this
context, egocentric video generation, which centers on the human perspective,
holds significant potential for enhancing applications in virtual reality,
augmented reality, and gaming. However, the generation of egocentric videos
presents substantial challenges due to the dynamic nature of egocentric
viewpoints, the intricate diversity of actions, and the complex variety of
scenes encountered. Existing datasets are inadequate for addressing these
challenges effectively. To bridge this gap, we present EgoVid-5M, the first
high-quality dataset specifically curated for egocentric video generation.
EgoVid-5M encompasses 5 million egocentric video clips and is enriched with
detailed action annotations, including fine-grained kinematic control and
high-level textual descriptions. To ensure the integrity and usability of the
dataset, we implement a sophisticated data cleaning pipeline designed to
maintain frame consistency, action coherence, and motion smoothness under
egocentric conditions. Furthermore, we introduce EgoDreamer, which is capable
of generating egocentric videos driven simultaneously by action descriptions
and kinematic control signals. The EgoVid-5M dataset, associated action
annotations, and all data cleansing metadata will be released for the
advancement of research in egocentric video generation.",2024-11-13 07:05:40+00:00,"['Xiaofeng Wang', 'Kang Zhao', 'Feng Liu', 'Jiayu Wang', 'Guosheng Zhao', 'Xiaoyi Bao', 'Zheng Zhu', 'Yingya Zhang', 'Xingang Wang']",http://arxiv.org/abs/2411.08380v1
Multimodal Fusion and Coherence Modeling for Video Topic Segmentation,"The video topic segmentation (VTS) task segments videos into intelligible,
non-overlapping topics, facilitating efficient comprehension of video content
and quick access to specific content. VTS is also critical to various
downstream video understanding tasks. Traditional VTS methods using shallow
features or unsupervised approaches struggle to accurately discern the nuances
of topical transitions. Recently, supervised approaches have achieved superior
performance on video action or scene segmentation over unsupervised approaches.
In this work, we improve supervised VTS by thoroughly exploring multimodal
fusion and multimodal coherence modeling. Specifically, (1) we enhance
multimodal fusion by exploring different architectures using cross-attention
and mixture of experts. (2) To generally strengthen multimodality alignment and
fusion, we pre-train and fine-tune the model with multimodal contrastive
learning. (3) We propose a new pre-training task tailored for the VTS task, and
a novel fine-tuning task for enhancing multimodal coherence modeling for VTS.
We evaluate the proposed approaches on educational videos, in the form of
lectures, due to the vital role of topic segmentation of educational videos in
boosting learning experiences. Additionally, we introduce a large-scale Chinese
lecture video dataset to augment the existing English corpus, promoting further
research in VTS. Experiments on both English and Chinese lecture datasets
demonstrate that our model achieves superior VTS performance compared to
competitive unsupervised and supervised baselines.",2024-08-01 08:10:32+00:00,"['Hai Yu', 'Chong Deng', 'Qinglin Zhang', 'Jiaqing Liu', 'Qian Chen', 'Wen Wang']",http://arxiv.org/abs/2408.00365v2
DriveEditor: A Unified 3D Information-Guided Framework for Controllable Object Editing in Driving Scenes,"Vision-centric autonomous driving systems require diverse data for robust
training and evaluation, which can be augmented by manipulating object
positions and appearances within existing scene captures. While recent
advancements in diffusion models have shown promise in video editing, their
application to object manipulation in driving scenarios remains challenging due
to imprecise positional control and difficulties in preserving high-fidelity
object appearances. To address these challenges in position and appearance
control, we introduce DriveEditor, a diffusion-based framework for object
editing in driving videos. DriveEditor offers a unified framework for
comprehensive object editing operations, including repositioning, replacement,
deletion, and insertion. These diverse manipulations are all achieved through a
shared set of varying inputs, processed by identical position control and
appearance maintenance modules. The position control module projects the given
3D bounding box while preserving depth information and hierarchically injects
it into the diffusion process, enabling precise control over object position
and orientation. The appearance maintenance module preserves consistent
attributes with a single reference image by employing a three-tiered approach:
low-level detail preservation, high-level semantic maintenance, and the
integration of 3D priors from a novel view synthesis model. Extensive
qualitative and quantitative evaluations on the nuScenes dataset demonstrate
DriveEditor's exceptional fidelity and controllability in generating diverse
driving scene edits, as well as its remarkable ability to facilitate downstream
tasks. Project page: https://yvanliang.github.io/DriveEditor.",2024-12-27 04:49:36+00:00,"['Yiyuan Liang', 'Zhiying Yan', 'Liqun Chen', 'Jiahuan Zhou', 'Luxin Yan', 'Sheng Zhong', 'Xu Zou']",http://arxiv.org/abs/2412.19458v2
Latent-Reframe: Enabling Camera Control for Video Diffusion Model without Training,"Precise camera pose control is crucial for video generation with diffusion
models. Existing methods require fine-tuning with additional datasets
containing paired videos and camera pose annotations, which are both
data-intensive and computationally costly, and can disrupt the pre-trained
model distribution. We introduce Latent-Reframe, which enables camera control
in a pre-trained video diffusion model without fine-tuning. Unlike existing
methods, Latent-Reframe operates during the sampling stage, maintaining
efficiency while preserving the original model distribution. Our approach
reframes the latent code of video frames to align with the input camera
trajectory through time-aware point clouds. Latent code inpainting and
harmonization then refine the model latent space, ensuring high-quality video
generation. Experimental results demonstrate that Latent-Reframe achieves
comparable or superior camera control precision and video quality to
training-based methods, without the need for fine-tuning on additional
datasets.",2024-12-08 18:59:54+00:00,"['Zhenghong Zhou', 'Jie An', 'Jiebo Luo']",http://arxiv.org/abs/2412.06029v1
MyGo: Consistent and Controllable Multi-View Driving Video Generation with Camera Control,"High-quality driving video generation is crucial for providing training data
for autonomous driving models. However, current generative models rarely focus
on enhancing camera motion control under multi-view tasks, which is essential
for driving video generation. Therefore, we propose MyGo, an end-to-end
framework for video generation, introducing motion of onboard cameras as
conditions to make progress in camera controllability and multi-view
consistency. MyGo employs additional plug-in modules to inject camera
parameters into the pre-trained video diffusion model, which retains the
extensive knowledge of the pre-trained model as much as possible. Furthermore,
we use epipolar constraints and neighbor view information during the generation
process of each view to enhance spatial-temporal consistency. Experimental
results show that MyGo has achieved state-of-the-art results in both general
camera-controlled video generation and multi-view driving video generation
tasks, which lays the foundation for more accurate environment simulation in
autonomous driving. Project page:
https://metadrivescape.github.io/papers_project/MyGo/page.html",2024-09-10 03:39:08+00:00,"['Yining Yao', 'Xi Guo', 'Chenjing Ding', 'Wei Wu']",http://arxiv.org/abs/2409.06189v2
AVID: Adapting Video Diffusion Models to World Models,"Large-scale generative models have achieved remarkable success in a number of
domains. However, for sequential decision-making problems, such as robotics,
action-labelled data is often scarce and therefore scaling-up foundation models
for decision-making remains a challenge. A potential solution lies in
leveraging widely-available unlabelled videos to train world models that
simulate the consequences of actions. If the world model is accurate, it can be
used to optimize decision-making in downstream tasks. Image-to-video diffusion
models are already capable of generating highly realistic synthetic videos.
However, these models are not action-conditioned, and the most powerful models
are closed-source which means they cannot be finetuned. In this work, we
propose to adapt pretrained video diffusion models to action-conditioned world
models, without access to the parameters of the pretrained model. Our approach,
AVID, trains an adapter on a small domain-specific dataset of action-labelled
videos. AVID uses a learned mask to modify the intermediate outputs of the
pretrained model and generate accurate action-conditioned videos. We evaluate
AVID on video game and real-world robotics data, and show that it outperforms
existing baselines for diffusion model adaptation.1 Our results demonstrate
that if utilized correctly, pretrained video models have the potential to be
powerful tools for embodied AI.",2024-10-01 13:48:31+00:00,"['Marc Rigter', 'Tarun Gupta', 'Agrin Hilmkil', 'Chao Ma']",http://arxiv.org/abs/2410.12822v2
CameraCtrl: Enabling Camera Control for Text-to-Video Generation,"Controllability plays a crucial role in video generation, as it allows users
to create and edit content more precisely. Existing models, however, lack
control of camera pose that serves as a cinematic language to express deeper
narrative nuances. To alleviate this issue, we introduce CameraCtrl, enabling
accurate camera pose control for video diffusion models. Our approach explores
effective camera trajectory parameterization along with a plug-and-play camera
pose control module that is trained on top of a video diffusion model, leaving
other modules of the base model untouched. Moreover, a comprehensive study on
the effect of various training datasets is conducted, suggesting that videos
with diverse camera distributions and similar appearance to the base model
indeed enhance controllability and generalization. Experimental results
demonstrate the effectiveness of CameraCtrl in achieving precise camera control
with different video generation models, marking a step forward in the pursuit
of dynamic and customized video storytelling from textual and camera pose
inputs.",2024-04-02 16:52:41+00:00,"['Hao He', 'Yinghao Xu', 'Yuwei Guo', 'Gordon Wetzstein', 'Bo Dai', 'Hongsheng Li', 'Ceyuan Yang']",http://arxiv.org/abs/2404.02101v2
DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos,"Estimating video depth in open-world scenarios is challenging due to the
diversity of videos in appearance, content motion, camera movement, and length.
We present DepthCrafter, an innovative method for generating temporally
consistent long depth sequences with intricate details for open-world videos,
without requiring any supplementary information such as camera poses or optical
flow. The generalization ability to open-world videos is achieved by training
the video-to-depth model from a pre-trained image-to-video diffusion model,
through our meticulously designed three-stage training strategy. Our training
approach enables the model to generate depth sequences with variable lengths at
one time, up to 110 frames, and harvest both precise depth details and rich
content diversity from realistic and synthetic datasets. We also propose an
inference strategy that can process extremely long videos through segment-wise
estimation and seamless stitching. Comprehensive evaluations on multiple
datasets reveal that DepthCrafter achieves state-of-the-art performance in
open-world video depth estimation under zero-shot settings. Furthermore,
DepthCrafter facilitates various downstream applications, including depth-based
visual effects and conditional video generation.",2024-09-03 17:52:03+00:00,"['Wenbo Hu', 'Xiangjun Gao', 'Xiaoyu Li', 'Sijie Zhao', 'Xiaodong Cun', 'Yong Zhang', 'Long Quan', 'Ying Shan']",http://arxiv.org/abs/2409.02095v2
STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment,"Visual and auditory perception are two crucial ways humans experience the
world. Text-to-video generation has made remarkable progress over the past
year, but the absence of harmonious audio in generated video limits its broader
applications. In this paper, we propose Semantic and Temporal Aligned
Video-to-Audio (STA-V2A), an approach that enhances audio generation from
videos by extracting both local temporal and global semantic video features and
combining these refined video features with text as cross-modal guidance. To
address the issue of information redundancy in videos, we propose an onset
prediction pretext task for local temporal feature extraction and an attentive
pooling module for global semantic feature extraction. To supplement the
insufficient semantic information in videos, we propose a Latent Diffusion
Model with Text-to-Audio priors initialization and cross-modal guidance. We
also introduce Audio-Audio Align, a new metric to assess audio-temporal
alignment. Subjective and objective metrics demonstrate that our method
surpasses existing Video-to-Audio models in generating audio with better
quality, semantic consistency, and temporal alignment. The ablation experiment
validated the effectiveness of each module. Audio samples are available at
https://y-ren16.github.io/STAV2A.",2024-09-13 07:31:44+00:00,"['Yong Ren', 'Chenxing Li', 'Manjie Xu', 'Wei Liang', 'Yu Gu', 'Rilin Chen', 'Dong Yu']",http://arxiv.org/abs/2409.08601v2
Adaptive Caching for Faster Video Generation with Diffusion Transformers,"Generating temporally-consistent high-fidelity videos can be computationally
expensive, especially over longer temporal spans. More-recent Diffusion
Transformers (DiTs) -- despite making significant headway in this context --
have only heightened such challenges as they rely on larger models and heavier
attention mechanisms, resulting in slower inference speeds. In this paper, we
introduce a training-free method to accelerate video DiTs, termed Adaptive
Caching (AdaCache), which is motivated by the fact that ""not all videos are
created equal"": meaning, some videos require fewer denoising steps to attain a
reasonable quality than others. Building on this, we not only cache
computations through the diffusion process, but also devise a caching schedule
tailored to each video generation, maximizing the quality-latency trade-off. We
further introduce a Motion Regularization (MoReg) scheme to utilize video
information within AdaCache, essentially controlling the compute allocation
based on motion content. Altogether, our plug-and-play contributions grant
significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video
generation) without sacrificing the generation quality, across multiple video
DiT baselines.",2024-11-04 18:59:44+00:00,"['Kumara Kahatapitiya', 'Haozhe Liu', 'Sen He', 'Ding Liu', 'Menglin Jia', 'Chenyang Zhang', 'Michael S. Ryoo', 'Tian Xie']",http://arxiv.org/abs/2411.02397v2
Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs,"Video understanding is a crucial next step for multimodal large language
models (MLLMs). Various benchmarks are introduced for better evaluating the
MLLMs. Nevertheless, current video benchmarks are still inefficient for
evaluating video models during iterative development due to the high cost of
constructing datasets and the difficulty in isolating specific skills. In this
paper, we propose VideoNIAH (Video Needle In A Haystack), a benchmark
construction framework through synthetic video generation. VideoNIAH decouples
video content from their query-responses by inserting unrelated visual
'needles' into original videos. The framework automates the generation of
query-response pairs using predefined rules, minimizing manual labor. The
queries focus on specific aspects of video understanding, enabling more
skill-specific evaluations. The separation between video content and the
queries also allow for increased video variety and evaluations across different
lengths. Utilizing VideoNIAH, we compile a video benchmark VNBench, which
includes tasks such as retrieval, ordering, and counting to evaluate three key
aspects of video understanding: temporal perception, chronological ordering,
and spatio-temporal coherence. We conduct a comprehensive evaluation of both
proprietary and open-source models, uncovering significant differences in their
video understanding capabilities across various tasks. Additionally, we perform
an in-depth analysis of the test results and model configurations. Based on
these findings, we provide some advice for improving video MLLM training,
offering valuable insights to guide future research and model development. The
code and data are available at https://github.com/joez17/VideoNIAH.",2024-06-13 17:50:05+00:00,"['Zijia Zhao', 'Haoyu Lu', 'Yuqi Huo', 'Yifan Du', 'Tongtian Yue', 'Longteng Guo', 'Bingning Wang', 'Weipeng Chen', 'Jing Liu']",http://arxiv.org/abs/2406.09367v3
Zero-Shot Video Editing through Adaptive Sliding Score Distillation,"The rapidly evolving field of Text-to-Video generation (T2V) has catalyzed
renewed interest in controllable video editing research. While the application
of editing prompts to guide diffusion model denoising has gained prominence,
mirroring advancements in image editing, this noise-based inference process
inherently compromises the original video's integrity, resulting in unintended
over-editing and temporal discontinuities. To address these challenges, this
study proposes a novel paradigm of video-based score distillation, facilitating
direct manipulation of original video content. Specifically, distinguishing it
from image-based score distillation, we propose an Adaptive Sliding Score
Distillation strategy, which incorporates both global and local video guidance
to reduce the impact of editing errors. Combined with our proposed Image-based
Joint Guidance mechanism, it has the ability to mitigate the inherent
instability of the T2V model and single-step sampling. Additionally, we design
a Weighted Attention Fusion module to further preserve the key features of the
original video and avoid over-editing. Extensive experiments demonstrate that
these strategies effectively address existing challenges, achieving superior
performance compared to current state-of-the-art methods.",2024-06-07 12:33:59+00:00,"['Lianghan Zhu', 'Yanqi Bao', 'Jing Huo', 'Jing Wu', 'Yu-Kun Lai', 'Wenbin Li', 'Yang Gao']",http://arxiv.org/abs/2406.04888v2
DIVE: Taming DINO for Subject-Driven Video Editing,"Building on the success of diffusion models in image generation and editing,
video editing has recently gained substantial attention. However, maintaining
temporal consistency and motion alignment still remains challenging. To address
these issues, this paper proposes DINO-guided Video Editing (DIVE), a framework
designed to facilitate subject-driven editing in source videos conditioned on
either target text prompts or reference images with specific identities. The
core of DIVE lies in leveraging the powerful semantic features extracted from a
pretrained DINOv2 model as implicit correspondences to guide the editing
process. Specifically, to ensure temporal motion consistency, DIVE employs DINO
features to align with the motion trajectory of the source video. Extensive
experiments on diverse real-world videos demonstrate that our framework can
achieve high-quality editing results with robust motion consistency,
highlighting the potential of DINO to contribute to video editing. For precise
subject editing, DIVE incorporates the DINO features of reference images into a
pretrained text-to-image model to learn Low-Rank Adaptations (LoRAs),
effectively registering the target subject's identity. Project page:
https://dino-video-editing.github.io",2024-12-04 14:28:43+00:00,"['Yi Huang', 'Wei Xiong', 'He Zhang', 'Chaoqi Chen', 'Jianzhuang Liu', 'Mingfu Yan', 'Shifeng Chen']",http://arxiv.org/abs/2412.03347v1
Compositional 3D-aware Video Generation with LLM Director,"Significant progress has been made in text-to-video generation through the
use of powerful generative models and large-scale internet data. However,
substantial challenges remain in precisely controlling individual concepts
within the generated video, such as the motion and appearance of specific
characters and the movement of viewpoints. In this work, we propose a novel
paradigm that generates each concept in 3D representation separately and then
composes them with priors from Large Language Models (LLM) and 2D diffusion
models. Specifically, given an input textual prompt, our scheme consists of
three stages: 1) We leverage LLM as the director to first decompose the complex
query into several sub-prompts that indicate individual concepts within the
video~(\textit{e.g.}, scene, objects, motions), then we let LLM to invoke
pre-trained expert models to obtain corresponding 3D representations of
concepts. 2) To compose these representations, we prompt multi-modal LLM to
produce coarse guidance on the scales and coordinates of trajectories for the
objects. 3) To make the generated frames adhere to natural image distribution,
we further leverage 2D diffusion priors and use Score Distillation Sampling to
refine the composition. Extensive experiments demonstrate that our method can
generate high-fidelity videos from text with diverse motion and flexible
control over each concept. Project page: \url{https://aka.ms/c3v}.",2024-08-31 23:07:22+00:00,"['Hanxin Zhu', 'Tianyu He', 'Anni Tang', 'Junliang Guo', 'Zhibo Chen', 'Jiang Bian']",http://arxiv.org/abs/2409.00558v1
On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection,"Large numbers of synthesized videos from diffusion models pose threats to
information security and authenticity, leading to an increasing demand for
generated content detection. However, existing video-level detection algorithms
primarily focus on detecting facial forgeries and often fail to identify
diffusion-generated content with a diverse range of semantics. To advance the
field of video forensics, we propose an innovative algorithm named Multi-Modal
Detection(MM-Det) for detecting diffusion-generated videos. MM-Det utilizes the
profound perceptual and comprehensive abilities of Large Multi-modal Models
(LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's
multi-modal space, enhancing its ability to detect unseen forgery content.
Besides, MM-Det leverages an In-and-Across Frame Attention (IAFA) mechanism for
feature augmentation in the spatio-temporal domain. A dynamic fusion strategy
helps refine forgery representations for the fusion. Moreover, we construct a
comprehensive diffusion video dataset, called Diffusion Video Forensics (DVF),
across a wide range of forgery videos. MM-Det achieves state-of-the-art
performance in DVF, demonstrating the effectiveness of our algorithm. Both
source code and DVF are available at https://github.com/SparkleXFantasy/MM-Det.",2024-10-31 04:20:47+00:00,"['Xiufeng Song', 'Xiao Guo', 'Jiache Zhang', 'Qirui Li', 'Lei Bai', 'Xiaoming Liu', 'Guangtao Zhai', 'Xiaohong Liu']",http://arxiv.org/abs/2410.23623v2
Novel View Extrapolation with Video Diffusion Priors,"The field of novel view synthesis has made significant strides thanks to the
development of radiance field methods. However, most radiance field techniques
are far better at novel view interpolation than novel view extrapolation where
the synthesis novel views are far beyond the observed training views. We design
ViewExtrapolator, a novel view synthesis approach that leverages the generative
priors of Stable Video Diffusion (SVD) for realistic novel view extrapolation.
By redesigning the SVD denoising process, ViewExtrapolator refines the
artifact-prone views rendered by radiance fields, greatly enhancing the clarity
and realism of the synthesized novel views. ViewExtrapolator is a generic novel
view extrapolator that can work with different types of 3D rendering such as
views rendered from point clouds when only a single view or monocular video is
available. Additionally, ViewExtrapolator requires no fine-tuning of SVD,
making it both data-efficient and computation-efficient. Extensive experiments
demonstrate the superiority of ViewExtrapolator in novel view extrapolation.
Project page: \url{https://kunhao-liu.github.io/ViewExtrapolator/}.",2024-11-21 15:16:48+00:00,"['Kunhao Liu', 'Ling Shao', 'Shijian Lu']",http://arxiv.org/abs/2411.14208v1
MoTrans: Customized Motion Transfer with Text-driven Video Diffusion Models,"Existing pretrained text-to-video (T2V) models have demonstrated impressive
abilities in generating realistic videos with basic motion or camera movement.
However, these models exhibit significant limitations when generating
intricate, human-centric motions. Current efforts primarily focus on
fine-tuning models on a small set of videos containing a specific motion. They
often fail to effectively decouple motion and the appearance in the limited
reference videos, thereby weakening the modeling capability of motion patterns.
To this end, we propose MoTrans, a customized motion transfer method enabling
video generation of similar motion in new context. Specifically, we introduce a
multimodal large language model (MLLM)-based recaptioner to expand the initial
prompt to focus more on appearance and an appearance injection module to adapt
appearance prior from video frames to the motion modeling process. These
complementary multimodal representations from recaptioned prompt and video
frames promote the modeling of appearance and facilitate the decoupling of
appearance and motion. In addition, we devise a motion-specific embedding for
further enhancing the modeling of the specific motion. Experimental results
demonstrate that our method effectively learns specific motion pattern from
singular or multiple reference videos, performing favorably against existing
methods in customized video generation.",2024-12-02 10:07:59+00:00,"['Xiaomin Li', 'Xu Jia', 'Qinghe Wang', 'Haiwen Diao', 'Mengmeng Ge', 'Pengxiang Li', 'You He', 'Huchuan Lu']",http://arxiv.org/abs/2412.01343v1
VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video Prompting,"Large Language Model (LLM)-based agents have shown promise in procedural
tasks, but the potential of multimodal instructions augmented by texts and
videos to assist users remains under-explored. To address this gap, we propose
the Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel
LLM-empowered Multimodal Procedural Planning (MPP) framework. It generates
cohesive text and video procedural plans given a specified high-level
objective. The main challenges are achieving textual and visual
informativeness, temporal coherence, and accuracy in procedural plans. VG-TVP
leverages the zero-shot reasoning capability of LLMs, the video-to-text
generation ability of the video captioning models, and the text-to-video
generation ability of diffusion models. VG-TVP improves the interaction between
modalities by proposing a novel Fusion of Captioning (FoC) method and using
Text-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs
to guide the generation of visually-grounded text plans and textual-grounded
video plans. To address the scarcity of datasets suitable for MPP, we have
curated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We
conduct comprehensive experiments and benchmarks to evaluate human preferences
(regarding textual and visual informativeness, temporal coherence, and plan
accuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP
dataset.",2024-12-16 10:08:38+00:00,"['Muhammet Furkan Ilaslan', 'Ali Koksal', 'Kevin Qinhong Lin', 'Burak Satar', 'Mike Zheng Shou', 'Qianli Xu']",http://arxiv.org/abs/2412.11621v1
SyncFlow: Toward Temporally Aligned Joint Audio-Video Generation from Text,"Video and audio are closely correlated modalities that humans naturally
perceive together. While recent advancements have enabled the generation of
audio or video from text, producing both modalities simultaneously still
typically relies on either a cascaded process or multi-modal contrastive
encoders. These approaches, however, often lead to suboptimal results due to
inherent information losses during inference and conditioning. In this paper,
we introduce SyncFlow, a system that is capable of simultaneously generating
temporally synchronized audio and video from text. The core of SyncFlow is the
proposed dual-diffusion-transformer (d-DiT) architecture, which enables joint
video and audio modelling with proper information fusion. To efficiently manage
the computational cost of joint audio and video modelling, SyncFlow utilizes a
multi-stage training strategy that separates video and audio learning before
joint fine-tuning. Our empirical evaluations demonstrate that SyncFlow produces
audio and video outputs that are more correlated than baseline methods with
significantly enhanced audio quality and audio-visual correspondence. Moreover,
we demonstrate strong zero-shot capabilities of SyncFlow, including zero-shot
video-to-audio generation and adaptation to novel video resolutions without
further training.",2024-12-03 21:48:08+00:00,"['Haohe Liu', 'Gael Le Lan', 'Xinhao Mei', 'Zhaoheng Ni', 'Anurag Kumar', 'Varun Nagaraja', 'Wenwu Wang', 'Mark D. Plumbley', 'Yangyang Shi', 'Vikas Chandra']",http://arxiv.org/abs/2412.15220v1
ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation,"Image-to-video (I2V) generation aims to use the initial frame (alongside a
text prompt) to create a video sequence. A grand challenge in I2V generation is
to maintain visual consistency throughout the video: existing methods often
struggle to preserve the integrity of the subject, background, and style from
the first frame, as well as ensure a fluid and logical progression within the
video narrative. To mitigate these issues, we propose ConsistI2V, a
diffusion-based method to enhance visual consistency for I2V generation.
Specifically, we introduce (1) spatiotemporal attention over the first frame to
maintain spatial and motion consistency, (2) noise initialization from the
low-frequency band of the first frame to enhance layout consistency. These two
approaches enable ConsistI2V to generate highly consistent videos. We also
extend the proposed approaches to show their potential to improve consistency
in auto-regressive long video generation and camera motion control. To verify
the effectiveness of our method, we propose I2V-Bench, a comprehensive
evaluation benchmark for I2V generation. Our automatic and human evaluation
results demonstrate the superiority of ConsistI2V over existing methods.",2024-02-06 19:08:18+00:00,"['Weiming Ren', 'Huan Yang', 'Ge Zhang', 'Cong Wei', 'Xinrun Du', 'Wenhao Huang', 'Wenhu Chen']",http://arxiv.org/abs/2402.04324v2
M3-CVC: Controllable Video Compression with Multimodal Generative Models,"Traditional and neural video codecs commonly encounter limitations in
controllability and generality under ultra-low-bitrate coding scenarios. To
overcome these challenges, we propose M3-CVC, a controllable video compression
framework incorporating multimodal generative models. The framework utilizes a
semantic-motion composite strategy for keyframe selection to retain critical
information. For each keyframe and its corresponding video clip, a
dialogue-based large multimodal model (LMM) approach extracts hierarchical
spatiotemporal details, enabling both inter-frame and intra-frame
representations for improved video fidelity while enhancing encoding
interpretability. M3-CVC further employs a conditional diffusion-based,
text-guided keyframe compression method, achieving high fidelity in frame
reconstruction. During decoding, textual descriptions derived from LMMs guide
the diffusion process to restore the original video's content accurately.
Experimental results demonstrate that M3-CVC significantly outperforms the
state-of-the-art VVC standard in ultra-low bitrate scenarios, particularly in
preserving semantic and perceptual fidelity.",2024-11-24 11:56:59+00:00,"['Rui Wan', 'Qi Zheng', 'Yibo Fan']",http://arxiv.org/abs/2411.15798v2
Repurposing Pre-trained Video Diffusion Models for Event-based Video Interpolation,"Video Frame Interpolation aims to recover realistic missing frames between
observed frames, generating a high-frame-rate video from a low-frame-rate
video. However, without additional guidance, the large motion between frames
makes this problem ill-posed. Event-based Video Frame Interpolation (EVFI)
addresses this challenge by using sparse, high-temporal-resolution event
measurements as motion guidance. This guidance allows EVFI methods to
significantly outperform frame-only methods. However, to date, EVFI methods
have relied on a limited set of paired event-frame training data, severely
limiting their performance and generalization capabilities. In this work, we
overcome the limited data challenge by adapting pre-trained video diffusion
models trained on internet-scale datasets to EVFI. We experimentally validate
our approach on real-world EVFI datasets, including a new one that we
introduce. Our method outperforms existing methods and generalizes across
cameras far better than existing approaches.",2024-12-10 18:55:30+00:00,"['Jingxi Chen', 'Brandon Y. Feng', 'Haoming Cai', 'Tianfu Wang', 'Levi Burner', 'Dehao Yuan', 'Cornelia Fermuller', 'Christopher A. Metzler', 'Yiannis Aloimonos']",http://arxiv.org/abs/2412.07761v2
Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos,"We investigate exocentric-to-egocentric cross-view translation, which aims to
generate a first-person (egocentric) view of an actor based on a video
recording that captures the actor from a third-person (exocentric) perspective.
To this end, we propose a generative framework called Exo2Ego that decouples
the translation process into two stages: high-level structure transformation,
which explicitly encourages cross-view correspondence between exocentric and
egocentric views, and a diffusion-based pixel-level hallucination, which
incorporates a hand layout prior to enhance the fidelity of the generated
egocentric view. To pave the way for future advancements in this field, we
curate a comprehensive exo-to-ego cross-view translation benchmark. It consists
of a diverse collection of synchronized ego-exo tabletop activity video pairs
sourced from three public datasets: H2O, Aria Pilot, and Assembly101. The
experimental results validate that Exo2Ego delivers photorealistic video
results with clear hand manipulation details and outperforms several baselines
in terms of both synthesis quality and generalization ability to new actions.",2024-03-11 01:00:00+00:00,"['Mi Luo', 'Zihui Xue', 'Alex Dimakis', 'Kristen Grauman']",http://arxiv.org/abs/2403.06351v1
Temporally Consistent Object Editing in Videos using Extended Attention,"Image generation and editing have seen a great deal of advancements with the
rise of large-scale diffusion models that allow user control of different
modalities such as text, mask, depth maps, etc. However, controlled editing of
videos still lags behind. Prior work in this area has focused on using 2D
diffusion models to globally change the style of an existing video. On the
other hand, in many practical applications, editing localized parts of the
video is critical. In this work, we propose a method to edit videos using a
pre-trained inpainting image diffusion model. We systematically redesign the
forward path of the model by replacing the self-attention modules with an
extended version of attention modules that creates frame-level dependencies. In
this way, we ensure that the edited information will be consistent across all
the video frames no matter what the shape and position of the masked area is.
We qualitatively compare our results with state-of-the-art in terms of accuracy
on several video editing tasks like object retargeting, object replacement, and
object removal tasks. Simulations demonstrate the superior performance of the
proposed strategy.",2024-06-01 02:31:16+00:00,"['AmirHossein Zamani', 'Amir G. Aghdam', 'Tiberiu Popa', 'Eugene Belilovsky']",http://arxiv.org/abs/2406.00272v1
LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity,"Text-to-video generation enhances content creation but is highly
computationally intensive: The computational cost of Diffusion Transformers
(DiTs) scales quadratically in the number of pixels. This makes minute-length
video generation extremely expensive, limiting most existing models to
generating videos of only 10-20 seconds length. We propose a Linear-complexity
text-to-video Generation (LinGen) framework whose cost scales linearly in the
number of pixels. For the first time, LinGen enables high-resolution
minute-length video generation on a single GPU without compromising quality. It
replaces the computationally-dominant and quadratic-complexity block,
self-attention, with a linear-complexity block called MATE, which consists of
an MA-branch and a TE-branch. The MA-branch targets short-to-long-range
correlations, combining a bidirectional Mamba2 block with our token
rearrangement method, Rotary Major Scan, and our review tokens developed for
long video generation. The TE-branch is a novel TEmporal Swin Attention block
that focuses on temporal correlations between adjacent tokens and medium-range
tokens. The MATE block addresses the adjacency preservation issue of Mamba and
improves the consistency of generated videos significantly. Experimental
results show that LinGen outperforms DiT (with a 75.6% win rate) in video
quality with up to 15$\times$ (11.5$\times$) FLOPs (latency) reduction.
Furthermore, both automatic metrics and human evaluation demonstrate our
LinGen-4B yields comparable video quality to state-of-the-art models (with a
50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling,
respectively). This paves the way to hour-length movie generation and real-time
interactive video generation. We provide 68s video generation results and more
examples in our project website: https://lineargen.github.io/.",2024-12-13 04:55:10+00:00,"['Hongjie Wang', 'Chih-Yao Ma', 'Yen-Cheng Liu', 'Ji Hou', 'Tao Xu', 'Jialiang Wang', 'Felix Juefei-Xu', 'Yaqiao Luo', 'Peizhao Zhang', 'Tingbo Hou', 'Peter Vajda', 'Niraj K. Jha', 'Xiaoliang Dai']",http://arxiv.org/abs/2412.09856v1
SSM Meets Video Diffusion Models: Efficient Long-Term Video Generation with Structured State Spaces,"Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their computational costs, which increase
quadratically with the sequence length. This limitation presents significant
challenges when generating longer video sequences using diffusion models. To
overcome this challenge, we propose leveraging state-space models (SSMs) as
temporal feature extractors. SSMs (e.g., Mamba) have recently gained attention
as promising alternatives due to their linear-time memory consumption relative
to sequence length. In line with previous research suggesting that using
bidirectional SSMs is effective for understanding spatial features in image
generation, we found that bidirectionality is also beneficial for capturing
temporal features in video data, rather than relying on traditional
unidirectional SSMs. We conducted comprehensive evaluations on multiple
long-term video datasets, such as MineRL Navigate, across various model sizes.
For sequences up to 256 frames, SSM-based models require less memory to achieve
the same FVD as attention-based models. Moreover, SSM-based models often
deliver better performance with comparable GPU memory usage. Our codes are
available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.",2024-03-12 14:53:56+00:00,"['Yuta Oshima', 'Shohei Taniguchi', 'Masahiro Suzuki', 'Yutaka Matsuo']",http://arxiv.org/abs/2403.07711v4
Frchet Video Motion Distance: A Metric for Evaluating Motion Consistency in Videos,"Significant advancements have been made in video generative models recently.
Unlike image generation, video generation presents greater challenges,
requiring not only generating high-quality frames but also ensuring temporal
consistency across these frames. Despite the impressive progress, research on
metrics for evaluating the quality of generated videos, especially concerning
temporal and motion consistency, remains underexplored. To bridge this research
gap, we propose Fr\'echet Video Motion Distance (FVMD) metric, which focuses on
evaluating motion consistency in video generation. Specifically, we design
explicit motion features based on key point tracking, and then measure the
similarity between these features via the Fr\'echet distance. We conduct
sensitivity analysis by injecting noise into real videos to verify the
effectiveness of FVMD. Further, we carry out a large-scale human study,
demonstrating that our metric effectively detects temporal noise and aligns
better with human perceptions of generated video quality than existing metrics.
Additionally, our motion features can consistently improve the performance of
Video Quality Assessment (VQA) models, indicating that our approach is also
applicable to unary video quality evaluation. Code is available at
https://github.com/ljh0v0/FMD-frechet-motion-distance.",2024-07-23 02:10:50+00:00,"['Jiahe Liu', 'Youran Qu', 'Qi Yan', 'Xiaohui Zeng', 'Lele Wang', 'Renjie Liao']",http://arxiv.org/abs/2407.16124v1
GenRec: Unifying Video Generation and Recognition with Diffusion Models,"Video diffusion models are able to generate high-quality videos by learning
strong spatial-temporal priors on large-scale datasets. In this paper, we aim
to investigate whether such priors derived from a generative process are
suitable for video recognition, and eventually joint optimization of generation
and recognition. Building upon Stable Video Diffusion, we introduce GenRec, the
first unified framework trained with a random-frame conditioning process so as
to learn generalized spatial-temporal representations. The resulting framework
can naturally supports generation and recognition, and more importantly is
robust even when visual inputs contain limited information. Extensive
experiments demonstrate the efficacy of GenRec for both recognition and
generation. In particular, GenRec achieves competitive recognition performance,
offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also
performs the best on class-conditioned image-to-video generation, achieving
46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore, GenRec
demonstrates extraordinary robustness in scenarios that only limited frames can
be observed. Code will be available at https://github.com/wengzejia1/GenRec.",2024-08-27 17:59:41+00:00,"['Zejia Weng', 'Xitong Yang', 'Zhen Xing', 'Zuxuan Wu', 'Yu-Gang Jiang']",http://arxiv.org/abs/2408.15241v2
Shape of Motion: 4D Reconstruction from a Single Video,"Monocular dynamic reconstruction is a challenging and long-standing vision
problem due to the highly ill-posed nature of the task. Existing approaches are
limited in that they either depend on templates, are effective only in
quasi-static scenes, or fail to model 3D motion explicitly. In this work, we
introduce a method capable of reconstructing generic dynamic scenes, featuring
explicit, full-sequence-long 3D motion, from casually captured monocular
videos. We tackle the under-constrained nature of the problem with two key
insights: First, we exploit the low-dimensional structure of 3D motion by
representing scene motion with a compact set of SE3 motion bases. Each point's
motion is expressed as a linear combination of these bases, facilitating soft
decomposition of the scene into multiple rigidly-moving groups. Second, we
utilize a comprehensive set of data-driven priors, including monocular depth
maps and long-range 2D tracks, and devise a method to effectively consolidate
these noisy supervisory signals, resulting in a globally consistent
representation of the dynamic scene. Experiments show that our method achieves
state-of-the-art performance for both long-range 3D/2D motion estimation and
novel view synthesis on dynamic scenes. Project Page:
https://shape-of-motion.github.io/",2024-07-18 17:59:08+00:00,"['Qianqian Wang', 'Vickie Ye', 'Hang Gao', 'Jake Austin', 'Zhengqi Li', 'Angjoo Kanazawa']",http://arxiv.org/abs/2407.13764v1
AKiRa: Augmentation Kit on Rays for optical video generation,"Recent advances in text-conditioned video diffusion have greatly improved
video quality. However, these methods offer limited or sometimes no control to
users on camera aspects, including dynamic camera motion, zoom, distorted lens
and focus shifts. These motion and optical aspects are crucial for adding
controllability and cinematic elements to generation frameworks, ultimately
resulting in visual content that draws focus, enhances mood, and guides
emotions according to filmmakers' controls. In this paper, we aim to close the
gap between controllable video generation and camera optics. To achieve this,
we propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework
that builds and trains a camera adapter with a complex camera model over an
existing video generation backbone. It enables fine-tuned control over camera
motion as well as complex optical parameters (focal length, distortion,
aperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh.
Extensive experiments demonstrate AKiRa's effectiveness in combining and
composing camera optics while outperforming all state-of-the-art methods. This
work sets a new landmark in controlled and optically enhanced video generation,
paving the way for future optical video generation methods.",2024-12-18 18:53:22+00:00,"['Xi Wang', 'Robin Courant', 'Marc Christie', 'Vicky Kalogeiton']",http://arxiv.org/abs/2412.14158v2
SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation,"Human beings are endowed with a complementary learning system, which bridges
the slow learning of general world dynamics with fast storage of episodic
memory from a new experience. Previous video generation models, however,
primarily focus on slow learning by pre-training on vast amounts of data,
overlooking the fast learning phase crucial for episodic memory storage. This
oversight leads to inconsistencies across temporally distant frames when
generating longer videos, as these frames fall beyond the model's context
window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning
system for action-driven long video generation. Our approach incorporates a
masked conditional video diffusion model for the slow learning of world
dynamics, alongside an inference-time fast learning strategy based on a
temporal LoRA module. Specifically, the fast learning process updates its
temporal LoRA parameters based on local inputs and outputs, thereby efficiently
storing episodic memory in its parameters. We further propose a slow-fast
learning loop algorithm that seamlessly integrates the inner fast learning loop
into the outer slow learning loop, enabling the recall of prior multi-episode
experiences for context-aware skill learning. To facilitate the slow learning
of an approximate world model, we collect a large-scale dataset of 200k videos
with language action annotations, covering a wide range of scenarios. Extensive
experiments show that SlowFast-VGen outperforms baselines across various
metrics for action-driven video generation, achieving an FVD score of 514
compared to 782, and maintaining consistency in longer videos, with an average
of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm
significantly enhances performances on long-horizon planning tasks as well.
Project Website: https://slowfast-vgen.github.io",2024-10-30 17:55:52+00:00,"['Yining Hong', 'Beide Liu', 'Maxine Wu', 'Yuanhao Zhai', 'Kai-Wei Chang', 'Linjie Li', 'Kevin Lin', 'Chung-Ching Lin', 'Jianfeng Wang', 'Zhengyuan Yang', 'Yingnian Wu', 'Lijuan Wang']",http://arxiv.org/abs/2410.23277v2
SOAF: Scene Occlusion-aware Neural Acoustic Field,"This paper tackles the problem of novel view audio-visual synthesis along an
arbitrary trajectory in an indoor scene, given the audio-video recordings from
other known trajectories of the scene. Existing methods often overlook the
effect of room geometry, particularly wall occlusion to sound propagation,
making them less accurate in multi-room environments. In this work, we propose
a new approach called Scene Occlusion-aware Acoustic Field (SOAF) for accurate
sound generation. Our approach derives a prior for sound energy field using
distance-aware parametric sound-propagation modelling and then transforms it
based on scene transmittance learned from the input video. We extract features
from the local acoustic field centred around the receiver using a Fibonacci
Sphere to generate binaural audio for novel views with a direction-aware
attention mechanism. Extensive experiments on the real dataset RWAVS and the
synthetic dataset SoundSpaces demonstrate that our method outperforms previous
state-of-the-art techniques in audio generation. Project page:
https://github.com/huiyu-gao/SOAF/.",2024-07-02 13:40:56+00:00,"['Huiyu Gao', 'Jiahao Ma', 'David Ahmedt-Aristizabal', 'Chuong Nguyen', 'Miaomiao Liu']",http://arxiv.org/abs/2407.02264v2
LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video Generation Priors,"Single-image 3D reconstruction remains a fundamental challenge in computer
vision due to inherent geometric ambiguities and limited viewpoint information.
Recent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D
priors learned from large-scale video data. However, leveraging these priors
effectively faces three key challenges: (1) degradation in quality across large
camera motions, (2) difficulties in achieving precise camera control, and (3)
geometric distortions inherent to the diffusion process that damage 3D
consistency. We address these challenges by proposing LiftImage3D, a framework
that effectively releases LVDMs' generative priors while ensuring 3D
consistency. Specifically, we design an articulated trajectory strategy to
generate video frames, which decomposes video sequences with large camera
motions into ones with controllable small motions. Then we use robust neural
matching models, i.e. MASt3R, to calibrate the camera poses of generated frames
and produce corresponding point clouds. Finally, we propose a distortion-aware
3D Gaussian splatting representation, which can learn independent distortions
between frames and output undistorted canonical Gaussians. Extensive
experiments demonstrate that LiftImage3D achieves state-of-the-art performance
on two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and
generalizes well to diverse in-the-wild images, from cartoon illustrations to
complex real-world scenes.",2024-12-12 18:58:42+00:00,"['Yabo Chen', 'Chen Yang', 'Jiemin Fang', 'Xiaopeng Zhang', 'Lingxi Xie', 'Wei Shen', 'Wenrui Dai', 'Hongkai Xiong', 'Qi Tian']",http://arxiv.org/abs/2412.09597v1
Time-series Initialization and Conditioning for Video-agnostic Stabilization of Video Super-Resolution using Recurrent Networks,"A Recurrent Neural Network (RNN) for Video Super Resolution (VSR) is
generally trained with randomly clipped and cropped short videos extracted from
original training videos due to various challenges in learning RNNs. However,
since this RNN is optimized to super-resolve short videos, VSR of long videos
is degraded due to the domain gap. Our preliminary experiments reveal that such
degradation changes depending on the video properties, such as the video length
and dynamics. To avoid this degradation, this paper proposes the training
strategy of RNN for VSR that can work efficiently and stably independently of
the video length and dynamics. The proposed training strategy stabilizes VSR by
training a VSR network with various RNN hidden states changed depending on the
video properties. Since computing such a variety of hidden states is
time-consuming, this computational cost is reduced by reusing the hidden states
for efficient training. In addition, training stability is further improved
with frame-number conditioning. Our experimental results demonstrate that the
proposed method performed better than base methods in videos with various
lengths and dynamics.",2024-03-23 13:16:07+00:00,"['Hiroshi Mori', 'Norimichi Ukita']",http://arxiv.org/abs/2403.15832v1
Towards Holistic Language-video Representation: the language model-enhanced MSR-Video to Text Dataset,"A more robust and holistic language-video representation is the key to
pushing video understanding forward. Despite the improvement in training
strategies, the quality of the language-video dataset is less attention to. The
current plain and simple text descriptions and the visual-only focus for the
language-video tasks result in a limited capacity in real-world natural
language video retrieval tasks where queries are much more complex. This paper
introduces a method to automatically enhance video-language datasets, making
them more modality and context-aware for more sophisticated representation
learning needs, hence helping all downstream tasks. Our multifaceted video
captioning method captures entities, actions, speech transcripts, aesthetics,
and emotional cues, providing detailed and correlating information from the
text side to the video side for training. We also develop an agent-like
strategy using language models to generate high-quality, factual textual
descriptions, reducing human intervention and enabling scalability. The
method's effectiveness in improving language-video representation is evaluated
through text-video retrieval using the MSR-VTT dataset and several multi-modal
retrieval models.",2024-06-19 20:16:17+00:00,"['Yuchen Yang', 'Yingxuan Duan']",http://arxiv.org/abs/2406.13809v1
EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval,"In Composed Video Retrieval, a video and a textual description which modifies
the video content are provided as inputs to the model. The aim is to retrieve
the relevant video with the modified content from a database of videos. In this
challenging task, the first step is to acquire large-scale training datasets
and collect high-quality benchmarks for evaluation. In this work, we introduce
EgoCVR, a new evaluation benchmark for fine-grained Composed Video Retrieval
using large-scale egocentric video datasets. EgoCVR consists of 2,295 queries
that specifically focus on high-quality temporal video understanding. We find
that existing Composed Video Retrieval frameworks do not achieve the necessary
high-quality temporal video understanding for this task. To address this
shortcoming, we adapt a simple training-free method, propose a generic
re-ranking framework for Composed Video Retrieval, and demonstrate that this
achieves strong results on EgoCVR. Our code and benchmark are freely available
at https://github.com/ExplainableML/EgoCVR.",2024-07-23 17:19:23+00:00,"['Thomas Hummel', 'Shyamgopal Karthik', 'Mariana-Iuliana Georgescu', 'Zeynep Akata']",http://arxiv.org/abs/2407.16658v1
VideoOrion: Tokenizing Object Dynamics in Videos,"We present VideoOrion, a Video Large Language Model (Video-LLM) that
explicitly captures the key semantic information in videos - the
spatial-temporal dynamics of objects throughout the videos. VideoOrion employs
expert vision models to extract object dynamics through a detect-segment-track
pipeline, encoding them into a set of object tokens by aggregating
spatial-temporal object features. Our method addresses the persistent challenge
in Video-LLMs of efficiently compressing high-dimensional video data into
semantic tokens that are comprehensible to LLMs. Compared to prior methods
which resort to downsampling the original video or aggregating visual tokens
using resamplers, leading to information loss and entangled semantics,
VideoOrion not only offers a more natural and efficient way to derive compact,
disentangled semantic representations but also enables explicit object modeling
of video content with minimal computational cost. Moreover, the introduced
object tokens naturally allow VideoOrion to accomplish video-based referring
tasks. Experimental results show that VideoOrion can learn to make good use of
the object tokens, and achieves competitive results on both general video
question answering and video-based referring benchmarks.",2024-11-25 07:32:02+00:00,"['Yicheng Feng', 'Yijiang Li', 'Wanpeng Zhang', 'Hao Luo', 'Zihao Yue', 'Sipeng Zheng', 'Zongqing Lu']",http://arxiv.org/abs/2411.16156v2
Abductive Ego-View Accident Video Understanding for Safe Driving Perception,"We present MM-AU, a novel dataset for Multi-Modal Accident video
Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each
with temporally aligned text descriptions. We annotate over 2.23 million object
boxes and 58,650 pairs of video-based accident reasons, covering 58 accident
categories. MM-AU supports various accident understanding tasks, particularly
multimodal video diffusion to understand accident cause-effect chains for safe
driving. With MM-AU, we present an Abductive accident Video understanding
framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video
diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven
by an abductive CLIP model. This model involves a contrastive interaction loss
to learn the pair co-occurrence of normal, near-accident, accident frames with
the corresponding text descriptions, such as accident reasons, prevention
advice, and accident categories. OAVD enforces the causal region learning while
fixing the content of the original frame background in video generation, to
find the dominant cause-effect chain for certain accidents. Extensive
experiments verify the abductive ability of AdVersa-SD and the superiority of
OAVD against the state-of-the-art diffusion models. Additionally, we provide
careful benchmark evaluations for object detection and accident reason
answering since AdVersa-SD relies on precise object and accident reason
information.",2024-03-01 10:42:52+00:00,"['Jianwu Fang', 'Lei-lei Li', 'Junfei Zhou', 'Junbin Xiao', 'Hongkai Yu', 'Chen Lv', 'Jianru Xue', 'Tat-Seng Chua']",http://arxiv.org/abs/2403.00436v1
CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion,"Video saliency prediction aims to identify the regions in a video that
attract human attention and gaze, driven by bottom-up features from the video
and top-down processes like memory and cognition. Among these top-down
influences, language plays a crucial role in guiding attention by shaping how
visual information is interpreted. Existing methods primarily focus on modeling
perceptual information while neglecting the reasoning process facilitated by
language, where ranking cues are crucial outcomes of this process and practical
guidance for saliency prediction. In this paper, we propose CaRDiff (Caption,
Rank, and generate with Diffusion), a framework that imitates the process by
integrating a multimodal large language model (MLLM), a grounding module, and a
diffusion model, to enhance video saliency prediction. Specifically, we
introduce a novel prompting method VSOR-CoT (Video Salient Object Ranking Chain
of Thought), which utilizes an MLLM with a grounding module to caption video
content and infer salient objects along with their rankings and positions. This
process derives ranking maps that can be sufficiently leveraged by the
diffusion model to decode the saliency maps for the given video accurately.
Extensive experiments show the effectiveness of VSOR-CoT in improving the
performance of video saliency prediction. The proposed CaRDiff performs better
than state-of-the-art models on the MVS dataset and demonstrates cross-dataset
capabilities on the DHF1k dataset through zero-shot evaluation.",2024-08-21 21:40:30+00:00,"['Yunlong Tang', 'Gen Zhan', 'Li Yang', 'Yiting Liao', 'Chenliang Xu']",http://arxiv.org/abs/2408.12009v1
Collaboratively Self-supervised Video Representation Learning for Action Recognition,"Considering the close connection between action recognition and human pose
estimation, we design a Collaboratively Self-supervised Video Representation
(CSVR) learning framework specific to action recognition by jointly factoring
in generative pose prediction and discriminative context matching as pretext
tasks. Specifically, our CSVR consists of three branches: a generative pose
prediction branch, a discriminative context matching branch, and a video
generating branch. Among them, the first one encodes dynamic motion feature by
utilizing Conditional-GAN to predict the human poses of future frames, and the
second branch extracts static context features by contrasting positive and
negative video feature and I-frame feature pairs. The third branch is designed
to generate both current and future video frames, for the purpose of
collaboratively improving dynamic motion features and static context features.
Extensive experiments demonstrate that our method achieves state-of-the-art
performance on multiple popular video datasets.",2024-01-15 10:42:04+00:00,"['Jie Zhang', 'Zhifan Wan', 'Lanqing Hu', 'Stephen Lin', 'Shuzhe Wu', 'Shiguang Shan']",http://arxiv.org/abs/2401.07584v2
MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation,"The growing demand for high-fidelity video generation from textual
descriptions has catalyzed significant research in this field. In this work, we
introduce MagicVideo-V2 that integrates the text-to-image model, video motion
generator, reference image embedding module and frame interpolation module into
an end-to-end video generation pipeline. Benefiting from these architecture
designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution
video with remarkable fidelity and smoothness. It demonstrates superior
performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,
Moon Valley and Stable Video Diffusion model via user evaluation at large
scale.",2024-01-09 10:12:52+00:00,"['Weimin Wang', 'Jiawei Liu', 'Zhijie Lin', 'Jiangqiao Yan', 'Shuo Chen', 'Chetwin Low', 'Tuyen Hoang', 'Jie Wu', 'Jun Hao Liew', 'Hanshu Yan', 'Daquan Zhou', 'Jiashi Feng']",http://arxiv.org/abs/2401.04468v1
SAVGBench: Benchmarking Spatially Aligned Audio-Video Generation,"This work addresses the lack of multimodal generative models capable of
producing high-quality videos with spatially aligned audio. While recent
advancements in generative models have been successful in video generation,
they often overlook the spatial alignment between audio and visuals, which is
essential for immersive experiences. To tackle this problem, we establish a new
research direction in benchmarking Spatially Aligned Audio-Video Generation
(SAVG). We propose three key components for the benchmark: dataset, baseline,
and metrics. We introduce a spatially aligned audio-visual dataset, derived
from an audio-visual dataset consisting of multichannel audio, video, and
spatiotemporal annotations of sound events. We propose a baseline audio-visual
diffusion model focused on stereo audio-visual joint learning to accommodate
spatial sound. Finally, we present metrics to evaluate video and spatial audio
quality, including a new spatial audio-visual alignment metric. Our
experimental result demonstrates that gaps exist between the baseline model and
ground truth in terms of video and audio quality, and spatial alignment between
both modalities.",2024-12-18 03:18:03+00:00,"['Kazuki Shimada', 'Christian Simon', 'Takashi Shibuya', 'Shusuke Takahashi', 'Yuki Mitsufuji']",http://arxiv.org/abs/2412.13462v1
High-Frequency Enhanced Hybrid Neural Representation for Video Compression,"Neural Representations for Videos (NeRV) have simplified the video codec
process and achieved swift decoding speeds by encoding video content into a
neural network, presenting a promising solution for video compression. However,
existing work overlooks the crucial issue that videos reconstructed by these
methods lack high-frequency details. To address this problem, this paper
introduces a High-Frequency Enhanced Hybrid Neural Representation Network. Our
method focuses on leveraging high-frequency information to improve the
synthesis of fine details by the network. Specifically, we design a wavelet
high-frequency encoder that incorporates Wavelet Frequency Decomposer (WFD)
blocks to generate high-frequency feature embeddings. Next, we design the
High-Frequency Feature Modulation (HFM) block, which leverages the extracted
high-frequency embeddings to enhance the fitting process of the decoder.
Finally, with the refined Harmonic decoder block and a Dynamic Weighted
Frequency Loss, we further reduce the potential loss of high-frequency
information. Experiments on the Bunny and UVG datasets demonstrate that our
method outperforms other methods, showing notable improvements in detail
preservation and compression performance.",2024-11-11 03:04:46+00:00,"['Li Yu', 'Zhihui Li', 'Jimin Xiao', 'Moncef Gabbouj']",http://arxiv.org/abs/2411.06685v1
HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data,"In the domain of Multimodal Large Language Models (MLLMs), achieving
human-centric video understanding remains a formidable challenge. Existing
benchmarks primarily emphasize object and action recognition, often neglecting
the intricate nuances of human emotions, behaviors, and speech-visual alignment
within video content. We present HumanVBench, an innovative benchmark
meticulously crafted to bridge these gaps in the evaluation of video MLLMs.
HumanVBench comprises 16 carefully designed tasks that explore two primary
dimensions: inner emotion and outer manifestations, spanning static and
dynamic, basic and complex, as well as single-modal and cross-modal aspects.
With two advanced automated pipelines for video annotation and
distractor-included QA generation, HumanVBench utilizes diverse
state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and
quality assessment, minimizing human annotation dependency tailored to
human-centric multimodal attributes. A comprehensive evaluation across 22 SOTA
video MLLMs reveals notable limitations in current performance, especially in
cross-modal and emotion perception, underscoring the necessity for further
refinement toward achieving more human-like understanding. HumanVBench is
open-sourced to facilitate future advancements and real-world applications in
video MLLMs.",2024-12-23 13:45:56+00:00,"['Ting Zhou', 'Daoyuan Chen', 'Qirui Jiao', 'Bolin Ding', 'Yaliang Li', 'Ying Shen']",http://arxiv.org/abs/2412.17574v2
Depth Any Video with Scalable Synthetic Data,"Video depth estimation has long been hindered by the scarcity of consistent
and scalable ground truth data, leading to inconsistent and unreliable results.
In this paper, we introduce Depth Any Video, a model that tackles the challenge
through two key innovations. First, we develop a scalable synthetic data
pipeline, capturing real-time video depth data from diverse virtual
environments, yielding 40,000 video clips of 5-second duration, each with
precise depth annotations. Second, we leverage the powerful priors of
generative video diffusion models to handle real-world videos effectively,
integrating advanced techniques such as rotary position encoding and flow
matching to further enhance flexibility and efficiency. Unlike previous models,
which are limited to fixed-length video sequences, our approach introduces a
novel mixed-duration training strategy that handles videos of varying lengths
and performs robustly across different frame rates-even on single frames. At
inference, we propose a depth interpolation method that enables our model to
infer high-resolution video depth across sequences of up to 150 frames. Our
model outperforms all previous generative depth models in terms of spatial
accuracy and temporal consistency. The code and model weights are open-sourced.",2024-10-14 17:59:46+00:00,"['Honghui Yang', 'Di Huang', 'Wei Yin', 'Chunhua Shen', 'Haifeng Liu', 'Xiaofei He', 'Binbin Lin', 'Wanli Ouyang', 'Tong He']",http://arxiv.org/abs/2410.10815v2
Blended Latent Diffusion under Attention Control for Real-World Video Editing,"Due to lack of fully publicly available text-to-video models, current video
editing methods tend to build on pre-trained text-to-image generation models,
however, they still face grand challenges in dealing with the local editing of
video with temporal information. First, although existing methods attempt to
focus on local area editing by a pre-defined mask, the preservation of the
outside-area background is non-ideal due to the spatially entire generation of
each frame. In addition, specially providing a mask by user is an additional
costly undertaking, so an autonomous masking strategy integrated into the
editing process is desirable. Last but not least, image-level pretrained model
hasn't learned temporal information across frames of a video which is vital for
expressing the motion and dynamics. In this paper, we propose to adapt a
image-level blended latent diffusion model to perform local video editing
tasks. Specifically, we leverage DDIM inversion to acquire the latents as
background latents instead of the randomly noised ones to better preserve the
background information of the input video. We further introduce an autonomous
mask manufacture mechanism derived from cross-attention maps in diffusion
steps. Finally, we enhance the temporal consistency across video frames by
transforming the self-attention blocks of U-Net into temporal-spatial blocks.
Through extensive experiments, our proposed approach demonstrates effectiveness
in different real-world video editing tasks.",2024-09-05 13:23:52+00:00,"['Deyin Liu', 'Lin Yuanbo Wu', 'Xianghua Xie']",http://arxiv.org/abs/2409.03514v1
Learning to See Through Dazzle,"Machine vision is susceptible to laser dazzle, where intense laser light can
blind and distort its perception of the environment through oversaturation or
permanent damage to sensor pixels. Here we employ a wavefront-coded phase mask
to diffuse the energy of laser light and introduce a sandwich generative
adversarial network (SGAN) to restore images from complex image degradations,
such as varying laser-induced image saturation, mask-induced image blurring,
unknown lighting conditions, and various noise corruptions. The SGAN
architecture combines discriminative and generative methods by wrapping two
GANs around a learnable image deconvolution module. In addition, we make use of
Fourier feature representations to reduce the spectral bias of neural networks
and improve its learning of high-frequency image details. End-to-end training
includes the realistic physics-based synthesis of a large set of training data
from publicly available images. We trained the SGAN to suppress the peak laser
irradiance as high as $10^6$ times the sensor saturation threshold - the point
at which camera sensors may experience damage without the mask. The trained
model was evaluated on both a synthetic data set and data collected from the
laboratory. The proposed image restoration model quantitatively and
qualitatively outperforms state-of-the-art methods for a wide range of scene
contents, laser powers, incident laser angles, ambient illumination strengths,
and noise characteristics.",2024-02-24 22:22:02+00:00,"['Xiaopeng Peng', 'Erin F. Fleet', 'Abbie T. Watnik', 'Grover A. Swartzlander']",http://arxiv.org/abs/2402.15919v2
VGMShield: Mitigating Misuse of Video Generative Models,"With the rapid advancement in video generation, people can conveniently
utilize video generation models to create videos tailored to their specific
desires. Nevertheless, there are also growing concerns about their potential
misuse in creating and disseminating false information.
  In this work, we introduce VGMShield: a set of three straightforward but
pioneering mitigations through the lifecycle of fake video generation. We start
from \textit{fake video detection} trying to understand whether there is
uniqueness in generated videos and whether we can differentiate them from real
videos; then, we investigate the \textit{tracing} problem, which maps a fake
video back to a model that generates it. Towards these, we propose to leverage
pre-trained models that focus on {\it spatial-temporal dynamics} as the
backbone to identify inconsistencies in videos. Through experiments on seven
state-of-the-art open-source models, we demonstrate that current models still
cannot perfectly handle spatial-temporal relationships, and thus, we can
accomplish detection and tracing with nearly perfect accuracy.
  Furthermore, anticipating future generative model improvements, we propose a
{\it prevention} method that adds invisible perturbations to images to make the
generated videos look unreal. Together with fake video detection and tracing,
our multi-faceted set of solutions can effectively mitigate misuse of video
generative models.",2024-02-20 16:39:23+00:00,"['Yan Pang', 'Yang Zhang', 'Tianhao Wang']",http://arxiv.org/abs/2402.13126v1
VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos,"We present a framework for learning to generate background music from video
inputs. Unlike existing works that rely on symbolic musical annotations, which
are limited in quantity and diversity, our method leverages large-scale web
videos accompanied by background music. This enables our model to learn to
generate realistic and diverse music. To accomplish this goal, we develop a
generative video-music Transformer with a novel semantic video-music alignment
scheme. Our model uses a joint autoregressive and contrastive learning
objective, which encourages the generation of music aligned with high-level
video content. We also introduce a novel video-beat alignment scheme to match
the generated music beats with the low-level motions in the video. Lastly, to
capture fine-grained visual cues in a video needed for realistic background
music generation, we introduce a new temporal video encoder architecture,
allowing us to efficiently process videos consisting of many densely sampled
frames. We train our framework on our newly curated DISCO-MV dataset,
consisting of 2.2M video-music samples, which is orders of magnitude larger
than any prior datasets used for video music generation. Our method outperforms
existing approaches on the DISCO-MV and MusicCaps datasets according to various
music generation evaluation metrics, including human evaluation. Results are
available at https://genjib.github.io/project_page/VMAs/index.html",2024-09-11 17:56:48+00:00,"['Yan-Bo Lin', 'Yu Tian', 'Linjie Yang', 'Gedas Bertasius', 'Heng Wang']",http://arxiv.org/abs/2409.07450v1
FaceVid-1K: A Large-Scale High-Quality Multiracial Human Face Video Dataset,"Generating talking face videos from various conditions has recently become a
highly popular research area within generative tasks. However, building a
high-quality face video generation model requires a well-performing pre-trained
backbone, a key obstacle that universal models fail to adequately address. Most
existing works rely on universal video or image generation models and optimize
control mechanisms, but they neglect the evident upper bound in video quality
due to the limited capabilities of the backbones, which is a result of the lack
of high-quality human face video datasets. In this work, we investigate the
unsatisfactory results from related studies, gather and trim existing public
talking face video datasets, and additionally collect and annotate a
large-scale dataset, resulting in a comprehensive, high-quality multiracial
face collection named \textbf{FaceVid-1K}. Using this dataset, we craft several
effective pre-trained backbone models for face video generation. Specifically,
we conduct experiments with several well-established video generation models,
including text-to-video, image-to-video, and unconditional video generation,
under various settings. We obtain the corresponding performance benchmarks and
compared them with those trained on public datasets to demonstrate the
superiority of our dataset. These experiments also allow us to investigate
empirical strategies for crafting domain-specific video generation tasks with
cost-effective settings. We will make our curated dataset, along with the
pre-trained talking face video generation models, publicly available as a
resource contribution to hopefully advance the research field.",2024-09-23 07:27:02+00:00,"['Donglin Di', 'He Feng', 'Wenzhang Sun', 'Yongjia Ma', 'Hao Li', 'Wei Chen', 'Xiaofei Gou', 'Tonghua Su', 'Xun Yang']",http://arxiv.org/abs/2410.07151v1
DeCoF: Generated Video Detection via Frame Consistency: The First Benchmark Dataset,"The escalating quality of video generated by advanced video generation
methods results in new security challenges, while there have been few relevant
research efforts: 1) There is no open-source dataset for generated video
detection, 2) No generated video detection method has been proposed so far. To
this end, we propose an open-source dataset and a detection method for
generated video for the first time. First, we propose a scalable dataset
consisting of 964 prompts, covering various forgery targets, scenes, behaviors,
and actions, as well as various generation models with different architectures
and generation methods, including the most popular commercial models like
OpenAI's Sora and Google's Veo. Second, we found via probing experiments that
spatial artifact-based detectors lack generalizability. Hence, we propose a
simple yet effective \textbf{de}tection model based on \textbf{f}rame
\textbf{co}nsistency (\textbf{DeCoF}), which focuses on temporal artifacts by
eliminating the impact of spatial artifacts during feature learning. Extensive
experiments demonstrate the efficacy of DeCoF in detecting videos generated by
unseen video generation models and confirm its powerful generalizability across
several commercially proprietary models. Our code and dataset will be released
at \url{https://github.com/wuwuwuyue/DeCoF}.",2024-02-03 08:52:06+00:00,"['Long Ma', 'Jiajia Zhang', 'Hongping Deng', 'Ningyu Zhang', 'Qinglang Guo', 'Haiyang Yu', 'Yong Liao', 'Pengyuan Zhou']",http://arxiv.org/abs/2402.02085v6
Your Image is Secretly the Last Frame of a Pseudo Video,"Diffusion models, which can be viewed as a special case of hierarchical
variational autoencoders (HVAEs), have shown profound success in generating
photo-realistic images. In contrast, standard HVAEs often produce images of
inferior quality compared to diffusion models. In this paper, we hypothesize
that the success of diffusion models can be partly attributed to the additional
self-supervision information for their intermediate latent states provided by
corrupted images, which along with the original image form a pseudo video.
Based on this hypothesis, we explore the possibility of improving other types
of generative models with such pseudo videos. Specifically, we first extend a
given image generative model to their video generative model counterpart, and
then train the video generative model on pseudo videos constructed by applying
data augmentation to the original images. Furthermore, we analyze the potential
issues of first-order Markov data augmentation methods, which are typically
used in diffusion models, and propose to use more expressive data augmentation
to construct more useful information in pseudo videos. Our empirical results on
the CIFAR10 and CelebA datasets demonstrate that improved image generation
quality can be achieved with additional self-supervised information from pseudo
videos.",2024-10-26 12:15:25+00:00,"['Wenlong Chen', 'Wenlin Chen', 'Lapo Rastrelli', 'Yingzhen Li']",http://arxiv.org/abs/2410.20158v2
Raformer: Redundancy-Aware Transformer for Video Wire Inpainting,"Video Wire Inpainting (VWI) is a prominent application in video inpainting,
aimed at flawlessly removing wires in films or TV series, offering significant
time and labor savings compared to manual frame-by-frame removal. However, wire
removal poses greater challenges due to the wires being longer and slimmer than
objects typically targeted in general video inpainting tasks, and often
intersecting with people and background objects irregularly, which adds
complexity to the inpainting process. Recognizing the limitations posed by
existing video wire datasets, which are characterized by their small size, poor
quality, and limited variety of scenes, we introduce a new VWI dataset with a
novel mask generation strategy, namely Wire Removal Video Dataset 2 (WRV2) and
Pseudo Wire-Shaped (PWS) Masks. WRV2 dataset comprises over 4,000 videos with
an average length of 80 frames, designed to facilitate the development and
efficacy of inpainting models. Building upon this, our research proposes the
Redundancy-Aware Transformer (Raformer) method that addresses the unique
challenges of wire removal in video inpainting. Unlike conventional approaches
that indiscriminately process all frame patches, Raformer employs a novel
strategy to selectively bypass redundant parts, such as static background
segments devoid of valuable information for inpainting. At the core of Raformer
is the Redundancy-Aware Attention (RAA) module, which isolates and accentuates
essential content through a coarse-grained, window-based attention mechanism.
This is complemented by a Soft Feature Alignment (SFA) module, which refines
these features and achieves end-to-end feature alignment. Extensive experiments
on both the traditional video inpainting datasets and our proposed WRV2 dataset
demonstrate that Raformer outperforms other state-of-the-art methods.",2024-04-24 11:02:13+00:00,"['Zhong Ji', 'Yimu Su', 'Yan Zhang', 'Jiacheng Hou', 'Yanwei Pang', 'Jungong Han']",http://arxiv.org/abs/2404.15802v1
"Your Interest, Your Summaries: Query-Focused Long Video Summarization","Generating a concise and informative video summary from a long video is
important, yet subjective due to varying scene importance. Users' ability to
specify scene importance through text queries enhances the relevance of such
summaries. This paper introduces an approach for query-focused video
summarization, aiming to align video summaries closely with user queries. To
this end, we propose the Fully Convolutional Sequence Network with Attention
(FCSNA-QFVS), a novel approach designed for this task. Leveraging temporal
convolutional and attention mechanisms, our model effectively extracts and
highlights relevant content based on user-specified queries. Experimental
validation on a benchmark dataset for query-focused video summarization
demonstrates the effectiveness of our approach.",2024-10-17 23:37:58+00:00,"['Nirav Patel', 'Payal Prajapati', 'Maitrik Shah']",http://arxiv.org/abs/2410.14087v1
Turb-Seg-Res: A Segment-then-Restore Pipeline for Dynamic Videos with Atmospheric Turbulence,"Tackling image degradation due to atmospheric turbulence, particularly in
dynamic environment, remains a challenge for long-range imaging systems.
Existing techniques have been primarily designed for static scenes or scenes
with small motion. This paper presents the first segment-then-restore pipeline
for restoring the videos of dynamic scenes in turbulent environment. We
leverage mean optical flow with an unsupervised motion segmentation method to
separate dynamic and static scene components prior to restoration. After camera
shake compensation and segmentation, we introduce foreground/background
enhancement leveraging the statistics of turbulence strength and a transformer
model trained on a novel noise-based procedural turbulence generator for fast
dataset augmentation. Benchmarked against existing restoration methods, our
approach restores most of the geometric distortion and enhances sharpness for
videos. We make our code, simulator, and data publicly available to advance the
field of video restoration from turbulence: riponcs.github.io/TurbSegRes",2024-04-21 10:28:34+00:00,"['Ripon Kumar Saha', 'Dehao Qin', 'Nianyi Li', 'Jinwei Ye', 'Suren Jayasuriya']",http://arxiv.org/abs/2404.13605v1
Semi-supervised Video Semantic Segmentation Using Unreliable Pseudo Labels for PVUW2024,"Pixel-level Scene Understanding is one of the fundamental problems in
computer vision, which aims at recognizing object classes, masks and semantics
of each pixel in the given image. Compared with image scene parsing, video
scene parsing introduces temporal information, which can effectively improve
the consistency and accuracy of prediction,because the real-world is actually
video-based rather than a static state. In this paper, we adopt semi-supervised
video semantic segmentation method based on unreliable pseudo labels. Then, We
ensemble the teacher network model with the student network model to generate
pseudo labels and retrain the student network. Our method achieves the mIoU
scores of 63.71% and 67.83% on development test and final test respectively.
Finally, we obtain the 1st place in the Video Scene Parsing in the Wild
Challenge at CVPR 2024.",2024-06-02 01:37:26+00:00,"['Biao Wu', 'Diankai Zhang', 'Si Gao', 'Chengjian Zheng', 'Shaoli Liu', 'Ning Wang']",http://arxiv.org/abs/2406.00587v1
One-Shot Learning Meets Depth Diffusion in Multi-Object Videos,"Creating editable videos that depict complex interactions between multiple
objects in various artistic styles has long been a challenging task in
filmmaking. Progress is often hampered by the scarcity of data sets that
contain paired text descriptions and corresponding videos that showcase these
interactions. This paper introduces a novel depth-conditioning approach that
significantly advances this field by enabling the generation of coherent and
diverse videos from just a single text-video pair using a pre-trained
depth-aware Text-to-Image (T2I) model. Our method fine-tunes the pre-trained
model to capture continuous motion by employing custom-designed spatial and
temporal attention mechanisms. During inference, we use the DDIM inversion to
provide structural guidance for video generation. This innovative technique
allows for continuously controllable depth in videos, facilitating the
generation of multiobject interactions while maintaining the concept generation
and compositional strengths of the original T2I model across various artistic
styles, such as photorealism, animation, and impressionism.",2024-08-29 16:58:10+00:00,['Anisha Jain'],http://arxiv.org/abs/2408.16704v1
Trajectory Attention for Fine-grained Video Motion Control,"Recent advancements in video generation have been greatly driven by video
diffusion models, with camera motion control emerging as a crucial challenge in
creating view-customized visual content. This paper introduces trajectory
attention, a novel approach that performs attention along available pixel
trajectories for fine-grained camera motion control. Unlike existing methods
that often yield imprecise outputs or neglect temporal correlations, our
approach possesses a stronger inductive bias that seamlessly injects trajectory
information into the video generation process. Importantly, our approach models
trajectory attention as an auxiliary branch alongside traditional temporal
attention. This design enables the original temporal attention and the
trajectory attention to work in synergy, ensuring both precise motion control
and new content generation capability, which is critical when the trajectory is
only partially available. Experiments on camera motion control for images and
videos demonstrate significant improvements in precision and long-range
consistency while maintaining high-quality generation. Furthermore, we show
that our approach can be extended to other video motion control tasks, such as
first-frame-guided video editing, where it excels in maintaining content
consistency over large spatial and temporal ranges.",2024-11-28 18:59:51+00:00,"['Zeqi Xiao', 'Wenqi Ouyang', 'Yifan Zhou', 'Shuai Yang', 'Lei Yang', 'Jianlou Si', 'Xingang Pan']",http://arxiv.org/abs/2411.19324v1
OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation,"Recent advancements in visual generation technologies have markedly increased
the scale and availability of video datasets, which are crucial for training
effective video generation models. However, a significant lack of high-quality,
human-centric video datasets presents a challenge to progress in this field. To
bridge this gap, we introduce OpenHumanVid, a large-scale and high-quality
human-centric video dataset characterized by precise and detailed captions that
encompass both human appearance and motion states, along with supplementary
human motion conditions, including skeleton sequences and speech audio. To
validate the efficacy of this dataset and the associated training strategies,
we propose an extension of existing classical diffusion transformer
architectures and conduct further pretraining of our models on the proposed
dataset. Our findings yield two critical insights: First, the incorporation of
a large-scale, high-quality dataset substantially enhances evaluation metrics
for generated human videos while preserving performance in general video
generation tasks. Second, the effective alignment of text with human
appearance, human motion, and facial motion is essential for producing
high-quality video outputs. Based on these insights and corresponding
methodologies, the straightforward extended network trained on the proposed
dataset demonstrates an obvious improvement in the generation of human-centric
videos. Project page https://fudan-generative-vision.github.io/OpenHumanVid",2024-11-28 07:01:06+00:00,"['Hui Li', 'Mingwang Xu', 'Yun Zhan', 'Shan Mu', 'Jiaye Li', 'Kaihui Cheng', 'Yuxuan Chen', 'Tan Chen', 'Mao Ye', 'Jingdong Wang', 'Siyu Zhu']",http://arxiv.org/abs/2412.00115v3
The Lost Melody: Empirical Observations on Text-to-Video Generation From A Storytelling Perspective,"Text-to-video generation task has witnessed a notable progress, with the
generated outcomes reflecting the text prompts with high fidelity and
impressive visual qualities. However, current text-to-video generation models
are invariably focused on conveying the visual elements of a single scene, and
have so far been indifferent to another important potential of the medium,
namely a storytelling. In this paper, we examine text-to-video generation from
a storytelling perspective, which has been hardly investigated, and make
empirical remarks that spotlight the limitations of current text-to-video
generation scheme. We also propose an evaluation framework for storytelling
aspects of videos, and discuss the potential future directions.",2024-05-13 02:25:08+00:00,"['Andrew Shin', 'Yusuke Mori', 'Kunitake Kaneko']",http://arxiv.org/abs/2405.08720v1
Lifelong Learning of Video Diffusion Models From a Single Video Stream,"This work demonstrates that training autoregressive video diffusion models
from a single, continuous video stream is not only possible but remarkably can
also be competitive with standard offline training approaches given the same
number of gradient steps. Our demonstration further reveals that this main
result can be achieved using experience replay that only retains a subset of
the preceding video stream. We also contribute three new single video
generative modeling datasets suitable for evaluating lifelong video model
learning: Lifelong Bouncing Balls, Lifelong 3D Maze, and Lifelong PLAICraft.
Each dataset contains over a million consecutive frames from a synthetic
environment of increasing complexity.",2024-06-07 10:32:23+00:00,"['Jason Yoo', 'Yingchen He', 'Saeid Naderiparizi', 'Dylan Green', 'Gido M. van de Ven', 'Geoff Pleiss', 'Frank Wood']",http://arxiv.org/abs/2406.04814v2
Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model,"4D driving simulation is essential for developing realistic autonomous
driving simulators. Despite advancements in existing methods for generating
driving scenes, significant challenges remain in view transformation and
spatial-temporal dynamic modeling. To address these limitations, we propose a
Spatial-Temporal simulAtion for drivinG (Stag-1) model to reconstruct
real-world scenes and design a controllable generative network to achieve 4D
simulation. Stag-1 constructs continuous 4D point cloud scenes using
surround-view data from autonomous vehicles. It decouples spatial-temporal
relationships and produces coherent keyframe videos. Additionally, Stag-1
leverages video generation models to obtain photo-realistic and controllable 4D
driving simulation videos from any perspective. To expand the range of view
generation, we train vehicle motion videos based on decomposed camera poses,
enhancing modeling capabilities for distant scenes. Furthermore, we reconstruct
vehicle camera trajectories to integrate 3D points across consecutive views,
enabling comprehensive scene understanding along the temporal dimension.
Following extensive multi-level scene training, Stag-1 can simulate from any
desired viewpoint and achieve a deep understanding of scene evolution under
static spatial-temporal conditions. Compared to existing methods, our approach
shows promising performance in multi-view scene consistency, background
coherence, and accuracy, and contributes to the ongoing advancements in
realistic autonomous driving simulation. Code: https://github.com/wzzheng/Stag.",2024-12-06 18:59:56+00:00,"['Lening Wang', 'Wenzhao Zheng', 'Dalong Du', 'Yunpeng Zhang', 'Yilong Ren', 'Han Jiang', 'Zhiyong Cui', 'Haiyang Yu', 'Jie Zhou', 'Jiwen Lu', 'Shanghang Zhang']",http://arxiv.org/abs/2412.05280v2
Foundation Models for Video Understanding: A Survey,"Video Foundation Models (ViFMs) aim to learn a general-purpose representation
for various video understanding tasks. Leveraging large-scale datasets and
powerful models, ViFMs achieve this by capturing robust and generic features
from video data. This survey analyzes over 200 video foundational models,
offering a comprehensive overview of benchmarks and evaluation metrics across
14 distinct video tasks categorized into 3 main categories. Additionally, we
offer an in-depth performance analysis of these models for the 6 most common
video tasks. We categorize ViFMs into three categories: 1) Image-based ViFMs,
which adapt existing image models for video tasks, 2) Video-Based ViFMs, which
utilize video-specific encoding methods, and 3) Universal Foundational Models
(UFMs), which combine multiple modalities (image, video, audio, and text etc.)
within a single framework. By comparing the performance of various ViFMs on
different tasks, this survey offers valuable insights into their strengths and
weaknesses, guiding future advancements in video understanding. Our analysis
surprisingly reveals that image-based foundation models consistently outperform
video-based models on most video understanding tasks. Additionally, UFMs, which
leverage diverse modalities, demonstrate superior performance on video tasks.
We share the comprehensive list of ViFMs studied in this work at:
\url{https://github.com/NeeluMadan/ViFM_Survey.git}",2024-05-06 18:09:48+00:00,"['Neelu Madan', 'Andreas Moegelmose', 'Rajat Modi', 'Yogesh S. Rawat', 'Thomas B. Moeslund']",http://arxiv.org/abs/2405.03770v1
MotionMaster: Training-free Camera Motion Transfer For Video Generation,"The emergence of diffusion models has greatly propelled the progress in image
and video generation. Recently, some efforts have been made in controllable
video generation, including text-to-video generation and video motion control,
among which camera motion control is an important topic. However, existing
camera motion control methods rely on training a temporal camera module, and
necessitate substantial computation resources due to the large amount of
parameters in video generation models. Moreover, existing methods pre-define
camera motion types during training, which limits their flexibility in camera
control. Therefore, to reduce training costs and achieve flexible camera
control, we propose COMD, a novel training-free video motion transfer model,
which disentangles camera motions and object motions in source videos and
transfers the extracted camera motions to new videos. We first propose a
one-shot camera motion disentanglement method to extract camera motion from a
single source video, which separates the moving objects from the background and
estimates the camera motion in the moving objects region based on the motion in
the background by solving a Poisson equation. Furthermore, we propose a
few-shot camera motion disentanglement method to extract the common camera
motion from multiple videos with similar camera motions, which employs a
window-based clustering technique to extract the common features in temporal
attention maps of multiple videos. Finally, we propose a motion combination
method to combine different types of camera motions together, enabling our
model a more controllable and flexible camera control. Extensive experiments
demonstrate that our training-free approach can effectively decouple
camera-object motion and apply the decoupled camera motion to a wide range of
controllable video generation tasks, achieving flexible and diverse camera
motion control.",2024-04-24 10:28:54+00:00,"['Teng Hu', 'Jiangning Zhang', 'Ran Yi', 'Yating Wang', 'Hongrui Huang', 'Jieyu Weng', 'Yabiao Wang', 'Lizhuang Ma']",http://arxiv.org/abs/2404.15789v2
OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation,"Text-to-video (T2V) generation has recently garnered significant attention
thanks to the large multi-modality model Sora. However, T2V generation still
faces two important challenges: 1) Lacking a precise open sourced high-quality
dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,
are either with low quality or too large for most research institutions.
Therefore, it is challenging but crucial to collect a precise high-quality
text-video pairs for T2V generation. 2) Ignoring to fully utilize textual
information. Recent T2V methods have focused on vision transformers, using a
simple cross attention module for video generation, which falls short of
thoroughly extracting semantic information from text prompt. To address these
issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive
captions. This open-scenario dataset contains over 1 million text-video pairs,
facilitating research on T2V generation. Furthermore, we curate 433K 1080p
videos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition
video generation. Additionally, we propose a novel Multi-modal Video Diffusion
Transformer (MVDiT) capable of mining both structure information from visual
tokens and semantic information from text tokens. Extensive experiments and
ablation studies verify the superiority of OpenVid-1M over previous datasets
and the effectiveness of our MVDiT.",2024-07-02 15:40:29+00:00,"['Kepan Nan', 'Rui Xie', 'Penghao Zhou', 'Tiehan Fan', 'Zhenheng Yang', 'Zhijie Chen', 'Xiang Li', 'Jian Yang', 'Ying Tai']",http://arxiv.org/abs/2407.02371v3
Mind the Time: Temporally-Controlled Multi-Event Video Generation,"Real-world videos consist of sequences of events. Generating such sequences
with precise temporal control is infeasible with existing video generators that
rely on a single paragraph of text as input. When tasked with generating
multiple events described using a single prompt, such methods often ignore some
of the events or fail to arrange them in the correct order. To address this
limitation, we present MinT, a multi-event video generator with temporal
control. Our key insight is to bind each event to a specific period in the
generated video, which allows the model to focus on one event at a time. To
enable time-aware interactions between event captions and video tokens, we
design a time-based positional encoding method, dubbed ReRoPE. This encoding
helps to guide the cross-attention operation. By fine-tuning a pre-trained
video diffusion transformer on temporally grounded data, our approach produces
coherent videos with smoothly connected events. For the first time in the
literature, our model offers control over the timing of events in generated
videos. Extensive experiments demonstrate that MinT outperforms existing
commercial and open-source models by a large margin.",2024-12-06 18:52:20+00:00,"['Ziyi Wu', 'Aliaksandr Siarohin', 'Willi Menapace', 'Ivan Skorokhodov', 'Yuwei Fang', 'Varnith Chordia', 'Igor Gilitschenski', 'Sergey Tulyakov']",http://arxiv.org/abs/2412.05263v2
Can video generation replace cinematographers? Research on the cinematic language of generated video,"Recent advancements in text-to-video (T2V) generation have leveraged
diffusion models to enhance the visual coherence of videos generated from
textual descriptions. However, most research has primarily focused on object
motion, with limited attention given to cinematic language in videos, which is
crucial for cinematographers to convey emotion and narrative pacing. To address
this limitation, we propose a threefold approach to enhance the ability of T2V
models to generate controllable cinematic language. Specifically, we introduce
a cinematic language dataset that encompasses shot framing, angle, and camera
movement, enabling models to learn diverse cinematic styles. Building on this,
to facilitate robust cinematic alignment evaluation, we present CameraCLIP, a
model fine-tuned on the proposed dataset that excels in understanding complex
cinematic language in generated videos and can further provide valuable
guidance in the multi-shot composition process. Finally, we propose CLIPLoRA, a
cost-guided dynamic LoRA composition method that facilitates smooth transitions
and realistic blending of cinematic language by dynamically fusing multiple
pre-trained cinematic LoRAs within a single video. Our experiments demonstrate
that CameraCLIP outperforms existing models in assessing the alignment between
cinematic language and video, achieving an R@1 score of 0.81. Additionally,
CLIPLoRA improves the ability for multi-shot composition, potentially bridging
the gap between automatically generated videos and those shot by professional
cinematographers.",2024-12-16 09:02:24+00:00,"['Xiaozhe Li', 'Kai WU', 'Siyi Yang', 'YiZhan Qu', 'Guohua. Zhang', 'Zhiyu Chen', 'Jiayao Li', 'Jiangchuan Mu', 'Xiaobin Hu', 'Wen Fang', 'Mingliang Xiong', 'Hao Deng', 'Qingwen Liu', 'Gang Li', 'Bin He']",http://arxiv.org/abs/2412.12223v1
StereoCrafter: Diffusion-based Generation of Long and High-fidelity Stereoscopic 3D from Monocular Videos,"This paper presents a novel framework for converting 2D videos to immersive
stereoscopic 3D, addressing the growing demand for 3D content in immersive
experience. Leveraging foundation models as priors, our approach overcomes the
limitations of traditional methods and boosts the performance to ensure the
high-fidelity generation required by the display devices. The proposed system
consists of two main steps: depth-based video splatting for warping and
extracting occlusion mask, and stereo video inpainting. We utilize pre-trained
stable video diffusion as the backbone and introduce a fine-tuning protocol for
the stereo video inpainting task. To handle input video with varying lengths
and resolutions, we explore auto-regressive strategies and tiled processing.
Finally, a sophisticated data processing pipeline has been developed to
reconstruct a large-scale and high-quality dataset to support our training. Our
framework demonstrates significant improvements in 2D-to-3D video conversion,
offering a practical solution for creating immersive content for 3D devices
like Apple Vision Pro and 3D displays. In summary, this work contributes to the
field by presenting an effective method for generating high-quality
stereoscopic videos from monocular input, potentially transforming how we
experience digital media.",2024-09-11 17:52:07+00:00,"['Sijie Zhao', 'Wenbo Hu', 'Xiaodong Cun', 'Yong Zhang', 'Xiaoyu Li', 'Zhe Kong', 'Xiangjun Gao', 'Muyao Niu', 'Ying Shan']",http://arxiv.org/abs/2409.07447v1
STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models,"Image generative models have made significant progress in generating
realistic and diverse images, supported by comprehensive guidance from various
evaluation metrics. However, current video generative models struggle to
generate even short video clips, with limited tools that provide insights for
improvements. Current video evaluation metrics are simple adaptations of image
metrics by switching the embeddings with video embedding networks, which may
underestimate the unique characteristics of video. Our analysis reveals that
the widely used Frechet Video Distance (FVD) has a stronger emphasis on the
spatial aspect than the temporal naturalness of video and is inherently
constrained by the input size of the embedding networks used, limiting it to 16
frames. Additionally, it demonstrates considerable instability and diverges
from human evaluations. To address the limitations, we propose STREAM, a new
video evaluation metric uniquely designed to independently evaluate spatial and
temporal aspects. This feature allows comprehensive analysis and evaluation of
video generative models from various perspectives, unconstrained by video
length. We provide analytical and experimental evidence demonstrating that
STREAM provides an effective evaluation tool for both visual and temporal
quality of videos, offering insights into area of improvement for video
generative models. To the best of our knowledge, STREAM is the first evaluation
metric that can separately assess the temporal and spatial aspects of videos.
Our code is available at https://github.com/pro2nit/STREAM.",2024-01-30 08:18:20+00:00,"['Pum Jun Kim', 'Seojun Kim', 'Jaejun Yoo']",http://arxiv.org/abs/2403.09669v3
DVOS: Self-Supervised Dense-Pattern Video Object Segmentation,"Video object segmentation approaches primarily rely on large-scale
pixel-accurate human-annotated datasets for model development. In Dense Video
Object Segmentation (DVOS) scenarios, each video frame encompasses hundreds of
small, dense, and partially occluded objects. Accordingly, the labor-intensive
manual annotation of even a single frame often takes hours, which hinders the
development of DVOS for many applications. Furthermore, in videos with dense
patterns, following a large number of objects that move in different directions
poses additional challenges. To address these challenges, we proposed a
semi-self-supervised spatiotemporal approach for DVOS utilizing a
diffusion-based method through multi-task learning. Emulating real videos'
optical flow and simulating their motion, we developed a methodology to
synthesize computationally annotated videos that can be used for training DVOS
models; The model performance was further improved by utilizing weakly labeled
(computationally generated but imprecise) data. To demonstrate the utility and
efficacy of the proposed approach, we developed DVOS models for wheat head
segmentation of handheld and drone-captured videos, capturing wheat crops in
fields of different locations across various growth stages, spanning from
heading to maturity. Despite using only a few manually annotated video frames,
the proposed approach yielded high-performing models, achieving a Dice score of
0.82 when tested on a drone-captured external test set. While we showed the
efficacy of the proposed approach for wheat head segmentation, its application
can be extended to other crops or DVOS in other domains, such as crowd analysis
or microscopic image analysis.",2024-06-07 17:58:36+00:00,"['Keyhan Najafian', 'Farhad Maleki', 'Ian Stavness', 'Lingling Jin']",http://arxiv.org/abs/2406.05131v1
FreeMask: Rethinking the Importance of Attention Masks for Zero-Shot Video Editing,"Text-to-video diffusion models have made remarkable advancements. Driven by
their ability to generate temporally coherent videos, research on zero-shot
video editing using these fundamental models has expanded rapidly. To enhance
editing quality, structural controls are frequently employed in video editing.
Among these techniques, cross-attention mask control stands out for its
effectiveness and efficiency. However, when cross-attention masks are naively
applied to video editing, they can introduce artifacts such as blurring and
flickering. Our experiments uncover a critical factor overlooked in previous
video editing research: cross-attention masks are not consistently clear but
vary with model structure and denoising timestep. To address this issue, we
propose the metric Mask Matching Cost (MMC) that quantifies this variability
and propose FreeMask, a method for selecting optimal masks tailored to specific
video editing tasks. Using MMC-selected masks, we further improve the masked
fusion mechanism within comprehensive attention features, e.g., temp, cross,
and self-attention modules. Our approach can be seamlessly integrated into
existing zero-shot video editing frameworks with better performance, requiring
no control assistance or parameter fine-tuning but enabling adaptive decoupling
of unedited semantic layouts with mask precision control. Extensive experiments
demonstrate that FreeMask achieves superior semantic fidelity, temporal
consistency, and editing quality compared to state-of-the-art methods.",2024-09-30 17:01:26+00:00,"['Lingling Cai', 'Kang Zhao', 'Hangjie Yuan', 'Yingya Zhang', 'Shiwei Zhang', 'Kejie Huang']",http://arxiv.org/abs/2409.20500v1
VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping,"Video face swapping is becoming increasingly popular across various
applications, yet existing methods primarily focus on static images and
struggle with video face swapping because of temporal consistency and complex
scenarios. In this paper, we present the first diffusion-based framework
specifically designed for video face swapping. Our approach introduces a novel
image-video hybrid training framework that leverages both abundant static image
data and temporal video sequences, addressing the inherent limitations of
video-only training. The framework incorporates a specially designed diffusion
model coupled with a VidFaceVAE that effectively processes both types of data
to better maintain temporal coherence of the generated videos. To further
disentangle identity and pose features, we construct the Attribute-Identity
Disentanglement Triplet (AIDT) Dataset, where each triplet has three face
images, with two images sharing the same pose and two sharing the same
identity. Enhanced with a comprehensive occlusion augmentation, this dataset
also improves robustness against occlusions. Additionally, we integrate 3D
reconstruction techniques as input conditioning to our network for handling
large pose variations. Extensive experiments demonstrate that our framework
achieves superior performance in identity preservation, temporal consistency,
and visual quality compared to existing methods, while requiring fewer
inference steps. Our approach effectively mitigates key challenges in video
face swapping, including temporal flickering, identity preservation, and
robustness to occlusions and pose variations.",2024-12-15 18:58:32+00:00,"['Hao Shao', 'Shulun Wang', 'Yang Zhou', 'Guanglu Song', 'Dailan He', 'Shuo Qin', 'Zhuofan Zong', 'Bingqi Ma', 'Yu Liu', 'Hongsheng Li']",http://arxiv.org/abs/2412.11279v1
Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting,"Gaussian Splatting (GS) has significantly elevated scene reconstruction
efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance
Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS
methods, whether based on GS or NeRF, primarily rely on camera parameters
provided by COLMAP and even utilize sparse point clouds generated by COLMAP for
initialization, which lack accuracy as well are time-consuming. This sometimes
results in poor dynamic scene representation, especially in scenes with large
object movements, or extreme camera conditions e.g. small translations combined
with large rotations. Some studies simultaneously optimize the estimation of
camera parameters and scenes, supervised by additional information like depth,
optical flow, etc. obtained from off-the-shelf models. Using this unverified
information as ground truth can reduce robustness and accuracy, which does
frequently occur for long monocular videos (with e.g. > hundreds of frames). We
propose a novel approach that learns a high-fidelity 4D GS scene representation
with self-calibration of camera parameters. It includes the extraction of 2D
point features that robustly represent 3D structure, and their use for
subsequent joint optimization of camera parameters and 3D structure towards
overall 4D scene optimization. We demonstrate the accuracy and time efficiency
of our method through extensive quantitative and qualitative experimental
results on several standard benchmarks. The results show significant
improvements over state-of-the-art methods for 4D novel view synthesis. The
source code will be released soon at https://github.com/fangli333/SC-4DGS.",2024-06-03 06:52:35+00:00,"['Fang Li', 'Hao Zhang', 'Narendra Ahuja']",http://arxiv.org/abs/2406.01042v2
EditBoard: Towards a Comprehensive Evaluation Benchmark for Text-Based Video Editing Models,"The rapid development of diffusion models has significantly advanced
AI-generated content (AIGC), particularly in Text-to-Image (T2I) and
Text-to-Video (T2V) generation. Text-based video editing, leveraging these
generative capabilities, has emerged as a promising field, enabling precise
modifications to videos based on text prompts. Despite the proliferation of
innovative video editing models, there is a conspicuous lack of comprehensive
evaluation benchmarks that holistically assess these models' performance across
various dimensions. Existing evaluations are limited and inconsistent,
typically summarizing overall performance with a single score, which obscures
models' effectiveness on individual editing tasks. To address this gap, we
propose EditBoard, the first comprehensive evaluation benchmark for text-based
video editing models. EditBoard encompasses nine automatic metrics across four
dimensions, evaluating models on four task categories and introducing three new
metrics to assess fidelity. This task-oriented benchmark facilitates objective
evaluation by detailing model performance and providing insights into each
model's strengths and weaknesses. By open-sourcing EditBoard, we aim to
standardize evaluation and advance the development of robust video editing
models.",2024-09-15 08:43:18+00:00,"['Yupeng Chen', 'Penglin Chen', 'Xiaoyu Zhang', 'Yixian Huang', 'Qian Xie']",http://arxiv.org/abs/2409.09668v2
Data Collection-free Masked Video Modeling,"Pre-training video transformers generally requires a large amount of data,
presenting significant challenges in terms of data collection costs and
concerns related to privacy, licensing, and inherent biases. Synthesizing data
is one of the promising ways to solve these issues, yet pre-training solely on
synthetic data has its own challenges. In this paper, we introduce an effective
self-supervised learning framework for videos that leverages readily available
and less costly static images. Specifically, we define the Pseudo Motion
Generator (PMG) module that recursively applies image transformations to
generate pseudo-motion videos from images. These pseudo-motion videos are then
leveraged in masked video modeling. Our approach is applicable to synthetic
images as well, thus entirely freeing video pre-training from data collection
costs and other concerns in real data. Through experiments in action
recognition tasks, we demonstrate that this framework allows effective learning
of spatio-temporal features through pseudo-motion videos, significantly
improving over existing methods which also use static images and partially
outperforming those using both real and synthetic videos. These results uncover
fragments of what video transformers learn through masked video modeling.",2024-09-10 17:34:07+00:00,"['Yuchi Ishikawa', 'Masayoshi Kondo', 'Yoshimitsu Aoki']",http://arxiv.org/abs/2409.06665v1
Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing,"With the widespread popularity of internet celebrity marketing all over the
world, short video production has gradually become a popular way of presenting
products information. However, the traditional video production industry
usually includes series of procedures as script writing, video filming in a
professional studio, video clipping, special effects rendering, customized
post-processing, and so forth. Not to mention that multilingual videos is not
accessible for those who could not speak multilingual languages. These
complicated procedures usually needs a professional team to complete, and this
made short video production costly in both time and money. This paper presents
an intelligent system that supports the automatic generation of talking avatar
videos, namely Virbo. With simply a user-specified script, Virbo could use a
deep generative model to generate a target talking videos. Meanwhile, the
system also supports multimodal inputs to customize the video with specified
face, specified voice and special effects. This system also integrated a
multilingual customization module that supports generate multilingual talking
avatar videos in a batch with hundreds of delicate templates and creative
special effects. Through a series of user studies and demo tests, we found that
Virbo can generate talking avatar videos that maintained a high quality of
videos as those from a professional team while reducing the entire production
costs significantly. This intelligent system will effectively promote the video
production industry and facilitate the internet marketing neglecting of
language barriers and cost challenges.",2024-03-18 11:56:35+00:00,"['Juan Zhang', 'Jiahao Chen', 'Cheng Wang', 'Zhiwang Yu', 'Tangquan Qi', 'Can Liu', 'Di Wu']",http://arxiv.org/abs/2403.11700v2
Cap2Sum: Learning to Summarize Videos by Generating Captions,"With the rapid growth of video data on the internet, video summarization is
becoming a very important AI technology. However, due to the high labelling
cost of video summarization, existing studies have to be conducted on
small-scale datasets, leading to limited performance and generalization
capacity. In this work, we introduce the use of dense video captions as a
supervision signal to train video summarization models. Motivated by this, we
propose Cap2Sum, a model that learns to summarize videos by generating
captions, to exploit dense video caption annotations. This weakly-supervised
approach allows us to train the models on large-scale dense video caption
datasets to achieve better performance and generalization capacity. To further
improve the generalization capacity, we introduce a CLIP (a strong
vision-language model) Prior mechanism to enhance the learning of important
objects that captions may ignore in the videos. In practice, Cap2Sum can
perform zero-shot video summarization or be fine-tuned by the ground-truth
summary or video caption of the target dataset. To examine the performance of
Cap2Sum after weakly-supervised fine-tuning by the video captions, we propose
two new datasets, TVSum-Caption and SumMe-Caption, which are derived from two
common video summarization datasets and will be publicly released. We conduct
extensive experiments and the results demonstrate that our method achieves
significant improvements in performance and generalization capacity compared
with previous methods.",2024-08-23 02:28:54+00:00,"['Cairong Zhao', 'Chutian Wang', 'Zifan Song', 'Guosheng Hu', 'Haonan Chen', 'Xiaofan Zhai']",http://arxiv.org/abs/2408.12800v1
Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training,"Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. However, it remains a challenge due to the domain gap between
humans and robots. Moreover, it is difficult to extract useful information
representing the dynamic world from human videos, because of its noisy and
multimodal data structure. In this paper, we introduce a novel framework to
tackle these challenges, which leverages a unified discrete diffusion to
combine generative pre-training on human videos and policy fine-tuning on a
small number of action-labeled robot videos. We start by compressing both human
and robot videos into unified video tokens. In the pre-training stage, we
employ a discrete diffusion model with a mask-and-replace diffusion strategy to
predict future video tokens in the latent space. In the fine-tuning stage, we
harness the imagined future videos to guide low-level action learning with a
limited set of robot data. Experiments demonstrate that our method generates
high-fidelity future videos for planning and enhances the fine-tuned policies
compared to previous state-of-the-art approaches with superior performance. Our
project website is available at https://video-diff.github.io/.",2024-02-22 09:48:47+00:00,"['Haoran He', 'Chenjia Bai', 'Ling Pan', 'Weinan Zhang', 'Bin Zhao', 'Xuelong Li']",http://arxiv.org/abs/2402.14407v4
Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions,"Most existing video diffusion models (VDMs) are limited to mere text
conditions. Thereby, they are usually lacking in control over visual appearance
and geometry structure of the generated videos. This work presents Moonshot, a
new video generation model that conditions simultaneously on multimodal inputs
of image and text. The model builts upon a core module, called multimodal video
block (MVB), which consists of conventional spatialtemporal layers for
representing video features, and a decoupled cross-attention layer to address
image and text inputs for appearance conditioning. In addition, we carefully
design the model architecture such that it can optionally integrate with
pre-trained image ControlNet modules for geometry visual conditions, without
needing of extra training overhead as opposed to prior methods. Experiments
show that with versatile multimodal conditioning mechanisms, Moonshot
demonstrates significant improvement on visual quality and temporal consistency
compared to existing models. In addition, the model can be easily repurposed
for a variety of generative applications, such as personalized video
generation, image animation and video editing, unveiling its potential to serve
as a fundamental architecture for controllable video generation. Models will be
made public on https://github.com/salesforce/LAVIS.",2024-01-03 16:43:47+00:00,"['David Junhao Zhang', 'Dongxu Li', 'Hung Le', 'Mike Zheng Shou', 'Caiming Xiong', 'Doyen Sahoo']",http://arxiv.org/abs/2401.01827v1
TVG: A Training-free Transition Video Generation Method with Diffusion Models,"Transition videos play a crucial role in media production, enhancing the flow
and coherence of visual narratives. Traditional methods like morphing often
lack artistic appeal and require specialized skills, limiting their
effectiveness. Recent advances in diffusion model-based video generation offer
new possibilities for creating transitions but face challenges such as poor
inter-frame relationship modeling and abrupt content changes. We propose a
novel training-free Transition Video Generation (TVG) approach using
video-level diffusion models that addresses these limitations without
additional training. Our method leverages Gaussian Process Regression
($\mathcal{GPR}$) to model latent representations, ensuring smooth and dynamic
transitions between frames. Additionally, we introduce interpolation-based
conditional controls and a Frequency-aware Bidirectional Fusion (FBiF)
architecture to enhance temporal control and transition reliability.
Evaluations of benchmark datasets and custom image pairs demonstrate the
effectiveness of our approach in generating high-quality smooth transition
videos. The code are provided in https://sobeymil.github.io/tvg.com.",2024-08-24 00:33:14+00:00,"['Rui Zhang', 'Yaosen Chen', 'Yuegen Liu', 'Wei Wang', 'Xuming Wen', 'Hongxia Wang']",http://arxiv.org/abs/2408.13413v1
DreamPhysics: Learning Physics-Based 3D Dynamics with Video Diffusion Priors,"Dynamic 3D interaction has been attracting a lot of attention recently.
However, creating such 4D content remains challenging. One solution is to
animate 3D scenes with physics-based simulation, which requires manually
assigning precise physical properties to the object or the simulated results
would become unnatural. Another solution is to learn the deformation of 3D
objects with the distillation of video generative models, which, however, tends
to produce 3D videos with small and discontinuous motions due to the
inappropriate extraction and application of physics priors. In this work, to
combine the strengths and complementing shortcomings of the above two
solutions, we propose to learn the physical properties of a material field with
video diffusion priors, and then utilize a physics-based Material-Point-Method
(MPM) simulator to generate 4D content with realistic motions. In particular,
we propose motion distillation sampling to emphasize video motion information
during distillation. In addition, to facilitate the optimization, we further
propose a KAN-based material field with frame boosting. Experimental results
demonstrate that our method enjoys more realistic motions than
state-of-the-arts do.",2024-06-03 16:05:25+00:00,"['Tianyu Huang', 'Haoze Zhang', 'Yihan Zeng', 'Zhilu Zhang', 'Hui Li', 'Wangmeng Zuo', 'Rynson W. H. Lau']",http://arxiv.org/abs/2406.01476v3
Neural Graph Matching for Video Retrieval in Large-Scale Video-driven E-commerce,"With the rapid development of the short video industry, traditional
e-commerce has encountered a new paradigm, video-driven e-commerce, which
leverages attractive videos for product showcases and provides both video and
item services for users. Benefitting from the dynamic and visualized
introduction of items,video-driven e-commerce has shown huge potential in
stimulating consumer confidence and promoting sales. In this paper, we focus on
the video retrieval task, facing the following challenges: (1) Howto handle the
heterogeneities among users, items, and videos? (2)How to mine the
complementarity between items and videos for better user understanding? In this
paper, we first leverage the dual graph to model the co-existing of user-video
and user-item interactions in video-driven e-commerce and innovatively reduce
user preference understanding to a graph matching problem. To solve it, we
further propose a novel bi-level Graph Matching Network(GMN), which mainly
consists of node- and preference-level graph matching. Given a user, node-level
graph matching aims to match videos and items, while preference-level graph
matching aims to match multiple user preferences extracted from both videos and
items. Then the proposed GMN can generate and improve user embedding by
aggregating matched nodes or preferences from the dual graph in a bi-level
manner. Comprehensive experiments show the superiority of the proposed GMN with
significant improvements over state-of-the-art approaches (e.g., AUC+1.9% and
CTR+7.15%). We have developed it on a well-known video-driven e-commerce
platform, serving hundreds of millions of users every day",2024-08-01 07:31:23+00:00,"['Houye Ji', 'Ye Tang', 'Zhaoxin Chen', 'Lixi Deng', 'Jun Hu', 'Lei Su']",http://arxiv.org/abs/2408.00346v1
YouTube Video Analytics for Patient Engagement: Evidence from Colonoscopy Preparation Videos,"Videos can be an effective way to deliver contextualized, just-in-time
medical information for patient education. However, video analysis, from topic
identification and retrieval to extraction and analysis of medical information
and understandability from a patient perspective are extremely challenging
tasks. This study demonstrates a data analysis pipeline that utilizes methods
to retrieve medical information from YouTube videos on preparing for a
colonoscopy exam, a much maligned and disliked procedure that patients find
challenging to get adequately prepared for. We first use the YouTube Data API
to collect metadata of desired videos on select search keywords and use Google
Video Intelligence API to analyze texts, frames and objects data. Then we
annotate the YouTube video materials on medical information, video
understandability and overall recommendation. We develop a bidirectional long
short-term memory (BiLSTM) model to identify medical terms in videos and build
three classifiers to group videos based on the levels of encoded medical
information and video understandability, and whether the videos are recommended
or not. Our study provides healthcare stakeholders with guidelines and a
scalable approach for generating new educational video content to enhance
management of a vast number of health conditions.",2024-10-01 19:38:46+00:00,"['Yawen Guo', 'Xiao Liu', 'Anjana Susarla', 'Padman Rema']",http://arxiv.org/abs/2410.02830v1
Agent-based Video Trimming,"As information becomes more accessible, user-generated videos are increasing
in length, placing a burden on viewers to sift through vast content for
valuable insights. This trend underscores the need for an algorithm to extract
key video information efficiently. Despite significant advancements in
highlight detection, moment retrieval, and video summarization, current
approaches primarily focus on selecting specific time intervals, often
overlooking the relevance between segments and the potential for segment
arranging. In this paper, we introduce a novel task called Video Trimming (VT),
which focuses on detecting wasted footage, selecting valuable segments, and
composing them into a final video with a coherent story. To address this task,
we propose Agent-based Video Trimming (AVT), structured into three phases:
Video Structuring, Clip Filtering, and Story Composition. Specifically, we
employ a Video Captioning Agent to convert video slices into structured textual
descriptions, a Filtering Module to dynamically discard low-quality footage
based on the structured information of each clip, and a Video Arrangement Agent
to select and compile valid clips into a coherent final narrative. For
evaluation, we develop a Video Evaluation Agent to assess trimmed videos,
conducting assessments in parallel with human evaluations. Additionally, we
curate a new benchmark dataset for video trimming using raw user videos from
the internet. As a result, AVT received more favorable evaluations in user
studies and demonstrated superior mAP and precision on the YouTube Highlights,
TVSum, and our own dataset for the highlight detection task. The code and
models are available at https://ylingfeng.github.io/AVT.",2024-12-12 17:59:28+00:00,"['Lingfeng Yang', 'Zhenyuan Chen', 'Xiang Li', 'Peiyang Jia', 'Liangqu Long', 'Jian Yang']",http://arxiv.org/abs/2412.09513v1
Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields,"The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.",2024-03-26 15:45:29+00:00,"['Rveyda Yilmaz', 'Dennis Eschweiler', 'Johannes Stegmaier']",http://arxiv.org/abs/2403.17808v1
Improved Video VAE for Latent Video Diffusion Model,"Variational Autoencoder (VAE) aims to compress pixel data into
low-dimensional latent space, playing an important role in OpenAI's Sora and
other latent video diffusion generation models. While most of existing video
VAEs inflate a pretrained image VAE into the 3D causal structure for
temporal-spatial compression, this paper presents two astonishing findings: (1)
The initialization from a well-trained image VAE with the same latent
dimensions suppresses the improvement of subsequent temporal compression
capabilities. (2) The adoption of causal reasoning leads to unequal information
interactions and unbalanced performance between frames. To alleviate these
problems, we propose a keyframe-based temporal compression (KTC) architecture
and a group causal convolution (GCConv) module to further improve video VAE
(IV-VAE). Specifically, the KTC architecture divides the latent space into two
branches, in which one half completely inherits the compression prior of
keyframes from a lower-dimension image VAE while the other half involves
temporal compression through 3D group causal convolution, reducing
temporal-spatial conflicts and accelerating the convergence speed of video VAE.
The GCConv in above 3D half uses standard convolution within each frame group
to ensure inter-frame equivalence, and employs causal logical padding between
groups to maintain flexibility in processing variable frame video. Extensive
experiments on five benchmarks demonstrate the SOTA video reconstruction and
generation capabilities of the proposed IV-VAE
(https://wpy1999.github.io/IV-VAE/).",2024-11-10 12:43:38+00:00,"['Pingyu Wu', 'Kai Zhu', 'Yu Liu', 'Liming Zhao', 'Wei Zhai', 'Yang Cao', 'Zheng-Jun Zha']",http://arxiv.org/abs/2411.06449v1
S2DM: Sector-Shaped Diffusion Models for Video Generation,"Diffusion models have achieved great success in image generation. However,
when leveraging this idea for video generation, we face significant challenges
in maintaining the consistency and continuity across video frames. This is
mainly caused by the lack of an effective framework to align frames of videos
with desired temporal features while preserving consistent semantic and
stochastic features. In this work, we propose a novel Sector-Shaped Diffusion
Model (S2DM) whose sector-shaped diffusion region is formed by a set of
ray-shaped reverse diffusion processes starting at the same noise point. S2DM
can generate a group of intrinsically related data sharing the same semantic
and stochastic features while varying on temporal features with appropriate
guided conditions. We apply S2DM to video generation tasks, and explore the use
of optical flow as temporal conditions. Our experimental results show that S2DM
outperforms many existing methods in the task of video generation without any
temporal-feature modelling modules. For text-to-video generation tasks where
temporal conditions are not explicitly given, we propose a two-stage generation
strategy which can decouple the generation of temporal features from
semantic-content features. We show that, without additional training, our model
integrated with another temporal conditions generative model can still achieve
comparable performance with existing works. Our results can be viewd at
https://s2dm.github.io/S2DM/.",2024-03-20 08:50:15+00:00,"['Haoran Lang', 'Yuxuan Ge', 'Zheng Tian']",http://arxiv.org/abs/2403.13408v2
3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation,"This paper aims to manipulate multi-entity 3D motions in video generation.
Previous methods on controllable video generation primarily leverage 2D control
signals to manipulate object motions and have achieved remarkable synthesis
results. However, 2D control signals are inherently limited in expressing the
3D nature of object motions. To overcome this problem, we introduce
3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D
space, given user-desired 6DoF pose (location and rotation) sequences of
entities. At the core of our approach is a plug-and-play 3D-motion grounded
object injector that fuses multiple input entities with their respective 3D
trajectories through a gated self-attention mechanism. In addition, we exploit
an injector architecture to preserve the video diffusion prior, which is
crucial for generalization ability. To mitigate video quality degradation, we
introduce a domain adaptor during training and employ an annealed sampling
strategy during inference. To address the lack of suitable training data, we
construct a 360-Motion Dataset, which first correlates collected 3D human and
animal assets with GPT-generated trajectory and then captures their motion with
12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments
show that 3DTrajMaster sets a new state-of-the-art in both accuracy and
generalization for controlling multi-entity 3D motions. Project page:
http://fuxiao0719.github.io/projects/3dtrajmaster",2024-12-10 18:55:13+00:00,"['Xiao Fu', 'Xian Liu', 'Xintao Wang', 'Sida Peng', 'Menghan Xia', 'Xiaoyu Shi', 'Ziyang Yuan', 'Pengfei Wan', 'Di Zhang', 'Dahua Lin']",http://arxiv.org/abs/2412.07759v2
ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models,"With the advance of diffusion models, today's video generation has achieved
impressive quality. But generating temporal consistent long videos is still
challenging. A majority of video diffusion models (VDMs) generate long videos
in an autoregressive manner, i.e., generating subsequent clips conditioned on
last frames of previous clip. However, existing approaches all involve
bidirectional computations, which restricts the receptive context of each
autoregression step, and results in the model lacking long-term dependencies.
Inspired from the huge success of large language models (LLMs) and following
GPT (generative pre-trained transformer), we bring causal (i.e.,
unidirectional) generation into VDMs, and use past frames as prompt to generate
future frames. For Causal Generation, we introduce causal temporal attention
into VDM, which forces each generated frame to depend on its previous frames.
For Frame as Prompt, we inject the conditional frames by concatenating them
with noisy frames (frames to be generated) along the temporal axis.
Consequently, we present Video Diffusion GPT (ViD-GPT). Based on the two key
designs, in each autoregression step, it is able to acquire long-term context
from prompting frames concatenated by all previously generated frames.
Additionally, we bring the kv-cache mechanism to VDMs, which eliminates the
redundant computation from overlapped frames, significantly boosting the
inference speed. Extensive experiments demonstrate that our ViD-GPT achieves
state-of-the-art performance both quantitatively and qualitatively on long
video generation. Code will be available at
https://github.com/Dawn-LX/Causal-VideoGen.",2024-06-16 15:37:22+00:00,"['Kaifeng Gao', 'Jiaxin Shi', 'Hanwang Zhang', 'Chunping Wang', 'Jun Xiao']",http://arxiv.org/abs/2406.10981v1
PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation,"We present PhysGen, a novel image-to-video generation method that converts a
single image and an input condition (e.g., force and torque applied to an
object in the image) to produce a realistic, physically plausible, and
temporally consistent video. Our key insight is to integrate model-based
physical simulation with a data-driven video generation process, enabling
plausible image-space dynamics. At the heart of our system are three core
components: (i) an image understanding module that effectively captures the
geometry, materials, and physical parameters of the image; (ii) an image-space
dynamics simulation model that utilizes rigid-body physics and inferred
parameters to simulate realistic behaviors; and (iii) an image-based rendering
and refinement module that leverages generative video diffusion to produce
realistic video footage featuring the simulated motion. The resulting videos
are realistic in both physics and appearance and are even precisely
controllable, showcasing superior results over existing data-driven
image-to-video generation works through quantitative comparison and
comprehensive user study. PhysGen's resulting videos can be used for various
downstream applications, such as turning an image into a realistic animation or
allowing users to interact with the image and create various dynamics. Project
page: https://stevenlsw.github.io/physgen/",2024-09-27 17:59:57+00:00,"['Shaowei Liu', 'Zhongzheng Ren', 'Saurabh Gupta', 'Shenlong Wang']",http://arxiv.org/abs/2409.18964v1
REDUCIO! Generating 1024$\times$1024 Video within 16 Seconds using Extremely Compressed Motion Latents,"Commercial video generation models have exhibited realistic, high-fidelity
results but are still restricted to limited access. One crucial obstacle for
large-scale applications is the expensive training and inference cost. In this
paper, we argue that videos contain much more redundant information than
images, thus can be encoded by very few motion latents based on a content
image. Towards this goal, we design an image-conditioned VAE to encode a video
to an extremely compressed motion latent space. This magic Reducio charm
enables 64x reduction of latents compared to a common 2D VAE, without
sacrificing the quality. Training diffusion models on such a compact
representation easily allows for generating 1K resolution videos. We then adopt
a two-stage video generation paradigm, which performs text-to-image and
text-image-to-video sequentially. Extensive experiments show that our
Reducio-DiT achieves strong performance in evaluation, though trained with
limited GPU resources. More importantly, our method significantly boost the
efficiency of video LDMs both in training and inference. We train Reducio-DiT
in around 3.2K training hours in total and generate a 16-frame 1024*1024 video
clip within 15.5 seconds on a single A100 GPU. Code released at
https://github.com/microsoft/Reducio-VAE .",2024-11-20 18:59:52+00:00,"['Rui Tian', 'Qi Dai', 'Jianmin Bao', 'Kai Qiu', 'Yifan Yang', 'Chong Luo', 'Zuxuan Wu', 'Yu-Gang Jiang']",http://arxiv.org/abs/2411.13552v2
YTCommentQA: Video Question Answerability in Instructional Videos,"Instructional videos provide detailed how-to guides for various tasks, with
viewers often posing questions regarding the content. Addressing these
questions is vital for comprehending the content, yet receiving immediate
answers is difficult. While numerous computational models have been developed
for Video Question Answering (Video QA) tasks, they are primarily trained on
questions generated based on video content, aiming to produce answers from
within the content. However, in real-world situations, users may pose questions
that go beyond the video's informational boundaries, highlighting the necessity
to determine if a video can provide the answer. Discerning whether a question
can be answered by video content is challenging due to the multi-modal nature
of videos, where visual and verbal information are intertwined. To bridge this
gap, we present the YTCommentQA dataset, which contains naturally-generated
questions from YouTube, categorized by their answerability and required
modality to answer -- visual, script, or both. Experiments with answerability
classification tasks demonstrate the complexity of YTCommentQA and emphasize
the need to comprehend the combined role of visual and script information in
video reasoning. The dataset is available at
https://github.com/lgresearch/YTCommentQA.",2024-01-30 14:18:37+00:00,"['Saelyne Yang', 'Sunghyun Park', 'Yunseok Jang', 'Moontae Lee']",http://arxiv.org/abs/2401.17343v1
Learning Video Representations without Natural Videos,"We show that useful video representations can be learned from synthetic
videos and natural images, without incorporating natural videos in the
training. We propose a progression of video datasets synthesized by simple
generative processes, that model a growing set of natural video properties
(e.g., motion, acceleration, and shape transformations). The downstream
performance of video models pre-trained on these generated datasets gradually
increases with the dataset progression. A VideoMAE model pre-trained on our
synthetic videos closes 97.2\% of the performance gap on UCF101 action
classification between training from scratch and self-supervised pre-training
from natural videos, and outperforms the pre-trained model on HMDB51.
Introducing crops of static images to the pre-training stage results in similar
performance to UCF101 pre-training and outperforms the UCF101 pre-trained model
on 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the
low-level properties of the datasets, we identify correlations between frame
diversity, frame similarity to natural data, and downstream performance. Our
approach provides a more controllable and transparent alternative to video data
curation processes for pre-training.",2024-10-31 17:59:30+00:00,"['Xueyang Yu', 'Xinlei Chen', 'Yossi Gandelsman']",http://arxiv.org/abs/2410.24213v2
SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge,"Learning commonsense reasoning from visual contexts and scenes in real-world
is a crucial step toward advanced artificial intelligence. However, existing
video reasoning benchmarks are still inadequate since they were mainly designed
for factual or situated reasoning and rarely involve broader knowledge in the
real world. Our work aims to delve deeper into reasoning evaluations,
specifically within dynamic, open-world, and structured context knowledge. We
propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K
situations with instance-level annotations depicted in the videos. The
reasoning process is required to understand and apply situated knowledge and
general knowledge for problem-solving. To create such a dataset, we propose an
automatic and scalable generation method to generate question-answer pairs,
knowledge graphs, and rationales by instructing the combinations of LLMs and
MLLMs. Concretely, we first extract observable situated entities, relations,
and processes from videos for situated knowledge and then extend to open-world
knowledge beyond the visible content. The task generation is facilitated
through multiple dialogues as iterations and subsequently corrected and refined
by our designed self-promptings and demonstrations. With a corpus of both
explicit situated facts and implicit commonsense, we generate associated
question-answer pairs and reasoning processes, finally followed by manual
reviews for quality assurance. We evaluated recent mainstream large
vision-language models on the benchmark and found several insightful
conclusions. For more information, please refer to our benchmark at
www.bobbywu.com/SOKBench.",2024-05-15 21:55:31+00:00,"['Andong Wang', 'Bo Wu', 'Sunli Chen', 'Zhenfang Chen', 'Haotian Guan', 'Wei-Ning Lee', 'Li Erran Li', 'Chuang Gan']",http://arxiv.org/abs/2405.09713v2
Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance,"Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal
vascular dynamics and aiding in the diagnosis of eye diseases. However, its
invasive nature and less accessibility compared to Color Fundus (CF) images
pose significant challenges. Current CF to FFA translation methods are limited
to static generation. In this work, we pioneer dynamic FFA video generation
from static CF images. We introduce an autoregressive GAN for smooth,
memory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic
lesion changes in FFA regions, we design a knowledge mask based on clinical
experience. Leveraging this mask, our approach integrates innovative knowledge
mask-guided techniques, including knowledge-boosted attention, knowledge-aware
discriminators, and mask-enhanced patchNCE loss, aimed at refining generation
in critical areas and addressing the pixel misalignment challenge. Our method
achieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common
video generation approaches. Human assessment by an ophthalmologist confirms
its high generation quality. Notably, our knowledge mask surpasses supervised
lesion segmentation masks, offering a promising non-invasive alternative to
traditional FFA for research and clinical applications. The code is available
at https://github.com/Michi-3000/Fundus2Video.",2024-08-27 17:30:49+00:00,"['Weiyi Zhang', 'Siyu Huang', 'Jiancheng Yang', 'Ruoyu Chen', 'Zongyuan Ge', 'Yingfeng Zheng', 'Danli Shi', 'Mingguang He']",http://arxiv.org/abs/2408.15217v1
Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation,"We present a method for generating video sequences with coherent motion
between a pair of input key frames. We adapt a pretrained large-scale
image-to-video diffusion model (originally trained to generate videos moving
forward in time from a single input image) for key frame interpolation, i.e.,
to produce a video in between two input frames. We accomplish this adaptation
through a lightweight fine-tuning technique that produces a version of the
model that instead predicts videos moving backwards in time from a single input
image. This model (along with the original forward-moving model) is
subsequently used in a dual-directional diffusion sampling process that
combines the overlapping model estimates starting from each of the two
keyframes. Our experiments show that our method outperforms both existing
diffusion-based methods and traditional frame interpolation techniques.",2024-08-27 17:57:14+00:00,"['Xiaojuan Wang', 'Boyang Zhou', 'Brian Curless', 'Ira Kemelmacher-Shlizerman', 'Aleksander Holynski', 'Steven M. Seitz']",http://arxiv.org/abs/2408.15239v2
Medical Imaging Complexity and its Effects on GAN Performance,"The proliferation of machine learning models in diverse clinical applications
has led to a growing need for high-fidelity, medical image training data. Such
data is often scarce due to cost constraints and privacy concerns. Alleviating
this burden, medical image synthesis via generative adversarial networks (GANs)
emerged as a powerful method for synthetically generating photo-realistic
images based on existing sets of real medical images. However, the exact image
set size required to efficiently train such a GAN is unclear. In this work, we
experimentally establish benchmarks that measure the relationship between a
sample dataset size and the fidelity of the generated images, given the
dataset's distribution of image complexities. We analyze statistical metrics
based on delentropy, an image complexity measure rooted in Shannon's entropy in
information theory. For our pipeline, we conduct experiments with two
state-of-the-art GANs, StyleGAN 3 and SPADE-GAN, trained on multiple medical
imaging datasets with variable sample sizes. Across both GANs, general
performance improved with increasing training set size but suffered with
increasing complexity.",2024-10-23 15:28:25+00:00,"['William Cagas', 'Chan Ko', 'Blake Hsiao', 'Shryuk Grandhi', 'Rishi Bhattacharya', 'Kevin Zhu', 'Michael Lam']",http://arxiv.org/abs/2410.17959v1
Deep Generative Models for 3D Medical Image Synthesis,"Deep generative modeling has emerged as a powerful tool for synthesizing
realistic medical images, driving advances in medical image analysis, disease
diagnosis, and treatment planning. This chapter explores various deep
generative models for 3D medical image synthesis, with a focus on Variational
Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Denoising
Diffusion Models (DDMs). We discuss the fundamental principles, recent
advances, as well as strengths and weaknesses of these models and examine their
applications in clinically relevant problems, including unconditional and
conditional generation tasks like image-to-image translation and image
reconstruction. We additionally review commonly used evaluation metrics for
assessing image fidelity, diversity, utility, and privacy and provide an
overview of current challenges in the field.",2024-10-23 08:33:23+00:00,"['Paul Friedrich', 'Yannik Frisch', 'Philippe C. Cattin']",http://arxiv.org/abs/2410.17664v1
CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer,"We present CogVideoX, a large-scale text-to-video generation model based on
diffusion transformer, which can generate 10-second continuous videos aligned
with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360
pixels. Previous video generation models often had limited movement and short
durations, and is difficult to generate videos with coherent narratives based
on text. We propose several designs to address these issues. First, we propose
a 3D Variational Autoencoder (VAE) to compress videos along both spatial and
temporal dimensions, to improve both compression rate and video fidelity.
Second, to improve the text-video alignment, we propose an expert transformer
with the expert adaptive LayerNorm to facilitate the deep fusion between the
two modalities. Third, by employing a progressive training and multi-resolution
frame pack technique, CogVideoX is adept at producing coherent, long-duration,
different shape videos characterized by significant motions. In addition, we
develop an effective text-video data processing pipeline that includes various
data preprocessing strategies and a video captioning method, greatly
contributing to the generation quality and semantic alignment. Results show
that CogVideoX demonstrates state-of-the-art performance across both multiple
machine metrics and human evaluations. The model weight of both 3D Causal VAE,
Video caption model and CogVideoX are publicly available at
https://github.com/THUDM/CogVideo.",2024-08-12 11:47:11+00:00,"['Zhuoyi Yang', 'Jiayan Teng', 'Wendi Zheng', 'Ming Ding', 'Shiyu Huang', 'Jiazheng Xu', 'Yuanming Yang', 'Wenyi Hong', 'Xiaohan Zhang', 'Guanyu Feng', 'Da Yin', 'Yuxuan Zhang', 'Weihan Wang', 'Yean Cheng', 'Bin Xu', 'Xiaotao Gu', 'Yuxiao Dong', 'Jie Tang']",http://arxiv.org/abs/2408.06072v3
Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks,"NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or
GIRAFFE have shown very high rendering quality under large representational
variety. However, rendering with Neural Radiance Fields poses challenges for 3D
applications: First, the significant computational demands of NeRF rendering
preclude its use on low-power devices, such as mobiles and VR/AR headsets.
Second, implicit representations based on neural networks are difficult to
incorporate into explicit 3D scenes, such as VR environments or video games. 3D
Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit
3D representation that can be rendered efficiently at high frame rates. In this
work, we present a novel approach that combines the high rendering quality of
NeRF-based 3D-aware GANs with the flexibility and computational advantages of
3DGS. By training a decoder that maps implicit NeRF representations to explicit
3D Gaussian Splatting attributes, we can integrate the representational
diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting
for the first time. Additionally, our approach allows for a high resolution GAN
inversion and real-time GAN editing with 3D Gaussian Splatting scenes. Project
page: florian-barthel.github.io/gaussian_decoder",2024-04-16 14:48:40+00:00,"['Florian Barthel', 'Arian Beckmann', 'Wieland Morgenstern', 'Anna Hilsmann', 'Peter Eisert']",http://arxiv.org/abs/2404.10625v2
Distilling Vision-Language Models on Millions of Videos,"The recent advance in vision-language models is largely attributed to the
abundance of image-text data. We aim to replicate this success for
video-language models, but there simply is not enough human-curated video-text
data available. We thus resort to fine-tuning a video-language model from a
strong image-language baseline with synthesized instructional data. The
resulting video model by video-instruction-tuning (VIIT) is then used to
auto-label millions of videos to generate high-quality captions. We show the
adapted video-language model performs well on a wide range of video-language
benchmarks. For instance, it surpasses the best prior result on open-ended
NExT-QA by 2.8%. Besides, our model generates detailed descriptions for
previously unseen videos, which provide better textual supervision than
existing methods. Experiments show that a video-language dual-encoder model
contrastively trained on these auto-generated captions is 3.8% better than the
strongest baseline that also leverages vision-language models. Our best model
outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video
retrieval by 6%. As a side product, we generate the largest video caption
dataset to date.",2024-01-11 18:59:53+00:00,"['Yue Zhao', 'Long Zhao', 'Xingyi Zhou', 'Jialin Wu', 'Chun-Te Chu', 'Hui Miao', 'Florian Schroff', 'Hartwig Adam', 'Ting Liu', 'Boqing Gong', 'Philipp Krhenbhl', 'Liangzhe Yuan']",http://arxiv.org/abs/2401.06129v2
SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale Spectral Diffusion Model,"Recent advancements in generative models have significantly enhanced talking
face video generation, yet singing video generation remains underexplored. The
differences between human talking and singing limit the performance of existing
talking face video generation models when applied to singing. The fundamental
differences between talking and singing-specifically in audio characteristics
and behavioral expressions-limit the effectiveness of existing models. We
observe that the differences between singing and talking audios manifest in
terms of frequency and amplitude. To address this, we have designed a
multi-scale spectral module to help the model learn singing patterns in the
spectral domain. Additionally, we develop a spectral-filtering module that aids
the model in learning the human behaviors associated with singing audio. These
two modules are integrated into the diffusion model to enhance singing video
generation performance, resulting in our proposed model, SINGER. Furthermore,
the lack of high-quality real-world singing face videos has hindered the
development of the singing video generation community. To address this gap, we
have collected an in-the-wild audio-visual singing dataset to facilitate
research in this area. Our experiments demonstrate that SINGER is capable of
generating vivid singing videos and outperforms state-of-the-art methods in
both objective and subjective evaluations.",2024-12-04 16:19:47+00:00,"['Yan Li', 'Ziya Zhou', 'Zhiqiang Wang', 'Wei Xue', 'Wenhan Luo', 'Yike Guo']",http://arxiv.org/abs/2412.03430v1
SonicVisionLM: Playing Sound with Vision Language Models,"There has been a growing interest in the task of generating sound for silent
videos, primarily because of its practicality in streamlining video
post-production. However, existing methods for video-sound generation attempt
to directly create sound from visual representations, which can be challenging
due to the difficulty of aligning visual representations with audio
representations. In this paper, we present SonicVisionLM, a novel framework
aimed at generating a wide range of sound effects by leveraging vision-language
models(VLMs). Instead of generating audio directly from video, we use the
capabilities of powerful VLMs. When provided with a silent video, our approach
first identifies events within the video using a VLM to suggest possible sounds
that match the video content. This shift in approach transforms the challenging
task of aligning image and audio into more well-studied sub-problems of
aligning image-to-text and text-to-audio through the popular diffusion models.
To improve the quality of audio recommendations with LLMs, we have collected an
extensive dataset that maps text descriptions to specific sound effects and
developed a time-controlled audio adapter. Our approach surpasses current
state-of-the-art methods for converting video to audio, enhancing
synchronization with the visuals, and improving alignment between audio and
video components. Project page:
https://yusiissy.github.io/SonicVisionLM.github.io/",2024-01-09 07:30:10+00:00,"['Zhifeng Xie', 'Shengye Yu', 'Qile He', 'Mengtian Li']",http://arxiv.org/abs/2401.04394v3
SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints,"Recent advancements in video diffusion models have shown exceptional
abilities in simulating real-world dynamics and maintaining 3D consistency.
This progress inspires us to investigate the potential of these models to
ensure dynamic consistency across various viewpoints, a highly desirable
feature for applications such as virtual filming. Unlike existing methods
focused on multi-view generation of single objects for 4D reconstruction, our
interest lies in generating open-world videos from arbitrary viewpoints,
incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play
module that enhances a pre-trained text-to-video model for multi-camera video
generation, ensuring consistent content across different viewpoints.
Specifically, we introduce a multi-view synchronization module to maintain
appearance and geometry consistency across these viewpoints. Given the scarcity
of high-quality training data, we design a hybrid training scheme that
leverages multi-camera images and monocular videos to supplement Unreal
Engine-rendered multi-camera videos. Furthermore, our method enables intriguing
extensions, such as re-rendering a video from novel viewpoints. We also release
a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project
page: https://jianhongbai.github.io/SynCamMaster/.",2024-12-10 18:55:17+00:00,"['Jianhong Bai', 'Menghan Xia', 'Xintao Wang', 'Ziyang Yuan', 'Xiao Fu', 'Zuozhu Liu', 'Haoji Hu', 'Pengfei Wan', 'Di Zhang']",http://arxiv.org/abs/2412.07760v1
MotionBridge: Dynamic Video Inbetweening with Flexible Controls,"By generating plausible and smooth transitions between two image frames,
video inbetweening is an essential tool for video editing and long video
synthesis. Traditional works lack the capability to generate complex large
motions. While recent video generation techniques are powerful in creating
high-quality results, they often lack fine control over the details of
intermediate frames, which can lead to results that do not align with the
creative mind. We introduce MotionBridge, a unified video inbetweening
framework that allows flexible controls, including trajectory strokes,
keyframes, masks, guide pixels, and text. However, learning such multi-modal
controls in a unified framework is a challenging task. We thus design two
generators to extract the control signal faithfully and encode feature through
dual-branch embedders to resolve ambiguities. We further introduce a curriculum
training strategy to smoothly learn various controls. Extensive qualitative and
quantitative experiments have demonstrated that such multi-modal controls
enable a more dynamic, customizable, and contextually accurate visual
narrative.",2024-12-17 18:59:33+00:00,"['Maham Tanveer', 'Yang Zhou', 'Simon Niklaus', 'Ali Mahdavi Amiri', 'Hao Zhang', 'Krishna Kumar Singh', 'Nanxuan Zhao']",http://arxiv.org/abs/2412.13190v3
Object-Centric Diffusion for Efficient Video Editing,"Diffusion-based video editing have reached impressive quality and can
transform either the global style, local structure, and attributes of given
video inputs, following textual edit prompts. However, such solutions typically
incur heavy memory and computational costs to generate temporally-coherent
frames, either in the form of diffusion inversion and/or cross-frame attention.
In this paper, we conduct an analysis of such inefficiencies, and suggest
simple yet effective modifications that allow significant speed-ups whilst
maintaining quality. Moreover, we introduce Object-Centric Diffusion, to fix
generation artifacts and further reduce latency by allocating more computations
towards foreground edited regions, arguably more important for perceptual
quality. We achieve this by two novel proposals: i) Object-Centric Sampling,
decoupling the diffusion steps spent on salient or background regions and
spending most on the former, and ii) Object-Centric Token Merging, which
reduces cost of cross-frame attention by fusing redundant tokens in unimportant
background regions. Both techniques are readily applicable to a given video
editing model without retraining, and can drastically reduce its memory and
computational cost. We evaluate our proposals on inversion-based and
control-signal-based editing pipelines, and show a latency reduction up to 10x
for a comparable synthesis quality. Project page:
qualcomm-ai-research.github.io/object-centric-diffusion.",2024-01-11 08:36:15+00:00,"['Kumara Kahatapitiya', 'Adil Karjauv', 'Davide Abati', 'Fatih Porikli', 'Yuki M. Asano', 'Amirhossein Habibian']",http://arxiv.org/abs/2401.05735v3
Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal,"Real-world vision tasks frequently suffer from the appearance of unexpected
adverse weather conditions, including rain, haze, snow, and raindrops. In the
last decade, convolutional neural networks and vision transformers have yielded
outstanding results in single-weather video removal. However, due to the
absence of appropriate adaptation, most of them fail to generalize to other
weather conditions. Although ViWS-Net is proposed to remove adverse weather
conditions in videos with a single set of pre-trained weights, it is seriously
blinded by seen weather at train-time and degenerates when coming to unseen
weather during test-time. In this work, we introduce test-time adaptation into
adverse weather removal in videos, and propose the first framework that
integrates test-time adaptation into the iterative diffusion reverse process.
Specifically, we devise a diffusion-based network with a novel temporal noise
model to efficiently explore frame-correlated information in degraded video
clips at training stage. During inference stage, we introduce a proxy task
named Diffusion Tubelet Self-Calibration to learn the primer distribution of
test video stream and optimize the model by approximating the temporal noise
model for online adaptation. Experimental results, on benchmark datasets,
demonstrate that our Test-Time Adaptation method with Diffusion-based
network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring
videos degraded by seen weather conditions. Its generalizable capability is
also validated with unseen weather conditions in both synthesized and
real-world videos.",2024-03-12 14:21:30+00:00,"['Yijun Yang', 'Hongtao Wu', 'Angelica I. Aviles-Rivero', 'Yulun Zhang', 'Jing Qin', 'Lei Zhu']",http://arxiv.org/abs/2403.07684v1
MM-Ego: Towards Building Egocentric Multimodal LLMs,"This research aims to comprehensively explore building a multimodal
foundation model for egocentric video understanding. To achieve this goal, we
work on three fronts. First, as there is a lack of QA data for egocentric video
understanding, we develop a data engine that efficiently generates 7M
high-quality QA samples for egocentric videos ranging from 30 seconds to one
hour long, based on human-annotated data. This is currently the largest
egocentric QA dataset. Second, we contribute a challenging egocentric QA
benchmark with 629 videos and 7,026 questions to evaluate the models' ability
in recognizing and memorizing visual details across videos of varying lengths.
We introduce a new de-biasing evaluation method to help mitigate the
unavoidable language bias present in the models being evaluated. Third, we
propose a specialized multimodal architecture featuring a novel ""Memory Pointer
Prompting"" mechanism. This design includes a global glimpse step to gain an
overarching understanding of the entire video and identify key visual
information, followed by a fallback step that utilizes the key visual
information to generate responses. This enables the model to more effectively
comprehend extended video content. With the data, benchmark, and model, we
successfully build MM-Ego, an egocentric multimodal LLM that shows powerful
performance on egocentric video understanding.",2024-10-09 17:59:59+00:00,"['Hanrong Ye', 'Haotian Zhang', 'Erik Daxberger', 'Lin Chen', 'Zongyu Lin', 'Yanghao Li', 'Bowen Zhang', 'Haoxuan You', 'Dan Xu', 'Zhe Gan', 'Jiasen Lu', 'Yinfei Yang']",http://arxiv.org/abs/2410.07177v1
E-Motion: Future Motion Simulation via Event Sequence Diffusion,"Forecasting a typical object's future motion is a critical task for
interpreting and interacting with dynamic environments in computer vision.
Event-based sensors, which could capture changes in the scene with exceptional
temporal granularity, may potentially offer a unique opportunity to predict
future motion with a level of detail and precision previously unachievable.
Inspired by that, we propose to integrate the strong learning capacity of the
video diffusion model with the rich motion information of an event camera as a
motion simulation framework. Specifically, we initially employ pre-trained
stable video diffusion models to adapt the event sequence dataset. This process
facilitates the transfer of extensive knowledge from RGB videos to an
event-centric domain. Moreover, we introduce an alignment mechanism that
utilizes reinforcement learning techniques to enhance the reverse generation
trajectory of the diffusion model, ensuring improved performance and accuracy.
Through extensive testing and validation, we demonstrate the effectiveness of
our method in various complex scenarios, showcasing its potential to
revolutionize motion flow prediction in computer vision applications such as
autonomous vehicle guidance, robotic navigation, and interactive media. Our
findings suggest a promising direction for future research in enhancing the
interpretative power and predictive accuracy of computer vision systems.",2024-10-11 09:19:23+00:00,"['Song Wu', 'Zhiyu Zhu', 'Junhui Hou', 'Guangming Shi', 'Jinjian Wu']",http://arxiv.org/abs/2410.08649v1
Instructional Video Generation,"Despite the recent strides in video generation, state-of-the-art methods
still struggle with elements of visual detail. One particularly challenging
case is the class of egocentric instructional videos in which the intricate
motion of the hand coupled with a mostly stable and non-distracting environment
is necessary to convey the appropriate visual action instruction. To address
these challenges, we introduce a new method for instructional video generation.
Our diffusion-based method incorporates two distinct innovations. First, we
propose an automatic method to generate the expected region of motion, guided
by both the visual context and the action text. Second, we introduce a critical
hand structure loss to guide the diffusion model to focus on smooth and
consistent hand poses. We evaluate our method on augmented instructional
datasets based on EpicKitchens and Ego4D, demonstrating significant
improvements over state-of-the-art methods in terms of instructional clarity,
especially of the hand motion in the target region, across diverse environments
and actions. Video results can be found in
https://excitedbutter.github.io/Instructional-Video-Generation/",2024-12-05 14:29:10+00:00,"['Yayuan Li', 'Zhi Cao', 'Jason J. Corso']",http://arxiv.org/abs/2412.04189v3
SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical Video Generation,"Medical video generation has transformative potential for enhancing surgical
understanding and pathology insights through precise and controllable visual
representations. However, current models face limitations in controllability
and authenticity. To bridge this gap, we propose SurgSora, a
motion-controllable surgical video generation framework that uses a single
input frame and user-controllable motion cues. SurgSora consists of three key
modules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB
and depth features from the input frame and integrates them with segmentation
cues to capture detailed spatial features of complex anatomical structures; the
Decoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D
features at multiple scales to enhance temporal understanding and object
spatial dynamics; and the Trajectory Controller (TC), which allows users to
specify motion directions and estimates sparse optical flow, guiding the video
generation process. The fused features are used as conditions for a frozen
Stable Diffusion model to produce realistic, temporally coherent surgical
videos. Extensive evaluations demonstrate that SurgSora outperforms
state-of-the-art methods in controllability and authenticity, showing its
potential to advance surgical video generation for medical education, training,
and research.",2024-12-18 16:34:51+00:00,"['Tong Chen', 'Shuya Yang', 'Junyi Wang', 'Long Bai', 'Hongliang Ren', 'Luping Zhou']",http://arxiv.org/abs/2412.14018v1
Uncovering Hidden Subspaces in Video Diffusion Models Using Re-Identification,"Latent Video Diffusion Models can easily deceive casual observers and domain
experts alike thanks to the produced image quality and temporal consistency.
Beyond entertainment, this creates opportunities around safe data sharing of
fully synthetic datasets, which are crucial in healthcare, as well as other
domains relying on sensitive personal information. However, privacy concerns
with this approach have not fully been addressed yet, and models trained on
synthetic data for specific downstream tasks still perform worse than those
trained on real data. This discrepancy may be partly due to the sampling space
being a subspace of the training videos, effectively reducing the training data
size for downstream models. Additionally, the reduced temporal consistency when
generating long videos could be a contributing factor.
  In this paper, we first show that training privacy-preserving models in
latent space is computationally more efficient and generalize better.
Furthermore, to investigate downstream degradation factors, we propose to use a
re-identification model, previously employed as a privacy preservation filter.
We demonstrate that it is sufficient to train this model on the latent space of
the video generator. Subsequently, we use these models to evaluate the subspace
covered by synthetic video datasets and thus introduce a new way to measure the
faithfulness of generative machine learning models. We focus on a specific
application in healthcare echocardiography to illustrate the effectiveness of
our novel methods. Our findings indicate that only up to 30.8% of the training
videos are learned in latent video diffusion models, which could explain the
lack of performance when training downstream tasks on synthetic data.",2024-11-07 18:32:00+00:00,"['Mischa Dombrowski', 'Hadrien Reynaud', 'Bernhard Kainz']",http://arxiv.org/abs/2411.04956v2
AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction,"Text-guided video prediction (TVP) involves predicting the motion of future
frames from the initial frame according to an instruction, which has wide
applications in virtual reality, robotics, and content creation. Previous TVP
methods make significant breakthroughs by adapting Stable Diffusion for this
task. However, they struggle with frame consistency and temporal stability
primarily due to the limited scale of video datasets. We observe that
pretrained Image2Video diffusion models possess good priors for video dynamics
but they lack textual control. Hence, transferring Image2Video models to
leverage their video dynamic priors while injecting instruction control to
generate controllable videos is both a meaningful and challenging task. To
achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to
predict future video states based on initial frames and text instructions. More
specifically, we design a dual query transformer (DQFormer) architecture, which
integrates the instructions and frames into the conditional embeddings for
future frame prediction. Additionally, we develop Long-Short Term Temporal
Adapters and Spatial Adapters that can quickly transfer general video diffusion
models to specific scenarios with minimal training costs. Experimental results
show that our method significantly outperforms state-of-the-art techniques on
four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and
UCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and
SSv2 respectively, demonstrating its effectiveness in various domains. More
examples can be found at our website https://chenhsing.github.io/AID.",2024-06-10 17:02:08+00:00,"['Zhen Xing', 'Qi Dai', 'Zejia Weng', 'Zuxuan Wu', 'Yu-Gang Jiang']",http://arxiv.org/abs/2406.06465v1
NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing,"We propose a video editing framework, NaRCan, which integrates a hybrid
deformation field and diffusion prior to generate high-quality natural
canonical images to represent the input video. Our approach utilizes homography
to model global motion and employs multi-layer perceptrons (MLPs) to capture
local residual deformations, enhancing the model's ability to handle complex
video dynamics. By introducing a diffusion prior from the early stages of
training, our model ensures that the generated images retain a high-quality
natural appearance, making the produced canonical images suitable for various
downstream tasks in video editing, a capability not achieved by current
canonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA)
fine-tuning and introduce a noise and diffusion prior update scheduling
technique that accelerates the training process by 14 times. Extensive
experimental results show that our method outperforms existing approaches in
various video editing tasks and produces coherent and high-quality edited video
sequences. See our project page for video results at
https://koi953215.github.io/NaRCan_page/.",2024-06-10 17:59:46+00:00,"['Ting-Hsuan Chen', 'Jiewen Chan', 'Hau-Shiang Shiu', 'Shih-Han Yen', 'Chang-Han Yeh', 'Yu-Lun Liu']",http://arxiv.org/abs/2406.06523v2
VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models,"Zero-shot customized video generation has gained significant attention due to
its substantial application potential. Existing methods rely on additional
models to extract and inject reference subject features, assuming that the
Video Diffusion Model (VDM) alone is insufficient for zero-shot customized
video generation. However, these methods often struggle to maintain consistent
subject appearance due to suboptimal feature extraction and injection
techniques. In this paper, we reveal that VDM inherently possesses the force to
extract and inject subject features. Departing from previous heuristic
approaches, we introduce a novel framework that leverages VDM's inherent force
to enable high-quality zero-shot customized video generation. Specifically, for
feature extraction, we directly input reference images into VDM and use its
intrinsic feature extraction process, which not only provides fine-grained
features but also significantly aligns with VDM's pre-trained knowledge. For
feature injection, we devise an innovative bidirectional interaction between
subject features and generated content through spatial self-attention within
VDM, ensuring that VDM has better subject fidelity while maintaining the
diversity of the generated video. Experiments on both customized human and
object video generation validate the effectiveness of our framework.",2024-12-27 13:49:25+00:00,"['Tao Wu', 'Yong Zhang', 'Xiaodong Cun', 'Zhongang Qi', 'Junfu Pu', 'Huanzhang Dou', 'Guangcong Zheng', 'Ying Shan', 'Xi Li']",http://arxiv.org/abs/2412.19645v2
Learning to Localize Actions in Instructional Videos with LLM-Based Multi-Pathway Text-Video Alignment,"Learning to localize temporal boundaries of procedure steps in instructional
videos is challenging due to the limited availability of annotated large-scale
training videos. Recent works focus on learning the cross-modal alignment
between video segments and ASR-transcripted narration texts through contrastive
learning. However, these methods fail to account for the alignment noise, i.e.,
irrelevant narrations to the instructional task in videos and unreliable
timestamps in narrations. To address these challenges, this work proposes a
novel training framework. Motivated by the strong capabilities of Large
Language Models (LLMs) in procedure understanding and text summarization, we
first apply an LLM to filter out task-irrelevant information and summarize
task-related procedure steps (LLM-steps) from narrations. To further generate
reliable pseudo-matching between the LLM-steps and the video for training, we
propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy. The key idea
is to measure alignment between LLM-steps and videos via multiple pathways,
including: (1) step-narration-video alignment using narration timestamps, (2)
direct step-to-video alignment based on their long-term semantic similarity,
and (3) direct step-to-video alignment focusing on short-term fine-grained
semantic similarity learned from general video domains. The results from
different pathways are fused to generate reliable pseudo step-video matching.
We conducted extensive experiments across various tasks and problem settings to
evaluate our proposed method. Our approach surpasses state-of-the-art methods
in three downstream tasks: procedure step grounding, step localization, and
narration grounding by 5.9\%, 3.1\%, and 2.8\%.",2024-09-22 18:40:55+00:00,"['Yuxiao Chen', 'Kai Li', 'Wentao Bao', 'Deep Patel', 'Yu Kong', 'Martin Renqiang Min', 'Dimitris N. Metaxas']",http://arxiv.org/abs/2409.16145v1
Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene Reconstruction,"Real-time 3D reconstruction of surgical scenes plays a vital role in
computer-assisted surgery, holding a promise to enhance surgeons' visibility.
Recent advancements in 3D Gaussian Splatting (3DGS) have shown great potential
for real-time novel view synthesis of general scenes, which relies on accurate
poses and point clouds generated by Structure-from-Motion (SfM) for
initialization. However, 3DGS with SfM fails to recover accurate camera poses
and geometry in surgical scenes due to the challenges of minimal textures and
photometric inconsistencies. To tackle this problem, in this paper, we propose
the first SfM-free 3DGS-based method for surgical scene reconstruction by
jointly optimizing the camera poses and scene representation. Based on the
video continuity, the key of our method is to exploit the immediate optical
flow priors to guide the projection flow derived from 3D Gaussians. Unlike most
previous methods relying on photometric loss only, we formulate the pose
estimation problem as minimizing the flow loss between the projection flow and
optical flow. A consistency check is further introduced to filter the flow
outliers by detecting the rigid and reliable points that satisfy the epipolar
geometry. During 3D Gaussian optimization, we randomly sample frames to
optimize the scene representations to grow the 3D Gaussian progressively.
Experiments on the SCARED dataset demonstrate our superior performance over
existing methods in novel view synthesis and pose estimation with high
efficiency. Code is available at https://github.com/wrld/Free-SurGS.",2024-07-03 08:49:35+00:00,"['Jiaxin Guo', 'Jiangliu Wang', 'Di Kang', 'Wenzhen Dong', 'Wenting Wang', 'Yun-hui Liu']",http://arxiv.org/abs/2407.02918v1
Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion,"Directly generating scenes from satellite imagery offers exciting
possibilities for integration into applications like games and map services.
However, challenges arise from significant view changes and scene scale.
Previous efforts mainly focused on image or video generation, lacking
exploration into the adaptability of scene generation for arbitrary views.
Existing 3D generation works either operate at the object level or are
difficult to utilize the geometry obtained from satellite imagery. To overcome
these limitations, we propose a novel architecture for direct 3D scene
generation by introducing diffusion models into 3D sparse representations and
combining them with neural rendering techniques. Specifically, our approach
generates texture colors at the point level for a given geometry using a 3D
diffusion model first, which is then transformed into a scene representation in
a feed-forward manner. The representation can be utilized to render arbitrary
views which would excel in both single-frame quality and inter-frame
consistency. Experiments in two city-scale datasets show that our model
demonstrates proficiency in generating photo-realistic street-view image
sequences and cross-view urban scenes from satellite imagery.",2024-01-19 16:15:37+00:00,"['Zuoyue Li', 'Zhenqiang Li', 'Zhaopeng Cui', 'Marc Pollefeys', 'Martin R. Oswald']",http://arxiv.org/abs/2401.10786v2
Training-Free Semantic Video Composition via Pre-trained Diffusion Model,"The video composition task aims to integrate specified foregrounds and
backgrounds from different videos into a harmonious composite. Current
approaches, predominantly trained on videos with adjusted foreground color and
lighting, struggle to address deep semantic disparities beyond superficial
adjustments, such as domain gaps. Therefore, we propose a training-free
pipeline employing a pre-trained diffusion model imbued with semantic prior
knowledge, which can process composite videos with broader semantic
disparities. Specifically, we process the video frames in a cascading manner
and handle each frame in two processes with the diffusion model. In the
inversion process, we propose Balanced Partial Inversion to obtain generation
initial points that balance reversibility and modifiability. Then, in the
generation process, we further propose Inter-Frame Augmented attention to
augment foreground continuity across frames. Experimental results reveal that
our pipeline successfully ensures the visual harmony and inter-frame coherence
of the outputs, demonstrating efficacy in managing broader semantic
disparities.",2024-01-17 13:07:22+00:00,"['Jiaqi Guo', 'Sitong Su', 'Junchen Zhu', 'Lianli Gao', 'Jingkuan Song']",http://arxiv.org/abs/2401.09195v1
Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices,"As one of the most popular and sought-after generative models in the recent
years, diffusion models have sparked the interests of many researchers and
steadily shown excellent advantage in various generative tasks such as image
synthesis, video generation, molecule design, 3D scene rendering and multimodal
generation, relying on their dense theoretical principles and reliable
application practices. The remarkable success of these recent efforts on
diffusion models comes largely from progressive design principles and efficient
architecture, training, inference, and deployment methodologies. However, there
has not been a comprehensive and in-depth review to summarize these principles
and practices to help the rapid understanding and application of diffusion
models. In this survey, we provide a new efficiency-oriented perspective on
these existing efforts, which mainly focuses on the profound principles and
efficient practices in architecture designs, model training, fast inference and
reliable deployment, to guide further theoretical research, algorithm migration
and model application for new scenarios in a reader-friendly way.
\url{https://github.com/ponyzym/Efficient-DMs-Survey}",2024-10-15 17:19:46+00:00,"['Zhiyuan Ma', 'Yuzhu Zhang', 'Guoli Jia', 'Liangliang Zhao', 'Yichao Ma', 'Mingjie Ma', 'Gaofeng Liu', 'Kaiyan Zhang', 'Jianjun Li', 'Bowen Zhou']",http://arxiv.org/abs/2410.11795v2
Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches,"3D Content Generation is at the heart of many computer graphics applications,
including video gaming, film-making, virtual and augmented reality, etc. This
paper proposes a novel deep-learning based approach for automatically
generating interactive and playable 3D game scenes, all from the user's casual
prompts such as a hand-drawn sketch. Sketch-based input offers a natural, and
convenient way to convey the user's design intention in the content creation
process. To circumvent the data-deficient challenge in learning (i.e. the lack
of large training data of 3D scenes), our method leverages a pre-trained 2D
denoising diffusion model to generate a 2D image of the scene as the conceptual
guidance. In this process, we adopt the isometric projection mode to factor out
unknown camera poses while obtaining the scene layout. From the generated
isometric image, we use a pre-trained image understanding method to segment the
image into meaningful parts, such as off-ground objects, trees, and buildings,
and extract the 2D scene layout. These segments and layouts are subsequently
fed into a procedural content generation (PCG) engine, such as a 3D video game
engine like Unity or Unreal, to create the 3D scene. The resulting 3D scene can
be seamlessly integrated into a game development environment and is readily
playable. Extensive tests demonstrate that our method can efficiently generate
high-quality and interactive 3D game scenes with layouts that closely follow
the user's intention.",2024-08-08 16:27:37+00:00,"['Yongzhi Xu', 'Yonhon Ng', 'Yifu Wang', 'Inkyu Sa', 'Yunfei Duan', 'Yang Li', 'Pan Ji', 'Hongdong Li']",http://arxiv.org/abs/2408.04567v1
CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects,"Customized text-to-video generation aims to generate high-quality videos
guided by text prompts and subject references. Current approaches for
personalizing text-to-video generation suffer from tackling multiple subjects,
which is a more challenging and practical scenario. In this work, our aim is to
promote multi-subject guided text-to-video customization. We propose
CustomVideo, a novel framework that can generate identity-preserving videos
with the guidance of multiple subjects. To be specific, firstly, we encourage
the co-occurrence of multiple subjects via composing them in a single image.
Further, upon a basic text-to-video diffusion model, we design a simple yet
effective attention control strategy to disentangle different subjects in the
latent space of diffusion model. Moreover, to help the model focus on the
specific area of the object, we segment the object from given reference images
and provide a corresponding object mask for attention learning. Also, we
collect a multi-subject text-to-video generation dataset as a comprehensive
benchmark, with 63 individual subjects from 13 different categories and 68
meaningful pairs. Extensive qualitative, quantitative, and user study results
demonstrate the superiority of our method compared to previous state-of-the-art
approaches. The project page is https://kyfafyd.wang/projects/customvideo.",2024-01-18 13:23:51+00:00,"['Zhao Wang', 'Aoxue Li', 'Lingting Zhu', 'Yong Guo', 'Qi Dou', 'Zhenguo Li']",http://arxiv.org/abs/2401.09962v2
Magic-Me: Identity-Specific Video Customized Diffusion,"Creating content with specified identities (ID) has attracted significant
interest in the field of generative models. In the field of text-to-image
generation (T2I), subject-driven creation has achieved great progress with the
identity controlled via reference images. However, its extension to video
generation is not well explored. In this work, we propose a simple yet
effective subject identity controllable video generation framework, termed
Video Custom Diffusion (VCD). With a specified identity defined by a few
images, VCD reinforces the identity characteristics and injects frame-wise
correlation at the initialization stage for stable video outputs. To achieve
this, we propose three novel components that are essential for high-quality
identity preservation and stable video generation: 1) a noise initialization
method with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID
module based on extended Textual Inversion trained with the cropped identity to
disentangle the ID information from the background 3) Face VCD and Tiled VCD
modules to reinforce faces and upscale the video to higher resolution while
preserving the identity's features. We conducted extensive experiments to
verify that VCD is able to generate stable videos with better ID over the
baselines. Besides, with the transferability of the encoded identity in the ID
module, VCD is also working well with personalized text-to-image models
available publicly. The codes are available at
https://github.com/Zhen-Dong/Magic-Me.",2024-02-14 18:13:51+00:00,"['Ze Ma', 'Daquan Zhou', 'Chun-Hsiao Yeh', 'Xue-She Wang', 'Xiuyu Li', 'Huanrui Yang', 'Zhen Dong', 'Kurt Keutzer', 'Jiashi Feng']",http://arxiv.org/abs/2402.09368v2
Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation,"Recent large-scale pre-trained diffusion models have demonstrated a powerful
generative ability to produce high-quality videos from detailed text
descriptions. However, exerting control over the motion of objects in videos
generated by any video diffusion model is a challenging problem. In this paper,
we propose a novel zero-shot moving object trajectory control framework,
Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video
diffusion model. To this end, an initial noise prior module is designed to
provide a position-based prior to improve the stability of the appearance of
the moving object and the accuracy of position. In addition, based on the
attention map of the U-net, spatial constraints are directly applied to the
denoising process of diffusion models, which further ensures the positional and
spatial consistency of moving objects during the inference. Furthermore,
temporal consistency is guaranteed with a proposed shift temporal attention
mechanism. Our method can be flexibly applied to various state-of-the-art video
diffusion models without any training process. Extensive experiments
demonstrate our proposed method can control the motion trajectories of objects
and generate high-quality videos. Our project page is
https://vpx-ecnu.github.io/MotionZero-website/",2024-01-18 17:22:37+00:00,"['Changgu Chen', 'Junwei Shu', 'Gaoqi He', 'Changbo Wang', 'Yang Li']",http://arxiv.org/abs/2401.10150v4
Text-based Talking Video Editing with Cascaded Conditional Diffusion,"Text-based talking-head video editing aims to efficiently insert, delete, and
substitute segments of talking videos through a user-friendly text editing
approach. It is challenging because of \textbf{1)} generalizable talking-face
representation, \textbf{2)} seamless audio-visual transitions, and \textbf{3)}
identity-preserved talking faces. Previous works either require minutes of
talking-face video training data and expensive test-time optimization for
customized talking video editing or directly generate a video sequence without
considering in-context information, leading to a poor generalizable
representation, or incoherent transitions, or even inconsistent identity. In
this paper, we propose an efficient cascaded conditional diffusion-based
framework, which consists of two stages: audio to dense-landmark motion and
motion to video. \textit{\textbf{In the first stage}}, we first propose a
dynamic weighted in-context diffusion module to synthesize dense-landmark
motions given an edited audio. \textit{\textbf{In the second stage}}, we
introduce a warping-guided conditional diffusion module. The module first
interpolates between the start and end frames of the editing interval to
generate smooth intermediate frames. Then, with the help of the audio-to-dense
motion images, these intermediate frames are warped to obtain coarse
intermediate frames. Conditioned on the warped intermedia frames, a diffusion
model is adopted to generate detailed and high-resolution target frames, which
guarantees coherent and identity-preserved transitions. The cascaded
conditional diffusion model decomposes the complex talking editing task into
two flexible generation tasks, which provides a generalizable talking-face
representation, seamless audio-visual transitions, and identity-preserved faces
on a small dataset. Experiments show the effectiveness and superiority of the
proposed method.",2024-07-20 10:55:19+00:00,"['Bo Han', 'Heqing Zou', 'Haoyang Li', 'Guangcong Wang', 'Chng Eng Siong']",http://arxiv.org/abs/2407.14841v1
Fast and Memory-Efficient Video Diffusion Using Streamlined Inference,"The rapid progress in artificial intelligence-generated content (AIGC),
especially with diffusion models, has significantly advanced development of
high-quality video generation. However, current video diffusion models exhibit
demanding computational requirements and high peak memory usage, especially for
generating longer and higher-resolution videos. These limitations greatly
hinder the practical application of video diffusion models on standard hardware
platforms. To tackle this issue, we present a novel, training-free framework
named Streamlined Inference, which leverages the temporal and spatial
properties of video diffusion models. Our approach integrates three core
components: Feature Slicer, Operator Grouping, and Step Rehash. Specifically,
Feature Slicer effectively partitions input features into sub-features and
Operator Grouping processes each sub-feature with a group of consecutive
operators, resulting in significant memory reduction without sacrificing the
quality or speed. Step Rehash further exploits the similarity between adjacent
steps in diffusion, and accelerates inference through skipping unnecessary
steps. Extensive experiments demonstrate that our approach significantly
reduces peak memory and computational overhead, making it feasible to generate
high-quality videos on a single consumer GPU (e.g., reducing peak memory of
AnimateDiff from 42GB to 11GB, featuring faster inference on 2080Ti).",2024-11-02 07:52:18+00:00,"['Zheng Zhan', 'Yushu Wu', 'Yifan Gong', 'Zichong Meng', 'Zhenglun Kong', 'Changdi Yang', 'Geng Yuan', 'Pu Zhao', 'Wei Niu', 'Yanzhi Wang']",http://arxiv.org/abs/2411.01171v1
Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM,"Text-to-video models have made remarkable advancements through optimization
on high-quality text-video pairs, where the textual prompts play a pivotal role
in determining quality of output videos. However, achieving the desired output
often entails multiple revisions and iterative inference to refine
user-provided prompts. Current automatic methods for refining prompts encounter
challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware
when applied to text-to-video diffusion models. To address these problem, we
introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,
which excels in crafting Video-Centric, Labor-Free and Preference-Aligned
prompts tailored to specific video diffusion model. Our approach involves a
meticulously crafted two-stage optimization and alignment system. Initially, we
conduct a reward-guided prompt evolution pipeline to automatically create
optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the
LLM. Then multi-dimensional rewards are employed to generate pairwise data for
the SFT model, followed by the direct preference optimization (DPO) algorithm
to further facilitate preference alignment. Through extensive experimentation
and comparative analyses, we validate the effectiveness of Prompt-A-Video
across diverse generation models, highlighting its potential to push the
boundaries of video generation.",2024-12-19 18:32:21+00:00,"['Yatai Ji', 'Jiacheng Zhang', 'Jie Wu', 'Shilong Zhang', 'Shoufa Chen', 'Chongjian GE', 'Peize Sun', 'Weifeng Chen', 'Wenqi Shao', 'Xuefeng Xiao', 'Weilin Huang', 'Ping Luo']",http://arxiv.org/abs/2412.15156v1
FoodMem: Near Real-time and Precise Food Video Segmentation,"Food segmentation, including in videos, is vital for addressing real-world
health, agriculture, and food biotechnology issues. Current limitations lead to
inaccurate nutritional analysis, inefficient crop management, and suboptimal
food processing, impacting food security and public health. Improving
segmentation techniques can enhance dietary assessments, agricultural
productivity, and the food production process. This study introduces the
development of a robust framework for high-quality, near-real-time segmentation
and tracking of food items in videos, using minimal hardware resources. We
present FoodMem, a novel framework designed to segment food items from video
sequences of 360-degree unbounded scenes. FoodMem can consistently generate
masks of food portions in a video sequence, overcoming the limitations of
existing semantic segmentation models, such as flickering and prohibitive
inference speeds in video processing contexts. To address these issues, FoodMem
leverages a two-phase solution: a transformer segmentation phase to create
initial segmentation masks and a memory-based tracking phase to monitor food
masks in complex scenes. Our framework outperforms current state-of-the-art
food segmentation models, yielding superior performance across various
conditions, such as camera angles, lighting, reflections, scene complexity, and
food diversity. This results in reduced segmentation noise, elimination of
artifacts, and completion of missing segments. Here, we also introduce a new
annotated food dataset encompassing challenging scenarios absent in previous
benchmarks. Extensive experiments conducted on MetaFood3D, Nutrition5k, and
Vegetables & Fruits datasets demonstrate that FoodMem enhances the
state-of-the-art by 2.5% mean average precision in food video segmentation and
is 58 x faster on average.",2024-07-16 19:15:07+00:00,"['Ahmad AlMughrabi', 'Adrin Galn', 'Ricardo Marques', 'Petia Radeva']",http://arxiv.org/abs/2407.12121v2
ExpressEdit: Video Editing with Natural Language and Sketching,"Informational videos serve as a crucial source for explaining conceptual and
procedural knowledge to novices and experts alike. When producing informational
videos, editors edit videos by overlaying text/images or trimming footage to
enhance the video quality and make it more engaging. However, video editing can
be difficult and time-consuming, especially for novice video editors who often
struggle with expressing and implementing their editing ideas. To address this
challenge, we first explored how multimodality$-$natural language (NL) and
sketching, which are natural modalities humans use for expression$-$can be
utilized to support video editors in expressing video editing ideas. We
gathered 176 multimodal expressions of editing commands from 10 video editors,
which revealed the patterns of use of NL and sketching in describing edit
intents. Based on the findings, we present ExpressEdit, a system that enables
editing videos via NL text and sketching on the video frame. Powered by LLM and
vision models, the system interprets (1) temporal, (2) spatial, and (3)
operational references in an NL command and spatial references from sketching.
The system implements the interpreted edits, which then the user can iterate
on. An observational study (N=10) showed that ExpressEdit enhanced the ability
of novice video editors to express and implement their edit ideas. The system
allowed participants to perform edits more efficiently and generate more ideas
by generating edits based on user's multimodal edit commands and supporting
iterations on the editing commands. This work offers insights into the design
of future multimodal interfaces and AI-based pipelines for video editing.",2024-03-26 13:34:21+00:00,"['Bekzat Tilekbay', 'Saelyne Yang', 'Michal Lewkowicz', 'Alex Suryapranata', 'Juho Kim']",http://arxiv.org/abs/2403.17693v1
RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks,"We present a novel unconditional video generative model designed to address
long-term spatial and temporal dependencies, with attention to computational
and dataset efficiency. To capture long spatio-temporal dependencies, our
approach incorporates a hybrid explicit-implicit tri-plane representation
inspired by 3D-aware generative frameworks developed for three-dimensional
object representation and employs a single latent code to model an entire video
clip. Individual video frames are then synthesized from an intermediate
tri-plane representation, which itself is derived from the primary latent code.
This novel strategy more than halves the computational complexity measured in
FLOPs compared to the most efficient state-of-the-art methods. Consequently,
our approach facilitates the efficient and temporally coherent generation of
videos. Moreover, our joint frame modeling approach, in contrast to
autoregressive methods, mitigates the generation of visual artifacts. We
further enhance the model's capabilities by integrating an optical flow-based
module within our Generative Adversarial Network (GAN) based generator
architecture, thereby compensating for the constraints imposed by a smaller
generator size. As a result, our model synthesizes high-fidelity video clips at
a resolution of $256\times256$ pixels, with durations extending to more than
$5$ seconds at a frame rate of 30 fps. The efficacy and versatility of our
approach are empirically validated through qualitative and quantitative
assessments across three different datasets comprising both synthetic and real
video clips. We will make our training and inference code public.",2024-01-11 16:48:44+00:00,"['Partha Ghosh', 'Soubhik Sanyal', 'Cordelia Schmid', 'Bernhard Schlkopf']",http://arxiv.org/abs/2401.06035v2
Sora Detector: A Unified Hallucination Detection for Large Text-to-Video Models,"The rapid advancement in text-to-video (T2V) generative models has enabled
the synthesis of high-fidelity video content guided by textual descriptions.
Despite this significant progress, these models are often susceptible to
hallucination, generating contents that contradict the input text, which poses
a challenge to their reliability and practical deployment. To address this
critical issue, we introduce the SoraDetector, a novel unified framework
designed to detect hallucinations across diverse large T2V models, including
the cutting-edge Sora model. Our framework is built upon a comprehensive
analysis of hallucination phenomena, categorizing them based on their
manifestation in the video content. Leveraging the state-of-the-art keyframe
extraction techniques and multimodal large language models, SoraDetector first
evaluates the consistency between extracted video content summary and textual
prompts, then constructs static and dynamic knowledge graphs (KGs) from frames
to detect hallucination both in single frames and across frames. Sora Detector
provides a robust and quantifiable measure of consistency, static and dynamic
hallucination. In addition, we have developed the Sora Detector Agent to
automate the hallucination detection process and generate a complete video
quality report for each input video. Lastly, we present a novel meta-evaluation
benchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of
advancements in T2V hallucination detection. Through extensive experiments on
videos generated by Sora and other large T2V models, we demonstrate the
efficacy of our approach in accurately detecting hallucinations. The code and
dataset can be accessed via GitHub.",2024-05-07 10:39:14+00:00,"['Zhixuan Chu', 'Lei Zhang', 'Yichen Sun', 'Siqiao Xue', 'Zhibo Wang', 'Zhan Qin', 'Kui Ren']",http://arxiv.org/abs/2405.04180v1
Robot Shape and Location Retention in Video Generation Using Diffusion Models,"Diffusion models have marked a significant milestone in the enhancement of
image and video generation technologies. However, generating videos that
precisely retain the shape and location of moving objects such as robots
remains a challenge. This paper presents diffusion models specifically tailored
to generate videos that accurately maintain the shape and location of mobile
robots. This development offers substantial benefits to those working on
detecting dangerous interactions between humans and robots by facilitating the
creation of training data for collision detection models, circumventing the
need for collecting data from the real world, which often involves legal and
ethical issues. Our models incorporate techniques such as embedding accessible
robot pose information and applying semantic mask regulation within the
ConvNext backbone network. These techniques are designed to refine intermediate
outputs, therefore improving the retention performance of shape and location.
Through extensive experimentation, our models have demonstrated notable
improvements in maintaining the shape and location of different robots, as well
as enhancing overall video generation quality, compared to the benchmark
diffusion model. Codes will be opensourced at
\href{https://github.com/PengPaulWang/diffusion-robots}{Github}.",2024-07-03 07:38:26+00:00,"['Peng Wang', 'Zhihao Guo', 'Abdul Latheef Sait', 'Minh Huy Pham']",http://arxiv.org/abs/2407.02873v1
Can Generative Video Models Help Pose Estimation?,"Pairwise pose estimation from images with little or no overlap is an open
challenge in computer vision. Existing methods, even those trained on
large-scale datasets, struggle in these scenarios due to the lack of
identifiable correspondences or visual overlap. Inspired by the human ability
to infer spatial relationships from diverse scenes, we propose a novel
approach, InterPose, that leverages the rich priors encoded within pre-trained
generative video models. We propose to use a video model to hallucinate
intermediate frames between two input images, effectively creating a dense,
visual transition, which significantly simplifies the problem of pose
estimation. Since current video models can still produce implausible motion or
inconsistent geometry, we introduce a self-consistency score that evaluates the
consistency of pose predictions from sampled videos. We demonstrate that our
approach generalizes among three state-of-the-art video models and show
consistent improvements over the state-of-the-art DUSt3R on four diverse
datasets encompassing indoor, outdoor, and object-centric scenes. Our findings
suggest a promising avenue for improving pose estimation models by leveraging
large generative models trained on vast amounts of video data, which is more
readily available than 3D data. See our project page for results:
https://inter-pose.github.io/.",2024-12-20 18:58:24+00:00,"['Ruojin Cai', 'Jason Y. Zhang', 'Philipp Henzler', 'Zhengqi Li', 'Noah Snavely', 'Ricardo Martin-Brualla']",http://arxiv.org/abs/2412.16155v1
Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity,"Reconstructing human dynamic vision from brain activity is a challenging task
with great scientific significance. Although prior video reconstruction methods
have made substantial progress, they still suffer from several limitations,
including: (1) difficulty in simultaneously reconciling semantic (e.g.
categorical descriptions), structure (e.g. size and color), and consistent
motion information (e.g. order of frames); (2) low temporal resolution of fMRI,
which poses a challenge in decoding multiple frames of video dynamics from a
single fMRI frame; (3) reliance on video generation models, which introduces
ambiguity regarding whether the dynamics observed in the reconstructed videos
are genuinely derived from fMRI data or are hallucinations from generative
model. To overcome these limitations, we propose a two-stage model named
Mind-Animator. During the fMRI-to-feature stage, we decouple semantic,
structure, and motion features from fMRI. Specifically, we employ
fMRI-vision-language tri-modal contrastive learning to decode semantic feature
from fMRI and design a sparse causal attention mechanism for decoding
multi-frame video motion features through a next-frame-prediction task. In the
feature-to-video stage, these features are integrated into videos using an
inflated Stable Diffusion, effectively eliminating external video data
interference. Extensive experiments on multiple video-fMRI datasets demonstrate
that our model achieves state-of-the-art performance. Comprehensive
visualization analyses further elucidate the interpretability of our model from
a neurobiological perspective. Project page:
https://mind-animator-design.github.io/.",2024-05-06 08:56:41+00:00,"['Yizhuo Lu', 'Changde Du', 'Chong Wang', 'Xuanliu Zhu', 'Liuyun Jiang', 'Xujin Li', 'Huiguang He']",http://arxiv.org/abs/2405.03280v2
MAKIMA: Tuning-free Multi-Attribute Open-domain Video Editing via Mask-Guided Attention Modulation,"Diffusion-based text-to-image (T2I) models have demonstrated remarkable
results in global video editing tasks. However, their focus is primarily on
global video modifications, and achieving desired attribute-specific changes
remains a challenging task, specifically in multi-attribute editing (MAE) in
video. Contemporary video editing approaches either require extensive
fine-tuning or rely on additional networks (such as ControlNet) for modeling
multi-object appearances, yet they remain in their infancy, offering only
coarse-grained MAE solutions. In this paper, we present MAKIMA, a tuning-free
MAE framework built upon pretrained T2I models for open-domain video editing.
Our approach preserves video structure and appearance information by
incorporating attention maps and features from the inversion process during
denoising. To facilitate precise editing of multiple attributes, we introduce
mask-guided attention modulation, enhancing correlations between spatially
corresponding tokens and suppressing cross-attribute interference in both
self-attention and cross-attention layers. To balance video frame generation
quality and efficiency, we implement consistent feature propagation, which
generates frame sequences by editing keyframes and propagating their features
throughout the sequence. Extensive experiments demonstrate that MAKIMA
outperforms existing baselines in open-domain multi-attribute video editing
tasks, achieving superior results in both editing accuracy and temporal
consistency while maintaining computational efficiency.",2024-12-28 02:36:51+00:00,"['Haoyu Zheng', 'Wenqiao Zhang', 'Zheqi Lv', 'Yu Zhong', 'Yang Dai', 'Jianxiang An', 'Yongliang Shen', 'Juncheng Li', 'Dongping Zhang', 'Siliang Tang', 'Yueting Zhuang']",http://arxiv.org/abs/2412.19978v1
Sparse Input View Synthesis: 3D Representations and Reliable Priors,"Novel view synthesis refers to the problem of synthesizing novel viewpoints
of a scene given the images from a few viewpoints. This is a fundamental
problem in computer vision and graphics, and enables a vast variety of
applications such as meta-verse, free-view watching of events, video gaming,
video stabilization and video compression. Recent 3D representations such as
radiance fields and multi-plane images significantly improve the quality of
images rendered from novel viewpoints. However, these models require a dense
sampling of input views for high quality renders. Their performance goes down
significantly when only a few input views are available. In this thesis, we
focus on the sparse input novel view synthesis problem for both static and
dynamic scenes. In the first part of this work, we mainly focus on sparse input
novel view synthesis of static scenes using neural radiance fields (NeRF). We
study the design of reliable and dense priors to better regularize the NeRF in
such situations. In particular, we propose a prior on the visibility of the
pixels in a pair of input views. We show that this visibility prior, which is
related to the relative depth of objects, is dense and more reliable than
existing priors on absolute depth. We compute the visibility prior using plane
sweep volumes without the need to train a neural network on large datasets. We
evaluate our approach on multiple datasets and show that our model outperforms
existing approaches for sparse input novel view synthesis. In the second part,
we aim to further improve the regularization by learning a scene-specific prior
that does not suffer from generalization issues. We achieve this by learning
the prior on the given scene alone without pre-training on large datasets. In
particular, we design augmented NeRFs to obtain better depth supervision in
certain regions of the scene for the main NeRF. Further, we extend this
framework to also apply to newer and faster radiance field models such as
TensoRF and ZipNeRF. Through extensive experiments on multiple datasets, we
show the superiority of our approach in sparse input novel view synthesis. The
design of sparse input fast dynamic radiance fields is severely constrained by
the lack of suitable representations and reliable priors for motion. We address
the first challenge by designing an explicit motion model based on factorized
volumes that is compact and optimizes quickly. We also introduce reliable
sparse flow priors to constrain the motion field, since we find that the
popularly employed dense optical flow priors are unreliable. We show the
benefits of our motion representation and reliable priors on multiple datasets.
In the final part of this thesis, we study the application of view synthesis
for frame rate upsampling in video gaming. Specifically, we consider the
problem of temporal view synthesis, where the goal is to predict the future
frames given the past frames and the camera motion. The key challenge here is
in predicting the future motion of the objects by estimating their past motion
and extrapolating it. We explore the use of multi-plane image representations
and scene depth to reliably estimate the object motion, particularly in the
occluded regions. We design a new database to effectively evaluate our approach
for temporal view synthesis of dynamic scenes and show that we achieve
state-of-the-art performance.",2024-11-20 18:45:46+00:00,['Nagabhushan Somraj'],http://arxiv.org/abs/2411.13631v1
Teaching Video Diffusion Model with Latent Physical Phenomenon Knowledge,"Video diffusion models have exhibited tremendous progress in various video
generation tasks. However, existing models struggle to capture latent physical
knowledge, failing to infer physical phenomena that are challenging to
articulate with natural language. Generating videos following the fundamental
physical laws is still an opening challenge. To address this challenge, we
propose a novel method to teach video diffusion models with latent physical
phenomenon knowledge, enabling the accurate generation of physically informed
phenomena. Specifically, we first pretrain Masked Autoencoders (MAE) to
reconstruct the physical phenomena, resulting in output embeddings that
encapsulate latent physical phenomenon knowledge. Leveraging these embeddings,
we could generate the pseudo-language prompt features based on the aligned
spatial relationships between CLIP vision and language encoders. Particularly,
given that diffusion models typically use CLIP's language encoder for text
prompt embeddings, our approach integrates the CLIP visual features informed by
latent physical knowledge into a quaternion hidden space. This enables the
modeling of spatial relationships to produce physical knowledge-informed
pseudo-language prompts. By incorporating these prompt features and fine-tuning
the video diffusion model in a parameter-efficient manner, the physical
knowledge-informed videos are successfully generated. We validate our method
extensively through both numerical simulations and real-world observations of
physical phenomena, demonstrating its remarkable performance across diverse
scenarios.",2024-11-18 07:26:09+00:00,"['Qinglong Cao', 'Ding Wang', 'Xirui Li', 'Yuntian Chen', 'Chao Ma', 'Xiaokang Yang']",http://arxiv.org/abs/2411.11343v1
FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models,"Diffusion model has demonstrated remarkable capability in video generation,
which further sparks interest in introducing trajectory control into the
generation process. While existing works mainly focus on training-based methods
(e.g., conditional adapter), we argue that diffusion model itself allows decent
control over the generated content without requiring any training. In this
study, we introduce a tuning-free framework to achieve trajectory-controllable
video generation, by imposing guidance on both noise construction and attention
computation. Specifically, 1) we first show several instructive phenomenons and
analyze how initial noises influence the motion trajectory of generated
content. 2) Subsequently, we propose FreeTraj, a tuning-free approach that
enables trajectory control by modifying noise sampling and attention
mechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger
video generation with controllable trajectories. Equipped with these designs,
users have the flexibility to provide trajectories manually or opt for
trajectories automatically generated by the LLM trajectory planner. Extensive
experiments validate the efficacy of our approach in enhancing the trajectory
controllability of video diffusion models.",2024-06-24 17:59:56+00:00,"['Haonan Qiu', 'Zhaoxi Chen', 'Zhouxia Wang', 'Yingqing He', 'Menghan Xia', 'Ziwei Liu']",http://arxiv.org/abs/2406.16863v1
A Wavelet Diffusion GAN for Image Super-Resolution,"In recent years, diffusion models have emerged as a superior alternative to
generative adversarial networks (GANs) for high-fidelity image generation, with
wide applications in text-to-image generation, image-to-image translation, and
super-resolution. However, their real-time feasibility is hindered by slow
training and inference speeds. This study addresses this challenge by proposing
a wavelet-based conditional Diffusion GAN scheme for Single-Image
Super-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm to
reduce the timesteps required by the reverse diffusion process and the Discrete
Wavelet Transform (DWT) to achieve dimensionality reduction, decreasing
training and inference times significantly. The results of an experimental
validation on the CelebA-HQ dataset confirm the effectiveness of our proposed
scheme. Our approach outperforms other state-of-the-art methodologies
successfully ensuring high-fidelity output while overcoming inherent drawbacks
associated with diffusion models in time-sensitive applications.",2024-10-23 15:34:06+00:00,"['Lorenzo Aloisi', 'Luigi Sigillo', 'Aurelio Uncini', 'Danilo Comminiello']",http://arxiv.org/abs/2410.17966v1
Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency,"With the introduction of diffusion-based video generation techniques,
audio-conditioned human video generation has recently achieved significant
breakthroughs in both the naturalness of motion and the synthesis of portrait
details. Due to the limited control of audio signals in driving human motion,
existing methods often add auxiliary spatial signals to stabilize movements,
which may compromise the naturalness and freedom of motion. In this paper, we
propose an end-to-end audio-only conditioned video diffusion model named Loopy.
Specifically, we designed an inter- and intra-clip temporal module and an
audio-to-latents module, enabling the model to leverage long-term motion
information from the data to learn natural motion patterns and improving
audio-portrait movement correlation. This method removes the need for manually
specified spatial motion templates used in existing methods to constrain motion
during inference. Extensive experiments show that Loopy outperforms recent
audio-driven portrait diffusion models, delivering more lifelike and
high-quality results across various scenarios.",2024-09-04 11:55:14+00:00,"['Jianwen Jiang', 'Chao Liang', 'Jiaqi Yang', 'Gaojie Lin', 'Tianyun Zhong', 'Yanbo Zheng']",http://arxiv.org/abs/2409.02634v2
Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework,"Despite the remarkable process of talking-head-based avatar-creating
solutions, directly generating anchor-style videos with full-body motions
remains challenging. In this study, we propose Make-Your-Anchor, a novel system
necessitating only a one-minute video clip of an individual for training,
subsequently enabling the automatic generation of anchor-style videos with
precise torso and hand movements. Specifically, we finetune a proposed
structure-guided diffusion model on input video to render 3D mesh conditions
into human appearances. We adopt a two-stage training strategy for the
diffusion model, effectively binding movements with specific appearances. To
produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise
diffusion model to a 3D style without additional training cost, and a simple
yet effective batch-overlapped temporal denoising module is proposed to bypass
the constraints on video length during inference. Finally, a novel
identity-specific face enhancement module is introduced to improve the visual
quality of facial regions in the output videos. Comparative experiments
demonstrate the effectiveness and superiority of the system in terms of visual
quality, temporal coherence, and identity preservation, outperforming SOTA
diffusion/non-diffusion methods. Project page:
\url{https://github.com/ICTMCG/Make-Your-Anchor}.",2024-03-25 07:54:18+00:00,"['Ziyao Huang', 'Fan Tang', 'Yong Zhang', 'Xiaodong Cun', 'Juan Cao', 'Jintao Li', 'Tong-Yee Lee']",http://arxiv.org/abs/2403.16510v1
Beyond Raw Videos: Understanding Edited Videos with Large Multimodal Model,"The emerging video LMMs (Large Multimodal Models) have achieved significant
improvements on generic video understanding in the form of VQA (Visual Question
Answering), where the raw videos are captured by cameras. However, a large
portion of videos in real-world applications are edited videos, \textit{e.g.},
users usually cut and add effects/modifications to the raw video before
publishing it on social media platforms. The edited videos usually have high
view counts but they are not covered in existing benchmarks of video LMMs,
\textit{i.e.}, ActivityNet-QA, or VideoChatGPT benchmark. In this paper, we
leverage the edited videos on a popular short video platform, \textit{i.e.},
TikTok, and build a video VQA benchmark (named EditVid-QA) covering four
typical editing categories, i.e., effect, funny, meme, and game. Funny and meme
videos benchmark nuanced understanding and high-level reasoning, while effect
and game evaluate the understanding capability of artificial design. Most of
the open-source video LMMs perform poorly on the EditVid-QA benchmark,
indicating a huge domain gap between edited short videos on social media and
regular raw videos. To improve the generalization ability of LMMs, we collect a
training set for the proposed benchmark based on both Panda-70M/WebVid raw
videos and small-scale TikTok/CapCut edited videos, which boosts the
performance on the proposed EditVid-QA benchmark, indicating the effectiveness
of high-quality training data. We also identified a serious issue in the
existing evaluation protocol using the GPT-3.5 judge, namely a ""sorry"" attack,
where a sorry-style naive answer can achieve an extremely high rating from the
GPT judge, e.g., over 4.3 for correctness score on VideoChatGPT evaluation
protocol. To avoid the ""sorry"" attacks, we evaluate results with GPT-4 judge
and keyword filtering. The dataset is released at
https://github.com/XenonLamb/EditVid-QA.",2024-06-15 03:28:52+00:00,"['Lu Xu', 'Sijie Zhu', 'Chunyuan Li', 'Chia-Wen Kuo', 'Fan Chen', 'Xinyao Wang', 'Guang Chen', 'Dawei Du', 'Ye Yuan', 'Longyin Wen']",http://arxiv.org/abs/2406.10484v2
UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control,"Video Diffusion Models have been developed for video generation, usually
integrating text and image conditioning to enhance control over the generated
content. Despite the progress, ensuring consistency across frames remains a
challenge, particularly when using text prompts as control conditions. To
address this problem, we introduce UniCtrl, a novel, plug-and-play method that
is universally applicable to improve the spatiotemporal consistency and motion
diversity of videos generated by text-to-video models without additional
training. UniCtrl ensures semantic consistency across different frames through
cross-frame self-attention control, and meanwhile, enhances the motion quality
and spatiotemporal consistency through motion injection and spatiotemporal
synchronization. Our experimental results demonstrate UniCtrl's efficacy in
enhancing various text-to-video models, confirming its effectiveness and
universality.",2024-03-04 18:58:11+00:00,"['Tian Xia', 'Xuweiyi Chen', 'Sihan Xu']",http://arxiv.org/abs/2403.02332v4
Cross-conditioned Diffusion Model for Medical Image to Image Translation,"Multi-modal magnetic resonance imaging (MRI) provides rich, complementary
information for analyzing diseases. However, the practical challenges of
acquiring multiple MRI modalities, such as cost, scan time, and safety
considerations, often result in incomplete datasets. This affects both the
quality of diagnosis and the performance of deep learning models trained on
such data. Recent advancements in generative adversarial networks (GANs) and
denoising diffusion models have shown promise in natural and medical
image-to-image translation tasks. However, the complexity of training GANs and
the computational expense associated with diffusion models hinder their
development and application in this task. To address these issues, we introduce
a Cross-conditioned Diffusion Model (CDM) for medical image-to-image
translation. The core idea of CDM is to use the distribution of target
modalities as guidance to improve synthesis quality while achieving higher
generation efficiency compared to conventional diffusion models. First, we
propose a Modality-specific Representation Model (MRM) to model the
distribution of target modalities. Then, we design a Modality-decoupled
Diffusion Network (MDN) to efficiently and effectively learn the distribution
from MRM. Finally, a Cross-conditioned UNet (C-UNet) with a Condition Embedding
module is designed to synthesize the target modalities with the source
modalities as input and the target distribution for guidance. Extensive
experiments conducted on the BraTS2023 and UPenn-GBM benchmark datasets
demonstrate the superiority of our method.",2024-09-13 02:48:56+00:00,"['Zhaohu Xing', 'Sicheng Yang', 'Sixiang Chen', 'Tian Ye', 'Yijun Yang', 'Jing Qin', 'Lei Zhu']",http://arxiv.org/abs/2409.08500v1
Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification,"Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,
and CogVideoX are pushing the boundaries of synthetic video generation, with
adoption seen in fields like robotics, autonomous driving, and entertainment.
As these models become prevalent, various metrics and benchmarks have emerged
to evaluate the quality of the generated videos. However, these metrics
emphasize visual quality and smoothness, neglecting temporal fidelity and
text-to-video alignment, which are crucial for safety-critical applications. To
address this gap, we introduce NeuS-V, a novel synthetic video evaluation
metric that rigorously assesses text-to-video alignment using neuro-symbolic
formal verification techniques. Our approach first converts the prompt into a
formally defined Temporal Logic (TL) specification and translates the generated
video into an automaton representation. Then, it evaluates the text-to-video
alignment by formally checking the video automaton against the TL
specification. Furthermore, we present a dataset of temporally extended prompts
to evaluate state-of-the-art video generation models against our benchmark. We
find that NeuS-V demonstrates a higher correlation by over 5x with human
evaluations when compared to existing metrics. Our evaluation further reveals
that current video generation models perform poorly on these temporally complex
prompts, highlighting the need for future work in improving text-to-video
generation capabilities.",2024-11-22 23:59:12+00:00,"['S. P. Sharan', 'Minkyu Choi', 'Sahil Shah', 'Harsh Goel', 'Mohammad Omama', 'Sandeep Chinchali']",http://arxiv.org/abs/2411.16718v3
Spectral Motion Alignment for Video Motion Transfer using Diffusion Models,"The evolution of diffusion models has greatly impacted video generation and
understanding. Particularly, text-to-video diffusion models (VDMs) have
significantly facilitated the customization of input video with target
appearance, motion, etc. Despite these advances, challenges persist in
accurately distilling motion information from video frames. While existing
works leverage the consecutive frame residual as the target motion vector, they
inherently lack global motion context and are vulnerable to frame-wise
distortions. To address this, we present Spectral Motion Alignment (SMA), a
novel framework that refines and aligns motion vectors using Fourier and
wavelet transforms. SMA learns motion patterns by incorporating
frequency-domain regularization, facilitating the learning of whole-frame
global motion dynamics, and mitigating spatial artifacts. Extensive experiments
demonstrate SMA's efficacy in improving motion transfer while maintaining
computational efficiency and compatibility across various video customization
frameworks.",2024-03-22 14:47:18+00:00,"['Geon Yeong Park', 'Hyeonho Jeong', 'Sang Wan Lee', 'Jong Chul Ye']",http://arxiv.org/abs/2403.15249v2
GANESH: Generalizable NeRF for Lensless Imaging,"Lensless imaging offers a significant opportunity to develop ultra-compact
cameras by removing the conventional bulky lens system. However, without a
focusing element, the sensor's output is no longer a direct image but a complex
multiplexed scene representation. Traditional methods have attempted to address
this challenge by employing learnable inversions and refinement models, but
these methods are primarily designed for 2D reconstruction and do not
generalize well to 3D reconstruction. We introduce GANESH, a novel framework
designed to enable simultaneous refinement and novel view synthesis from
multi-view lensless images. Unlike existing methods that require scene-specific
training, our approach supports on-the-fly inference without retraining on each
scene. Moreover, our framework allows us to tune our model to specific scenes,
enhancing the rendering and refinement quality. To facilitate research in this
area, we also present the first multi-view lensless dataset, LenslessScenes.
Extensive experiments demonstrate that our method outperforms current
approaches in reconstruction accuracy and refinement quality. Code and video
results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/",2024-11-07 15:47:07+00:00,"['Rakesh Raj Madavan', 'Akshat Kaimal', 'Badhrinarayanan K V', 'Vinayak Gupta', 'Rohit Choudhary', 'Chandrakala Shanmuganathan', 'Kaushik Mitra']",http://arxiv.org/abs/2411.04810v1
GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration,"Text-to-video generation models have shown significant progress in the recent
years. However, they still struggle with generating complex dynamic scenes
based on compositional text prompts, such as attribute binding for multiple
objects, temporal dynamics associated with different objects, and interactions
between objects. Our key motivation is that complex tasks can be decomposed
into simpler ones, each handled by a role-specialized MLLM agent. Multiple
agents can collaborate together to achieve collective intelligence for complex
goals. We propose GenMAC, an iterative, multi-agent framework that enables
compositional text-to-video generation. The collaborative workflow includes
three stages: Design, Generation, and Redesign, with an iterative loop between
the Generation and Redesign stages to progressively verify and refine the
generated videos. The Redesign stage is the most challenging stage that aims to
verify the generated videos, suggest corrections, and redesign the text
prompts, frame-wise layouts, and guidance scales for the next iteration of
generation. To avoid hallucination of a single MLLM agent, we decompose this
stage to four sequentially-executed MLLM-based agents: verification agent,
suggestion agent, correction agent, and output structuring agent. Furthermore,
to tackle diverse scenarios of compositional text-to-video generation, we
design a self-routing mechanism to adaptively select the proper correction
agent from a collection of correction agents each specialized for one scenario.
Extensive experiments demonstrate the effectiveness of GenMAC, achieving
state-of-the art performance in compositional text-to-video generation.",2024-12-05 18:56:05+00:00,"['Kaiyi Huang', 'Yukun Huang', 'Xuefei Ning', 'Zinan Lin', 'Yu Wang', 'Xihui Liu']",http://arxiv.org/abs/2412.04440v1
IV-Mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis,"The multi-step sampling mechanism, a key feature of visual diffusion models,
has significant potential to replicate the success of OpenAI's Strawberry in
enhancing performance by increasing the inference computational cost.
Sufficient prior studies have demonstrated that correctly scaling up
computation in the sampling process can successfully lead to improved
generation quality, enhanced image editing, and compositional generalization.
While there have been rapid advancements in developing inference-heavy
algorithms for improved image generation, relatively little work has explored
inference scaling laws in video diffusion models (VDMs). Furthermore, existing
research shows only minimal performance gains that are perceptible to the naked
eye. To address this, we design a novel training-free algorithm IV-Mixed
Sampler that leverages the strengths of image diffusion models (IDMs) to assist
VDMs surpass their current capabilities. The core of IV-Mixed Sampler is to use
IDMs to significantly enhance the quality of each video frame and VDMs ensure
the temporal coherence of the video during the sampling process. Our
experiments have demonstrated that IV-Mixed Sampler achieves state-of-the-art
performance on 4 benchmarks including UCF-101-FVD, MSR-VTT-FVD,
Chronomagic-Bench-150, and Chronomagic-Bench-1649. For example, the open-source
Animatediff with IV-Mixed Sampler reduces the UMT-FVD score from 275.2 to
228.6, closing to 223.1 from the closed-source Pika-2.0.",2024-10-05 14:33:28+00:00,"['Shitong Shao', 'Zikai Zhou', 'Lichen Bai', 'Haoyi Xiong', 'Zeke Xie']",http://arxiv.org/abs/2410.04171v2
Beyond GFVC: A Progressive Face Video Compression Framework with Adaptive Visual Tokens,"Recently, deep generative models have greatly advanced the progress of face
video coding towards promising rate-distortion performance and diverse
application functionalities. Beyond traditional hybrid video coding paradigms,
Generative Face Video Compression (GFVC) relying on the strong capabilities of
deep generative models and the philosophy of early Model-Based Coding (MBC) can
facilitate the compact representation and realistic reconstruction of visual
face signal, thus achieving ultra-low bitrate face video communication.
However, these GFVC algorithms are sometimes faced with unstable reconstruction
quality and limited bitrate ranges. To address these problems, this paper
proposes a novel Progressive Face Video Compression framework, namely PFVC,
that utilizes adaptive visual tokens to realize exceptional trade-offs between
reconstruction robustness and bandwidth intelligence. In particular, the
encoder of the proposed PFVC projects the high-dimensional face signal into
adaptive visual tokens in a progressive manner, whilst the decoder can further
reconstruct these adaptive visual tokens for motion estimation and signal
synthesis with different granularity levels. Experimental results demonstrate
that the proposed PFVC framework can achieve better coding flexibility and
superior rate-distortion performance in comparison with the latest Versatile
Video Coding (VVC) codec and the state-of-the-art GFVC algorithms. The project
page can be found at https://github.com/Berlin0610/PFVC.",2024-10-11 03:24:21+00:00,"['Bolin Chen', 'Shanzhi Yin', 'Zihan Zhang', 'Jie Chen', 'Ru-Ling Liao', 'Lingyu Zhu', 'Shiqi Wang', 'Yan Ye']",http://arxiv.org/abs/2410.08485v1
Listen and Move: Improving GANs Coherency in Agnostic Sound-to-Video Generation,"Deep generative models have demonstrated the ability to create realistic
audiovisual content, sometimes driven by domains of different nature. However,
smooth temporal dynamics in video generation is a challenging problem. This
work focuses on generic sound-to-video generation and proposes three main
features to enhance both image quality and temporal coherency in generative
adversarial models: a triple sound routing scheme, a multi-scale residual and
dilated recurrent network for extended sound analysis, and a novel recurrent
and directional convolutional layer for video prediction. Each of the proposed
features improves, in both quality and coherency, the baseline neural
architecture typically used in the SoTA, with the video prediction layer
providing an extra temporal refinement.",2024-06-23 16:30:11+00:00,['Rafael Redondo'],http://arxiv.org/abs/2406.16155v1
Deformation-aware GAN for Medical Image Synthesis with Substantially Misaligned Pairs,"Medical image synthesis generates additional imaging modalities that are
costly, invasive or harmful to acquire, which helps to facilitate the clinical
workflow. When training pairs are substantially misaligned (e.g., lung MRI-CT
pairs with respiratory motion), accurate image synthesis remains a critical
challenge. Recent works explored the directional registration module to adjust
misalignment in generative adversarial networks (GANs); however, substantial
misalignment will lead to 1) suboptimal data mapping caused by correspondence
ambiguity, and 2) degraded image fidelity caused by morphology influence on
discriminators. To address the challenges, we propose a novel Deformation-aware
GAN (DA-GAN) to dynamically correct the misalignment during the image synthesis
based on multi-objective inverse consistency. Specifically, in the generative
process, three levels of inverse consistency cohesively optimise symmetric
registration and image generation for improved correspondence. In the
adversarial process, to further improve image fidelity under misalignment, we
design deformation-aware discriminators to disentangle the mismatched spatial
morphology from the judgement of image fidelity. Experimental results show that
DA-GAN achieved superior performance on a public dataset with simulated
misalignments and a real-world lung MRI-CT dataset with respiratory motion
misalignment. The results indicate the potential for a wide range of medical
image synthesis tasks such as radiotherapy planning.",2024-08-18 10:29:35+00:00,"['Bowen Xin', 'Tony Young', 'Claire E Wainwright', 'Tamara Blake', 'Leo Lebrat', 'Thomas Gaass', 'Thomas Benkert', 'Alto Stemmer', 'David Coman', 'Jason Dowling']",http://arxiv.org/abs/2408.09432v1
InternVideo2: Scaling Foundation Models for Multimodal Video Understanding,"We introduce InternVideo2, a new family of video foundation models (ViFM)
that achieve the state-of-the-art results in video recognition, video-text
tasks, and video-centric dialogue. Our core design is a progressive training
approach that unifies the masked video modeling, crossmodal contrastive
learning, and next token prediction, scaling up the video encoder size to 6B
parameters. At the data level, we prioritize spatiotemporal consistency by
semantically segmenting videos and generating video-audio-speech captions. This
improves the alignment between video and text. Through extensive experiments,
we validate our designs and demonstrate superior performance on over 60 video
and audio tasks. Notably, our model outperforms others on various video-related
dialogue and long video understanding benchmarks, highlighting its ability to
reason and comprehend longer contexts. Code and models are available at
https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.",2024-03-22 17:57:42+00:00,"['Yi Wang', 'Kunchang Li', 'Xinhao Li', 'Jiashuo Yu', 'Yinan He', 'Chenting Wang', 'Guo Chen', 'Baoqi Pei', 'Ziang Yan', 'Rongkun Zheng', 'Jilan Xu', 'Zun Wang', 'Yansong Shi', 'Tianxiang Jiang', 'Songze Li', 'Hongjie Zhang', 'Yifei Huang', 'Yu Qiao', 'Yali Wang', 'Limin Wang']",http://arxiv.org/abs/2403.15377v4
Storyboard guided Alignment for Fine-grained Video Action Recognition,"Fine-grained video action recognition can be conceptualized as a video-text
matching problem. Previous approaches often rely on global video semantics to
consolidate video embeddings, which can lead to misalignment in video-text
pairs due to a lack of understanding of action semantics at an atomic
granularity level. To tackle this challenge, we propose a multi-granularity
framework based on two observations: (i) videos with different global semantics
may share similar atomic actions or appearances, and (ii) atomic actions within
a video can be momentary, slow, or even non-directly related to the global
video semantics. Inspired by the concept of storyboarding, which disassembles a
script into individual shots, we enhance global video semantics by generating
fine-grained descriptions using a pre-trained large language model. These
detailed descriptions capture common atomic actions depicted in videos. A
filtering metric is proposed to select the descriptions that correspond to the
atomic actions present in both the videos and the descriptions. By employing
global semantics and fine-grained descriptions, we can identify key frames in
videos and utilize them to aggregate embeddings, thereby making the embedding
more accurate. Extensive experiments on various video action recognition
datasets demonstrate superior performance of our proposed method in supervised,
few-shot, and zero-shot settings.",2024-10-18 07:40:41+00:00,"['Enqi Liu', 'Liyuan Pan', 'Yan Yang', 'Yiran Zhong', 'Zhijing Wu', 'Xinxiao Wu', 'Liu Liu']",http://arxiv.org/abs/2410.14238v1
Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling,"We introduce Motion-I2V, a novel framework for consistent and controllable
image-to-video generation (I2V). In contrast to previous methods that directly
learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into
two stages with explicit motion modeling. For the first stage, we propose a
diffusion-based motion field predictor, which focuses on deducing the
trajectories of the reference image's pixels. For the second stage, we propose
motion-augmented temporal attention to enhance the limited 1-D temporal
attention in video latent diffusion models. This module can effectively
propagate reference image's feature to synthesized frames with the guidance of
predicted trajectories from the first stage. Compared with existing methods,
Motion-I2V can generate more consistent videos even at the presence of large
motion and viewpoint variation. By training a sparse trajectory ControlNet for
the first stage, Motion-I2V can support users to precisely control motion
trajectories and motion regions with sparse trajectory and region annotations.
This offers more controllability of the I2V process than solely relying on
textual instructions. Additionally, Motion-I2V's second stage naturally
supports zero-shot video-to-video translation. Both qualitative and
quantitative comparisons demonstrate the advantages of Motion-I2V over prior
approaches in consistent and controllable image-to-video generation. Please see
our project page at https://xiaoyushi97.github.io/Motion-I2V/.",2024-01-29 09:06:43+00:00,"['Xiaoyu Shi', 'Zhaoyang Huang', 'Fu-Yun Wang', 'Weikang Bian', 'Dasong Li', 'Yi Zhang', 'Manyuan Zhang', 'Ka Chun Cheung', 'Simon See', 'Hongwei Qin', 'Jifeng Dai', 'Hongsheng Li']",http://arxiv.org/abs/2401.15977v2
Towards the Detection of AI-Synthesized Human Face Images,"Over the past years, image generation and manipulation have achieved
remarkable progress due to the rapid development of generative AI based on deep
learning. Recent studies have devoted significant efforts to address the
problem of face image manipulation caused by deepfake techniques. However, the
problem of detecting purely synthesized face images has been explored to a
lesser extent. In particular, the recent popular Diffusion Models (DMs) have
shown remarkable success in image synthesis. Existing detectors struggle to
generalize between synthesized images created by different generative models.
In this work, a comprehensive benchmark including human face images produced by
Generative Adversarial Networks (GANs) and a variety of DMs has been
established to evaluate both the generalization ability and robustness of
state-of-the-art detectors. Then, the forgery traces introduced by different
generative models have been analyzed in the frequency domain to draw various
insights. The paper further demonstrates that a detector trained with frequency
representation can generalize well to other unseen generative models.",2024-02-13 19:37:44+00:00,"['Yuhang Lu', 'Touradj Ebrahimi']",http://arxiv.org/abs/2402.08750v1
VideoClusterNet: Self-Supervised and Adaptive Face Clustering For Videos,"With the rise of digital media content production, the need for analyzing
movies and TV series episodes to locate the main cast of characters precisely
is gaining importance.Specifically, Video Face Clustering aims to group
together detected video face tracks with common facial identities. This problem
is very challenging due to the large range of pose, expression, appearance, and
lighting variations of a given face across video frames. Generic pre-trained
Face Identification (ID) models fail to adapt well to the video production
domain, given its high dynamic range content and also unique cinematic style.
Furthermore, traditional clustering algorithms depend on hyperparameters
requiring individual tuning across datasets. In this paper, we present a novel
video face clustering approach that learns to adapt a generic face ID model to
new video face tracks in a fully self-supervised fashion. We also propose a
parameter-free clustering algorithm that is capable of automatically adapting
to the finetuned model's embedding space for any input video. Due to the lack
of comprehensive movie face clustering benchmarks, we also present a
first-of-kind movie dataset: MovieFaceCluster. Our dataset is handpicked by
film industry professionals and contains extremely challenging face ID
scenarios. Experiments show our method's effectiveness in handling difficult
mainstream movie scenes on our benchmark dataset and state-of-the-art
performance on traditional TV series datasets.",2024-07-16 23:34:55+00:00,"['Devesh Walawalkar', 'Pablo Garrido']",http://arxiv.org/abs/2407.12214v2
STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training,"Video Large Language Models (Video-LLMs) have recently shown strong
performance in basic video understanding tasks, such as captioning and
coarse-grained question answering, but struggle with compositional reasoning
that requires multi-step spatio-temporal inference across object relations,
interactions, and events. The hurdles to enhancing this capability include
extensive manual labor, the lack of spatio-temporal compositionality in
existing data and the absence of explicit reasoning supervision. In this paper,
we propose STEP, a novel graph-guided self-training method that enables
Video-LLMs to generate reasoning-rich fine-tuning data from any raw videos to
improve itself. Specifically, we first induce Spatio-Temporal Scene Graph
(STSG) representation of diverse videos to capture fine-grained, multi-granular
video semantics. Then, the STSGs guide the derivation of multi-step reasoning
Question-Answer (QA) data with Chain-of-Thought (CoT) rationales. Both answers
and rationales are integrated as training objective, aiming to enhance model's
reasoning abilities by supervision over explicit reasoning steps. Experimental
results demonstrate the effectiveness of STEP across models of varying scales,
with a significant 21.3\% improvement in tasks requiring three or more
reasoning steps. Furthermore, it achieves superior performance with a minimal
amount of self-generated rationale-enriched training samples in both
compositional reasoning and comprehensive understanding benchmarks,
highlighting the broad applicability and vast potential.",2024-11-29 11:54:55+00:00,"['Haiyi Qiu', 'Minghe Gao', 'Long Qian', 'Kaihang Pan', 'Qifan Yu', 'Juncheng Li', 'Wenjie Wang', 'Siliang Tang', 'Yueting Zhuang', 'Tat-Seng Chua']",http://arxiv.org/abs/2412.00161v1
Combining Genre Classification and Harmonic-Percussive Features with Diffusion Models for Music-Video Generation,"This study presents a novel method for generating music visualisers using
diffusion models, combining audio input with user-selected artwork. The process
involves two main stages: image generation and video creation. First, music
captioning and genre classification are performed, followed by the retrieval of
artistic style descriptions. A diffusion model then generates images based on
the user's input image and the derived artistic style descriptions. The video
generation stage utilises the same diffusion model to interpolate frames,
controlled by audio energy vectors derived from key musical features of
harmonics and percussives. The method demonstrates promising results across
various genres, and a new metric, Audio-Visual Synchrony (AVS), is introduced
to quantitatively evaluate the synchronisation between visual and audio
elements. Comparative analysis shows significantly higher AVS values for videos
generated using the proposed method with audio energy vectors, compared to
linear interpolation. This approach has potential applications in diverse
fields, including independent music video creation, film production, live music
events, and enhancing audio-visual experiences in public spaces.",2024-12-07 16:43:02+00:00,"['Leonardo Pina', 'Yongmin Li']",http://arxiv.org/abs/2412.05694v1
RoboDreamer: Learning Compositional World Models for Robot Imagination,"Text-to-video models have demonstrated substantial potential in robotic
decision-making, enabling the imagination of realistic plans of future actions
as well as accurate environment simulation. However, one major issue in such
models is generalization -- models are limited to synthesizing videos subject
to language instructions similar to those seen at training time. This is
heavily limiting in decision-making, where we seek a powerful world model to
synthesize plans of unseen combinations of objects and actions in order to
solve previously unseen tasks in new environments. To resolve this issue, we
introduce RoboDreamer, an innovative approach for learning a compositional
world model by factorizing the video generation. We leverage the natural
compositionality of language to parse instructions into a set of lower-level
primitives, which we condition a set of models on to generate videos. We
illustrate how this factorization naturally enables compositional
generalization, by allowing us to formulate a new natural language instruction
as a combination of previously seen components. We further show how such a
factorization enables us to add additional multimodal goals, allowing us to
specify a video we wish to generate given both natural language instructions
and a goal image. Our approach can successfully synthesize video plans on
unseen goals in the RT-X, enables successful robot execution in simulation, and
substantially outperforms monolithic baseline approaches to video generation.",2024-04-18 17:58:03+00:00,"['Siyuan Zhou', 'Yilun Du', 'Jiaben Chen', 'Yandong Li', 'Dit-Yan Yeung', 'Chuang Gan']",http://arxiv.org/abs/2404.12377v1
Denoising Reuse: Exploiting Inter-frame Motion Consistency for Efficient Video Latent Generation,"Video generation using diffusion-based models is constrained by high
computational costs due to the frame-wise iterative diffusion process. This
work presents a Diffusion Reuse MOtion (Dr. Mo) network to accelerate latent
video generation. Our key discovery is that coarse-grained noises in earlier
denoising steps have demonstrated high motion consistency across consecutive
video frames. Following this observation, Dr. Mo propagates those
coarse-grained noises onto the next frame by incorporating carefully designed,
lightweight inter-frame motions, eliminating massive computational redundancy
in frame-wise diffusion models. The more sensitive and fine-grained noises are
still acquired via later denoising steps, which can be essential to retain
visual qualities. As such, deciding which intermediate steps should switch from
motion-based propagations to denoising can be a crucial problem and a key
tradeoff between efficiency and quality. Dr. Mo employs a meta-network named
Denoising Step Selector (DSS) to dynamically determine desirable intermediate
steps across video frames. Extensive evaluations on video generation and
editing tasks have shown that Dr. Mo can substantially accelerate diffusion
models in video tasks with improved visual qualities.",2024-09-19 07:50:34+00:00,"['Chenyu Wang', 'Shuo Yan', 'Yixuan Chen', 'Yujiang Wang', 'Mingzhi Dong', 'Xiaochen Yang', 'Dongsheng Li', 'Robert P. Dick', 'Qin Lv', 'Fan Yang', 'Tun Lu', 'Ning Gu', 'Li Shang']",http://arxiv.org/abs/2409.12532v1
VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding,"Multimodal large language models (MLLMs) have recently shown significant
advancements in video understanding, excelling in content reasoning and
instruction-following tasks. However, the problem of hallucination, where
models generate inaccurate or misleading content, remains underexplored in the
video domain. Building on the observation that the visual encoder of MLLMs
often struggles to differentiate between video pairs that are visually distinct
but semantically similar, we introduce VidHalluc, the largest benchmark
designed to examine hallucinations in MLLMs for video understanding tasks.
VidHalluc assesses hallucinations across three critical dimensions: (1) action,
(2) temporal sequence, and (3) scene transition. VidHalluc consists of 5,002
videos, paired based on semantic similarity and visual differences, focusing on
cases where hallucinations are most likely to occur. Through comprehensive
testing, our experiments show that most MLLMs are vulnerable to hallucinations
across these dimensions. Furthermore, we propose DINO-HEAL, a training-free
method that reduces hallucinations by incorporating spatial saliency
information from DINOv2 to reweight visual features during inference. Our
results demonstrate that DINO-HEAL consistently improves performance on
VidHalluc, achieving an average improvement of 3.02% in mitigating
hallucinations among all tasks. Both the VidHalluc benchmark and DINO-HEAL code
can be accessed via $\href{https://vid-halluc.github.io/}{\text{this link}}$.",2024-12-04 22:03:19+00:00,"['Chaoyu Li', 'Eun Woo Im', 'Pooyan Fazli']",http://arxiv.org/abs/2412.03735v1
FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation,"Diffusion-based audio-driven talking avatar methods have recently gained
attention for their high-fidelity, vivid, and expressive results. However,
their slow inference speed limits practical applications. Despite the
development of various distillation techniques for diffusion models, we found
that naive diffusion distillation methods do not yield satisfactory results.
Distilled models exhibit reduced robustness with open-set input images and a
decreased correlation between audio and video compared to teacher models,
undermining the advantages of diffusion models. To address this, we propose
FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG
Distillation). We first designed a mixed-supervised loss to leverage data of
varying quality and enhance the overall model capability as well as robustness.
Additionally, we propose a multi-CFG distillation with learnable tokens to
utilize the correlation between audio and reference image conditions, reducing
the threefold inference runs caused by multi-CFG with acceptable quality
degradation. Extensive experiments across multiple datasets show that FADA
generates vivid videos comparable to recent diffusion model-based methods while
achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage
http://fadavatar.github.io.",2024-12-22 08:19:22+00:00,"['Tianyun Zhong', 'Chao Liang', 'Jianwen Jiang', 'Gaojie Lin', 'Jiaqi Yang', 'Zhou Zhao']",http://arxiv.org/abs/2412.16915v1
"Unified Editing of Panorama, 3D Scenes, and Videos Through Disentangled Self-Attention Injection","While text-to-image models have achieved impressive capabilities in image
generation and editing, their application across various modalities often
necessitates training separate models. Inspired by existing method of single
image editing with self attention injection and video editing with shared
attention, we propose a novel unified editing framework that combines the
strengths of both approaches by utilizing only a basic 2D image text-to-image
(T2I) diffusion model. Specifically, we design a sampling method that
facilitates editing consecutive images while maintaining semantic consistency
utilizing shared self-attention features during both reference and consecutive
image sampling processes. Experimental results confirm that our method enables
editing across diverse modalities including 3D scenes, videos, and panorama
images.",2024-05-27 04:44:36+00:00,"['Gihyun Kwon', 'Jangho Park', 'Jong Chul Ye']",http://arxiv.org/abs/2405.16823v1
Diffusion-Promoted HDR Video Reconstruction,"High dynamic range (HDR) video reconstruction aims to generate HDR videos
from low dynamic range (LDR) frames captured with alternating exposures. Most
existing works solely rely on the regression-based paradigm, leading to adverse
effects such as ghosting artifacts and missing details in saturated regions. In
this paper, we propose a diffusion-promoted method for HDR video
reconstruction, termed HDR-V-Diff, which incorporates a diffusion model to
capture the HDR distribution. As such, HDR-V-Diff can reconstruct HDR videos
with realistic details while alleviating ghosting artifacts. However, the
direct introduction of video diffusion models would impose massive
computational burden. Instead, to alleviate this burden, we first propose an
HDR Latent Diffusion Model (HDR-LDM) to learn the distribution prior of single
HDR frames. Specifically, HDR-LDM incorporates a tonemapping strategy to
compress HDR frames into the latent space and a novel exposure embedding to
aggregate the exposure information into the diffusion process. We then propose
a Temporal-Consistent Alignment Module (TCAM) to learn the temporal information
as a complement for HDR-LDM, which conducts coarse-to-fine feature alignment at
different scales among video frames. Finally, we design a Zero-Init
Cross-Attention (ZiCA) mechanism to effectively integrate the learned
distribution prior and temporal information for generating HDR frames.
Extensive experiments validate that HDR-V-Diff achieves state-of-the-art
results on several representative datasets.",2024-06-12 13:38:10+00:00,"['Yuanshen Guan', 'Ruikang Xu', 'Mingde Yao', 'Ruisheng Gao', 'Lizhi Wang', 'Zhiwei Xiong']",http://arxiv.org/abs/2406.08204v1
DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion,"Diffusion-based methods have achieved remarkable achievements in 2D image or
3D object generation, however, the generation of 3D scenes and even
$360^{\circ}$ images remains constrained, due to the limited number of scene
datasets, the complexity of 3D scenes themselves, and the difficulty of
generating consistent multi-view images. To address these issues, we first
establish a large-scale panoramic video-text dataset containing millions of
consecutive panoramic keyframes with corresponding panoramic depths, camera
poses, and text descriptions. Then, we propose a novel text-driven panoramic
generation framework, termed DiffPano, to achieve scalable, consistent, and
diverse panoramic scene generation. Specifically, benefiting from the powerful
generative capabilities of stable diffusion, we fine-tune a single-view
text-to-panorama diffusion model with LoRA on the established panoramic
video-text dataset. We further design a spherical epipolar-aware multi-view
diffusion model to ensure the multi-view consistency of the generated panoramic
images. Extensive experiments demonstrate that DiffPano can generate scalable,
consistent, and diverse panoramic images with given unseen text descriptions
and camera poses.",2024-10-31 17:57:02+00:00,"['Weicai Ye', 'Chenhao Ji', 'Zheng Chen', 'Junyao Gao', 'Xiaoshui Huang', 'Song-Hai Zhang', 'Wanli Ouyang', 'Tong He', 'Cairong Zhao', 'Guofeng Zhang']",http://arxiv.org/abs/2410.24203v1
NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF and Neural View Synthesis Methods,"Neural View Synthesis (NVS) has demonstrated efficacy in generating
high-fidelity dense viewpoint videos using a image set with sparse views.
However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not
tailored for the scenes with dense viewpoints synthesized by NVS and NeRF
variants, thus, they often fall short in capturing the perceptual quality,
including spatial and angular aspects of NVS-synthesized scenes. Furthermore,
the lack of dense ground truth views makes the full reference quality
assessment on NVS-synthesized scenes challenging. For instance, datasets such
as LLFF provide only sparse images, insufficient for complete full-reference
assessments. To address the issues above, we propose NeRF-NQA, the first
no-reference quality assessment method for densely-observed scenes synthesized
from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment
strategy, integrating both viewwise and pointwise approaches, to evaluate the
quality of NVS-generated scenes. The viewwise approach assesses the spatial
quality of each individual synthesized view and the overall inter-views
consistency, while the pointwise approach focuses on the angular qualities of
scene surface points and their compound inter-point quality. Extensive
evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality
assessment methods (from fields of image, video, and light-field assessment).
The results demonstrate NeRF-NQA outperforms the existing assessment methods
significantly and it shows substantial superiority on assessing NVS-synthesized
scenes without references. An implementation of this paper are available at
https://github.com/VincentQQu/NeRF-NQA.",2024-12-11 02:17:33+00:00,"['Qiang Qu', 'Hanxue Liang', 'Xiaoming Chen', 'Yuk Ying Chung', 'Yiran Shen']",http://arxiv.org/abs/2412.08029v1
Video Generation with Consistency Tuning,"Currently, various studies have been exploring generation of long videos.
However, the generated frames in these videos often exhibit jitter and noise.
Therefore, in order to generate the videos without these noise, we propose a
novel framework composed of four modules: separate tuning module, average
fusion module, combined tuning module, and inter-frame consistency module. By
applying our newly proposed modules subsequently, the consistency of the
background and foreground in each video frames is optimized. Besides, the
experimental results demonstrate that videos generated by our method exhibit a
high quality in comparison of the state-of-the-art methods.",2024-03-11 01:11:28+00:00,"['Chaoyi Wang', 'Yaozhe Song', 'Yafeng Zhang', 'Jun Pei', 'Lijie Xia', 'Jianpo Liu']",http://arxiv.org/abs/2403.06356v1
LTX-Video: Realtime Video Latent Diffusion,"We introduce LTX-Video, a transformer-based latent diffusion model that
adopts a holistic approach to video generation by seamlessly integrating the
responsibilities of the Video-VAE and the denoising transformer. Unlike
existing methods, which treat these components as independent, LTX-Video aims
to optimize their interaction for improved efficiency and quality. At its core
is a carefully designed Video-VAE that achieves a high compression ratio of
1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled
by relocating the patchifying operation from the transformer's input to the
VAE's input. Operating in this highly compressed latent space enables the
transformer to efficiently perform full spatiotemporal self-attention, which is
essential for generating high-resolution videos with temporal consistency.
However, the high compression inherently limits the representation of fine
details. To address this, our VAE decoder is tasked with both latent-to-pixel
conversion and the final denoising step, producing the clean result directly in
pixel space. This approach preserves the ability to generate fine details
without incurring the runtime cost of a separate upsampling module. Our model
supports diverse use cases, including text-to-video and image-to-video
generation, with both capabilities trained simultaneously. It achieves
faster-than-real-time generation, producing 5 seconds of 24 fps video at
768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all
existing models of similar scale. The source code and pre-trained models are
publicly available, setting a new benchmark for accessible and scalable video
generation.",2024-12-30 19:00:25+00:00,"['Yoav HaCohen', 'Nisan Chiprut', 'Benny Brazowski', 'Daniel Shalem', 'Dudu Moshe', 'Eitan Richardson', 'Eran Levin', 'Guy Shiran', 'Nir Zabari', 'Ori Gordon', 'Poriya Panet', 'Sapir Weissbuch', 'Victor Kulikov', 'Yaki Bitterman', 'Zeev Melumian', 'Ofir Bibi']",http://arxiv.org/abs/2501.00103v1
Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity,"Video-to-audio (V2A) generation leverages visual-only video features to
render plausible sounds that match the scene. Importantly, the generated sound
onsets should match the visual actions that are aligned with them, otherwise
unnatural synchronization artifacts arise. Recent works have explored the
progression of conditioning sound generators on still images and then video
features, focusing on quality and semantic matching while ignoring
synchronization, or by sacrificing some amount of quality to focus on improving
synchronization only. In this work, we propose a V2A generative model, named
MaskVAT, that interconnects a full-band high-quality general audio codec with a
sequence-to-sequence masked generative model. This combination allows modeling
both high audio quality, semantic matching, and temporal synchronicity at the
same time. Our results show that, by combining a high-quality codec with the
proper pre-trained audio-visual features and a sequence-to-sequence parallel
structure, we are able to yield highly synchronized results on one hand, whilst
being competitive with the state of the art of non-codec generative audio
models. Sample videos and generated audios are available at
https://maskvat.github.io .",2024-07-15 01:49:59+00:00,"['Santiago Pascual', 'Chunghsin Yeh', 'Ioannis Tsiamas', 'Joan Serr']",http://arxiv.org/abs/2407.10387v1
Adapting Image-to-Video Diffusion Models for Large-Motion Frame Interpolation,"With the development of video generation models has advanced significantly in
recent years, we adopt large-scale image-to-video diffusion models for video
frame interpolation. We present a conditional encoder designed to adapt an
image-to-video model for large-motion frame interpolation. To enhance
performance, we integrate a dual-branch feature extractor and propose a
cross-frame attention mechanism that effectively captures both spatial and
temporal information, enabling accurate interpolations of intermediate frames.
Our approach demonstrates superior performance on the Fr\'echet Video Distance
(FVD) metric when evaluated against other state-of-the-art approaches,
particularly in handling large motion scenarios, highlighting advancements in
generative-based methodologies.",2024-12-22 14:49:55+00:00,"['Luoxu Jin', 'Hiroshi Watanabe']",http://arxiv.org/abs/2412.17042v3
Speech2rtMRI: Speech-Guided Diffusion Model for Real-time MRI Video of the Vocal Tract during Speech,"Understanding speech production both visually and kinematically can inform
second language learning system designs, as well as the creation of speaking
characters in video games and animations. In this work, we introduce a
data-driven method to visually represent articulator motion in Magnetic
Resonance Imaging (MRI) videos of the human vocal tract during speech based on
arbitrary audio or speech input. We leverage large pre-trained speech models,
which are embedded with prior knowledge, to generalize the visual domain to
unseen data using a speech-to-video diffusion model. Our findings demonstrate
that the visual generation significantly benefits from the pre-trained speech
representations. We also observed that evaluating phonemes in isolation is
challenging but becomes more straightforward when assessed within the context
of spoken words. Limitations of the current results include the presence of
unsmooth tongue motion and video distortion when the tongue contacts the
palate.",2024-09-23 20:19:24+00:00,"['Hong Nguyen', 'Sean Foley', 'Kevin Huang', 'Xuan Shi', 'Tiantian Feng', 'Shrikanth Narayanan']",http://arxiv.org/abs/2409.15525v1
MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation,"The image-to-video (I2V) generation is conditioned on the static image, which
has been enhanced recently by the motion intensity as an additional control
signal. These motion-aware models are appealing to generate diverse motion
patterns, yet there lacks a reliable motion estimator for training such models
on large-scale video set in the wild. Traditional metrics, e.g., SSIM or
optical flow, are hard to generalize to arbitrary videos, while, it is very
tough for human annotators to label the abstract motion intensity neither.
Furthermore, the motion intensity shall reveal both local object motion and
global camera movement, which has not been studied before. This paper addresses
the challenge with a new motion estimator, capable of measuring the decoupled
motion intensities of objects and cameras in video. We leverage the contrastive
learning on randomly paired videos and distinguish the video with greater
motion intensity. Such a paradigm is friendly for annotation and easy to scale
up to achieve stable performance on motion estimation. We then present a new
I2V model, named MotionStone, developed with the decoupled motion estimator.
Experimental results demonstrate the stability of the proposed motion estimator
and the state-of-the-art performance of MotionStone on I2V generation. These
advantages warrant the decoupled motion estimator to serve as a general plug-in
enhancer for both data processing and video generation training.",2024-12-08 08:12:37+00:00,"['Shuwei Shi', 'Biao Gong', 'Xi Chen', 'Dandan Zheng', 'Shuai Tan', 'Zizheng Yang', 'Yuyuan Li', 'Jingwen He', 'Kecheng Zheng', 'Jingdong Chen', 'Ming Yang', 'Yinqiang Zheng']",http://arxiv.org/abs/2412.05848v1
Scaling Up Video Summarization Pretraining with Large Language Models,"Long-form video content constitutes a significant portion of internet
traffic, making automated video summarization an essential research problem.
However, existing video summarization datasets are notably limited in their
size, constraining the effectiveness of state-of-the-art methods for
generalization. Our work aims to overcome this limitation by capitalizing on
the abundance of long-form videos with dense speech-to-video alignment and the
remarkable capabilities of recent large language models (LLMs) in summarizing
long text. We introduce an automated and scalable pipeline for generating a
large-scale video summarization dataset using LLMs as Oracle summarizers. By
leveraging the generated dataset, we analyze the limitations of existing
approaches and propose a new video summarization model that effectively
addresses them. To facilitate further research in the field, our work also
presents a new benchmark dataset that contains 1200 long videos each with
high-quality summaries annotated by professionals. Extensive experiments
clearly indicate that our proposed approach sets a new state-of-the-art in
video summarization across several benchmarks.",2024-04-04 11:59:06+00:00,"['Dawit Mureja Argaw', 'Seunghyun Yoon', 'Fabian Caba Heilbron', 'Hanieh Deilamsalehy', 'Trung Bui', 'Zhaowen Wang', 'Franck Dernoncourt', 'Joon Son Chung']",http://arxiv.org/abs/2404.03398v1
WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image Synthesis,"Due to the three-dimensional nature of CT- or MR-scans, generative modeling
of medical images is a particularly challenging task. Existing approaches
mostly apply patch-wise, slice-wise, or cascaded generation techniques to fit
the high-dimensional data into the limited GPU memory. However, these
approaches may introduce artifacts and potentially restrict the model's
applicability for certain downstream tasks. This work presents WDM, a
wavelet-based medical image synthesis framework that applies a diffusion model
on wavelet decomposed images. The presented approach is a simple yet effective
way of scaling 3D diffusion models to high resolutions and can be trained on a
single \SI{40}{\giga\byte} GPU. Experimental results on BraTS and LIDC-IDRI
unconditional image generation at a resolution of $128 \times 128 \times 128$
demonstrate state-of-the-art image fidelity (FID) and sample diversity
(MS-SSIM) scores compared to recent GANs, Diffusion Models, and Latent
Diffusion Models. Our proposed method is the only one capable of generating
high-quality images at a resolution of $256 \times 256 \times 256$,
outperforming all comparing methods.",2024-02-29 11:11:05+00:00,"['Paul Friedrich', 'Julia Wolleb', 'Florentin Bieder', 'Alicia Durrer', 'Philippe C. Cattin']",http://arxiv.org/abs/2402.19043v2
VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models,"This paper presents a novel method for building scalable 3D generative models
utilizing pre-trained video diffusion models. The primary obstacle in
developing foundation 3D generative models is the limited availability of 3D
data. Unlike images, texts, or videos, 3D data are not readily accessible and
are difficult to acquire. This results in a significant disparity in scale
compared to the vast quantities of other types of data. To address this issue,
we propose using a video diffusion model, trained with extensive volumes of
text, images, and videos, as a knowledge source for 3D data. By unlocking its
multi-view generative capabilities through fine-tuning, we generate a
large-scale synthetic multi-view dataset to train a feed-forward 3D generative
model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view
data, can generate a 3D asset from a single image in seconds and achieves
superior performance when compared to current SOTA feed-forward 3D generative
models, with users preferring our results over 90% of the time.",2024-03-18 17:59:12+00:00,"['Junlin Han', 'Filippos Kokkinos', 'Philip Torr']",http://arxiv.org/abs/2403.12034v2
Dual-path Collaborative Generation Network for Emotional Video Captioning,"Emotional Video Captioning is an emerging task that aims to describe factual
content with the intrinsic emotions expressed in videos. The essential of the
EVC task is to effectively perceive subtle and ambiguous visual emotional cues
during the caption generation, which is neglected by the traditional video
captioning. Existing emotional video captioning methods perceive global visual
emotional cues at first, and then combine them with the video features to guide
the emotional caption generation, which neglects two characteristics of the EVC
task. Firstly, their methods neglect the dynamic subtle changes in the
intrinsic emotions of the video, which makes it difficult to meet the needs of
common scenes with diverse and changeable emotions. Secondly, as their methods
incorporate emotional cues into each step, the guidance role of emotion is
overemphasized, which makes factual content more or less ignored during
generation. To this end, we propose a dual-path collaborative generation
network, which dynamically perceives visual emotional cues evolutions while
generating emotional captions by collaborative learning. Specifically, in the
dynamic emotion perception path, we propose a dynamic emotion evolution module,
which first aggregates visual features and historical caption features to
summarize the global visual emotional cues, and then dynamically selects
emotional cues required to be re-composed at each stage. Besides, in the
adaptive caption generation path, to balance the description of factual content
and emotional cues, we propose an emotion adaptive decoder. Thus, our methods
can generate emotion-related words at the necessary time step, and our caption
generation balances the guidance of factual content and emotional cues well.
Extensive experiments on three challenging datasets demonstrate the superiority
of our approach and each proposed module.",2024-08-06 07:30:53+00:00,"['Cheng Ye', 'Weidong Chen', 'Jingyu Li', 'Lei Zhang', 'Zhendong Mao']",http://arxiv.org/abs/2408.03006v1
FlexCache: Flexible Approximate Cache System for Video Diffusion,"Text-to-Video applications receive increasing attention from the public.
Among these, diffusion models have emerged as the most prominent approach,
offering impressive quality in visual content generation. However, it still
suffers from substantial computational complexity, often requiring several
minutes to generate a single video. While prior research has addressed the
computational overhead in text-to-image diffusion models, the techniques
developed are not directly suitable for video diffusion models due to the
significantly larger cache requirements and enhanced computational demands
associated with video generation.
  We present FlexCache, a flexible approximate cache system that addresses the
challenges in two main designs. First, we compress the caches before saving
them to storage. Our compression strategy can reduce 6.7 times consumption on
average. Then we find that the approximate cache system can achieve higher hit
rate and computation savings by decoupling the object and background. We
further design a tailored cache replacement policy to support the two
techniques mentioned above better. Through our evaluation, FlexCache reaches
1.26 times higher throughput and 25% lower cost compared to the
state-of-the-art diffusion approximate cache system.",2024-12-18 00:35:16+00:00,"['Desen Sun', 'Henry Tian', 'Tim Lu', 'Sihang Liu']",http://arxiv.org/abs/2501.04012v1
CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training,"Benefiting from large-scale pre-training of text-video pairs, current
text-to-video (T2V) diffusion models can generate high-quality videos from the
text description. Besides, given some reference images or videos, the
parameter-efficient fine-tuning method, i.e. LoRA, can generate high-quality
customized concepts, e.g., the specific subject or the motions from a reference
video. However, combining the trained multiple concepts from different
references into a single network shows obvious artifacts. To this end, we
propose CustomTTT, where we can joint custom the appearance and the motion of
the given video easily. In detail, we first analyze the prompt influence in the
current video diffusion model and find the LoRAs are only needed for the
specific layers for appearance and motion customization. Besides, since each
LoRA is trained individually, we propose a novel test-time training technique
to update parameters after combination utilizing the trained customized models.
We conduct detailed experiments to verify the effectiveness of the proposed
methods. Our method outperforms several state-of-the-art works in both
qualitative and quantitative evaluations.",2024-12-20 08:05:13+00:00,"['Xiuli Bi', 'Jian Lu', 'Bo Liu', 'Xiaodong Cun', 'Yong Zhang', 'Weisheng Li', 'Bin Xiao']",http://arxiv.org/abs/2412.15646v2
TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation,"Despite significant advancements in customizing text-to-image and video
generation models, generating images and videos that effectively integrate
multiple personalized concepts remains a challenging task. To address this, we
present TweedieMix, a novel method for composing customized diffusion models
during the inference phase. By analyzing the properties of reverse diffusion
sampling, our approach divides the sampling process into two stages. During the
initial steps, we apply a multiple object-aware sampling technique to ensure
the inclusion of the desired target objects. In the later steps, we blend the
appearances of the custom concepts in the de-noised image space using Tweedie's
formula. Our results demonstrate that TweedieMix can generate multiple
personalized concepts with higher fidelity than existing methods. Moreover, our
framework can be effortlessly extended to image-to-video diffusion models,
enabling the generation of videos that feature multiple personalized concepts.
Results and source code are in our anonymous project page.",2024-10-08 01:06:01+00:00,"['Gihyun Kwon', 'Jong Chul Ye']",http://arxiv.org/abs/2410.05591v2
ShareGPT4Video: Improving Video Understanding and Generation with Better Captions,"We present the ShareGPT4Video series, aiming to facilitate the video
understanding of large video-language models (LVLMs) and the video generation
of text-to-video models (T2VMs) via dense and precise captions. The series
comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with
various lengths and sources, developed through carefully designed data
filtering and annotating strategy. 2) ShareCaptioner-Video, an efficient and
capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic
videos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that
reached SOTA performance on three advancing video benchmarks. To achieve this,
taking aside the non-scalable costly human annotators, we find using GPT4V to
caption video with a naive multi-frame or frame-concatenation input strategy
leads to less detailed and sometimes temporal-confused results. We argue the
challenge of designing a high-quality video captioning strategy lies in three
aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame
detailed content description. 3) Frame-number scalability for arbitrary-length
videos. To this end, we meticulously designed a differential video captioning
strategy, which is stable, scalable, and efficient for generating captions for
videos with arbitrary resolution, aspect ratios, and length. Based on it, we
construct ShareGPT4Video, which contains 40K high-quality videos spanning a
wide range of categories, and the resulting captions encompass rich world
knowledge, object attributes, camera movements, and crucially, detailed and
precise temporal descriptions of events. Based on ShareGPT4Video, we further
develop ShareCaptioner-Video, a superior captioner capable of efficiently
generating high-quality captions for arbitrary videos...",2024-06-06 17:58:54+00:00,"['Lin Chen', 'Xilin Wei', 'Jinsong Li', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Zehui Chen', 'Haodong Duan', 'Bin Lin', 'Zhenyu Tang', 'Li Yuan', 'Yu Qiao', 'Dahua Lin', 'Feng Zhao', 'Jiaqi Wang']",http://arxiv.org/abs/2406.04325v1
DiVE: DiT-based Video Generation with Enhanced Control,"Generating high-fidelity, temporally consistent videos in autonomous driving
scenarios faces a significant challenge, e.g. problematic maneuvers in corner
cases. Despite recent video generation works are proposed to tackcle the
mentioned problem, i.e. models built on top of Diffusion Transformers (DiT),
works are still missing which are targeted on exploring the potential for
multi-view videos generation scenarios. Noticeably, we propose the first
DiT-based framework specifically designed for generating temporally and
multi-view consistent videos which precisely match the given bird's-eye view
layouts control. Specifically, the proposed framework leverages a
parameter-free spatial view-inflated attention mechanism to guarantee the
cross-view consistency, where joint cross-attention modules and
ControlNet-Transformer are integrated to further improve the precision of
control. To demonstrate our advantages, we extensively investigate the
qualitative comparisons on nuScenes dataset, particularly in some most
challenging corner cases. In summary, the effectiveness of our proposed method
in producing long, controllable, and highly consistent videos under difficult
conditions is proven to be effective.",2024-09-03 04:29:59+00:00,"['Junpeng Jiang', 'Gangyi Hong', 'Lijun Zhou', 'Enhui Ma', 'Hengtong Hu', 'Xia Zhou', 'Jie Xiang', 'Fan Liu', 'Kaicheng Yu', 'Haiyang Sun', 'Kun Zhan', 'Peng Jia', 'Miao Zhang']",http://arxiv.org/abs/2409.01595v1
Using Diffusion Priors for Video Amodal Segmentation,"Object permanence in humans is a fundamental cue that helps in understanding
persistence of objects, even when they are fully occluded in the scene. Present
day methods in object segmentation do not account for this amodal nature of the
world, and only work for segmentation of visible or modal objects. Few amodal
methods exist; single-image segmentation methods cannot handle high-levels of
occlusions which are better inferred using temporal information, and
multi-frame methods have focused solely on segmenting rigid objects. To this
end, we propose to tackle video amodal segmentation by formulating it as a
conditional generation task, capitalizing on the foundational knowledge in
video generative models. Our method is simple; we repurpose these models to
condition on a sequence of modal mask frames of an object along with contextual
pseudo-depth maps, to learn which object boundary may be occluded and
therefore, extended to hallucinate the complete extent of an object. This is
followed by a content completion stage which is able to inpaint the occluded
regions of an object. We benchmark our approach alongside a wide array of
state-of-the-art methods on four datasets and show a dramatic improvement of
upto 13% for amodal segmentation in an object's occluded region.",2024-12-05 21:30:40+00:00,"['Kaihua Chen', 'Deva Ramanan', 'Tarasha Khurana']",http://arxiv.org/abs/2412.04623v1
ODVista: An Omnidirectional Video Dataset for super-resolution and Quality Enhancement Tasks,"Omnidirectional or 360-degree video is being increasingly deployed, largely
due to the latest advancements in immersive virtual reality (VR) and extended
reality (XR) technology. However, the adoption of these videos in streaming
encounters challenges related to bandwidth and latency, particularly in
mobility conditions such as with unmanned aerial vehicles (UAVs). Adaptive
resolution and compression aim to preserve quality while maintaining low
latency under these constraints, yet downscaling and encoding can still degrade
quality and introduce artifacts. Machine learning (ML)-based super-resolution
(SR) and quality enhancement techniques offer a promising solution by enhancing
detail recovery and reducing compression artifacts. However, current publicly
available 360-degree video SR datasets lack compression artifacts, which limit
research in this field. To bridge this gap, this paper introduces
omnidirectional video streaming dataset (ODVista), which comprises 200
high-resolution and high quality videos downscaled and encoded at four bitrate
ranges using the high-efficiency video coding (HEVC)/H.265 standard.
Evaluations show that the dataset not only features a wide variety of scenes
but also spans different levels of content complexity, which is crucial for
robust solutions that perform well in real-world scenarios and generalize
across diverse visual environments. Additionally, we evaluate the performance,
considering both quality enhancement and runtime, of two handcrafted and two
ML-based SR models on the validation and testing sets of ODVista.",2024-03-01 15:30:04+00:00,"['Ahmed Telili', 'Ibrahim Farhat', 'Wassim Hamidouche', 'Hadi Amirpour']",http://arxiv.org/abs/2403.00604v2
Hawk: Learning to Understand Open-World Video Anomalies,"Video Anomaly Detection (VAD) systems can autonomously monitor and identify
disturbances, reducing the need for manual labor and associated costs. However,
current VAD systems are often limited by their superficial semantic
understanding of scenes and minimal user interaction. Additionally, the
prevalent data scarcity in existing datasets restricts their applicability in
open-world scenarios. In this paper, we introduce Hawk, a novel framework that
leverages interactive large Visual Language Models (VLM) to interpret video
anomalies precisely. Recognizing the difference in motion information between
abnormal and normal videos, Hawk explicitly integrates motion modality to
enhance anomaly identification. To reinforce motion attention, we construct an
auxiliary consistency loss within the motion and video space, guiding the video
branch to focus on the motion modality. Moreover, to improve the interpretation
of motion-to-language, we establish a clear supervisory relationship between
motion and its linguistic representation. Furthermore, we have annotated over
8,000 anomaly videos with language descriptions, enabling effective training
across diverse open-world scenarios, and also created 8,000 question-answering
pairs for users' open-world questions. The final results demonstrate that Hawk
achieves SOTA performance, surpassing existing baselines in both video
description generation and question-answering. Our codes/dataset/demo will be
released at https://github.com/jqtangust/hawk.",2024-05-27 07:08:58+00:00,"['Jiaqi Tang', 'Hao Lu', 'Ruizheng Wu', 'Xiaogang Xu', 'Ke Ma', 'Cheng Fang', 'Bin Guo', 'Jiangbo Lu', 'Qifeng Chen', 'Ying-Cong Chen']",http://arxiv.org/abs/2405.16886v1
DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation,"World models have demonstrated superiority in autonomous driving,
particularly in the generation of multi-view driving videos. However,
significant challenges still exist in generating customized driving videos. In
this paper, we propose DriveDreamer-2, which builds upon the framework of
DriveDreamer and incorporates a Large Language Model (LLM) to generate
user-defined driving videos. Specifically, an LLM interface is initially
incorporated to convert a user's query into agent trajectories. Subsequently, a
HDMap, adhering to traffic regulations, is generated based on the trajectories.
Ultimately, we propose the Unified Multi-View Model to enhance temporal and
spatial coherence in the generated driving videos. DriveDreamer-2 is the first
world model to generate customized driving videos, it can generate uncommon
driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.
Besides, experimental results demonstrate that the generated videos enhance the
training of driving perception methods (e.g., 3D detection and tracking).
Furthermore, video generation quality of DriveDreamer-2 surpasses other
state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,
representing relative improvements of 30% and 50%.",2024-03-11 16:03:35+00:00,"['Guosheng Zhao', 'Xiaofeng Wang', 'Zheng Zhu', 'Xinze Chen', 'Guan Huang', 'Xiaoyi Bao', 'Xingang Wang']",http://arxiv.org/abs/2403.06845v2
ViMo: Generating Motions from Casual Videos,"Although humans have the innate ability to imagine multiple possible actions
from videos, it remains an extraordinary challenge for computers due to the
intricate camera movements and montages. Most existing motion generation
methods predominantly rely on manually collected motion datasets, usually
tediously sourced from motion capture (Mocap) systems or Multi-View cameras,
unavoidably resulting in a limited size that severely undermines their
generalizability. Inspired by recent advance of diffusion models, we probe a
simple and effective way to capture motions from videos and propose a novel
Video-to-Motion-Generation framework (ViMo) which could leverage the immense
trove of untapped video content to produce abundant and diverse 3D human
motions. Distinct from prior work, our videos could be more causal, including
complicated camera movements and occlusions. Striking experimental results
demonstrate the proposed model could generate natural motions even for videos
where rapid movements, varying perspectives, or frequent occlusions might
exist. We also show this work could enable three important downstream
applications, such as generating dancing motions according to arbitrary music
and source video style. Extensive experimental results prove that our model
offers an effective and scalable way to generate diversity and realistic
motions. Code and demos will be public soon.",2024-08-13 03:57:35+00:00,"['Liangdong Qiu', 'Chengxing Yu', 'Yanran Li', 'Zhao Wang', 'Haibin Huang', 'Chongyang Ma', 'Di Zhang', 'Pengfei Wan', 'Xiaoguang Han']",http://arxiv.org/abs/2408.06614v1
Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation,"This paper explores higher-resolution video outpainting with extensive
content generation. We point out common issues faced by existing methods when
attempting to largely outpaint videos: the generation of low-quality content
and limitations imposed by GPU memory. To address these challenges, we propose
a diffusion-based method called \textit{Follow-Your-Canvas}. It builds upon two
core designs. First, instead of employing the common practice of ""single-shot""
outpainting, we distribute the task across spatial windows and seamlessly merge
them. It allows us to outpaint videos of any size and resolution without being
constrained by GPU memory. Second, the source video and its relative positional
relation are injected into the generation process of each window. It makes the
generated spatial layout within each window harmonize with the source video.
Coupling with these two designs enables us to generate higher-resolution
outpainting videos with rich content while keeping spatial and temporal
consistency. Follow-Your-Canvas excels in large-scale video outpainting, e.g.,
from 512X512 to 1152X2048 (9X), while producing high-quality and aesthetically
pleasing results. It achieves the best quantitative results across various
resolution and scale setups. The code is released on
https://github.com/mayuelala/FollowYourCanvas",2024-09-02 08:28:57+00:00,"['Qihua Chen', 'Yue Ma', 'Hongfa Wang', 'Junkun Yuan', 'Wenzhe Zhao', 'Qi Tian', 'Hongmei Wang', 'Shaobo Min', 'Qifeng Chen', 'Wei Liu']",http://arxiv.org/abs/2409.01055v1
LoopAnimate: Loopable Salient Object Animation,"Research on diffusion model-based video generation has advanced rapidly.
However, limitations in object fidelity and generation length hinder its
practical applications. Additionally, specific domains like animated wallpapers
require seamless looping, where the first and last frames of the video match
seamlessly. To address these challenges, this paper proposes LoopAnimate, a
novel method for generating videos with consistent start and end frames. To
enhance object fidelity, we introduce a framework that decouples multi-level
image appearance and textual semantic information. Building upon an
image-to-image diffusion model, our approach incorporates both pixel-level and
feature-level information from the input image, injecting image appearance and
textual semantic embeddings at different positions of the diffusion model.
Existing UNet-based video generation models require to input the entire videos
during training to encode temporal and positional information at once. However,
due to limitations in GPU memory, the number of frames is typically restricted
to 16. To address this, this paper proposes a three-stage training strategy
with progressively increasing frame numbers and reducing fine-tuning modules.
Additionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend
the capacity for encoding temporal and positional information up to 36 frames.
The proposed LoopAnimate, which for the first time extends the single-pass
generation length of UNet-based video generation models to 35 frames while
maintaining high-quality video generation. Experiments demonstrate that
LoopAnimate achieves state-of-the-art performance in both objective metrics,
such as fidelity and temporal consistency, and subjective evaluation results.",2024-04-14 07:36:18+00:00,"['Fanyi Wang', 'Peng Liu', 'Haotian Hu', 'Dan Meng', 'Jingwen Su', 'Jinjin Xu', 'Yanhao Zhang', 'Xiaoming Ren', 'Zhiwang Zhang']",http://arxiv.org/abs/2404.09172v2
Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided Mixture-of-Experts,"This paper introduces Stereo-Talker, a novel one-shot audio-driven human
video synthesis system that generates 3D talking videos with precise lip
synchronization, expressive body gestures, temporally consistent
photo-realistic quality, and continuous viewpoint control. The process follows
a two-stage approach. In the first stage, the system maps audio input to
high-fidelity motion sequences, encompassing upper-body gestures and facial
expressions. To enrich motion diversity and authenticity, large language model
(LLM) priors are integrated with text-aligned semantic audio features,
leveraging LLMs' cross-modal generalization power to enhance motion quality. In
the second stage, we improve diffusion-based video generation models by
incorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided
MoE focuses on view-specific attributes, while a mask-guided MoE enhances
region-based rendering stability. Additionally, a mask prediction module is
devised to derive human masks from motion data, enhancing the stability and
accuracy of masks and enabling mask guiding during inference. We also introduce
a comprehensive human video dataset with 2,203 identities, covering diverse
body gestures and detailed annotations, facilitating broad generalization. The
code, data, and pre-trained models will be released for research purposes.",2024-10-31 11:32:33+00:00,"['Xiang Deng', 'Youxin Pang', 'Xiaochen Zhao', 'Chao Xu', 'Lizhen Wang', 'Hongjiang Xiao', 'Shi Yan', 'Hongwen Zhang', 'Yebin Liu']",http://arxiv.org/abs/2410.23836v1
FineVQ: Fine-Grained User Generated Content Video Quality Assessment,"The rapid growth of user-generated content (UGC) videos has produced an
urgent need for effective video quality assessment (VQA) algorithms to monitor
video quality and guide optimization and recommendation procedures. However,
current VQA models generally only give an overall rating for a UGC video, which
lacks fine-grained labels for serving video processing and recommendation
applications. To address the challenges and promote the development of UGC
videos, we establish the first large-scale Fine-grained Video quality
assessment Database, termed FineVD, which comprises 6104 UGC videos with
fine-grained quality scores and descriptions across multiple dimensions. Based
on this database, we propose a Fine-grained Video Quality assessment (FineVQ)
model to learn the fine-grained quality of UGC videos, with the capabilities of
quality rating, quality scoring, and quality attribution. Extensive
experimental results demonstrate that our proposed FineVQ can produce
fine-grained video-quality results and achieve state-of-the-art performance on
FineVD and other commonly used UGC-VQA datasets. Both Both FineVD and FineVQ
will be made publicly available.",2024-12-26 14:44:47+00:00,"['Huiyu Duan', 'Qiang Hu', 'Jiarui Wang', 'Liu Yang', 'Zitong Xu', 'Lu Liu', 'Xiongkuo Min', 'Chunlei Cai', 'Tianxiao Ye', 'Xiaoyun Zhang', 'Guangtao Zhai']",http://arxiv.org/abs/2412.19238v1
Step Differences in Instructional Video,"Comparing a user video to a reference how-to video is a key requirement for
AR/VR technology delivering personalized assistance tailored to the user's
progress. However, current approaches for language-based assistance can only
answer questions about a single video. We propose an approach that first
automatically generates large amounts of visual instruction tuning data
involving pairs of videos from HowTo100M by leveraging existing step
annotations and accompanying narrations, and then trains a video-conditioned
language model to jointly reason across multiple raw videos. Our model achieves
state-of-the-art performance at identifying differences between video pairs and
ranking videos based on the severity of these differences, and shows promising
ability to perform general reasoning over multiple videos. Project page:
https://github.com/facebookresearch/stepdiff",2024-04-24 21:49:59+00:00,"['Tushar Nagarajan', 'Lorenzo Torresani']",http://arxiv.org/abs/2404.16222v2
NAVERO: Unlocking Fine-Grained Semantics for Video-Language Compositionality,"We study the capability of Video-Language (VidL) models in understanding
compositions between objects, attributes, actions and their relations.
Composition understanding becomes particularly challenging for video data since
the compositional relations rapidly change over time in videos. We first build
a benchmark named AARO to evaluate composition understanding related to actions
on top of spatial concepts. The benchmark is constructed by generating negative
texts with incorrect action descriptions for a given video and the model is
expected to pair a positive text with its corresponding video. Furthermore, we
propose a training method called NAVERO which utilizes video-text data
augmented with negative texts to enhance composition understanding. We also
develop a negative-augmented visual-language matching loss which is used
explicitly to benefit from the generated negative text. We compare NAVERO with
other state-of-the-art methods in terms of compositional understanding as well
as video-text retrieval performance. NAVERO achieves significant improvement
over other methods for both video-language and image-language composition
understanding, while maintaining strong performance on traditional text-video
retrieval tasks.",2024-08-18 15:27:06+00:00,"['Chaofan Tao', 'Gukyeong Kwon', 'Varad Gunjal', 'Hao Yang', 'Zhaowei Cai', 'Yonatan Dukler', 'Ashwin Swaminathan', 'R. Manmatha', 'Colin Jon Taylor', 'Stefano Soatto']",http://arxiv.org/abs/2408.09511v1
EdgeRelight360: Text-Conditioned 360-Degree HDR Image Generation for Real-Time On-Device Video Portrait Relighting,"In this paper, we present EdgeRelight360, an approach for real-time video
portrait relighting on mobile devices, utilizing text-conditioned generation of
360-degree high dynamic range image (HDRI) maps. Our method proposes a
diffusion-based text-to-360-degree image generation in the HDR domain, taking
advantage of the HDR10 standard. This technique facilitates the generation of
high-quality, realistic lighting conditions from textual descriptions, offering
flexibility and control in portrait video relighting task. Unlike the previous
relighting frameworks, our proposed system performs video relighting directly
on-device, enabling real-time inference with real 360-degree HDRI maps. This
on-device processing ensures both privacy and guarantees low runtime, providing
an immediate response to changes in lighting conditions or user inputs. Our
approach paves the way for new possibilities in real-time video applications,
including video conferencing, gaming, and augmented reality, by allowing
dynamic, text-based control of lighting conditions.",2024-04-15 16:45:08+00:00,"['Min-Hui Lin', 'Mahesh Reddy', 'Guillaume Berger', 'Michel Sarkis', 'Fatih Porikli', 'Ning Bi']",http://arxiv.org/abs/2404.09918v1
TrackGo: A Flexible and Efficient Method for Controllable Video Generation,"Recent years have seen substantial progress in diffusion-based controllable
video generation. However, achieving precise control in complex scenarios,
including fine-grained object parts, sophisticated motion trajectories, and
coherent background movement, remains a challenge. In this paper, we introduce
TrackGo, a novel approach that leverages free-form masks and arrows for
conditional video generation. This method offers users with a flexible and
precise mechanism for manipulating video content. We also propose the
TrackAdapter for control implementation, an efficient and lightweight adapter
designed to be seamlessly integrated into the temporal self-attention layers of
a pretrained video generation model. This design leverages our observation that
the attention map of these layers can accurately activate regions corresponding
to motion in videos. Our experimental results demonstrate that our new
approach, enhanced by the TrackAdapter, achieves state-of-the-art performance
on key metrics such as FVD, FID, and ObjMC scores.",2024-08-21 09:42:04+00:00,"['Haitao Zhou', 'Chuang Wang', 'Rui Nie', 'Jinlin Liu', 'Dongdong Yu', 'Qian Yu', 'Changhu Wang']",http://arxiv.org/abs/2408.11475v3
Artificial Intelligence for Biomedical Video Generation,"As a prominent subfield of Artificial Intelligence Generated Content (AIGC),
video generation has achieved notable advancements in recent years. The
introduction of Sora-alike models represents a pivotal breakthrough in video
generation technologies, significantly enhancing the quality of synthesized
videos. Particularly in the realm of biomedicine, video generation technology
has shown immense potential such as medical concept explanation, disease
simulation, and biomedical data augmentation. In this article, we thoroughly
examine the latest developments in video generation models and explore their
applications, challenges, and future opportunities in the biomedical sector. We
have conducted an extensive review and compiled a comprehensive list of
datasets from various sources to facilitate the development and evaluation of
video generative models in biomedicine. Given the rapid progress in this field,
we have also created a github repository to regularly update the advances of
biomedical video generation at:
https://github.com/Lee728243228/Biomedical-Video-Generation",2024-11-12 08:05:58+00:00,"['Linyuan Li', 'Jianing Qiu', 'Anujit Saha', 'Lin Li', 'Poyuan Li', 'Mengxian He', 'Ziyu Guo', 'Wu Yuan']",http://arxiv.org/abs/2411.07619v1
InTraGen: Trajectory-controlled Video Generation for Object Interactions,"Advances in video generation have significantly improved the realism and
quality of created scenes. This has fueled interest in developing intuitive
tools that let users leverage video generation as world simulators.
Text-to-video (T2V) generation is one such approach, enabling video creation
from text descriptions only. Yet, due to the inherent ambiguity in texts and
the limited temporal information offered by text prompts, researchers have
explored additional control signals like trajectory-guided systems, for more
accurate T2V generation. Nonetheless, methods to evaluate whether T2V models
can generate realistic interactions between multiple objects are lacking. We
introduce InTraGen, a pipeline for improved trajectory-based generation of
object interaction scenarios. We propose 4 new datasets and a novel trajectory
quality metric to evaluate the performance of the proposed InTraGen. To achieve
object interaction, we introduce a multi-modal interaction encoding pipeline
with an object ID injection mechanism that enriches object-environment
interactions. Our results demonstrate improvements in both visual fidelity and
quantitative performance. Code and datasets are available at
https://github.com/insait-institute/InTraGen",2024-11-25 14:27:50+00:00,"['Zuhao Liu', 'Aleksandar Yanev', 'Ahmad Mahmood', 'Ivan Nikolov', 'Saman Motamed', 'Wei-Shi Zheng', 'Xi Wang', 'Luc Van Gool', 'Danda Pani Paudel']",http://arxiv.org/abs/2411.16804v1
Video Diffusion Models are Training-free Motion Interpreter and Controller,"Video generation primarily aims to model authentic and customized motion
across frames, making understanding and controlling the motion a crucial topic.
Most diffusion-based studies on video motion focus on motion customization with
training-based paradigms, which, however, demands substantial training
resources and necessitates retraining for diverse models. Crucially, these
approaches do not explore how video diffusion models encode cross-frame motion
information in their features, lacking interpretability and transparency in
their effectiveness. To answer this question, this paper introduces a novel
perspective to understand, localize, and manipulate motion-aware features in
video diffusion models. Through analysis using Principal Component Analysis
(PCA), our work discloses that robust motion-aware feature already exists in
video diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating
content correlation information and filtering motion channels. MOFT provides a
distinct set of benefits, including the ability to encode comprehensive motion
information with clear interpretability, extraction without the need for
training, and generalizability across diverse architectures. Leveraging MOFT,
we propose a novel training-free video motion control framework. Our method
demonstrates competitive performance in generating natural and faithful motion,
providing architecture-agnostic insights and applicability in a variety of
downstream tasks.",2024-05-23 17:59:40+00:00,"['Zeqi Xiao', 'Yifan Zhou', 'Shuai Yang', 'Xingang Pan']",http://arxiv.org/abs/2405.14864v3
Kinetic Typography Diffusion Model,"This paper introduces a method for realistic kinetic typography that
generates user-preferred animatable 'text content'. We draw on recent advances
in guided video diffusion models to achieve visually-pleasing text appearances.
To do this, we first construct a kinetic typography dataset, comprising about
600K videos. Our dataset is made from a variety of combinations in 584
templates designed by professional motion graphics designers and involves
changing each letter's position, glyph, and size (i.e., flying, glitches,
chromatic aberration, reflecting effects, etc.). Next, we propose a video
diffusion model for kinetic typography. For this, there are three requirements:
aesthetic appearances, motion effects, and readable letters. This paper
identifies the requirements. For this, we present static and dynamic captions
used as spatial and temporal guidance of a video diffusion model, respectively.
The static caption describes the overall appearance of the video, such as
colors, texture and glyph which represent a shape of each letter. The dynamic
caption accounts for the movements of letters and backgrounds. We add one more
guidance with zero convolution to determine which text content should be
visible in the video. We apply the zero convolution to the text content, and
impose it on the diffusion model. Lastly, our glyph loss, only minimizing a
difference between the predicted word and its ground-truth, is proposed to make
the prediction letters readable. Experiments show that our model generates
kinetic typography videos with legible and artistic letter motions based on
text prompts.",2024-07-15 07:04:45+00:00,"['Seonmi Park', 'Inhwan Bae', 'Seunghyun Shin', 'Hae-Gon Jeon']",http://arxiv.org/abs/2407.10476v1
SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution,"Diffusion-based Video Super-Resolution (VSR) is renowned for generating
perceptually realistic videos, yet it grapples with maintaining detail
consistency across frames due to stochastic fluctuations. The traditional
approach of pixel-level alignment is ineffective for diffusion-processed frames
because of iterative disruptions. To overcome this, we introduce SeeClear--a
novel VSR framework leveraging conditional video generation, orchestrated by
instance-centric and channel-wise semantic controls. This framework integrates
a Semantic Distiller and a Pixel Condenser, which synergize to extract and
upscale semantic details from low-resolution frames. The Instance-Centric
Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate
pixels within and across frames, enhancing coherency. Additionally, the
Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge,
capitalizing on long-standing semantic textures. Our method also innovates the
blurring diffusion process with the ResShift mechanism, finely balancing
between sharpness and diffusion effects. Comprehensive experiments confirm our
framework's advantage over state-of-the-art diffusion-based VSR techniques. The
code is available: https://github.com/Tang1705/SeeClear-NeurIPS24.",2024-10-08 08:33:47+00:00,"['Qi Tang', 'Yao Zhao', 'Meiqin Liu', 'Chao Yao']",http://arxiv.org/abs/2410.05799v4
A Reinforcement Learning-Based Automatic Video Editing Method Using Pre-trained Vision-Language Model,"In this era of videos, automatic video editing techniques attract more and
more attention from industry and academia since they can reduce workloads and
lower the requirements for human editors. Existing automatic editing systems
are mainly scene- or event-specific, e.g., soccer game broadcasting, yet the
automatic systems for general editing, e.g., movie or vlog editing which covers
various scenes and events, were rarely studied before, and converting the
event-driven editing method to a general scene is nontrivial. In this paper, we
propose a two-stage scheme for general editing. Firstly, unlike previous works
that extract scene-specific features, we leverage the pre-trained
Vision-Language Model (VLM) to extract the editing-relevant representations as
editing context. Moreover, to close the gap between the professional-looking
videos and the automatic productions generated with simple guidelines, we
propose a Reinforcement Learning (RL)-based editing framework to formulate the
editing problem and train the virtual editor to make better sequential editing
decisions. Finally, we evaluate the proposed method on a more general editing
task with a real movie dataset. Experimental results demonstrate the
effectiveness and benefits of the proposed context representation and the
learning ability of our RL-based editing framework.",2024-11-07 18:20:28+00:00,"['Panwen Hu', 'Nan Xiao', 'Feifei Li', 'Yongquan Chen', 'Rui Huang']",http://arxiv.org/abs/2411.04942v1
MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation,"Conventional GAN-based models for talking head generation often suffer from
limited quality and unstable training. Recent approaches based on diffusion
models aimed to address these limitations and improve fidelity. However, they
still face challenges, including extensive sampling times and difficulties in
maintaining temporal consistency due to the high stochasticity of diffusion
models. To overcome these challenges, we propose a novel motion-disentangled
diffusion model for high-quality talking head generation, dubbed MoDiTalker. We
introduce the two modules: audio-to-motion (AToM), designed to generate a
synchronized lip motion from audio, and motion-to-video (MToV), designed to
produce high-quality head video following the generated motion. AToM excels in
capturing subtle lip movements by leveraging an audio attention mechanism. In
addition, MToV enhances temporal consistency by leveraging an efficient
tri-plane representation. Our experiments conducted on standard benchmarks
demonstrate that our model achieves superior performance compared to existing
models. We also provide comprehensive ablation studies and user study results.",2024-03-28 04:35:42+00:00,"['Seyeon Kim', 'Siyoon Jin', 'Jihye Park', 'Kihong Kim', 'Jiyoung Kim', 'Jisu Nam', 'Seungryong Kim']",http://arxiv.org/abs/2403.19144v1
Context-aware Video Anomaly Detection in Long-Term Datasets,"Video anomaly detection research is generally evaluated on short, isolated
benchmark videos only a few minutes long. However, in real-world environments,
security cameras observe the same scene for months or years at a time, and the
notion of anomalous behavior critically depends on context, such as the time of
day, day of week, or schedule of events. Here, we propose a context-aware video
anomaly detection algorithm, Trinity, specifically targeted to these scenarios.
Trinity is especially well-suited to crowded scenes in which individuals cannot
be easily tracked, and anomalies are due to speed, direction, or absence of
group motion. Trinity is a contrastive learning framework that aims to learn
alignments between context, appearance, and motion, and uses alignment quality
to classify videos as normal or anomalous. We evaluate our algorithm on both
conventional benchmarks and a public webcam-based dataset we collected that
spans more than three months of activity.",2024-04-11 16:17:36+00:00,"['Zhengye Yang', 'Richard Radke']",http://arxiv.org/abs/2404.07887v1
GenAD: Generalized Predictive Model for Autonomous Driving,"In this paper, we introduce the first large-scale video prediction model in
the autonomous driving discipline. To eliminate the restriction of high-cost
data collection and empower the generalization ability of our model, we acquire
massive data from the web and pair it with diverse and high-quality text
descriptions. The resultant dataset accumulates over 2000 hours of driving
videos, spanning areas all over the world with diverse weather conditions and
traffic scenarios. Inheriting the merits from recent latent diffusion models,
our model, dubbed GenAD, handles the challenging dynamics in driving scenes
with novel temporal reasoning blocks. We showcase that it can generalize to
various unseen driving datasets in a zero-shot manner, surpassing general or
driving-specific video prediction counterparts. Furthermore, GenAD can be
adapted into an action-conditioned prediction model or a motion planner,
holding great potential for real-world driving applications.",2024-03-14 17:58:33+00:00,"['Jiazhi Yang', 'Shenyuan Gao', 'Yihang Qiu', 'Li Chen', 'Tianyu Li', 'Bo Dai', 'Kashyap Chitta', 'Penghao Wu', 'Jia Zeng', 'Ping Luo', 'Jun Zhang', 'Andreas Geiger', 'Yu Qiao', 'Hongyang Li']",http://arxiv.org/abs/2403.09630v2
Generative Expansion of Small Datasets: An Expansive Graph Approach,"Limited data availability in machine learning significantly impacts
performance and generalization. Traditional augmentation methods enhance
moderately sufficient datasets. GANs struggle with convergence when generating
diverse samples. Diffusion models, while effective, have high computational
costs. We introduce an Expansive Synthesis model generating large-scale,
information-rich datasets from minimal samples. It uses expander graph mappings
and feature interpolation to preserve data distribution and feature
relationships. The model leverages neural networks' non-linear latent space,
captured by a Koopman operator, to create a linear feature space for dataset
expansion. An autoencoder with self-attention layers and optimal transport
refines distributional consistency. We validate by comparing classifiers
trained on generated data to those trained on original datasets. Results show
comparable performance, demonstrating the model's potential to augment training
data effectively. This work advances data generation, addressing scarcity in
machine learning applications.",2024-06-25 02:59:02+00:00,"['Vahid Jebraeeli', 'Bo Jiang', 'Hamid Krim', 'Derya Cansever']",http://arxiv.org/abs/2406.17238v2
Animate3D: Animating Any 3D Model with Multi-view Video Diffusion,"Recent advances in 4D generation mainly focus on generating 4D content by
distilling pre-trained text or single-view image-conditioned models. It is
inconvenient for them to take advantage of various off-the-shelf 3D assets with
multi-view attributes, and their results suffer from spatiotemporal
inconsistency owing to the inherent ambiguity in the supervision signals. In
this work, we present Animate3D, a novel framework for animating any static 3D
model. The core idea is two-fold: 1) We propose a novel multi-view video
diffusion model (MV-VDM) conditioned on multi-view renderings of the static 3D
object, which is trained on our presented large-scale multi-view video dataset
(MV-Video). 2) Based on MV-VDM, we introduce a framework combining
reconstruction and 4D Score Distillation Sampling (4D-SDS) to leverage the
multi-view video diffusion priors for animating 3D objects. Specifically, for
MV-VDM, we design a new spatiotemporal attention module to enhance spatial and
temporal consistency by integrating 3D and video diffusion models.
Additionally, we leverage the static 3D model's multi-view renderings as
conditions to preserve its identity. For animating 3D models, an effective
two-stage pipeline is proposed: we first reconstruct motions directly from
generated multi-view videos, followed by the introduced 4D-SDS to refine both
appearance and motion. Benefiting from accurate motion learning, we could
achieve straightforward mesh animation. Qualitative and quantitative
experiments demonstrate that Animate3D significantly outperforms previous
approaches. Data, code, and models will be open-released.",2024-07-16 05:35:57+00:00,"['Yanqin Jiang', 'Chaohui Yu', 'Chenjie Cao', 'Fan Wang', 'Weiming Hu', 'Jin Gao']",http://arxiv.org/abs/2407.11398v2
NU-Class Net: A Novel Approach for Video Quality Enhancement,"Video content has experienced a surge in popularity, asserting its dominance
over internet traffic and Internet of Things (IoT) networks. Video compression
has long been regarded as the primary means of efficiently managing the
substantial multimedia traffic generated by video-capturing devices.
Nevertheless, video compression algorithms entail significant computational
demands in order to achieve substantial compression ratios. This complexity
presents a formidable challenge when implementing efficient video coding
standards in resource-constrained embedded systems, such as IoT edge node
cameras. To tackle this challenge, this paper introduces NU-Class Net, an
innovative deep-learning model designed to mitigate compression artifacts
stemming from lossy compression codecs. This enhancement significantly elevates
the perceptible quality of low-bit-rate videos. By employing the NU-Class Net,
the video encoder within the video-capturing node can reduce output quality,
thereby generating low-bit-rate videos and effectively curtailing both
computation and bandwidth requirements at the edge. On the decoder side, which
is typically less encumbered by resource limitations, NU-Class Net is applied
after the video decoder to compensate for artifacts and approximate the quality
of the original video. Experimental results affirm the efficacy of the proposed
model in enhancing the perceptible quality of videos, especially those streamed
at low bit rates.",2024-01-02 11:46:42+00:00,"['Parham Zilouchian Moghaddam', 'Mehdi Modarressi', 'Mohammad Amin Sadeghi']",http://arxiv.org/abs/2401.01163v3
VANE-Bench: Video Anomaly Evaluation Benchmark for Conversational LMMs,"The recent developments in Large Multi-modal Video Models (Video-LMMs) have
significantly enhanced our ability to interpret and analyze video data. Despite
their impressive capabilities, current Video-LMMs have not been evaluated for
anomaly detection tasks, which is critical to their deployment in practical
scenarios e.g., towards identifying deepfakes, manipulated video content,
traffic accidents and crimes. In this paper, we introduce VANE-Bench, a
benchmark designed to assess the proficiency of Video-LMMs in detecting and
localizing anomalies and inconsistencies in videos. Our dataset comprises an
array of videos synthetically generated using existing state-of-the-art
text-to-video generation models, encompassing a variety of subtle anomalies and
inconsistencies grouped into five categories: unnatural transformations,
unnatural appearance, pass-through, disappearance and sudden appearance.
Additionally, our benchmark features real-world samples from existing anomaly
detection datasets, focusing on crime-related irregularities, atypical
pedestrian behavior, and unusual events. The task is structured as a visual
question-answering challenge to gauge the models' ability to accurately detect
and localize the anomalies within the videos. We evaluate nine existing
Video-LMMs, both open and closed sources, on this benchmarking task and find
that most of the models encounter difficulties in effectively identifying the
subtle anomalies. In conclusion, our research offers significant insights into
the current capabilities of Video-LMMs in the realm of anomaly detection,
highlighting the importance of our work in evaluating and improving these
models for real-world applications. Our code and data is available at
https://hananshafi.github.io/vane-benchmark/",2024-06-14 17:59:01+00:00,"['Rohit Bharadwaj', 'Hanan Gani', 'Muzammal Naseer', 'Fahad Shahbaz Khan', 'Salman Khan']",http://arxiv.org/abs/2406.10326v2
SCBench: A Sports Commentary Benchmark for Video LLMs,"Recently, significant advances have been made in Video Large Language Models
(Video LLMs) in both academia and industry. However, methods to evaluate and
benchmark the performance of different Video LLMs, especially their
fine-grained, temporal visual capabilities, remain very limited. On one hand,
current benchmarks use relatively simple videos (e.g., subtitled movie clips)
where the model can understand the entire video by processing just a few
frames. On the other hand, their datasets lack diversity in task format,
comprising only QA or multi-choice QA, which overlooks the models' capacity for
generating in-depth and precise texts. Sports videos, which feature intricate
visual information, sequential events, and emotionally charged commentary,
present a critical challenge for Video LLMs, making sports commentary an ideal
benchmarking task. Inspired by these challenges, we propose a novel task:
sports video commentary generation, developed $\textbf{SCBench}$ for Video
LLMs. To construct such a benchmark, we introduce (1) $\textbf{SCORES}$, a
six-dimensional metric specifically designed for our task, upon which we
propose a GPT-based evaluation method, and (2) $\textbf{CommentarySet}$, a
dataset consisting of 5,775 annotated video clips and ground-truth labels
tailored to our metric. Based on SCBench, we conduct comprehensive evaluations
on multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought
baseline methods. Our results found that InternVL-Chat-2 achieves the best
performance with 5.44, surpassing the second-best by 1.04. Our work provides a
fresh perspective for future research, aiming to enhance models' overall
capabilities in complex visual understanding tasks. Our dataset will be
released soon.",2024-12-23 15:13:56+00:00,"['Kuangzhi Ge', 'Lingjun Chen', 'Kevin Zhang', 'Yulin Luo', 'Tianyu Shi', 'Liaoyuan Fan', 'Xiang Li', 'Guanqun Wang', 'Shanghang Zhang']",http://arxiv.org/abs/2412.17637v1
Video Prediction Models as General Visual Encoders,"This study explores the potential of open-source video conditional generation
models as encoders for downstream tasks, focusing on instance segmentation
using the BAIR Robot Pushing Dataset. The researchers propose using video
prediction models as general visual encoders, leveraging their ability to
capture critical spatial and temporal information which is essential for tasks
such as instance segmentation. Inspired by human vision studies, particularly
Gestalts principle of common fate, the approach aims to develop a latent space
representative of motion from images to effectively discern foreground from
background information. The researchers utilize a 3D Vector-Quantized
Variational Autoencoder 3D VQVAE video generative encoder model conditioned on
an input frame, coupled with downstream segmentation tasks. Experiments involve
adapting pre-trained video generative models, analyzing their latent spaces,
and training custom decoders for foreground-background segmentation. The
findings demonstrate promising results in leveraging generative pretext
learning for downstream tasks, working towards enhanced scene analysis and
segmentation in computer vision applications.",2024-05-25 23:55:47+00:00,"['James Maier', 'Nishanth Mohankumar']",http://arxiv.org/abs/2405.16382v1
MSC: Multi-Scale Spatio-Temporal Causal Attention for Autoregressive Video Diffusion,"Diffusion transformers enable flexible generative modeling for video.
However, it is still technically challenging and computationally expensive to
generate high-resolution videos with rich semantics and complex motion. Similar
to languages, video data are also auto-regressive by nature, so it is
counter-intuitive to use attention mechanism with bi-directional dependency in
the model. Here we propose a Multi-Scale Causal (MSC) framework to address
these problems. Specifically, we introduce multiple resolutions in the spatial
dimension and high-low frequencies in the temporal dimension to realize
efficient attention calculation. Furthermore, attention blocks on multiple
scales are combined in a controlled way to allow causal conditioning on noisy
image frames for diffusion training, based on the idea that noise destroys
information at different rates on different resolutions. We theoretically show
that our approach can greatly reduce the computational complexity and enhance
the efficiency of training. The causal attention diffusion framework can also
be used for auto-regressive long video generation, without violating the
natural order of frame sequences.",2024-12-13 03:39:09+00:00,"['Xunnong Xu', 'Mengying Cao']",http://arxiv.org/abs/2412.09828v1
ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler,"Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V)
diffusion models has greatly enhanced video generation, especially in terms of
keyframe interpolation. However, current image-to-video diffusion models, while
powerful in generating videos from a single conditioning frame, need adaptation
for two-frame (start & end) conditioned generation, which is essential for
effective bounded interpolation. Unfortunately, existing approaches that fuse
temporally forward and backward paths in parallel often suffer from
off-manifold issues, leading to artifacts or requiring multiple iterative
re-noising steps. In this work, we introduce a novel, bidirectional sampling
strategy to address these off-manifold issues without requiring extensive
re-noising or fine-tuning. Our method employs sequential sampling along both
forward and backward paths, conditioned on the start and end frames,
respectively, ensuring more coherent and on-manifold generation of intermediate
frames. Additionally, we incorporate advanced guidance techniques, CFG++ and
DDS, to further enhance the interpolation process. By integrating these, our
method achieves state-of-the-art performance, efficiently generating
high-quality, smooth videos between keyframes. On a single 3090 GPU, our method
can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds,
establishing it as a leading solution for keyframe interpolation.",2024-10-08 03:01:54+00:00,"['Serin Yang', 'Taesung Kwon', 'Jong Chul Ye']",http://arxiv.org/abs/2410.05651v3
HARIVO: Harnessing Text-to-Image Models for Video Generation,"We present a method to create diffusion-based video models from pretrained
Text-to-Image (T2I) models. Recently, AnimateDiff proposed freezing the T2I
model while only training temporal layers. We advance this method by proposing
a unique architecture, incorporating a mapping network and frame-wise tokens,
tailored for video generation while maintaining the diversity and creativity of
the original T2I model. Key innovations include novel loss functions for
temporal smoothness and a mitigating gradient sampling technique, ensuring
realistic and temporally consistent video generation despite limited public
video data. We have successfully integrated video-specific inductive biases
into the architecture and loss functions. Our method, built on the frozen
StableDiffusion model, simplifies training processes and allows for seamless
integration with off-the-shelf models like ControlNet and DreamBooth. project
page: https://kwonminki.github.io/HARIVO",2024-10-10 09:47:39+00:00,"['Mingi Kwon', 'Seoung Wug Oh', 'Yang Zhou', 'Difan Liu', 'Joon-Young Lee', 'Haoran Cai', 'Baqiao Liu', 'Feng Liu', 'Youngjung Uh']",http://arxiv.org/abs/2410.07763v1
StarVid: Enhancing Semantic Alignment in Video Diffusion Models via Spatial and SynTactic Guided Attention Refocusing,"Recent advances in text-to-video (T2V) generation with diffusion models have
garnered significant attention. However, they typically perform well in scenes
with a single object and motion, struggling in compositional scenarios with
multiple objects and distinct motions to accurately reflect the semantic
content of text prompts. To address these challenges, we propose
\textbf{StarVid}, a plug-and-play, training-free method that improves semantic
alignment between multiple subjects, their motions, and text prompts in T2V
models. StarVid first leverages the spatial reasoning capabilities of large
language models (LLMs) for two-stage motion trajectory planning based on text
prompts. Such trajectories serve as spatial priors, guiding a spatial-aware
loss to refocus cross-attention (CA) maps into distinctive regions.
Furthermore, we propose a syntax-guided contrastive constraint to strengthen
the correlation between the CA maps of verbs and their corresponding nouns,
enhancing motion-subject binding. Both qualitative and quantitative evaluations
demonstrate that the proposed framework significantly outperforms baseline
methods, delivering videos of higher quality with improved semantic
consistency.",2024-09-23 17:56:03+00:00,"['Yuanhang Li', 'Qi Mao', 'Lan Chen', 'Zhen Fang', 'Lei Tian', 'Xinyan Xiao', 'Libiao Jin', 'Hua Wu']",http://arxiv.org/abs/2409.15259v2
MMDisCo: Multi-Modal Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation,"This study aims to construct an audio-video generative model with minimal
computational cost by leveraging pre-trained single-modal generative models for
audio and video. To achieve this, we propose a novel method that guides
single-modal models to cooperatively generate well-aligned samples across
modalities. Specifically, given two pre-trained base diffusion models, we train
a lightweight joint guidance module to adjust scores separately estimated by
the base models to match the score of joint distribution over audio and video.
We show that this guidance can be computed using the gradient of the optimal
discriminator, which distinguishes real audio-video pairs from fake ones
independently generated by the base models. Based on this analysis, we
construct a joint guidance module by training this discriminator. Additionally,
we adopt a loss function to stabilize the discriminator's gradient and make it
work as a noise estimator, as in standard diffusion models. Empirical
evaluations on several benchmark datasets demonstrate that our method improves
both single-modal fidelity and multimodal alignment with relatively few
parameters. The code is available at: https://github.com/SonyResearch/MMDisCo.",2024-05-28 05:43:03+00:00,"['Akio Hayakawa', 'Masato Ishii', 'Takashi Shibuya', 'Yuki Mitsufuji']",http://arxiv.org/abs/2405.17842v2
Mojito: Motion Trajectory and Intensity Control for Video Generation,"Recent advancements in diffusion models have shown great promise in producing
high-quality video content. However, efficiently training video diffusion
models capable of integrating directional guidance and controllable motion
intensity remains a challenging and under-explored area. To tackle these
challenges, this paper introduces Mojito, a diffusion model that incorporates
both motion trajectory and intensity control for text-to-video generation.
Specifically, Mojito features a Directional Motion Control (DMC) module that
leverages cross-attention to efficiently direct the generated object's motion
without training, alongside a Motion Intensity Modulator (MIM) that uses
optical flow maps generated from videos to guide varying levels of motion
intensity. Extensive experiments demonstrate Mojito's effectiveness in
achieving precise trajectory and intensity control with high computational
efficiency, generating motion patterns that closely match specified directions
and intensities, providing realistic dynamics that align well with natural
motion in real-world scenarios.",2024-12-12 05:26:43+00:00,"['Xuehai He', 'Shuohang Wang', 'Jianwei Yang', 'Xiaoxia Wu', 'Yiping Wang', 'Kuan Wang', 'Zheng Zhan', 'Olatunji Ruwase', 'Yelong Shen', 'Xin Eric Wang']",http://arxiv.org/abs/2412.08948v2
VideoDPO: Omni-Preference Alignment for Video Diffusion Generation,"Recent progress in generative diffusion models has greatly advanced
text-to-video generation. While text-to-video models trained on large-scale,
diverse datasets can produce varied outputs, these generations often deviate
from user preferences, highlighting the need for preference alignment on
pre-trained models. Although Direct Preference Optimization (DPO) has
demonstrated significant improvements in language and image generation, we
pioneer its adaptation to video diffusion models and propose a VideoDPO
pipeline by making several key adjustments. Unlike previous image alignment
methods that focus solely on either (i) visual quality or (ii) semantic
alignment between text and videos, we comprehensively consider both dimensions
and construct a preference score accordingly, which we term the OmniScore. We
design a pipeline to automatically collect preference pair data based on the
proposed OmniScore and discover that re-weighting these pairs based on the
score significantly impacts overall preference alignment. Our experiments
demonstrate substantial improvements in both visual quality and semantic
alignment, ensuring that no preference aspect is neglected. Code and data will
be shared at https://videodpo.github.io/.",2024-12-18 18:59:49+00:00,"['Runtao Liu', 'Haoyu Wu', 'Zheng Ziqiang', 'Chen Wei', 'Yingqing He', 'Renjie Pi', 'Qifeng Chen']",http://arxiv.org/abs/2412.14167v1
LightningDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos,"Accuracy and speed are critical in image editing tasks. Pan et al. introduced
a drag-based image editing framework that achieves pixel-level control using
Generative Adversarial Networks (GANs). A flurry of subsequent studies enhanced
this framework's generality by leveraging large-scale diffusion models.
However, these methods often suffer from inordinately long processing times
(exceeding 1 minute per edit) and low success rates. Addressing these issues
head on, we present LightningDrag, a rapid approach enabling high quality
drag-based image editing in ~1 second. Unlike most previous methods, we
redefine drag-based editing as a conditional generation task, eliminating the
need for time-consuming latent optimization or gradient-based guidance during
inference. In addition, the design of our pipeline allows us to train our model
on large-scale paired video frames, which contain rich motion information such
as object translations, changing poses and orientations, zooming in and out,
etc. By learning from videos, our approach can significantly outperform
previous methods in terms of accuracy and consistency. Despite being trained
solely on videos, our model generalizes well to perform local shape
deformations not presented in the training data (e.g., lengthening of hair,
twisting rainbows, etc.). Extensive qualitative and quantitative evaluations on
benchmark datasets corroborate the superiority of our approach. The code and
model will be released at https://github.com/magic-research/LightningDrag.",2024-05-22 15:14:00+00:00,"['Yujun Shi', 'Jun Hao Liew', 'Hanshu Yan', 'Vincent Y. F. Tan', 'Jiashi Feng']",http://arxiv.org/abs/2405.13722v2
DRSM: efficient neural 4d decomposition for dynamic reconstruction in stationary monocular cameras,"With the popularity of monocular videos generated by video sharing and live
broadcasting applications, reconstructing and editing dynamic scenes in
stationary monocular cameras has become a special but anticipated technology.
In contrast to scene reconstructions that exploit multi-view observations, the
problem of modeling a dynamic scene from a single view is significantly more
under-constrained and ill-posed. Inspired by recent progress in neural
rendering, we present a novel framework to tackle 4D decomposition problem for
dynamic scenes in monocular cameras. Our framework utilizes decomposed static
and dynamic feature planes to represent 4D scenes and emphasizes the learning
of dynamic regions through dense ray casting. Inadequate 3D clues from a
single-view and occlusion are also particular challenges in scene
reconstruction. To overcome these difficulties, we propose deep supervised
optimization and ray casting strategies. With experiments on various videos,
our method generates higher-fidelity results than existing methods for
single-view dynamic scene representation.",2024-02-01 16:38:51+00:00,"['Weixing Xie', 'Xiao Dong', 'Yong Yang', 'Qiqin Lin', 'Jingze Chen', 'Junfeng Yao', 'Xiaohu Guo']",http://arxiv.org/abs/2402.00740v1
What You See Is What Matters: A Novel Visual and Physics-Based Metric for Evaluating Video Generation Quality,"As video generation models advance rapidly, assessing the quality of
generated videos has become increasingly critical. Existing metrics, such as
Fr\'echet Video Distance (FVD), Inception Score (IS), and ClipSim, measure
quality primarily in latent space rather than from a human visual perspective,
often overlooking key aspects like appearance and motion consistency to
physical laws. In this paper, we propose a novel metric, VAMP (Visual
Appearance and Motion Plausibility), that evaluates both the visual appearance
and physical plausibility of generated videos. VAMP is composed of two main
components: an appearance score, which assesses color, shape, and texture
consistency across frames, and a motion score, which evaluates the realism of
object movements. We validate VAMP through two experiments: corrupted video
evaluation and generated video evaluation. In the corrupted video evaluation,
we introduce various types of corruptions into real videos and measure the
correlation between corruption severity and VAMP scores. In the generated video
evaluation, we use state-of-the-art models to generate videos from carefully
designed prompts and compare VAMP's performance to human evaluators' rankings.
Our results demonstrate that VAMP effectively captures both visual fidelity and
temporal consistency, offering a more comprehensive evaluation of video quality
than traditional methods.",2024-11-20 01:40:00+00:00,"['Zihan Wang', 'Songlin Li', 'Lingyan Hao', 'Xinyu Hu', 'Bowen Song']",http://arxiv.org/abs/2411.13609v2
How Far is Video Generation from World Model: A Physical Law Perspective,"OpenAI's Sora highlights the potential of video generation for developing
world models that adhere to fundamental physical laws. However, the ability of
video generation models to discover such laws purely from visual data without
human priors can be questioned. A world model learning the true law should give
predictions robust to nuances and correctly extrapolate on unseen scenarios. In
this work, we evaluate across three key scenarios: in-distribution,
out-of-distribution, and combinatorial generalization. We developed a 2D
simulation testbed for object movement and collisions to generate videos
deterministically governed by one or more classical mechanics laws. This
provides an unlimited supply of data for large-scale experimentation and
enables quantitative evaluation of whether the generated videos adhere to
physical laws. We trained diffusion-based video generation models to predict
object movements based on initial frames. Our scaling experiments show perfect
generalization within the distribution, measurable scaling behavior for
combinatorial generalization, but failure in out-of-distribution scenarios.
Further experiments reveal two key insights about the generalization mechanisms
of these models: (1) the models fail to abstract general physical rules and
instead exhibit ""case-based"" generalization behavior, i.e., mimicking the
closest training example; (2) when generalizing to new cases, models are
observed to prioritize different factors when referencing training data: color
> size > velocity > shape. Our study suggests that scaling alone is
insufficient for video generation models to uncover fundamental physical laws,
despite its role in Sora's broader success. See our project page at
https://phyworld.github.io",2024-11-04 18:53:05+00:00,"['Bingyi Kang', 'Yang Yue', 'Rui Lu', 'Zhijie Lin', 'Yang Zhao', 'Kaixin Wang', 'Gao Huang', 'Jiashi Feng']",http://arxiv.org/abs/2411.02385v1
MVBIND: Self-Supervised Music Recommendation For Videos Via Embedding Space Binding,"Recent years have witnessed the rapid development of short videos, which
usually contain both visual and audio modalities. Background music is important
to the short videos, which can significantly influence the emotions of the
viewers. However, at present, the background music of short videos is generally
chosen by the video producer, and there is a lack of automatic music
recommendation methods for short videos. This paper introduces MVBind, an
innovative Music-Video embedding space Binding model for cross-modal retrieval.
MVBind operates as a self-supervised approach, acquiring inherent knowledge of
intermodal relationships directly from data, without the need of manual
annotations. Additionally, to compensate the lack of a corresponding
musical-visual pair dataset for short videos, we construct a dataset,
SVM-10K(Short Video with Music-10K), which mainly consists of meticulously
selected short videos. On this dataset, MVBind manifests significantly improved
performance compared to other baseline methods. The constructed dataset and
code will be released to facilitate future research.",2024-05-15 12:11:28+00:00,"['Jiajie Teng', 'Huiyu Duan', 'Yucheng Zhu', 'Sijing Wu', 'Guangtao Zhai']",http://arxiv.org/abs/2405.09286v1
A Human-Annotated Video Dataset for Training and Evaluation of 360-Degree Video Summarization Methods,"In this paper we introduce a new dataset for 360-degree video summarization:
the transformation of 360-degree video content to concise 2D-video summaries
that can be consumed via traditional devices, such as TV sets and smartphones.
The dataset includes ground-truth human-generated summaries, that can be used
for training and objectively evaluating 360-degree video summarization methods.
Using this dataset, we train and assess two state-of-the-art summarization
methods that were originally proposed for 2D-video summarization, to serve as a
baseline for future comparisons with summarization methods that are
specifically tailored to 360-degree video. Finally, we present an interactive
tool that was developed to facilitate the data annotation process and can
assist other annotation activities that rely on video fragment selection.",2024-06-05 06:43:48+00:00,"['Ioannis Kontostathis', 'Evlampios Apostolidis', 'Vasileios Mezaris']",http://arxiv.org/abs/2406.02991v1
VidCompress: Memory-Enhanced Temporal Compression for Video Understanding in Large Language Models,"Video-based multimodal large language models (Video-LLMs) possess significant
potential for video understanding tasks. However, most Video-LLMs treat videos
as a sequential set of individual frames, which results in insufficient
temporal-spatial interaction that hinders fine-grained comprehension and
difficulty in processing longer videos due to limited visual token capacity. To
address these challenges, we propose VidCompress, a novel Video-LLM featuring
memory-enhanced temporal compression. VidCompress employs a dual-compressor
approach: a memory-enhanced compressor captures both short-term and long-term
temporal relationships in videos and compresses the visual tokens using a
multiscale transformer with a memory-cache mechanism, while a text-perceived
compressor generates condensed visual tokens by utilizing Q-Former and
integrating temporal contexts into query embeddings with cross attention.
Experiments on several VideoQA datasets and comprehensive benchmarks
demonstrate that VidCompress efficiently models complex temporal-spatial
relations and significantly outperforms existing Video-LLMs.",2024-10-15 09:07:25+00:00,"['Xiaohan Lan', 'Yitian Yuan', 'Zequn Jie', 'Lin Ma']",http://arxiv.org/abs/2410.11417v1
SyncVIS: Synchronized Video Instance Segmentation,"Recent DETR-based methods have advanced the development of Video Instance
Segmentation (VIS) through transformers' efficiency and capability in modeling
spatial and temporal information. Despite harvesting remarkable progress,
existing works follow asynchronous designs, which model video sequences via
either video-level queries only or adopting query-sensitive cascade structures,
resulting in difficulties when handling complex and challenging video
scenarios. In this work, we analyze the cause of this phenomenon and the
limitations of the current solutions, and propose to conduct synchronized
modeling via a new framework named SyncVIS. Specifically, SyncVIS explicitly
introduces video-level query embeddings and designs two key modules to
synchronize video-level query with frame-level query embeddings: a synchronized
video-frame modeling paradigm and a synchronized embedding optimization
strategy. The former attempts to promote the mutual learning of frame- and
video-level embeddings with each other and the latter divides large video
sequences into small clips for easier optimization. Extensive experimental
evaluations are conducted on the challenging YouTube-VIS 2019 & 2021 & 2022,
and OVIS benchmarks and SyncVIS achieves state-of-the-art results, which
demonstrates the effectiveness and generality of the proposed approach. The
code is available at https://github.com/rkzheng99/SyncVIS.",2024-12-01 16:43:20+00:00,"['Rongkun Zheng', 'Lu Qi', 'Xi Chen', 'Yi Wang', 'Kun Wang', 'Yu Qiao', 'Hengshuang Zhao']",http://arxiv.org/abs/2412.00882v1
SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device,"We have witnessed the unprecedented success of diffusion-based video
generation over the past year. Recently proposed models from the community have
wielded the power to generate cinematic and high-resolution videos with smooth
motions from arbitrary input prompts. However, as a supertask of image
generation, video generation models require more computation and are thus
hosted mostly on cloud servers, limiting broader adoption among content
creators. In this work, we propose a comprehensive acceleration framework to
bring the power of the large-scale video diffusion model to the hands of edge
users. From the network architecture scope, we initialize from a compact image
backbone and search out the design and arrangement of temporal layers to
maximize hardware efficiency. In addition, we propose a dedicated adversarial
fine-tuning algorithm for our efficient model and reduce the denoising steps to
4. Our model, with only 0.6B parameters, can generate a 5-second video on an
iPhone 16 PM within 5 seconds. Compared to server-side models that take minutes
on powerful GPUs to generate a single video, we accelerate the generation by
magnitudes while delivering on-par quality.",2024-12-13 18:59:56+00:00,"['Yushu Wu', 'Zhixing Zhang', 'Yanyu Li', 'Yanwu Xu', 'Anil Kag', 'Yang Sui', 'Huseyin Coskun', 'Ke Ma', 'Aleksei Lebedev', 'Ju Hu', 'Dimitris Metaxas', 'Yanzhi Wang', 'Sergey Tulyakov', 'Jian Ren']",http://arxiv.org/abs/2412.10494v1
Pre-training for Action Recognition with Automatically Generated Fractal Datasets,"In recent years, interest in synthetic data has grown, particularly in the
context of pre-training the image modality to support a range of computer
vision tasks, including object classification, medical imaging etc. Previous
work has demonstrated that synthetic samples, automatically produced by various
generative processes, can replace real counterparts and yield strong visual
representations. This approach resolves issues associated with real data such
as collection and labeling costs, copyright and privacy.
  We extend this trend to the video domain applying it to the task of action
recognition. Employing fractal geometry, we present methods to automatically
produce large-scale datasets of short synthetic video clips, which can be
utilized for pre-training neural models. The generated video clips are
characterized by notable variety, stemmed by the innate ability of fractals to
generate complex multi-scale structures. To narrow the domain gap, we further
identify key properties of real videos and carefully emulate them during
pre-training. Through thorough ablations, we determine the attributes that
strengthen downstream results and offer general guidelines for pre-training
with synthetic videos. The proposed approach is evaluated by fine-tuning
pre-trained models on established action recognition datasets HMDB51 and UCF101
as well as four other video benchmarks related to group action recognition,
fine-grained action recognition and dynamic scenes. Compared to standard
Kinetics pre-training, our reported results come close and are even superior on
a portion of downstream datasets. Code and samples of synthetic videos are
available at https://github.com/davidsvy/fractal_video .",2024-11-26 16:51:11+00:00,"['Davyd Svyezhentsev', 'George Retsinas', 'Petros Maragos']",http://arxiv.org/abs/2411.17584v1
Mobile Video Diffusion,"Video diffusion models have achieved impressive realism and controllability
but are limited by high computational demands, restricting their use on mobile
devices. This paper introduces the first mobile-optimized video diffusion
model. Starting from a spatio-temporal UNet from Stable Video Diffusion (SVD),
we reduce memory and computational cost by reducing the frame resolution,
incorporating multi-scale temporal representations, and introducing two novel
pruning schema to reduce the number of channels and temporal blocks.
Furthermore, we employ adversarial finetuning to reduce the denoising to a
single step. Our model, coined as MobileVD, is 523x more efficient (1817.2 vs.
4.34 TFLOPs) with a slight quality drop (FVD 149 vs. 171), generating latents
for a 14x512x256 px clip in 1.7 seconds on a Xiaomi-14 Pro. Our results are
available at https://qualcomm-ai-research.github.io/mobile-video-diffusion/",2024-12-10 15:19:10+00:00,"['Haitam Ben Yahia', 'Denis Korzhenkov', 'Ioannis Lelekas', 'Amir Ghodrati', 'Amirhossein Habibian']",http://arxiv.org/abs/2412.07583v1
Audio-Driven Emotional 3D Talking-Head Generation,"Audio-driven video portrait synthesis is a crucial and useful technology in
virtual human interaction and film-making applications. Recent advancements
have focused on improving the image fidelity and lip-synchronization. However,
generating accurate emotional expressions is an important aspect of realistic
talking-head generation, which has remained underexplored in previous works. We
present a novel system in this paper for synthesizing high-fidelity,
audio-driven video portraits with accurate emotional expressions. Specifically,
we utilize a variational autoencoder (VAE)-based audio-to-motion module to
generate facial landmarks. These landmarks are concatenated with emotional
embeddings to produce emotional landmarks through our motion-to-emotion module.
These emotional landmarks are then used to render realistic emotional
talking-head video using a Neural Radiance Fields (NeRF)-based emotion-to-video
module. Additionally, we propose a pose sampling method that generates natural
idle-state (non-speaking) videos in response to silent audio inputs. Extensive
experiments demonstrate that our method obtains more accurate emotion
generation with higher fidelity.",2024-10-07 08:23:05+00:00,"['Wenqing Wang', 'Yun Fu']",http://arxiv.org/abs/2410.17262v1
Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning,"Despite advancements in Text-to-Video (T2V) generation, producing videos with
realistic motion remains challenging. Current models often yield static or
minimally dynamic outputs, failing to capture complex motions described by
text. This issue stems from the internal biases in text encoding, which
overlooks motions, and inadequate conditioning mechanisms in T2V generation
models. To address this, we propose a novel framework called DEcomposed MOtion
(DEMO), which enhances motion synthesis in T2V generation by decomposing both
text encoding and conditioning into content and motion components. Our method
includes a content encoder for static elements and a motion encoder for
temporal dynamics, alongside separate content and motion conditioning
mechanisms. Crucially, we introduce text-motion and video-motion supervision to
improve the model's understanding and generation of motion. Evaluations on
benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench
demonstrate DEMO's superior ability to produce videos with enhanced motion
dynamics while maintaining high visual quality. Our approach significantly
advances T2V generation by integrating comprehensive motion understanding
directly from textual descriptions. Project page:
https://PR-Ryan.github.io/DEMO-project/",2024-10-31 17:59:53+00:00,"['Penghui Ruan', 'Pichao Wang', 'Divya Saxena', 'Jiannong Cao', 'Yuhui Shi']",http://arxiv.org/abs/2410.24219v1
"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming","This paper offers an insightful examination of how currently top-trending AI
technologies, i.e., generative artificial intelligence (Generative AI) and
large language models (LLMs), are reshaping the field of video technology,
including video generation, understanding, and streaming. It highlights the
innovative use of these technologies in producing highly realistic videos, a
significant leap in bridging the gap between real-world dynamics and digital
creation. The study also delves into the advanced capabilities of LLMs in video
understanding, demonstrating their effectiveness in extracting meaningful
information from visual content, thereby enhancing our interaction with videos.
In the realm of video streaming, the paper discusses how LLMs contribute to
more efficient and user-centric streaming experiences, adapting content
delivery to individual viewer preferences. This comprehensive review navigates
through the current achievements, ongoing challenges, and future possibilities
of applying Generative AI and LLMs to video-related tasks, underscoring the
immense potential these technologies hold for advancing the field of video
technology related to multimedia, networking, and AI communities.",2024-01-30 14:37:10+00:00,"['Pengyuan Zhou', 'Lin Wang', 'Zhi Liu', 'Yanbin Hao', 'Pan Hui', 'Sasu Tarkoma', 'Jussi Kangasharju']",http://arxiv.org/abs/2404.16038v1
MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation,"Sounding Video Generation (SVG) is an audio-video joint generation task
challenged by high-dimensional signal spaces, distinct data formats, and
different patterns of content information. To address these issues, we
introduce a novel multi-modal latent diffusion model (MM-LDM) for the SVG task.
We first unify the representation of audio and video data by converting them
into a single or a couple of images. Then, we introduce a hierarchical
multi-modal autoencoder that constructs a low-level perceptual latent space for
each modality and a shared high-level semantic feature space. The former space
is perceptually equivalent to the raw signal space of each modality but
drastically reduces signal dimensions. The latter space serves to bridge the
information gap between modalities and provides more insightful cross-modal
guidance. Our proposed method achieves new state-of-the-art results with
significant quality and efficiency gains. Specifically, our method achieves a
comprehensive improvement on all evaluation metrics and a faster training and
sampling speed on Landscape and AIST++ datasets. Moreover, we explore its
performance on open-domain sounding video generation, long sounding video
generation, audio continuation, video continuation, and conditional
single-modal generation tasks for a comprehensive evaluation, where our MM-LDM
demonstrates exciting adaptability and generalization ability.",2024-10-02 14:32:24+00:00,"['Mingzhen Sun', 'Weining Wang', 'Yanyuan Qiao', 'Jiahui Sun', 'Zihan Qin', 'Longteng Guo', 'Xinxin Zhu', 'Jing Liu']",http://arxiv.org/abs/2410.01594v1
SPECTRUM: Semantic Processing and Emotion-informed video-Captioning Through Retrieval and Understanding Modalities,"Capturing a video's meaning and critical concepts by analyzing the subtle
details is a fundamental yet challenging task in video captioning. Identifying
the dominant emotional tone in a video significantly enhances the perception of
its context. Despite a strong emphasis on video captioning, existing models
often need to adequately address emotional themes, resulting in suboptimal
captioning results. To address these limitations, this paper proposes a novel
Semantic Processing and Emotion-informed video-Captioning Through Retrieval and
Understanding Modalities (SPECTRUM) framework to empower the generation of
emotionally and semantically credible captions. Leveraging our pioneering
structure, SPECTRUM discerns multimodal semantics and emotional themes using
Visual Text Attribute Investigation (VTAI) and determines the orientation of
descriptive captions through a Holistic Concept-Oriented Theme (HCOT),
expressing emotionally-informed and field-acquainted references. They exploit
video-to-text retrieval capabilities and the multifaceted nature of video
content to estimate the emotional probabilities of candidate captions. Then,
the dominant theme of the video is determined by appropriately weighting
embedded attribute vectors and applying coarse- and fine-grained emotional
concepts, which define the video's contextual alignment. Furthermore, using two
loss functions, SPECTRUM is optimized to integrate emotional information and
minimize prediction errors. Extensive experiments on the EmVidCap, MSVD, and
MSRVTT video captioning datasets demonstrate that our model significantly
surpasses state-of-the-art methods. Quantitative and qualitative evaluations
highlight the model's ability to accurately capture and convey video emotions
and multimodal attributes.",2024-11-04 10:51:47+00:00,"['Ehsan Faghihi', 'Mohammedreza Zarenejad', 'Ali-Asghar Beheshti Shirazi']",http://arxiv.org/abs/2411.01975v1
CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding,"Most existing video understanding benchmarks for multimodal large language
models (MLLMs) focus only on short videos. The limited number of benchmarks for
long video understanding often rely solely on multiple-choice questions (MCQs).
However, because of the inherent limitation of MCQ-based evaluation and the
increasing reasoning ability of MLLMs, models can give the current answer
purely by combining short video understanding with elimination, without
genuinely understanding the video content. To address this gap, we introduce
CG-Bench, a novel benchmark designed for clue-grounded question answering in
long videos. CG-Bench emphasizes the model's ability to retrieve relevant clues
for questions, enhancing evaluation credibility. It features 1,219 manually
curated videos categorized by a granular system with 14 primary categories, 171
secondary categories, and 638 tertiary categories, making it the largest
benchmark for long video analysis. The benchmark includes 12,129 QA pairs in
three major question types: perception, reasoning, and hallucination.
Compensating the drawbacks of pure MCQ-based evaluation, we design two novel
clue-based evaluation methods: clue-grounded white box and black box
evaluations, to assess whether the model generates answers based on the correct
understanding of the video. We evaluate multiple closed-source and open-source
MLLMs on CG-Bench. Results indicate that current models significantly
underperform in understanding long videos compared to short ones, and a
significant gap exists between open-source and commercial models. We hope
CG-Bench can advance the development of more trustworthy and capable MLLMs for
long video understanding. All annotations and video data are released at
https://cg-bench.github.io/leaderboard/.",2024-12-16 18:46:45+00:00,"['Guo Chen', 'Yicheng Liu', 'Yifei Huang', 'Yuping He', 'Baoqi Pei', 'Jilan Xu', 'Yali Wang', 'Tong Lu', 'Limin Wang']",http://arxiv.org/abs/2412.12075v1
LongVLM: Efficient Long Video Understanding via Large Language Models,"Empowered by Large Language Models (LLMs), recent advancements in Video-based
LLMs (VideoLLMs) have driven progress in various video understanding tasks.
These models encode video representations through pooling or query aggregation
over a vast number of visual tokens, making computational and memory costs
affordable. Despite successfully providing an overall comprehension of video
content, existing VideoLLMs still face challenges in achieving detailed
understanding due to overlooking local information in long-term videos. To
tackle this challenge, we introduce LongVLM, a simple yet powerful VideoLLM for
long video understanding, building upon the observation that long videos often
consist of sequential key events, complex actions, and camera movements. Our
approach proposes to decompose long videos into multiple short-term segments
and encode local features for each segment via a hierarchical token merging
module. These features are concatenated in temporal order to maintain the
storyline across sequential short-term segments. Additionally, we propose to
integrate global semantics into each local feature to enhance context
understanding. In this way, we encode video representations that incorporate
both local and global information, enabling the LLM to generate comprehensive
responses for long-term videos. Experimental results on the VideoChatGPT
benchmark and zero-shot video question-answering datasets demonstrate the
superior capabilities of our model over the previous state-of-the-art methods.
Qualitative examples show that our model produces more precise responses for
long video understanding. Code is available at
https://github.com/ziplab/LongVLM.",2024-04-04 11:33:29+00:00,"['Yuetian Weng', 'Mingfei Han', 'Haoyu He', 'Xiaojun Chang', 'Bohan Zhuang']",http://arxiv.org/abs/2404.03384v3
Streaming Long Video Understanding with Large Language Models,"This paper presents VideoStreaming, an advanced vision-language large model
(VLLM) for video understanding, that capably understands arbitrary-length video
with a constant number of video tokens streamingly encoded and adaptively
selected. The challenge of video understanding in the vision language area
mainly lies in the significant computational burden caused by the great number
of tokens extracted from long videos. Previous works rely on sparse sampling or
frame compression to reduce tokens. However, such approaches either disregard
temporal information in a long time span or sacrifice spatial details,
resulting in flawed compression. To address these limitations, our
VideoStreaming has two core designs: Memory-Propagated Streaming Encoding and
Adaptive Memory Selection. The Memory-Propagated Streaming Encoding
architecture segments long videos into short clips and sequentially encodes
each clip with a propagated memory. In each iteration, we utilize the encoded
results of the preceding clip as historical memory, which is integrated with
the current clip to distill a condensed representation that encapsulates the
video content up to the current timestamp. After the encoding process, the
Adaptive Memory Selection strategy selects a constant number of
question-related memories from all the historical memories and feeds them into
the LLM to generate informative responses. The question-related selection
reduces redundancy within the memories, enabling efficient and precise video
understanding. Meanwhile, the disentangled video extraction and reasoning
design allows the LLM to answer different questions about a video by directly
selecting corresponding memories, without the need to encode the whole video
for each question. Our model achieves superior performance and higher
efficiency on long video benchmarks, showcasing precise temporal comprehension
for detailed question answering.",2024-05-25 02:22:09+00:00,"['Rui Qian', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Shuangrui Ding', 'Dahua Lin', 'Jiaqi Wang']",http://arxiv.org/abs/2405.16009v1
Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models,"Video Large Language Models (Video-LLMs) have demonstrated remarkable
capabilities in coarse-grained video understanding, however, they struggle with
fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,
a novel Video-LLM adept at perceiving and reasoning over specific video moments
in a fine-grained manner. We identify that current Video-LLMs have limitations
for fine-grained video understanding since they lack effective temporal
modeling and timestamp representation. In light of this, we sharpen our model
by incorporating (1) an additional temporal stream to encode the relationships
between frames and (2) discrete temporal tokens enriched with specific time
knowledge to represent timestamps. To optimize the training of
Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with
simple video-captioning tasks and progressively introducing video temporal
grounding tasks of increasing complexity. To further enhance
Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded
VideoQA dataset by an automatic annotation pipeline. Extensive experiments
demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding
tasks such as temporal sentence grounding, dense video captioning, and grounded
VideoQA, but also shows great potential as a versatile video assistant for
general video understanding.",2024-10-04 10:04:37+00:00,"['Haibo Wang', 'Zhiyang Xu', 'Yu Cheng', 'Shizhe Diao', 'Yufan Zhou', 'Yixin Cao', 'Qifan Wang', 'Weifeng Ge', 'Lifu Huang']",http://arxiv.org/abs/2410.03290v1
DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation,"Talking head generation intends to produce vivid and realistic talking head
videos from a single portrait and speech audio clip. Although significant
progress has been made in diffusion-based talking head generation, almost all
methods rely on autoregressive strategies, which suffer from limited context
utilization beyond the current generation step, error accumulation, and slower
generation speed. To address these challenges, we present DAWN (Dynamic frame
Avatar With Non-autoregressive diffusion), a framework that enables all-at-once
generation of dynamic-length video sequences. Specifically, it consists of two
main components: (1) audio-driven holistic facial dynamics generation in the
latent motion space, and (2) audio-driven head pose and blink generation.
Extensive experiments demonstrate that our method generates authentic and vivid
videos with precise lip motions, and natural pose/blink movements.
Additionally, with a high generation speed, DAWN possesses strong extrapolation
capabilities, ensuring the stable production of high-quality long videos. These
results highlight the considerable promise and potential impact of DAWN in the
field of talking head video generation. Furthermore, we hope that DAWN sparks
further exploration of non-autoregressive approaches in diffusion models. Our
code will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch.",2024-10-17 16:32:36+00:00,"['Hanbo Cheng', 'Limin Lin', 'Chenyu Liu', 'Pengcheng Xia', 'Pengfei Hu', 'Jiefeng Ma', 'Jun Du', 'Jia Pan']",http://arxiv.org/abs/2410.13726v3
AnimateDiff-Lightning: Cross-Model Diffusion Distillation,"We present AnimateDiff-Lightning for lightning-fast video generation. Our
model uses progressive adversarial diffusion distillation to achieve new
state-of-the-art in few-step video generation. We discuss our modifications to
adapt it for the video modality. Furthermore, we propose to simultaneously
distill the probability flow of multiple base diffusion models, resulting in a
single distilled motion module with broader style compatibility. We are pleased
to release our distilled AnimateDiff-Lightning model for the community's use.",2024-03-19 13:08:54+00:00,"['Shanchuan Lin', 'Xiao Yang']",http://arxiv.org/abs/2403.12706v1
Deep video representation learning: a survey,"This paper provides a review on representation learning for videos. We
classify recent spatiotemporal feature learning methods for sequential visual
data and compare their pros and cons for general video analysis. Building
effective features for videos is a fundamental problem in computer vision tasks
involving video analysis and understanding. Existing features can be generally
categorized into spatial and temporal features. Their effectiveness under
variations of illumination, occlusion, view and background are discussed.
Finally, we discuss the remaining challenges in existing deep video
representation learning studies.",2024-05-10 16:20:11+00:00,"['Elham Ravanbakhsh', 'Yongqing Liang', 'J. Ramanujam', 'Xin Li']",http://arxiv.org/abs/2405.06574v1
Video Generation with Learned Action Prior,"Stochastic video generation is particularly challenging when the camera is
mounted on a moving platform, as camera motion interacts with observed image
pixels, creating complex spatio-temporal dynamics and making the problem
partially observable. Existing methods typically address this by focusing on
raw pixel-level image reconstruction without explicitly modelling camera motion
dynamics. We propose a solution by considering camera motion or action as part
of the observed image state, modelling both image and action within a
multi-modal learning framework. We introduce three models: Video Generation
with Learning Action Prior (VG-LeAP) treats the image-action pair as an
augmented state generated from a single latent stochastic process and uses
variational inference to learn the image-action latent prior; Causal-LeAP,
which establishes a causal relationship between action and the observed image
frame at time $t$, learning an action prior conditioned on the observed image
states; and RAFI, which integrates the augmented image-action state concept
into flow matching with diffusion generative processes, demonstrating that this
action-conditioned image generation concept can be extended to other
diffusion-based models. We emphasize the importance of multi-modal training in
partially observable video generation problems through detailed empirical
studies on our new video action dataset, RoAM.",2024-06-20 16:00:07+00:00,"['Meenakshi Sarkar', 'Devansh Bhardwaj', 'Debasish Ghose']",http://arxiv.org/abs/2406.14436v1
On depth prediction for autonomous driving using self-supervised learning,"Perception of the environment is a critical component for enabling autonomous
driving. It provides the vehicle with the ability to comprehend its
surroundings and make informed decisions. Depth prediction plays a pivotal role
in this process, as it helps the understanding of the geometry and motion of
the environment. This thesis focuses on the challenge of depth prediction using
monocular self-supervised learning techniques. The problem is approached from a
broader perspective first, exploring conditional generative adversarial
networks (cGANs) as a potential technique to achieve better generalization was
performed. In doing so, a fundamental contribution to the conditional GANs, the
acontrario cGAN was proposed. The second contribution entails a single
image-to-depth self-supervised method, proposing a solution for the rigid-scene
assumption using a novel transformer-based method that outputs a pose for each
dynamic object. The third significant aspect involves the introduction of a
video-to-depth map forecasting approach. This method serves as an extension of
self-supervised techniques to predict future depths. This involves the creation
of a novel transformer model capable of predicting the future depth of a given
scene. Moreover, the various limitations of the aforementioned methods were
addressed and a video-to-video depth maps model was proposed. This model
leverages the spatio-temporal consistency of the input and output sequence to
predict a more accurate depth sequence output. These methods have significant
applications in autonomous driving (AD) and advanced driver assistance systems
(ADAS).",2024-03-10 12:33:12+00:00,['Houssem Boulahbal'],http://arxiv.org/abs/2403.06194v1
ScreenWriter: Automatic Screenplay Generation and Movie Summarisation,"The proliferation of creative video content has driven demand for textual
descriptions or summaries that allow users to recall key plot points or get an
overview without watching. The volume of movie content and speed of turnover
motivates automatic summarisation, which is nevertheless challenging, requiring
identifying character intentions and very long-range temporal dependencies. The
few existing methods attempting this task rely heavily on textual screenplays
as input, greatly limiting their applicability. In this work, we propose the
task of automatic screenplay generation, and a method, ScreenWriter, that
operates only on video and produces output which includes dialogue, speaker
names, scene breaks, and visual descriptions. ScreenWriter introduces a novel
algorithm to segment the video into scenes based on the sequence of visual
vectors, and a novel method for the challenging problem of determining
character names, based on a database of actors' faces. We further demonstrate
how these automatic screenplays can be used to generate plot synopses with a
hierarchical summarisation method based on scene breaks. We test the quality of
the final summaries on the recent MovieSum dataset, which we augment with
videos, and show that they are superior to a number of comparison models which
assume access to goldstandard screenplays.",2024-10-17 07:59:54+00:00,"['Louis Mahon', 'Mirella Lapata']",http://arxiv.org/abs/2410.19809v1
Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated Objects,"We present Knowledge NeRF to synthesize novel views for dynamic scenes.
Reconstructing dynamic 3D scenes from few sparse views and rendering them from
arbitrary perspectives is a challenging problem with applications in various
domains. Previous dynamic NeRF methods learn the deformation of articulated
objects from monocular videos. However, qualities of their reconstructed scenes
are limited. To clearly reconstruct dynamic scenes, we propose a new framework
by considering two frames at a time.We pretrain a NeRF model for an articulated
object.When articulated objects moves, Knowledge NeRF learns to generate novel
views at the new state by incorporating past knowledge in the pretrained NeRF
model with minimal observations in the present state. We propose a projection
module to adapt NeRF for dynamic scenes, learning the correspondence between
pretrained knowledge base and current states. Experimental results demonstrate
the effectiveness of our method in reconstructing dynamic 3D scenes with 5
input images in one state. Knowledge NeRF is a new pipeline and promising
solution for novel view synthesis in dynamic articulated objects. The data and
implementation are publicly available at
https://github.com/RussRobin/Knowledge_NeRF.",2024-03-31 12:45:23+00:00,"['Wenxiao Cai', 'Xinyue Lei', 'Xinyu He', 'Junming Leo Chen', 'Yangang Wang']",http://arxiv.org/abs/2404.00674v2
Adaptive Super Resolution For One-Shot Talking-Head Generation,"The one-shot talking-head generation learns to synthesize a talking-head
video with one source portrait image under the driving of same or different
identity video. Usually these methods require plane-based pixel transformations
via Jacobin matrices or facial image warps for novel poses generation. The
constraints of using a single image source and pixel displacements often
compromise the clarity of the synthesized images. Some methods try to improve
the quality of synthesized videos by introducing additional super-resolution
modules, but this will undoubtedly increase computational consumption and
destroy the original data distribution. In this work, we propose an adaptive
high-quality talking-head video generation method, which synthesizes
high-resolution video without additional pre-trained modules. Specifically,
inspired by existing super-resolution methods, we down-sample the one-shot
source image, and then adaptively reconstruct high-frequency details via an
encoder-decoder module, resulting in enhanced video clarity. Our method
consistently improves the quality of generated videos through a straightforward
yet effective strategy, substantiated by quantitative and qualitative
evaluations. The code and demo video are available on:
\url{https://github.com/Songluchuan/AdaSR-TalkingHead/}.",2024-03-23 22:14:38+00:00,"['Luchuan Song', 'Pinxin Liu', 'Guojun Yin', 'Chenliang Xu']",http://arxiv.org/abs/2403.15944v1
Improving Generative Adversarial Networks for Video Super-Resolution,"In this research, we explore different ways to improve generative adversarial
networks for video super-resolution tasks from a base single image
super-resolution GAN model. Our primary objective is to identify potential
techniques that enhance these models and to analyze which of these techniques
yield the most significant improvements. We evaluate our results using Peak
Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Our
findings indicate that the most effective techniques include temporal
smoothing, long short-term memory (LSTM) layers, and a temporal loss function.
The integration of these methods results in an 11.97% improvement in PSNR and
an 8% improvement in SSIM compared to the baseline video super-resolution
generative adversarial network (GAN) model. This substantial improvement
suggests potential further applications to enhance current state-of-the-art
models.",2024-06-24 06:57:51+00:00,['Daniel Wen'],http://arxiv.org/abs/2406.16359v1
VistaDream: Sampling multiview consistent images for single-view scene reconstruction,"In this paper, we propose VistaDream a novel framework to reconstruct a 3D
scene from a single-view image. Recent diffusion models enable generating
high-quality novel-view images from a single-view input image. Most existing
methods only concentrate on building the consistency between the input image
and the generated images while losing the consistency between the generated
images. VistaDream addresses this problem by a two-stage pipeline. In the first
stage, VistaDream begins with building a global coarse 3D scaffold by zooming
out a little step with inpainted boundaries and an estimated depth map. Then,
on this global scaffold, we use iterative diffusion-based RGB-D inpainting to
generate novel-view images to inpaint the holes of the scaffold. In the second
stage, we further enhance the consistency between the generated novel-view
images by a novel training-free Multiview Consistency Sampling (MCS) that
introduces multi-view consistency constraints in the reverse sampling process
of diffusion models. Experimental results demonstrate that without training or
fine-tuning existing diffusion models, VistaDream achieves consistent and
high-quality novel view synthesis using just single-view images and outperforms
baseline methods by a large margin. The code, videos, and interactive demos are
available at https://vistadream-project-page.github.io/.",2024-10-22 10:55:59+00:00,"['Haiping Wang', 'Yuan Liu', 'Ziwei Liu', 'Wenping Wang', 'Zhen Dong', 'Bisheng Yang']",http://arxiv.org/abs/2410.16892v1
Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation,"Image-to-video (I2V) generation tasks always suffer from keeping high
fidelity in the open domains. Traditional image animation techniques primarily
focus on specific domains such as faces or human poses, making them difficult
to generalize to open domains. Several recent I2V frameworks based on diffusion
models can generate dynamic content for open domain images but fail to maintain
fidelity. We found that two main factors of low fidelity are the loss of image
details and the noise prediction biases during the denoising process. To this
end, we propose an effective method that can be applied to mainstream video
diffusion models. This method achieves high fidelity based on supplementing
more precise image information and noise rectification. Specifically, given a
specified image, our method first adds noise to the input image latent to keep
more details, then denoises the noisy latent with proper rectification to
alleviate the noise prediction biases. Our method is tuning-free and
plug-and-play. The experimental results demonstrate the effectiveness of our
approach in improving the fidelity of generated videos. For more image-to-video
generated results, please refer to the project website:
https://noise-rectification.github.io.",2024-03-05 09:57:47+00:00,"['Weijie Li', 'Litong Gong', 'Yiran Zhu', 'Fanda Fan', 'Biao Wang', 'Tiezheng Ge', 'Bo Zheng']",http://arxiv.org/abs/2403.02827v1
VideoTetris: Towards Compositional Text-to-Video Generation,"Diffusion models have demonstrated great success in text-to-video (T2V)
generation. However, existing methods may face challenges when handling complex
(long) video generation scenarios that involve multiple objects or dynamic
changes in object numbers. To address these limitations, we propose
VideoTetris, a novel framework that enables compositional T2V generation.
Specifically, we propose spatio-temporal compositional diffusion to precisely
follow complex textual semantics by manipulating and composing the attention
maps of denoising networks spatially and temporally. Moreover, we propose an
enhanced video data preprocessing to enhance the training data regarding motion
dynamics and prompt understanding, equipped with a new reference frame
attention mechanism to improve the consistency of auto-regressive video
generation. Extensive experiments demonstrate that our VideoTetris achieves
impressive qualitative and quantitative results in compositional T2V
generation. Code is available at: https://github.com/YangLing0818/VideoTetris",2024-06-06 17:25:33+00:00,"['Ye Tian', 'Ling Yang', 'Haotian Yang', 'Yuan Gao', 'Yufan Deng', 'Jingmin Chen', 'Xintao Wang', 'Zhaochen Yu', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Bin Cui']",http://arxiv.org/abs/2406.04277v2
Boosting Camera Motion Control for Video Diffusion Transformers,"Recent advancements in diffusion models have significantly enhanced the
quality of video generation. However, fine-grained control over camera pose
remains a challenge. While U-Net-based models have shown promising results for
camera control, transformer-based diffusion models (DiT)-the preferred
architecture for large-scale video generation - suffer from severe degradation
in camera motion accuracy. In this paper, we investigate the underlying causes
of this issue and propose solutions tailored to DiT architectures. Our study
reveals that camera control performance depends heavily on the choice of
conditioning methods rather than camera pose representations that is commonly
believed. To address the persistent motion degradation in DiT, we introduce
Camera Motion Guidance (CMG), based on classifier-free guidance, which boosts
camera control by over 400%. Additionally, we present a sparse camera control
pipeline, significantly simplifying the process of specifying camera poses for
long videos. Our method universally applies to both U-Net and DiT models,
offering improved camera control for video generation tasks.",2024-10-14 17:58:07+00:00,"['Soon Yau Cheong', 'Duygu Ceylan', 'Armin Mustafa', 'Andrew Gilbert', 'Chun-Hao Paul Huang']",http://arxiv.org/abs/2410.10802v1
Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model,"ControlNets are widely used for adding spatial control to text-to-image
diffusion models with different conditions, such as depth maps,
scribbles/sketches, and human poses. However, when it comes to controllable
video generation, ControlNets cannot be directly integrated into new backbones
due to feature space mismatches, and training ControlNets for new backbones can
be a significant burden for many users. Furthermore, applying ControlNets
independently to different frames cannot effectively maintain object temporal
consistency. To address these challenges, we introduce Ctrl-Adapter, an
efficient and versatile framework that adds diverse controls to any image/video
diffusion model through the adaptation of pretrained ControlNets. Ctrl-Adapter
offers strong and diverse capabilities, including image and video control,
sparse-frame video control, fine-grained patch-level multi-condition control
(via an MoE router), zero-shot adaptation to unseen conditions, and supports a
variety of downstream tasks beyond spatial control, including video editing,
video style transfer, and text-guided motion control. With six diverse
U-Net/DiT-based image/video diffusion models (SDXL, PixArt-$\alpha$, I2VGen-XL,
SVD, Latte, Hotshot-XL), Ctrl-Adapter matches the performance of pretrained
ControlNets on COCO and achieves the state-of-the-art on DAVIS 2017 with
significantly lower computation (< 10 GPU hours).",2024-04-15 17:45:36+00:00,"['Han Lin', 'Jaemin Cho', 'Abhay Zala', 'Mohit Bansal']",http://arxiv.org/abs/2404.09967v2
T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models,"The recent development of Sora leads to a new era in text-to-video (T2V)
generation. Along with this comes the rising concern about its security risks.
The generated videos may contain illegal or unethical content, and there is a
lack of comprehensive quantitative understanding of their safety, posing a
challenge to their reliability and practical deployment. Previous evaluations
primarily focus on the quality of video generation. While some evaluations of
text-to-image models have considered safety, they cover fewer aspects and do
not address the unique temporal risk inherent in video generation. To bridge
this research gap, we introduce T2VSafetyBench, a new benchmark designed for
conducting safety-critical assessments of text-to-video models. We define 12
critical aspects of video generation safety and construct a malicious prompt
dataset including real-world prompts, LLM-generated prompts and jailbreak
attack-based prompts. Based on our evaluation results, we draw several
important findings, including: 1) no single model excels in all aspects, with
different models showing various strengths; 2) the correlation between GPT-4
assessments and manual reviews is generally high; 3) there is a trade-off
between the usability and safety of text-to-video generative models. This
indicates that as the field of video generation rapidly advances, safety risks
are set to surge, highlighting the urgency of prioritizing video safety. We
hope that T2VSafetyBench can provide insights for better understanding the
safety of video generation in the era of generative AI.",2024-07-08 14:04:58+00:00,"['Yibo Miao', 'Yifan Zhu', 'Yinpeng Dong', 'Lijia Yu', 'Jun Zhu', 'Xiao-Shan Gao']",http://arxiv.org/abs/2407.05965v3
ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions,"The goal of this work is to generate step-by-step visual instructions in the
form of a sequence of images, given an input image that provides the scene
context and the sequence of textual instructions. This is a challenging problem
as it requires generating multi-step image sequences to achieve a complex goal
while being grounded in a specific environment. Part of the challenge stems
from the lack of large-scale training data for this problem. The contribution
of this work is thus three-fold. First, we introduce an automatic approach for
collecting large step-by-step visual instruction training data from
instructional videos. We apply this approach to one million videos and create a
large-scale, high-quality dataset of 0.6M sequences of image-text pairs.
Second, we develop and train ShowHowTo, a video diffusion model capable of
generating step-by-step visual instructions consistent with the provided input
image. Third, we evaluate the generated image sequences across three dimensions
of accuracy (step, scene, and task) and show our model achieves
state-of-the-art results on all of them. Our code, dataset, and trained models
are publicly available.",2024-12-02 21:40:17+00:00,"['Tom Souek', 'Prajwal Gatti', 'Michael Wray', 'Ivan Laptev', 'Dima Damen', 'Josef Sivic']",http://arxiv.org/abs/2412.01987v2
Autoregressive Video Generation without Vector Quantization,"This paper presents a novel approach that enables autoregressive video
generation with high efficiency. We propose to reformulate the video generation
problem as a non-quantized autoregressive modeling of temporal frame-by-frame
prediction and spatial set-by-set prediction. Unlike raster-scan prediction in
prior autoregressive models or joint distribution modeling of fixed-length
tokens in diffusion models, our approach maintains the causal property of
GPT-style models for flexible in-context capabilities, while leveraging
bidirectional modeling within individual frames for efficiency. With the
proposed approach, we train a novel video autoregressive model without vector
quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior
autoregressive video models in data efficiency, inference speed, visual
fidelity, and video fluency, even with a much smaller model capacity, i.e.,
0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models
in text-to-image generation tasks, with a significantly lower training cost.
Additionally, NOVA generalizes well across extended video durations and enables
diverse zero-shot applications in one unified model. Code and models are
publicly available at https://github.com/baaivision/NOVA.",2024-12-18 18:59:53+00:00,"['Haoge Deng', 'Ting Pan', 'Haiwen Diao', 'Zhengxiong Luo', 'Yufeng Cui', 'Huchuan Lu', 'Shiguang Shan', 'Yonggang Qi', 'Xinlong Wang']",http://arxiv.org/abs/2412.14169v2
Generative Human Video Compression with Multi-granularity Temporal Trajectory Factorization,"In this paper, we propose a novel Multi-granularity Temporal Trajectory
Factorization framework for generative human video compression, which holds
great potential for bandwidth-constrained human-centric video communication. In
particular, the proposed motion factorization strategy can facilitate to
implicitly characterize the high-dimensional visual signal into compact motion
vectors for representation compactness and further transform these vectors into
a fine-grained field for motion expressibility. As such, the coded bit-stream
can be entailed with enough visual motion information at the lowest
representation cost. Meanwhile, a resolution-expandable generative module is
developed with enhanced background stability, such that the proposed framework
can be optimized towards higher reconstruction robustness and more flexible
resolution adaptation. Experimental results show that proposed method
outperforms latest generative models and the state-of-the-art video coding
standard Versatile Video Coding (VVC) on both talking-face videos and
moving-body videos in terms of both objective and subjective quality. The
project page can be found at
https://github.com/xyzysz/Extreme-Human-Video-Compression-with-MTTF.",2024-10-14 05:34:32+00:00,"['Shanzhi Yin', 'Bolin Chen', 'Shiqi Wang', 'Yan Ye']",http://arxiv.org/abs/2410.10171v1
InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption,"Text-to-video generation has evolved rapidly in recent years, delivering
remarkable results. Training typically relies on video-caption paired data,
which plays a crucial role in enhancing generation performance. However,
current video captions often suffer from insufficient details, hallucinations
and imprecise motion depiction, affecting the fidelity and consistency of
generated videos. In this work, we propose a novel instance-aware structured
caption framework, termed InstanceCap, to achieve instance-level and
fine-grained video caption for the first time. Based on this scheme, we design
an auxiliary models cluster to convert original video into instances to enhance
instance fidelity. Video instances are further used to refine dense prompts
into structured phrases, achieving concise yet precise descriptions.
Furthermore, a 22K InstanceVid dataset is curated for training, and an
enhancement pipeline that tailored to InstanceCap structure is proposed for
inference. Experimental results demonstrate that our proposed InstanceCap
significantly outperform previous models, ensuring high fidelity between
captions and videos while reducing hallucinations.",2024-12-12 13:48:40+00:00,"['Tiehan Fan', 'Kepan Nan', 'Rui Xie', 'Penghao Zhou', 'Zhenheng Yang', 'Chaoyou Fu', 'Xiang Li', 'Jian Yang', 'Ying Tai']",http://arxiv.org/abs/2412.09283v1
PolySmart @ TRECVid 2024 Medical Video Question Answering,"Video Corpus Visual Answer Localization (VCVAL) includes question-related
video retrieval and visual answer localization in the videos. Specifically, we
use text-to-text retrieval to find relevant videos for a medical question based
on the similarity of video transcript and answers generated by GPT4. For the
visual answer localization, the start and end timestamps of the answer are
predicted by the alignments on both visual content and subtitles with queries.
For the Query-Focused Instructional Step Captioning (QFISC) task, the step
captions are generated by GPT4. Specifically, we provide the video captions
generated by the LLaVA-Next-Video model and the video subtitles with timestamps
as context, and ask GPT4 to generate step captions for the given medical query.
We only submit one run for evaluation and it obtains a F-score of 11.92 and
mean IoU of 9.6527.",2024-12-20 02:59:59+00:00,"['Jiaxin Wu', 'Yiyang Jiang', 'Xiao-Yong Wei', 'Qing Li']",http://arxiv.org/abs/2412.15514v1
OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving,"Understanding the evolution of 3D scenes is important for effective
autonomous driving. While conventional methods mode scene development with the
motion of individual instances, world models emerge as a generative framework
to describe the general scene dynamics. However, most existing methods adopt an
autoregressive framework to perform next-token prediction, which suffer from
inefficiency in modeling long-term temporal evolutions. To address this, we
propose a diffusion-based 4D occupancy generation model, OccSora, to simulate
the development of the 3D world for autonomous driving. We employ a 4D scene
tokenizer to obtain compact discrete spatial-temporal representations for 4D
occupancy input and achieve high-quality reconstruction for long-sequence
occupancy videos. We then learn a diffusion transformer on the spatial-temporal
representations and generate 4D occupancy conditioned on a trajectory prompt.
We conduct extensive experiments on the widely used nuScenes dataset with Occ3D
occupancy annotations. OccSora can generate 16s-videos with authentic 3D layout
and temporal consistency, demonstrating its ability to understand the spatial
and temporal distributions of driving scenes. With trajectory-aware 4D
generation, OccSora has the potential to serve as a world simulator for the
decision-making of autonomous driving. Code is available at:
https://github.com/wzzheng/OccSora.",2024-05-30 17:59:42+00:00,"['Lening Wang', 'Wenzhao Zheng', 'Yilong Ren', 'Han Jiang', 'Zhiyong Cui', 'Haiyang Yu', 'Jiwen Lu']",http://arxiv.org/abs/2405.20337v1
FAKER: Full-body Anonymization with Human Keypoint Extraction for Real-time Video Deidentification,"In the contemporary digital era, protection of personal information has
become a paramount issue. The exponential growth of the media industry has
heightened concerns regarding the anonymization of individuals captured in
video footage. Traditional methods, such as blurring or pixelation, are
commonly employed, while recent advancements have introduced generative
adversarial networks (GAN) to redraw faces in videos. In this study, we propose
a novel approach that employs a significantly smaller model to achieve
real-time full-body anonymization of individuals in videos. Unlike conventional
techniques that often fail to effectively remove personal identification
information such as skin color, clothing, accessories, and body shape while our
method successfully eradicates all such details. Furthermore, by leveraging
pose estimation algorithms, our approach accurately represents information
regarding individuals' positions, movements, and postures. This algorithm can
be seamlessly integrated into CCTV or IP camera systems installed in various
industrial settings, functioning in real-time and thus facilitating the
widespread adoption of full-body anonymization technology.",2024-08-06 04:59:23+00:00,"['Byunghyun Ban', 'Hyoseok Lee']",http://arxiv.org/abs/2408.11829v1
Benchmarking Multi-dimensional AIGC Video Quality Assessment: A Dataset and Unified Model,"In recent years, artificial intelligence (AI)-driven video generation has
gained significant attention. Consequently, there is a growing need for
accurate video quality assessment (VQA) metrics to evaluate the perceptual
quality of AI-generated content (AIGC) videos and optimize video generation
models. However, assessing the quality of AIGC videos remains a significant
challenge because these videos often exhibit highly complex distortions, such
as unnatural actions and irrational objects. To address this challenge, we
systematically investigate the AIGC-VQA problem, considering both subjective
and objective quality assessment perspectives. For the subjective perspective,
we construct the Large-scale Generated Video Quality assessment (LGVQ) dataset,
consisting of 2,808 AIGC videos generated by 6 video generation models using
468 carefully curated text prompts. We evaluate the perceptual quality of AIGC
videos from three critical dimensions: spatial quality, temporal quality, and
text-video alignment. For the objective perspective, we establish a benchmark
for evaluating existing quality assessment metrics on the LGVQ dataset. Our
findings show that current metrics perform poorly on this dataset, highlighting
a gap in effective evaluation tools. To bridge this gap, we propose the Unify
Generated Video Quality assessment (UGVQ) model, designed to accurately
evaluate the multi-dimensional quality of AIGC videos. The UGVQ model
integrates the visual and motion features of videos with the textual features
of their corresponding prompts, forming a unified quality-aware feature
representation tailored to AIGC videos. Experimental results demonstrate that
UGVQ achieves state-of-the-art performance on the LGVQ dataset across all three
quality dimensions. Both the LGVQ dataset and the UGVQ model are publicly
available on https://github.com/zczhang-sjtu/UGVQ.git.",2024-07-31 07:54:26+00:00,"['Zhichao Zhang', 'Wei Sun', 'Xinyue Li', 'Jun Jia', 'Xiongkuo Min', 'Zicheng Zhang', 'Chunyi Li', 'Zijian Chen', 'Puyi Wang', 'Fengyu Sun', 'Shangling Jui', 'Guangtao Zhai']",http://arxiv.org/abs/2407.21408v2
State-space Decomposition Model for Video Prediction Considering Long-term Motion Trend,"Stochastic video prediction enables the consideration of uncertainty in
future motion, thereby providing a better reflection of the dynamic nature of
the environment. Stochastic video prediction methods based on image
auto-regressive recurrent models need to feed their predictions back into the
latent space. Conversely, the state-space models, which decouple frame
synthesis and temporal prediction, proves to be more efficient. However,
inferring long-term temporal information about motion and generalizing to
dynamic scenarios under non-stationary assumptions remains an unresolved
challenge. In this paper, we propose a state-space decomposition stochastic
video prediction model that decomposes the overall video frame generation into
deterministic appearance prediction and stochastic motion prediction. Through
adaptive decomposition, the model's generalization capability to dynamic
scenarios is enhanced. In the context of motion prediction, obtaining a prior
on the long-term trend of future motion is crucial. Thus, in the stochastic
motion prediction branch, we infer the long-term motion trend from conditional
frames to guide the generation of future frames that exhibit high consistency
with the conditional frames. Experimental results demonstrate that our model
outperforms baselines on multiple datasets.",2024-04-17 17:19:48+00:00,"['Fei Cui', 'Jiaojiao Fang', 'Xiaojiang Wu', 'Zelong Lai', 'Mengke Yang', 'Menghan Jia', 'Guizhong Liu']",http://arxiv.org/abs/2404.11576v1
AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation,"Recent Diffusion Transformers (DiTs) have shown impressive capabilities in
generating high-quality single-modality content, including images, videos, and
audio. However, it is still under-explored whether the transformer-based
diffuser can efficiently denoise the Gaussian noises towards superb multimodal
content creation. To bridge this gap, we introduce AV-DiT, a novel and
efficient audio-visual diffusion transformer designed to generate high-quality,
realistic videos with both visual and audio tracks. To minimize model
complexity and computational costs, AV-DiT utilizes a shared DiT backbone
pre-trained on image-only data, with only lightweight, newly inserted adapters
being trainable. This shared backbone facilitates both audio and video
generation. Specifically, the video branch incorporates a trainable temporal
attention layer into a frozen pre-trained DiT block for temporal consistency.
Additionally, a small number of trainable parameters adapt the image-based DiT
block for audio generation. An extra shared DiT block, equipped with
lightweight parameters, facilitates feature interaction between audio and
visual modalities, ensuring alignment. Extensive experiments on the AIST++ and
Landscape datasets demonstrate that AV-DiT achieves state-of-the-art
performance in joint audio-visual generation with significantly fewer tunable
parameters. Furthermore, our results highlight that a single shared image
generative backbone with modality-specific adaptations is sufficient for
constructing a joint audio-video generator. Our source code and pre-trained
models will be released.",2024-06-11 20:05:58+00:00,"['Kai Wang', 'Shijian Deng', 'Jing Shi', 'Dimitrios Hatzinakos', 'Yapeng Tian']",http://arxiv.org/abs/2406.07686v1
SPAgent: Adaptive Task Decomposition and Model Selection for General Video Generation and Editing,"While open-source video generation and editing models have made significant
progress, individual models are typically limited to specific tasks, failing to
meet the diverse needs of users. Effectively coordinating these models can
unlock a wide range of video generation and editing capabilities. However,
manual coordination is complex and time-consuming, requiring users to deeply
understand task requirements and possess comprehensive knowledge of each
model's performance, applicability, and limitations, thereby increasing the
barrier to entry. To address these challenges, we propose a novel video
generation and editing system powered by our Semantic Planning Agent (SPAgent).
SPAgent bridges the gap between diverse user intents and the effective
utilization of existing generative models, enhancing the adaptability,
efficiency, and overall quality of video generation and editing. Specifically,
the SPAgent assembles a tool library integrating state-of-the-art open-source
image and video generation and editing models as tools. After fine-tuning on
our manually annotated dataset, SPAgent can automatically coordinate the tools
for video generation and editing, through our novelly designed three-step
framework: (1) decoupled intent recognition, (2) principle-guided route
planning, and (3) capability-based execution model selection. Additionally, we
enhance the SPAgent's video quality evaluation capability, enabling it to
autonomously assess and incorporate new video generation and editing models
into its tool library without human intervention. Experimental results
demonstrate that the SPAgent effectively coordinates models to generate or edit
videos, highlighting its versatility and adaptability across various video
tasks.",2024-11-28 08:07:32+00:00,"['Rong-Cheng Tu', 'Wenhao Sun', 'Zhao Jin', 'Jingyi Liao', 'Jiaxing Huang', 'Dacheng Tao']",http://arxiv.org/abs/2411.18983v1
Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion,"The text-guided video inpainting technique has significantly improved the
performance of content generation applications. A recent family for these
improvements uses diffusion models, which have become essential for achieving
high-quality video inpainting results, yet they still face performance
bottlenecks in temporal consistency and computational efficiency. This
motivates us to propose a new video inpainting framework using optical
Flow-guided Efficient Diffusion (FloED) for higher video coherence.
Specifically, FloED employs a dual-branch architecture, where the time-agnostic
flow branch restores corrupted flow first, and the multi-scale flow adapters
provide motion guidance to the main inpainting branch. Besides, a training-free
latent interpolation method is proposed to accelerate the multi-step denoising
process using flow warping. With the flow attention cache mechanism, FLoED
efficiently reduces the computational cost of incorporating optical flow.
Extensive experiments on background restoration and object removal tasks show
that FloED outperforms state-of-the-art diffusion-based methods in both quality
and efficiency. Our codes and models will be made publicly available.",2024-12-01 15:45:26+00:00,"['Bohai Gu', 'Hao Luo', 'Song Guo', 'Peiran Dong', 'Qihua Zhou']",http://arxiv.org/abs/2412.00857v3
Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering,"Learning object-centric representations from unsupervised videos is
challenging. Unlike most previous approaches that focus on decomposing 2D
images, we present a 3D generative model named DynaVol-S for dynamic scenes
that enables object-centric learning within a differentiable volume rendering
framework. The key idea is to perform object-centric voxelization to capture
the 3D nature of the scene, which infers per-object occupancy probabilities at
individual spatial locations. These voxel features evolve through a
canonical-space deformation function and are optimized in an inverse rendering
pipeline with a compositional NeRF. Additionally, our approach integrates 2D
semantic features to create 3D semantic grids, representing the scene through
multiple disentangled voxel grids. DynaVol-S significantly outperforms existing
models in both novel view synthesis and unsupervised decomposition tasks for
dynamic scenes. By jointly considering geometric structures and semantic
features, it effectively addresses challenging real-world scenarios involving
complex object interactions. Furthermore, once trained, the explicitly
meaningful voxel features enable additional capabilities that 2D scene
decomposition methods cannot achieve, such as novel scene generation through
editing geometric shapes or manipulating the motion trajectories of objects.",2024-07-30 15:33:58+00:00,"['Yanpeng Zhao', 'Yiwei Hao', 'Siyu Gao', 'Yunbo Wang', 'Xiaokang Yang']",http://arxiv.org/abs/2407.20908v2
Efficient Continuous Video Flow Model for Video Prediction,"Multi-step prediction models, such as diffusion and rectified flow models,
have emerged as state-of-the-art solutions for generation tasks. However, these
models exhibit higher latency in sampling new frames compared to single-step
methods. This latency issue becomes a significant bottleneck when adapting such
methods for video prediction tasks, given that a typical 60-second video
comprises approximately 1.5K frames. In this paper, we propose a novel approach
to modeling the multi-step process, aimed at alleviating latency constraints
and facilitating the adaptation of such processes for video prediction tasks.
Our approach not only reduces the number of sample steps required to predict
the next frame but also minimizes computational demands by reducing the model
size to one-third of the original size. We evaluate our method on standard
video prediction datasets, including KTH, BAIR action robot, Human3.6M and
UCF101, demonstrating its efficacy in achieving state-of-the-art performance on
these benchmarks.",2024-12-07 12:11:25+00:00,"['Gaurav Shrivastava', 'Abhinav Shrivastava']",http://arxiv.org/abs/2412.05633v1
What Makes A Video Radicalizing? Identifying Sources of Influence in QAnon Videos,"In recent years, radicalization is being increasingly attempted on
video-sharing platforms. Previous studies have been proposed to identify online
radicalization using generic social context analysis, without taking into
account comprehensive viewer traits and how those can affect viewers'
perception of radicalizing content. To address the challenge, we examine QAnon,
a conspiracy-based radicalizing group, and have designed a comprehensive
questionnaire aiming to understand viewers' perceptions of QAnon videos. We
outline the traits of viewers that QAnon videos are the most appealing to, and
identify influential factors that impact viewers' perception of the videos.",2024-04-22 22:55:31+00:00,"['Lin Ai', 'Yu-Wen Chen', 'Yuwen Yu', 'Seoyoung Kweon', 'Julia Hirschberg', 'Sarah Ita Levitan']",http://arxiv.org/abs/2404.14616v1
Pose-Guided Fine-Grained Sign Language Video Generation,"Sign language videos are an important medium for spreading and learning sign
language. However, most existing human image synthesis methods produce sign
language images with details that are distorted, blurred, or structurally
incorrect. They also produce sign language video frames with poor temporal
consistency, with anomalies such as flickering and abrupt detail changes
between the previous and next frames. To address these limitations, we propose
a novel Pose-Guided Motion Model (PGMM) for generating fine-grained and
motion-consistent sign language videos. Firstly, we propose a new Coarse Motion
Module (CMM), which completes the deformation of features by optical flow
warping, thus transfering the motion of coarse-grained structures without
changing the appearance; Secondly, we propose a new Pose Fusion Module (PFM),
which guides the modal fusion of RGB and pose features, thus completing the
fine-grained generation. Finally, we design a new metric, Temporal Consistency
Difference (TCD) to quantitatively assess the degree of temporal consistency of
a video by comparing the difference between the frames of the reconstructed
video and the previous and next frames of the target video. Extensive
qualitative and quantitative experiments show that our method outperforms
state-of-the-art methods in most benchmark tests, with visible improvements in
details and temporal consistency.",2024-09-25 07:54:53+00:00,"['Tongkai Shi', 'Lianyu Hu', 'Fanhua Shang', 'Jichao Feng', 'Peidong Liu', 'Wei Feng']",http://arxiv.org/abs/2409.16709v1
When Video Coding Meets Multimodal Large Language Models: A Unified Paradigm for Video Coding,"Existing codecs are designed to eliminate intrinsic redundancies to create a
compact representation for compression. However, strong external priors from
Multimodal Large Language Models (MLLMs) have not been explicitly explored in
video compression. Herein, we introduce a unified paradigm for Cross-Modality
Video Coding (CMVC), which is a pioneering approach to explore multimodality
representation and video generative models in video coding. Specifically, on
the encoder side, we disentangle a video into spatial content and motion
components, which are subsequently transformed into distinct modalities to
achieve very compact representation by leveraging MLLMs. During decoding,
previously encoded components and video generation models are leveraged to
create multiple encoding-decoding modes that optimize video reconstruction
quality for specific decoding requirements, including Text-Text-to-Video (TT2V)
mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)
mode to achieve superb perceptual consistency. In addition, we propose an
efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)
tuning to guarantee perceptual quality, which allows the generated motion cues
to behave smoothly. Experiments on benchmarks indicate that TT2V achieves
effective semantic reconstruction, while IT2V exhibits competitive perceptual
consistency. These results highlight potential directions for future research
in video coding.",2024-08-15 11:36:18+00:00,"['Pingping Zhang', 'Jinlong Li', 'Kecheng Chen', 'Meng Wang', 'Long Xu', 'Haoliang Li', 'Nicu Sebe', 'Sam Kwong', 'Shiqi Wang']",http://arxiv.org/abs/2408.08093v3
Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content,"As visual generation technologies continue to advance, the scale of video
datasets has expanded rapidly, and the quality of these datasets is critical to
the performance of video generation models. We argue that temporal splitting,
detailed captions, and video quality filtering are three key factors that
determine dataset quality. However, existing datasets exhibit various
limitations in these areas. To address these challenges, we introduce
Koala-36M, a large-scale, high-quality video dataset featuring accurate
temporal splitting, detailed captions, and superior video quality. The core of
our approach lies in improving the consistency between fine-grained conditions
and video content. Specifically, we employ a linear classifier on probability
distributions to enhance the accuracy of transition detection, ensuring better
temporal consistency. We then provide structured captions for the splitted
videos, with an average length of 200 words, to improve text-video alignment.
Additionally, we develop a Video Training Suitability Score (VTSS) that
integrates multiple sub-metrics, allowing us to filter high-quality videos from
the original corpus. Finally, we incorporate several metrics into the training
process of the generation model, further refining the fine-grained conditions.
Our experiments demonstrate the effectiveness of our data processing pipeline
and the quality of the proposed Koala-36M dataset. Our dataset and code will be
released at https://koala36m.github.io/.",2024-10-10 17:57:49+00:00,"['Qiuheng Wang', 'Yukai Shi', 'Jiarong Ou', 'Rui Chen', 'Ke Lin', 'Jiahao Wang', 'Boyuan Jiang', 'Haotian Yang', 'Mingwu Zheng', 'Xin Tao', 'Fei Yang', 'Pengfei Wan', 'Di Zhang']",http://arxiv.org/abs/2410.08260v1
Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention,"Generating multi-view videos for autonomous driving training has recently
gained much attention, with the challenge of addressing both cross-view and
cross-frame consistency. Existing methods typically apply decoupled attention
mechanisms for spatial, temporal, and view dimensions. However, these
approaches often struggle to maintain consistency across dimensions,
particularly when handling fast-moving objects that appear at different times
and viewpoints. In this paper, we present CogDriving, a novel network designed
for synthesizing high-quality multi-view driving videos. CogDriving leverages a
Diffusion Transformer architecture with holistic-4D attention modules, enabling
simultaneous associations across the spatial, temporal, and viewpoint
dimensions. We also propose a lightweight controller tailored for CogDriving,
i.e., Micro-Controller, which uses only 1.1% of the parameters of the standard
ControlNet, enabling precise control over Bird's-Eye-View layouts. To enhance
the generation of object instances crucial for autonomous driving, we propose a
re-weighted learning objective, dynamically adjusting the learning weights for
object instances during training. CogDriving demonstrates strong performance on
the nuScenes validation set, achieving an FVD score of 37.8, highlighting its
ability to generate realistic driving videos. The project can be found at
https://luhannan.github.io/CogDrivingPage/.",2024-12-04 18:02:49+00:00,"['Hannan Lu', 'Xiaohe Wu', 'Shudong Wang', 'Xiameng Qin', 'Xinyu Zhang', 'Junyu Han', 'Wangmeng Zuo', 'Ji Tao']",http://arxiv.org/abs/2412.03520v2
Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis,"Recent advances in diffusion models have revolutionized audio-driven talking
head synthesis. Beyond precise lip synchronization, diffusion-based methods
excel in generating subtle expressions and natural head movements that are
well-aligned with the audio signal. However, these methods are confronted by
slow inference speed, insufficient fine-grained control over facial motions,
and occasional visual artifacts largely due to an implicit latent space derived
from Variational Auto-Encoders (VAE), which prevent their adoption in realtime
interaction applications. To address these issues, we introduce Ditto, a
diffusion-based framework that enables controllable realtime talking head
synthesis. Our key innovation lies in bridging motion generation and
photorealistic neural rendering through an explicit identity-agnostic motion
space, replacing conventional VAE representations. This design substantially
reduces the complexity of diffusion learning while enabling precise control
over the synthesized talking heads. We further propose an inference strategy
that jointly optimizes three key components: audio feature extraction, motion
generation, and video synthesis. This optimization enables streaming
processing, realtime inference, and low first-frame delay, which are the
functionalities crucial for interactive applications such as AI assistants.
Extensive experimental results demonstrate that Ditto generates compelling
talking head videos and substantially outperforms existing methods in both
motion control and realtime performance.",2024-11-29 07:01:31+00:00,"['Tianqi Li', 'Ruobing Zheng', 'Minghui Yang', 'Jingdong Chen', 'Ming Yang']",http://arxiv.org/abs/2411.19509v2
Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward,"Preference modeling techniques, such as direct preference optimization (DPO),
has shown effective in enhancing the generalization abilities of large language
model (LLM). However, in tasks involving video instruction-following, providing
informative feedback, especially for detecting hallucinations in generated
responses, remains a significant challenge. Previous studies have explored
using large large multimodal models (LMMs) as reward models to guide preference
modeling, but their ability to accurately assess the factuality of generated
responses compared to corresponding videos has not been conclusively
established. This paper introduces a novel framework that utilizes detailed
video captions as a proxy of video content, enabling language models to
incorporate this information as supporting evidence for scoring video Question
Answering (QA) predictions. Our approach demonstrates robust alignment with
OpenAI GPT-4V model's reward mechanism, which directly takes video frames as
input. Furthermore, we show that applying this tailored reward through DPO
significantly improves the performance of video LMMs on video QA tasks.",2024-04-01 17:28:16+00:00,"['Ruohong Zhang', 'Liangke Gui', 'Zhiqing Sun', 'Yihao Feng', 'Keyang Xu', 'Yuanhan Zhang', 'Di Fu', 'Chunyuan Li', 'Alexander Hauptmann', 'Yonatan Bisk', 'Yiming Yang']",http://arxiv.org/abs/2404.01258v2
SVASTIN: Sparse Video Adversarial Attack via Spatio-Temporal Invertible Neural Networks,"Robust and imperceptible adversarial video attack is challenging due to the
spatial and temporal characteristics of videos. The existing video adversarial
attack methods mainly take a gradient-based approach and generate adversarial
videos with noticeable perturbations. In this paper, we propose a novel Sparse
Adversarial Video Attack via Spatio-Temporal Invertible Neural Networks
(SVASTIN) to generate adversarial videos through spatio-temporal feature space
information exchanging. It consists of a Guided Target Video Learning (GTVL)
module to balance the perturbation budget and optimization speed and a
Spatio-Temporal Invertible Neural Network (STIN) module to perform
spatio-temporal feature space information exchanging between a source video and
the target feature tensor learned by GTVL module. Extensive experiments on
UCF-101 and Kinetics-400 demonstrate that our proposed SVASTIN can generate
adversarial examples with higher imperceptibility than the state-of-the-art
methods with the higher fooling rate. Code is available at
\href{https://github.com/Brittany-Chen/SVASTIN}{https://github.com/Brittany-Chen/SVASTIN}.",2024-06-04 01:58:32+00:00,"['Yi Pan', 'Jun-Jie Huang', 'Zihan Chen', 'Wentao Zhao', 'Ziyue Wang']",http://arxiv.org/abs/2406.01894v1
InterDyn: Controllable Interactive Dynamics with Video Diffusion Models,"Predicting the dynamics of interacting objects is essential for both humans
and intelligent systems. However, existing approaches are limited to
simplified, toy settings and lack generalizability to complex, real-world
environments. Recent advances in generative models have enabled the prediction
of state transitions based on interventions, but focus on generating a single
future state which neglects the continuous dynamics resulting from the
interaction. To address this gap, we propose InterDyn, a novel framework that
generates videos of interactive dynamics given an initial frame and a control
signal encoding the motion of a driving object or actor. Our key insight is
that large video generation models can act as both neural renderers and
implicit physics simulators, having learned interactive dynamics from
large-scale video data. To effectively harness this capability, we introduce an
interactive control mechanism that conditions the video generation process on
the motion of the driving entity. Qualitative results demonstrate that InterDyn
generates plausible, temporally consistent videos of complex object
interactions while generalizing to unseen objects. Quantitative evaluations
show that InterDyn outperforms baselines that focus on static state
transitions. This work highlights the potential of leveraging video generative
models as implicit physics engines. Code and trained models will be released
at: https://interdyn.is.tue.mpg.de/",2024-12-16 13:57:02+00:00,"['Rick Akkerman', 'Haiwen Feng', 'Michael J. Black', 'Dimitrios Tzionas', 'Victoria Fernndez Abrevaya']",http://arxiv.org/abs/2412.11785v2
Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization,"Generative Adversarial Networks (GANs) have been at the forefront of image
synthesis, especially in medical fields like histopathology, where they help
address challenges such as data scarcity, patient privacy, and class imbalance.
However, several inherent and domain-specific issues remain. For GANs, training
instability, mode collapse, and insufficient feedback from binary
classification can undermine performance. These challenges are particularly
pronounced with high-resolution histopathology images due to their complex
feature representation and high spatial detail. In response to these
challenges, this work proposes a novel framework integrating a contrastive
learning-based Multistage Progressive Finetuning Siamese Neural Network
(MFT-SNN) with a Reinforcement Learning-based External Optimizer (RL-EO). The
MFT-SNN improves feature similarity extraction in histopathology data, while
the RL-EO acts as a reward-based guide to balance GAN training, addressing mode
collapse and enhancing output quality. The proposed approach is evaluated
against state-of-the-art (SOTA) GAN models and demonstrates superior
performance across multiple metrics.",2024-09-30 14:39:56+00:00,['Osama Mustafa'],http://arxiv.org/abs/2409.20340v3
ReVideo: Remake a Video with Motion and Content Control,"Despite significant advancements in video generation and editing using
diffusion models, achieving accurate and localized video editing remains a
substantial challenge. Additionally, most existing video editing methods
primarily focus on altering visual content, with limited research dedicated to
motion editing. In this paper, we present a novel attempt to Remake a Video
(ReVideo) which stands out from existing methods by allowing precise video
editing in specific areas through the specification of both content and motion.
Content editing is facilitated by modifying the first frame, while the
trajectory-based motion control offers an intuitive user interaction
experience. ReVideo addresses a new task involving the coupling and training
imbalance between content and motion control. To tackle this, we develop a
three-stage training strategy that progressively decouples these two aspects
from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion
module to integrate content and motion control across various sampling steps
and spatial locations. Extensive experiments demonstrate that our ReVideo has
promising performance on several accurate video editing applications, i.e., (1)
locally changing video content while keeping the motion constant, (2) keeping
content unchanged and customizing new motion trajectories, (3) modifying both
content and motion trajectories. Our method can also seamlessly extend these
applications to multi-area editing without specific training, demonstrating its
flexibility and robustness.",2024-05-22 17:46:08+00:00,"['Chong Mou', 'Mingdeng Cao', 'Xintao Wang', 'Zhaoyang Zhang', 'Ying Shan', 'Jian Zhang']",http://arxiv.org/abs/2405.13865v1
VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format,"Recent researches on video large language models (VideoLLM) predominantly
focus on model architectures and training datasets, leaving the interaction
format between the user and the model under-explored. In existing works, users
often interact with VideoLLMs by using the entire video and a query as input,
after which the model generates a response. This interaction format constrains
the application of VideoLLMs in scenarios such as live-streaming comprehension
where videos do not end and responses are required in a real-time manner, and
also results in unsatisfactory performance on time-sensitive tasks that
requires localizing video segments. In this paper, we focus on a video-text
duet interaction format. This interaction format is characterized by the
continuous playback of the video, and both the user and the model can insert
their text messages at any position during the video playback. When a text
message ends, the video continues to play, akin to the alternative of two
performers in a duet. We construct MMDuetIT, a video-text training dataset
designed to adapt VideoLLMs to video-text duet interaction format. We also
introduce the Multi-Answer Grounded Video Question Answering (MAGQA) task to
benchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT,
MMDuet demonstrates that adopting the video-text duet interaction format
enables the model to achieve significant improvements in various time-sensitive
tasks (76% CIDEr on YouCook2 dense video captioning, 90\% mAP on QVHighlights
highlight detection and 25% R@0.5 on Charades-STA temporal video grounding)
with minimal training efforts, and also enable VideoLLMs to reply in a
real-time manner as the video plays. Code, data and demo are available at:
https://github.com/yellow-binary-tree/MMDuet.",2024-11-27 02:15:34+00:00,"['Yueqian Wang', 'Xiaojun Meng', 'Yuxuan Wang', 'Jianxin Liang', 'Jiansheng Wei', 'Huishuai Zhang', 'Dongyan Zhao']",http://arxiv.org/abs/2411.17991v1
Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis,"We propose to synthesize high-quality and synchronized audio, given video and
optional text conditions, using a novel multimodal joint training framework
MMAudio. In contrast to single-modality training conditioned on (limited) video
data only, MMAudio is jointly trained with larger-scale, readily available
text-audio data to learn to generate semantically aligned high-quality audio
samples. Additionally, we improve audio-visual synchrony with a conditional
synchronization module that aligns video conditions with audio latents at the
frame level. Trained with a flow matching objective, MMAudio achieves new
video-to-audio state-of-the-art among public models in terms of audio quality,
semantic alignment, and audio-visual synchronization, while having a low
inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio
also achieves surprisingly competitive performance in text-to-audio generation,
showing that joint training does not hinder single-modality performance. Code
and demo are available at: https://hkchengrex.github.io/MMAudio",2024-12-19 18:59:55+00:00,"['Ho Kei Cheng', 'Masato Ishii', 'Akio Hayakawa', 'Takashi Shibuya', 'Alexander Schwing', 'Yuki Mitsufuji']",http://arxiv.org/abs/2412.15322v1
Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation,"How can robot manipulation policies generalize to novel tasks involving
unseen object types and new motions? In this paper, we provide a solution in
terms of predicting motion information from web data through human video
generation and conditioning a robot policy on the generated video. Instead of
attempting to scale robot data collection which is expensive, we show how we
can leverage video generation models trained on easily available web data, for
enabling generalization. Our approach Gen2Act casts language-conditioned
manipulation as zero-shot human video generation followed by execution with a
single policy conditioned on the generated video. To train the policy, we use
an order of magnitude less robot interaction data compared to what the video
prediction model was trained on. Gen2Act doesn't require fine-tuning the video
model at all and we directly use a pre-trained model for generating human
videos. Our results on diverse real-world scenarios show how Gen2Act enables
manipulating unseen object types and performing novel motions for tasks not
present in the robot data. Videos are at https://homangab.github.io/gen2act/",2024-09-24 17:57:33+00:00,"['Homanga Bharadhwaj', 'Debidatta Dwibedi', 'Abhinav Gupta', 'Shubham Tulsiani', 'Carl Doersch', 'Ted Xiao', 'Dhruv Shah', 'Fei Xia', 'Dorsa Sadigh', 'Sean Kirmani']",http://arxiv.org/abs/2409.16283v1
Comp4D: LLM-Guided Compositional 4D Scene Generation,"Recent advancements in diffusion models for 2D and 3D content creation have
sparked a surge of interest in generating 4D content. However, the scarcity of
3D scene datasets constrains current methodologies to primarily object-centric
generation. To overcome this limitation, we present Comp4D, a novel framework
for Compositional 4D Generation. Unlike conventional methods that generate a
singular 4D representation of the entire scene, Comp4D innovatively constructs
each 4D object within the scene separately. Utilizing Large Language Models
(LLMs), the framework begins by decomposing an input text prompt into distinct
entities and maps out their trajectories. It then constructs the compositional
4D scene by accurately positioning these objects along their designated paths.
To refine the scene, our method employs a compositional score distillation
technique guided by the pre-defined trajectories, utilizing pre-trained
diffusion models across text-to-image, text-to-video, and text-to-3D domains.
Extensive experiments demonstrate our outstanding 4D content creation
capability compared to prior arts, showcasing superior visual quality, motion
fidelity, and enhanced object interactions.",2024-03-25 17:55:52+00:00,"['Dejia Xu', 'Hanwen Liang', 'Neel P. Bhatt', 'Hezhen Hu', 'Hanxue Liang', 'Konstantinos N. Plataniotis', 'Zhangyang Wang']",http://arxiv.org/abs/2403.16993v1
Fundus to Fluorescein Angiography Video Generation as a Retinal Generative Foundation Model,"Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoring
retinal vascular issues but is limited by its invasive nature and restricted
accessibility compared to color fundus (CF) imaging. Existing methods that
convert CF images to FFA are confined to static image generation, missing the
dynamic lesional changes. We introduce Fundus2Video, an autoregressive
generative adversarial network (GAN) model that generates dynamic FFA videos
from single CF images. Fundus2Video excels in video generation, achieving an
FVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated the
fidelity of the generated videos. Additionally, the model's generator
demonstrates remarkable downstream transferability across ten external public
datasets, including blood vessel segmentation, retinal disease diagnosis,
systemic disease prediction, and multimodal retrieval, showcasing impressive
zero-shot and few-shot capabilities. These findings position Fundus2Video as a
powerful, non-invasive alternative to FFA exams and a versatile retinal
generative foundation model that captures both static and temporal retinal
features, enabling the representation of complex inter-modality relationships.",2024-10-17 05:53:13+00:00,"['Weiyi Zhang', 'Jiancheng Yang', 'Ruoyu Chen', 'Siyu Huang', 'Pusheng Xu', 'Xiaolan Chen', 'Shanfu Lu', 'Hongyu Cao', 'Mingguang He', 'Danli Shi']",http://arxiv.org/abs/2410.13242v2
Segment Anything for Videos: A Systematic Survey,"The recent wave of foundation models has witnessed tremendous success in
computer vision (CV) and beyond, with the segment anything model (SAM) having
sparked a passion for exploring task-agnostic visual foundation models.
Empowered by its remarkable zero-shot generalization, SAM is currently
challenging numerous traditional paradigms in CV, delivering extraordinary
performance not only in various image segmentation and multi-modal segmentation
(\eg, text-to-mask) tasks, but also in the video domain. Additionally, the
latest released SAM 2 is once again sparking research enthusiasm in the realm
of promptable visual segmentation for both images and videos. However, existing
surveys mainly focus on SAM in various image processing tasks, a comprehensive
and in-depth review in the video domain is notably absent. To address this gap,
this work conducts a systematic review on SAM for videos in the era of
foundation models. As the first to review the progress of SAM for videos, this
work focuses on its applications to various tasks by discussing its recent
advances, and innovation opportunities of developing foundation models on broad
applications. We begin with a brief introduction to the background of SAM and
video-related research domains. Subsequently, we present a systematic taxonomy
that categorizes existing methods into three key areas: video understanding,
video generation, and video editing, analyzing and summarizing their advantages
and limitations. Furthermore, comparative results of SAM-based and current
state-of-the-art methods on representative benchmarks, as well as insightful
analysis are offered. Finally, we discuss the challenges faced by current
research and envision several future research directions in the field of SAM
for video and beyond.",2024-07-31 02:24:53+00:00,"['Chunhui Zhang', 'Yawen Cui', 'Weilin Lin', 'Guanjie Huang', 'Yan Rong', 'Li Liu', 'Shiguang Shan']",http://arxiv.org/abs/2408.08315v1
IF-MDM: Implicit Face Motion Diffusion Model for High-Fidelity Realtime Talking Head Generation,"We introduce a novel approach for high-resolution talking head generation
from a single image and audio input. Prior methods using explicit face models,
like 3D morphable models (3DMM) and facial landmarks, often fall short in
generating high-fidelity videos due to their lack of appearance-aware motion
representation. While generative approaches such as video diffusion models
achieve high video quality, their slow processing speeds limit practical
application. Our proposed model, Implicit Face Motion Diffusion Model (IF-MDM),
employs implicit motion to encode human faces into appearance-aware compressed
facial latents, enhancing video generation. Although implicit motion lacks the
spatial disentanglement of explicit models, which complicates alignment with
subtle lip movements, we introduce motion statistics to help capture
fine-grained motion information. Additionally, our model provides motion
controllability to optimize the trade-off between motion intensity and visual
quality during inference. IF-MDM supports real-time generation of 512x512
resolution videos at up to 45 frames per second (fps). Extensive evaluations
demonstrate its superior performance over existing diffusion and explicit face
models. The code will be released publicly, available alongside supplementary
materials. The video results can be found on
https://bit.ly/ifmdm_supplementary.",2024-12-05 09:20:48+00:00,"['Sejong Yang', 'Seoung Wug Oh', 'Yang Zhou', 'Seon Joo Kim']",http://arxiv.org/abs/2412.04000v2
WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs,"Several text-to-video diffusion models have demonstrated commendable
capabilities in synthesizing high-quality video content. However, it remains a
formidable challenge pertaining to maintaining temporal consistency and
ensuring action smoothness throughout the generated sequences. In this paper,
we present an innovative video generation AI agent that harnesses the power of
Sora-inspired multimodal learning to build skilled world models framework based
on textual prompts and accompanying images. The framework includes two parts:
prompt enhancer and full video translation. The first part employs the
capabilities of ChatGPT to meticulously distill and proactively construct
precise prompts for each subsequent step, thereby guaranteeing the utmost
accuracy in prompt communication and accurate execution in following model
operations. The second part employ compatible with existing advanced diffusion
techniques to expansively generate and refine the key frame at the conclusion
of a video. Then we can expertly harness the power of leading and trailing key
frames to craft videos with enhanced temporal consistency and action
smoothness. The experimental results confirm that our method has strong
effectiveness and novelty in constructing world models from text and image
inputs over the other methods.",2024-03-10 16:09:02+00:00,"['Deshun Yang', 'Luhui Hu', 'Yu Tian', 'Zihao Li', 'Chris Kelly', 'Bang Yang', 'Cindy Yang', 'Yuexian Zou']",http://arxiv.org/abs/2403.07944v1
Mimir: Improving Video Diffusion Models for Precise Text Understanding,"Text serves as the key control signal in video generation due to its
narrative nature. To render text descriptions into video clips, current video
diffusion models borrow features from text encoders yet struggle with limited
text comprehension. The recent success of large language models (LLMs)
showcases the power of decoder-only transformers, which offers three clear
benefits for text-to-video (T2V) generation, namely, precise text understanding
resulting from the superior scalability, imagination beyond the input text
enabled by next token prediction, and flexibility to prioritize user interests
through instruction tuning. Nevertheless, the feature distribution gap emerging
from the two different text modeling paradigms hinders the direct use of LLMs
in established T2V models. This work addresses this challenge with Mimir, an
end-to-end training framework featuring a carefully tailored token fuser to
harmonize the outputs from text encoders and LLMs. Such a design allows the T2V
model to fully leverage learned video priors while capitalizing on the
text-related capability of LLMs. Extensive quantitative and qualitative results
demonstrate the effectiveness of Mimir in generating high-quality videos with
excellent text comprehension, especially when processing short captions and
managing shifting motions. Project page:
https://lucaria-academy.github.io/Mimir/",2024-12-04 07:26:44+00:00,"['Shuai Tan', 'Biao Gong', 'Yutong Feng', 'Kecheng Zheng', 'Dandan Zheng', 'Shuwei Shi', 'Yujun Shen', 'Jingdong Chen', 'Ming Yang']",http://arxiv.org/abs/2412.03085v1
RAIN: Real-time Animation of Infinite Video Stream,"Live animation has gained immense popularity for enhancing online engagement,
yet achieving high-quality, real-time, and stable animation with diffusion
models remains challenging, especially on consumer-grade GPUs. Existing methods
struggle with generating long, consistent video streams efficiently, often
being limited by latency issues and degraded visual quality over extended
periods. In this paper, we introduce RAIN, a pipeline solution capable of
animating infinite video streams in real-time with low latency using a single
RTX 4090 GPU. The core idea of RAIN is to efficiently compute frame-token
attention across different noise levels and long time-intervals while
simultaneously denoising a significantly larger number of frame-tokens than
previous stream-based methods. This design allows RAIN to generate video frames
with much shorter latency and faster speed, while maintaining long-range
attention over extended video streams, resulting in enhanced continuity and
consistency. Consequently, a Stable Diffusion model fine-tuned with RAIN in
just a few epochs can produce video streams in real-time and low latency
without much compromise in quality or consistency, up to infinite long. Despite
its advanced capabilities, the RAIN only introduces a few additional 1D
attention blocks, imposing minimal additional burden. Experiments in benchmark
datasets and generating super-long videos demonstrating that RAIN can animate
characters in real-time with much better quality, accuracy, and consistency
than competitors while costing less latency. All code and models will be made
publicly available.",2024-12-27 07:13:15+00:00,"['Zhilei Shu', 'Ruili Feng', 'Yang Cao', 'Zheng-Jun Zha']",http://arxiv.org/abs/2412.19489v1
RoMo: Robust Motion Segmentation Improves Structure from Motion,"There has been extensive progress in the reconstruction and generation of 4D
scenes from monocular casually-captured video. While these tasks rely heavily
on known camera poses, the problem of finding such poses using
structure-from-motion (SfM) often depends on robustly separating static from
dynamic parts of a video. The lack of a robust solution to this problem limits
the performance of SfM camera-calibration pipelines. We propose a novel
approach to video-based motion segmentation to identify the components of a
scene that are moving w.r.t. a fixed world frame. Our simple but effective
iterative method, RoMo, combines optical flow and epipolar cues with a
pre-trained video segmentation model. It outperforms unsupervised baselines for
motion segmentation as well as supervised baselines trained from synthetic
data. More importantly, the combination of an off-the-shelf SfM pipeline with
our segmentation masks establishes a new state-of-the-art on camera calibration
for scenes with dynamic content, outperforming existing methods by a
substantial margin.",2024-11-27 01:09:56+00:00,"['Lily Goli', 'Sara Sabour', 'Mark Matthews', 'Marcus Brubaker', 'Dmitry Lagun', 'Alec Jacobson', 'David J. Fleet', 'Saurabh Saxena', 'Andrea Tagliasacchi']",http://arxiv.org/abs/2411.18650v1
Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching Considering the Diversity of the Video,"Current video summarization methods rely heavily on supervised computer
vision techniques, which demands time-consuming and subjective manual
annotations. To overcome these limitations, we investigated self-supervised
video summarization. Inspired by the success of Large Language Models (LLMs),
we explored the feasibility in transforming the video summarization task into a
Natural Language Processing (NLP) task. By leveraging the advantages of LLMs in
context understanding, we aim to enhance the effectiveness of self-supervised
video summarization. Our method begins by generating captions for individual
video frames, which are then synthesized into text summaries by LLMs.
Subsequently, we measure semantic distance between the captions and the text
summary. Notably, we propose a novel loss function to optimize our model
according to the diversity of the video. Finally, the summarized video can be
generated by selecting the frames with captions similar to the text summary.
Our method achieves state-of-the-art performance on the SumMe dataset in rank
correlation coefficients. In addition, our method has a novel feature of being
able to achieve personalized summarization.",2024-05-14 18:07:04+00:00,"['Tomoya Sugihara', 'Shuntaro Masuda', 'Ling Xiao', 'Toshihiko Yamasaki']",http://arxiv.org/abs/2405.08890v2
VidGen-1M: A Large-Scale Dataset for Text-to-video Generation,"The quality of video-text pairs fundamentally determines the upper bound of
text-to-video models. Currently, the datasets used for training these models
suffer from significant shortcomings, including low temporal consistency,
poor-quality captions, substandard video quality, and imbalanced data
distribution. The prevailing video curation process, which depends on image
models for tagging and manual rule-based curation, leads to a high
computational load and leaves behind unclean data. As a result, there is a lack
of appropriate training datasets for text-to-video models. To address this
problem, we present VidGen-1M, a superior training dataset for text-to-video
models. Produced through a coarse-to-fine curation strategy, this dataset
guarantees high-quality videos and detailed captions with excellent temporal
consistency. When used to train the video generation model, this dataset has
led to experimental results that surpass those obtained with other models.",2024-08-05 16:53:23+00:00,"['Zhiyu Tan', 'Xiaomeng Yang', 'Luozheng Qin', 'Hao Li']",http://arxiv.org/abs/2408.02629v1
SDI-Paste: Synthetic Dynamic Instance Copy-Paste for Video Instance Segmentation,"Data augmentation methods such as Copy-Paste have been studied as effective
ways to expand training datasets while incurring minimal costs. While such
methods have been extensively implemented for image level tasks, we found no
scalable implementation of Copy-Paste built specifically for video tasks. In
this paper, we leverage the recent growth in video fidelity of generative
models to explore effective ways of incorporating synthetically generated
objects into existing video datasets to artificially expand object instance
pools. We first procure synthetic video sequences featuring objects that morph
dynamically with time. Our carefully devised pipeline automatically segments
then copy-pastes these dynamic instances across the frames of any target
background video sequence. We name our video data augmentation pipeline
Synthetic Dynamic Instance Copy-Paste, and test it on the complex task of Video
Instance Segmentation which combines detection, segmentation and tracking of
object instances across a video sequence. Extensive experiments on the popular
Youtube-VIS 2021 dataset using two separate popular networks as baselines
achieve strong gains of +2.9 AP (6.5%) and +2.1 AP (4.9%). We make our code and
models publicly available.",2024-10-16 12:11:34+00:00,"['Sahir Shrestha', 'Weihao Li', 'Gao Zhu', 'Nick Barnes']",http://arxiv.org/abs/2410.13565v1
Video Repurposing from User Generated Content: A Large-scale Dataset and Benchmark,"The demand for producing short-form videos for sharing on social media
platforms has experienced significant growth in recent times. Despite notable
advancements in the fields of video summarization and highlight detection,
which can create partially usable short films from raw videos, these approaches
are often domain-specific and require an in-depth understanding of real-world
video content. To tackle this predicament, we propose Repurpose-10K, an
extensive dataset comprising over 10,000 videos with more than 120,000
annotated clips aimed at resolving the video long-to-short task. Recognizing
the inherent constraints posed by untrained human annotators, which can result
in inaccurate annotations for repurposed videos, we propose a two-stage
solution to obtain annotations from real-world user-generated content.
Furthermore, we offer a baseline model to address this challenging task by
integrating audio, visual, and caption aspects through a cross-modal fusion and
alignment framework. We aspire for our work to ignite groundbreaking research
in the lesser-explored realms of video repurposing.",2024-12-12 02:27:46+00:00,"['Yongliang Wu', 'Wenbo Zhu', 'Jiawang Cao', 'Yi Lu', 'Bozheng Li', 'Weiheng Chi', 'Zihan Qiu', 'Lirian Su', 'Haolin Zheng', 'Jay Wu', 'Xu Yang']",http://arxiv.org/abs/2412.08879v2
Disentangled Motion Modeling for Video Frame Interpolation,"Video Frame Interpolation (VFI) aims to synthesize intermediate frames
between existing frames to enhance visual smoothness and quality. Beyond the
conventional methods based on the reconstruction loss, recent works have
employed generative models for improved perceptual quality. However, they
require complex training and large computational costs for pixel space
modeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a
diffusion-based approach for VFI that enhances visual quality by focusing on
intermediate motion modeling. We propose a disentangled two-stage training
process. In the initial stage, frame synthesis and flow models are trained to
generate accurate frames and flows optimal for synthesis. In the subsequent
stage, we introduce a motion diffusion model, which incorporates our novel
U-Net architecture specifically designed for optical flow, to generate
bi-directional flows between frames. By learning the simpler low-frequency
representation of motions, MoMo achieves superior perceptual quality with
reduced computational demands compared to the generative modeling methods on
the pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics
across various benchmarks, demonstrating its efficacy and efficiency in VFI.",2024-06-25 03:50:20+00:00,"['Jaihyun Lew', 'Jooyoung Choi', 'Chaehun Shin', 'Dahuin Jung', 'Sungroh Yoon']",http://arxiv.org/abs/2406.17256v2
A General Method to Incorporate Spatial Information into Loss Functions for GAN-based Super-resolution Models,"Generative Adversarial Networks (GANs) have shown great performance on
super-resolution problems since they can generate more visually realistic
images and video frames. However, these models often introduce side effects
into the outputs, such as unexpected artifacts and noises. To reduce these
artifacts and enhance the perceptual quality of the results, in this paper, we
propose a general method that can be effectively used in most GAN-based
super-resolution (SR) models by introducing essential spatial information into
the training process. We extract spatial information from the input data and
incorporate it into the training loss, making the corresponding loss a
spatially adaptive (SA) one. After that, we utilize it to guide the training
process. We will show that the proposed approach is independent of the methods
used to extract the spatial information and independent of the SR tasks and
models. This method consistently guides the training process towards generating
visually pleasing SR images and video frames, substantially mitigating
artifacts and noise, ultimately leading to enhanced perceptual quality.",2024-03-15 17:29:16+00:00,"['Xijun Wang', 'Santiago Lpez-Tapia', 'Alice Lucas', 'Xinyi Wu', 'Rafael Molina', 'Aggelos K. Katsaggelos']",http://arxiv.org/abs/2403.10589v1
FADE: A Dataset for Detecting Falling Objects around Buildings in Video,"Falling objects from buildings can cause severe injuries to pedestrians due
to the great impact force they exert. Although surveillance cameras are
installed around some buildings, it is challenging for humans to capture such
events in surveillance videos due to the small size and fast motion of falling
objects, as well as the complex background. Therefore, it is necessary to
develop methods to automatically detect falling objects around buildings in
surveillance videos. To facilitate the investigation of falling object
detection, we propose a large, diverse video dataset called FADE (FAlling
Object DEtection around Buildings) for the first time. FADE contains 1,881
videos from 18 scenes, featuring 8 falling object categories, 4 weather
conditions, and 4 video resolutions. Additionally, we develop a new object
detection method called FADE-Net, which effectively leverages motion
information and produces small-sized but high-quality proposals for detecting
falling objects around buildings. Importantly, our method is extensively
evaluated and analyzed by comparing it with the previous approaches used for
generic object detection, video object detection, and moving object detection
on the FADE dataset. Experimental results show that the proposed FADE-Net
significantly outperforms other methods, providing an effective baseline for
future research. The dataset and code are publicly available at
https://fadedataset.github.io/FADE.github.io/.",2024-08-11 11:43:56+00:00,"['Zhigang Tu', 'Zitao Gao', 'Zhengbo Zhang', 'Chunluan Zhou', 'Junsong Yuan', 'Bo Du']",http://arxiv.org/abs/2408.05750v1
ReToMe-VA: Recursive Token Merging for Video Diffusion-based Unrestricted Adversarial Attack,"Recent diffusion-based unrestricted attacks generate imperceptible
adversarial examples with high transferability compared to previous
unrestricted attacks and restricted attacks. However, existing works on
diffusion-based unrestricted attacks are mostly focused on images yet are
seldom explored in videos. In this paper, we propose the Recursive Token
Merging for Video Diffusion-based Unrestricted Adversarial Attack (ReToMe-VA),
which is the first framework to generate imperceptible adversarial video clips
with higher transferability. Specifically, to achieve spatial imperceptibility,
ReToMe-VA adopts a Timestep-wise Adversarial Latent Optimization (TALO)
strategy that optimizes perturbations in diffusion models' latent space at each
denoising step. TALO offers iterative and accurate updates to generate more
powerful adversarial frames. TALO can further reduce memory consumption in
gradient computation. Moreover, to achieve temporal imperceptibility, ReToMe-VA
introduces a Recursive Token Merging (ReToMe) mechanism by matching and merging
tokens across video frames in the self-attention module, resulting in
temporally consistent adversarial videos. ReToMe concurrently facilitates
inter-frame interactions into the attack process, inducing more diverse and
robust gradients, thus leading to better adversarial transferability. Extensive
experiments demonstrate the efficacy of ReToMe-VA, particularly in surpassing
state-of-the-art attacks in adversarial transferability by more than 14.16% on
average.",2024-08-10 08:10:30+00:00,"['Ziyi Gao', 'Kai Chen', 'Zhipeng Wei', 'Tingshu Mou', 'Jingjing Chen', 'Zhiyu Tan', 'Hao Li', 'Yu-Gang Jiang']",http://arxiv.org/abs/2408.05479v1
UniMLVG: Unified Framework for Multi-view Long Video Generation with Comprehensive Control Capabilities for Autonomous Driving,"The creation of diverse and realistic driving scenarios has become essential
to enhance perception and planning capabilities of the autonomous driving
system. However, generating long-duration, surround-view consistent driving
videos remains a significant challenge. To address this, we present UniMLVG, a
unified framework designed to generate extended street multi-perspective videos
under precise control. By integrating single- and multi-view driving videos
into the training data, our approach updates a DiT-based diffusion model
equipped with cross-frame and cross-view modules across three stages with multi
training objectives, substantially boosting the diversity and quality of
generated visual content. Importantly, we propose an innovative explicit
viewpoint modeling approach for multi-view video generation to effectively
improve motion transition consistency. Capable of handling various input
reference formats (e.g., text, images, or video), our UniMLVG generates
high-quality multi-view videos according to the corresponding condition
constraints such as 3D bounding boxes or frame-level text descriptions.
Compared to the best models with similar capabilities, our framework achieves
improvements of 48.2% in FID and 35.2% in FVD.",2024-12-06 08:27:53+00:00,"['Rui Chen', 'Zehuan Wu', 'Yichen Liu', 'Yuxin Guo', 'Jingcheng Ni', 'Haifeng Xia', 'Siyu Xia']",http://arxiv.org/abs/2412.04842v3
FIFO-Diffusion: Generating Infinite Videos from Text without Training,"We propose a novel inference technique based on a pretrained diffusion model
for text-conditional video generation. Our approach, called FIFO-Diffusion, is
conceptually capable of generating infinitely long videos without additional
training. This is achieved by iteratively performing diagonal denoising, which
simultaneously processes a series of consecutive frames with increasing noise
levels in a queue; our method dequeues a fully denoised frame at the head while
enqueuing a new random noise frame at the tail. However, diagonal denoising is
a double-edged sword as the frames near the tail can take advantage of cleaner
frames by forward reference but such a strategy induces the discrepancy between
training and inference. Hence, we introduce latent partitioning to reduce the
training-inference gap and lookahead denoising to leverage the benefit of
forward referencing. Practically, FIFO-Diffusion consumes a constant amount of
memory regardless of the target video length given a baseline model, while
well-suited for parallel inference on multiple GPUs. We have demonstrated the
promising results and effectiveness of the proposed methods on existing
text-to-video generation baselines. Generated video examples and source codes
are available at our project page.",2024-05-19 07:48:41+00:00,"['Jihwan Kim', 'Junoh Kang', 'Jinyoung Choi', 'Bohyung Han']",http://arxiv.org/abs/2405.11473v4
A Comprehensive Survey on Synthetic Infrared Image synthesis,"Synthetic infrared (IR) scene and target generation is an important computer
vision problem as it allows the generation of realistic IR images and targets
for training and testing of various applications, such as remote sensing,
surveillance, and target recognition. It also helps reduce the cost and risk
associated with collecting real-world IR data. This survey paper aims to
provide a comprehensive overview of the conventional mathematical
modelling-based methods and deep learning-based methods used for generating
synthetic IR scenes and targets. The paper discusses the importance of
synthetic IR scene and target generation and briefly covers the mathematics of
blackbody and grey body radiations, as well as IR image-capturing methods. The
potential use cases of synthetic IR scenes and target generation are also
described, highlighting the significance of these techniques in various fields.
Additionally, the paper explores possible new ways of developing new techniques
to enhance the efficiency and effectiveness of synthetic IR scenes and target
generation while highlighting the need for further research to advance this
field.",2024-08-13 13:06:50+00:00,"['Avinash Upadhyay', 'Manoj sharma', 'Prerana Mukherjee', 'Amit Singhal', 'Brejesh Lall']",http://arxiv.org/abs/2408.06868v2
ControlNeXt: Powerful and Efficient Control for Image and Video Generation,"Diffusion models have demonstrated remarkable and robust abilities in both
image and video generation. To achieve greater control over generated results,
researchers introduce additional architectures, such as ControlNet, Adapters
and ReferenceNet, to integrate conditioning controls. However, current
controllable generation methods often require substantial additional
computational resources, especially for video generation, and face challenges
in training or exhibit weak control. In this paper, we propose ControlNeXt: a
powerful and efficient method for controllable image and video generation. We
first design a more straightforward and efficient architecture, replacing heavy
additional branches with minimal additional cost compared to the base model.
Such a concise structure also allows our method to seamlessly integrate with
other LoRA weights, enabling style alteration without the need for additional
training. As for training, we reduce up to 90% of learnable parameters compared
to the alternatives. Furthermore, we propose another method called Cross
Normalization (CN) as a replacement for Zero-Convolution' to achieve fast and
stable training convergence. We have conducted various experiments with
different base models across images and videos, demonstrating the robustness of
our method.",2024-08-12 11:41:18+00:00,"['Bohao Peng', 'Jian Wang', 'Yuechen Zhang', 'Wenbo Li', 'Ming-Chang Yang', 'Jiaya Jia']",http://arxiv.org/abs/2408.06070v3
CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship Modeling in Aerial Videos,"Video scene graph generation (VidSGG) has emerged as a transformative
approach to capturing and interpreting the intricate relationships among
objects and their temporal dynamics in video sequences. In this paper, we
introduce the new AeroEye dataset that focuses on multi-object relationship
modeling in aerial videos. Our AeroEye dataset features various drone scenes
and includes a visually comprehensive and precise collection of predicates that
capture the intricate relationships and spatial arrangements among objects. To
this end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that
allows the model to capture both direct and long-range temporal dependencies by
continuously updating the history of interactions in a circular manner. The
proposed approach also allows one to handle sequences with inherent cyclical
patterns and process object relationships in the correct sequential order.
Therefore, it can effectively capture periodic and overlapping relationships
while minimizing information loss. The extensive experiments on the AeroEye
dataset demonstrate the effectiveness of the proposed CYCLO model,
demonstrating its potential to perform scene understanding on drone videos.
Finally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results
on two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.",2024-06-03 06:24:55+00:00,"['Trong-Thuan Nguyen', 'Pha Nguyen', 'Xin Li', 'Jackson Cothren', 'Alper Yilmaz', 'Khoa Luu']",http://arxiv.org/abs/2406.01029v4
MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy,"Style transfer is a promising approach to close the sim-to-real gap in
medical endoscopy. Rendering synthetic endoscopic videos by traversing
pre-operative scans (such as MRI or CT) can generate structurally accurate
simulations as well as ground truth camera poses and depth maps. Although
image-to-image (I2I) translation models such as CycleGAN can imitate realistic
endoscopic images from these simulations, they are unsuitable for
video-to-video synthesis due to the lack of temporal consistency, resulting in
artifacts between frames. We propose MeshBrush, a neural mesh stylization
method to synthesize temporally consistent videos with differentiable
rendering. MeshBrush uses the underlying geometry of patient imaging data while
leveraging existing I2I methods. With learned per-vertex textures, the stylized
mesh guarantees consistency while producing high-fidelity outputs. We
demonstrate that mesh stylization is a promising approach for creating
realistic simulations for downstream tasks such as training networks and
preoperative planning. Although our method is tested and designed for
ureteroscopy, its components are transferable to general endoscopic and
laparoscopic procedures. The code will be made public on GitHub.",2024-04-03 18:40:48+00:00,"['John J. Han', 'Ayberk Acar', 'Nicholas Kavoussi', 'Jie Ying Wu']",http://arxiv.org/abs/2404.02999v2
Beyond Deepfake Images: Detecting AI-Generated Videos,"Recent advances in generative AI have led to the development of techniques to
generate visually realistic synthetic video. While a number of techniques have
been developed to detect AI-generated synthetic images, in this paper we show
that synthetic image detectors are unable to detect synthetic videos. We
demonstrate that this is because synthetic video generators introduce
substantially different traces than those left by image generators. Despite
this, we show that synthetic video traces can be learned, and used to perform
reliable synthetic video detection or generator source attribution even after
H.264 re-compression. Furthermore, we demonstrate that while detecting videos
from new generators through zero-shot transferability is challenging, accurate
detection of videos from a new generator can be achieved through few-shot
learning.",2024-04-24 16:19:31+00:00,"['Danial Samadi Vahdati', 'Tai D. Nguyen', 'Aref Azizpour', 'Matthew C. Stamm']",http://arxiv.org/abs/2404.15955v1
Can Generative AI Replace Immunofluorescent Staining Processes? A Comparison Study of Synthetically Generated CellPainting Images from Brightfield,"Cell imaging assays utilizing fluorescence stains are essential for observing
sub-cellular organelles and their responses to perturbations. Immunofluorescent
staining process is routinely in labs, however the recent innovations in
generative AI is challenging the idea of IF staining are required. This is
especially true when the availability and cost of specific fluorescence dyes is
a problem to some labs. Furthermore, staining process takes time and leads to
inter-intra technician and hinders downstream image and data analysis, and the
reusability of image data for other projects. Recent studies showed the use of
generated synthetic immunofluorescence (IF) images from brightfield (BF) images
using generative AI algorithms in the literature. Therefore, in this study, we
benchmark and compare five models from three types of IF generation backbones,
CNN, GAN, and diffusion models, using a publicly available dataset. This paper
not only serves as a comparative study to determine the best-performing model
but also proposes a comprehensive analysis pipeline for evaluating the efficacy
of generators in IF image synthesis. We highlighted the potential of deep
learning-based generators for IF image synthesis, while also discussed
potential issues and future research directions. Although generative AI shows
promise in simplifying cell phenotyping using only BF images with IF staining,
further research and validations are needed to address the key challenges of
model generalisability, batch effects, feature relevance and computational
costs.",2024-06-15 09:42:11+00:00,"['Xiaodan Xing', 'Siofra Murdoch', 'Chunling Tang', 'Giorgos Papanastasiou', 'Jan Cross-Zamirski', 'Yunzhe Guo', 'Xianglu Xiao', 'Carola-Bibiane Schnlieb', 'Yinhai Wang', 'Guang Yang']",http://arxiv.org/abs/2407.09507v2
InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models,"We present InfiniCube, a scalable method for generating unbounded dynamic 3D
driving scenes with high fidelity and controllability. Previous methods for
scene generation either suffer from limited scales or lack geometric and
appearance consistency along generated sequences. In contrast, we leverage the
recent advancements in scalable 3D representation and video models to achieve
large dynamic scene generation that allows flexible controls through HD maps,
vehicle bounding boxes, and text descriptions. First, we construct a
map-conditioned sparse-voxel-based 3D generative model to unleash its power for
unbounded voxel world generation. Then, we re-purpose a video model and ground
it on the voxel world through a set of carefully designed pixel-aligned
guidance buffers, synthesizing a consistent appearance. Finally, we propose a
fast feed-forward approach that employs both voxel and pixel branches to lift
the dynamic videos to dynamic 3D Gaussians with controllable objects. Our
method can generate controllable and realistic 3D driving scenes, and extensive
experiments validate the effectiveness and superiority of our model.",2024-12-05 07:32:20+00:00,"['Yifan Lu', 'Xuanchi Ren', 'Jiawei Yang', 'Tianchang Shen', 'Zhangjie Wu', 'Jun Gao', 'Yue Wang', 'Siheng Chen', 'Mike Chen', 'Sanja Fidler', 'Jiahui Huang']",http://arxiv.org/abs/2412.03934v1
Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation,"Diffusion models have proven to be highly effective in image and video
generation; however, they encounter challenges in the correct composition of
objects when generating images of varying sizes due to single-scale training
data. Adapting large pre-trained diffusion models to higher resolution demands
substantial computational and optimization resources, yet achieving generation
capabilities comparable to low-resolution models remains challenging. This
paper proposes a novel self-cascade diffusion model that leverages the
knowledge gained from a well-trained low-resolution image/video generation
model, enabling rapid adaptation to higher-resolution generation. Building on
this, we employ the pivot replacement strategy to facilitate a tuning-free
version by progressively leveraging reliable semantic guidance derived from the
low-resolution model. We further propose to integrate a sequence of learnable
multi-scale upsampler modules for a tuning version capable of efficiently
learning structural details at a new scale from a small amount of newly
acquired high-resolution training data. Compared to full fine-tuning, our
approach achieves a $5\times$ training speed-up and requires only 0.002M tuning
parameters. Extensive experiments demonstrate that our approach can quickly
adapt to higher-resolution image and video synthesis by fine-tuning for just
$10k$ steps, with virtually no additional inference time.",2024-02-16 07:48:35+00:00,"['Lanqing Guo', 'Yingqing He', 'Haoxin Chen', 'Menghan Xia', 'Xiaodong Cun', 'Yufei Wang', 'Siyu Huang', 'Yong Zhang', 'Xintao Wang', 'Qifeng Chen', 'Ying Shan', 'Bihan Wen']",http://arxiv.org/abs/2402.10491v2
VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis,"We propose VLOGGER, a method for audio-driven human video generation from a
single input image of a person, which builds on the success of recent
generative diffusion models. Our method consists of 1) a stochastic
human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture
that augments text-to-image models with both spatial and temporal controls.
This supports the generation of high quality video of variable length, easily
controllable through high-level representations of human faces and bodies. In
contrast to previous work, our method does not require training for each
person, does not rely on face detection and cropping, generates the complete
image (not just the face or the lips), and considers a broad spectrum of
scenarios (e.g. visible torso or diverse subject identities) that are critical
to correctly synthesize humans who communicate. We also curate MENTOR, a new
and diverse dataset with 3d pose and expression annotations, one order of
magnitude larger than previous ones (800,000 identities) and with dynamic
gestures, on which we train and ablate our main technical contributions.
  VLOGGER outperforms state-of-the-art methods in three public benchmarks,
considering image quality, identity preservation and temporal consistency while
also generating upper-body gestures. We analyze the performance of VLOGGER with
respect to multiple diversity metrics, showing that our architectural choices
and the use of MENTOR benefit training a fair and unbiased model at scale.
Finally we show applications in video editing and personalization.",2024-03-13 17:59:02+00:00,"['Enric Corona', 'Andrei Zanfir', 'Eduard Gabriel Bazavan', 'Nikos Kolotouros', 'Thiemo Alldieck', 'Cristian Sminchisescu']",http://arxiv.org/abs/2403.08764v1
4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives,"Dynamic 3D scene representation and novel view synthesis from captured videos
are crucial for enabling immersive experiences required by AR/VR and metaverse
applications. However, this task is challenging due to the complexity of
unconstrained real-world scenes and their temporal dynamics. In this paper, we
frame dynamic scenes as a spatio-temporal 4D volume learning problem, offering
a native explicit reformulation with minimal assumptions about motion, which
serves as a versatile dynamic scene learning framework. Specifically, we
represent a target dynamic scene using a collection of 4D Gaussian primitives
with explicit geometry and appearance features, dubbed as 4D Gaussian splatting
(4DGS). This approach can capture relevant information in space and time by
fitting the underlying spatio-temporal volume. Modeling the spacetime as a
whole with 4D Gaussians parameterized by anisotropic ellipses that can rotate
arbitrarily in space and time, our model can naturally learn view-dependent and
time-evolved appearance with 4D spherindrical harmonics. Notably, our 4DGS
model is the first solution that supports real-time rendering of
high-resolution, photorealistic novel views for complex dynamic scenes. To
enhance efficiency, we derive several compact variants that effectively reduce
memory footprint and mitigate the risk of overfitting. Extensive experiments
validate the superiority of 4DGS in terms of visual quality and efficiency
across a range of dynamic scene-related tasks (e.g., novel view synthesis, 4D
generation, scene understanding) and scenarios (e.g., single object, indoor
scenes, driving environments, synthetic and real data).",2024-12-30 05:30:26+00:00,"['Zeyu Yang', 'Zijie Pan', 'Xiatian Zhu', 'Li Zhang', 'Yu-Gang Jiang', 'Philip H. S. Torr']",http://arxiv.org/abs/2412.20720v1
Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models,"The availability of large-scale multimodal datasets and advancements in
diffusion models have significantly accelerated progress in 4D content
generation. Most prior approaches rely on multiple image or video diffusion
models, utilizing score distillation sampling for optimization or generating
pseudo novel views for direct supervision. However, these methods are hindered
by slow optimization speeds and multi-view inconsistency issues. Spatial and
temporal consistency in 4D geometry has been extensively explored respectively
in 3D-aware diffusion models and traditional monocular video diffusion models.
Building on this foundation, we propose a strategy to migrate the temporal
consistency in video diffusion models to the spatial-temporal consistency
required for 4D generation. Specifically, we present a novel framework,
\textbf{Diffusion4D}, for efficient and scalable 4D content generation.
Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware
video diffusion model capable of synthesizing orbital views of dynamic 3D
assets. To control the dynamic strength of these assets, we introduce a
3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel
motion magnitude reconstruction loss and 3D-aware classifier-free guidance to
refine the learning and generation of motion dynamics. After obtaining orbital
views of the 4D asset, we perform explicit 4D construction with Gaussian
splatting in a coarse-to-fine manner. The synthesized multi-view consistent 4D
image set enables us to swiftly generate high-fidelity and diverse 4D assets
within just several minutes. Extensive experiments demonstrate that our method
surpasses prior state-of-the-art techniques in terms of generation efficiency
and 4D geometry consistency across various prompt modalities.",2024-05-26 17:47:34+00:00,"['Hanwen Liang', 'Yuyang Yin', 'Dejia Xu', 'Hanxue Liang', 'Zhangyang Wang', 'Konstantinos N. Plataniotis', 'Yao Zhao', 'Yunchao Wei']",http://arxiv.org/abs/2405.16645v1
One-Click Upgrade from 2D to 3D: Sandwiched RGB-D Video Compression for Stereoscopic Teleconferencing,"Stereoscopic video conferencing is still challenging due to the need to
compress stereo RGB-D video in real-time. Though hardware implementations of
standard video codecs such as H.264 / AVC and HEVC are widely available, they
are not designed for stereoscopic videos and suffer from reduced quality and
performance. Specific multiview or 3D extensions of these codecs are complex
and lack efficient implementations. In this paper, we propose a new approach to
upgrade a 2D video codec to support stereo RGB-D video compression, by wrapping
it with a neural pre- and post-processor pair. The neural networks are
end-to-end trained with an image codec proxy, and shown to work with a more
sophisticated video codec. We also propose a geometry-aware loss function to
improve rendering quality. We train the neural pre- and post-processors on a
synthetic 4D people dataset, and evaluate it on both synthetic and
real-captured stereo RGB-D videos. Experimental results show that the neural
networks generalize well to unseen data and work out-of-box with various video
codecs. Our approach saves about 30% bit-rate compared to a conventional video
coding scheme and MV-HEVC at the same level of rendering quality from a novel
view, without the need of a task-specific hardware upgrade.",2024-04-15 17:56:05+00:00,"['Yueyu Hu', 'Onur G. Guleryuz', 'Philip A. Chou', 'Danhang Tang', 'Jonathan Taylor', 'Rus Maxham', 'Yao Wang']",http://arxiv.org/abs/2404.09979v1
A Vlogger-augmented Graph Neural Network Model for Micro-video Recommendation,"Existing micro-video recommendation models exploit the interactions between
users and micro-videos and/or multi-modal information of micro-videos to
predict the next micro-video a user will watch, ignoring the information
related to vloggers, i.e., the producers of micro-videos. However, in
micro-video scenarios, vloggers play a significant role in user-video
interactions, since vloggers generally focus on specific topics and users tend
to follow the vloggers they are interested in. Therefore, in the paper, we
propose a vlogger-augmented graph neural network model VA-GNN, which takes the
effect of vloggers into consideration. Specifically, we construct a tripartite
graph with users, micro-videos, and vloggers as nodes, capturing user
preferences from different views, i.e., the video-view and the vlogger-view.
Moreover, we conduct cross-view contrastive learning to keep the consistency
between node embeddings from the two different views. Besides, when predicting
the next user-video interaction, we adaptively combine the user preferences for
a video itself and its vlogger. We conduct extensive experiments on two
real-world datasets. The experimental results show that VA-GNN outperforms
multiple existing GNN-based recommendation models.",2024-05-28 15:13:29+00:00,"['Weijiang Lai', 'Beihong Jin', 'Beibei Li', 'Yiyuan Zheng', 'Rui Zhao']",http://arxiv.org/abs/2405.18260v1
DL-KDD: Dual-Light Knowledge Distillation for Action Recognition in the Dark,"Human action recognition in dark videos is a challenging task for computer
vision. Recent research focuses on applying dark enhancement methods to improve
the visibility of the video. However, such video processing results in the loss
of critical information in the original (un-enhanced) video. Conversely,
traditional two-stream methods are capable of learning information from both
original and processed videos, but it can lead to a significant increase in the
computational cost during the inference phase in the task of video
classification. To address these challenges, we propose a novel teacher-student
video classification framework, named Dual-Light KnowleDge Distillation for
Action Recognition in the Dark (DL-KDD). This framework enables the model to
learn from both original and enhanced video without introducing additional
computational cost during inference. Specifically, DL-KDD utilizes the strategy
of knowledge distillation during training. The teacher model is trained with
enhanced video, and the student model is trained with both the original video
and the soft target generated by the teacher model. This teacher-student
framework allows the student model to predict action using only the original
input video during inference. In our experiments, the proposed DL-KDD framework
outperforms state-of-the-art methods on the ARID, ARID V1.5, and Dark-48
datasets. We achieve the best performance on each dataset and up to a 4.18%
improvement on Dark-48, using only original video inputs, thus avoiding the use
of two-stream framework or enhancement modules for inference. We further
validate the effectiveness of the distillation strategy in ablative
experiments. The results highlight the advantages of our knowledge distillation
framework in dark human action recognition.",2024-06-04 16:38:06+00:00,"['Chi-Jui Chang', 'Oscar Tai-Yuan Chen', 'Vincent S. Tseng']",http://arxiv.org/abs/2406.02468v1
VideoLLM-online: Online Video Large Language Model for Streaming Video,"Recent Large Language Models have been enhanced with vision capabilities,
enabling them to comprehend images, videos, and interleaved vision-language
content. However, the learning methods of these large multimodal models
typically treat videos as predetermined clips, making them less effective and
efficient at handling streaming video inputs. In this paper, we propose a novel
Learning-In-Video-Stream (LIVE) framework, which enables temporally aligned,
long-context, and real-time conversation within a continuous video stream. Our
LIVE framework comprises comprehensive approaches to achieve video streaming
dialogue, encompassing: (1) a training objective designed to perform language
modeling for continuous streaming inputs, (2) a data generation scheme that
converts offline temporal annotations into a streaming dialogue format, and (3)
an optimized inference pipeline to speed up the model responses in real-world
video streams. With our LIVE framework, we built VideoLLM-online model upon
Llama-2/Llama-3 and demonstrate its significant advantages in processing
streaming videos. For instance, on average, our model can support streaming
dialogue in a 5-minute video clip at over 10 FPS on an A100 GPU. Moreover, it
also showcases state-of-the-art performance on public offline video benchmarks,
such as recognition, captioning, and forecasting. The code, model, data, and
demo have been made available at https://showlab.github.io/videollm-online.",2024-06-17 17:55:32+00:00,"['Joya Chen', 'Zhaoyang Lv', 'Shiwei Wu', 'Kevin Qinghong Lin', 'Chenan Song', 'Difei Gao', 'Jia-Wei Liu', 'Ziteng Gao', 'Dongxing Mao', 'Mike Zheng Shou']",http://arxiv.org/abs/2406.11816v1
CLIPVQA:Video Quality Assessment via CLIP,"In learning vision-language representations from web-scale data, the
contrastive language-image pre-training (CLIP) mechanism has demonstrated a
remarkable performance in many vision tasks. However, its application to the
widely studied video quality assessment (VQA) task is still an open issue. In
this paper, we propose an efficient and effective CLIP-based Transformer method
for the VQA problem (CLIPVQA). Specifically, we first design an effective video
frame perception paradigm with the goal of extracting the rich spatiotemporal
quality and content information among video frames. Then, the spatiotemporal
quality features are adequately integrated together using a self-attention
mechanism to yield video-level quality representation. To utilize the quality
language descriptions of videos for supervision, we develop a CLIP-based
encoder for language embedding, which is then fully aggregated with the
generated content information via a cross-attention module for producing
video-language representation. Finally, the video-level quality and
video-language representations are fused together for final video quality
prediction, where a vectorized regression loss is employed for efficient
end-to-end optimization. Comprehensive experiments are conducted on eight
in-the-wild video datasets with diverse resolutions to evaluate the performance
of CLIPVQA. The experimental results show that the proposed CLIPVQA achieves
new state-of-the-art VQA performance and up to 37% better generalizability than
existing benchmark VQA methods. A series of ablation studies are also performed
to validate the effectiveness of each module in CLIPVQA.",2024-07-06 02:32:28+00:00,"['Fengchuang Xing', 'Mingjie Li', 'Yuan-Gen Wang', 'Guopu Zhu', 'Xiaochun Cao']",http://arxiv.org/abs/2407.04928v1
Learning Video Context as Interleaved Multimodal Sequences,"Narrative videos, such as movies, pose significant challenges in video
understanding due to their rich contexts (characters, dialogues, storylines)
and diverse demands (identify who, relationship, and reason). In this paper, we
introduce MovieSeq, a multimodal language model developed to address the wide
range of challenges in understanding video contexts. Our core idea is to
represent videos as interleaved multimodal sequences (including images, plots,
videos, and subtitles), either by linking external knowledge databases or using
offline models (such as whisper for subtitles). Through instruction-tuning,
this approach empowers the language model to interact with videos using
interleaved multimodal instructions. For example, instead of solely relying on
video as input, we jointly provide character photos alongside their names and
dialogues, allowing the model to associate these elements and generate more
comprehensive responses. To demonstrate its effectiveness, we validate
MovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)
across five settings (video classification, audio description, video-text
retrieval, video captioning, and video question-answering). The code will be
public at https://github.com/showlab/MovieSeq.",2024-07-31 17:23:57+00:00,"['Kevin Qinghong Lin', 'Pengchuan Zhang', 'Difei Gao', 'Xide Xia', 'Joya Chen', 'Ziteng Gao', 'Jinheng Xie', 'Xuhong Xiao', 'Mike Zheng Shou']",http://arxiv.org/abs/2407.21757v2
E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding,"Recent advances in Video Large Language Models (Video-LLMs) have demonstrated
their great potential in general-purpose video understanding. To verify the
significance of these models, a number of benchmarks have been proposed to
diagnose their capabilities in different scenarios. However, existing
benchmarks merely evaluate models through video-level question-answering,
lacking fine-grained event-level assessment and task diversity. To fill this
gap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding
Benchmark), a large-scale and high-quality benchmark for open-ended event-level
video understanding. Categorized within a 3-level task taxonomy, E.T. Bench
encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)
under 8 domains, providing comprehensive evaluations. We extensively evaluated
8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that
state-of-the-art models for coarse-level (video-level) understanding struggle
to solve our fine-grained tasks, e.g., grounding event-of-interests within
videos, largely due to the short video context length, improper time
representations, and lack of multi-event training data. Focusing on these
issues, we further propose a strong baseline model, E.T. Chat, together with an
instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained
event-level understanding. Our simple but effective solution demonstrates
superior performance in multiple scenarios.",2024-09-26 17:53:04+00:00,"['Ye Liu', 'Zongyang Ma', 'Zhongang Qi', 'Yang Wu', 'Ying Shan', 'Chang Wen Chen']",http://arxiv.org/abs/2409.18111v1
From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding,"The integration of Large Language Models (LLMs) with visual encoders has
recently shown promising performance in visual understanding tasks, leveraging
their inherent capability to comprehend and generate human-like text for visual
reasoning. Given the diverse nature of visual data, MultiModal Large Language
Models (MM-LLMs) exhibit variations in model designing and training for
understanding images, short videos, and long videos. Our paper focuses on the
substantial differences and unique challenges posed by long video understanding
compared to static image and short video understanding. Unlike static images,
short videos encompass sequential frames with both spatial and within-event
temporal information, while long videos consist of multiple events with
between-event and long-term temporal information. In this survey, we aim to
trace and summarize the advancements of MM-LLMs from image understanding to
long video understanding. We review the differences among various visual
understanding tasks and highlight the challenges in long video understanding,
including more fine-grained spatiotemporal details, dynamic events, and
long-term dependencies. We then provide a detailed summary of the advancements
in MM-LLMs in terms of model design and training methodologies for
understanding long videos. Finally, we compare the performance of existing
MM-LLMs on video understanding benchmarks of various lengths and discuss
potential future directions for MM-LLMs in long video understanding.",2024-09-27 17:38:36+00:00,"['Heqing Zou', 'Tianze Luo', 'Guiyang Xie', 'Victor', 'Zhang', 'Fengmao Lv', 'Guangcong Wang', 'Junyang Chen', 'Zhuochen Wang', 'Hansheng Zhang', 'Huaijian Zhang']",http://arxiv.org/abs/2409.18938v2
Video Seal: Open and Efficient Video Watermarking,"The proliferation of AI-generated content and sophisticated video editing
tools has made it both important and challenging to moderate digital platforms.
Video watermarking addresses these challenges by embedding imperceptible
signals into videos, allowing for identification. However, the rare open tools
and methods often fall short on efficiency, robustness, and flexibility. To
reduce these gaps, this paper introduces Video Seal, a comprehensive framework
for neural video watermarking and a competitive open-sourced model. Our
approach jointly trains an embedder and an extractor, while ensuring the
watermark robustness by applying transformations in-between, e.g., video
codecs. This training is multistage and includes image pre-training, hybrid
post-training and extractor fine-tuning. We also introduce temporal watermark
propagation, a technique to convert any image watermarking model to an
efficient video watermarking model without the need to watermark every
high-resolution frame. We present experimental results demonstrating the
effectiveness of the approach in terms of speed, imperceptibility, and
robustness. Video Seal achieves higher robustness compared to strong baselines
especially under challenging distortions combining geometric transformations
and video compression. Additionally, we provide new insights such as the impact
of video compression during training, and how to compare methods operating on
different payloads. Contributions in this work - including the codebase,
models, and a public demo - are open-sourced under permissive licenses to
foster further research and development in the field.",2024-12-12 17:41:49+00:00,"['Pierre Fernandez', 'Hady Elsahar', 'I. Zeki Yalniz', 'Alexandre Mourachko']",http://arxiv.org/abs/2412.09492v1
TINQ: Temporal Inconsistency Guided Blind Video Quality Assessment,"Blind video quality assessment (BVQA) has been actively researched for
user-generated content (UGC) videos. Recently, super-resolution (SR) techniques
have been widely applied in UGC. Therefore, an effective BVQA method for both
UGC and SR scenarios is essential. Temporal inconsistency, referring to
irregularities between consecutive frames, is relevant to video quality.
Current BVQA approaches typically model temporal relationships in UGC videos
using statistics of motion information, but inconsistencies remain unexplored.
Additionally, different from temporal inconsistency in UGC videos, such
inconsistency in SR videos is amplified due to upscaling algorithms. In this
paper, we introduce the Temporal Inconsistency Guided Blind Video Quality
Assessment (TINQ) metric, demonstrating that exploring temporal inconsistency
is crucial for effective BVQA. Since temporal inconsistencies vary between UGC
and SR videos, they are calculated in different ways. Based on this, a spatial
module highlights inconsistent areas across consecutive frames at coarse and
fine granularities. In addition, a temporal module aggregates features over
time in two stages. The first stage employs a visual memory capacity block to
adaptively segment the time dimension based on estimated complexity, while the
second stage focuses on selecting key features. The stages work together
through Consistency-aware Fusion Units to regress cross-time-scale video
quality. Extensive experiments on UGC and SR video quality datasets show that
our method outperforms existing state-of-the-art BVQA methods. Code is
available at https://github.com/Lighting-YXLI/TINQ.",2024-12-25 15:43:41+00:00,"['Yixiao Li', 'Xiaoyuan Yang', 'Weide Liu', 'Xin Jin', 'Xu Jia', 'Yukun Lai', 'Haotao Liu', 'Paul L Rosin', 'Wei Zhou']",http://arxiv.org/abs/2412.18933v1
PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance,"The past year has witnessed the significant advancement of video-based large
language models. However, the challenge of developing a unified model for both
short and long video understanding remains unresolved. Most existing video LLMs
cannot handle hour-long videos, while methods custom for long videos tend to be
ineffective for shorter videos and images. In this paper, we identify the key
issue as the redundant content in videos. To address this, we propose a novel
pooling strategy that simultaneously achieves token compression and
instruction-aware visual feature aggregation. Our model is termed Prompt-guided
Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three
core components: the CLIP-based visual-prompt alignment that extracts visual
information relevant to the user's instructions, the prompt-guided pooling that
compresses the visual sequence to arbitrary scales using convolution-style
pooling, and the clip context extension designed for lengthy prompt common in
visual dialogue. Moreover, our codebase also integrates the most advanced video
Direct Preference Optimization (DPO) and visual interleave training. Extensive
experiments have validated the performance of our model. With superior
throughput and only 1024 visual context, PPLLaVA achieves better results on
image benchmarks as a video LLM, while achieving state-of-the-art performance
across various video benchmarks, excelling in tasks ranging from caption
generation to multiple-choice questions, and handling video lengths from
seconds to hours. Codes have been available at
https://github.com/farewellthree/PPLLaVA.",2024-11-04 17:50:36+00:00,"['Ruyang Liu', 'Haoran Tang', 'Haibo Liu', 'Yixiao Ge', 'Ying Shan', 'Chen Li', 'Jiankun Yang']",http://arxiv.org/abs/2411.02327v2
SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation,"Methods for image-to-video generation have achieved impressive,
photo-realistic quality. However, adjusting specific elements in generated
videos, such as object motion or camera movement, is often a tedious process of
trial and error, e.g., involving re-generating videos with different random
seeds. Recent techniques address this issue by fine-tuning a pre-trained model
to follow conditioning signals, such as bounding boxes or point trajectories.
Yet, this fine-tuning procedure can be computationally expensive, and it
requires datasets with annotated object motion, which can be difficult to
procure. In this work, we introduce SG-I2V, a framework for controllable
image-to-video generation that is self-guided$\unicode{x2013}$offering
zero-shot control by relying solely on the knowledge present in a pre-trained
image-to-video diffusion model without the need for fine-tuning or external
knowledge. Our zero-shot method outperforms unsupervised baselines while
significantly narrowing down the performance gap with supervised models in
terms of visual quality and motion fidelity. Additional details and video
results are available on our project page:
https://kmcode1.github.io/Projects/SG-I2V",2024-11-07 18:56:11+00:00,"['Koichi Namekata', 'Sherwin Bahmani', 'Ziyi Wu', 'Yash Kant', 'Igor Gilitschenski', 'David B. Lindell']",http://arxiv.org/abs/2411.04989v3
VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE,"Unsupervised video object learning seeks to decompose video scenes into
structural object representations without any supervision from depth, optical
flow, or segmentation. We present VONet, an innovative approach that is
inspired by MONet. While utilizing a U-Net architecture, VONet employs an
efficient and effective parallel attention inference process, generating
attention masks for all slots simultaneously. Additionally, to enhance the
temporal consistency of each mask across consecutive video frames, VONet
develops an object-wise sequential VAE framework. The integration of these
innovative encoder-side techniques, in conjunction with an expressive
transformer-based decoder, establishes VONet as the leading unsupervised method
for object learning across five MOVI datasets, encompassing videos of diverse
complexities. Code is available at https://github.com/hnyu/vonet.",2024-01-20 04:13:54+00:00,"['Haonan Yu', 'Wei Xu']",http://arxiv.org/abs/2401.11110v1
Fast Deep Predictive Coding Networks for Videos Feature Extraction without Labels,"Brain-inspired deep predictive coding networks (DPCNs) effectively model and
capture video features through a bi-directional information flow, even without
labels. They are based on an overcomplete description of video scenes, and one
of the bottlenecks has been the lack of effective sparsification techniques to
find discriminative and robust dictionaries. FISTA has been the best
alternative. This paper proposes a DPCN with a fast inference of internal model
variables (states and causes) that achieves high sparsity and accuracy of
feature clustering. The proposed unsupervised learning procedure, inspired by
adaptive dynamic programming with a majorization-minimization framework, and
its convergence are rigorously analyzed. Experiments in the data sets CIFAR-10,
Super Mario Bros video game, and Coil-100 validate the approach, which
outperforms previous versions of DPCNs on learning rate, sparsity ratio, and
feature clustering accuracy. Because of DCPN's solid foundation and
explainability, this advance opens the door for general applications in object
recognition in video without labels.",2024-09-08 01:53:25+00:00,"['Wenqian Xue', 'Chi Ding', 'Jose Principe']",http://arxiv.org/abs/2409.04945v1
LocoMotion: Learning Motion-Focused Video-Language Representations,"This paper strives for motion-focused video-language representations.
Existing methods to learn video-language representations use spatial-focused
data, where identifying the objects and scene is often enough to distinguish
the relevant caption. We instead propose LocoMotion to learn from
motion-focused captions that describe the movement and temporal progression of
local object motions. We achieve this by adding synthetic motions to videos and
using the parameters of these motions to generate corresponding captions.
Furthermore, we propose verb-variation paraphrasing to increase the caption
variety and learn the link between primitive motions and high-level verbs. With
this, we are able to learn a motion-focused video-language representation.
Experiments demonstrate our approach is effective for a variety of downstream
tasks, particularly when limited data is available for fine-tuning. Code is
available: https://hazeldoughty.github.io/Papers/LocoMotion/",2024-10-15 19:33:57+00:00,"['Hazel Doughty', 'Fida Mohammad Thoker', 'Cees G. M. Snoek']",http://arxiv.org/abs/2410.12018v2
A Simple and Effective Temporal Grounding Pipeline for Basketball Broadcast Footage,"We present a reliable temporal grounding pipeline for video-to-analytic
alignment of basketball broadcast footage. Given a series of frames as input,
our method quickly and accurately extracts time-remaining and quarter values
from basketball broadcast scenes. Our work intends to expedite the development
of large, multi-modal video datasets to train data-hungry video models in the
sports action recognition domain. Our method aligns a pre-labeled corpus of
play-by-play annotations containing dense event annotations to video frames,
enabling quick retrieval of labeled video segments. Unlike previous methods, we
forgo the need to localize game clocks by fine-tuning an out-of-the-box object
detector to find semantic text regions directly. Our end-to-end approach
improves the generality of our work. Additionally, interpolation and
parallelization techniques prepare our pipeline for deployment in a large
computing cluster. All code is made publicly available.",2024-10-30 17:27:44+00:00,['Levi Harris'],http://arxiv.org/abs/2411.00862v1
RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation,"Vision-and-Language Navigation (VLN) suffers from the limited diversity and
scale of training data, primarily constrained by the manual curation of
existing simulators. To address this, we introduce RoomTour3D, a
video-instruction dataset derived from web-based room tour videos that capture
real-world indoor spaces and human walking demonstrations. Unlike existing VLN
datasets, RoomTour3D leverages the scale and diversity of online videos to
generate open-ended human walking trajectories and open-world navigable
instructions. To compensate for the lack of navigation data in online videos,
we perform 3D reconstruction and obtain 3D trajectories of walking paths
augmented with additional information on the room types, object locations and
3D shape of surrounding scenes. Our dataset includes $\sim$100K open-ended
description-enriched trajectories with $\sim$200K instructions, and 17K
action-enriched trajectories from 1847 room tour environments. We demonstrate
experimentally that RoomTour3D enables significant improvements across multiple
VLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D
facilitates the development of trainable zero-shot VLN agents, showcasing the
potential and challenges of advancing towards open-world navigation.",2024-12-11 18:10:21+00:00,"['Mingfei Han', 'Liang Ma', 'Kamila Zhumakhanova', 'Ekaterina Radionova', 'Jingyi Zhang', 'Xiaojun Chang', 'Xiaodan Liang', 'Ivan Laptev']",http://arxiv.org/abs/2412.08591v2
Investigating Memorization in Video Diffusion Models,"Diffusion models, widely used for image and video generation, face a
significant limitation: the risk of memorizing and reproducing training data
during inference, potentially generating unauthorized copyrighted content.
While prior research has focused on image diffusion models (IDMs), video
diffusion models (VDMs) remain underexplored. To address this gap, we first
formally define the two types of memorization in VDMs (content memorization and
motion memorization) in a practical way that focuses on privacy preservation
and applies to all generation types. We then introduce new metrics specifically
designed to separately assess content and motion memorization in VDMs.
Additionally, we curate a dataset of text prompts that are most prone to
triggering memorization when used as conditioning in VDMs. By leveraging these
prompts, we generate diverse videos from various open-source VDMs, successfully
extracting numerous training videos from each tested model. Through the
application of our proposed metrics, we systematically analyze memorization
across various pretrained VDMs, including text-conditional and unconditional
models, on a variety of datasets. Our comprehensive study reveals that
memorization is widespread across all tested VDMs, indicating that VDMs can
also memorize image training data in addition to video datasets. Finally, we
propose efficient and effective detection strategies for both content and
motion memorization, offering a foundational approach for improving privacy in
VDMs.",2024-10-29 02:34:06+00:00,"['Chen Chen', 'Enhuai Liu', 'Daochang Liu', 'Mubarak Shah', 'Chang Xu']",http://arxiv.org/abs/2410.21669v1
ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation,"Diffusion transformers have demonstrated remarkable performance in visual
generation tasks, such as generating realistic images or videos based on
textual instructions. However, larger model sizes and multi-frame processing
for video generation lead to increased computational and memory costs, posing
challenges for practical deployment on edge devices. Post-Training Quantization
(PTQ) is an effective method for reducing memory costs and computational
complexity. When quantizing diffusion transformers, we find that existing
quantization methods face challenges when applied to text-to-image and video
tasks. To address these challenges, we begin by systematically analyzing the
source of quantization error and conclude with the unique challenges posed by
DiT quantization. Accordingly, we design an improved quantization scheme:
ViDiT-Q (Video & Image Diffusion Transformer Quantization), tailored
specifically for DiT models. We validate the effectiveness of ViDiT-Q across a
variety of text-to-image and video models, achieving W8A8 and W4A8 with
negligible degradation in visual quality and metrics. Additionally, we
implement efficient GPU kernels to achieve practical 2-2.5x memory saving and a
1.4-1.7x end-to-end latency speedup.",2024-06-04 17:57:10+00:00,"['Tianchen Zhao', 'Tongcheng Fang', 'Haofeng Huang', 'Enshu Liu', 'Rui Wan', 'Widyadewi Soedarmadji', 'Shiyao Li', 'Zinan Lin', 'Guohao Dai', 'Shengen Yan', 'Huazhong Yang', 'Xuefei Ning', 'Yu Wang']",http://arxiv.org/abs/2406.02540v3
How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs,"Recent advancements in Large Language Models (LLMs) have led to the
development of Video Large Multi-modal Models (Video-LMMs) that can handle a
wide range of video understanding tasks. These models have the potential to be
deployed in real-world applications such as robotics, AI assistants, medical
surgery, and autonomous vehicles. The widespread adoption of Video-LMMs in our
daily lives underscores the importance of ensuring and evaluating their robust
performance in mirroring human-like reasoning and interaction capabilities in
complex, real-world contexts. However, existing benchmarks for Video-LMMs
primarily focus on general video comprehension abilities and neglect assessing
their reasoning capabilities over complex videos in the real-world context, and
robustness of these models through the lens of user prompts as text queries. In
this paper, we present the Complex Video Reasoning and Robustness Evaluation
Suite (CVRR-ES), a novel benchmark that comprehensively assesses the
performance of Video-LMMs across 11 diverse real-world video dimensions. We
evaluate 9 recent models, including both open-source and closed-source
variants, and find that most of the Video-LMMs, especially open-source ones,
struggle with robustness and reasoning when dealing with complex videos. Based
on our analysis, we develop a training-free Dual-Step Contextual Prompting
(DSCP) technique to enhance the performance of existing Video-LMMs. Our
findings provide valuable insights for building the next generation of
human-centric AI systems with advanced robustness and reasoning capabilities.
Our dataset and code are publicly available at:
https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/.",2024-05-06 17:59:45+00:00,"['Muhammad Uzair Khattak', 'Muhammad Ferjad Naeem', 'Jameel Hassan', 'Muzammal Naseer', 'Federico Tombari', 'Fahad Shahbaz Khan', 'Salman Khan']",http://arxiv.org/abs/2405.03690v2
Vript: A Video Is Worth Thousands of Words,"Advancements in multimodal learning, particularly in video understanding and
generation, require high-quality video-text datasets for improved model
performance. Vript addresses this issue with a meticulously annotated corpus of
12K high-resolution videos, offering detailed, dense, and script-like captions
for over 420K clips. Each clip has a caption of ~145 words, which is over 10x
longer than most video-text datasets. Unlike captions only documenting static
content in previous datasets, we enhance video captioning to video scripting by
documenting not just the content, but also the camera operations, which include
the shot types (medium shot, close-up, etc) and camera movements (panning,
tilting, etc). By utilizing the Vript, we explore three training paradigms of
aligning more text with the video modality rather than clip-caption pairs. This
results in Vriptor, a top-performing video captioning model among open-source
models, comparable to GPT-4V in performance. Vriptor is also a powerful model
capable of end-to-end generation of dense and detailed captions for long
videos. Moreover, we introduce Vript-Hard, a benchmark consisting of three
video understanding tasks that are more challenging than existing benchmarks:
Vript-HAL is the first benchmark evaluating action and object hallucinations in
video LLMs, Vript-RR combines reasoning with retrieval resolving question
ambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the
temporal understanding of events in long videos rather than actions in short
videos in previous works. All code, models, and datasets are available in
https://github.com/mutonix/Vript. PS: We have included more video-text datasets
(Vript_CN & Vript_Multilingual) in the Vript series.",2024-06-10 06:17:55+00:00,"['Dongjie Yang', 'Suyuan Huang', 'Chengqiang Lu', 'Xiaodong Han', 'Haoxin Zhang', 'Yan Gao', 'Yao Hu', 'Hai Zhao']",http://arxiv.org/abs/2406.06040v2
vid-TLDR: Training Free Token merging for Light-weight Video Transformer,"Video Transformers have become the prevalent solution for various video
downstream tasks with superior expressive power and flexibility. However, these
video transformers suffer from heavy computational costs induced by the massive
number of tokens across the entire video frames, which has been the major
barrier to training the model. Further, the patches irrelevant to the main
contents, e.g., backgrounds, degrade the generalization performance of models.
To tackle these issues, we propose training free token merging for lightweight
video Transformer (vid-TLDR) that aims to enhance the efficiency of video
Transformers by merging the background tokens without additional training. For
vid-TLDR, we introduce a novel approach to capture the salient regions in
videos only with the attention map. Further, we introduce the saliency-aware
token merging strategy by dropping the background tokens and sharpening the
object scores. Our experiments show that vid-TLDR significantly mitigates the
computational complexity of video Transformers while achieving competitive
performance compared to the base model without vid-TLDR. Code is available at
https://github.com/mlvlab/vid-TLDR.",2024-03-20 07:15:22+00:00,"['Joonmyung Choi', 'Sanghyeok Lee', 'Jaewon Chu', 'Minhyuk Choi', 'Hyunwoo J. Kim']",http://arxiv.org/abs/2403.13347v2
Understanding Long Videos with Multimodal Language Models,"Large Language Models (LLMs) have allowed recent LLM-based approaches to
achieve excellent performance on long-video understanding benchmarks. We
investigate how extensive world knowledge and strong reasoning skills of
underlying LLMs influence this strong performance. Surprisingly, we discover
that LLM-based approaches can yield surprisingly good accuracy on long-video
tasks with limited video information, sometimes even with no video specific
information. Building on this, we explore injecting video-specific information
into an LLM-based framework. We utilize off-the-shelf vision tools to extract
three object-centric information modalities from videos, and then leverage
natural language as a medium for fusing this information. Our resulting
Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art
performance across multiple video understanding benchmarks. Strong performance
also on robotics domain tasks establish its strong generality. Code:
https://github.com/kahnchana/mvu",2024-03-25 17:59:09+00:00,"['Kanchana Ranasinghe', 'Xiang Li', 'Kumara Kahatapitiya', 'Michael S. Ryoo']",http://arxiv.org/abs/2403.16998v4
Every Shot Counts: Using Exemplars for Repetition Counting in Videos,"Video repetition counting infers the number of repetitions of recurring
actions or motion within a video. We propose an exemplar-based approach that
discovers visual correspondence of video exemplars across repetitions within
target videos. Our proposed Every Shot Counts (ESCounts) model is an
attention-based encoder-decoder that encodes videos of varying lengths
alongside exemplars from the same and different videos. In training, ESCounts
regresses locations of high correspondence to the exemplars within the video.
In tandem, our method learns a latent that encodes representations of general
repetitive motions, which we use for exemplar-free, zero-shot inference.
Extensive experiments over commonly used datasets (RepCount, Countix, and
UCFRep) showcase ESCounts obtaining state-of-the-art performance across all
three datasets. Detailed ablations further demonstrate the effectiveness of our
method.",2024-03-26 19:54:21+00:00,"['Saptarshi Sinha', 'Alexandros Stergiou', 'Dima Damen']",http://arxiv.org/abs/2403.18074v2
AU-vMAE: Knowledge-Guide Action Units Detection via Video Masked Autoencoder,"Current Facial Action Unit (FAU) detection methods generally encounter
difficulties due to the scarcity of labeled video training data and the limited
number of training face IDs, which renders the trained feature extractor
insufficient coverage for modeling the large diversity of inter-person facial
structures and movements. To explicitly address the above challenges, we
propose a novel video-level pre-training scheme by fully exploring the
multi-label property of FAUs in the video as well as the temporal label
consistency. At the heart of our design is a pre-trained video feature
extractor based on the video-masked autoencoder together with a fine-tuning
network that jointly completes the multi-level video FAUs analysis tasks,
\emph{i.e.} integrating both video-level and frame-level FAU detections, thus
dramatically expanding the supervision set from sparse FAUs annotations to ALL
video frames including masked ones. Moreover, we utilize inter-frame and
intra-frame AU pair state matrices as prior knowledge to guide network training
instead of traditional Graph Neural Networks, for better temporal supervision.
Our approach demonstrates substantial enhancement in performance compared to
the existing state-of-the-art methods used in BP4D and DISFA FAUs datasets.",2024-07-16 08:07:47+00:00,"['Qiaoqiao Jin', 'Rui Shi', 'Yishun Dou', 'Bingbing Ni']",http://arxiv.org/abs/2407.11468v1
Delving Deep into Engagement Prediction of Short Videos,"Understanding and modeling the popularity of User Generated Content (UGC)
short videos on social media platforms presents a critical challenge with broad
implications for content creators and recommendation systems. This study delves
deep into the intricacies of predicting engagement for newly published videos
with limited user interactions. Surprisingly, our findings reveal that Mean
Opinion Scores from previous video quality assessment datasets do not strongly
correlate with video engagement levels. To address this, we introduce a
substantial dataset comprising 90,000 real-world UGC short videos from
Snapchat. Rather than relying on view count, average watch time, or rate of
likes, we propose two metrics: normalized average watch percentage (NAWP) and
engagement continuation rate (ECR) to describe the engagement levels of short
videos. Comprehensive multi-modal features, including visual content,
background music, and text data, are investigated to enhance engagement
prediction. With the proposed dataset and two key metrics, our method
demonstrates its ability to predict engagements of short videos purely from
video content.",2024-09-30 23:57:07+00:00,"['Dasong Li', 'Wenjie Li', 'Baili Lu', 'Hongsheng Li', 'Sizhuo Ma', 'Gurunandan Krishnan', 'Jian Wang']",http://arxiv.org/abs/2410.00289v1
Video Instruction Tuning With Synthetic Data,"The development of video large multimodal models (LMMs) has been hindered by
the difficulty of curating large amounts of high-quality raw data from the web.
To address this, we propose an alternative approach by creating a high-quality
synthetic dataset specifically for video instruction-following, namely
LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,
open-ended question-answering (QA), and multiple-choice QA. By training on this
dataset, in combination with existing visual instruction tuning data, we
introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that
LLaVA-Video achieves strong performance across various video benchmarks,
highlighting the effectiveness of our dataset. We plan to release the dataset,
its generation pipeline, and the model checkpoints.",2024-10-03 17:36:49+00:00,"['Yuanhan Zhang', 'Jinming Wu', 'Wei Li', 'Bo Li', 'Zejun Ma', 'Ziwei Liu', 'Chunyuan Li']",http://arxiv.org/abs/2410.02713v2
Whats in a Video: Factorized Autoregressive Decoding for Online Dense Video Captioning,"Generating automatic dense captions for videos that accurately describe their
contents remains a challenging area of research. Most current models require
processing the entire video at once. Instead, we propose an efficient, online
approach which outputs frequent, detailed and temporally aligned captions,
without access to future frames. Our model uses a novel autoregressive
factorized decoding architecture, which models the sequence of visual features
for each time segment, outputting localized descriptions and efficiently
leverages the context from the previous video segments. This allows the model
to output frequent, detailed captions to more comprehensively describe the
video, according to its actual local content, rather than mimic the training
data. Second, we propose an optimization for efficient training and inference,
which enables scaling to longer videos. Our approach shows excellent
performance compared to both offline and online methods, and uses 20\% less
compute. The annotations produced are much more comprehensive and frequent, and
can further be utilized in automatic video tagging and in large-scale video
data harvesting.",2024-11-22 02:46:44+00:00,"['AJ Piergiovanni', 'Dahun Kim', 'Michael S. Ryoo', 'Isaac Noble', 'Anelia Angelova']",http://arxiv.org/abs/2411.14688v1
Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment,"With the rapid development of generative models, Artificial
Intelligence-Generated Contents (AIGC) have exponentially increased in daily
lives. Among them, Text-to-Video (T2V) generation has received widespread
attention. Though many T2V models have been released for generating high
perceptual quality videos, there is still lack of a method to evaluate the
quality of these videos quantitatively. To solve this issue, we establish the
largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The
dataset is composed of 10,000 videos generated by 9 different T2V models. We
also conduct a subjective study to obtain each video's corresponding mean
opinion score. Based on T2VQA-DB, we propose a novel transformer-based model
for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model
extracts features from text-video alignment and video fidelity perspectives,
then it leverages the ability of a large language model to give the prediction
score. Experimental results show that T2VQA outperforms existing T2V metrics
and SOTA video quality assessment models. Quantitative analysis indicates that
T2VQA is capable of giving subjective-align predictions, validating its
effectiveness. The dataset and code will be released at
https://github.com/QMME/T2VQA.",2024-03-18 16:52:49+00:00,"['Tengchuan Kou', 'Xiaohong Liu', 'Zicheng Zhang', 'Chunyi Li', 'Haoning Wu', 'Xiongkuo Min', 'Guangtao Zhai', 'Ning Liu']",http://arxiv.org/abs/2403.11956v5
Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks,"Large language models (LLMs) and large visual language models (LVLMs) have
been at the forefront of the artificial intelligence field, particularly for
tasks like text generation, video captioning, and question-answering.
Typically, it is more applicable to train these models on broader knowledge
bases or datasets to increase generalizability, learn relationships between
topics, and recognize patterns. Instead, we propose to provide instructional
datasets specific to the task of each modality within a distinct domain and
then fine-tune the parameters of the model using LORA. With our approach, we
can eliminate all noise irrelevant to the given task while also ensuring that
the model generates with enhanced precision. For this work, we use Video-LLaVA
to generate recipes given cooking videos without transcripts. Video-LLaVA's
multimodal architecture allows us to provide cooking images to its image
encoder, cooking videos to its video encoder, and general cooking questions to
its text encoder. Thus, we aim to remove all noise unrelated to cooking while
improving our model's capabilities to generate specific ingredient lists and
detailed instructions. As a result, our approach to fine-tuning Video-LLaVA
leads to gains over the baseline Video-LLaVA by 2% on the YouCook2 dataset.
While this may seem like a marginal increase, our model trains on an image
instruction dataset 2.5% the size of Video-LLaVA's and a video instruction
dataset 23.76% of Video-LLaVA's.",2024-06-24 06:39:02+00:00,"['Daniel Wen', 'Nafisa Hussain']",http://arxiv.org/abs/2406.16346v1
ADV2E: Bridging the Gap Between Analogue Circuit and Discrete Frames in the Video-to-Events Simulator,"Event cameras operate fundamentally differently from traditional Active Pixel
Sensor (APS) cameras, offering significant advantages. Recent research has
developed simulators to convert video frames into events, addressing the
shortage of real event datasets. Current simulators primarily focus on the
logical behavior of event cameras. However, the fundamental analogue properties
of pixel circuits are seldom considered in simulator design. The gap between
analogue pixel circuit and discrete video frames causes the degeneration of
synthetic events, particularly in high-contrast scenes. In this paper, we
propose a novel method of generating reliable event data based on a detailed
analysis of the pixel circuitry in event cameras. We incorporate the analogue
properties of event camera pixel circuits into the simulator design: (1)
analogue filtering of signals from light intensity to events, and (2) a cutoff
frequency that is independent of video frame rate. Experimental results on two
relevant tasks, including semantic segmentation and image reconstruction,
validate the reliability of simulated event data, even in high-contrast scenes.
This demonstrates that deep neural networks exhibit strong generalization from
simulated to real event data, confirming that the synthetic events generated by
the proposed method are both realistic and well-suited for effective training.",2024-11-19 05:52:51+00:00,"['Xiao Jiang', 'Fei Zhou', 'Jiongzhi Lin']",http://arxiv.org/abs/2411.12250v1
TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to Video Generation,"Text-driven Image to Video Generation (TI2V) aims to generate controllable
video given the first frame and corresponding textual description. The primary
challenges of this task lie in two parts: (i) how to identify the target
objects and ensure the consistency between the movement trajectory and the
textual description. (ii) how to improve the subjective quality of generated
videos. To tackle the above challenges, we propose a new diffusion-based TI2V
framework, termed TIV-Diffusion, via object-centric textual-visual alignment,
intending to achieve precise control and high-quality video generation based on
textual-described motion for different objects. Concretely, we enable our
TIV-Diffuion model to perceive the textual-described objects and their motion
trajectory by incorporating the fused textual and visual knowledge through
scale-offset modulation. Moreover, to mitigate the problems of object
disappearance and misaligned objects and motion, we introduce an object-centric
textual-visual alignment module, which reduces the risk of misaligned
objects/motion by decoupling the objects in the reference image and aligning
textual features with each object individually. Based on the above innovations,
our TIV-Diffusion achieves state-of-the-art high-quality video generation
compared with existing TI2V methods.",2024-12-13 16:52:13+00:00,"['Xingrui Wang', 'Xin Li', 'Yaosi Hu', 'Hanxin Zhu', 'Chen Hou', 'Cuiling Lan', 'Zhibo Chen']",http://arxiv.org/abs/2412.10275v2
MaterialPicker: Multi-Modal Material Generation with Diffusion Transformers,"High-quality material generation is key for virtual environment authoring and
inverse rendering. We propose MaterialPicker, a multi-modal material generator
leveraging a Diffusion Transformer (DiT) architecture, improving and
simplifying the creation of high-quality materials from text prompts and/or
photographs. Our method can generate a material based on an image crop of a
material sample, even if the captured surface is distorted, viewed at an angle
or partially occluded, as is often the case in photographs of natural scenes.
We further allow the user to specify a text prompt to provide additional
guidance for the generation. We finetune a pre-trained DiT-based video
generator into a material generator, where each material map is treated as a
frame in a video sequence. We evaluate our approach both quantitatively and
qualitatively and show that it enables more diverse material generation and
better distortion correction than previous work.",2024-12-04 11:23:15+00:00,"['Xiaohe Ma', 'Valentin Deschaintre', 'Milo Haan', 'Fujun Luan', 'Kun Zhou', 'Hongzhi Wu', 'Yiwei Hu']",http://arxiv.org/abs/2412.03225v2
"A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights","Human video generation is a dynamic and rapidly evolving task that aims to
synthesize 2D human body video sequences with generative models given control
conditions such as text, audio, and pose. With the potential for wide-ranging
applications in film, gaming, and virtual communication, the ability to
generate natural and realistic human video is critical. Recent advancements in
generative models have laid a solid foundation for the growing interest in this
area. Despite the significant progress, the task of human video generation
remains challenging due to the consistency of characters, the complexity of
human motion, and difficulties in their relationship with the environment. This
survey provides a comprehensive review of the current state of human video
generation, marking, to the best of our knowledge, the first extensive
literature review in this domain. We start with an introduction to the
fundamentals of human video generation and the evolution of generative models
that have facilitated the field's growth. We then examine the main methods
employed for three key sub-tasks within human video generation: text-driven,
audio-driven, and pose-driven motion generation. These areas are explored
concerning the conditions that guide the generation process. Furthermore, we
offer a collection of the most commonly utilized datasets and the evaluation
metrics that are crucial in assessing the quality and realism of generated
videos. The survey concludes with a discussion of the current challenges in the
field and suggests possible directions for future research. The goal of this
survey is to offer the research community a clear and holistic view of the
advancements in human video generation, highlighting the milestones achieved
and the challenges that lie ahead.",2024-07-11 12:09:05+00:00,"['Wentao Lei', 'Jinting Wang', 'Fengji Ma', 'Guanjie Huang', 'Li Liu']",http://arxiv.org/abs/2407.08428v1
AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration,"Diffusion Transformers (DiTs) have proven effective in generating
high-quality videos but are hindered by high computational costs. Existing
video DiT sampling acceleration methods often rely on costly fine-tuning or
exhibit limited generalization capabilities. We propose Asymmetric Reduction
and Restoration (AsymRnR), a training-free and model-agnostic method to
accelerate video DiTs. It builds on the observation that redundancies of
feature tokens in DiTs vary significantly across different model blocks,
denoising steps, and feature types. Our AsymRnR asymmetrically reduces
redundant tokens in the attention operation, achieving acceleration with
negligible degradation in output quality and, in some cases, even improving it.
We also tailored a reduction schedule to distribute the reduction across
components adaptively. To further accelerate this process, we introduce a
matching cache for more efficient reduction. Backed by theoretical foundations
and extensive experimental validation, AsymRnR integrates into state-of-the-art
video DiTs and offers substantial speedup.",2024-12-16 12:28:22+00:00,"['Wenhao Sun', 'Rong-Cheng Tu', 'Jingyi Liao', 'Zhao Jin', 'Dacheng Tao']",http://arxiv.org/abs/2412.11706v2
Advancing Video Quality Assessment for AIGC,"In recent years, AI generative models have made remarkable progress across
various domains, including text generation, image generation, and video
generation. However, assessing the quality of text-to-video generation is still
in its infancy, and existing evaluation frameworks fall short when compared to
those for natural videos. Current video quality assessment (VQA) methods
primarily focus on evaluating the overall quality of natural videos and fail to
adequately account for the substantial quality discrepancies between frames in
generated videos. To address this issue, we propose a novel loss function that
combines mean absolute error with cross-entropy loss to mitigate inter-frame
quality inconsistencies. Additionally, we introduce the innovative S2CNet
technique to retain critical content, while leveraging adversarial training to
enhance the model's generalization capabilities. Experimental results
demonstrate that our method outperforms existing VQA techniques on the AIGC
Video dataset, surpassing the previous state-of-the-art by 3.1% in terms of
PLCC.",2024-09-23 10:36:22+00:00,"['Xinli Yue', 'Jianhui Sun', 'Han Kong', 'Liangchao Yao', 'Tianyi Wang', 'Lei Li', 'Fengyun Rao', 'Jing Lv', 'Fan Xia', 'Yuetang Deng', 'Qian Wang', 'Lingchen Zhao']",http://arxiv.org/abs/2409.14888v1
Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos,"Recent works on dynamic 3D neural field reconstruction assume the input from
synchronized multi-view videos whose poses are known. The input constraints are
often not satisfied in real-world setups, making the approach impractical. We
show that unsynchronized videos from unknown poses can generate dynamic neural
fields as long as the videos capture human motion. Humans are one of the most
common dynamic subjects captured in videos, and their shapes and poses can be
estimated using state-of-the-art libraries. While noisy, the estimated human
shape and pose parameters provide a decent initialization point to start the
highly non-convex and under-constrained problem of training a consistent
dynamic neural representation. Given the shape and pose parameters of humans in
individual frames, we formulate methods to calculate the time offsets between
videos, followed by camera pose estimations that analyze the 3D joint
positions. Then, we train the dynamic neural fields employing multiresolution
grids while we concurrently refine both time offsets and camera poses. The
setup still involves optimizing many parameters; therefore, we introduce a
robust progressive learning strategy to stabilize the process. Experiments show
that our approach achieves accurate spatio-temporal calibration and
high-quality scene reconstruction in challenging conditions.",2024-12-26 07:04:20+00:00,"['Changwoon Choi', 'Jeongjun Kim', 'Geonho Cha', 'Minkwan Kim', 'Dongyoon Wee', 'Young Min Kim']",http://arxiv.org/abs/2412.19089v2
Match Stereo Videos via Bidirectional Alignment,"Video stereo matching is the task of estimating consistent disparity maps
from rectified stereo videos. There is considerable scope for improvement in
both datasets and methods within this area. Recent learning-based methods often
focus on optimizing performance for independent stereo pairs, leading to
temporal inconsistencies in videos. Existing video methods typically employ
sliding window operation over time dimension, which can result in low-frequency
oscillations corresponding to the window size. To address these challenges, we
propose a bidirectional alignment mechanism for adjacent frames as a
fundamental operation. Building on this, we introduce a novel video processing
framework, BiDAStereo, and a plugin stabilizer network, BiDAStabilizer,
compatible with general image-based methods. Regarding datasets, current
synthetic object-based and indoor datasets are commonly used for training and
benchmarking, with a lack of outdoor nature scenarios. To bridge this gap, we
present a realistic synthetic dataset and benchmark focused on natural scenes,
along with a real-world dataset captured by a stereo camera in diverse urban
scenes for qualitative evaluation. Extensive experiments on in-domain,
out-of-domain, and robustness evaluation demonstrate the contribution of our
methods and datasets, showcasing improvements in prediction quality and
achieving state-of-the-art results on various commonly used benchmarks. The
project page, demos, code, and datasets are available at:
\url{https://tomtomtommi.github.io/BiDAVideo/}.",2024-09-30 13:37:29+00:00,"['Junpeng Jing', 'Ye Mao', 'Anlan Qiu', 'Krystian Mikolajczyk']",http://arxiv.org/abs/2409.20283v1
Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation,"Echocardiography video is a primary modality for diagnosing heart diseases,
but the limited data poses challenges for both clinical teaching and machine
learning training. Recently, video generative models have emerged as a
promising strategy to alleviate this issue. However, previous methods often
relied on holistic conditions during generation, hindering the flexible
movement control over specific cardiac structures. In this context, we propose
an explainable and controllable method for echocardiography video generation,
taking an initial frame and a motion curve as guidance. Our contributions are
three-fold. First, we extract motion information from each heart substructure
to construct motion curves, enabling the diffusion model to synthesize
customized echocardiography videos by modifying these curves. Second, we
propose the structure-to-motion alignment module, which can map semantic
features onto motion curves across cardiac structures. Third, The
position-aware attention mechanism is designed to enhance video consistency
utilizing Gaussian masks with structural position information. Extensive
experiments on three echocardiography datasets show that our method outperforms
others regarding fidelity and consistency. The full code will be released at
https://github.com/mlmi-2024-72/ECM.",2024-07-31 09:59:20+00:00,"['Junxuan Yu', 'Rusi Chen', 'Yongsong Zhou', 'Yanlin Chen', 'Yaofei Duan', 'Yuhao Huang', 'Han Zhou', 'Tan Tao', 'Xin Yang', 'Dong Ni']",http://arxiv.org/abs/2407.21490v1
Video-to-Audio Generation with Fine-grained Temporal Semantics,"With recent advances of AIGC, video generation have gained a surge of
research interest in both academia and industry (e.g., Sora). However, it
remains a challenge to produce temporally aligned audio to synchronize the
generated video, considering the complicated semantic information included in
the latter. In this work, inspired by the recent success of text-to-audio (TTA)
generation, we first investigate the video-to-audio (VTA) generation framework
based on latent diffusion model (LDM). Similar to latest pioneering exploration
in VTA, our preliminary results also show great potentials of LDM in VTA task,
but it still suffers from sub-optimal temporal alignment. To this end, we
propose to enhance the temporal alignment of VTA with frame-level semantic
information. With the recently popular grounding segment anything model
(Grounding SAM), we can extract the fine-grained semantics in video frames to
enable VTA to produce better-aligned audio signal. Extensive experiments
demonstrate the effectiveness of our system on both objective and subjective
evaluation metrics, which shows both better audio quality and fine-grained
temporal alignment.",2024-09-23 05:05:47+00:00,"['Yuchen Hu', 'Yu Gu', 'Chenxing Li', 'Rilin Chen', 'Dong Yu']",http://arxiv.org/abs/2409.14709v1
SONIQUE: Video Background Music Generation Using Unpaired Audio-Visual Data,"We present SONIQUE, a model for generating background music tailored to video
content. Unlike traditional video-to-music generation approaches, which rely
heavily on paired audio-visual datasets, SONIQUE leverages unpaired data,
combining royalty-free music and independent video sources. By utilizing large
language models (LLMs) for video understanding and converting visual
descriptions into musical tags, alongside a U-Net-based conditional diffusion
model, SONIQUE enables customizable music generation. Users can control
specific aspects of the music, such as instruments, genres, tempo, and
melodies, ensuring the generated output fits their creative vision. SONIQUE is
open-source, with a demo available online.",2024-10-04 19:22:35+00:00,"['Liqian Zhang', 'Magdalena Fuentes']",http://arxiv.org/abs/2410.03879v2
DragEntity: Trajectory Guided Video Generation using Entity and Positional Relationships,"In recent years, diffusion models have achieved tremendous success in the
field of video generation, with controllable video generation receiving
significant attention. However, existing control methods still face two
limitations: Firstly, control conditions (such as depth maps, 3D Mesh) are
difficult for ordinary users to obtain directly. Secondly, it's challenging to
drive multiple objects through complex motions with multiple trajectories
simultaneously. In this paper, we introduce DragEntity, a video generation
model that utilizes entity representation for controlling the motion of
multiple objects. Compared to previous methods, DragEntity offers two main
advantages: 1) Our method is more user-friendly for interaction because it
allows users to drag entities within the image rather than individual pixels.
2) We use entity representation to represent any object in the image, and
multiple objects can maintain relative spatial relationships. Therefore, we
allow multiple trajectories to control multiple objects in the image with
different levels of complexity simultaneously. Our experiments validate the
effectiveness of DragEntity, demonstrating its excellent performance in
fine-grained control in video generation.",2024-10-14 17:24:35+00:00,"['Zhang Wan', 'Sheng Tang', 'Jiawei Wei', 'Ruize Zhang', 'Juan Cao']",http://arxiv.org/abs/2410.10751v1
"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models","Sora is a text-to-video generative AI model, released by OpenAI in February
2024. The model is trained to generate videos of realistic or imaginative
scenes from text instructions and show potential in simulating the physical
world. Based on public technical reports and reverse engineering, this paper
presents a comprehensive review of the model's background, related
technologies, applications, remaining challenges, and future directions of
text-to-video AI models. We first trace Sora's development and investigate the
underlying technologies used to build this ""world simulator"". Then, we describe
in detail the applications and potential impact of Sora in multiple industries
ranging from film-making and education to marketing. We discuss the main
challenges and limitations that need to be addressed to widely deploy Sora,
such as ensuring safe and unbiased video generation. Lastly, we discuss the
future development of Sora and video generation models in general, and how
advancements in the field could enable new ways of human-AI interaction,
boosting productivity and creativity of video generation.",2024-02-27 03:30:58+00:00,"['Yixin Liu', 'Kai Zhang', 'Yuan Li', 'Zhiling Yan', 'Chujie Gao', 'Ruoxi Chen', 'Zhengqing Yuan', 'Yue Huang', 'Hanchi Sun', 'Jianfeng Gao', 'Lifang He', 'Lichao Sun']",http://arxiv.org/abs/2402.17177v3
UniPaint: Unified Space-time Video Inpainting via Mixture-of-Experts,"In this paper, we present UniPaint, a unified generative space-time video
inpainting framework that enables spatial-temporal inpainting and
interpolation. Different from existing methods that treat video inpainting and
video interpolation as two distinct tasks, we leverage a unified inpainting
framework to tackle them and observe that these two tasks can mutually enhance
synthesis performance. Specifically, we first introduce a plug-and-play
space-time video inpainting adapter, which can be employed in various
personalized models. The key insight is to propose a Mixture of Experts (MoE)
attention to cover various tasks. Then, we design a spatial-temporal masking
strategy during the training stage to mutually enhance each other and improve
performance. UniPaint produces high-quality and aesthetically pleasing results,
achieving the best quantitative results across various tasks and scale setups.
The code and checkpoints will be available soon.",2024-12-09 09:45:14+00:00,"['Zhen Wan', 'Yue Ma', 'Chenyang Qi', 'Zhiheng Liu', 'Tao Gui']",http://arxiv.org/abs/2412.06340v1
Image Compression Using Novel View Synthesis Priors,"Real-time visual feedback is essential for tetherless control of remotely
operated vehicles, particularly during inspection and manipulation tasks.
Though acoustic communication is the preferred choice for medium-range
communication underwater, its limited bandwidth renders it impractical to
transmit images or videos in real-time. To address this, we propose a
model-based image compression technique that leverages prior mission
information. Our approach employs trained machine-learning based novel view
synthesis models, and uses gradient descent optimization to refine latent
representations to help generate compressible differences between camera images
and rendered images. We evaluate the proposed compression technique using a
dataset from an artificial ocean basin, demonstrating superior compression
ratios and image quality over existing techniques. Moreover, our method
exhibits robustness to introduction of new objects within the scene,
highlighting its potential for advancing tetherless remotely operated vehicle
operations.",2024-11-21 05:46:06+00:00,"['Luyuan Peng', 'Mandar Chitre', 'Hari Vishnu', 'Yuen Min Too', 'Bharath Kalyan', 'Rajat Mishra', 'Soo Pieng Tan']",http://arxiv.org/abs/2411.13862v2
Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance,"Recent advances in image generation, particularly via diffusion models, have
led to impressive improvements in image synthesis quality. Despite this,
diffusion models are still challenged by model-induced artifacts and limited
stability in image fidelity. In this work, we hypothesize that the primary
cause of this issue is the improper resampling operation that introduces
aliasing in the diffusion model and a careful alias-free resampling dictated by
image processing theory can improve the model's performance in image synthesis.
We propose the integration of alias-free resampling layers into the UNet
architecture of diffusion models without adding extra trainable parameters,
thereby maintaining computational efficiency. We then assess whether these
theory-driven modifications enhance image quality and rotational equivariance.
Our experimental results on benchmark datasets, including CIFAR-10, MNIST, and
MNIST-M, reveal consistent gains in image quality, particularly in terms of FID
and KID scores. Furthermore, we propose a modified diffusion process that
enables user-controlled rotation of generated images without requiring
additional training. Our findings highlight the potential of theory-driven
enhancements such as alias-free resampling in generative models to improve
image quality while maintaining model efficiency and pioneer future research
directions to incorporate them into video-generating diffusion models, enabling
deeper exploration of the applications of alias-free resampling in generative
modeling.",2024-11-14 04:23:28+00:00,['Md Fahim Anjum'],http://arxiv.org/abs/2411.09174v1
Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis,"One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from
an unseen image, and then animate it with a reference video or audio to
generate a talking portrait video. The existing methods fail to simultaneously
achieve the goals of accurate 3D avatar reconstruction and stable talking face
animation. Besides, while the existing works mainly focus on synthesizing the
head part, it is also vital to generate natural torso and background segments
to obtain a realistic talking portrait video. To address these limitations, we
present Real3D-Potrait, a framework that (1) improves the one-shot 3D
reconstruction power with a large image-to-plane model that distills 3D prior
knowledge from a 3D face generative model; (2) facilitates accurate
motion-conditioned animation with an efficient motion adapter; (3) synthesizes
realistic video with natural torso movement and switchable background using a
head-torso-background super-resolution model; and (4) supports one-shot
audio-driven talking face generation with a generalizable audio-to-motion
model. Extensive experiments show that Real3D-Portrait generalizes well to
unseen identities and generates more realistic talking portrait videos compared
to previous methods. Video samples and source code are available at
https://real3dportrait.github.io .",2024-01-16 17:04:30+00:00,"['Zhenhui Ye', 'Tianyun Zhong', 'Yi Ren', 'Jiaqi Yang', 'Weichuang Li', 'Jiawei Huang', 'Ziyue Jiang', 'Jinzheng He', 'Rongjie Huang', 'Jinglin Liu', 'Chen Zhang', 'Xiang Yin', 'Zejun Ma', 'Zhou Zhao']",http://arxiv.org/abs/2401.08503v3
HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness,"We study the problem of precisely swapping objects in videos, with a focus on
those interacted with by hands, given one user-provided reference object image.
Despite the great advancements that diffusion models have made in video editing
recently, these models often fall short in handling the intricacies of
hand-object interactions (HOI), failing to produce realistic edits --
especially when object swapping results in object shape or functionality
changes. To bridge this gap, we present HOI-Swap, a novel diffusion-based video
editing framework trained in a self-supervised manner. Designed in two stages,
the first stage focuses on object swapping in a single frame with HOI
awareness; the model learns to adjust the interaction patterns, such as the
hand grasp, based on changes in the object's properties. The second stage
extends the single-frame edit across the entire sequence; we achieve
controllable motion alignment with the original video by: (1) warping a new
sequence from the stage-I edited frame based on sampled motion points and (2)
conditioning video generation on the warped sequence. Comprehensive qualitative
and quantitative evaluations demonstrate that HOI-Swap significantly
outperforms existing methods, delivering high-quality video edits with
realistic HOIs.",2024-06-11 22:31:29+00:00,"['Zihui Xue', 'Mi Luo', 'Changan Chen', 'Kristen Grauman']",http://arxiv.org/abs/2406.07754v2
TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models,"Recent advances in text-to-video generation have demonstrated the utility of
powerful diffusion models. Nevertheless, the problem is not trivial when
shaping diffusion models to animate static image (i.e., image-to-video
generation). The difficulty originates from the aspect that the diffusion
process of subsequent animated frames should not only preserve the faithful
alignment with the given image but also pursue temporal coherence among
adjacent frames. To alleviate this, we present TRIP, a new recipe of
image-to-video diffusion paradigm that pivots on image noise prior derived from
static image to jointly trigger inter-frame relational reasoning and ease the
coherent temporal modeling via temporal residual learning. Technically, the
image noise prior is first attained through one-step backward diffusion process
based on both static image and noised video latent codes. Next, TRIP executes a
residual-like dual-path scheme for noise prediction: 1) a shortcut path that
directly takes image noise prior as the reference noise of each frame to
amplify the alignment between the first frame and subsequent frames; 2) a
residual path that employs 3D-UNet over noised video and static image latent
codes to enable inter-frame relational reasoning, thereby easing the learning
of the residual noise for each frame. Furthermore, both reference and residual
noise of each frame are dynamically merged via attention mechanism for final
video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT
datasets demonstrate the effectiveness of our TRIP for image-to-video
generation. Please see our project page at https://trip-i2v.github.io/TRIP/.",2024-03-25 17:59:40+00:00,"['Zhongwei Zhang', 'Fuchen Long', 'Yingwei Pan', 'Zhaofan Qiu', 'Ting Yao', 'Yang Cao', 'Tao Mei']",http://arxiv.org/abs/2403.17005v1
Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization,"In light of recent advances in multimodal Large Language Models (LLMs), there
is increasing attention to scaling them from image-text data to more
informative real-world videos. Compared to static images, video poses unique
challenges for effective large-scale pre-training due to the modeling of its
spatiotemporal dynamics. In this paper, we address such limitations in
video-language pre-training with an efficient video decomposition that
represents each video as keyframes and temporal motions. These are then adapted
to an LLM using well-designed tokenizers that discretize visual and temporal
information as a few tokens, thus enabling unified generative pre-training of
videos, images, and text. At inference, the generated tokens from the LLM are
carefully recovered to the original continuous pixel space to create various
video content. Our proposed framework is both capable of comprehending and
generating image and video content, as demonstrated by its competitive
performance across 13 multimodal benchmarks in image and video understanding
and generation. Our code and models are available at
https://video-lavit.github.io.",2024-02-05 16:30:49+00:00,"['Yang Jin', 'Zhicheng Sun', 'Kun Xu', 'Kun Xu', 'Liwei Chen', 'Hao Jiang', 'Quzhe Huang', 'Chengru Song', 'Yuliang Liu', 'Di Zhang', 'Yang Song', 'Kun Gai', 'Yadong Mu']",http://arxiv.org/abs/2402.03161v3
AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks,"In the dynamic field of digital content creation using generative models,
state-of-the-art video editing models still do not offer the level of quality
and control that users desire. Previous works on video editing either extended
from image-based generative models in a zero-shot manner or necessitated
extensive fine-tuning, which can hinder the production of fluid video edits.
Furthermore, these methods frequently rely on textual input as the editing
guidance, leading to ambiguities and limiting the types of edits they can
perform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free
paradigm designed to simplify video editing into two primary steps: (1)
employing an off-the-shelf image editing model to modify the first frame, (2)
utilizing an existing image-to-video generation model to generate the edited
video through temporal feature injection. AnyV2V can leverage any existing
image editing tools to support an extensive array of video editing tasks,
including prompt-based editing, reference-based style transfer, subject-driven
editing, and identity manipulation, which were unattainable by previous
methods. AnyV2V can also support any video length. Our evaluation shows that
AnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,
AnyV2V significantly outperformed these baselines in human evaluations,
demonstrating notable improvements in visual consistency with the source video
while producing high-quality edits across all editing tasks.",2024-03-21 15:15:00+00:00,"['Max Ku', 'Cong Wei', 'Weiming Ren', 'Harry Yang', 'Wenhu Chen']",http://arxiv.org/abs/2403.14468v4
Goldfish: Vision-Language Understanding of Arbitrarily Long Videos,"Most current LLM-based models for video understanding can process videos
within minutes. However, they struggle with lengthy videos due to challenges
such as ""noise and redundancy"", as well as ""memory and computation""
constraints. In this paper, we present Goldfish, a methodology tailored for
comprehending videos of arbitrary lengths. We also introduce the TVQA-long
benchmark, specifically designed to evaluate models' capabilities in
understanding long videos with questions in both vision and text content.
Goldfish approaches these challenges with an efficient retrieval mechanism that
initially gathers the top-k video clips relevant to the instruction before
proceeding to provide the desired response. This design of the retrieval
mechanism enables the Goldfish to efficiently process arbitrarily long video
sequences, facilitating its application in contexts such as movies or
television series. To facilitate the retrieval process, we developed
MiniGPT4-Video that generates detailed descriptions for the video clips. In
addressing the scarcity of benchmarks for long video evaluation, we adapted the
TVQA short video benchmark for extended content analysis by aggregating
questions from entire episodes, thereby shifting the evaluation from partial to
full episode comprehension. We attained a 41.78% accuracy rate on the TVQA-long
benchmark, surpassing previous methods by 14.94%. Our MiniGPT4-Video also shows
exceptional performance in short video comprehension, exceeding existing
state-of-the-art methods by 3.23%, 2.03%, 16.5% and 23.59% on the MSVD, MSRVTT,
TGIF, and TVQA short video benchmarks, respectively. These results indicate
that our models have significant improvements in both long and short-video
understanding. Our models and code have been made publicly available at
https://vision-cair.github.io/Goldfish_website/",2024-07-17 15:59:32+00:00,"['Kirolos Ataallah', 'Xiaoqian Shen', 'Eslam Abdelrahman', 'Essam Sleiman', 'Mingchen Zhuge', 'Jian Ding', 'Deyao Zhu', 'Jrgen Schmidhuber', 'Mohamed Elhoseiny']",http://arxiv.org/abs/2407.12679v1
"You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale","Recent 3D generation models typically rely on limited-scale 3D `gold-labels'
or 2D diffusion priors for 3D content creation. However, their performance is
upper-bounded by constrained 3D priors due to the lack of scalable learning
paradigms. In this work, we present See3D, a visual-conditional multi-view
diffusion model trained on large-scale Internet videos for open-world 3D
creation. The model aims to Get 3D knowledge by solely Seeing the visual
contents from the vast and rapidly growing video data -- You See it, You Got
it. To achieve this, we first scale up the training data using a proposed data
curation pipeline that automatically filters out multi-view inconsistencies and
insufficient observations from source videos. This results in a high-quality,
richly diverse, large-scale dataset of multi-view images, termed WebVi3D,
containing 320M frames from 16M video clips. Nevertheless, learning generic 3D
priors from videos without explicit 3D geometry or camera pose annotations is
nontrivial, and annotating poses for web-scale videos is prohibitively
expensive. To eliminate the need for pose conditions, we introduce an
innovative visual-condition - a purely 2D-inductive visual signal generated by
adding time-dependent noise to the masked video data. Finally, we introduce a
novel visual-conditional 3D generation framework by integrating See3D into a
warping-based pipeline for high-fidelity 3D generation. Our numerical and
visual comparisons on single and sparse reconstruction benchmarks show that
See3D, trained on cost-effective and scalable video data, achieves notable
zero-shot and open-world generation capabilities, markedly outperforming models
trained on costly and constrained 3D datasets. Please refer to our project page
at: https://vision.baai.ac.cn/see3d",2024-12-09 17:44:56+00:00,"['Baorui Ma', 'Huachen Gao', 'Haoge Deng', 'Zhengxiong Luo', 'Tiejun Huang', 'Lulu Tang', 'Xinlong Wang']",http://arxiv.org/abs/2412.06699v3
DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark,"Recently, video generation techniques have advanced rapidly. Given the
popularity of video content on social media platforms, these models intensify
concerns about the spread of fake information. Therefore, there is a growing
demand for detectors capable of distinguishing between fake AI-generated videos
and mitigating the potential harm caused by fake information. However, the lack
of large-scale datasets from the most advanced video generators poses a barrier
to the development of such detectors. To address this gap, we introduce the
first AI-generated video detection dataset, GenVideo. It features the following
characteristics: (1) a large volume of videos, including over one million
AI-generated and real videos collected; (2) a rich diversity of generated
content and methodologies, covering a broad spectrum of video categories and
generation techniques. We conducted extensive studies of the dataset and
proposed two evaluation methods tailored for real-world-like scenarios to
assess the detectors' performance: the cross-generator video classification
task assesses the generalizability of trained detectors on generators; the
degraded video classification task evaluates the robustness of detectors to
handle videos that have degraded in quality during dissemination. Moreover, we
introduced a plug-and-play module, named Detail Mamba (DeMamba), designed to
enhance the detectors by identifying AI-generated videos through the analysis
of inconsistencies in temporal and spatial dimensions. Our extensive
experiments demonstrate DeMamba's superior generalizability and robustness on
GenVideo compared to existing detectors. We believe that the GenVideo dataset
and the DeMamba module will significantly advance the field of AI-generated
video detection. Our code and dataset will be aviliable at
\url{https://github.com/chenhaoxing/DeMamba}.",2024-05-30 05:36:12+00:00,"['Haoxing Chen', 'Yan Hong', 'Zizheng Huang', 'Zhuoer Xu', 'Zhangxuan Gu', 'Yaohui Li', 'Jun Lan', 'Huijia Zhu', 'Jianfu Zhang', 'Weiqiang Wang', 'Huaxiong Li']",http://arxiv.org/abs/2405.19707v3
StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation,"For recent diffusion-based generative models, maintaining consistent content
across a series of generated images, especially those containing subjects and
complex details, presents a significant challenge. In this paper, we propose a
new way of self-attention calculation, termed Consistent Self-Attention, that
significantly boosts the consistency between the generated images and augments
prevalent pretrained diffusion-based text-to-image models in a zero-shot
manner. To extend our method to long-range video generation, we further
introduce a novel semantic space temporal motion prediction module, named
Semantic Motion Predictor. It is trained to estimate the motion conditions
between two provided images in the semantic spaces. This module converts the
generated sequence of images into videos with smooth transitions and consistent
subjects that are significantly more stable than the modules based on latent
spaces only, especially in the context of long video generation. By merging
these two novel components, our framework, referred to as StoryDiffusion, can
describe a text-based story with consistent images or videos encompassing a
rich variety of contents. The proposed StoryDiffusion encompasses pioneering
explorations in visual story generation with the presentation of images and
videos, which we hope could inspire more research from the aspect of
architectural modifications. Our code is made publicly available at
https://github.com/HVision-NKU/StoryDiffusion.",2024-05-02 16:25:16+00:00,"['Yupeng Zhou', 'Daquan Zhou', 'Ming-Ming Cheng', 'Jiashi Feng', 'Qibin Hou']",http://arxiv.org/abs/2405.01434v1
Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners,"Video and audio content creation serves as the core technique for the movie
industry and professional users. Recently, existing diffusion-based methods
tackle video and audio generation separately, which hinders the technique
transfer from academia to industry. In this work, we aim at filling the gap,
with a carefully designed optimization-based framework for cross-visual-audio
and joint-visual-audio generation. We observe the powerful generation ability
of off-the-shelf video or audio generation models. Thus, instead of training
the giant models from scratch, we propose to bridge the existing strong models
with a shared latent representation space. Specifically, we propose a
multimodality latent aligner with the pre-trained ImageBind model. Our latent
aligner shares a similar core as the classifier guidance that guides the
diffusion denoising process during inference time. Through carefully designed
optimization strategy and loss functions, we show the superior performance of
our method on joint video-audio generation, visual-steered audio generation,
and audio-steered visual generation tasks. The project website can be found at
https://yzxing87.github.io/Seeing-and-Hearing/",2024-02-27 17:57:04+00:00,"['Yazhou Xing', 'Yingqing He', 'Zeyue Tian', 'Xintao Wang', 'Qifeng Chen']",http://arxiv.org/abs/2402.17723v1
Interactive Generation of Laparoscopic Videos with Diffusion Models,"Generative AI, in general, and synthetic visual data generation, in specific,
hold much promise for benefiting surgical training by providing photorealism to
simulation environments. Current training methods primarily rely on reading
materials and observing live surgeries, which can be time-consuming and
impractical. In this work, we take a significant step towards improving the
training process. Specifically, we use diffusion models in combination with a
zero-shot video diffusion method to interactively generate realistic
laparoscopic images and videos by specifying a surgical action through text and
guiding the generation with tool positions through segmentation masks. We
demonstrate the performance of our approach using the publicly available Cholec
dataset family and evaluate the fidelity and factual correctness of our
generated images using a surgical action recognition model as well as the
pixel-wise F1-score for the spatial control of tool generation. We achieve an
FID of 38.097 and an F1-score of 0.71.",2024-04-23 12:36:07+00:00,"['Ivan Iliash', 'Simeon Allmendinger', 'Felix Meissen', 'Niklas Khl', 'Daniel Rckert']",http://arxiv.org/abs/2406.06537v1
Motion Prompting: Controlling Video Generation with Motion Trajectories,"Motion control is crucial for generating expressive and compelling video
content; however, most existing video generation models rely mainly on text
prompts for control, which struggle to capture the nuances of dynamic actions
and temporal compositions. To this end, we train a video generation model
conditioned on spatio-temporally sparse or dense motion trajectories. In
contrast to prior motion conditioning work, this flexible representation can
encode any number of trajectories, object-specific or global scene motion, and
temporally sparse motion; due to its flexibility we refer to this conditioning
as motion prompts. While users may directly specify sparse trajectories, we
also show how to translate high-level user requests into detailed, semi-dense
motion prompts, a process we term motion prompt expansion. We demonstrate the
versatility of our approach through various applications, including camera and
object motion control, ""interacting"" with an image, motion transfer, and image
editing. Our results showcase emergent behaviors, such as realistic physics,
suggesting the potential of motion prompts for probing video models and
interacting with future generative world models. Finally, we evaluate
quantitatively, conduct a human study, and demonstrate strong performance.
Video results are available on our webpage: https://motion-prompting.github.io/",2024-12-03 18:59:56+00:00,"['Daniel Geng', 'Charles Herrmann', 'Junhwa Hur', 'Forrester Cole', 'Serena Zhang', 'Tobias Pfaff', 'Tatiana Lopez-Guevara', 'Carl Doersch', 'Yusuf Aytar', 'Michael Rubinstein', 'Chen Sun', 'Oliver Wang', 'Andrew Owens', 'Deqing Sun']",http://arxiv.org/abs/2412.02700v1
E2ED^2:Direct Mapping from Noise to Data for Enhanced Diffusion Models,"Diffusion models have established themselves as the de facto primary paradigm
in visual generative modeling, revolutionizing the field through remarkable
success across various diverse applications ranging from high-quality image
synthesis to temporal aware video generation. Despite these advancements, three
fundamental limitations persist, including 1) discrepancy between training and
inference processes, 2) progressive information leakage throughout the noise
corruption procedures, and 3) inherent constraints preventing effective
integration of modern optimization criteria like perceptual and adversarial
loss. To mitigate these critical challenges, we in this paper present a novel
end-to-end learning paradigm that establishes direct optimization from the
final generated samples to initial noises. Our proposed End-to-End
Differentiable Diffusion, dubbed E2ED^2, introduces several key improvements:
it eliminates the sequential training-sampling mismatch and intermediate
information leakage via conceptualizing training as a direct transformation
from isotropic Gaussian noise to the target data distribution. Additionally,
such training framework enables seamless incorporation of adversarial and
perceptual losses into the core optimization objective. Comprehensive
evaluation across standard benchmarks including COCO30K and HW30K reveals that
our method achieves substantial performance gains in terms of Fr\'echet
Inception Distance (FID) and CLIP score, even with fewer sampling steps (less
than 4). Our findings highlight that the end-to-end mechanism might pave the
way for more robust and efficient solutions, \emph{i.e.,} combining diffusion
stability with GAN-like discriminative optimization in an end-to-end manner.",2024-12-30 16:06:31+00:00,"['Zhiyu Tan', 'WenXu Qian', 'Hesen Chen', 'Mengping Yang', 'Lei Chen', 'Hao Li']",http://arxiv.org/abs/2412.21044v2
Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile,"Currently, image generation and synthesis have remarkably progressed with
generative models. Despite photo-realistic results, intrinsic discrepancies are
still observed in the frequency domain. The spectral discrepancy appeared not
only in generative adversarial networks but in diffusion models. In this study,
we propose a framework to effectively mitigate the disparity in frequency
domain of the generated images to improve generative performance of both GAN
and diffusion models. This is realized by spectrum translation for the
refinement of image generation (STIG) based on contrastive learning. We adopt
theoretical logic of frequency components in various generative networks. The
key idea, here, is to refine the spectrum of the generated image via the
concept of image-to-image translation and contrastive learning in terms of
digital signal processing. We evaluate our framework across eight fake image
datasets and various cutting-edge models to demonstrate the effectiveness of
STIG. Our framework outperforms other cutting-edges showing significant
decreases in FID and log frequency distance of spectrum. We further emphasize
that STIG improves image quality by decreasing the spectral anomaly.
Additionally, validation results present that the frequency-based deepfake
detector confuses more in the case where fake spectrums are manipulated by
STIG.",2024-03-08 06:39:24+00:00,"['Seokjun Lee', 'Seung-Won Jung', 'Hyunseok Seo']",http://arxiv.org/abs/2403.05093v1
QVD: Post-training Quantization for Video Diffusion Models,"Recently, video diffusion models (VDMs) have garnered significant attention
due to their notable advancements in generating coherent and realistic video
content. However, processing multiple frame features concurrently, coupled with
the considerable model size, results in high latency and extensive memory
consumption, hindering their broader application. Post-training quantization
(PTQ) is an effective technique to reduce memory footprint and improve
computational efficiency. Unlike image diffusion, we observe that the temporal
features, which are integrated into all frame features, exhibit pronounced
skewness. Furthermore, we investigate significant inter-channel disparities and
asymmetries in the activation of video diffusion models, resulting in low
coverage of quantization levels by individual channels and increasing the
challenge of quantization. To address these issues, we introduce the first PTQ
strategy tailored for video diffusion models, dubbed QVD. Specifically, we
propose the High Temporal Discriminability Quantization (HTDQ) method, designed
for temporal features, which retains the high discriminability of quantized
features, providing precise temporal guidance for all video frames. In
addition, we present the Scattered Channel Range Integration (SCRI) method
which aims to improve the coverage of quantization levels across individual
channels. Experimental validations across various models, datasets, and
bit-width settings demonstrate the effectiveness of our QVD in terms of diverse
metrics. In particular, we achieve near-lossless performance degradation on
W8A8, outperforming the current methods by 205.12 in FVD.",2024-07-16 10:47:27+00:00,"['Shilong Tian', 'Hong Chen', 'Chengtao Lv', 'Yu Liu', 'Jinyang Guo', 'Xianglong Liu', 'Shengxi Li', 'Hao Yang', 'Tao Xie']",http://arxiv.org/abs/2407.11585v2
SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset,"To mitigate the risk of harmful outputs from large vision models (LVMs), we
introduce the SafeSora dataset to promote research on aligning text-to-video
generation with human values. This dataset encompasses human preferences in
text-to-video generation tasks along two primary dimensions: helpfulness and
harmlessness. To capture in-depth human preferences and facilitate structured
reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and
harmlessness into 12 sub-categories, serving as the basis for pilot
annotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique
videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations
labeled by humans. We further demonstrate the utility of the SafeSora dataset
through several applications, including training the text-video moderation
model and aligning LVMs with human preference by fine-tuning a prompt
augmentation module or the diffusion model. These applications highlight its
potential as the foundation for text-to-video alignment research, such as human
preference modeling and the development and validation of alignment algorithms.",2024-06-20 16:38:56+00:00,"['Josef Dai', 'Tianle Chen', 'Xuyao Wang', 'Ziran Yang', 'Taiye Chen', 'Jiaming Ji', 'Yaodong Yang']",http://arxiv.org/abs/2406.14477v1
ECHOPulse: ECG controlled echocardio-grams video generation,"Echocardiography (ECHO) is essential for cardiac assessments, but its video
quality and interpretation heavily relies on manual expertise, leading to
inconsistent results from clinical and portable devices. ECHO video generation
offers a solution by improving automated monitoring through synthetic data and
generating high-quality videos from routine health data. However, existing
models often face high computational costs, slow inference, and rely on complex
conditional prompts that require experts' annotations. To address these
challenges, we propose ECHOPULSE, an ECG-conditioned ECHO video generation
model. ECHOPULSE introduces two key advancements: (1) it accelerates ECHO video
generation by leveraging VQ-VAE tokenization and masked visual token modeling
for fast decoding, and (2) it conditions on readily accessible ECG signals,
which are highly coherent with ECHO videos, bypassing complex conditional
prompts. To the best of our knowledge, this is the first work to use
time-series prompts like ECG signals for ECHO video generation. ECHOPULSE not
only enables controllable synthetic ECHO data generation but also provides
updated cardiac function information for disease monitoring and prediction
beyond ECG alone. Evaluations on three public and private datasets demonstrate
state-of-the-art performance in ECHO video generation across both qualitative
and quantitative measures. Additionally, ECHOPULSE can be easily generalized to
other modality generation tasks, such as cardiac MRI, fMRI, and 3D CT
generation. Demo can seen from
\url{https://github.com/levyisthebest/ECHOPulse_Prelease}.",2024-10-04 04:49:56+00:00,"['Yiwei Li', 'Sekeun Kim', 'Zihao Wu', 'Hanqi Jiang', 'Yi Pan', 'Pengfei Jin', 'Sifan Song', 'Yucheng Shi', 'Tianming Liu', 'Quanzheng Li', 'Xiang Li']",http://arxiv.org/abs/2410.03143v2
Decoupling Degradations with Recurrent Network for Video Restoration in Under-Display Camera,"Under-display camera (UDC) systems are the foundation of full-screen display
devices in which the lens mounts under the display. The pixel array of
light-emitting diodes used for display diffracts and attenuates incident light,
causing various degradations as the light intensity changes. Unlike general
video restoration which recovers video by treating different degradation
factors equally, video restoration for UDC systems is more challenging that
concerns removing diverse degradation over time while preserving temporal
consistency. In this paper, we introduce a novel video restoration network,
called D$^2$RNet, specifically designed for UDC systems. It employs a set of
Decoupling Attention Modules (DAM) that effectively separate the various video
degradation factors. More specifically, a soft mask generation function is
proposed to formulate each frame into flare and haze based on the diffraction
arising from incident light of different intensities, followed by the proposed
flare and haze removal components that leverage long- and short-term feature
learning to handle the respective degradations. Such a design offers an
targeted and effective solution to eliminating various types of degradation in
UDC systems. We further extend our design into multi-scale to overcome the
scale-changing of degradation that often occur in long-range videos. To
demonstrate the superiority of D$^2$RNet, we propose a large-scale UDC video
benchmark by gathering HDR videos and generating realistically degraded videos
using the point spread function measured by a commercial UDC system. Extensive
quantitative and qualitative evaluations demonstrate the superiority of
D$^2$RNet compared to other state-of-the-art video restoration and UDC image
restoration methods. Code is available at
https://github.com/ChengxuLiu/DDRNet.git",2024-03-08 20:21:45+00:00,"['Chengxu Liu', 'Xuan Wang', 'Yuanting Fan', 'Shuai Li', 'Xueming Qian']",http://arxiv.org/abs/2403.05660v1
OmniVid: A Generative Framework for Universal Video Understanding,"The core of video understanding tasks, such as recognition, captioning, and
tracking, is to automatically detect objects or actions in a video and analyze
their temporal evolution. Despite sharing a common goal, different tasks often
rely on distinct model architectures and annotation formats. In contrast,
natural language processing benefits from a unified output space, i.e., text
sequences, which simplifies the training of powerful foundational language
models, such as GPT-3, with extensive training corpora. Inspired by this, we
seek to unify the output space of video understanding tasks by using languages
as labels and additionally introducing time and box tokens. In this way, a
variety of video tasks could be formulated as video-grounded token generation.
This enables us to address various types of video tasks, including
classification (such as action recognition), captioning (covering clip
captioning, video question answering, and dense video captioning), and
localization tasks (such as visual object tracking) within a fully shared
encoder-decoder architecture, following a generative framework. Through
comprehensive experiments, we demonstrate such a simple and straightforward
idea is quite effective and can achieve state-of-the-art or competitive results
on seven video benchmarks, providing a novel perspective for more universal
video understanding. Code is available at https://github.com/wangjk666/OmniVid.",2024-03-26 17:59:24+00:00,"['Junke Wang', 'Dongdong Chen', 'Chong Luo', 'Bo He', 'Lu Yuan', 'Zuxuan Wu', 'Yu-Gang Jiang']",http://arxiv.org/abs/2403.17935v1
Enhancing Video Summarization with Context Awareness,"Video summarization is a crucial research area that aims to efficiently
browse and retrieve relevant information from the vast amount of video content
available today. With the exponential growth of multimedia data, the ability to
extract meaningful representations from videos has become essential. Video
summarization techniques automatically generate concise summaries by selecting
keyframes, shots, or segments that capture the video's essence. This process
improves the efficiency and accuracy of various applications, including video
surveillance, education, entertainment, and social media. Despite the
importance of video summarization, there is a lack of diverse and
representative datasets, hindering comprehensive evaluation and benchmarking of
algorithms. Existing evaluation metrics also fail to fully capture the
complexities of video summarization, limiting accurate algorithm assessment and
hindering the field's progress. To overcome data scarcity challenges and
improve evaluation, we propose an unsupervised approach that leverages video
data structure and information for generating informative summaries. By moving
away from fixed annotations, our framework can produce representative summaries
effectively. Moreover, we introduce an innovative evaluation pipeline tailored
specifically for video summarization. Human participants are involved in the
evaluation, comparing our generated summaries to ground truth summaries and
assessing their informativeness. This human-centric approach provides valuable
insights into the effectiveness of our proposed techniques. Experimental
results demonstrate that our training-free framework outperforms existing
unsupervised approaches and achieves competitive results compared to
state-of-the-art supervised methods.",2024-04-06 09:08:34+00:00,"['Hai-Dang Huynh-Lam', 'Ngoc-Phuong Ho-Thi', 'Minh-Triet Tran', 'Trung-Nghia Le']",http://arxiv.org/abs/2404.04564v1
VideoGigaGAN: Towards Detail-rich Video Super-Resolution,"Video super-resolution (VSR) approaches have shown impressive temporal
consistency in upsampled videos. However, these approaches tend to generate
blurrier results than their image counterparts as they are limited in their
generative capability. This raises a fundamental question: can we extend the
success of a generative image upsampler to the VSR task while preserving the
temporal consistency? We introduce VideoGigaGAN, a new generative VSR model
that can produce videos with high-frequency details and temporal consistency.
VideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. Simply
inflating GigaGAN to a video model by adding temporal modules produces severe
temporal flickering. We identify several key issues and propose techniques that
significantly improve the temporal consistency of upsampled videos. Our
experiments show that, unlike previous VSR methods, VideoGigaGAN generates
temporally consistent videos with more fine-grained appearance details. We
validate the effectiveness of VideoGigaGAN by comparing it with
state-of-the-art VSR models on public datasets and showcasing video results
with $8\times$ super-resolution.",2024-04-18 17:59:53+00:00,"['Yiran Xu', 'Taesung Park', 'Richard Zhang', 'Yang Zhou', 'Eli Shechtman', 'Feng Liu', 'Jia-Bin Huang', 'Difan Liu']",http://arxiv.org/abs/2404.12388v2
Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion,"This paper proposes Instruct 4D-to-4D that achieves 4D awareness and
spatial-temporal consistency for 2D diffusion models to generate high-quality
instruction-guided dynamic scene editing results. Traditional applications of
2D diffusion models in dynamic scene editing often result in inconsistency,
primarily due to their inherent frame-by-frame editing methodology. Addressing
the complexities of extending instruction-guided editing to 4D, our key insight
is to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems:
achieving temporal consistency in video editing and applying these edits to the
pseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P)
model with an anchor-aware attention module for batch processing and consistent
editing. Additionally, we integrate optical flow-guided appearance propagation
in a sliding window fashion for more precise frame-to-frame editing and
incorporate depth-based projection to manage the extensive data of pseudo-3D
scenes, followed by iterative editing to achieve convergence. We extensively
evaluate our approach in various scenes and editing instructions, and
demonstrate that it achieves spatially and temporally consistent editing
results, with significantly enhanced detail and sharpness over the prior art.
Notably, Instruct 4D-to-4D is general and applicable to both monocular and
challenging multi-camera scenes. Code and more results are available at
immortalco.github.io/Instruct-4D-to-4D.",2024-06-13 17:59:30+00:00,"['Linzhan Mou', 'Jun-Kun Chen', 'Yu-Xiong Wang']",http://arxiv.org/abs/2406.09402v1
UniScene: Unified Occupancy-centric Driving Scene Generation,"Generating high-fidelity, controllable, and annotated training data is
critical for autonomous driving. Existing methods typically generate a single
data form directly from a coarse scene layout, which not only fails to output
rich data forms required for diverse downstream tasks but also struggles to
model the direct layout-to-data distribution. In this paper, we introduce
UniScene, the first unified framework for generating three key data forms -
semantic occupancy, video, and LiDAR - in driving scenes. UniScene employs a
progressive generation process that decomposes the complex task of scene
generation into two hierarchical steps: (a) first generating semantic occupancy
from a customized scene layout as a meta scene representation rich in both
semantic and geometric information, and then (b) conditioned on occupancy,
generating video and LiDAR data, respectively, with two novel transfer
strategies of Gaussian-based Joint Rendering and Prior-guided Sparse Modeling.
This occupancy-centric approach reduces the generation burden, especially for
intricate scenes, while providing detailed intermediate representations for the
subsequent generation stages. Extensive experiments demonstrate that UniScene
outperforms previous SOTAs in the occupancy, video, and LiDAR generation, which
also indeed benefits downstream driving tasks. Project page:
https://arlo0o.github.io/uniscene/",2024-12-06 21:41:52+00:00,"['Bohan Li', 'Jiazhe Guo', 'Hongsi Liu', 'Yingshuang Zou', 'Yikang Ding', 'Xiwu Chen', 'Hu Zhu', 'Feiyang Tan', 'Chi Zhang', 'Tiancai Wang', 'Shuchang Zhou', 'Li Zhang', 'Xiaojuan Qi', 'Hao Zhao', 'Mu Yang', 'Wenjun Zeng', 'Xin Jin']",http://arxiv.org/abs/2412.05435v2
Tora: Trajectory-oriented Diffusion Transformer for Video Generation,"Recent advancements in Diffusion Transformer (DiT) have demonstrated
remarkable proficiency in producing high-quality video content. Nonetheless,
the potential of transformer-based diffusion models for effectively generating
videos with controllable motion remains an area of limited exploration. This
paper introduces Tora, the first trajectory-oriented DiT framework that
concurrently integrates textual, visual, and trajectory conditions, thereby
enabling scalable video generation with effective motion guidance.
Specifically, Tora consists of a Trajectory Extractor (TE), a Spatial-Temporal
DiT, and a Motion-guidance Fuser (MGF). The TE encodes arbitrary trajectories
into hierarchical spacetime motion patches with a 3D motion compression
network. The MGF integrates the motion patches into the DiT blocks to generate
consistent videos that accurately follow designated trajectories. Our design
aligns seamlessly with DiT's scalability, allowing precise control of video
content's dynamics with diverse durations, aspect ratios, and resolutions.
Extensive experiments demonstrate that Tora excels in achieving high motion
fidelity compared to the foundational DiT model, while also accurately
simulating the complex movements of the physical world. Code is made available
at https://github.com/alibaba/Tora .",2024-07-31 15:53:20+00:00,"['Zhenghao Zhang', 'Junchao Liao', 'Menghao Li', 'Zuozhuo Dai', 'Bingxue Qiu', 'Siyu Zhu', 'Long Qin', 'Weizhi Wang']",http://arxiv.org/abs/2407.21705v4
Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering,"The correct insertion of virtual objects in images of real-world scenes
requires a deep understanding of the scene's lighting, geometry and materials,
as well as the image formation process. While recent large-scale diffusion
models have shown strong generative and inpainting capabilities, we find that
current models do not sufficiently ""understand"" the scene shown in a single
picture to generate consistent lighting effects (shadows, bright reflections,
etc.) while preserving the identity and details of the composited object. We
propose using a personalized large diffusion model as guidance to a physically
based inverse rendering process. Our method recovers scene lighting and
tone-mapping parameters, allowing the photorealistic composition of arbitrary
virtual objects in single frames or videos of indoor or outdoor scenes. Our
physically based pipeline further enables automatic materials and tone-mapping
refinement.",2024-08-19 05:15:45+00:00,"['Ruofan Liang', 'Zan Gojcic', 'Merlin Nimier-David', 'David Acuna', 'Nandita Vijaykumar', 'Sanja Fidler', 'Zian Wang']",http://arxiv.org/abs/2408.09702v1
EMOdiffhead: Continuously Emotional Control in Talking Head Generation via Diffusion,"The task of audio-driven portrait animation involves generating a talking
head video using an identity image and an audio track of speech. While many
existing approaches focus on lip synchronization and video quality, few tackle
the challenge of generating emotion-driven talking head videos. The ability to
control and edit emotions is essential for producing expressive and realistic
animations. In response to this challenge, we propose EMOdiffhead, a novel
method for emotional talking head video generation that not only enables
fine-grained control of emotion categories and intensities but also enables
one-shot generation. Given the FLAME 3D model's linearity in expression
modeling, we utilize the DECA method to extract expression vectors, that are
combined with audio to guide a diffusion model in generating videos with
precise lip synchronization and rich emotional expressiveness. This approach
not only enables the learning of rich facial information from
emotion-irrelevant data but also facilitates the generation of emotional
videos. It effectively overcomes the limitations of emotional data, such as the
lack of diversity in facial and background information, and addresses the
absence of emotional details in emotion-irrelevant data. Extensive experiments
and user studies demonstrate that our approach achieves state-of-the-art
performance compared to other emotion portrait animation methods.",2024-09-11 13:23:22+00:00,"['Jian Zhang', 'Weijian Mai', 'Zhijun Zhang']",http://arxiv.org/abs/2409.07255v1
DOLLAR: Few-Step Video Generation via Distillation and Latent Reward Optimization,"Diffusion probabilistic models have shown significant progress in video
generation; however, their computational efficiency is limited by the large
number of sampling steps required. Reducing sampling steps often compromises
video quality or generation diversity. In this work, we introduce a
distillation method that combines variational score distillation and
consistency distillation to achieve few-step video generation, maintaining both
high quality and diversity. We also propose a latent reward model fine-tuning
approach to further enhance video generation performance according to any
specified reward metric. This approach reduces memory usage and does not
require the reward to be differentiable. Our method demonstrates
state-of-the-art performance in few-step generation for 10-second videos (128
frames at 12 FPS). The distilled student model achieves a score of 82.57 on
VBench, surpassing the teacher model as well as baseline models Gen-3,
T2V-Turbo, and Kling. One-step distillation accelerates the teacher model's
diffusion sampling by up to 278.6 times, enabling near real-time generation.
Human evaluations further validate the superior performance of our 4-step
student models compared to teacher model using 50-step DDIM sampling.",2024-12-20 09:07:36+00:00,"['Zihan Ding', 'Chi Jin', 'Difan Liu', 'Haitian Zheng', 'Krishna Kumar Singh', 'Qiang Zhang', 'Yan Kang', 'Zhe Lin', 'Yuchen Liu']",http://arxiv.org/abs/2412.15689v1
CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion,"With advancements in video generative AI models (e.g., SORA), creators are
increasingly using these techniques to enhance video previsualization. However,
they face challenges with incomplete and mismatched AI workflows. Existing
methods mainly rely on text descriptions and struggle with camera placement, a
key component of previsualization. To address these issues, we introduce
CinePreGen, a visual previsualization system enhanced with engine-powered
diffusion. It features a novel camera and storyboard interface that offers
dynamic control, from global to local camera adjustments. This is combined with
a user-friendly AI rendering workflow, which aims to achieve consistent results
through multi-masked IP-Adapter and engine simulation guidelines. In our
comprehensive evaluation study, we demonstrate that our system reduces
development viscosity (i.e., the complexity and challenges in the development
process), meets users' needs for extensive control and iteration in the design
process, and outperforms other AI video production workflows in cinematic
camera movement, as shown by our experiments and a within-subjects user study.
With its intuitive camera controls and realistic rendering of camera motion,
CinePreGen shows great potential for improving video production for both
individual creators and industry professionals.",2024-08-30 17:16:18+00:00,"['Yiran Chen', 'Anyi Rao', 'Xuekun Jiang', 'Shishi Xiao', 'Ruiqing Ma', 'Zeyu Wang', 'Hui Xiong', 'Bo Dai']",http://arxiv.org/abs/2408.17424v1
4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion,"We propose 4Real-Video, a novel framework for generating 4D videos, organized
as a grid of video frames with both time and viewpoint axes. In this grid, each
row contains frames sharing the same timestep, while each column contains
frames from the same viewpoint. We propose a novel two-stream architecture. One
stream performs viewpoint updates on columns, and the other stream performs
temporal updates on rows. After each diffusion transformer layer, a
synchronization layer exchanges information between the two token streams. We
propose two implementations of the synchronization layer, using either hard or
soft synchronization. This feedforward architecture improves upon previous work
in three ways: higher inference speed, enhanced visual quality (measured by
FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency
(measured by VideoScore and Dust3R-Confidence).",2024-12-05 18:59:41+00:00,"['Chaoyang Wang', 'Peiye Zhuang', 'Tuan Duc Ngo', 'Willi Menapace', 'Aliaksandr Siarohin', 'Michael Vasilkovsky', 'Ivan Skorokhodov', 'Sergey Tulyakov', 'Peter Wonka', 'Hsin-Ying Lee']",http://arxiv.org/abs/2412.04462v1
Technical Report: Competition Solution For Modelscope-Sora,"This report presents the approach adopted in the Modelscope-Sora challenge,
which focuses on fine-tuning data for video generation models. The challenge
evaluates participants' ability to analyze, clean, and generate high-quality
datasets for video-based text-to-video tasks under specific computational
constraints. The provided methodology involves data processing techniques such
as video description generation, filtering, and acceleration. This report
outlines the procedures and tools utilized to enhance the quality of training
data, ensuring improved performance in text-to-video generation models.",2024-09-24 02:45:09+00:00,"['Shengfu Chen', 'Hailong Liu', 'Wenzhao Wei']",http://arxiv.org/abs/2410.07194v1
CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities,"Customized video generation aims to generate high-quality videos guided by
text prompts and subject's reference images. However, since it is only trained
on static images, the fine-tuning process of subject learning disrupts
abilities of video diffusion models (VDMs) to combine concepts and generate
motions. To restore these abilities, some methods use additional video similar
to the prompt to fine-tune or guide the model. This requires frequent changes
of guiding videos and even re-tuning of the model when generating different
motions, which is very inconvenient for users. In this paper, we propose
CustomCrafter, a novel framework that preserves the model's motion generation
and conceptual combination abilities without additional video and fine-tuning
to recovery. For preserving conceptual combination ability, we design a
plug-and-play module to update few parameters in VDMs, enhancing the model's
ability to capture the appearance details and the ability of concept
combinations for new subjects. For motion generation, we observed that VDMs
tend to restore the motion of video in the early stage of denoising, while
focusing on the recovery of subject details in the later stage. Therefore, we
propose Dynamic Weighted Video Sampling Strategy. Using the pluggability of our
subject learning modules, we reduce the impact of this module on motion
generation in the early stage of denoising, preserving the ability to generate
motion of VDMs. In the later stage of denoising, we restore this module to
repair the appearance details of the specified subject, thereby ensuring the
fidelity of the subject's appearance. Experimental results show that our method
has a significant improvement compared to previous methods. Code is available
at https://github.com/WuTao-CS/CustomCrafter",2024-08-23 17:26:06+00:00,"['Tao Wu', 'Yong Zhang', 'Xintao Wang', 'Xianpan Zhou', 'Guangcong Zheng', 'Zhongang Qi', 'Ying Shan', 'Xi Li']",http://arxiv.org/abs/2408.13239v2
GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space,"We train a feed-forward text-to-3D diffusion generator for human characters
using only single-view 2D data for supervision. Existing 3D generative models
cannot yet match the fidelity of image or video generative models.
State-of-the-art 3D generators are either trained with explicit 3D supervision
and are thus limited by the volume and diversity of existing 3D data.
Meanwhile, generators that can be trained with only 2D data as supervision
typically produce coarser results, cannot be text-conditioned, or must revert
to test-time optimization. We observe that GAN- and diffusion-based generators
have complementary qualities: GANs can be trained efficiently with 2D
supervision to produce high-quality 3D objects but are hard to condition on
text. In contrast, denoising diffusion models can be conditioned efficiently
but tend to be hard to train with only 2D supervision. We introduce GANFusion,
which starts by generating unconditional triplane features for 3D data using a
GAN architecture trained with only single-view 2D data. We then generate random
samples from the GAN, caption them, and train a text-conditioned diffusion
model that directly learns to sample from the space of good triplane features
that can be decoded into 3D objects.",2024-12-21 17:59:17+00:00,"['Souhaib Attaiki', 'Paul Guerrero', 'Duygu Ceylan', 'Niloy J. Mitra', 'Maks Ovsjanikov']",http://arxiv.org/abs/2412.16717v1
SimGen: Simulator-conditioned Driving Scene Generation,"Controllable synthetic data generation can substantially lower the annotation
cost of training data. Prior works use diffusion models to generate driving
images conditioned on the 3D object layout. However, those models are trained
on small-scale datasets like nuScenes, which lack appearance and layout
diversity. Moreover, overfitting often happens, where the trained models can
only generate images based on the layout data from the validation set of the
same dataset. In this work, we introduce a simulator-conditioned scene
generation framework called SimGen that can learn to generate diverse driving
scenes by mixing data from the simulator and the real world. It uses a novel
cascade diffusion pipeline to address challenging sim-to-real gaps and
multi-condition conflicts. A driving video dataset DIVA is collected to enhance
the generative diversity of SimGen, which contains over 147.5 hours of
real-world driving videos from 73 locations worldwide and simulated driving
data from the MetaDrive simulator. SimGen achieves superior generation quality
and diversity while preserving controllability based on the text prompt and the
layout pulled from a simulator. We further demonstrate the improvements brought
by SimGen for synthetic data augmentation on the BEV detection and segmentation
task and showcase its capability in safety-critical data generation.",2024-06-13 17:58:32+00:00,"['Yunsong Zhou', 'Michael Simon', 'Zhenghao Peng', 'Sicheng Mo', 'Hongzi Zhu', 'Minyi Guo', 'Bolei Zhou']",http://arxiv.org/abs/2406.09386v3
Scalable Indoor Novel-View Synthesis using Drone-Captured 360 Imagery with 3D Gaussian Splatting,"Scene reconstruction and novel-view synthesis for large, complex,
multi-story, indoor scenes is a challenging and time-consuming task. Prior
methods have utilized drones for data capture and radiance fields for scene
reconstruction, both of which present certain challenges. First, in order to
capture diverse viewpoints with the drone's front-facing camera, some
approaches fly the drone in an unstable zig-zag fashion, which hinders
drone-piloting and generates motion blur in the captured data. Secondly, most
radiance field methods do not easily scale to arbitrarily large number of
images. This paper proposes an efficient and scalable pipeline for indoor
novel-view synthesis from drone-captured 360 videos using 3D Gaussian
Splatting. 360 cameras capture a wide set of viewpoints, allowing for
comprehensive scene capture under a simple straightforward drone trajectory. To
scale our method to large scenes, we devise a divide-and-conquer strategy to
automatically split the scene into smaller blocks that can be reconstructed
individually and in parallel. We also propose a coarse-to-fine alignment
strategy to seamlessly match these blocks together to compose the entire scene.
Our experiments demonstrate marked improvement in both reconstruction quality,
i.e. PSNR and SSIM, and computation time compared to prior approaches.",2024-10-15 05:08:47+00:00,"['Yuanbo Chen', 'Chengyu Zhang', 'Jason Wang', 'Xuefan Gao', 'Avideh Zakhor']",http://arxiv.org/abs/2410.11285v1
Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual Try-on in Videos,"Video try-on is a challenging task and has not been well tackled in previous
works. The main obstacle lies in preserving the details of the clothing and
modeling the coherent motions simultaneously. Faced with those difficulties, we
address video try-on by proposing a diffusion-based framework named ""Tunnel
Try-on."" The core idea is excavating a ""focus tunnel"" in the input video that
gives close-up shots around the clothing regions. We zoom in on the region in
the tunnel to better preserve the fine details of the clothing. To generate
coherent motions, we first leverage the Kalman filter to construct smooth crops
in the focus tunnel and inject the position embedding of the tunnel into
attention layers to improve the continuity of the generated videos. In
addition, we develop an environment encoder to extract the context information
outside the tunnels as supplementary cues. Equipped with these techniques,
Tunnel Try-on keeps the fine details of the clothing and synthesizes stable and
smooth videos. Demonstrating significant advancements, Tunnel Try-on could be
regarded as the first attempt toward the commercial-level application of
virtual try-on in videos.",2024-04-26 17:55:26+00:00,"['Zhengze Xu', 'Mengting Chen', 'Zhao Wang', 'Linyu Xing', 'Zhonghua Zhai', 'Nong Sang', 'Jinsong Lan', 'Shuai Xiao', 'Changxin Gao']",http://arxiv.org/abs/2404.17571v1
The Dawn of Video Generation: Preliminary Explorations with SORA-like Models,"High-quality video generation, encompassing text-to-video (T2V),
image-to-video (I2V), and video-to-video (V2V) generation, holds considerable
significance in content creation to benefit anyone express their inherent
creativity in new ways and world simulation to modeling and understanding the
world. Models like SORA have advanced generating videos with higher resolution,
more natural motion, better vision-language alignment, and increased
controllability, particularly for long video sequences. These improvements have
been driven by the evolution of model architectures, shifting from UNet to more
scalable and parameter-rich DiT models, along with large-scale data expansion
and refined training strategies. However, despite the emergence of DiT-based
closed-source and open-source models, a comprehensive investigation into their
capabilities and limitations remains lacking. Furthermore, the rapid
development has made it challenging for recent benchmarks to fully cover
SORA-like models and recognize their significant advancements. Additionally,
evaluation metrics often fail to align with human preferences.",2024-10-07 17:35:10+00:00,"['Ailing Zeng', 'Yuhang Yang', 'Weidong Chen', 'Wei Liu']",http://arxiv.org/abs/2410.05227v2
Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs,"Facial video inpainting plays a crucial role in a wide range of applications,
including but not limited to the removal of obstructions in video conferencing
and telemedicine, enhancement of facial expression analysis, privacy
protection, integration of graphical overlays, and virtual makeup. This domain
presents serious challenges due to the intricate nature of facial features and
the inherent human familiarity with faces, heightening the need for accurate
and persuasive completions. In addressing challenges specifically related to
occlusion removal in this context, our focus is on the progressive task of
generating complete images from facial data covered by masks, ensuring both
spatial and temporal coherence. Our study introduces a network designed for
expression-based video inpainting, employing generative adversarial networks
(GANs) to handle static and moving occlusions across all frames. By utilizing
facial landmarks and an occlusion-free reference image, our model maintains the
user's identity consistently across frames. We further enhance emotional
preservation through a customized facial expression recognition (FER) loss
function, ensuring detailed inpainted outputs. Our proposed framework exhibits
proficiency in eliminating occlusions from facial videos in an adaptive form,
whether appearing static or dynamic on the frames, while providing realistic
and coherent results.",2024-02-14 11:20:47+00:00,"['Fatemeh Ghorbani Lohesara', 'Karen Egiazarian', 'Sebastian Knorr']",http://arxiv.org/abs/2402.09100v1
A Simple but Strong Baseline for Sounding Video Generation: Effective Adaptation of Audio and Video Diffusion Models for Joint Generation,"In this work, we build a simple but strong baseline for sounding video
generation. Given base diffusion models for audio and video, we integrate them
with additional modules into a single model and train it to make the model
jointly generate audio and video. To enhance alignment between audio-video
pairs, we introduce two novel mechanisms in our model. The first one is
timestep adjustment, which provides different timestep information to each base
model. It is designed to align how samples are generated along with timesteps
across modalities. The second one is a new design of the additional modules,
termed Cross-Modal Conditioning as Positional Encoding (CMC-PE). In CMC-PE,
cross-modal information is embedded as if it represents temporal position
information, and the embeddings are fed into the model like positional
encoding. Compared with the popular cross-attention mechanism, CMC-PE provides
a better inductive bias for temporal alignment in the generated data.
Experimental results validate the effectiveness of the two newly introduced
mechanisms and also demonstrate that our method outperforms existing methods.",2024-09-26 05:39:52+00:00,"['Masato Ishii', 'Akio Hayakawa', 'Takashi Shibuya', 'Yuki Mitsufuji']",http://arxiv.org/abs/2409.17550v2
VIRES: Video Instance Repainting via Sketch and Text Guided Generation,"We introduce VIRES, a video instance repainting method with sketch and text
guidance, enabling video instance repainting, replacement, generation, and
removal. Existing approaches struggle with temporal consistency and accurate
alignment with the provided sketch sequence. VIRES leverages the generative
priors of text-to-video models to maintain temporal consistency and produce
visually pleasing results. We propose the Sequential ControlNet with the
standardized self-scaling, which effectively extracts structure layouts and
adaptively captures high-contrast sketch details. We further augment the
diffusion transformer backbone with the sketch attention to interpret and
inject fine-grained sketch semantics. A sketch-aware encoder ensures that
repainted results are aligned with the provided sketch sequence. Additionally,
we contribute the VireSet, a dataset with detailed annotations tailored for
training and evaluating video instance editing methods. Experimental results
demonstrate the effectiveness of VIRES, which outperforms state-of-the-art
methods in visual quality, temporal consistency, condition alignment, and human
ratings. Project page:https://suimuc.github.io/suimu.github.io/projects/VIRES/",2024-11-25 08:55:41+00:00,"['Shuchen Weng', 'Haojie Zheng', 'Peixuan Zhan', 'Yuchen Hong', 'Han Jiang', 'Si Li', 'Boxin Shi']",http://arxiv.org/abs/2411.16199v4
Tarsier: Recipes for Training and Evaluating Large Video Description Models,"Generating fine-grained video descriptions is a fundamental challenge in
video understanding. In this work, we introduce Tarsier, a family of
large-scale video-language models designed to generate high-quality video
descriptions. Tarsier employs CLIP-ViT to encode frames separately and then
uses an LLM to model temporal relationships. Despite its simple architecture,
we demonstrate that with a meticulously designed two-stage training procedure,
the Tarsier models exhibit substantially stronger video description
capabilities than any existing open-source model, showing a $+51.4\%$ advantage
in human side-by-side evaluation over the strongest model. Additionally, they
are comparable to state-of-the-art proprietary models, with a $+12.3\%$
advantage against GPT-4V and a $-6.7\%$ disadvantage against Gemini 1.5 Pro.
When upgraded to Tarsier2 by building upon SigLIP and Qwen2-7B, it further
improves significantly with a $+4.8\%$ advantage against GPT-4o. Besides video
description, Tarsier proves to be a versatile generalist model, achieving new
state-of-the-art results across nine public benchmarks, including multi-choice
VQA, open-ended VQA, and zero-shot video captioning. Our second contribution is
the introduction of a new benchmark -- DREAM-1K
(https://tarsier-vlm.github.io/) for evaluating video description models,
consisting of a new challenging dataset featuring videos from diverse sources
and varying complexity, along with an automatic method specifically designed to
assess the quality of fine-grained video descriptions. We make our models and
evaluation benchmark publicly available at
https://github.com/bytedance/tarsier.",2024-06-30 09:21:01+00:00,"['Jiawei Wang', 'Liping Yuan', 'Yuchen Zhang', 'Haomiao Sun']",http://arxiv.org/abs/2407.00634v2
PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos,"Recent advancements in video-based large language models (Video LLMs) have
witnessed the emergence of diverse capabilities to reason and interpret dynamic
visual content. Among them, gameplay videos stand out as a distinctive data
source, often containing glitches that defy physics commonsense. This
characteristic renders them an effective benchmark for assessing the
under-explored capability of physical commonsense understanding in video LLMs.
In this paper, we propose PhysGame as a pioneering benchmark to evaluate
physical commonsense violations in gameplay videos. PhysGame comprises 880
videos associated with glitches spanning four fundamental domains (i.e.,
mechanics, kinematics, optics, and material properties) and across 12 distinct
physical commonsense. Through extensively evaluating various state-ofthe-art
video LLMs, our findings reveal that the performance of current open-source
video LLMs significantly lags behind that of proprietary counterparts. To
bridge this gap, we curate an instruction tuning dataset PhysInstruct with
140,057 question-answering pairs to facilitate physical commonsense learning.
In addition, we also propose a preference optimization dataset PhysDPO with
34,358 training pairs, where the dis-preferred responses are generated
conditioned on misleading titles (i.e., meta information hacking), fewer frames
(i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking).
Based on the suite of datasets, we propose PhysVLM as a physical
knowledge-enhanced video LLM. Extensive experiments on both physical-oriented
benchmark PhysGame and general video understanding benchmarks demonstrate the
state-ofthe-art performance of PhysVLM.",2024-12-02 18:47:25+00:00,"['Meng Cao', 'Haoran Tang', 'Haoze Zhao', 'Hangyu Guo', 'Jiaheng Liu', 'Ge Zhang', 'Ruyang Liu', 'Qiang Sun', 'Ian Reid', 'Xiaodan Liang']",http://arxiv.org/abs/2412.01800v1
Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image Attacking,"In Virtual Reality (VR), adversarial attack remains a significant security
threat. Most deep learning-based methods for physical and digital adversarial
attacks focus on enhancing attack performance by crafting adversarial examples
that contain large printable distortions that are easy for human observers to
identify. However, attackers rarely impose limitations on the naturalness and
comfort of the appearance of the generated attack image, resulting in a
noticeable and unnatural attack. To address this challenge, we propose a
framework to incorporate style transfer to craft adversarial inputs of natural
styles that exhibit minimal detectability and maximum natural appearance, while
maintaining superior attack capabilities.",2024-03-21 18:49:20+00:00,"['Qianyu Guo', 'Jiaming Fu', 'Yawen Lu', 'Dongming Gan']",http://arxiv.org/abs/2403.14778v1
SimVS: Simulating World Inconsistencies for Robust View Synthesis,"Novel-view synthesis techniques achieve impressive results for static scenes
but struggle when faced with the inconsistencies inherent to casual capture
settings: varying illumination, scene motion, and other unintended effects that
are difficult to model explicitly. We present an approach for leveraging
generative video models to simulate the inconsistencies in the world that can
occur during capture. We use this process, along with existing multi-view
datasets, to create synthetic data for training a multi-view harmonization
network that is able to reconcile inconsistent observations into a consistent
3D scene. We demonstrate that our world-simulation strategy significantly
outperforms traditional augmentation methods in handling real-world scene
variations, thereby enabling highly accurate static 3D reconstructions in the
presence of a variety of challenging inconsistencies. Project page:
https://alextrevithick.github.io/simvs",2024-12-10 17:35:12+00:00,"['Alex Trevithick', 'Roni Paiss', 'Philipp Henzler', 'Dor Verbin', 'Rundi Wu', 'Hadi Alzayer', 'Ruiqi Gao', 'Ben Poole', 'Jonathan T. Barron', 'Aleksander Holynski', 'Ravi Ramamoorthi', 'Pratul P. Srinivasan']",http://arxiv.org/abs/2412.07696v1
STAR: A Benchmark for Situated Reasoning in Real-World Videos,"Reasoning in the real world is not divorced from situations. How to capture
the present knowledge from surrounding situations and perform reasoning
accordingly is crucial and challenging for machine intelligence. This paper
introduces a new benchmark that evaluates the situated reasoning ability via
situation abstraction and logic-grounded question answering for real-world
videos, called Situated Reasoning in Real-World Videos (STAR Benchmark). This
benchmark is built upon the real-world videos associated with human actions or
interactions, which are naturally dynamic, compositional, and logical. The
dataset includes four types of questions, including interaction, sequence,
prediction, and feasibility. We represent the situations in real-world videos
by hyper-graphs connecting extracted atomic entities and relations (e.g.,
actions, persons, objects, and relationships). Besides visual perception,
situated reasoning also requires structured situation comprehension and logical
reasoning. Questions and answers are procedurally generated. The answering
logic of each question is represented by a functional program based on a
situation hyper-graph. We compare various existing video reasoning models and
find that they all struggle on this challenging situated reasoning task. We
further propose a diagnostic neuro-symbolic model that can disentangle visual
perception, situation abstraction, language understanding, and functional
reasoning to understand the challenges of this benchmark.",2024-05-15 21:53:54+00:00,"['Bo Wu', 'Shoubin Yu', 'Zhenfang Chen', 'Joshua B Tenenbaum', 'Chuang Gan']",http://arxiv.org/abs/2405.09711v1
AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI,"The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks. We have open-sourced the
dataset and evaluation code on the project website:
https://www.benchcouncil.org/AIGCBench.",2024-01-03 10:08:40+00:00,"['Fanda Fan', 'Chunjie Luo', 'Wanling Gao', 'Jianfeng Zhan']",http://arxiv.org/abs/2401.01651v3
Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features,"The development of AI-Generated Content (AIGC) has empowered the creation of
remarkably realistic AI-generated videos, such as those involving Sora.
However, the widespread adoption of these models raises concerns regarding
potential misuse, including face video scams and copyright disputes. Addressing
these concerns requires the development of robust tools capable of accurately
determining video authenticity. The main challenges lie in the dataset and
neural classifier for training. Current datasets lack a varied and
comprehensive repository of real and generated content for effective
discrimination. In this paper, we first introduce an extensive video dataset
designed specifically for AI-Generated Video Detection (GenVidDet). It includes
over 2.66 M instances of both real and generated videos, varying in categories,
frames per second, resolutions, and lengths. The comprehensiveness of GenVidDet
enables the training of a generalizable video detector. We also present the
Dual-Branch 3D Transformer (DuB3D), an innovative and effective method for
distinguishing between real and generated videos, enhanced by incorporating
motion information alongside visual appearance. DuB3D utilizes a dual-branch
architecture that adaptively leverages and fuses raw spatio-temporal data and
optical flow. We systematically explore the critical factors affecting
detection performance, achieving the optimal configuration for DuB3D. Trained
on GenVidDet, DuB3D can distinguish between real and generated video content
with 96.77% accuracy, and strong generalization capability even for unseen
types.",2024-05-24 08:26:04+00:00,"['Lichuan Ji', 'Yingqi Lin', 'Zhenhua Huang', 'Yan Han', 'Xiaogang Xu', 'Jiafei Wu', 'Chong Wang', 'Zhe Liu']",http://arxiv.org/abs/2405.15343v1
VidMusician: Video-to-Music Generation with Semantic-Rhythmic Alignment via Hierarchical Visual Features,"Video-to-music generation presents significant potential in video production,
requiring the generated music to be both semantically and rhythmically aligned
with the video. Achieving this alignment demands advanced music generation
capabilities, sophisticated video understanding, and an efficient mechanism to
learn the correspondence between the two modalities. In this paper, we propose
VidMusician, a parameter-efficient video-to-music generation framework built
upon text-to-music models. VidMusician leverages hierarchical visual features
to ensure semantic and rhythmic alignment between video and music.
Specifically, our approach utilizes global visual features as semantic
conditions and local visual features as rhythmic cues. These features are
integrated into the generative backbone via cross-attention and in-attention
mechanisms, respectively. Through a two-stage training process, we
incrementally incorporate semantic and rhythmic features, utilizing zero
initialization and identity initialization to maintain the inherent
music-generative capabilities of the backbone. Additionally, we construct a
diverse video-music dataset, DVMSet, encompassing various scenarios, such as
promo videos, commercials, and compilations. Experiments demonstrate that
VidMusician outperforms state-of-the-art methods across multiple evaluation
metrics and exhibits robust performance on AI-generated videos. Samples are
available at \url{https://youtu.be/EPOSXwtl1jw}.",2024-12-09 08:40:10+00:00,"['Sifei Li', 'Binxin Yang', 'Chunji Yin', 'Chong Sun', 'Yuxin Zhang', 'Weiming Dong', 'Chen Li']",http://arxiv.org/abs/2412.06296v1
MotionClone: Training-Free Motion Cloning for Controllable Video Generation,"Motion-based controllable video generation offers the potential for creating
captivating visual content. Existing methods typically necessitate model
training to encode particular motion cues or incorporate fine-tuning to inject
certain motion patterns, resulting in limited flexibility and generalization.
In this work, we propose MotionClone, a training-free framework that enables
motion cloning from reference videos to versatile motion-controlled video
generation, including text-to-video and image-to-video. Based on the
observation that the dominant components in temporal-attention maps drive
motion synthesis, while the rest mainly capture noisy or very subtle motions,
MotionClone utilizes sparse temporal attention weights as motion
representations for motion guidance, facilitating diverse motion transfer
across varying scenarios. Meanwhile, MotionClone allows for the direct
extraction of motion representation through a single denoising step, bypassing
the cumbersome inversion processes and thus promoting both efficiency and
flexibility. Extensive experiments demonstrate that MotionClone exhibits
proficiency in both global camera motion and local object motion, with notable
superiority in terms of motion fidelity, textual alignment, and temporal
consistency.",2024-06-08 03:44:25+00:00,"['Pengyang Ling', 'Jiazi Bu', 'Pan Zhang', 'Xiaoyi Dong', 'Yuhang Zang', 'Tong Wu', 'Huaian Chen', 'Jiaqi Wang', 'Yi Jin']",http://arxiv.org/abs/2406.05338v6
Technical Report for Soccernet 2023 -- Dense Video Captioning,"In the task of dense video captioning of Soccernet dataset, we propose to
generate a video caption of each soccer action and locate the timestamp of the
caption. Firstly, we apply Blip as our video caption framework to generate
video captions. Then we locate the timestamp by using (1) multi-size sliding
windows (2) temporal proposal generation and (3) proposal classification.",2024-10-31 14:06:30+00:00,"['Zheng Ruan', 'Ruixuan Liu', 'Shimin Chen', 'Mengying Zhou', 'Xinquan Yang', 'Wei Li', 'Chen Chen', 'Wei Shen']",http://arxiv.org/abs/2411.00882v1
CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention,"Diffusion-based video generation technology has advanced significantly,
catalyzing a proliferation of research in human animation. However, the
majority of these studies are confined to same-modality driving settings, with
cross-modality human body animation remaining relatively underexplored. In this
paper, we introduce, an end-to-end audio-driven human animation framework that
ensures hand integrity, identity consistency, and natural motion. The key
design of CyberHost is the Region Codebook Attention mechanism, which improves
the generation quality of facial and hand animations by integrating
fine-grained local features with learned motion pattern priors. Furthermore, we
have developed a suite of human-prior-guided training strategies, including
body movement map, hand clarity score, pose-aligned reference feature, and
local enhancement supervision, to improve synthesis results. To our knowledge,
CyberHost is the first end-to-end audio-driven human diffusion model capable of
facilitating zero-shot video generation within the scope of human body.
Extensive experiments demonstrate that CyberHost surpasses previous works in
both quantitative and qualitative aspects.",2024-09-03 13:19:31+00:00,"['Gaojie Lin', 'Jianwen Jiang', 'Chao Liang', 'Tianyun Zhong', 'Jiaqi Yang', 'Yanbo Zheng']",http://arxiv.org/abs/2409.01876v2
Accelerating Diffusion Transformers with Token-wise Feature Caching,"Diffusion transformers have shown significant effectiveness in both image and
video synthesis at the expense of huge computation costs. To address this
problem, feature caching methods have been introduced to accelerate diffusion
transformers by caching the features in previous timesteps and reusing them in
the following timesteps. However, previous caching methods ignore that
different tokens exhibit different sensitivities to feature caching, and
feature caching on some tokens may lead to 10$\times$ more destruction to the
overall generation quality compared with other tokens. In this paper, we
introduce token-wise feature caching, allowing us to adaptively select the most
suitable tokens for caching, and further enable us to apply different caching
ratios to neural layers in different types and depths. Extensive experiments on
PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image
and video generation with no requirements for training. For instance,
2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and
PixArt-$\alpha$ with almost no drop in generation quality.",2024-10-05 03:47:06+00:00,"['Chang Zou', 'Xuyang Liu', 'Ting Liu', 'Siteng Huang', 'Linfeng Zhang']",http://arxiv.org/abs/2410.05317v4
SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers,"Diffusion Transformers (DiT) have emerged as powerful generative models for
various tasks, including image, video, and speech synthesis. However, their
inference process remains computationally expensive due to the repeated
evaluation of resource-intensive attention and feed-forward modules. To address
this, we introduce SmoothCache, a model-agnostic inference acceleration
technique for DiT architectures. SmoothCache leverages the observed high
similarity between layer outputs across adjacent diffusion timesteps. By
analyzing layer-wise representation errors from a small calibration set,
SmoothCache adaptively caches and reuses key features during inference. Our
experiments demonstrate that SmoothCache achieves 8% to 71% speed up while
maintaining or even improving generation quality across diverse modalities. We
showcase its effectiveness on DiT-XL for image generation, Open-Sora for
text-to-video, and Stable Audio Open for text-to-audio, highlighting its
potential to enable real-time applications and broaden the accessibility of
powerful DiT models.",2024-11-15 16:24:02+00:00,"['Joseph Liu', 'Joshua Geddes', 'Ziyu Guo', 'Haomiao Jiang', 'Mahesh Kumar Nandwana']",http://arxiv.org/abs/2411.10510v1
Importance-based Token Merging for Diffusion Models,"Diffusion models excel at high-quality image and video generation. However, a
major drawback is their high latency. A simple yet powerful way to speed them
up is by merging similar tokens for faster computation, though this can result
in some quality loss. In this paper, we demonstrate that preserving important
tokens during merging significantly improves sample quality. Notably, the
importance of each token can be reliably determined using the classifier-free
guidance magnitude, as this measure is strongly correlated with the
conditioning input and corresponds to output fidelity. Since classifier-free
guidance incurs no additional computational cost or requires extra modules, our
method can be easily integrated into most diffusion-based frameworks.
Experiments show that our approach significantly outperforms the baseline
across various applications, including text-to-image synthesis, multi-view
image generation, and video generation.",2024-11-23 02:01:49+00:00,"['Haoyu Wu', 'Jingyi Xu', 'Hieu Le', 'Dimitris Samaras']",http://arxiv.org/abs/2411.16720v1
E2HQV: High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning,"The bio-inspired event cameras or dynamic vision sensors are capable of
asynchronously capturing per-pixel brightness changes (called event-streams) in
high temporal resolution and high dynamic range. However, the non-structural
spatial-temporal event-streams make it challenging for providing intuitive
visualization with rich semantic information for human vision. It calls for
events-to-video (E2V) solutions which take event-streams as input and generate
high quality video frames for intuitive visualization. However, current
solutions are predominantly data-driven without considering the prior knowledge
of the underlying statistics relating event-streams and video frames. It highly
relies on the non-linearity and generalization capability of the deep neural
networks, thus, is struggling on reconstructing detailed textures when the
scenes are complex. In this work, we propose \textbf{E2HQV}, a novel E2V
paradigm designed to produce high-quality video frames from events. This
approach leverages a model-aided deep learning framework, underpinned by a
theory-inspired E2V model, which is meticulously derived from the fundamental
imaging principles of event cameras. To deal with the issue of state-reset in
the recurrent components of E2HQV, we also design a temporal shift embedding
module to further improve the quality of the video frames. Comprehensive
evaluations on the real world event camera datasets validate our approach, with
E2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the
second best by over 40\% for some evaluation metrics.",2024-01-16 05:10:50+00:00,"['Qiang Qu', 'Yiran Shen', 'Xiaoming Chen', 'Yuk Ying Chung', 'Tongliang Liu']",http://arxiv.org/abs/2401.08117v1
Domain-adaptive Video Deblurring via Test-time Blurring,"Dynamic scene video deblurring aims to remove undesirable blurry artifacts
captured during the exposure process. Although previous video deblurring
methods have achieved impressive results, they suffer from significant
performance drops due to the domain gap between training and testing videos,
especially for those captured in real-world scenarios. To address this issue,
we propose a domain adaptation scheme based on a blurring model to achieve
test-time fine-tuning for deblurring models in unseen domains. Since blurred
and sharp pairs are unavailable for fine-tuning during inference, our scheme
can generate domain-adaptive training pairs to calibrate a deblurring model for
the target domain. First, a Relative Sharpness Detection Module is proposed to
identify relatively sharp regions from the blurry input images and regard them
as pseudo-sharp images. Next, we utilize a blurring model to produce blurred
images based on the pseudo-sharp images extracted during testing. To synthesize
blurred images in compliance with the target data distribution, we propose a
Domain-adaptive Blur Condition Generation Module to create domain-specific blur
conditions for the blurring model. Finally, the generated pseudo-sharp and
blurred pairs are used to fine-tune a deblurring model for better performance.
Extensive experimental results demonstrate that our approach can significantly
improve state-of-the-art video deblurring methods, providing performance gains
of up to 7.54dB on various real-world video deblurring datasets. The source
code is available at https://github.com/Jin-Ting-He/DADeblur.",2024-07-12 07:28:01+00:00,"['Jin-Ting He', 'Fu-Jen Tsai', 'Jia-Hao Wu', 'Yan-Tsung Peng', 'Chung-Chi Tsai', 'Chia-Wen Lin', 'Yen-Yu Lin']",http://arxiv.org/abs/2407.09059v1
iRAG: Advancing RAG for Videos with an Incremental Approach,"Retrieval-augmented generation (RAG) systems combine the strengths of
language generation and information retrieval to power many real-world
applications like chatbots. Use of RAG for understanding of videos is appealing
but there are two critical limitations. One-time, upfront conversion of all
content in large corpus of videos into text descriptions entails high
processing times. Also, not all information in the rich video data is typically
captured in the text descriptions. Since user queries are not known apriori,
developing a system for video to text conversion and interactive querying of
video data is challenging.
  To address these limitations, we propose an incremental RAG system called
iRAG, which augments RAG with a novel incremental workflow to enable
interactive querying of a large corpus of videos. Unlike traditional RAG, iRAG
quickly indexes large repositories of videos, and in the incremental workflow,
it uses the index to opportunistically extract more details from select
portions of the videos to retrieve context relevant to an interactive user
query. Such an incremental workflow avoids long video to text conversion times,
and overcomes information loss issues due to conversion of video to text, by
doing on-demand query-specific extraction of details in video data. This
ensures high quality of responses to interactive user queries that are often
not known apriori. To the best of our knowledge, iRAG is the first system to
augment RAG with an incremental workflow to support efficient interactive
querying of a large corpus of videos. Experimental results on real-world
datasets demonstrate 23x to 25x faster video to text ingestion, while ensuring
that latency and quality of responses to interactive user queries is comparable
to responses from a traditional RAG where all video data is converted to text
upfront before any user querying.",2024-04-18 16:38:02+00:00,"['Md Adnan Arefeen', 'Biplob Debnath', 'Md Yusuf Sarwar Uddin', 'Srimat Chakradhar']",http://arxiv.org/abs/2404.12309v2
Dance Any Beat: Blending Beats with Visuals in Dance Video Generation,"Generating dance from music is crucial for advancing automated choreography.
Current methods typically produce skeleton keypoint sequences instead of dance
videos and lack the capability to make specific individuals dance, which
reduces their real-world applicability. These methods also require precise
keypoint annotations, complicating data collection and limiting the use of
self-collected video datasets. To overcome these challenges, we introduce a
novel task: generating dance videos directly from images of individuals guided
by music. This task enables the dance generation of specific individuals
without requiring keypoint annotations, making it more versatile and applicable
to various situations. Our solution, the Dance Any Beat Diffusion model
(DabFusion), utilizes a reference image and a music piece to generate dance
videos featuring various dance types and choreographies. The music is analyzed
by our specially designed music encoder, which identifies essential features
including dance style, movement, and rhythm. DabFusion excels in generating
dance videos not only for individuals in the training dataset but also for any
previously unseen person. This versatility stems from its approach of
generating latent optical flow, which contains all necessary motion information
to animate any person in the image. We evaluate DabFusion's performance using
the AIST++ dataset, focusing on video quality, audio-video synchronization, and
motion-music alignment. We propose a 2D Motion-Music Alignment Score (2D-MM
Align), which builds on the Beat Alignment Score to more effectively evaluate
motion-music alignment for this new task. Experiments show that our DabFusion
establishes a solid baseline for this innovative task. Video results can be
found on our project page: https://DabFusion.github.io.",2024-05-15 11:33:07+00:00,"['Xuanchen Wang', 'Heng Wang', 'Dongnan Liu', 'Weidong Cai']",http://arxiv.org/abs/2405.09266v3
STIV: Scalable Text and Image Conditioned Video Generation,"The field of video generation has made remarkable advancements, yet there
remains a pressing need for a clear, systematic recipe that can guide the
development of robust and scalable models. In this work, we present a
comprehensive study that systematically explores the interplay of model
architectures, training recipes, and data curation strategies, culminating in a
simple and scalable text-image-conditioned video generation method, named STIV.
Our framework integrates image condition into a Diffusion Transformer (DiT)
through frame replacement, while incorporating text conditioning via a joint
image-text conditional classifier-free guidance. This design enables STIV to
perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks
simultaneously. Additionally, STIV can be easily extended to various
applications, such as video prediction, frame interpolation, multi-view
generation, and long video generation, etc. With comprehensive ablation studies
on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple
design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V,
surpassing both leading open and closed-source models like CogVideoX-5B, Pika,
Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result
of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and
extensible recipe for building cutting-edge video generation models, we aim to
empower future research and accelerate progress toward more versatile and
reliable video generation solutions.",2024-12-10 18:27:06+00:00,"['Zongyu Lin', 'Wei Liu', 'Chen Chen', 'Jiasen Lu', 'Wenze Hu', 'Tsu-Jui Fu', 'Jesse Allardice', 'Zhengfeng Lai', 'Liangchen Song', 'Bowen Zhang', 'Cha Chen', 'Yiran Fei', 'Yifan Jiang', 'Lezhi Li', 'Yizhou Sun', 'Kai-Wei Chang', 'Yinfei Yang']",http://arxiv.org/abs/2412.07730v1
LatentArtiFusion: An Effective and Efficient Histological Artifacts Restoration Framework,"Histological artifacts pose challenges for both pathologists and
Computer-Aided Diagnosis (CAD) systems, leading to errors in analysis. Current
approaches for histological artifact restoration, based on Generative
Adversarial Networks (GANs) and pixel-level Diffusion Models, suffer from
performance limitations and computational inefficiencies. In this paper, we
propose a novel framework, LatentArtiFusion, which leverages the latent
diffusion model (LDM) to reconstruct histological artifacts with high
performance and computational efficiency. Unlike traditional pixel-level
diffusion frameworks, LatentArtiFusion executes the restoration process in a
lower-dimensional latent space, significantly improving computational
efficiency. Moreover, we introduce a novel regional artifact reconstruction
algorithm in latent space to prevent mistransfer in non-artifact regions,
distinguishing our approach from GAN-based methods. Through extensive
experiments on real-world histology datasets, LatentArtiFusion demonstrates
remarkable speed, outperforming state-of-the-art pixel-level diffusion
frameworks by more than 30X. It also consistently surpasses GAN-based methods
by at least 5% across multiple evaluation metrics. Furthermore, we evaluate the
effectiveness of our proposed framework in downstream tissue classification
tasks, showcasing its practical utility. Code is available at
https://github.com/bugs-creator/LatentArtiFusion.",2024-07-29 17:00:32+00:00,"['Zhenqi He', 'Wenrui Liu', 'Minghao Yin', 'Kai Han']",http://arxiv.org/abs/2407.20172v1
Unveiling Context-Related Anomalies: Knowledge Graph Empowered Decoupling of Scene and Action for Human-Related Video Anomaly Detection,"Detecting anomalies in human-related videos is crucial for surveillance
applications. Current methods primarily include appearance-based and
action-based techniques. Appearance-based methods rely on low-level visual
features such as color, texture, and shape. They learn a large number of pixel
patterns and features related to known scenes during training, making them
effective in detecting anomalies within these familiar contexts. However, when
encountering new or significantly changed scenes, i.e., unknown scenes, they
often fail because existing SOTA methods do not effectively capture the
relationship between actions and their surrounding scenes, resulting in low
generalization. In contrast, action-based methods focus on detecting anomalies
in human actions but are usually less informative because they tend to overlook
the relationship between actions and their scenes, leading to incorrect
detection. For instance, the normal event of running on the beach and the
abnormal event of running on the street might both be considered normal due to
the lack of scene information. In short, current methods struggle to integrate
low-level visual and high-level action features, leading to poor anomaly
detection in varied and complex scenes. To address this challenge, we propose a
novel decoupling-based architecture for human-related video anomaly detection
(DecoAD). DecoAD significantly improves the integration of visual and action
features through the decoupling and interweaving of scenes and actions, thereby
enabling a more intuitive and accurate understanding of complex behaviors and
scenes. DecoAD supports fully supervised, weakly supervised, and unsupervised
settings.",2024-09-05 04:13:13+00:00,"['Chenglizhao Chen', 'Xinyu Liu', 'Mengke Song', 'Luming Li', 'Xu Yu', 'Shanchen Pang']",http://arxiv.org/abs/2409.03236v1
"Survey on Visual Signal Coding and Processing with Generative Models: Technologies, Standards and Optimization","This paper provides a survey of the latest developments in visual signal
coding and processing with generative models. Specifically, our focus is on
presenting the advancement of generative models and their influence on research
in the domain of visual signal coding and processing. This survey study begins
with a brief introduction of well-established generative models, including the
Variational Autoencoder (VAE) models, Generative Adversarial Network (GAN)
models, Autoregressive (AR) models, Normalizing Flows and Diffusion models. The
subsequent section of the paper explores the advancements in visual signal
coding based on generative models, as well as the ongoing international
standardization activities. In the realm of visual signal processing, our focus
lies on the application and development of various generative models in the
research of visual signal restoration. We also present the latest developments
in generative visual signal synthesis and editing, along with visual signal
quality assessment using generative models and quality assessment for
generative models. The practical implementation of these studies is closely
linked to the investigation of fast optimization. This paper additionally
presents the latest advancements in fast optimization on visual signal coding
and processing with generative models. We hope to advance this field by
providing researchers and practitioners a comprehensive literature review on
the topic of visual signal coding and processing with generative models.",2024-05-23 06:32:27+00:00,"['Zhibo Chen', 'Heming Sun', 'Li Zhang', 'Fan Zhang']",http://arxiv.org/abs/2405.14221v1
Explorative Inbetweening of Time and Space,"We introduce bounded generation as a generalized task to control video
generation to synthesize arbitrary camera and subject motion based only on a
given start and end frame. Our objective is to fully leverage the inherent
generalization capability of an image-to-video model without additional
training or fine-tuning of the original model. This is achieved through the
proposed new sampling strategy, which we call Time Reversal Fusion, that fuses
the temporally forward and backward denoising paths conditioned on the start
and end frame, respectively. The fused path results in a video that smoothly
connects the two frames, generating inbetweening of faithful subject motion,
novel views of static scenes, and seamless video looping when the two bounding
frames are identical. We curate a diverse evaluation dataset of image pairs and
compare against the closest existing methods. We find that Time Reversal Fusion
outperforms related work on all subtasks, exhibiting the ability to generate
complex motions and 3D-consistent views guided by bounded frames. See project
page at https://time-reversal.github.io.",2024-03-21 17:57:31+00:00,"['Haiwen Feng', 'Zheng Ding', 'Zhihao Xia', 'Simon Niklaus', 'Victoria Abrevaya', 'Michael J. Black', 'Xuaner Zhang']",http://arxiv.org/abs/2403.14611v1
Text-to-Audio Generation Synchronized with Videos,"In recent times, the focus on text-to-audio (TTA) generation has intensified,
as researchers strive to synthesize audio from textual descriptions. However,
most existing methods, though leveraging latent diffusion models to learn the
correlation between audio and text embeddings, fall short when it comes to
maintaining a seamless synchronization between the produced audio and its
video. This often results in discernible audio-visual mismatches. To bridge
this gap, we introduce a groundbreaking benchmark for Text-to-Audio generation
that aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself
with three novel metrics dedicated to evaluating visual alignment and temporal
consistency. To complement this, we also present a simple yet effective
video-aligned TTA generation model, namely T2AV. Moving beyond traditional
methods, T2AV refines the latent diffusion approach by integrating
visual-aligned text embeddings as its conditional foundation. It employs a
temporal multi-head attention transformer to extract and understand temporal
nuances from video data, a feat amplified by our Audio-Visual ControlNet that
adeptly merges temporal visual representations with text embeddings. Further
enhancing this integration, we weave in a contrastive learning objective,
designed to ensure that the visual-aligned text embeddings resonate closely
with the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench
demonstrate that our T2AV sets a new standard for video-aligned TTA generation
in ensuring visual alignment and temporal consistency.",2024-03-08 22:27:38+00:00,"['Shentong Mo', 'Jing Shi', 'Yapeng Tian']",http://arxiv.org/abs/2403.07938v1
Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model,"Co-speech gestures, if presented in the lively form of videos, can achieve
superior visual effects in human-machine interaction. While previous works
mostly generate structural human skeletons, resulting in the omission of
appearance information, we focus on the direct generation of audio-driven
co-speech gesture videos in this work. There are two main challenges: 1) A
suitable motion feature is needed to describe complex human movements with
crucial appearance information. 2) Gestures and speech exhibit inherent
dependencies and should be temporally aligned even of arbitrary length. To
solve these problems, we present a novel motion-decoupled framework to generate
co-speech gesture videos. Specifically, we first introduce a well-designed
nonlinear TPS transformation to obtain latent motion features preserving
essential appearance information. Then a transformer-based diffusion model is
proposed to learn the temporal correlation between gestures and speech, and
performs generation in the latent motion space, followed by an optimal motion
selection module to produce long-term coherent and consistent gesture videos.
For better visual perception, we further design a refinement network focusing
on missing details of certain areas. Extensive experimental results show that
our proposed framework significantly outperforms existing approaches in both
motion and video-related evaluations. Our code, demos, and more resources are
available at https://github.com/thuhcsi/S2G-MDDiffusion.",2024-04-02 11:40:34+00:00,"['Xu He', 'Qiaochu Huang', 'Zhensong Zhang', 'Zhiwei Lin', 'Zhiyong Wu', 'Sicheng Yang', 'Minglei Li', 'Zhiyi Chen', 'Songcen Xu', 'Xiaofei Wu']",http://arxiv.org/abs/2404.01862v1
MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes,"While controllable generative models for images and videos have achieved
remarkable success, high-quality models for 3D scenes, particularly in
unbounded scenarios like autonomous driving, remain underdeveloped due to high
data acquisition costs. In this paper, we introduce MagicDrive3D, a novel
pipeline for controllable 3D street scene generation that supports
multi-condition control, including BEV maps, 3D objects, and text descriptions.
Unlike previous methods that reconstruct before training the generative models,
MagicDrive3D first trains a video generation model and then reconstructs from
the generated data. This innovative approach enables easily controllable
generation and static scene acquisition, resulting in high-quality scene
reconstruction. To address the minor errors in generated content, we propose
deformable Gaussian splatting with monocular depth initialization and
appearance modeling to manage exposure discrepancies across viewpoints.
Validated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality
3D driving scenes that support any-view rendering and enhance downstream tasks
like BEV segmentation. Our results demonstrate the framework's superior
performance, showcasing its potential for autonomous driving simulation and
beyond.",2024-05-23 12:04:51+00:00,"['Ruiyuan Gao', 'Kai Chen', 'Zhihao Li', 'Lanqing Hong', 'Zhenguo Li', 'Qiang Xu']",http://arxiv.org/abs/2405.14475v3
SUGAR: Subject-Driven Video Customization in a Zero-Shot Manner,"We present SUGAR, a zero-shot method for subject-driven video customization.
Given an input image, SUGAR is capable of generating videos for the subject
contained in the image and aligning the generation with arbitrary visual
attributes such as style and motion specified by user-input text. Unlike
previous methods, which require test-time fine-tuning or fail to generate
text-aligned videos, SUGAR achieves superior results without the need for extra
cost at test-time. To enable zero-shot capability, we introduce a scalable
pipeline to construct synthetic dataset which is specifically designed for
subject-driven customization, leading to 2.5 millions of image-video-text
triplets. Additionally, we propose several methods to enhance our model,
including special attention designs, improved training strategies, and a
refined sampling algorithm. Extensive experiments are conducted. Compared to
previous methods, SUGAR achieves state-of-the-art results in identity
preservation, video dynamics, and video-text alignment for subject-driven video
customization, demonstrating the effectiveness of our proposed method.",2024-12-13 20:01:51+00:00,"['Yufan Zhou', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Nanxuan Zhao', 'Jing Shi', 'Tong Sun']",http://arxiv.org/abs/2412.10533v1
OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation,"One image to editable dynamic 3D model and video generation is novel
direction and change in the research area of single image to 3D representation
or 3D reconstruction of image. Gaussian Splatting has demonstrated its
advantages in implicit 3D reconstruction, compared with the original Neural
Radiance Fields. As the rapid development of technologies and principles,
people tried to used the Stable Diffusion models to generate targeted models
with text instructions. However, using the normal implicit machine learning
methods is hard to gain the precise motions and actions control, further more,
it is difficult to generate a long content and semantic continuous 3D video. To
address this issue, we propose the OneTo3D, a method and theory to used one
single image to generate the editable 3D model and generate the targeted
semantic continuous time-unlimited 3D video. We used a normal basic Gaussian
Splatting model to generate the 3D model from a single image, which requires
less volume of video memory and computer calculation ability. Subsequently, we
designed an automatic generation and self-adaptive binding mechanism for the
object armature. Combined with the re-editable motions and actions analyzing
and controlling algorithm we proposed, we can achieve a better performance than
the SOTA projects in the area of building the 3D model precise motions and
actions control, and generating a stable semantic continuous time-unlimited 3D
video with the input text instructions. Here we will analyze the detailed
implementation methods and theories analyses. Relative comparisons and
conclusions will be presented. The project code is open source.",2024-05-10 15:44:11+00:00,['Jinwei Lin'],http://arxiv.org/abs/2405.06547v1
Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy,"Diffusion models have recently achieved great success in the synthesis of
high-quality images and videos. However, the existing denoising techniques in
diffusion models are commonly based on step-by-step noise predictions, which
suffers from high computation cost, resulting in a prohibitive latency for
interactive applications. In this paper, we propose AdaptiveDiffusion to
relieve this bottleneck by adaptively reducing the noise prediction steps
during the denoising process. Our method considers the potential of skipping as
many noise prediction steps as possible while keeping the final denoised
results identical to the original full-step ones. Specifically, the skipping
strategy is guided by the third-order latent difference that indicates the
stability between timesteps during the denoising process, which benefits the
reusing of previous noise prediction results. Extensive experiments on image
and video diffusion models demonstrate that our method can significantly speed
up the denoising process while generating identical results to the original
process, achieving up to an average 2~5x speedup without quality degradation.",2024-10-13 15:19:18+00:00,"['Hancheng Ye', 'Jiakang Yuan', 'Renqiu Xia', 'Xiangchao Yan', 'Tao Chen', 'Junchi Yan', 'Botian Shi', 'Bo Zhang']",http://arxiv.org/abs/2410.09873v1
Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips,"Laparoscopic video tracking primarily focuses on two target types: surgical
instruments and anatomy. The former could be used for skill assessment, while
the latter is necessary for the projection of virtual overlays. Where
instrument and anatomy tracking have often been considered two separate
problems, in this paper, we propose a method for joint tracking of all
structures simultaneously. Based on a single 2D monocular video clip, we train
a neural field to represent a continuous spatiotemporal scene, used to create
3D tracks of all surfaces visible in at least one frame. Due to the small size
of instruments, they generally cover a small part of the image only, resulting
in decreased tracking accuracy. Therefore, we propose enhanced class weighting
to improve the instrument tracks. We evaluate tracking on video clips from
laparoscopic cholecystectomies, where we find mean tracking accuracies of 92.4%
for anatomical structures and 87.4% for instruments. Additionally, we assess
the quality of depth maps obtained from the method's scene reconstructions. We
show that these pseudo-depths have comparable quality to a state-of-the-art
pre-trained depth estimator. On laparoscopic videos in the SCARED dataset, the
method predicts depth with an MAE of 2.9 mm and a relative error of 9.2%. These
results show the feasibility of using neural fields for monocular 3D
reconstruction of laparoscopic scenes.",2024-03-28 09:44:20+00:00,"['Beerend G. A. Gerats', 'Jelmer M. Wolterink', 'Seb P. Mol', 'Ivo A. M. J. Broeders']",http://arxiv.org/abs/2403.19265v1
Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos,"Egocentric videos provide valuable insights into human interactions with the
physical world, which has sparked growing interest in the computer vision and
robotics communities. A critical challenge in fully understanding the geometry
and dynamics of egocentric videos is dense scene reconstruction. However, the
lack of high-quality labeled datasets in this field has hindered the
effectiveness of current supervised learning methods. In this work, we aim to
address this issue by exploring an self-supervised dynamic scene reconstruction
approach. We introduce EgoMono4D, a novel model that unifies the estimation of
multiple variables necessary for Egocentric Monocular 4D reconstruction,
including camera intrinsic, camera poses, and video depth, all within a fast
feed-forward framework. Starting from pretrained single-frame depth and
intrinsic estimation model, we extend it with camera poses estimation and align
multi-frame results on large-scale unlabeled egocentric videos. We evaluate
EgoMono4D in both in-domain and zero-shot generalization settings, achieving
superior performance in dense pointclouds sequence reconstruction compared to
all baselines. EgoMono4D represents the first attempt to apply self-supervised
learning for pointclouds sequence reconstruction to the label-scarce egocentric
field, enabling fast, dense, and generalizable reconstruction. The interactable
visualization, code and trained models are released
https://egomono4d.github.io/",2024-11-14 02:57:11+00:00,"['Chengbo Yuan', 'Geng Chen', 'Li Yi', 'Yang Gao']",http://arxiv.org/abs/2411.09145v3
Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion,"We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.",2024-03-21 17:59:03+00:00,"['Xiang Fan', 'Anand Bhattad', 'Ranjay Krishna']",http://arxiv.org/abs/2403.14617v3
GenMM: Geometrically and Temporally Consistent Multimodal Data Generation for Video and LiDAR,"Multimodal synthetic data generation is crucial in domains such as autonomous
driving, robotics, augmented/virtual reality, and retail. We propose a novel
approach, GenMM, for jointly editing RGB videos and LiDAR scans by inserting
temporally and geometrically consistent 3D objects. Our method uses a reference
image and 3D bounding boxes to seamlessly insert and blend new objects into
target videos. We inpaint the 2D Regions of Interest (consistent with 3D boxes)
using a diffusion-based video inpainting model. We then compute semantic
boundaries of the object and estimate it's surface depth using state-of-the-art
semantic segmentation and monocular depth estimation techniques. Subsequently,
we employ a geometry-based optimization algorithm to recover the 3D shape of
the object's surface, ensuring it fits precisely within the 3D bounding box.
Finally, LiDAR rays intersecting with the new object surface are updated to
reflect consistent depths with its geometry. Our experiments demonstrate the
effectiveness of GenMM in inserting various 3D objects across video and LiDAR
modalities.",2024-06-15 19:29:01+00:00,"['Bharat Singh', 'Viveka Kulharia', 'Luyu Yang', 'Avinash Ravichandran', 'Ambrish Tyagi', 'Ashish Shrivastava']",http://arxiv.org/abs/2406.10722v1
"FFA Sora, video generation as fundus fluorescein angiography simulator","Fundus fluorescein angiography (FFA) is critical for diagnosing retinal
vascular diseases, but beginners often struggle with image interpretation. This
study develops FFA Sora, a text-to-video model that converts FFA reports into
dynamic videos via a Wavelet-Flow Variational Autoencoder (WF-VAE) and a
diffusion transformer (DiT). Trained on an anonymized dataset, FFA Sora
accurately simulates disease features from the input text, as confirmed by
objective metrics: Frechet Video Distance (FVD) = 329.78, Learned Perceptual
Image Patch Similarity (LPIPS) = 0.48, and Visual-question-answering Score
(VQAScore) = 0.61. Specific evaluations showed acceptable alignment between the
generated videos and textual prompts, with BERTScore of 0.35. Additionally, the
model demonstrated strong privacy-preserving performance in retrieval
evaluations, achieving an average Recall@K of 0.073. Human assessments
indicated satisfactory visual quality, with an average score of 1.570(scale: 1
= best, 5 = worst). This model addresses privacy concerns associated with
sharing large-scale FFA data and enhances medical education.",2024-12-23 07:18:13+00:00,"['Xinyuan Wu', 'Lili Wang', 'Ruoyu Chen', 'Bowen Liu', 'Weiyi Zhang', 'Xi Yang', 'Yifan Feng', 'Mingguang He', 'Danli Shi']",http://arxiv.org/abs/2412.17346v1
KVQ: Kwai Video Quality Assessment for Short-form Videos,"Short-form UGC video platforms, like Kwai and TikTok, have been an emerging
and irreplaceable mainstream media form, thriving on user-friendly engagement,
and kaleidoscope creation, etc. However, the advancing content-generation
modes, e.g., special effects, and sophisticated processing workflows, e.g.,
de-artifacts, have introduced significant challenges to recent UGC video
quality assessment: (i) the ambiguous contents hinder the identification of
quality-determined regions. (ii) the diverse and complicated hybrid distortions
are hard to distinguish. To tackle the above challenges and assist in the
development of short-form videos, we establish the first large-scale
Kaleidoscope short Video database for Quality assessment, termed KVQ, which
comprises 600 user-uploaded short videos and 3600 processed videos through the
diverse practical processing workflows, including pre-processing, transcoding,
and enhancement. Among them, the absolute quality score of each video and
partial ranking score among indistinguishable samples are provided by a team of
professional researchers specializing in image processing. Based on this
database, we propose the first short-form video quality evaluator, i.e., KSVQE,
which enables the quality evaluator to identify the quality-determined
semantics with the content understanding of large vision language models (i.e.,
CLIP) and distinguish the distortions with the distortion understanding module.
Experimental results have shown the effectiveness of KSVQE on our KVQ database
and popular VQA databases.",2024-02-11 14:37:54+00:00,"['Yiting Lu', 'Xin Li', 'Yajing Pei', 'Kun Yuan', 'Qizhi Xie', 'Yunpeng Qu', 'Ming Sun', 'Chao Zhou', 'Zhibo Chen']",http://arxiv.org/abs/2402.07220v2
VideoPrism: A Foundational Visual Encoder for Video Understanding,"We introduce VideoPrism, a general-purpose video encoder that tackles diverse
video understanding tasks with a single frozen model. We pretrain VideoPrism on
a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M
video clips with noisy parallel text (e.g., ASR transcripts). The pretraining
approach improves upon masked autoencoding by global-local distillation of
semantic video embeddings and a token shuffling scheme, enabling VideoPrism to
focus primarily on the video modality while leveraging the invaluable text
associated with videos. We extensively test VideoPrism on four broad groups of
video understanding tasks, from web video question answering to CV for science,
achieving state-of-the-art performance on 31 out of 33 video understanding
benchmarks.",2024-02-20 18:29:49+00:00,"['Long Zhao', 'Nitesh B. Gundavarapu', 'Liangzhe Yuan', 'Hao Zhou', 'Shen Yan', 'Jennifer J. Sun', 'Luke Friedman', 'Rui Qian', 'Tobias Weyand', 'Yue Zhao', 'Rachel Hornung', 'Florian Schroff', 'Ming-Hsuan Yang', 'David A. Ross', 'Huisheng Wang', 'Hartwig Adam', 'Mikhail Sirotenko', 'Ting Liu', 'Boqing Gong']",http://arxiv.org/abs/2402.13217v2
Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement,"Video Corpus Moment Retrieval (VCMR) is a new video retrieval task aimed at
retrieving a relevant moment from a large corpus of untrimmed videos using a
text query. The relevance between the video and query is partial, mainly
evident in two aspects:~(1)~Scope: The untrimmed video contains many frames,
but not all are relevant to the query. Strong relevance is typically observed
only within the relevant moment.~(2)~Modality: The relevance of the query
varies with different modalities. Action descriptions align more with visual
elements, while character conversations are more related to textual
information.Existing methods often treat all video contents equally, leading to
sub-optimal moment retrieval. We argue that effectively capturing the partial
relevance between the query and video is essential for the VCMR task. To this
end, we propose a Partial Relevance Enhanced Model~(PREM) to improve VCMR. VCMR
involves two sub-tasks: video retrieval and moment localization. To align with
their distinct objectives, we implement specialized partial relevance
enhancement strategies. For video retrieval, we introduce a multi-modal
collaborative video retriever, generating different query representations for
the two modalities by modality-specific pooling, ensuring a more effective
match. For moment localization, we propose the focus-then-fuse moment
localizer, utilizing modality-specific gates to capture essential content. We
also introduce relevant content-enhanced training methods for both retriever
and localizer to enhance the ability of model to capture relevant content.
Experimental results on TVR and DiDeMo datasets show that the proposed model
outperforms the baselines, achieving a new state-of-the-art of VCMR. The code
is available at \url{https://github.com/hdy007007/PREM}.",2024-02-21 07:16:06+00:00,"['Danyang Hou', 'Liang Pang', 'Huawei Shen', 'Xueqi Cheng']",http://arxiv.org/abs/2402.13576v2
Shorts vs. Regular Videos on YouTube: A Comparative Analysis of User Engagement and Content Creation Trends,"YouTube introduced the Shorts video format in 2021, allowing users to upload
short videos that are prominently displayed on its website and app. Despite
having such a large visual footprint, there are no studies to date that have
looked at the impact Shorts introduction had on the production and consumption
of content on YouTube. This paper presents the first comparative analysis of
YouTube Shorts versus regular videos with respect to user engagement (i.e.,
views, likes, and comments), content creation frequency and video categories.
We collected a dataset containing information about 70k channels that posted at
least one Short, and we analyzed the metadata of all the videos (9.9M Shorts
and 6.9M regular videos) they uploaded between January 2021 and December 2022,
spanning a two-year period including the introduction of Shorts. Our
longitudinal analysis shows that content creators consistently increased the
frequency of Shorts production over this period, especially for newly-created
channels, which surpassed that of regular videos. We also observe that Shorts
target mostly entertainment categories, while regular videos cover a wide
variety of categories. In general, Shorts attract more views and likes per view
than regular videos, but attract less comments per view. However, Shorts do not
outperform regular videos in the education and political categories as much as
they do in other categories. Our study contributes to understanding social
media dynamics, to quantifying the spread of short-form content, and to
motivating future research on its impact on society.",2024-03-01 11:20:54+00:00,"['Caroline Violot', 'Turulcan Elmas', 'Igor Bilogrevic', 'Mathias Humbert']",http://arxiv.org/abs/2403.00454v1
TempCompass: Do Video LLMs Really Understand Videos?,"Recently, there is a surge in interest surrounding video large language
models (Video LLMs). However, existing benchmarks fail to provide a
comprehensive feedback on the temporal perception ability of Video LLMs. On the
one hand, most of them are unable to distinguish between different temporal
aspects (e.g., speed, direction) and thus cannot reflect the nuanced
performance on these specific aspects. On the other hand, they are limited in
the diversity of task formats (e.g., only multi-choice QA), which hinders the
understanding of how temporal perception performance may vary across different
types of tasks. Motivated by these two problems, we propose the
\textbf{TempCompass} benchmark, which introduces a diversity of temporal
aspects and task formats. To collect high-quality test data, we devise two
novel strategies: (1) In video collection, we construct conflicting videos that
share the same static content but differ in a specific temporal aspect, which
prevents Video LLMs from leveraging single-frame bias or language priors. (2)
To collect the task instructions, we propose a paradigm where humans first
annotate meta-information for a video and then an LLM generates the
instruction. We also design an LLM-based approach to automatically and
accurately evaluate the responses from Video LLMs. Based on TempCompass, we
comprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs,
and reveal the discerning fact that these models exhibit notably poor temporal
perception ability. Our data will be available at
https://github.com/llyx97/TempCompass.",2024-03-01 12:02:19+00:00,"['Yuanxin Liu', 'Shicheng Li', 'Yi Liu', 'Yuxiang Wang', 'Shuhuai Ren', 'Lei Li', 'Sishuo Chen', 'Xu Sun', 'Lu Hou']",http://arxiv.org/abs/2403.00476v3
Koala: Key frame-conditioned long video-LLM,"Long video question answering is a challenging task that involves recognizing
short-term activities and reasoning about their fine-grained relationships.
State-of-the-art video Large Language Models (vLLMs) hold promise as a viable
solution due to their demonstrated emergent capabilities on new tasks. However,
despite being trained on millions of short seconds-long videos, vLLMs are
unable to understand minutes-long videos and accurately answer questions about
them. To address this limitation, we propose a lightweight and self-supervised
approach, Key frame-conditioned long video-LLM (Koala), that introduces
learnable spatiotemporal queries to adapt pretrained vLLMs for generalizing to
longer videos. Our approach introduces two new tokenizers that condition on
visual tokens computed from sparse video key frames for understanding short and
long video moments. We train our proposed approach on HowTo100M and demonstrate
its effectiveness on zero-shot long video understanding benchmarks, where it
outperforms state-of-the-art large models by 3 - 6% in absolute accuracy across
all tasks. Surprisingly, we also empirically show that our approach not only
helps a pretrained vLLM to understand long videos but also improves its
accuracy on short-term action recognition.",2024-04-05 18:33:04+00:00,"['Reuben Tan', 'Ximeng Sun', 'Ping Hu', 'Jui-hsien Wang', 'Hanieh Deilamsalehy', 'Bryan A. Plummer', 'Bryan Russell', 'Kate Saenko']",http://arxiv.org/abs/2404.04346v3
Object-Attribute-Relation Representation Based Video Semantic Communication,"With the rapid growth of multimedia data volume, there is an increasing need
for efficient video transmission in applications such as virtual reality and
future video streaming services. Semantic communication is emerging as a vital
technique for ensuring efficient and reliable transmission in low-bandwidth,
high-noise settings. However, most current approaches focus on joint
source-channel coding (JSCC) that depends on end-to-end training. These methods
often lack an interpretable semantic representation and struggle with
adaptability to various downstream tasks. In this paper, we introduce the use
of object-attribute-relation (OAR) as a semantic framework for videos to
facilitate low bit-rate coding and enhance the JSCC process for more effective
video transmission. We utilize OAR sequences for both low bit-rate
representation and generative video reconstruction. Additionally, we
incorporate OAR into the image JSCC model to prioritize communication resources
for areas more critical to downstream tasks. Our experiments on traffic
surveillance video datasets assess the effectiveness of our approach in terms
of video transmission performance. The empirical findings demonstrate that our
OAR-based video coding method not only outperforms H.265 coding at lower
bit-rates but also synergizes with JSCC to deliver robust and efficient video
transmission.",2024-06-15 02:19:31+00:00,"['Qiyuan Du', 'Yiping Duan', 'Qianqian Yang', 'Xiaoming Tao', 'Mrouane Debbah']",http://arxiv.org/abs/2406.10469v2
"Standard compliant video coding using low complexity, switchable neural wrappers","The proliferation of high resolution videos posts great storage and bandwidth
pressure on cloud video services, driving the development of next-generation
video codecs. Despite great progress made in neural video coding, existing
approaches are still far from economical deployment considering the complexity
and rate-distortion performance tradeoff. To clear the roadblocks for neural
video coding, in this paper we propose a new framework featuring standard
compatibility, high performance, and low decoding complexity. We employ a set
of jointly optimized neural pre- and post-processors, wrapping a standard video
codec, to encode videos at different resolutions. The rate-distorion optimal
downsampling ratio is signaled to the decoder at the per-sequence level for
each target rate. We design a low complexity neural post-processor architecture
that can handle different upsampling ratios. The change of resolution exploits
the spatial redundancy in high-resolution videos, while the neural wrapper
further achieves rate-distortion performance improvement through end-to-end
optimization with a codec proxy. Our light-weight post-processor architecture
has a complexity of 516 MACs / pixel, and achieves 9.3% BD-Rate reduction over
VVC on the UVG dataset, and 6.4% on AOM CTC Class A1. Our approach has the
potential to further advance the performance of the latest video coding
standards using neural processing with minimal added complexity.",2024-07-10 06:36:45+00:00,"['Yueyu Hu', 'Chenhao Zhang', 'Onur G. Guleryuz', 'Debargha Mukherjee', 'Yao Wang']",http://arxiv.org/abs/2407.07395v1
Video DataFlywheel: Resolving the Impossible Data Trinity in Video-Language Understanding,"Recently, video-language understanding has achieved great success through
large-scale pre-training. However, data scarcity remains a prevailing
challenge. This study quantitatively reveals an ""impossible trinity"" among data
quantity, diversity, and quality in pre-training datasets. Recent efforts seek
to refine large-scale, diverse ASR datasets compromised by low quality through
synthetic annotations. These methods successfully leverage useful information
in multimodal video content (frames, tags, ASR transcripts, etc.) to refine the
original annotations. Nevertheless, they struggle to mitigate noise within
synthetic annotations and lack scalability as the dataset size expands. To
address these issues, we introduce the Video DataFlywheel framework, which
iteratively refines video annotations with improved noise control methods. For
iterative refinement, we first leverage a video-language model to generate
synthetic annotations, resulting in a refined dataset. Then, we pre-train on it
and fine-tune on human refinement examples for a stronger model. These
processes are repeated for continuous improvement. For noise control, we
present AdaTaiLr, a novel noise control method that requires weaker assumptions
on noise distribution, thereby proving more effective in large datasets with
theoretical guarantees. The combination of iterative refinement and AdaTaiLr
can achieve better scalability in video-language understanding. Extensive
experiments show that our framework outperforms existing data refinement
baselines, delivering a 3% performance boost and improving dataset quality with
minimal diversity loss. Furthermore, our refined dataset facilitates
significant improvements in various video-language understanding tasks,
including video question answering and text-video retrieval.",2024-09-29 03:33:35+00:00,"['Xiao Wang', 'Jianlong Wu', 'Zijia Lin', 'Fuzheng Zhang', 'Di Zhang', 'Liqiang Nie']",http://arxiv.org/abs/2409.19532v1
TRACE: Temporal Grounding Video LLM via Causal Event Modeling,"Video Temporal Grounding (VTG) is a crucial capability for video
understanding models and plays a vital role in downstream tasks such as video
browsing and editing. To effectively handle various tasks simultaneously and
enable zero-shot prediction, there is a growing trend in employing video LLMs
for VTG tasks. However, current video LLM-based methods rely exclusively on
natural language generation, lacking the ability to model the clear structure
inherent in videos, which restricts their effectiveness in tackling VTG tasks.
To address this issue, this paper first formally introduces causal event
modeling framework, which represents video LLM outputs as sequences of events,
and predict the current event using previous events, video inputs, and textural
instructions. Each event consists of three components: timestamps, salient
scores, and textual captions. We then propose a novel task-interleaved video
LLM called TRACE to effectively implement the causal event modeling framework
in practice. The TRACE process visual frames, timestamps, salient scores, and
text as distinct tasks, employing various encoders and decoding heads for each.
Task tokens are arranged in an interleaved sequence according to the causal
event modeling framework's formulation. Extensive experiments on various VTG
tasks and datasets demonstrate the superior performance of TRACE compared to
state-of-the-art video LLMs. Our model and code are available at
https://github.com/gyxxyg/TRACE.",2024-10-08 02:46:30+00:00,"['Yongxin Guo', 'Jingyu Liu', 'Mingda Li', 'Qingbin Liu', 'Xi Chen', 'Xiaoying Tang']",http://arxiv.org/abs/2410.05643v3
PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection and Natural Language Interpretation,"Video crime detection is a significant application of computer vision and
artificial intelligence. However, existing datasets primarily focus on
detecting severe crimes by analyzing entire video clips, often neglecting the
precursor activities (i.e., privacy violations) that could potentially prevent
these crimes. To address this limitation, we present PV-VTT (Privacy Violation
Video To Text), a unique multimodal dataset aimed at identifying privacy
violations. PV-VTT provides detailed annotations for both video and text in
scenarios. To ensure the privacy of individuals in the videos, we only provide
video feature vectors, avoiding the release of any raw video data. This
privacy-focused approach allows researchers to use the dataset while protecting
participant confidentiality. Recognizing that privacy violations are often
ambiguous and context-dependent, we propose a Graph Neural Network (GNN)-based
video description model. Our model generates a GNN-based prompt with image for
Large Language Model (LLM), which deliver cost-effective and high-quality video
descriptions. By leveraging a single video frame along with relevant text, our
method reduces the number of input tokens required, maintaining descriptive
quality while optimizing LLM API-usage. Extensive experiments validate the
effectiveness and interpretability of our approach in video description tasks
and flexibility of our PV-VTT dataset.",2024-10-30 01:02:20+00:00,"['Ryozo Masukawa', 'Sanggeon Yun', 'Yoshiki Yamaguchi', 'Mohsen Imani']",http://arxiv.org/abs/2410.22623v2
Large Motion Video Autoencoding with Cross-modal Video VAE,"Learning a robust video Variational Autoencoder (VAE) is essential for
reducing video redundancy and facilitating efficient video generation. Directly
applying image VAEs to individual frames in isolation can result in temporal
inconsistencies and suboptimal compression rates due to a lack of temporal
compression. Existing Video VAEs have begun to address temporal compression;
however, they often suffer from inadequate reconstruction performance. In this
paper, we present a novel and powerful video autoencoder capable of
high-fidelity video encoding. First, we observe that entangling spatial and
temporal compression by merely extending the image VAE to a 3D VAE can
introduce motion blur and detail distortion artifacts. Thus, we propose
temporal-aware spatial compression to better encode and decode the spatial
information. Additionally, we integrate a lightweight motion compression model
for further temporal compression. Second, we propose to leverage the textual
information inherent in text-to-video datasets and incorporate text guidance
into our model. This significantly enhances reconstruction quality,
particularly in terms of detail preservation and temporal stability. Third, we
further improve the versatility of our model through joint training on both
images and videos, which not only enhances reconstruction quality but also
enables the model to perform both image and video autoencoding. Extensive
evaluations against strong recent baselines demonstrate the superior
performance of our method. The project website can be found
at~\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.",2024-12-23 18:58:24+00:00,"['Yazhou Xing', 'Yang Fei', 'Yingqing He', 'Jingye Chen', 'Jiaxin Xie', 'Xiaowei Chi', 'Qifeng Chen']",http://arxiv.org/abs/2412.17805v1
MegActor: Harness the Power of Raw Video for Vivid Portrait Animation,"Despite raw driving videos contain richer information on facial expressions
than intermediate representations such as landmarks in the field of portrait
animation, they are seldom the subject of research. This is due to two
challenges inherent in portrait animation driven with raw videos: 1)
significant identity leakage; 2) Irrelevant background and facial details such
as wrinkles degrade performance. To harnesses the power of the raw videos for
vivid portrait animation, we proposed a pioneering conditional diffusion model
named as MegActor. First, we introduced a synthetic data generation framework
for creating videos with consistent motion and expressions but inconsistent IDs
to mitigate the issue of ID leakage. Second, we segmented the foreground and
background of the reference image and employed CLIP to encode the background
details. This encoded information is then integrated into the network via a
text embedding module, thereby ensuring the stability of the background.
Finally, we further style transfer the appearance of the reference image to the
driving video to eliminate the influence of facial details in the driving
videos. Our final model was trained solely on public datasets, achieving
results comparable to commercial models. We hope this will help the open-source
community.The code is available at
https://github.com/megvii-research/MegFaceAnimate.",2024-05-31 14:33:13+00:00,"['Shurong Yang', 'Huadong Li', 'Juhao Wu', 'Minhao Jing', 'Linze Li', 'Renhe Ji', 'Jiajun Liang', 'Haoqiang Fan']",http://arxiv.org/abs/2405.20851v2
Grounded Video Caption Generation,"We propose a new task, dataset and model for grounded video caption
generation. This task unifies captioning and object grounding in video, where
the objects in the caption are grounded in the video via temporally consistent
bounding boxes. We introduce the following contributions. First, we present a
task definition and a manually annotated test dataset for this task, referred
to as GROunded Video Caption Generation (GROC). Second, we introduce a
large-scale automatic annotation method leveraging an existing model for
grounded still image captioning together with an LLM for summarising
frame-level captions into temporally consistent captions in video. Furthermore,
we prompt the LLM to track by language -- classifying noun phrases from the
frame-level captions into noun phrases of the video-level generated caption. We
apply this approach to videos from the HowTo100M dataset, which results in a
new large-scale training dataset, called HowToGround, with automatically
annotated captions and spatio-temporally consistent bounding boxes with
coherent natural language labels. Third, we introduce a new grounded video
caption generation model, called VideoGround, and train the model on the new
automatically annotated HowToGround dataset. Finally, results of our
VideoGround model set the state of the art for the new task of grounded video
caption generation. We perform extensive ablations and demonstrate the
importance of key technical contributions of our model.",2024-11-12 06:44:24+00:00,"['Evangelos Kazakos', 'Cordelia Schmid', 'Josef Sivic']",http://arxiv.org/abs/2411.07584v1
Video as the New Language for Real-World Decision Making,"Both text and video data are abundant on the internet and support large-scale
self-supervised learning through next token or frame prediction. However, they
have not been equally leveraged: language models have had significant
real-world impact, whereas video generation has remained largely limited to
media entertainment. Yet video data captures important information about the
physical world that is difficult to express in language. To address this gap,
we discuss an under-appreciated opportunity to extend video generation to solve
tasks in the real world. We observe how, akin to language, video can serve as a
unified interface that can absorb internet knowledge and represent diverse
tasks. Moreover, we demonstrate how, like language models, video generation can
serve as planners, agents, compute engines, and environment simulators through
techniques such as in-context learning, planning and reinforcement learning. We
identify major impact opportunities in domains such as robotics, self-driving,
and science, supported by recent work that demonstrates how such advanced
capabilities in video generation are plausibly within reach. Lastly, we
identify key challenges in video generation that mitigate progress. Addressing
these challenges will enable video generation models to demonstrate unique
value alongside language models in a wider array of AI applications.",2024-02-27 02:05:29+00:00,"['Sherry Yang', 'Jacob Walker', 'Jack Parker-Holder', 'Yilun Du', 'Jake Bruce', 'Andre Barreto', 'Pieter Abbeel', 'Dale Schuurmans']",http://arxiv.org/abs/2402.17139v1
A Survey of AI-Generated Video Evaluation,"The growing capabilities of AI in generating video content have brought
forward significant challenges in effectively evaluating these videos. Unlike
static images or text, video content involves complex spatial and temporal
dynamics which may require a more comprehensive and systematic evaluation of
its contents in aspects like video presentation quality, semantic information
delivery, alignment with human intentions, and the virtual-reality consistency
with our physical world. This survey identifies the emerging field of
AI-Generated Video Evaluation (AIGVE), highlighting the importance of assessing
how well AI-generated videos align with human perception and meet specific
instructions. We provide a structured analysis of existing methodologies that
could be potentially used to evaluate AI-generated videos. By outlining the
strengths and gaps in current approaches, we advocate for the development of
more robust and nuanced evaluation frameworks that can handle the complexities
of video content, which include not only the conventional metric-based
evaluations, but also the current human-involved evaluations, and the future
model-centered evaluations. This survey aims to establish a foundational
knowledge base for both researchers from academia and practitioners from the
industry, facilitating the future advancement of evaluation methods for
AI-generated video content.",2024-10-24 23:08:39+00:00,"['Xiao Liu', 'Xinhao Xiang', 'Zizhong Li', 'Yongheng Wang', 'Zhuoheng Li', 'Zhuosheng Liu', 'Weidi Zhang', 'Weiqi Ye', 'Jiawei Zhang']",http://arxiv.org/abs/2410.19884v1
T-SVG: Text-Driven Stereoscopic Video Generation,"The advent of stereoscopic videos has opened new horizons in multimedia,
particularly in extended reality (XR) and virtual reality (VR) applications,
where immersive content captivates audiences across various platforms. Despite
its growing popularity, producing stereoscopic videos remains challenging due
to the technical complexities involved in generating stereo parallax. This
refers to the positional differences of objects viewed from two distinct
perspectives and is crucial for creating depth perception. This complex process
poses significant challenges for creators aiming to deliver convincing and
engaging presentations. To address these challenges, this paper introduces the
Text-driven Stereoscopic Video Generation (T-SVG) system. This innovative,
model-agnostic, zero-shot approach streamlines video generation by using text
prompts to create reference videos. These videos are transformed into 3D point
cloud sequences, which are rendered from two perspectives with subtle parallax
differences, achieving a natural stereoscopic effect. T-SVG represents a
significant advancement in stereoscopic content creation by integrating
state-of-the-art, training-free techniques in text-to-video generation, depth
estimation, and video inpainting. Its flexible architecture ensures high
efficiency and user-friendliness, allowing seamless updates with newer models
without retraining. By simplifying the production pipeline, T-SVG makes
stereoscopic video generation accessible to a broader audience, demonstrating
its potential to revolutionize the field.",2024-12-12 14:48:46+00:00,"['Qiao Jin', 'Xiaodong Chen', 'Wu Liu', 'Tao Mei', 'Yongdong Zhang']",http://arxiv.org/abs/2412.09323v1
Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks,"Text-video retrieval is a challenging task that aims to identify relevant
videos given textual queries. Compared to conventional textual retrieval, the
main obstacle for text-video retrieval is the semantic gap between the textual
nature of queries and the visual richness of video content. Previous works
primarily focus on aligning the query and the video by finely aggregating
word-frame matching signals. Inspired by the human cognitive process of
modularly judging the relevance between text and video, the judgment needs
high-order matching signal due to the consecutive and complex nature of video
contents. In this paper, we propose chunk-level text-video matching, where the
query chunks are extracted to describe a specific retrieval unit, and the video
chunks are segmented into distinct clips from videos. We formulate the
chunk-level matching as n-ary correlations modeling between words of the query
and frames of the video and introduce a multi-modal hypergraph for n-ary
correlation modeling. By representing textual units and video frames as nodes
and using hyperedges to depict their relationships, a multi-modal hypergraph is
constructed. In this way, the query and the video can be aligned in a
high-order semantic space. In addition, to enhance the model's generalization
ability, the extracted features are fed into a variational inference component
for computation, obtaining the variational representation under the Gaussian
distribution. The incorporation of hypergraphs and variational inference allows
our model to capture complex, n-ary interactions among textual and visual
contents. Experimental results demonstrate that our proposed method achieves
state-of-the-art performance on the text-video retrieval task.",2024-01-06 09:38:55+00:00,"['Qian Li', 'Lixin Su', 'Jiashu Zhao', 'Long Xia', 'Hengyi Cai', 'Suqi Cheng', 'Hengzhu Tang', 'Junfeng Wang', 'Dawei Yin']",http://arxiv.org/abs/2401.03177v1
Trusted Video Inpainting Localization via Deep Attentive Noise Learning,"Digital video inpainting techniques have been substantially improved with
deep learning in recent years. Although inpainting is originally designed to
repair damaged areas, it can also be used as malicious manipulation to remove
important objects for creating false scenes and facts. As such it is
significant to identify inpainted regions blindly. In this paper, we present a
Trusted Video Inpainting Localization network (TruVIL) with excellent
robustness and generalization ability. Observing that high-frequency noise can
effectively unveil the inpainted regions, we design deep attentive noise
learning in multiple stages to capture the inpainting traces. Firstly, a
multi-scale noise extraction module based on 3D High Pass (HP3D) layers is used
to create the noise modality from input RGB frames. Then the correlation
between such two complementary modalities are explored by a cross-modality
attentive fusion module to facilitate mutual feature learning. Lastly, spatial
details are selectively enhanced by an attentive noise decoding module to boost
the localization performance of the network. To prepare enough training
samples, we also build a frame-level video object segmentation dataset of 2500
videos with pixel-level annotation for all frames. Extensive experimental
results validate the superiority of TruVIL compared with the state-of-the-arts.
In particular, both quantitative and qualitative evaluations on various
inpainted videos verify the remarkable robustness and generalization ability of
our proposed TruVIL. Code and dataset will be available at
https://github.com/multimediaFor/TruVIL.",2024-06-19 14:08:58+00:00,"['Zijie Lou', 'Gang Cao', 'Man Lin']",http://arxiv.org/abs/2406.13576v1
Adult learners recall and recognition performance and affective feedback when learning from an AI-generated synthetic video,"The widespread use of generative AI has led to multiple applications of
AI-generated text and media to potentially enhance learning outcomes. However,
there are a limited number of well-designed experimental studies investigating
the impact of learning gains and affective feedback from AI-generated media
compared to traditional media (e.g., text from documents and human recordings
of video). The current study recruited 500 participants to investigate adult
learners recall and recognition performances as well as their affective
feedback on the AI-generated synthetic video, using a mixed-methods approach
with a pre-and post-test design. Specifically, four learning conditions,
AI-generated framing of human instructor-generated text, AI-generated synthetic
videos with human instructor-generated text, human instructor-generated videos,
and human instructor-generated text frame (baseline), were considered. The
results indicated no statistically significant difference amongst conditions on
recall and recognition performance. In addition, the participants affective
feedback was not statistically significantly different between the two video
conditions. However, adult learners preferred to learn from the video formats
rather than text materials.",2024-11-28 21:40:28+00:00,"['Zoe Ruo-Yu Li', 'Caswell Barry', 'Mutlu Cukurova']",http://arxiv.org/abs/2412.10384v1
Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout,"In this paper, we present our solution for the Second Multimodal Emotion
Recognition Challenge Track 1(MER2024-SEMI). To enhance the accuracy and
generalization performance of emotion recognition, we propose several methods
for Multimodal Emotion Recognition. Firstly, we introduce EmoVCLIP, a model
fine-tuned based on CLIP using vision-language prompt learning, designed for
video-based emotion recognition tasks. By leveraging prompt learning on CLIP,
EmoVCLIP improves the performance of pre-trained CLIP on emotional videos.
Additionally, to address the issue of modality dependence in multimodal fusion,
we employ modality dropout for robust information fusion. Furthermore, to aid
Baichuan in better extracting emotional information, we suggest using GPT-4 as
the prompt for Baichuan. Lastly, we utilize a self-training strategy to
leverage unlabeled videos. In this process, we use unlabeled videos with
high-confidence pseudo-labels generated by our model and incorporate them into
the training set. Experimental results demonstrate that our model ranks 1st in
the MER2024-SEMI track, achieving an accuracy of 90.15% on the test set.",2024-09-11 08:06:47+00:00,"['Anbin QI', 'Zhongliang Liu', 'Xinyong Zhou', 'Jinba Xiao', 'Fengrun Zhang', 'Qi Gan', 'Ming Tao', 'Gaozheng Zhang', 'Lu Zhang']",http://arxiv.org/abs/2409.07078v1
Rhythmic Foley: A Framework For Seamless Audio-Visual Alignment In Video-to-Audio Synthesis,"Our research introduces an innovative framework for video-to-audio synthesis,
which solves the problems of audio-video desynchronization and semantic loss in
the audio. By incorporating a semantic alignment adapter and a temporal
synchronization adapter, our method significantly improves semantic integrity
and the precision of beat point synchronization, particularly in fast-paced
action sequences. Utilizing a contrastive audio-visual pre-trained encoder, our
model is trained with video and high-quality audio data, improving the quality
of the generated audio. This dual-adapter approach empowers users with enhanced
control over audio semantics and beat effects, allowing the adjustment of the
controller to achieve better results. Extensive experiments substantiate the
effectiveness of our framework in achieving seamless audio-visual alignment.",2024-09-13 08:33:03+00:00,"['Zhiqi Huang', 'Dan Luo', 'Jun Wang', 'Huan Liao', 'Zhiheng Li', 'Zhiyong Wu']",http://arxiv.org/abs/2409.08628v1
Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-MLP Architecture,"Pain assessment is essential in developing optimal pain management protocols
to alleviate suffering and prevent functional decline in patients.
Consequently, reliable and accurate automatic pain assessment systems are
essential for continuous and effective patient monitoring. This study presents
synthetic thermal videos generated by Generative Adversarial Networks
integrated into the pain recognition pipeline and evaluates their efficacy. A
framework consisting of a Vision-MLP and a Transformer-based module is
utilized, employing RGB and synthetic thermal videos in unimodal and multimodal
settings. Experiments conducted on facial videos from the BioVid database
demonstrate the effectiveness of synthetic thermal videos and underline the
potential advantages of it.",2024-07-29 09:04:11+00:00,"['Stefanos Gkikas', 'Manolis Tsiknakis']",http://arxiv.org/abs/2407.19811v1
VimTS: A Unified Video and Image Text Spotter for Enhancing the Cross-domain Generalization,"Text spotting, a task involving the extraction of textual information from
image or video sequences, faces challenges in cross-domain adaption, such as
image-to-image and image-to-video generalization. In this paper, we introduce a
new method, termed VimTS, which enhances the generalization ability of the
model by achieving better synergy among different tasks. Typically, we propose
a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively
convert the original single-task model into a multi-task model suitable for
both image and video scenarios with minimal additional parameters. The Prompt
Queries Generation Module facilitates explicit interaction between different
tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable
features for each task. Additionally, to further enable the model to learn
temporal information at a lower cost, we propose a synthetic video text dataset
(VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm.
Notably, our method outperforms the state-of-the-art method by an average of
2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and
TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses
the previous end-to-end video spotting method in ICDAR2015 video and DSText v2
by an average of 5.5% on the MOTA metric, using only image-level data. We
further demonstrate that existing Large Multimodal Models exhibit limitations
in generating cross-domain scene text spotting, in contrast to our VimTS model
which requires significantly fewer parameters and data. The code and datasets
will be made available at the https://VimTextSpotter.github.io.",2024-04-30 15:49:03+00:00,"['Yuliang Liu', 'Mingxin Huang', 'Hao Yan', 'Linger Deng', 'Weijia Wu', 'Hao Lu', 'Chunhua Shen', 'Lianwen Jin', 'Xiang Bai']",http://arxiv.org/abs/2404.19652v4
Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer,"We present a novel approach for generating 360-degree high-quality,
spatio-temporally coherent human videos from a single image. Our framework
combines the strengths of diffusion transformers for capturing global
correlations across viewpoints and time, and CNNs for accurate condition
injection. The core is a hierarchical 4D transformer architecture that
factorizes self-attention across views, time steps, and spatial dimensions,
enabling efficient modeling of the 4D space. Precise conditioning is achieved
by injecting human identity, camera parameters, and temporal signals into the
respective transformers. To train this model, we collect a multi-dimensional
dataset spanning images, videos, multi-view data, and limited 4D footage, along
with a tailored multi-dimensional training strategy. Our approach overcomes the
limitations of previous methods based on generative adversarial networks or
vanilla diffusion models, which struggle with complex motions, viewpoint
changes, and generalization. Through extensive experiments, we demonstrate our
method's ability to synthesize 360-degree realistic, coherent human motion
videos, paving the way for advanced multimedia applications in areas such as
virtual reality and animation.",2024-05-27 17:53:29+00:00,"['Ruizhi Shao', 'Youxin Pang', 'Zerong Zheng', 'Jingxiang Sun', 'Yebin Liu']",http://arxiv.org/abs/2405.17405v2
UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing,"Recent advances in text-guided video editing have showcased promising results
in appearance editing (e.g., stylization). However, video motion editing in the
temporal dimension (e.g., from eating to waving), which distinguishes video
editing from image editing, is underexplored. In this work, we present UniEdit,
a tuning-free framework that supports both video motion and appearance editing
by harnessing the power of a pre-trained text-to-video generator within an
inversion-then-generation framework. To realize motion editing while preserving
source video content, based on the insights that temporal and spatial
self-attention layers encode inter-frame and intra-frame dependency
respectively, we introduce auxiliary motion-reference and reconstruction
branches to produce text-guided motion and source features respectively. The
obtained features are then injected into the main editing path via temporal and
spatial self-attention layers. Extensive experiments demonstrate that UniEdit
covers video motion editing and various appearance editing scenarios, and
surpasses the state-of-the-art methods. Our code will be publicly available.",2024-02-20 17:52:12+00:00,"['Jianhong Bai', 'Tianyu He', 'Yuchi Wang', 'Junliang Guo', 'Haoji Hu', 'Zuozhu Liu', 'Jiang Bian']",http://arxiv.org/abs/2402.13185v4
Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT,"With the rise of short video platforms represented by TikTok, the trend of
users expressing their creativity through photos and videos has increased
dramatically. However, ordinary users lack the professional skills to produce
high-quality videos using professional creation software. To meet the demand
for intelligent and user-friendly video creation tools, we propose the Dynamic
Visual Composition (DVC) task, an interesting and challenging task that aims to
automatically integrate various media elements based on user requirements and
create storytelling videos. We propose an Intelligent Director framework,
utilizing LENS to generate descriptions for images and video frames and
combining ChatGPT to generate coherent captions while recommending appropriate
music names. Then, the best-matched music is obtained through music retrieval.
Then, materials such as captions, images, videos, and music are integrated to
seamlessly synthesize the video. Finally, we apply AnimeGANv2 for style
transfer. We construct UCF101-DVC and Personal Album datasets and verified the
effectiveness of our framework in solving DVC through qualitative and
quantitative comparisons, along with user studies, demonstrating its
substantial potential.",2024-02-24 06:58:15+00:00,"['Sixiao Zheng', 'Jingyang Huo', 'Yu Wang', 'Yanwei Fu']",http://arxiv.org/abs/2402.15746v1
3rd Place Solution for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation,"Referring video object segmentation (RVOS) relies on natural language
expressions to segment target objects in video, emphasizing modeling dense
text-video relations. The current RVOS methods typically use independently
pre-trained vision and language models as backbones, resulting in a significant
domain gap between video and text. In cross-modal feature interaction, text
features are only used as query initialization and do not fully utilize
important information in the text. In this work, we propose using frozen
pre-trained vision-language models (VLM) as backbones, with a specific emphasis
on enhancing cross-modal feature interaction. Firstly, we use frozen
convolutional CLIP backbone to generate feature-aligned vision and text
features, alleviating the issue of domain gap and reducing training costs.
Secondly, we add more cross-modal feature fusion in the pipeline to enhance the
utilization of multi-modal information. Furthermore, we propose a novel video
query initialization method to generate higher quality video queries. Without
bells and whistles, our method achieved 51.5 J&F on the MeViS test set and
ranked 3rd place for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression
guided Video Segmentation.",2024-06-07 11:15:03+00:00,"['Feiyu Pan', 'Hao Fang', 'Xiankai Lu']",http://arxiv.org/abs/2406.04842v1
SITAR: Semi-supervised Image Transformer for Action Recognition,"Recognizing actions from a limited set of labeled videos remains a challenge
as annotating visual data is not only tedious but also can be expensive due to
classified nature. Moreover, handling spatio-temporal data using deep $3$D
transformers for this can introduce significant computational complexity. In
this paper, our objective is to address video action recognition in a
semi-supervised setting by leveraging only a handful of labeled videos along
with a collection of unlabeled videos in a compute efficient manner.
Specifically, we rearrange multiple frames from the input videos in row-column
form to construct super images. Subsequently, we capitalize on the vast pool of
unlabeled samples and employ contrastive learning on the encoded super images.
Our proposed approach employs two pathways to generate representations for
temporally augmented super images originating from the same video.
Specifically, we utilize a 2D image-transformer to generate representations and
apply a contrastive loss function to minimize the similarity between
representations from different videos while maximizing the representations of
identical videos. Our method demonstrates superior performance compared to
existing state-of-the-art approaches for semi-supervised action recognition
across various benchmark datasets, all while significantly reducing
computational costs.",2024-09-04 17:49:54+00:00,"['Owais Iqbal', 'Omprakash Chakraborty', 'Aftab Hussain', 'Rameswar Panda', 'Abir Das']",http://arxiv.org/abs/2409.02910v1
StoryNavi: On-Demand Narrative-Driven Reconstruction of Video Play With Generative AI,"Manually navigating lengthy videos to seek information or answer questions
can be a tedious and time-consuming task for users. We introduce StoryNavi, a
novel system powered by VLLMs for generating customised video play experiences
by retrieving materials from original videos. It directly answers users' query
by constructing non-linear sequence with identified relevant clips to form a
cohesive narrative. StoryNavi offers two modes of playback of the constructed
video plays: 1) video-centric, which plays original audio and skips irrelevant
segments, and 2) narrative-centric, narration guides the experience, and the
original audio is muted. Our technical evaluation showed adequate retrieval
performance compared to human retrieval. Our user evaluation shows that
maintaining narrative coherence significantly enhances user engagement when
viewing disjointed video segments. However, factors like video genre, content,
and the query itself may lead to varying user preferences for the playback
mode.",2024-10-04 07:44:01+00:00,"['Alston Lantian Xu', 'Tianwei Ma', 'Tianmeng Liu', 'Can Liu', 'Alvaro Cassinelli']",http://arxiv.org/abs/2410.03207v1
Variational Quantum Circuits Enhanced Generative Adversarial Network,"Generative adversarial network (GAN) is one of the widely-adopted
machine-learning frameworks for a wide range of applications such as generating
high-quality images, video, and audio contents. However, training a GAN could
become computationally expensive for large neural networks. In this work, we
propose a hybrid quantum-classical architecture for improving GAN (denoted as
QC-GAN). The performance was examed numerically by benchmarking with a
classical GAN using MindSpore Quantum on the task of hand-written image
generation. The generator of the QC-GAN consists of a quantum variational
circuit together with a one-layer neural network, and the discriminator
consists of a traditional neural network. Leveraging the entangling and
expressive power of quantum circuits, our hybrid architecture achieved better
performance (Frechet Inception Distance) than the classical GAN, with much
fewer training parameters and number of iterations for convergence. We have
also demonstrated the superiority of QC-GAN over an alternative quantum GAN,
namely pathGAN, which could hardly generate 16$\times$16 or larger images. This
work demonstrates the value of combining ideas from quantum computing with
machine learning for both areas of Quantum-for-AI and AI-for-Quantum.",2024-02-02 03:59:35+00:00,"['Runqiu Shu', 'Xusheng Xu', 'Man-Hong Yung', 'Wei Cui']",http://arxiv.org/abs/2402.01791v1
Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation,"Diffusion models are the main driver of progress in image and video
synthesis, but suffer from slow inference speed. Distillation methods, like the
recently introduced adversarial diffusion distillation (ADD) aim to shift the
model from many-shot to single-step inference, albeit at the cost of expensive
and difficult optimization due to its reliance on a fixed pretrained DINOv2
discriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a
novel distillation approach overcoming the limitations of ADD. In contrast to
pixel-based ADD, LADD utilizes generative features from pretrained latent
diffusion models. This approach simplifies training and enhances performance,
enabling high-resolution multi-aspect ratio image synthesis. We apply LADD to
Stable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the
performance of state-of-the-art text-to-image generators using only four
unguided sampling steps. Moreover, we systematically investigate its scaling
behavior and demonstrate LADD's effectiveness in various applications such as
image editing and inpainting.",2024-03-18 17:51:43+00:00,"['Axel Sauer', 'Frederic Boesel', 'Tim Dockhorn', 'Andreas Blattmann', 'Patrick Esser', 'Robin Rombach']",http://arxiv.org/abs/2403.12015v1
DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control,"Generating customized content in videos has received increasing attention
recently. However, existing works primarily focus on customized text-to-video
generation for single subject, suffering from subject-missing and
attribute-binding problems when the video is expected to contain multiple
subjects. Furthermore, existing models struggle to assign the desired actions
to the corresponding subjects (action-binding problem), failing to achieve
satisfactory multi-subject generation performance. To tackle the problems, in
this paper, we propose DisenStudio, a novel framework that can generate
text-guided videos for customized multiple subjects, given few images for each
subject. Specifically, DisenStudio enhances a pretrained diffusion-based
text-to-video model with our proposed spatial-disentangled cross-attention
mechanism to associate each subject with the desired action. Then the model is
customized for the multiple subjects with the proposed motion-preserved
disentangled finetuning, which involves three tuning strategies: multi-subject
co-occurrence tuning, masked single-subject tuning, and multi-subject
motion-preserved tuning. The first two strategies guarantee the subject
occurrence and preserve their visual attributes, and the third strategy helps
the model maintain the temporal motion-generation ability when finetuning on
static images. We conduct extensive experiments to demonstrate our proposed
DisenStudio significantly outperforms existing methods in various metrics.
Additionally, we show that DisenStudio can be used as a powerful tool for
various controllable generation applications.",2024-05-21 13:44:55+00:00,"['Hong Chen', 'Xin Wang', 'Yipeng Zhang', 'Yuwei Zhou', 'Zeyang Zhang', 'Siao Tang', 'Wenwu Zhu']",http://arxiv.org/abs/2405.12796v1
VSD2M: A Large-scale Vision-language Sticker Dataset for Multi-frame Animated Sticker Generation,"As a common form of communication in social media,stickers win users' love in
the internet scenarios, for their ability to convey emotions in a vivid, cute,
and interesting way. People prefer to get an appropriate sticker through
retrieval rather than creation for the reason that creating a sticker is
time-consuming and relies on rule-based creative tools with limited
capabilities. Nowadays, advanced text-to-video algorithms have spawned numerous
general video generation systems that allow users to customize high-quality,
photo-realistic videos by only providing simple text prompts. However, creating
customized animated stickers, which have lower frame rates and more abstract
semantics than videos, is greatly hindered by difficulties in data acquisition
and incomplete benchmarks. To facilitate the exploration of researchers in
animated sticker generation (ASG) field, we firstly construct the currently
largest vision-language sticker dataset named VSD2M at a two-million scale that
contains static and animated stickers. Secondly, to improve the performance of
traditional video generation methods on ASG tasks with discrete
characteristics, we propose a Spatial Temporal Interaction (STI) layer that
utilizes semantic interaction and detail preservation to address the issue of
insufficient information utilization. Moreover, we train baselines with several
video generation methods (e.g., transformer-based, diffusion-based methods) on
VSD2M and conduct a detailed analysis to establish systemic supervision on ASG
task. To the best of our knowledge, this is the most comprehensive large-scale
benchmark for multi-frame animated sticker generation, and we hope this work
can provide valuable inspiration for other scholars in intelligent creation.",2024-12-11 10:11:41+00:00,"['Zhiqiang Yuan', 'Jiapei Zhang', 'Ying Deng', 'Yeshuang Zhu', 'Jie Zhou', 'Jinchao Zhang']",http://arxiv.org/abs/2412.08259v2
ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation,"Text-to-video models have recently undergone rapid and substantial
advancements. Nevertheless, due to limitations in data and computational
resources, achieving efficient generation of long videos with rich motion
dynamics remains a significant challenge. To generate high-quality, dynamic,
and temporally consistent long videos, this paper presents ARLON, a novel
framework that boosts diffusion Transformers with autoregressive models for
long video generation, by integrating the coarse spatial and long-range
temporal information provided by the AR model to guide the DiT model.
Specifically, ARLON incorporates several key innovations: 1) A latent Vector
Quantized Variational Autoencoder (VQ-VAE) compresses the input latent space of
the DiT model into compact visual tokens, bridging the AR and DiT models and
balancing the learning complexity and information density; 2) An adaptive
norm-based semantic injection module integrates the coarse discrete visual
units from the AR model into the DiT model, ensuring effective guidance during
video generation; 3) To enhance the tolerance capability of noise introduced
from the AR inference, the DiT model is trained with coarser visual latent
tokens incorporated with an uncertainty sampling module. Experimental results
demonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on
eight out of eleven metrics selected from VBench, with notable improvements in
dynamic degree and aesthetic quality, while delivering competitive results on
the remaining three and simultaneously accelerating the generation process. In
addition, ARLON achieves state-of-the-art performance in long video generation.
Detailed analyses of the improvements in inference efficiency are presented,
alongside a practical application that demonstrates the generation of long
videos using progressive text prompts. See demos of ARLON at
http://aka.ms/arlon.",2024-10-27 16:28:28+00:00,"['Zongyi Li', 'Shujie Hu', 'Shujie Liu', 'Long Zhou', 'Jeongsoo Choi', 'Lingwei Meng', 'Xun Guo', 'Jinyu Li', 'Hefei Ling', 'Furu Wei']",http://arxiv.org/abs/2410.20502v2
SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer,"Recent advances in 2D/3D generative models enable the generation of dynamic
3D objects from a single-view video. Existing approaches utilize score
distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D
Gaussians. However, these methods struggle to strike a balance among reference
view alignment, spatio-temporal consistency, and motion fidelity under
single-view conditions due to the implicit nature of NeRF or the intricate
dense Gaussian motion prediction. To address these issues, this paper proposes
an efficient, sparse-controlled video-to-4D framework named SC4D, that
decouples motion and appearance to achieve superior video-to-4D generation.
Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian
Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity
of the learned motion and shape. Comprehensive experimental results demonstrate
that our method surpasses existing methods in both quality and efficiency. In
addition, facilitated by the disentangled modeling of motion and appearance of
SC4D, we devise a novel application that seamlessly transfers the learned
motion onto a diverse array of 4D entities according to textual descriptions.",2024-04-04 18:05:18+00:00,"['Zijie Wu', 'Chaohui Yu', 'Yanqin Jiang', 'Chenjie Cao', 'Fan Wang', 'Xiang Bai']",http://arxiv.org/abs/2404.03736v2
Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving,"The field of autonomous driving increasingly demands high-quality annotated
video training data. In this paper, we propose Panacea+, a powerful and
universally applicable framework for generating video data in driving scenes.
Built upon the foundation of our previous work, Panacea, Panacea+ adopts a
multi-view appearance noise prior mechanism and a super-resolution module for
enhanced consistency and increased resolution. Extensive experiments show that
the generated video samples from Panacea+ greatly benefit a wide range of tasks
on different datasets, including 3D object tracking, 3D object detection, and
lane detection tasks on the nuScenes and Argoverse 2 dataset. These results
strongly prove Panacea+ to be a valuable data generation framework for
autonomous driving.",2024-08-14 15:10:13+00:00,"['Yuqing Wen', 'Yucheng Zhao', 'Yingfei Liu', 'Binyuan Huang', 'Fan Jia', 'Yanhui Wang', 'Chi Zhang', 'Tiancai Wang', 'Xiaoyan Sun', 'Xiangyu Zhang']",http://arxiv.org/abs/2408.07605v1
DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control,"Text-conditioned human motion generation, which allows for user interaction
through natural language, has become increasingly popular. Existing methods
typically generate short, isolated motions based on a single input sentence.
However, human motions are continuous and can extend over long periods,
carrying rich semantics. Creating long, complex motions that precisely respond
to streams of text descriptions, particularly in an online and real-time
setting, remains a significant challenge. Furthermore, incorporating spatial
constraints into text-conditioned motion generation presents additional
challenges, as it requires aligning the motion semantics specified by text
descriptions with geometric information, such as goal locations and 3D scene
geometry. To address these limitations, we propose DartControl, in short DART,
a Diffusion-based Autoregressive motion primitive model for Real-time
Text-driven motion control. Our model effectively learns a compact motion
primitive space jointly conditioned on motion history and text inputs using
latent diffusion models. By autoregressively generating motion primitives based
on the preceding history and current text input, DART enables real-time,
sequential motion generation driven by natural language descriptions.
Additionally, the learned motion primitive space allows for precise spatial
motion control, which we formulate either as a latent noise optimization
problem or as a Markov decision process addressed through reinforcement
learning. We present effective algorithms for both approaches, demonstrating
our model's versatility and superior performance in various motion synthesis
tasks. Experiments show our method outperforms existing baselines in motion
realism, efficiency, and controllability. Video results are available on the
project page: https://zkf1997.github.io/DART/.",2024-10-07 17:58:22+00:00,"['Kaifeng Zhao', 'Gen Li', 'Siyu Tang']",http://arxiv.org/abs/2410.05260v2
VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models,"Video generation has witnessed significant advancements, yet evaluating these
models remains a challenge. A comprehensive evaluation benchmark for video
generation is indispensable for two reasons: 1) Existing metrics do not fully
align with human perceptions; 2) An ideal evaluation system should provide
insights to inform future developments of video generation. To this end, we
present VBench, a comprehensive benchmark suite that dissects ""video generation
quality"" into specific, hierarchical, and disentangled dimensions, each with
tailored prompts and evaluation methods. VBench has several appealing
properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in
video generation (e.g., subject identity inconsistency, motion smoothness,
temporal flickering, and spatial relationship, etc). The evaluation metrics
with fine-grained levels reveal individual models' strengths and weaknesses. 2)
Human Alignment: We also provide a dataset of human preference annotations to
validate our benchmarks' alignment with human perception, for each evaluation
dimension respectively. 3) Valuable Insights: We look into current models'
ability across various evaluation dimensions, and various content types. We
also investigate the gaps between video and image generation models. 4)
Versatile Benchmarking: VBench++ supports evaluating text-to-video and
image-to-video. We introduce a high-quality Image Suite with an adaptive aspect
ratio to enable fair evaluations across different image-to-video generation
settings. Beyond assessing technical quality, VBench++ evaluates the
trustworthiness of video generative models, providing a more holistic view of
model performance. 5) Full Open-Sourcing: We fully open-source VBench++ and
continually add new video generation models to our leaderboard to drive forward
the field of video generation.",2024-11-20 17:54:41+00:00,"['Ziqi Huang', 'Fan Zhang', 'Xiaojie Xu', 'Yinan He', 'Jiashuo Yu', 'Ziyue Dong', 'Qianli Ma', 'Nattapol Chanpaisit', 'Chenyang Si', 'Yuming Jiang', 'Yaohui Wang', 'Xinyuan Chen', 'Ying-Cong Chen', 'Limin Wang', 'Dahua Lin', 'Yu Qiao', 'Ziwei Liu']",http://arxiv.org/abs/2411.13503v1
Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian Splatting for Dynamic Surgical Videos,"Reconstructing endoscopic videos is crucial for high-fidelity visualization
and the efficiency of surgical operations. Despite the importance, existing 3D
reconstruction methods encounter several challenges, including stringent
demands for accuracy, imprecise camera positioning, intricate dynamic scenes,
and the necessity for rapid reconstruction. Addressing these issues, this paper
presents the first camera-pose-free scene reconstruction framework, Free-DyGS,
tailored for dynamic surgical videos, leveraging 3D Gaussian splatting
technology. Our approach employs a frame-by-frame reconstruction strategy and
is delineated into four distinct phases: Scene Initialization, Joint Learning,
Scene Expansion, and Retrospective Learning. We introduce a Generalizable
Gaussians Parameterization module within the Scene Initialization and Expansion
phases to proficiently generate Gaussian attributes for each pixel from the
RGBD frames. The Joint Learning phase is crafted to concurrently deduce scene
deformation and camera pose, facilitated by an innovative flexible deformation
module. In the scene expansion stage, the Gaussian points gradually grow as the
camera moves. The Retrospective Learning phase is dedicated to enhancing the
precision of scene deformation through the reassessment of prior frames. The
efficacy of the proposed Free-DyGS is substantiated through experiments on two
datasets: the StereoMIS and Hamlyn datasets. The experimental outcomes
underscore that Free-DyGS surpasses conventional baseline models in both
rendering fidelity and computational efficiency.",2024-09-02 07:28:14+00:00,"['Qian Li', 'Shuojue Yang', 'Daiyun Shen', 'Yueming Jin']",http://arxiv.org/abs/2409.01003v2
Movie Gen: A Cast of Media Foundation Models,"We present Movie Gen, a cast of foundation models that generates
high-quality, 1080p HD videos with different aspect ratios and synchronized
audio. We also show additional capabilities such as precise instruction-based
video editing and generation of personalized videos based on a user's image.
Our models set a new state-of-the-art on multiple tasks: text-to-video
synthesis, video personalization, video editing, video-to-audio generation, and
text-to-audio generation. Our largest video generation model is a 30B parameter
transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We
show multiple technical innovations and simplifications on the architecture,
latent spaces, training objectives and recipes, data curation, evaluation
protocols, parallelization techniques, and inference optimizations that allow
us to reap the benefits of scaling pre-training data, model size, and training
compute for training large scale media generation models. We hope this paper
helps the research community to accelerate progress and innovation in media
generation models. All videos from this paper are available at
https://go.fb.me/MovieGenResearchVideos.",2024-10-17 16:22:46+00:00,"['Adam Polyak', 'Amit Zohar', 'Andrew Brown', 'Andros Tjandra', 'Animesh Sinha', 'Ann Lee', 'Apoorv Vyas', 'Bowen Shi', 'Chih-Yao Ma', 'Ching-Yao Chuang', 'David Yan', 'Dhruv Choudhary', 'Dingkang Wang', 'Geet Sethi', 'Guan Pang', 'Haoyu Ma', 'Ishan Misra', 'Ji Hou', 'Jialiang Wang', 'Kiran Jagadeesh', 'Kunpeng Li', 'Luxin Zhang', 'Mannat Singh', 'Mary Williamson', 'Matt Le', 'Matthew Yu', 'Mitesh Kumar Singh', 'Peizhao Zhang', 'Peter Vajda', 'Quentin Duval', 'Rohit Girdhar', 'Roshan Sumbaly', 'Sai Saketh Rambhatla', 'Sam Tsai', 'Samaneh Azadi', 'Samyak Datta', 'Sanyuan Chen', 'Sean Bell', 'Sharadh Ramaswamy', 'Shelly Sheynin', 'Siddharth Bhattacharya', 'Simran Motwani', 'Tao Xu', 'Tianhe Li', 'Tingbo Hou', 'Wei-Ning Hsu', 'Xi Yin', 'Xiaoliang Dai', 'Yaniv Taigman', 'Yaqiao Luo', 'Yen-Cheng Liu', 'Yi-Chiao Wu', 'Yue Zhao', 'Yuval Kirstain', 'Zecheng He', 'Zijian He', 'Albert Pumarola', 'Ali Thabet', 'Artsiom Sanakoyeu', 'Arun Mallya', 'Baishan Guo', 'Boris Araya', 'Breena Kerr', 'Carleigh Wood', 'Ce Liu', 'Cen Peng', 'Dimitry Vengertsev', 'Edgar Schonfeld', 'Elliot Blanchard', 'Felix Juefei-Xu', 'Fraylie Nord', 'Jeff Liang', 'John Hoffman', 'Jonas Kohler', 'Kaolin Fire', 'Karthik Sivakumar', 'Lawrence Chen', 'Licheng Yu', 'Luya Gao', 'Markos Georgopoulos', 'Rashel Moritz', 'Sara K. Sampson', 'Shikai Li', 'Simone Parmeggiani', 'Steve Fine', 'Tara Fowler', 'Vladan Petrovic', 'Yuming Du']",http://arxiv.org/abs/2410.13720v2
4Diffusion: Multi-view Video Diffusion Model for 4D Generation,"Current 4D generation methods have achieved noteworthy efficacy with the aid
of advanced diffusion generative models. However, these methods lack multi-view
spatial-temporal modeling and encounter challenges in integrating diverse prior
knowledge from multiple diffusion models, resulting in inconsistent temporal
appearance and flickers. In this paper, we propose a novel 4D generation
pipeline, namely 4Diffusion, aimed at generating spatial-temporally consistent
4D content from a monocular video. We first design a unified diffusion model
tailored for multi-view video generation by incorporating a learnable motion
module into a frozen 3D-aware diffusion model to capture multi-view
spatial-temporal correlations. After training on a curated dataset, our
diffusion model acquires reasonable temporal consistency and inherently
preserves the generalizability and spatial consistency of the 3D-aware
diffusion model. Subsequently, we propose 4D-aware Score Distillation Sampling
loss, which is based on our multi-view video diffusion model, to optimize 4D
representation parameterized by dynamic NeRF. This aims to eliminate
discrepancies arising from multiple diffusion models, allowing for generating
spatial-temporally consistent 4D content. Moreover, we devise an anchor loss to
enhance the appearance details and facilitate the learning of dynamic NeRF.
Extensive qualitative and quantitative experiments demonstrate that our method
achieves superior performance compared to previous methods.",2024-05-31 08:18:39+00:00,"['Haiyu Zhang', 'Xinyuan Chen', 'Yaohui Wang', 'Xihui Liu', 'Yunhong Wang', 'Yu Qiao']",http://arxiv.org/abs/2405.20674v2
MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions,"Sora's high-motion intensity and long consistent videos have significantly
impacted the field of video generation, attracting unprecedented attention.
However, existing publicly available datasets are inadequate for generating
Sora-like videos, as they mainly contain short videos with low motion intensity
and brief captions. To address these issues, we propose MiraData, a
high-quality video dataset that surpasses previous ones in video duration,
caption detail, motion strength, and visual quality. We curate MiraData from
diverse, manually selected sources and meticulously process the data to obtain
semantically consistent clips. GPT-4V is employed to annotate structured
captions, providing detailed descriptions from four different perspectives
along with a summarized dense caption. To better assess temporal consistency
and motion intensity in video generation, we introduce MiraBench, which
enhances existing benchmarks by adding 3D consistency and tracking-based motion
strength metrics. MiraBench includes 150 evaluation prompts and 17 metrics
covering temporal consistency, motion strength, 3D consistency, visual quality,
text-video alignment, and distribution similarity. To demonstrate the utility
and effectiveness of MiraData, we conduct experiments using our DiT-based video
generation model, MiraDiT. The experimental results on MiraBench demonstrate
the superiority of MiraData, especially in motion strength.",2024-07-08 19:58:59+00:00,"['Xuan Ju', 'Yiming Gao', 'Zhaoyang Zhang', 'Ziyang Yuan', 'Xintao Wang', 'Ailing Zeng', 'Yu Xiong', 'Qiang Xu', 'Ying Shan']",http://arxiv.org/abs/2407.06358v1
T2VIndexer: A Generative Video Indexer for Efficient Text-Video Retrieval,"Current text-video retrieval methods mainly rely on cross-modal matching
between queries and videos to calculate their similarity scores, which are then
sorted to obtain retrieval results. This method considers the matching between
each candidate video and the query, but it incurs a significant time cost and
will increase notably with the increase of candidates. Generative models are
common in natural language processing and computer vision, and have been
successfully applied in document retrieval, but their application in multimodal
retrieval remains unexplored. To enhance retrieval efficiency, in this paper,
we introduce a model-based video indexer named T2VIndexer, which is a
sequence-to-sequence generative model directly generating video identifiers and
retrieving candidate videos with constant time complexity. T2VIndexer aims to
reduce retrieval time while maintaining high accuracy. To achieve this goal, we
propose video identifier encoding and query-identifier augmentation approaches
to represent videos as short sequences while preserving their semantic
information. Our method consistently enhances the retrieval efficiency of
current state-of-the-art models on four standard datasets. It enables baselines
with only 30\%-50\% of the original retrieval time to achieve better retrieval
performance on MSR-VTT (+1.0%), MSVD (+1.8%), ActivityNet (+1.5%), and DiDeMo
(+0.2%). The code is available at
https://github.com/Lilidamowang/T2VIndexer-generativeSearch.",2024-08-21 08:40:45+00:00,"['Yili Li', 'Jing Yu', 'Keke Gai', 'Bang Liu', 'Gang Xiong', 'Qi Wu']",http://arxiv.org/abs/2408.11432v1
TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation,"Video generation models are revolutionizing content creation, with
image-to-video models drawing increasing attention due to their enhanced
controllability, visual consistency, and practical applications. However,
despite their popularity, these models rely on user-provided text and image
prompts, and there is currently no dedicated dataset for studying these
prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of
over 1.70 million unique user-provided Text and Image Prompts specifically for
Image-to-Video generation. Additionally, we provide the corresponding generated
videos from five state-of-the-art image-to-video models. We begin by outlining
the time-consuming and costly process of curating this large-scale dataset.
Next, we compare TIP-I2V to two popular prompt datasets, VidProM
(text-to-video) and DiffusionDB (text-to-image), highlighting differences in
both basic and semantic information. This dataset enables advancements in
image-to-video research. For instance, to develop better models, researchers
can use the prompts in TIP-I2V to analyze user preferences and evaluate the
multi-dimensional performance of their trained models; and to enhance model
safety, they may focus on addressing the misinformation issue caused by
image-to-video models. The new research inspired by TIP-I2V and the differences
with existing datasets emphasize the importance of a specialized image-to-video
prompt dataset. The project is publicly available at https://tip-i2v.github.io.",2024-11-05 18:52:43+00:00,"['Wenhao Wang', 'Yi Yang']",http://arxiv.org/abs/2411.04709v1
Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing,"With the advance of diffusion models, today's video generation has achieved
impressive quality. To extend the generation length and facilitate real-world
applications, a majority of video diffusion models (VDMs) generate videos in an
autoregressive manner, i.e., generating subsequent clips conditioned on the
last frame(s) of the previous clip. However, existing autoregressive VDMs are
highly inefficient and redundant: The model must re-compute all the conditional
frames that are overlapped between adjacent clips. This issue is exacerbated
when the conditional frames are extended autoregressively to provide the model
with long-term context. In such cases, the computational demands increase
significantly (i.e., with a quadratic complexity w.r.t. the autoregression
step). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with
Causal generation and Cache sharing. For causal generation, it introduces
unidirectional feature computation, which ensures that the cache of conditional
frames can be precomputed in previous autoregression steps and reused in every
subsequent step, eliminating redundant computations. For cache sharing, it
shares the cache across all denoising steps to avoid the huge cache storage
cost. Extensive experiments demonstrated that our Ca2-VDM achieves
state-of-the-art quantitative and qualitative video generation results and
significantly improves the generation speed. Code is available at
https://github.com/Dawn-LX/CausalCache-VDM",2024-11-25 13:33:41+00:00,"['Kaifeng Gao', 'Jiaxin Shi', 'Hanwang Zhang', 'Chunping Wang', 'Jun Xiao', 'Long Chen']",http://arxiv.org/abs/2411.16375v1
Harnessing Large Language Models for Training-free Video Anomaly Detection,"Video anomaly detection (VAD) aims to temporally locate abnormal events in a
video. Existing works mostly rely on training deep models to learn the
distribution of normality with either video-level supervision, one-class
supervision, or in an unsupervised setting. Training-based methods are prone to
be domain-specific, thus being costly for practical deployment as any domain
change will involve data collection and model training. In this paper, we
radically depart from previous efforts and propose LAnguage-based VAD (LAVAD),
a method tackling VAD in a novel, training-free paradigm, exploiting the
capabilities of pre-trained large language models (LLMs) and existing
vision-language models (VLMs). We leverage VLM-based captioning models to
generate textual descriptions for each frame of any test video. With the
textual scene description, we then devise a prompting mechanism to unlock the
capability of LLMs in terms of temporal aggregation and anomaly score
estimation, turning LLMs into an effective video anomaly detector. We further
leverage modality-aligned VLMs and propose effective techniques based on
cross-modal similarity for cleaning noisy captions and refining the LLM-based
anomaly scores. We evaluate LAVAD on two large datasets featuring real-world
surveillance scenarios (UCF-Crime and XD-Violence), showing that it outperforms
both unsupervised and one-class methods without requiring any training or data
collection.",2024-04-01 09:34:55+00:00,"['Luca Zanella', 'Willi Menapace', 'Massimiliano Mancini', 'Yiming Wang', 'Elisa Ricci']",http://arxiv.org/abs/2404.01014v1
CinePile: A Long Video Question Answering Dataset and Benchmark,"Current datasets for long-form video understanding often fall short of
providing genuine long-form comprehension challenges, as many tasks derived
from these datasets can be successfully tackled by analyzing just one or a few
random frames from a video. To address this issue, we present a novel dataset
and benchmark, CinePile, specifically designed for authentic long-form video
understanding. This paper details our innovative approach for creating a
question-answer dataset, utilizing advanced LLMs with human-in-the-loop and
building upon human-generated raw data. Our comprehensive dataset comprises
305,000 multiple-choice questions (MCQs), covering various visual and
multimodal aspects, including temporal comprehension, understanding
human-object interactions, and reasoning about events or actions within a
scene. Additionally, we fine-tuned open-source Video-LLMs on the training split
and evaluated both open-source and proprietary video-centric LLMs on the test
split of our dataset. The findings indicate that although current models
underperform compared to humans, fine-tuning these models can lead to
significant improvements in their performance.",2024-05-14 17:59:02+00:00,"['Ruchit Rawal', 'Khalid Saifullah', 'Miquel Farr', 'Ronen Basri', 'David Jacobs', 'Gowthami Somepalli', 'Tom Goldstein']",http://arxiv.org/abs/2405.08813v3
Learning Semantic Traversability with Egocentric Video and Automated Annotation Strategy,"For reliable autonomous robot navigation in urban settings, the robot must
have the ability to identify semantically traversable terrains in the image
based on the semantic understanding of the scene. This reasoning ability is
based on semantic traversability, which is frequently achieved using semantic
segmentation models fine-tuned on the testing domain. This fine-tuning process
often involves manual data collection with the target robot and annotation by
human labelers which is prohibitively expensive and unscalable. In this work,
we present an effective methodology for training a semantic traversability
estimator using egocentric videos and an automated annotation process.
Egocentric videos are collected from a camera mounted on a pedestrian's chest.
The dataset for training the semantic traversability estimator is then
automatically generated by extracting semantically traversable regions in each
video frame using a recent foundation model in image segmentation and its
prompting technique. Extensive experiments with videos taken across several
countries and cities, covering diverse urban scenarios, demonstrate the high
scalability and generalizability of the proposed annotation method.
Furthermore, performance analysis and real-world deployment for autonomous
robot navigation showcase that the trained semantic traversability estimator is
highly accurate, able to handle diverse camera viewpoints, computationally
light, and real-world applicable. The summary video is available at
https://youtu.be/EUVoH-wA-lA.",2024-06-05 06:40:04+00:00,"['Yunho Kim', 'Jeong Hyun Lee', 'Choongin Lee', 'Juhyeok Mun', 'Donghoon Youm', 'Jeongsoo Park', 'Jemin Hwangbo']",http://arxiv.org/abs/2406.02989v2
OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding,"Surgical scene perception via videos is critical for advancing robotic
surgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.
However, the scarcity of diverse and richly annotated video datasets has
hindered the development of intelligent systems for surgical workflow analysis.
Existing datasets face challenges such as small scale, lack of diversity in
surgery and phase categories, and absence of time-localized annotations. These
limitations impede action understanding and model generalization validation in
complex and diverse real-world surgical scenarios. To address this gap, we
introduce OphNet, a large-scale, expert-annotated video benchmark for
ophthalmic surgical workflow understanding. OphNet features: 1) A diverse
collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma,
and corneal surgeries, with detailed annotations for 102 unique surgical phases
and 150 fine-grained operations. 2) Sequential and hierarchical annotations for
each surgery, phase, and operation, enabling comprehensive understanding and
improved interpretability. 3) Time-localized annotations, facilitating temporal
localization and prediction tasks within surgical workflows. With approximately
285 hours of surgical videos, OphNet is about 20 times larger than the largest
existing surgical workflow analysis benchmark. Code and dataset are available
at: https://minghu0830.github.io/OphNet-benchmark/.",2024-06-11 17:18:11+00:00,"['Ming Hu', 'Peng Xia', 'Lin Wang', 'Siyuan Yan', 'Feilong Tang', 'Zhongxing Xu', 'Yimin Luo', 'Kaimin Song', 'Jurgen Leitner', 'Xuelian Cheng', 'Jun Cheng', 'Chi Liu', 'Kaijing Zhou', 'Zongyuan Ge']",http://arxiv.org/abs/2406.07471v4
FMI-TAL: Few-shot Multiple Instances Temporal Action Localization by Probability Distribution Learning and Interval Cluster Refinement,"The present few-shot temporal action localization model can't handle the
situation where videos contain multiple action instances. So the purpose of
this paper is to achieve manifold action instances localization in a lengthy
untrimmed query video using limited trimmed support videos. To address this
challenging problem effectively, we proposed a novel solution involving a
spatial-channel relation transformer with probability learning and cluster
refinement. This method can accurately identify the start and end boundaries of
actions in the query video, utilizing only a limited number of labeled videos.
Our proposed method is adept at capturing both temporal and spatial contexts to
effectively classify and precisely locate actions in videos, enabling a more
comprehensive utilization of these crucial details. The selective cosine
penalization algorithm is designed to suppress temporal boundaries that do not
include action scene switches. The probability learning combined with the label
generation algorithm alleviates the problem of action duration diversity and
enhances the model's ability to handle fuzzy action boundaries. The interval
cluster can help us get the final results with multiple instances situations in
few-shot temporal action localization. Our model achieves competitive
performance through meticulous experimentation utilizing the benchmark datasets
ActivityNet1.3 and THUMOS14. Our code is readily available at
https://github.com/ycwfs/FMI-TAL.",2024-08-25 08:17:25+00:00,"['Fengshun Wang', 'Qiurui Wang', 'Yuting Wang']",http://arxiv.org/abs/2408.13765v1
Authentication and integrity of smartphone videos through multimedia container structure analysis,"Nowadays, mobile devices have become the natural substitute for the digital
camera, as they capture everyday situations easily and quickly, encouraging
users to express themselves through images and videos. These videos can be
shared across different platforms exposing them to any kind of intentional
manipulation by criminals who are aware of the weaknesses of forensic
techniques to accuse an innocent person or exonerate a guilty person in a
judicial process. Commonly, manufacturers do not comply 100% with the
specifications of the standards for the creation of videos. Also, videos shared
on social networks, and instant messaging applications go through filtering and
compression processes to reduce their size, facilitate their transfer, and
optimize storage on their platforms. The omission of specifications and results
of transformations carried out by the platforms embed a features pattern in the
multimedia container of the videos. These patterns make it possible to
distinguish the brand of the device that generated the video, social network,
and instant messaging application that was used for the transfer. Research in
recent years has focused on the analysis of AVI containers and tiny video
datasets. This work presents a novel technique to detect possible attacks
against MP4, MOV, and 3GP format videos that affect their integrity and
authenticity. The method is based on the analysis of the structure of video
containers generated by mobile devices and their behavior when shared through
social networks, instant messaging applications, or manipulated by editing
programs. The objectives of the proposal are to verify the integrity of videos,
identify the source of acquisition and distinguish between original and
manipulated videos.",2024-02-05 22:34:24+00:00,"['Carlos Quinto Huamn', 'Ana Lucila Sandoval Orozco', 'Luis Javier Garca Villalba']",http://arxiv.org/abs/2402.06661v1
Conditional Brownian Bridge Diffusion Model for VHR SAR to Optical Image Translation,"Synthetic Aperture Radar (SAR) imaging technology provides the unique
advantage of being able to collect data regardless of weather conditions and
time. However, SAR images exhibit complex backscatter patterns and speckle
noise, which necessitate expertise for interpretation. Research on translating
SAR images into optical-like representations has been conducted to aid the
interpretation of SAR data. Nevertheless, existing studies have predominantly
utilized low-resolution satellite imagery datasets and have largely been based
on Generative Adversarial Network (GAN) which are known for their training
instability and low fidelity. To overcome these limitations of low-resolution
data usage and GAN-based approaches, this paper introduces a conditional
image-to-image translation approach based on Brownian Bridge Diffusion Model
(BBDM). We conducted comprehensive experiments on the MSAW dataset, a paired
SAR and optical images collection of 0.5m Very-High-Resolution (VHR). The
experimental results indicate that our method surpasses both the Conditional
Diffusion Models (CDMs) and the GAN-based models in diverse perceptual quality
metrics.",2024-08-15 05:43:46+00:00,"['Seon-Hoon Kim', 'Dae-Won Chung']",http://arxiv.org/abs/2408.07947v3
CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers,"We extend multimodal transformers to include 3D camera motion as a
conditioning signal for the task of video generation. Generative video models
are becoming increasingly powerful, thus focusing research efforts on methods
of controlling the output of such models. We propose to add virtual 3D camera
controls to generative video methods by conditioning generated video on an
encoding of three-dimensional camera movement over the course of the generated
video. Results demonstrate that we are (1) able to successfully control the
camera during video generation, starting from a single frame and a camera
signal, and (2) we demonstrate the accuracy of the generated 3D camera paths
using traditional computer vision methods.",2024-05-21 20:54:27+00:00,"['Andrew Marmon', 'Grant Schindler', 'Jos Lezama', 'Dan Kondratyuk', 'Bryan Seybold', 'Irfan Essa']",http://arxiv.org/abs/2405.13195v1
VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation,"Recent advances in audio generation have focused on text-to-audio (T2A) and
video-to-audio (V2A) tasks. However, T2A or V2A methods cannot generate
holistic sounds (onscreen and off-screen). This is because T2A cannot generate
sounds aligning with onscreen objects, while V2A cannot generate semantically
complete (offscreen sounds missing). In this work, we address the task of
holistic audio generation: given a video and a text prompt, we aim to generate
both onscreen and offscreen sounds that are temporally synchronized with the
video and semantically aligned with text and video. Previous approaches for
joint text and video-to-audio generation often suffer from modality bias,
favoring one modality over the other. To overcome this limitation, we introduce
VinTAGe, a flow-based transformer model that jointly considers text and video
to guide audio generation. Our framework comprises two key components: a
Visual-Text Encoder and a Joint VT-SiT model. To reduce modality bias and
improve generation quality, we employ pretrained uni-modal text-to-audio and
video-to-audio generation models for additional guidance. Due to the lack of
appropriate benchmarks, we also introduce VinTAGe-Bench, a dataset of 636
video-text-audio pairs containing both onscreen and offscreen sounds. Our
comprehensive experiments on VinTAGe-Bench demonstrate that joint text and
visual interaction is necessary for holistic audio generation. Furthermore,
VinTAGe achieves state-of-the-art results on the VGGSound benchmark. Our source
code and pre-trained models will be released. Demo is available at:
https://www.youtube.com/watch?v=QmqWhUjPkJI.",2024-12-14 09:36:10+00:00,"['Saksham Singh Kushwaha', 'Yapeng Tian']",http://arxiv.org/abs/2412.10768v1
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation,"Robot learning of manipulation skills is hindered by the scarcity of diverse,
unbiased datasets. While curated datasets can help, challenges remain in
generalizability and real-world transfer. Meanwhile, large-scale ""in-the-wild""
video datasets have driven progress in computer vision through self-supervised
techniques. Translating this to robotics, recent works have explored learning
manipulation skills by passively watching abundant videos sourced online.
Showing promising results, such video-based learning paradigms provide scalable
supervision while reducing dataset bias. This survey reviews foundations such
as video feature representation learning techniques, object affordance
understanding, 3D hand/body modeling, and large-scale robot resources, as well
as emerging techniques for acquiring robot manipulation skills from
uncontrolled video demonstrations. We discuss how learning only from observing
large-scale human videos can enhance generalization and sample efficiency for
robotic manipulation. The survey summarizes video-based learning approaches,
analyses their benefits over standard datasets, survey metrics, and benchmarks,
and discusses open challenges and future directions in this nascent domain at
the intersection of computer vision, natural language processing, and robot
learning.",2024-02-11 08:41:42+00:00,"['Chrisantus Eze', 'Christopher Crick']",http://arxiv.org/abs/2402.07127v2
Place Anything into Any Video,"Controllable video editing has demonstrated remarkable potential across
diverse applications, particularly in scenarios where capturing or re-capturing
real-world videos is either impractical or costly. This paper introduces a
novel and efficient system named Place-Anything, which facilitates the
insertion of any object into any video solely based on a picture or text
description of the target object or element. The system comprises three
modules: 3D generation, video reconstruction, and 3D target insertion. This
integrated approach offers an efficient and effective solution for producing
and editing high-quality videos by seamlessly inserting realistic objects.
Through a user study, we demonstrate that our system can effortlessly place any
object into any video using just a photograph of the object. Our demo video can
be found at https://youtu.be/afXqgLLRnTE. Please also visit our project page
https://place-anything.github.io to get access.",2024-02-22 06:19:22+00:00,"['Ziling Liu', 'Jinyu Yang', 'Mingqi Gao', 'Feng Zheng']",http://arxiv.org/abs/2402.14316v1
Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction,"We propose a deep learning based novel prediction framework for enhanced
bandwidth reduction in motion transfer enabled video applications such as video
conferencing, virtual reality gaming and privacy preservation for patient
health monitoring. To model complex motion, we use the First Order Motion Model
(FOMM) that represents dynamic objects using learned keypoints along with their
local affine transformations. Keypoints are extracted by a self-supervised
keypoint detector and organized in a time series corresponding to the video
frames. Prediction of keypoints, to enable transmission using lower frames per
second on the source device, is performed using a Variational Recurrent Neural
Network (VRNN). The predicted keypoints are then synthesized to video frames
using an optical flow estimator and a generator network. This efficacy of
leveraging keypoint based representations in conjunction with VRNN based
prediction for both video animation and reconstruction is demonstrated on three
diverse datasets. For real-time applications, our results show the
effectiveness of our proposed architecture by enabling up to 2x additional
bandwidth reduction over existing keypoint based video motion transfer
frameworks without significantly compromising video quality.",2024-03-17 20:36:43+00:00,"['Xue Bai', 'Tasmiah Haque', 'Sumit Mohan', 'Yuliang Cai', 'Byungheon Jeong', 'Adam Halasz', 'Srinjoy Das']",http://arxiv.org/abs/2403.11337v1
Enhancing Traffic Safety with Parallel Dense Video Captioning for End-to-End Event Analysis,"This paper introduces our solution for Track 2 in AI City Challenge 2024. The
task aims to solve traffic safety description and analysis with the dataset of
Woven Traffic Safety (WTS), a real-world Pedestrian-Centric Traffic Video
Dataset for Fine-grained Spatial-Temporal Understanding. Our solution mainly
focuses on the following points: 1) To solve dense video captioning, we
leverage the framework of dense video captioning with parallel decoding (PDVC)
to model visual-language sequences and generate dense caption by chapters for
video. 2) Our work leverages CLIP to extract visual features to more
efficiently perform cross-modality training between visual and textual
representations. 3) We conduct domain-specific model adaptation to mitigate
domain shift problem that poses recognition challenge in video understanding.
4) Moreover, we leverage BDD-5K captioned videos to conduct knowledge transfer
for better understanding WTS videos and more accurate captioning. Our solution
has yielded on the test set, achieving 6th place in the competition. The open
source code will be available at https://github.com/UCF-SST-Lab/AICity2024CVPRW",2024-04-12 04:08:21+00:00,"['Maged Shoman', 'Dongdong Wang', 'Armstrong Aboah', 'Mohamed Abdel-Aty']",http://arxiv.org/abs/2404.08229v1
SIAVC: Semi-Supervised Framework for Industrial Accident Video Classification,"Semi-supervised learning suffers from the imbalance of labeled and unlabeled
training data in the video surveillance scenario. In this paper, we propose a
new semi-supervised learning method called SIAVC for industrial accident video
classification. Specifically, we design a video augmentation module called the
Super Augmentation Block (SAB). SAB adds Gaussian noise and randomly masks
video frames according to historical loss on the unlabeled data for model
optimization. Then, we propose a Video Cross-set Augmentation Module (VCAM) to
generate diverse pseudo-label samples from the high-confidence unlabeled
samples, which alleviates the mismatch of sampling experience and provides
high-quality training data. Additionally, we construct a new industrial
accident surveillance video dataset with frame-level annotation, namely ECA9,
to evaluate our proposed method. Compared with the state-of-the-art
semi-supervised learning based methods, SIAVC demonstrates outstanding video
classification performance, achieving 88.76\% and 89.13\% accuracy on ECA9 and
Fire Detection datasets, respectively. The source code and the constructed
dataset ECA9 will be released in \url{https://github.com/AlchemyEmperor/SIAVC}.",2024-05-23 12:44:51+00:00,"['Zuoyong Li', 'Qinghua Lin', 'Haoyi Fan', 'Tiesong Zhao', 'David Zhang']",http://arxiv.org/abs/2405.14506v1
Vision-based Manipulation from Single Human Video with Open-World Object Graphs,"We present an object-centric approach to empower robots to learn vision-based
manipulation skills from human videos. We investigate the problem of imitating
robot manipulation from a single human video in the open-world setting, where a
robot must learn to manipulate novel objects from one video demonstration. We
introduce ORION, an algorithm that tackles the problem by extracting an
object-centric manipulation plan from a single RGB-D video and deriving a
policy that conditions on the extracted plan. Our method enables the robot to
learn from videos captured by daily mobile devices such as an iPad and
generalize the policies to deployment environments with varying visual
backgrounds, camera angles, spatial layouts, and novel object instances. We
systematically evaluate our method on both short-horizon and long-horizon
tasks, demonstrating the efficacy of ORION in learning from a single human
video in the open world. Videos can be found in the project website
https://ut-austin-rpl.github.io/ORION-release.",2024-05-30 17:56:54+00:00,"['Yifeng Zhu', 'Arisrei Lim', 'Peter Stone', 'Yuke Zhu']",http://arxiv.org/abs/2405.20321v1
Live Video Captioning,"Dense video captioning is the task that involves the detection and
description of events within video sequences. While traditional approaches
focus on offline solutions where the entire video of analysis is available for
the captioning model, in this work we introduce a paradigm shift towards Live
Video Captioning (LVC). In LVC, dense video captioning models must generate
captions for video streams in an online manner, facing important constraints
such as having to work with partial observations of the video, the need for
temporal anticipation and, of course, ensuring ideally a real-time response. In
this work we formally introduce the novel problem of LVC and propose new
evaluation metrics tailored for the online scenario, demonstrating their
superiority over traditional metrics. We also propose an LVC model integrating
deformable transformers and temporal filtering to address the LVC new
challenges. Experimental evaluations on the ActivityNet Captions dataset
validate the effectiveness of our approach, highlighting its performance in LVC
compared to state-of-the-art offline methods. Results of our model as well as
an evaluation kit with the novel metrics integrated are made publicly available
to encourage further research on LVC.",2024-06-20 11:25:16+00:00,"['Eduardo Blanco-Fernndez', 'Carlos Gutirrez-lvarez', 'Nadia Nasri', 'Saturnino Maldonado-Bascn', 'Roberto J. Lpez-Sastre']",http://arxiv.org/abs/2406.14206v1
Video In-context Learning: Autoregressive Transformers are Zero-Shot Video Imitators,"People interact with the real-world largely dependent on visual signal, which
are ubiquitous and illustrate detailed demonstrations. In this paper, we
explore utilizing visual signals as a new interface for models to interact with
the environment. Specifically, we choose videos as a representative visual
signal. And by training autoregressive Transformers on video datasets in a
self-supervised objective, we find that the model emerges a zero-shot
capability to infer the semantics from a demonstration video, and imitate the
semantics to an unseen scenario. This allows the models to perform unseen tasks
by watching the demonstration video in an in-context manner, without further
fine-tuning. To validate the imitation capacity, we design various evaluation
metrics including both objective and subjective measures. The results show that
our models can generate high-quality video clips that accurately align with the
semantic guidance provided by the demonstration videos, and we also show that
the imitation capacity follows the scaling law. Code and models have been
open-sourced.",2024-07-10 04:27:06+00:00,"['Wentao Zhang', 'Junliang Guo', 'Tianyu He', 'Li Zhao', 'Linli Xu', 'Jiang Bian']",http://arxiv.org/abs/2407.07356v2
SLVideo: A Sign Language Video Moment Retrieval Framework,"SLVideo is a video moment retrieval system for Sign Language videos that
incorporates facial expressions, addressing this gap in existing technology.
The system extracts embedding representations for the hand and face signs from
video frames to capture the signs in their entirety, enabling users to search
for a specific sign language video segment with text queries. A collection of
eight hours of annotated Portuguese Sign Language videos is used as the
dataset, and a CLIP model is used to generate the embeddings. The initial
results are promising in a zero-shot setting. In addition, SLVideo incorporates
a thesaurus that enables users to search for similar signs to those retrieved,
using the video segment embeddings, and also supports the edition and creation
of video sign language annotations. Project web page:
https://novasearch.github.io/SLVideo/",2024-07-22 14:29:36+00:00,"['Gonalo Vinagre Martins', 'Joo Magalhes', 'Afonso Quinaz', 'Carla Viegas', 'Sofia Cavaco']",http://arxiv.org/abs/2407.15668v2
AIM 2024 Challenge on Efficient Video Super-Resolution for AV1 Compressed Content,"Video super-resolution (VSR) is a critical task for enhancing low-bitrate and
low-resolution videos, particularly in streaming applications. While numerous
solutions have been developed, they often suffer from high computational
demands, resulting in low frame rates (FPS) and poor power efficiency,
especially on mobile platforms. In this work, we compile different methods to
address these challenges, the solutions are end-to-end real-time video
super-resolution frameworks optimized for both high performance and low
runtime. We also introduce a new test set of high-quality 4K videos to further
validate the approaches. The proposed solutions tackle video up-scaling for two
applications: 540p to 4K (x4) as a general case, and 360p to 1080p (x3) more
tailored towards mobile devices. In both tracks, the solutions have a reduced
number of parameters and operations (MACs), allow high FPS, and improve VMAF
and PSNR over interpolation baselines. This report gauges some of the most
efficient video super-resolution methods to date.",2024-09-25 18:12:19+00:00,"['Marcos V Conde', 'Zhijun Lei', 'Wen Li', 'Christos Bampis', 'Ioannis Katsavounidis', 'Radu Timofte']",http://arxiv.org/abs/2409.17256v1
Neural Video Representation for Redundancy Reduction and Consistency Preservation,"Implicit neural representation (INR) embed various signals into neural
networks. They have gained attention in recent years because of their
versatility in handling diverse signal types. In the context of video, INR
achieves video compression by embedding video signals directly into networks
and compressing them. Conventional methods either use an index that expresses
the time of the frame or features extracted from individual frames as network
inputs. The latter method provides greater expressive capability as the input
is specific to each video. However, the features extracted from frames often
contain redundancy, which contradicts the purpose of video compression.
Additionally, such redundancies make it challenging to accurately reconstruct
high-frequency components in the frames. To address these problems, we focus on
separating the high-frequency and low-frequency components of the reconstructed
frame. We propose a video representation method that generates both the
high-frequency and low-frequency components of the frame, using features
extracted from the high-frequency components and temporal information,
respectively. Experimental results demonstrate that our method outperforms the
existing HNeRV method, achieving superior results in 96 percent of the videos.",2024-09-27 07:30:12+00:00,"['Taiga Hayami', 'Takahiro Shindo', 'Shunsuke Akamatsu', 'Hiroshi Watanabe']",http://arxiv.org/abs/2409.18497v2
NCST: Neural-based Color Style Transfer for Video Retouching,"Video color style transfer aims to transform the color style of an original
video by using a reference style image. Most existing methods employ neural
networks, which come with challenges like opaque transfer processes and limited
user control over the outcomes. Typically, users cannot fine-tune the resulting
images or videos. To tackle this issue, we introduce a method that predicts
specific parameters for color style transfer using two images. Initially, we
train a neural network to learn the corresponding color adjustment parameters.
When applying style transfer to a video, we fine-tune the network with key
frames from the video and the chosen style image, generating precise
transformation parameters. These are then applied to convert the color style of
both images and videos. Our experimental results demonstrate that our algorithm
surpasses current methods in color style transfer quality. Moreover, each
parameter in our method has a specific, interpretable meaning, enabling users
to understand the color style transfer process and allowing them to perform
manual fine-tuning if desired.",2024-11-01 03:25:15+00:00,"['Xintao Jiang', 'Yaosen Chen', 'Siqin Zhang', 'Wei Wang', 'Xuming Wen']",http://arxiv.org/abs/2411.00335v1
IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos,"Shape assembly is a ubiquitous task in daily life, integral for constructing
complex 3D structures like IKEA furniture. While significant progress has been
made in developing autonomous agents for shape assembly, existing datasets have
not yet tackled the 4D grounding of assembly instructions in videos, essential
for a holistic understanding of assembly in 3D space over time. We introduce
IKEA Video Manuals, a dataset that features 3D models of furniture parts,
instructional manuals, assembly videos from the Internet, and most importantly,
annotations of dense spatio-temporal alignments between these data modalities.
To demonstrate the utility of IKEA Video Manuals, we present five applications
essential for shape assembly: assembly plan generation, part-conditioned
segmentation, part-conditioned pose estimation, video object segmentation, and
furniture assembly based on instructional video manuals. For each application,
we provide evaluation metrics and baseline methods. Through experiments on our
annotated data, we highlight many challenges in grounding assembly instructions
in videos to improve shape assembly, including handling occlusions, varying
viewpoints, and extended assembly sequences.",2024-11-18 09:30:05+00:00,"['Yunong Liu', 'Cristobal Eyzaguirre', 'Manling Li', 'Shubh Khanna', 'Juan Carlos Niebles', 'Vineeth Ravi', 'Saumitra Mishra', 'Weiyu Liu', 'Jiajun Wu']",http://arxiv.org/abs/2411.11409v1
Optimal Transcoding Preset Selection for Live Video Streaming,"In today's digital landscape, video content dominates internet traffic,
underscoring the need for efficient video processing to support seamless live
streaming experiences on platforms like YouTube Live, Twitch, and Facebook
Live. This paper introduces a comprehensive framework designed to optimize
video transcoding parameters, with a specific focus on preset and bitrate
selection to minimize distortion while respecting constraints on bitrate and
transcoding time. The framework comprises three main steps: feature extraction,
prediction, and optimization. It leverages extracted features to predict
transcoding time and rate-distortion, employing both supervised and
unsupervised methods. By utilizing integer linear programming, it identifies
the optimal sequence of presets and bitrates for video segments, ensuring
real-time application feasibility under set constraints. The results
demonstrate the framework's effectiveness in enhancing video quality for live
streaming, maintaining high standards of video delivery while managing
computational resources efficiently. This optimization approach meets the
evolving demands of video delivery by offering a solution for real-time
transcoding optimization. Evaluation using the User Generated Content dataset
showed an average PSNR improvement of 1.5 dB over the default Twitch
configuration, highlighting significant PSNR gains. Additionally, subsequent
experiments demonstrated a BD-rate reduction of -49.60%, reinforcing the
framework's superior performance over Twitch's default configuration.",2024-11-21 22:29:15+00:00,"['Zahra Nabizadeh', 'Maedeh Jamali', 'Nader Karimi', 'Shadrokh Samavi', 'Shahram Shirani']",http://arxiv.org/abs/2411.14613v1
Playable Game Generation,"In recent years, Artificial Intelligence Generated Content (AIGC) has
advanced from text-to-image generation to text-to-video and multimodal video
synthesis. However, generating playable games presents significant challenges
due to the stringent requirements for real-time interaction, high visual
quality, and accurate simulation of game mechanics. Existing approaches often
fall short, either lacking real-time capabilities or failing to accurately
simulate interactive mechanics. To tackle the playability issue, we propose a
novel method called \emph{PlayGen}, which encompasses game data generation, an
autoregressive DiT-based diffusion model, and a comprehensive playability-based
evaluation framework. Validated on well-known 2D and 3D games, PlayGen achieves
real-time interaction, ensures sufficient visual quality, and provides accurate
interactive mechanics simulation. Notably, these results are sustained even
after over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our code is
publicly available: https://github.com/GreatX3/Playable-Game-Generation. Our
playable demo generated by AI is: http://124.156.151.207.",2024-12-01 16:53:02+00:00,"['Mingyu Yang', 'Junyou Li', 'Zhongbin Fang', 'Sheng Chen', 'Yangbin Yu', 'Qiang Fu', 'Wei Yang', 'Deheng Ye']",http://arxiv.org/abs/2412.00887v1
Exocentric To Egocentric Transfer For Action Recognition: A Short Survey,"Egocentric vision captures the scene from the point of view of the camera
wearer while exocentric vision captures the overall scene context. Jointly
modeling ego and exo views is crucial to developing next-generation AI agents.
The community has regained interest in the field of egocentric vision. While
the third-person view and first-person have been thoroughly investigated, very
few works aim to study both synchronously. Exocentric videos contain many
relevant signals that are transferrable to egocentric videos. In this paper, we
provide a broad overview of works combining egocentric and exocentric visions.",2024-10-27 22:38:51+00:00,"['Anirudh Thatipelli', 'Shao-Yuan Lo', 'Amit K. Roy-Chowdhury']",http://arxiv.org/abs/2410.20621v1
Temporally Consistent Dynamic Scene Graphs: An End-to-End Approach for Action Tracklet Generation,"Understanding video content is pivotal for advancing real-world applications
like activity recognition, autonomous systems, and human-computer interaction.
While scene graphs are adept at capturing spatial relationships between objects
in individual frames, extending these representations to capture dynamic
interactions across video sequences remains a significant challenge. To address
this, we present TCDSG, Temporally Consistent Dynamic Scene Graphs, an
innovative end-to-end framework that detects, tracks, and links subject-object
relationships across time, generating action tracklets, temporally consistent
sequences of entities and their interactions. Our approach leverages a novel
bipartite matching mechanism, enhanced by adaptive decoder queries and feedback
loops, ensuring temporal coherence and robust tracking over extended sequences.
This method not only establishes a new benchmark by achieving over 60%
improvement in temporal recall@k on the Action Genome, OpenPVSG, and MEVA
datasets but also pioneers the augmentation of MEVA with persistent object ID
annotations for comprehensive tracklet generation. By seamlessly integrating
spatial and temporal dynamics, our work sets a new standard in multi-frame
video analysis, opening new avenues for high-impact applications in
surveillance, autonomous navigation, and beyond.",2024-12-03 20:19:20+00:00,"['Raphael Ruschel', 'Md Awsafur Rahman', 'Hardik Prajapati', 'Suya You', 'B. S. Manjuanth']",http://arxiv.org/abs/2412.02808v1
Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph Generation,"To equip artificial intelligence with a comprehensive understanding towards a
temporal world, video and 4D panoptic scene graph generation abstracts visual
data into nodes to represent entities and edges to capture temporal relations.
Existing methods encode entity masks tracked across temporal dimensions (mask
tubes), then predict their relations with temporal pooling operation, which
does not fully utilize the motion indicative of the entities' relation. To
overcome this limitation, we introduce a contrastive representation learning
framework that focuses on motion pattern for temporal scene graph generation.
Firstly, our framework encourages the model to learn close representations for
mask tubes of similar subject-relation-object triplets. Secondly, we seek to
push apart mask tubes from their temporally shuffled versions. Moreover, we
also learn distant representations for mask tubes belonging to the same video
but different triplets. Extensive experiments show that our motion-aware
contrastive framework significantly improves state-of-the-art methods on both
video and 4D datasets.",2024-12-10 03:41:07+00:00,"['Thong Thanh Nguyen', 'Xiaobao Wu', 'Yi Bin', 'Cong-Duy T Nguyen', 'See-Kiong Ng', 'Anh Tuan Luu']",http://arxiv.org/abs/2412.07160v2
Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling,"Diffusion models have emerged as a powerful tool for generating high-quality
images, videos, and 3D content. While sampling guidance techniques like CFG
improve quality, they reduce diversity and motion. Autoguidance mitigates these
issues but demands extra weak model training, limiting its practicality for
large-scale models. In this work, we introduce Spatiotemporal Skip Guidance
(STG), a simple training-free sampling guidance method for enhancing
transformer-based video diffusion models. STG employs an implicit weak model
via self-perturbation, avoiding the need for external models or additional
training. By selectively skipping spatiotemporal layers, STG produces an
aligned, degraded version of the original model to boost sample quality without
compromising diversity or dynamic degree. Our contributions include: (1)
introducing STG as an efficient, high-performing guidance technique for video
diffusion models, (2) eliminating the need for auxiliary models by simulating a
weak model through layer skipping, and (3) ensuring quality-enhanced guidance
without compromising sample diversity or dynamics unlike CFG. For additional
results, visit https://junhahyung.github.io/STGuidance.",2024-11-27 15:59:48+00:00,"['Junha Hyung', 'Kinam Kim', 'Susung Hong', 'Min-Jung Kim', 'Jaegul Choo']",http://arxiv.org/abs/2411.18664v1
"Recent Advances in Digital Image and Video Forensics, Anti-forensics and Counter Anti-forensics","Image and video forensics have recently gained increasing attention due to
the proliferation of manipulated images and videos, especially on social media
platforms, such as Twitter and Instagram, which spread disinformation and fake
news. This survey explores image and video identification and forgery detection
covering both manipulated digital media and generative media. However, media
forgery detection techniques are susceptible to anti-forensics; on the other
hand, such anti-forensics techniques can themselves be detected. We therefore
further cover both anti-forensics and counter anti-forensics techniques in
image and video. Finally, we conclude this survey by highlighting some open
problems in this domain.",2024-02-03 09:01:34+00:00,"['Maryam Al-Fehani', 'Saif Al-Kuwari']",http://arxiv.org/abs/2402.02089v1
Presenting the Sense of Effort through Vibration Based on Force Estimated by Inverse Dynamics in Videos,"We present the sense of effort through vibration to help the video viewer
understand how the person in the video moves the body. We suppose sense of
effort is related to force, so we generate vibration based on force and present
the sense of effort through the vibration. We use perceived intensity to make
sense of effort proportional to vibration. In our demonstration, you can
experience vibration while watching a video. We can create vibration on the
spot, so you can experience vibration made from a video taken on the spot.",2024-11-07 19:33:59+00:00,"['Ryoma Akai', 'Masashi Konyo', 'Satoshi Tadokoro']",http://arxiv.org/abs/2411.05105v1
Video Representation Learning with Joint-Embedding Predictive Architectures,"Video representation learning is an increasingly important topic in machine
learning research. We present Video JEPA with Variance-Covariance
Regularization (VJ-VCR): a joint-embedding predictive architecture for
self-supervised video representation learning that employs variance and
covariance regularization to avoid representation collapse. We show that hidden
representations from our VJ-VCR contain abstract, high-level information about
the input data. Specifically, they outperform representations obtained from a
generative baseline on downstream tasks that require understanding of the
underlying dynamics of moving objects in the videos. Additionally, we explore
different ways to incorporate latent variables into the VJ-VCR framework that
capture information about uncertainty in the future in non-deterministic
settings.",2024-12-14 18:33:29+00:00,"['Katrina Drozdov', 'Ravid Shwartz-Ziv', 'Yann LeCun']",http://arxiv.org/abs/2412.10925v1
Using Physics Informed Generative Adversarial Networks to Model 3D porous media,"Micro-CT scanning of rocks significantly enhances our understanding of
pore-scale physics in porous media. With advancements in pore-scale simulation
methods, such as pore network models, it is now possible to accurately simulate
multiphase flow properties, including relative permeability, from CT-scanned
rock samples. However, the limited number of CT-scanned samples and the
challenge of connecting pore-scale networks to field-scale rock properties
often make it difficult to use pore-scale simulated properties in realistic
field-scale reservoir simulations. Deep learning approaches to create synthetic
3D rock structures allow us to simulate variations in CT rock structures, which
can then be used to compute representative rock properties and flow functions.
However, most current deep learning methods for 3D rock structure synthesis
don't consider rock properties derived from well observations, lacking a direct
link between pore-scale structures and field-scale data. We present a method to
construct 3D rock structures constrained to observed rock properties using
generative adversarial networks (GANs) with conditioning accomplished through a
gradual Gaussian deformation process. We begin by pre-training a Wasserstein
GAN to reconstruct 3D rock structures. Subsequently, we use a pore network
model simulator to compute rock properties. The latent vectors for image
generation in GAN are progressively altered using the Gaussian deformation
approach to produce 3D rock structures constrained by well-derived conditioning
data. This GAN and Gaussian deformation approach enables high-resolution
synthetic image generation and reproduces user-defined rock properties such as
porosity, permeability, and pore size distribution. Our research provides a
novel way to link GAN-generated models to field-derived quantities.",2024-09-17 20:28:10+00:00,"['Zihan Ren', 'Sanjay Srinivasan']",http://arxiv.org/abs/2409.11541v1
SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation,"Dynamic scenes contain intricate spatio-temporal information, crucial for
mobile robots, UAVs, and autonomous driving systems to make informed decisions.
Parsing these scenes into semantic triplets <Subject-Predicate-Object> for
accurate Scene Graph Generation (SGG) is highly challenging due to the
fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities
of Large Language Models (LLMs), we propose SceneLLM, a novel framework that
leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework
introduces a Video-to-Language (V2L) mapping module that transforms video
frames into linguistic signals (scene tokens), making the input more
comprehensible for LLMs. To better encode spatial information, we devise a
Spatial Information Aggregation (SIA) scheme, inspired by the structure of
Chinese characters, which encodes spatial data into tokens. Using Optimal
Transport (OT), we generate an implicit language signal from the frame-level
token sequence that captures the video's spatio-temporal information. To
further improve the LLM's ability to process this implicit linguistic input, we
apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a
transformer-based SGG predictor to decode the LLM's reasoning and predict
semantic triplets. Our method achieves state-of-the-art results on the Action
Genome (AG) benchmark, and extensive experiments show the effectiveness of
SceneLLM in understanding and generating accurate dynamic scene graphs.",2024-12-15 02:41:31+00:00,"['Hang Zhang', 'Zhuoling Li', 'Jun Liu']",http://arxiv.org/abs/2412.11026v1
Subjective and Objective Analysis of Indian Social Media Video Quality,"We conducted a large-scale subjective study of the perceptual quality of
User-Generated Mobile Video Content on a set of mobile-originated videos
obtained from the Indian social media platform ShareChat. The content viewed by
volunteer human subjects under controlled laboratory conditions has the benefit
of culturally diversifying the existing corpus of User-Generated Content (UGC)
video quality datasets. There is a great need for large and diverse UGC-VQA
datasets, given the explosive global growth of the visual internet and social
media platforms. This is particularly true in regard to videos obtained by
smartphones, especially in rapidly emerging economies like India. ShareChat
provides a safe and cultural community oriented space for users to generate and
share content in their preferred Indian languages and dialects. Our subjective
quality study, which is based on this data, offers a boost of cultural, visual,
and language diversification to the video quality research community. We expect
that this new data resource will also allow for the development of systems that
can predict the perceived visual quality of Indian social media videos, to
control scaling and compression protocols for streaming, provide better user
recommendations, and guide content analysis and processing. We demonstrate the
value of the new data resource by conducting a study of leading blind video
quality models on it, including a new model, called MoEVA, which deploys a
mixture of experts to predict video quality. Both the new LIVE-ShareChat
dataset and sample source code for MoEVA are being made freely available to the
research community at https://github.com/sandeep-sm/LIVE-SC",2024-01-05 13:13:09+00:00,"['Sandeep Mishra', 'Mukul Jha', 'Alan C. Bovik']",http://arxiv.org/abs/2401.02794v1
"Saliency Detection in Educational Videos: Analyzing the Performance of Current Models, Identifying Limitations and Advancement Directions","Identifying the regions of a learning resource that a learner pays attention
to is crucial for assessing the material's impact and improving its design and
related support systems. Saliency detection in videos addresses the automatic
recognition of attention-drawing regions in single frames. In educational
settings, the recognition of pertinent regions in a video's visual stream can
enhance content accessibility and information retrieval tasks such as video
segmentation, navigation, and summarization. Such advancements can pave the way
for the development of advanced AI-assisted technologies that support learning
with greater efficacy. However, this task becomes particularly challenging for
educational videos due to the combination of unique characteristics such as
text, voice, illustrations, animations, and more. To the best of our knowledge,
there is currently no study that evaluates saliency detection approaches in
educational videos. In this paper, we address this gap by evaluating four
state-of-the-art saliency detection approaches for educational videos. We
reproduce the original studies and explore the replication capabilities for
general-purpose (non-educational) datasets. Then, we investigate the
generalization capabilities of the models and evaluate their performance on
educational videos. We conduct a comprehensive analysis to identify common
failure scenarios and possible areas of improvement. Our experimental results
show that educational videos remain a challenging context for generic video
saliency detection models.",2024-08-08 15:15:48+00:00,"['Evelyn Navarrete', 'Ralph Ewerth', 'Anett Hoppe']",http://arxiv.org/abs/2408.04515v1
Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics,"We present Puppet-Master, an interactive video generative model that can
serve as a motion prior for part-level dynamics. At test time, given a single
image and a sparse set of motion trajectories (i.e., drags), Puppet-Master can
synthesize a video depicting realistic part-level motion faithful to the given
drag interactions. This is achieved by fine-tuning a large-scale pre-trained
video diffusion model, for which we propose a new conditioning architecture to
inject the dragging control effectively. More importantly, we introduce the
all-to-first attention mechanism, a drop-in replacement for the widely adopted
spatial attention modules, which significantly improves generation quality by
addressing the appearance and background issues in existing models. Unlike
other motion-conditioned video generators that are trained on in-the-wild
videos and mostly move an entire object, Puppet-Master is learned from
Objaverse-Animation-HQ, a new dataset of curated part-level motion clips. We
propose a strategy to automatically filter out sub-optimal animations and
augment the synthetic renderings with meaningful motion trajectories.
Puppet-Master generalizes well to real images across various categories and
outperforms existing methods in a zero-shot manner on a real-world benchmark.
See our project page for more results: vgg-puppetmaster.github.io.",2024-08-08 17:59:38+00:00,"['Ruining Li', 'Chuanxia Zheng', 'Christian Rupprecht', 'Andrea Vedaldi']",http://arxiv.org/abs/2408.04631v1
Do As I Do: Pose Guided Human Motion Copy,"Human motion copy is an intriguing yet challenging task in artificial
intelligence and computer vision, which strives to generate a fake video of a
target person performing the motion of a source person. The problem is
inherently challenging due to the subtle human-body texture details to be
generated and the temporal consistency to be considered. Existing approaches
typically adopt a conventional GAN with an L1 or L2 loss to produce the target
fake video, which intrinsically necessitates a large number of training samples
that are challenging to acquire. Meanwhile, current methods still have
difficulties in attaining realistic image details and temporal consistency,
which unfortunately can be easily perceived by human observers. Motivated by
this, we try to tackle the issues from three aspects: (1) We constrain
pose-to-appearance generation with a perceptual loss and a theoretically
motivated Gromov-Wasserstein loss to bridge the gap between pose and
appearance. (2) We present an episodic memory module in the pose-to-appearance
generation to propel continuous learning that helps the model learn from its
past poor generations. We also utilize geometrical cues of the face to optimize
facial details and refine each key body part with a dedicated local GAN. (3) We
advocate generating the foreground in a sequence-to-sequence manner rather than
a single-frame manner, explicitly enforcing temporal inconsistency. Empirical
results on five datasets, iPER, ComplexMotion, SoloDance, Fish, and Mouse
datasets, demonstrate that our method is capable of generating realistic target
videos while precisely copying motion from a source video. Our method
significantly outperforms state-of-the-art approaches and gains 7.2% and 12.4%
improvements in PSNR and FID respectively.",2024-06-24 12:41:51+00:00,"['Sifan Wu', 'Zhenguang Liu', 'Beibei Zhang', 'Roger Zimmermann', 'Zhongjie Ba', 'Xiaosong Zhang', 'Kui Ren']",http://arxiv.org/abs/2406.16601v1
Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization,"Videos contain a wealth of information, and generating detailed and accurate
descriptions in natural language is a key aspect of video understanding. In
this paper, we present video-SALMONN 2, an advanced audio-visual large language
model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with
paired audio) captioning through directed preference optimization (DPO). We
propose new metrics to evaluate the completeness and accuracy of video
descriptions, which are optimized using DPO. To further improve training, we
introduce a novel multi-round DPO (mrDPO) approach, which involves periodically
updating the DPO reference model, merging and re-initializing the LoRA module
as a proxy for parameter updates after each training round (1,000 steps), and
incorporating guidance from ground-truth video captions to stabilize the
process. To address potential catastrophic forgetting of non-captioning
abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO
LLM by using the captions generated by the mrDPO-trained model as supervised
labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's
captioning accuracy, reducing global and local error rates by 40\% and 20\%,
respectively, while decreasing the repetition rate by 35\%. The final
video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models
such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining
competitive performance to the state-of-the-art on widely used video
question-answering benchmark among models of similar size. Upon acceptance, we
will release the code, model checkpoints, and training and test data. Demos are
available at
\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.",2024-10-09 08:44:47+00:00,"['Changli Tang', 'Yixuan Li', 'Yudong Yang', 'Jimin Zhuang', 'Guangzhi Sun', 'Wei Li', 'Zujun Ma', 'Chao Zhang']",http://arxiv.org/abs/2410.06682v2
MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance,"In recent years, generative artificial intelligence has achieved significant
advancements in the field of image generation, spawning a variety of
applications. However, video generation still faces considerable challenges in
various aspects, such as controllability, video length, and richness of
details, which hinder the application and popularization of this technology. In
this work, we propose a controllable video generation framework, dubbed
MimicMotion, which can generate high-quality videos of arbitrary length
mimicking specific motion guidance. Compared with previous methods, our
approach has several highlights. Firstly, we introduce confidence-aware pose
guidance that ensures high frame quality and temporal smoothness. Secondly, we
introduce regional loss amplification based on pose confidence, which
significantly reduces image distortion. Lastly, for generating long and smooth
videos, we propose a progressive latent fusion strategy. By this means, we can
produce videos of arbitrary length with acceptable resource consumption. With
extensive experiments and user studies, MimicMotion demonstrates significant
improvements over previous approaches in various aspects. Detailed results and
comparisons are available on our project page:
https://tencent.github.io/MimicMotion .",2024-06-28 06:40:53+00:00,"['Yuang Zhang', 'Jiaxi Gu', 'Li-Wen Wang', 'Han Wang', 'Junqi Cheng', 'Yuefeng Zhu', 'Fangyuan Zou']",http://arxiv.org/abs/2406.19680v1
DragTraffic: Interactive and Controllable Traffic Scene Generation for Autonomous Driving,"Evaluating and training autonomous driving systems require diverse and
scalable corner cases. However, most existing scene generation methods lack
controllability, accuracy, and versatility, resulting in unsatisfactory
generation results. Inspired by DragGAN in image generation, we propose
DragTraffic, a generalized, interactive, and controllable traffic scene
generation framework based on conditional diffusion. DragTraffic enables
non-experts to generate a variety of realistic driving scenarios for different
types of traffic agents through an adaptive mixture expert architecture. We
employ a regression model to provide a general initial solution and a
refinement process based on the conditional diffusion model to ensure
diversity. User-customized context is introduced through cross-attention to
ensure high controllability. Experiments on a real-world driving dataset show
that DragTraffic outperforms existing methods in terms of authenticity,
diversity, and freedom. Demo videos and code are available at
https://chantsss.github.io/Dragtraffic/.",2024-04-19 04:49:28+00:00,"['Sheng Wang', 'Ge Sun', 'Fulong Ma', 'Tianshuai Hu', 'Qiang Qin', 'Yongkang Song', 'Lei Zhu', 'Junwei Liang']",http://arxiv.org/abs/2404.12624v2
Fleximo: Towards Flexible Text-to-Human Motion Video Generation,"Current methods for generating human motion videos rely on extracting pose
sequences from reference videos, which restricts flexibility and control.
Additionally, due to the limitations of pose detection techniques, the
extracted pose sequences can sometimes be inaccurate, leading to low-quality
video outputs. We introduce a novel task aimed at generating human motion
videos solely from reference images and natural language. This approach offers
greater flexibility and ease of use, as text is more accessible than the
desired guidance videos. However, training an end-to-end model for this task
requires millions of high-quality text and human motion video pairs, which are
challenging to obtain. To address this, we propose a new framework called
Fleximo, which leverages large-scale pre-trained text-to-3D motion models. This
approach is not straightforward, as the text-generated skeletons may not
consistently match the scale of the reference image and may lack detailed
information. To overcome these challenges, we introduce an anchor point based
rescale method and design a skeleton adapter to fill in missing details and
bridge the gap between text-to-motion and motion-to-video generation. We also
propose a video refinement process to further enhance video quality. A large
language model (LLM) is employed to decompose natural language into discrete
motion sequences, enabling the generation of motion videos of any desired
length. To assess the performance of Fleximo, we introduce a new benchmark
called MotionBench, which includes 400 videos across 20 identities and 20
motions. We also propose a new metric, MotionScore, to evaluate the accuracy of
motion following. Both qualitative and quantitative results demonstrate that
our method outperforms existing text-conditioned image-to-video generation
methods. All code and model weights will be made publicly available.",2024-11-29 04:09:13+00:00,"['Yuhang Zhang', 'Yuan Zhou', 'Zeyu Liu', 'Yuxuan Cai', 'Qiuyue Wang', 'Aidong Men', 'Huan Yang']",http://arxiv.org/abs/2411.19459v1
Radiance Field Learners As UAV First-Person Viewers,"First-Person-View (FPV) holds immense potential for revolutionizing the
trajectory of Unmanned Aerial Vehicles (UAVs), offering an exhilarating avenue
for navigating complex building structures. Yet, traditional Neural Radiance
Field (NeRF) methods face challenges such as sampling single points per
iteration and requiring an extensive array of views for supervision. UAV videos
exacerbate these issues with limited viewpoints and significant spatial scale
variations, resulting in inadequate detail rendering across diverse scales. In
response, we introduce FPV-NeRF, addressing these challenges through three key
facets: (1) Temporal consistency. Leveraging spatio-temporal continuity ensures
seamless coherence between frames; (2) Global structure. Incorporating various
global features during point sampling preserves space integrity; (3) Local
granularity. Employing a comprehensive framework and multi-resolution
supervision for multi-scale scene feature representation tackles the
intricacies of UAV video spatial scales. Additionally, due to the scarcity of
publicly available FPV videos, we introduce an innovative view synthesis method
using NeRF to generate FPV perspectives from UAV footage, enhancing spatial
perception for drones. Our novel dataset spans diverse trajectories, from
outdoor to indoor environments, in the UAV domain, differing significantly from
traditional NeRF scenarios. Through extensive experiments encompassing both
interior and exterior building structures, FPV-NeRF demonstrates a superior
understanding of the UAV flying space, outperforming state-of-the-art methods
in our curated UAV dataset. Explore our project page for further insights:
https://fpv-nerf.github.io/.",2024-08-10 12:29:11+00:00,"['Liqi Yan', 'Qifan Wang', 'Junhan Zhao', 'Qiang Guan', 'Zheng Tang', 'Jianhui Zhang', 'Dongfang Liu']",http://arxiv.org/abs/2408.05533v1
FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model,"We aim to develop a model-based planning framework for world models that can
be scaled with increasing model and data budgets for general-purpose
manipulation tasks with only language and vision inputs. To this end, we
present FLow-centric generative Planning (FLIP), a model-based planning
algorithm on visual space that features three key modules: 1. a multi-modal
flow generation model as the general-purpose action proposal module; 2. a
flow-conditioned video generation model as the dynamics module; and 3. a
vision-language representation learning model as the value module. Given an
initial image and language instruction as the goal, FLIP can progressively
search for long-horizon flow and video plans that maximize the discounted
return to accomplish the task. FLIP is able to synthesize long-horizon plans
across objects, robots, and tasks with image flows as the general action
representation, and the dense flow information also provides rich guidance for
long-horizon video generation. In addition, the synthesized flow and video
plans can guide the training of low-level control policies for robot execution.
Experiments on diverse benchmarks demonstrate that FLIP can improve both the
success rates and quality of long-horizon video plan synthesis and has the
interactive world model property, opening up wider applications for future
works.Video demos are on our website: https://nus-lins-lab.github.io/flipweb/.",2024-12-11 10:17:00+00:00,"['Chongkai Gao', 'Haozhuo Zhang', 'Zhixuan Xu', 'Zhehao Cai', 'Lin Shao']",http://arxiv.org/abs/2412.08261v2
Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis,"In this paper, we propose a new task -- generating speech from videos of
people and their transcripts (VTTS) -- to motivate new techniques for
multimodal speech generation. This task generalizes the task of generating
speech from cropped lip videos, and is also more complicated than the task of
generating generic audio clips (e.g., dog barking) from videos and text.
Multilingual versions of the task could lead to new techniques for
cross-lingual dubbing. We also present a decoder-only multimodal model for this
task, which we call Visatronic. This model embeds vision, text and speech
directly into the common subspace of a transformer model and uses an
autoregressive loss to learn a generative model of discretized mel-spectrograms
conditioned on speaker videos and transcripts of their speech. By embedding all
modalities into a common subspace, Visatronic can achieve improved results over
models that use only text or video as input. Further, it presents a much
simpler approach for multimodal speech generation compared to prevailing
approaches which rely on lip-detectors and complicated architectures to fuse
modalities while producing better results. Since the model is flexible enough
to accommodate different ways of ordering inputs as a sequence, we carefully
explore different strategies to better understand the best way to propagate
information to the generative steps. To facilitate further research on VTTS, we
will release (i) our code, (ii) clean transcriptions for the large-scale
VoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS
incorporating both objective and subjective metrics.",2024-11-26 18:57:29+00:00,"['Akshita Gupta', 'Tatiana Likhomanenko', 'Karren Dai Yang', 'Richard He Bai', 'Zakaria Aldeneh', 'Navdeep Jaitly']",http://arxiv.org/abs/2411.17690v1
"Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries","Generative AI is reshaping the media landscape, enabling unprecedented
capabilities in video creation, personalization, and scalability. This paper
presents a comprehensive SWOT analysis of Metas Movie Gen, a cutting-edge
generative AI foundation model designed to produce 1080p HD videos with
synchronized audio from simple text prompts. We explore its strengths,
including high-resolution video generation, precise editing, and seamless audio
integration, which make it a transformative tool across industries such as
filmmaking, advertising, and education. However, the analysis also addresses
limitations, such as constraints on video length and potential biases in
generated content, which pose challenges for broader adoption. In addition, we
examine the evolving regulatory and ethical considerations surrounding
generative AI, focusing on issues like content authenticity, cultural
representation, and responsible use. Through comparative insights with leading
models like DALL-E and Google Imagen, this paper highlights Movie Gens unique
features, such as video personalization and multimodal synthesis, while
identifying opportunities for innovation and areas requiring further research.
Our findings provide actionable insights for stakeholders, emphasizing both the
opportunities and challenges of deploying generative AI in media production.
This work aims to guide future advancements in generative AI, ensuring
scalability, quality, and ethical integrity in this rapidly evolving field.",2024-12-05 03:01:53+00:00,"['Abul Ehtesham', 'Saket Kumar', 'Aditi Singh', 'Tala Talaei Khoei']",http://arxiv.org/abs/2412.03837v1
Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos,"Creating controllable 3D human portraits from casual smartphone videos is
highly desirable due to their immense value in AR/VR applications. The recent
development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering
quality and training efficiency. However, it still remains a challenge to
accurately model and disentangle head movements and facial expressions from a
single-view capture to achieve high-quality renderings. In this paper, we
introduce Rig3DGS to address this challenge. We represent the entire scene,
including the dynamic subject, using a set of 3D Gaussians in a canonical
space. Using a set of control signals, such as head pose and expressions, we
transform them to the 3D space with learned deformations to generate the
desired rendering. Our key innovation is a carefully designed deformation
method which is guided by a learnable prior derived from a 3D morphable model.
This approach is highly efficient in training and effective in controlling
facial expressions, head positions, and view synthesis across various captures.
We demonstrate the effectiveness of our learned deformation through extensive
quantitative and qualitative experiments. The project page can be found at
http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html",2024-02-06 05:40:53+00:00,"['Alfredo Rivero', 'ShahRukh Athar', 'Zhixin Shu', 'Dimitris Samaras']",http://arxiv.org/abs/2402.03723v1
GAN with Skip Patch Discriminator for Biological Electron Microscopy Image Generation,"Generating realistic electron microscopy (EM) images has been a challenging
problem due to their complex global and local structures. Isola et al. proposed
pix2pix, a conditional Generative Adversarial Network (GAN), for the general
purpose of image-to-image translation; which fails to generate realistic EM
images. We propose a new architecture for the discriminator in the GAN
providing access to multiple patch sizes using skip patches and generating
realistic EM images.",2024-03-31 04:39:40+00:00,"['Nishith Ranjon Roy', 'Nailah Rawnaq', 'Tulin Kaman']",http://arxiv.org/abs/2404.00558v1
G$^{2}$TR: Generalized Grounded Temporal Reasoning for Robot Instruction Following by Combining Large Pre-trained Models,"Consider the scenario where a human cleans a table and a robot observing the
scene is instructed with the task ""Remove the cloth using which I wiped the
table"". Instruction following with temporal reasoning requires the robot to
identify the relevant past object interaction, ground the object of interest in
the present scene, and execute the task according to the human's instruction.
Directly grounding utterances referencing past interactions to grounded objects
is challenging due to the multi-hop nature of references to past interactions
and large space of object groundings in a video stream observing the robot's
workspace. Our key insight is to factor the temporal reasoning task as (i)
estimating the video interval associated with event reference, (ii) performing
spatial reasoning over the interaction frames to infer the intended object
(iii) semantically track the object's location till the current scene to enable
future robot interactions. Our approach leverages existing large pre-trained
models (which possess inherent generalization capabilities) and combines them
appropriately for temporal grounding tasks. Evaluation on a video-language
corpus acquired with a robot manipulator displaying rich temporal interactions
in spatially-complex scenes displays an average accuracy of 70.10%. The
dataset, code, and videos are available at
https://reail-iitdelhi.github.io/temporalreasoning.github.io/ .",2024-10-10 00:07:27+00:00,"['Riya Arora', 'Niveditha Narendranath', 'Aman Tambi', 'Sandeep S. Zachariah', 'Souvik Chakraborty', 'Rohan Paul']",http://arxiv.org/abs/2410.07494v1
Taming Rectified Flow for Inversion and Editing,"Rectified-flow-based diffusion transformers like FLUX and OpenSora have
demonstrated outstanding performance in the field of image and video
generation. Despite their robust generative capabilities, these models often
struggle with inversion inaccuracies, which could further limit their
effectiveness in downstream tasks such as image and video editing. To address
this issue, we propose RF-Solver, a novel training-free sampler that
effectively enhances inversion precision by mitigating the errors in the
ODE-solving process of rectified flow. Specifically, we derive the exact
formulation of the rectified flow ODE and apply the high-order Taylor expansion
to estimate its nonlinear components, significantly enhancing the precision of
ODE solutions at each timestep. Building upon RF-Solver, we further propose
RF-Edit, a general feature-sharing-based framework for image and video editing.
By incorporating self-attention features from the inversion process into the
editing process, RF-Edit effectively preserves the structural information of
the source image or video while achieving high-quality editing results. Our
approach is compatible with any pre-trained rectified-flow-based models for
image and video tasks, requiring no additional training or optimization.
Extensive experiments across generation, inversion, and editing tasks in both
image and video modalities demonstrate the superiority and versatility of our
method. The source code is available at
https://github.com/wangjiangshan0725/RF-Solver-Edit.",2024-11-07 14:29:02+00:00,"['Jiangshan Wang', 'Junfu Pu', 'Zhongang Qi', 'Jiayi Guo', 'Yue Ma', 'Nisha Huang', 'Yuxin Chen', 'Xiu Li', 'Ying Shan']",http://arxiv.org/abs/2411.04746v2
Motion by Queries: Identity-Motion Trade-offs in Text-to-Video Generation,"Text-to-video diffusion models have shown remarkable progress in generating
coherent video clips from textual descriptions. However, the interplay between
motion, structure, and identity representations in these models remains
under-explored. Here, we investigate how self-attention query features (a.k.a.
Q features) simultaneously govern motion, structure, and identity and examine
the challenges arising when these representations interact. Our analysis
reveals that Q affects not only layout, but that during denoising Q also has a
strong effect on subject identity, making it hard to transfer motion without
the side-effect of transferring identity. Understanding this dual role enabled
us to control query feature injection (Q injection) and demonstrate two
applications: (1) a zero-shot motion transfer method that is 20 times more
efficient than existing approaches, and (2) a training-free technique for
consistent multi-shot video generation, where characters maintain identity
across multiple video shots while Q injection enhances motion fidelity.",2024-12-10 18:49:39+00:00,"['Yuval Atzmon', 'Rinon Gal', 'Yoad Tewel', 'Yoni Kasten', 'Gal Chechik']",http://arxiv.org/abs/2412.07750v2
L-MAGIC: Language Model Assisted Generation of Images with Coherence,"In the current era of generative AI breakthroughs, generating panoramic
scenes from a single input image remains a key challenge. Most existing methods
use diffusion-based iterative or simultaneous multi-view inpainting. However,
the lack of global scene layout priors leads to subpar outputs with duplicated
objects (e.g., multiple beds in a bedroom) or requires time-consuming human
text inputs for each view. We propose L-MAGIC, a novel method leveraging large
language models for guidance while diffusing multiple coherent views of 360
degree panoramic scenes. L-MAGIC harnesses pre-trained diffusion and language
models without fine-tuning, ensuring zero-shot performance. The output quality
is further enhanced by super-resolution and multi-view fusion techniques.
Extensive experiments demonstrate that the resulting panoramic scenes feature
better scene layouts and perspective view rendering quality compared to related
works, with >70% preference in human evaluations. Combined with conditional
diffusion models, L-MAGIC can accept various input modalities, including but
not limited to text, depth maps, sketches, and colored scripts. Applying depth
estimation further enables 3D point cloud generation and dynamic scene
exploration with fluid camera motion. Code is available at
https://github.com/IntelLabs/MMPano. The video presentation is available at
https://youtu.be/XDMNEzH4-Ec?list=PLG9Zyvu7iBa0-a7ccNLO8LjcVRAoMn57s.",2024-06-03 23:28:57+00:00,"['Zhipeng Cai', 'Matthias Mueller', 'Reiner Birkl', 'Diana Wofk', 'Shao-Yen Tseng', 'JunDa Cheng', 'Gabriela Ben-Melech Stan', 'Vasudev Lal', 'Michael Paulitsch']",http://arxiv.org/abs/2406.01843v1
FacEnhance: Facial Expression Enhancing with Recurrent DDPMs,"Facial expressions, vital in non-verbal human communication, have found
applications in various computer vision fields like virtual reality, gaming,
and emotional AI assistants. Despite advancements, many facial expression
generation models encounter challenges such as low resolution (e.g., 32x32 or
64x64 pixels), poor quality, and the absence of background details. In this
paper, we introduce FacEnhance, a novel diffusion-based approach addressing
constraints in existing low-resolution facial expression generation models.
FacEnhance enhances low-resolution facial expression videos (64x64 pixels) to
higher resolutions (192x192 pixels), incorporating background details and
improving overall quality. Leveraging conditional denoising within a diffusion
framework, guided by a background-free low-resolution video and a single
neutral expression high-resolution image, FacEnhance generates a video
incorporating the facial expression from the low-resolution video performed by
the individual with background from the neutral image. By complementing
lightweight low-resolution models, FacEnhance strikes a balance between
computational efficiency and desirable image resolution and quality. Extensive
experiments on the MUG facial expression database demonstrate the efficacy of
FacEnhance in enhancing low-resolution model outputs to state-of-the-art
quality while preserving content and identity consistency. FacEnhance
represents significant progress towards resource-efficient, high-fidelity
facial expression generation, Renewing outdated low-resolution methods to
up-to-date standards.",2024-06-13 12:23:35+00:00,"['Hamza Bouzid', 'Lahoucine Ballihi']",http://arxiv.org/abs/2406.09040v1
PoM: Efficient Image and Video Generation with the Polynomial Mixer,"Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous
to generate high quality images and videos. However, encoding an image or a
video as a sequence of patches results in costly attention patterns, as the
requirements both in terms of memory and compute grow quadratically. To
alleviate this problem, we propose a drop-in replacement for MHA called the
Polynomial Mixer (PoM) that has the benefit of encoding the entire sequence
into an explicit state. PoM has a linear complexity with respect to the number
of tokens. This explicit state also allows us to generate frames in a
sequential fashion, minimizing memory and compute requirement, while still
being able to train in parallel. We show the Polynomial Mixer is a universal
sequence-to-sequence approximator, just like regular MHA. We adapt several
Diffusion Transformers (DiT) for generating images and videos with PoM
replacing MHA, and we obtain high quality samples while using less
computational resources. The code is available at
https://github.com/davidpicard/HoMM.",2024-11-19 17:16:31+00:00,"['David Picard', 'Nicolas Dufour']",http://arxiv.org/abs/2411.12663v1
CPA: Camera-pose-awareness Diffusion Transformer for Video Generation,"Despite the significant advancements made by Diffusion Transformer
(DiT)-based methods in video generation, there remains a notable gap with
controllable camera pose perspectives. Existing works such as OpenSora do NOT
adhere precisely to anticipated trajectories and physical interactions, thereby
limiting the flexibility in downstream applications. To alleviate this issue,
we introduce CPA, a unified camera-pose-awareness text-to-video generation
approach that elaborates the camera movement and integrates the textual,
visual, and spatial conditions. Specifically, we deploy the Sparse Motion
Encoding (SME) module to transform camera pose information into a
spatial-temporal embedding and activate the Temporal Attention Injection (TAI)
module to inject motion patches into each ST-DiT block. Our plug-in
architecture accommodates the original DiT parameters, facilitating diverse
types of camera poses and flexible object movement. Extensive qualitative and
quantitative experiments demonstrate that our method outperforms LDM-based
methods for long video generation while achieving optimal performance in
trajectory consistency and object consistency.",2024-12-02 12:10:00+00:00,"['Yuelei Wang', 'Jian Zhang', 'Pengtao Jiang', 'Hao Zhang', 'Jinwei Chen', 'Bo Li']",http://arxiv.org/abs/2412.01429v1
This&That: Language-Gesture Controlled Video Generation for Robot Planning,"We propose a robot learning method for communicating, planning, and executing
a wide range of tasks, dubbed This&That. We achieve robot planning for general
tasks by leveraging the power of video generative models trained on
internet-scale data containing rich physical and semantic context. In this
work, we tackle three fundamental challenges in video-based planning: 1)
unambiguous task communication with simple human instructions, 2) controllable
video generation that respects user intents, and 3) translating visual planning
into robot actions. We propose language-gesture conditioning to generate
videos, which is both simpler and clearer than existing language-only methods,
especially in complex and uncertain environments. We then suggest a behavioral
cloning design that seamlessly incorporates the video plans. This&That
demonstrates state-of-the-art effectiveness in addressing the above three
challenges, and justifies the use of video generation as an intermediate
representation for generalizable task planning and execution. Project website:
https://cfeng16.github.io/this-and-that/.",2024-07-08 00:28:41+00:00,"['Boyang Wang', 'Nikhil Sridhar', 'Chao Feng', 'Mark Van der Merwe', 'Adam Fishman', 'Nima Fazeli', 'Jeong Joon Park']",http://arxiv.org/abs/2407.05530v1
Light-VQA+: A Video Quality Assessment Model for Exposure Correction with Vision-Language Guidance,"Recently, User-Generated Content (UGC) videos have gained popularity in our
daily lives. However, UGC videos often suffer from poor exposure due to the
limitations of photographic equipment and techniques. Therefore, Video Exposure
Correction (VEC) algorithms have been proposed, Low-Light Video Enhancement
(LLVE) and Over-Exposed Video Recovery (OEVR) included. Equally important to
the VEC is the Video Quality Assessment (VQA). Unfortunately, almost all
existing VQA models are built generally, measuring the quality of a video from
a comprehensive perspective. As a result, Light-VQA, trained on LLVE-QA, is
proposed for assessing LLVE. We extend the work of Light-VQA by expanding the
LLVE-QA dataset into Video Exposure Correction Quality Assessment (VEC-QA)
dataset with over-exposed videos and their corresponding corrected versions. In
addition, we propose Light-VQA+, a VQA model specialized in assessing VEC.
Light-VQA+ differs from Light-VQA mainly from the usage of the CLIP model and
the vision-language guidance during the feature extraction, followed by a new
module referring to the Human Visual System (HVS) for more accurate assessment.
Extensive experimental results show that our model achieves the best
performance against the current State-Of-The-Art (SOTA) VQA models on the
VEC-QA dataset and other public datasets.",2024-05-06 10:26:06+00:00,"['Xunchu Zhou', 'Xiaohong Liu', 'Yunlong Dong', 'Tengchuan Kou', 'Yixuan Gao', 'Zicheng Zhang', 'Chunyi Li', 'Haoning Wu', 'Guangtao Zhai']",http://arxiv.org/abs/2405.03333v2
"AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark","Video detailed captioning is a key task which aims to generate comprehensive
and coherent textual descriptions of video content, benefiting both video
understanding and generation. In this paper, we propose AuroraCap, a video
captioner based on a large multimodal model. We follow the simplest
architecture design without additional parameters for temporal modeling. To
address the overhead caused by lengthy video sequences, we implement the token
merging strategy, reducing the number of input visual tokens. Surprisingly, we
found that this strategy results in little performance loss. AuroraCap shows
superior performance on various video and image captioning benchmarks, for
example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and
Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include
simple descriptions, consisting of a few dozen words, which limits research in
this field. Therefore, we develop VDC, a video detailed captioning benchmark
with over one thousand carefully annotated structured captions. In addition, we
propose a new LLM-assisted metric VDCscore for bettering evaluation, which
adopts a divide-and-conquer strategy to transform long caption evaluation into
multiple short question-answer pairs. With the help of human Elo ranking, our
experiments show that this benchmark better correlates with human judgments of
video detailed captioning quality.",2024-10-04 00:13:54+00:00,"['Wenhao Chai', 'Enxin Song', 'Yilun Du', 'Chenlin Meng', 'Vashisht Madhavan', 'Omer Bar-Tal', 'Jenq-Neng Hwang', 'Saining Xie', 'Christopher D. Manning']",http://arxiv.org/abs/2410.03051v3
SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations,"With the rise of generative AI and rapid growth of high-quality video
generation, video guardrails have become more crucial than ever to ensure
safety and security across platforms. Current video guardrails, however, are
either overly simplistic, relying on pure classification models trained on
simple policies with limited unsafe categories, which lack detailed
explanations, or prompting multimodal large language models (MLLMs) with long
safety guidelines, which are inefficient and impractical for guardrailing
real-world content. To bridge this gap, we propose SafeWatch, an efficient
MLLM-based video guardrail model designed to follow customized safety policies
and provide multi-label video guardrail outputs with content-specific
explanations in a zero-shot manner. In particular, unlike traditional
MLLM-based guardrails that encode all safety policies autoregressively, causing
inefficiency and bias, SafeWatch uniquely encodes each policy chunk in parallel
and eliminates their position bias such that all policies are attended
simultaneously with equal importance. In addition, to improve efficiency and
accuracy, SafeWatch incorporates a policy-aware visual token pruning algorithm
that adaptively selects the most relevant video tokens for each policy,
discarding noisy or irrelevant information. This allows for more focused,
policy-compliant guardrail with significantly reduced computational overhead.
Considering the limitations of existing video guardrail benchmarks, we propose
SafeWatch-Bench, a large-scale video guardrail benchmark comprising over 2M
videos spanning six safety categories which covers over 30 tasks to ensure a
comprehensive coverage of all potential safety scenarios. SafeWatch outperforms
SOTA by 28.2% on SafeWatch-Bench, 13.6% on benchmarks, cuts costs by 10%, and
delivers top-tier explanations validated by LLM and human reviews.",2024-12-09 18:59:04+00:00,"['Zhaorun Chen', 'Francesco Pinto', 'Minzhou Pan', 'Bo Li']",http://arxiv.org/abs/2412.06878v1
Video Anomaly Detection with Motion and Appearance Guided Patch Diffusion Model,"A recent endeavor in one class of video anomaly detection is to leverage
diffusion models and posit the task as a generation problem, where the
diffusion model is trained to recover normal patterns exclusively, thus
reporting abnormal patterns as outliers. Yet, existing attempts neglect the
various formations of anomaly and predict normal samples at the feature level
regardless that abnormal objects in surveillance videos are often relatively
small. To address this, a novel patch-based diffusion model is proposed,
specifically engineered to capture fine-grained local information. We further
observe that anomalies in videos manifest themselves as deviations in both
appearance and motion. Therefore, we argue that a comprehensive solution must
consider both of these aspects simultaneously to achieve accurate frame
prediction. To address this, we introduce innovative motion and appearance
conditions that are seamlessly integrated into our patch diffusion model. These
conditions are designed to guide the model in generating coherent and
contextually appropriate predictions for both semantic content and motion
relations. Experimental results in four challenging video anomaly detection
datasets empirically substantiate the efficacy of our proposed approach,
demonstrating that it consistently outperforms most existing methods in
detecting abnormal behaviors.",2024-12-12 07:42:50+00:00,"['Hang Zhou', 'Jiale Cai', 'Yuteng Ye', 'Yonghui Feng', 'Chenxing Gao', 'Junqing Yu', 'Zikai Song', 'Wei Yang']",http://arxiv.org/abs/2412.09026v1
4Dynamic: Text-to-4D Generation with Hybrid Priors,"Due to the fascinating generative performance of text-to-image diffusion
models, growing text-to-3D generation works explore distilling the 2D
generative priors into 3D, using the score distillation sampling (SDS) loss, to
bypass the data scarcity problem. The existing text-to-3D methods have achieved
promising results in realism and 3D consistency, but text-to-4D generation
still faces challenges, including lack of realism and insufficient dynamic
motions. In this paper, we propose a novel method for text-to-4D generation,
which ensures the dynamic amplitude and authenticity through direct supervision
provided by a video prior. Specifically, we adopt a text-to-video diffusion
model to generate a reference video and divide 4D generation into two stages:
static generation and dynamic generation. The static 3D generation is achieved
under the guidance of the input text and the first frame of the reference
video, while in the dynamic generation stage, we introduce a customized SDS
loss to ensure multi-view consistency, a video-based SDS loss to improve
temporal consistency, and most importantly, direct priors from the reference
video to ensure the quality of geometry and texture. Moreover, we design a
prior-switching training strategy to avoid conflicts between different priors
and fully leverage the benefits of each prior. In addition, to enrich the
generated motion, we further introduce a dynamic modeling representation
composed of a deformation network and a topology network, which ensures dynamic
continuity while modeling topological changes. Our method not only supports
text-to-4D generation but also enables 4D generation from monocular videos. The
comparison experiments demonstrate the superiority of our method compared to
existing methods.",2024-07-17 16:02:55+00:00,"['Yu-Jie Yuan', 'Leif Kobbelt', 'Jiwen Liu', 'Yuan Zhang', 'Pengfei Wan', 'Yu-Kun Lai', 'Lin Gao']",http://arxiv.org/abs/2407.12684v1
AutoVFX: Physically Realistic Video Editing from Natural Language Instructions,"Modern visual effects (VFX) software has made it possible for skilled artists
to create imagery of virtually anything. However, the creation process remains
laborious, complex, and largely inaccessible to everyday users. In this work,
we present AutoVFX, a framework that automatically creates realistic and
dynamic VFX videos from a single video and natural language instructions. By
carefully integrating neural scene modeling, LLM-based code generation, and
physical simulation, AutoVFX is able to provide physically-grounded,
photorealistic editing effects that can be controlled directly using natural
language instructions. We conduct extensive experiments to validate AutoVFX's
efficacy across a diverse spectrum of videos and instructions. Quantitative and
qualitative results suggest that AutoVFX outperforms all competing methods by a
large margin in generative quality, instruction alignment, editing versatility,
and physical plausibility.",2024-11-04 18:59:05+00:00,"['Hao-Yu Hsu', 'Zhi-Hao Lin', 'Albert Zhai', 'Hongchi Xia', 'Shenlong Wang']",http://arxiv.org/abs/2411.02394v1
StoryAgent: Customized Storytelling Video Generation via Multi-Agent Collaboration,"The advent of AI-Generated Content (AIGC) has spurred research into automated
video generation to streamline conventional processes. However, automating
storytelling video production, particularly for customized narratives, remains
challenging due to the complexity of maintaining subject consistency across
shots. While existing approaches like Mora and AesopAgent integrate multiple
agents for Story-to-Video (S2V) generation, they fall short in preserving
protagonist consistency and supporting Customized Storytelling Video Generation
(CSVG). To address these limitations, we propose StoryAgent, a multi-agent
framework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks
assigned to specialized agents, mirroring the professional production process.
Notably, our framework includes agents for story design, storyboard generation,
video creation, agent coordination, and result evaluation. Leveraging the
strengths of different models, StoryAgent enhances control over the generation
process, significantly improving character consistency. Specifically, we
introduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance
intra-shot temporal consistency, while a novel storyboard generation pipeline
is proposed to maintain subject consistency across shots. Extensive experiments
demonstrate the effectiveness of our approach in synthesizing highly consistent
storytelling videos, outperforming state-of-the-art methods. Our contributions
include the introduction of StoryAgent, a versatile framework for video
generation tasks, and novel techniques for preserving protagonist consistency.",2024-11-07 18:00:33+00:00,"['Panwen Hu', 'Jin Jiang', 'Jianqi Chen', 'Mingfei Han', 'Shengcai Liao', 'Xiaojun Chang', 'Xiaodan Liang']",http://arxiv.org/abs/2411.04925v2
AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era,"Animation has gained significant interest in the recent film and TV industry.
Despite the success of advanced video generation models like Sora, Kling, and
CogVideoX in generating natural videos, they lack the same effectiveness in
handling animation videos. Evaluating animation video generation is also a
great challenge due to its unique artist styles, violating the laws of physics
and exaggerated motions. In this paper, we present a comprehensive system,
AniSora, designed for animation video generation, which includes a data
processing pipeline, a controllable generation model, and an evaluation
dataset. Supported by the data processing pipeline with over 10M high-quality
data, the generation model incorporates a spatiotemporal mask module to
facilitate key animation production functions such as image-to-video
generation, frame interpolation, and localized image-guided animation. We also
collect an evaluation benchmark of 948 various animation videos, the evaluation
on VBench and human double-blind test demonstrates consistency in character and
motion, achieving state-of-the-art results in animation video generation. Our
evaluation benchmark will be publicly available at
https://github.com/bilibili/Index-anisora.",2024-12-13 16:24:58+00:00,"['Yudong Jiang', 'Baohan Xu', 'Siqian Yang', 'Mingyu Yin', 'Jing Liu', 'Chao Xu', 'Siqi Wang', 'Yidi Wu', 'Bingwen Zhu', 'Xinwen Zhang', 'Xingyu Zheng', 'Jixuan Xu', 'Yue Zhang', 'Jinlong Hou', 'Huyang Sun']",http://arxiv.org/abs/2412.10255v3
Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation,"Recent advances in latent diffusion-based generative models for portrait
image animation, such as Hallo, have achieved impressive results in
short-duration video synthesis. In this paper, we present updates to Hallo,
introducing several design enhancements to extend its capabilities. First, we
extend the method to produce long-duration videos. To address substantial
challenges such as appearance drift and temporal artifacts, we investigate
augmentation strategies within the image space of conditional motion frames.
Specifically, we introduce a patch-drop technique augmented with Gaussian noise
to enhance visual consistency and temporal coherence over long duration.
Second, we achieve 4K resolution portrait video generation. To accomplish this,
we implement vector quantization of latent codes and apply temporal alignment
techniques to maintain coherence across the temporal dimension. By integrating
a high-quality decoder, we realize visual synthesis at 4K resolution. Third, we
incorporate adjustable semantic textual labels for portrait expressions as
conditional inputs. This extends beyond traditional audio cues to improve
controllability and increase the diversity of the generated content. To the
best of our knowledge, Hallo2, proposed in this paper, is the first method to
achieve 4K resolution and generate hour-long, audio-driven portrait image
animations enhanced with textual prompts. We have conducted extensive
experiments to evaluate our method on publicly available datasets, including
HDTF, CelebV, and our introduced ""Wild"" dataset. The experimental results
demonstrate that our approach achieves state-of-the-art performance in
long-duration portrait video animation, successfully generating rich and
controllable content at 4K resolution for duration extending up to tens of
minutes. Project page https://fudan-generative-vision.github.io/hallo2",2024-10-10 08:34:41+00:00,"['Jiahao Cui', 'Hui Li', 'Yao Yao', 'Hao Zhu', 'Hanlin Shang', 'Kaihui Cheng', 'Hang Zhou', 'Siyu Zhu', 'Jingdong Wang']",http://arxiv.org/abs/2410.07718v2
I$^2$VC: A Unified Framework for Intra- & Inter-frame Video Compression,"Video compression aims to reconstruct seamless frames by encoding the motion
and residual information from existing frames. Previous neural video
compression methods necessitate distinct codecs for three types of frames
(I-frame, P-frame and B-frame), which hinders a unified approach and
generalization across different video contexts. Intra-codec techniques lack the
advanced Motion Estimation and Motion Compensation (MEMC) found in inter-codec,
leading to fragmented frameworks lacking uniformity. Our proposed Intra- &
Inter-frame Video Compression (I$^2$VC) framework employs a single
spatio-temporal codec that guides feature compression rates according to
content importance. This unified codec transforms the dependence across frames
into a conditional coding scheme, thus integrating intra- and inter-frame
compression into one cohesive strategy. Given the absence of explicit motion
data, achieving competent inter-frame compression with only a conditional codec
poses a challenge. To resolve this, our approach includes an implicit
inter-frame alignment mechanism. With the pre-trained diffusion denoising
process, the utilization of a diffusion-inverted reference feature rather than
random noise supports the initial compression state. This process allows for
selective denoising of motion-rich regions based on decoded features,
facilitating accurate alignment without the need for MEMC. Our experimental
findings, across various compression configurations (AI, LD and RA) and frame
types, prove that I$^2$VC outperforms the state-of-the-art perceptual learned
codecs. Impressively, it exhibits a 58.4% enhancement in perceptual
reconstruction performance when benchmarked against the H.266/VVC standard
(VTM). Official implementation can be found at https://github.com/GYukai/I2VC.",2024-05-23 09:07:35+00:00,"['Meiqin Liu', 'Chenming Xu', 'Yukai Gu', 'Chao Yao', 'Yao Zhao']",http://arxiv.org/abs/2405.14336v3
DTSGAN: Learning Dynamic Textures via Spatiotemporal Generative Adversarial Network,"Dynamic texture synthesis aims to generate sequences that are visually
similar to a reference video texture and exhibit specific stationary properties
in time. In this paper, we introduce a spatiotemporal generative adversarial
network (DTSGAN) that can learn from a single dynamic texture by capturing its
motion and content distribution. With the pipeline of DTSGAN, a new video
sequence is generated from the coarsest scale to the finest one. To avoid mode
collapse, we propose a novel strategy for data updates that helps improve the
diversity of generated results. Qualitative and quantitative experiments show
that our model is able to generate high quality dynamic textures and natural
motion.",2024-12-22 09:49:48+00:00,"['Xiangtian Li', 'Xiaobo Wang', 'Zhen Qi', 'Han Cao', 'Zhaoyang Zhang', 'Ao Xiang']",http://arxiv.org/abs/2412.16948v1
Accelerated Image-Aware Generative Diffusion Modeling,"We propose in this paper an analytically new construct of a diffusion model
whose drift and diffusion parameters yield an exponentially time-decaying
Signal to Noise Ratio in the forward process. In reverse, the construct
cleverly carries out the learning of the diffusion coefficients on the
structure of clean images using an autoencoder. The proposed methodology
significantly accelerates the diffusion process, reducing the required
diffusion time steps from around 1000 seen in conventional models to 200-500
without compromising image quality in the reverse-time diffusion. In a
departure from conventional models which typically use time-consuming multiple
runs, we introduce a parallel data-driven model to generate a reverse-time
diffusion trajectory in a single run of the model. The resulting collective
block-sequential generative model eliminates the need for MCMC-based
sub-sampling correction for safeguarding and improving image quality, to
further improve the acceleration of image generation. Collectively, these
advancements yield a generative model that is an order of magnitude faster than
conventional approaches, while maintaining high fidelity and diversity in
generated images, hence promising widespread applicability in rapid image
synthesis tasks.",2024-08-15 17:58:55+00:00,"['Tanmay Asthana', 'Yufang Bao', 'Hamid Krim']",http://arxiv.org/abs/2408.08306v1
A Literature Review on Fetus Brain Motion Correction in MRI,"This paper provides a comprehensive review of the latest advancements in
fetal motion correction in MRI. We delve into various contemporary
methodologies and technological advancements aimed at overcoming these
challenges. It includes traditional 3D fetal MRI correction methods like Slice
to Volume Registration (SVR), deep learning-based techniques such as
Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) Networks,
Transformers, Generative Adversarial Networks (GANs) and most recent
advancements of Diffusion Models. The insights derived from this literature
review reflect a thorough understanding of both the technical intricacies and
practical implications of fetal motion in MRI studies, offering a reasoned
perspective on potential solutions and future improvements in this field.",2024-01-30 06:43:40+00:00,"['Haoran Zhang', 'Yun Wang']",http://arxiv.org/abs/2401.16782v1
Disrupting Style Mimicry Attacks on Video Imagery,"Generative AI models are often used to perform mimicry attacks, where a
pretrained model is fine-tuned on a small sample of images to learn to mimic a
specific artist of interest. While researchers have introduced multiple
anti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence
points to a growing trend of mimicry models using videos as sources of training
data. This paper presents our experiences exploring techniques to disrupt style
mimicry on video imagery. We first validate that mimicry attacks can succeed by
training on individual frames extracted from videos. We show that while
anti-mimicry tools can offer protection when applied to individual frames, this
approach is vulnerable to an adaptive countermeasure that removes protection by
exploiting randomness in optimization results of consecutive (nearly-identical)
frames. We develop a new, tool-agnostic framework that segments videos into
short scenes based on frame-level similarity, and use a per-scene optimization
baseline to remove inter-frame randomization while reducing computational cost.
We show via both image level metrics and an end-to-end user study that the
resulting protection restores protection against mimicry (including the
countermeasure). Finally, we develop another adaptive countermeasure and find
that it falls short against our framework.",2024-05-11 01:40:19+00:00,"['Josephine Passananti', 'Stanley Wu', 'Shawn Shan', 'Haitao Zheng', 'Ben Y. Zhao']",http://arxiv.org/abs/2405.06865v1
Global Motion Understanding in Large-Scale Video Object Segmentation,"In this paper, we show that transferring knowledge from other domains of
video understanding combined with large-scale learning can improve robustness
of Video Object Segmentation (VOS) under complex circumstances. Namely, we
focus on integrating scene global motion knowledge to improve large-scale
semi-supervised Video Object Segmentation. Prior works on VOS mostly rely on
direct comparison of semantic and contextual features to perform dense matching
between current and past frames, passing over actual motion structure. On the
other hand, Optical Flow Estimation task aims to approximate the scene motion
field, exposing global motion patterns which are typically undiscoverable
during all pairs similarity search. We present WarpFormer, an architecture for
semi-supervised Video Object Segmentation that exploits existing knowledge in
motion understanding to conduct smoother propagation and more accurate
matching. Our framework employs a generic pretrained Optical Flow Estimation
network whose prediction is used to warp both past frames and instance
segmentation masks to the current frame domain. Consequently, warped
segmentation masks are refined and fused together aiming to inpaint occluded
regions and eliminate artifacts caused by flow field imperfects. Additionally,
we employ novel large-scale MOSE 2023 dataset to train model on various complex
scenarios. Our method demonstrates strong performance on DAVIS 2016/2017
validation (93.0% and 85.9%), DAVIS 2017 test-dev (80.6%) and YouTube-VOS 2019
validation (83.8%) that is competitive with alternative state-of-the-art
methods while using much simpler memory mechanism and instance understanding
logic.",2024-05-11 15:09:22+00:00,"['Volodymyr Fedynyak', 'Yaroslav Romanus', 'Oles Dobosevych', 'Igor Babin', 'Roman Riazantsev']",http://arxiv.org/abs/2405.07031v1
T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation,"Text-to-video (T2V) generative models have advanced significantly, yet their
ability to compose different objects, attributes, actions, and motions into a
video remains unexplored. Previous text-to-video benchmarks also neglect this
important ability for evaluation. In this work, we conduct the first systematic
study on compositional text-to-video generation. We propose T2V-CompBench, the
first benchmark tailored for compositional text-to-video generation.
T2V-CompBench encompasses diverse aspects of compositionality, including
consistent attribute binding, dynamic attribute binding, spatial relationships,
motion binding, action binding, object interactions, and generative numeracy.
We further carefully design evaluation metrics of multimodal large language
model (MLLM)-based, detection-based, and tracking-based metrics, which can
better reflect the compositional text-to-video generation quality of seven
proposed categories with 1400 text prompts. The effectiveness of the proposed
metrics is verified by correlation with human evaluations. We also benchmark
various text-to-video generative models and conduct in-depth analysis across
different models and various compositional categories. We find that
compositional text-to-video generation is highly challenging for current
models, and we hope our attempt could shed light on future research in this
direction.",2024-07-19 17:58:36+00:00,"['Kaiyue Sun', 'Kaiyi Huang', 'Xian Liu', 'Yue Wu', 'Zihan Xu', 'Zhenguo Li', 'Xihui Liu']",http://arxiv.org/abs/2407.14505v2
Synthesizing Audio from Silent Video using Sequence to Sequence Modeling,"Generating audio from a video's visual context has multiple practical
applications in improving how we interact with audio-visual media - for
example, enhancing CCTV footage analysis, restoring historical videos (e.g.,
silent movies), and improving video generation models. We propose a novel
method to generate audio from video using a sequence-to-sequence model,
improving on prior work that used CNNs and WaveNet and faced sound diversity
and generalization challenges. Our approach employs a 3D Vector Quantized
Variational Autoencoder (VQ-VAE) to capture the video's spatial and temporal
structures, decoding with a custom audio decoder for a broader range of sounds.
Trained on the Youtube8M dataset segment, focusing on specific domains, our
model aims to enhance applications like CCTV footage analysis, silent movie
restoration, and video generation models.",2024-04-25 22:19:42+00:00,"['Hugo Garrido-Lestache Belinchon', 'Helina Mulugeta', 'Adam Haile']",http://arxiv.org/abs/2404.17608v1
Method and Software Tool for Generating Artificial Databases of Biomedical Images Based on Deep Neural Networks,"A wide variety of biomedical image data, as well as methods for generating
training images using basic deep neural networks, were analyzed. Additionally,
all platforms for creating images were analyzed, considering their
characteristics. The article develops a method for generating artificial
biomedical images based on GAN. GAN architecture has been developed for
biomedical image synthesis. The data foundation and module for generating
training images were designed and implemented in a software system. A
comparison of the generated image database with known databases was made.",2024-05-25 08:15:21+00:00,"['Oleh Berezsky', 'Petro Liashchynskyi', 'Oleh Pitsun', 'Grygoriy Melnyk']",http://arxiv.org/abs/2405.16119v1
GVDIFF: Grounded Text-to-Video Generation with Diffusion Models,"In text-to-video (T2V) generation, significant attention has been directed
toward its development, yet unifying discrete and continuous grounding
conditions in T2V generation remains under-explored. This paper proposes a
Grounded text-to-Video generation framework, termed GVDIFF. First, we inject
the grounding condition into the self-attention through an uncertainty-based
representation to explicitly guide the focus of the network. Second, we
introduce a spatial-temporal grounding layer that connects the grounding
condition with target objects and enables the model with the grounded
generation capacity in the spatial-temporal domain. Third, our dynamic gate
network adaptively skips the redundant grounding process to selectively extract
grounding information and semantics while improving efficiency. We extensively
evaluate the grounded generation capacity of GVDIFF and demonstrate its
versatility in applications, including long-range video generation, sequential
prompts, and object-specific editing.",2024-07-02 03:36:23+00:00,"['Huanzhang Dou', 'Ruixiang Li', 'Wei Su', 'Xi Li']",http://arxiv.org/abs/2407.01921v2
Audio-Synchronized Visual Animation,"Current visual generation methods can produce high quality videos guided by
texts. However, effectively controlling object dynamics remains a challenge.
This work explores audio as a cue to generate temporally synchronized image
animations. We introduce Audio Synchronized Visual Animation (ASVA), a task
animating a static image to demonstrate motion dynamics, temporally guided by
audio clips across multiple classes. To this end, we present AVSync15, a
dataset curated from VGGSound with videos featuring synchronized audio visual
events across 15 categories. We also present a diffusion model, AVSyncD,
capable of generating dynamic animations guided by audios. Extensive
evaluations validate AVSync15 as a reliable benchmark for synchronized
generation and demonstrate our models superior performance. We further explore
AVSyncDs potential in a variety of audio synchronized generation tasks, from
generating full videos without a base image to controlling object motions with
various sounds. We hope our established benchmark can open new avenues for
controllable visual generation. More videos on project webpage
https://lzhangbj.github.io/projects/asva/asva.html.",2024-03-08 20:17:34+00:00,"['Lin Zhang', 'Shentong Mo', 'Yijing Zhang', 'Pedro Morgado']",http://arxiv.org/abs/2403.05659v2
Discrete to Continuous: Generating Smooth Transition Poses from Sign Language Observation,"Generating continuous sign language videos from discrete segments is
challenging due to the need for smooth transitions that preserve natural flow
and meaning. Traditional approaches that simply concatenate isolated signs
often result in abrupt transitions, disrupting video coherence. To address
this, we propose a novel framework, Sign-D2C, that employs a conditional
diffusion model to synthesize contextually smooth transition frames, enabling
the seamless construction of continuous sign language sequences. Our approach
transforms the unsupervised problem of transition frame generation into a
supervised training task by simulating the absence of transition frames through
random masking of segments in long-duration sign videos. The model learns to
predict these masked frames by denoising Gaussian noise, conditioned on the
surrounding sign observations, allowing it to handle complex, unstructured
transitions. During inference, we apply a linearly interpolating padding
strategy that initializes missing frames through interpolation between boundary
frames, providing a stable foundation for iterative refinement by the diffusion
model. Extensive experiments on the PHOENIX14T, USTC-CSL100, and USTC-SLR500
datasets demonstrate the effectiveness of our method in producing continuous,
natural sign language videos.",2024-11-25 15:06:49+00:00,"['Shengeng Tang', 'Jiayi He', 'Lechao Cheng', 'Jingjing Wu', 'Dan Guo', 'Richang Hong']",http://arxiv.org/abs/2411.16810v1
Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images,"3D GAN inversion aims to project a single image into the latent space of a 3D
Generative Adversarial Network (GAN), thereby achieving 3D geometry
reconstruction. While there exist encoders that achieve good results in 3D GAN
inversion, they are predominantly built on EG3D, which specializes in
synthesizing near-frontal views and is limiting in synthesizing comprehensive
3D scenes from diverse viewpoints. In contrast to existing approaches, we
propose a novel framework built on PanoHead, which excels in synthesizing
images from a 360-degree perspective. To achieve realistic 3D modeling of the
input image, we introduce a dual encoder system tailored for high-fidelity
reconstruction and realistic generation from different viewpoints. Accompanying
this, we propose a stitching framework on the triplane domain to get the best
predictions from both. To achieve seamless stitching, both encoders must output
consistent results despite being specialized for different tasks. For this
reason, we carefully train these encoders using specialized losses, including
an adversarial loss based on our novel occlusion-aware triplane discriminator.
Experiments reveal that our approach surpasses the existing encoder training
methods qualitatively and quantitatively. Please visit the project page:
https://berkegokmen1.github.io/dual-enc-3d-gan-inv.",2024-09-30 17:30:23+00:00,"['Bahri Batuhan Bilecen', 'Ahmet Berke Gokmen', 'Aysegul Dundar']",http://arxiv.org/abs/2409.20530v1
STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering,"Recently we have witnessed the rapid development of video question answering
models. However, most models can only handle simple videos in terms of temporal
reasoning, and their performance tends to drop when answering
temporal-reasoning questions on long and informative videos. To tackle this
problem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable
Intermediate Results for video question answering. STAIR is a neural module
network, which contains a program generator to decompose a given question into
a hierarchical combination of several sub-tasks, and a set of lightweight
neural modules to complete each of these sub-tasks. Though neural module
networks are already widely studied on image-text tasks, applying them to
videos is a non-trivial task, as reasoning on videos requires different
abilities. In this paper, we define a set of basic video-text sub-tasks for
video question answering and design a set of lightweight modules to complete
them. Different from most prior works, modules of STAIR return intermediate
outputs specific to their intentions instead of always returning attention
maps, which makes it easier to interpret and collaborate with pre-trained
models. We also introduce intermediate supervision to make these intermediate
outputs more accurate. We conduct extensive experiments on several video
question answering datasets under various settings to show STAIR's performance,
explainability, compatibility with pre-trained models, and applicability when
program annotations are not available. Code:
https://github.com/yellow-binary-tree/STAIR",2024-01-08 14:01:59+00:00,"['Yueqian Wang', 'Yuxuan Wang', 'Kai Chen', 'Dongyan Zhao']",http://arxiv.org/abs/2401.03901v1
NERV++: An Enhanced Implicit Neural Video Representation,"Neural fields, also known as implicit neural representations (INRs), have
shown a remarkable capability of representing, generating, and manipulating
various data types, allowing for continuous data reconstruction at a low memory
footprint. Though promising, INRs applied to video compression still need to
improve their rate-distortion performance by a large margin, and require a huge
number of parameters and long training iterations to capture high-frequency
details, limiting their wider applicability. Resolving this problem remains a
quite challenging task, which would make INRs more accessible in compression
tasks. We take a step towards resolving these shortcomings by introducing
neural representations for videos NeRV++, an enhanced implicit neural video
representation, as more straightforward yet effective enhancement over the
original NeRV decoder architecture, featuring separable conv2d residual blocks
(SCRBs) that sandwiches the upsampling block (UB), and a bilinear interpolation
skip layer for improved feature representation. NeRV++ allows videos to be
directly represented as a function approximated by a neural network, and
significantly enhance the representation capacity beyond current INR-based
video codecs. We evaluate our method on UVG, MCL JVC, and Bunny datasets,
achieving competitive results for video compression with INRs. This achievement
narrows the gap to autoencoder-based video coding, marking a significant stride
in INR-based video compression research.",2024-02-28 13:00:32+00:00,"['Ahmed Ghorbel', 'Wassim Hamidouche', 'Luce Morin']",http://arxiv.org/abs/2402.18305v1
VQ-NeRV: A Vector Quantized Neural Representation for Videos,"Implicit neural representations (INR) excel in encoding videos within neural
networks, showcasing promise in computer vision tasks like video compression
and denoising. INR-based approaches reconstruct video frames from
content-agnostic embeddings, which hampers their efficacy in video frame
regression and restricts their generalization ability for video interpolation.
To address these deficiencies, Hybrid Neural Representation for Videos (HNeRV)
was introduced with content-adaptive embeddings. Nevertheless, HNeRV's
compression ratios remain relatively low, attributable to an oversight in
leveraging the network's shallow features and inter-frame residual information.
In this work, we introduce an advanced U-shaped architecture, Vector
Quantized-NeRV (VQ-NeRV), which integrates a novel component--the VQ-NeRV
Block. This block incorporates a codebook mechanism to discretize the network's
shallow residual features and inter-frame residual information effectively.
This approach proves particularly advantageous in video compression, as it
results in smaller size compared to quantized features. Furthermore, we
introduce an original codebook optimization technique, termed shallow codebook
optimization, designed to refine the utility and efficiency of the codebook.
The experimental evaluations indicate that VQ-NeRV outperforms HNeRV on video
regression tasks, delivering superior reconstruction quality (with an increase
of 1-2 dB in Peak Signal-to-Noise Ratio (PSNR)), better bit per pixel (bpp)
efficiency, and improved video inpainting outcomes.",2024-03-19 03:19:07+00:00,"['Yunjie Xu', 'Xiang Feng', 'Feiwei Qin', 'Ruiquan Ge', 'Yong Peng', 'Changmiao Wang']",http://arxiv.org/abs/2403.12401v1
V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation Localization and Copyright Protection,"AI-generated video has revolutionized short video production, filmmaking, and
personalized media, making video local editing an essential tool. However, this
progress also blurs the line between reality and fiction, posing challenges in
multimedia forensics. To solve this urgent issue, V2A-Mark is proposed to
address the limitations of current video tampering forensics, such as poor
generalizability, singular function, and single modality focus. Combining the
fragility of video-into-video steganography with deep robust watermarking, our
method can embed invisible visual-audio localization watermarks and copyright
watermarks into the original video frames and audio, enabling precise
manipulation localization and copyright protection. We also design a temporal
alignment and fusion module and degradation prompt learning to enhance the
localization accuracy and decoding robustness. Meanwhile, we introduce a
sample-level audio localization method and a cross-modal copyright extraction
mechanism to couple the information of audio and video frames. The
effectiveness of V2A-Mark has been verified on a visual-audio tampering
dataset, emphasizing its superiority in localization precision and copyright
accuracy, crucial for the sustainable development of video editing in the AIGC
video era.",2024-04-25 17:59:45+00:00,"['Xuanyu Zhang', 'Youmin Xu', 'Runyi Li', 'Jiwen Yu', 'Weiqi Li', 'Zhipei Xu', 'Jian Zhang']",http://arxiv.org/abs/2404.16824v4
MLLM as Video Narrator: Mitigating Modality Imbalance in Video Moment Retrieval,"Video Moment Retrieval (VMR) aims to localize a specific temporal segment
within an untrimmed long video given a natural language query. Existing methods
often suffer from inadequate training annotations, i.e., the sentence typically
matches with a fraction of the prominent video content in the foreground with
limited wording diversity. This intrinsic modality imbalance leaves a
considerable portion of visual information remaining unaligned with text. It
confines the cross-modal alignment knowledge within the scope of a limited text
corpus, thereby leading to sub-optimal visual-textual modeling and poor
generalizability. By leveraging the visual-textual understanding capability of
multi-modal large language models (MLLM), in this work, we take an MLLM as a
video narrator to generate plausible textual descriptions of the video, thereby
mitigating the modality imbalance and boosting the temporal localization. To
effectively maintain temporal sensibility for localization, we design to get
text narratives for each certain video timestamp and construct a structured
text paragraph with time information, which is temporally aligned with the
visual content. Then we perform cross-modal feature merging between the
temporal-aware narratives and corresponding video temporal features to produce
semantic-enhanced video representation sequences for query localization.
Subsequently, we introduce a uni-modal narrative-query matching mechanism,
which encourages the model to extract complementary information from contextual
cohesive descriptions for improved retrieval. Extensive experiments on two
benchmarks show the effectiveness and generalizability of our proposed method.",2024-06-25 18:39:43+00:00,"['Weitong Cai', 'Jiabo Huang', 'Shaogang Gong', 'Hailin Jin', 'Yang Liu']",http://arxiv.org/abs/2406.17880v1
Cuboid-Net: A Multi-Branch Convolutional Neural Network for Joint Space-Time Video Super Resolution,"The demand for high-resolution videos has been consistently rising across
various domains, propelled by continuous advancements in science, technology,
and societal. Nonetheless, challenges arising from limitations in imaging
equipment capabilities, imaging conditions, as well as economic and temporal
factors often result in obtaining low-resolution images in particular
situations. Space-time video super-resolution aims to enhance the spatial and
temporal resolutions of low-resolution and low-frame-rate videos. The currently
available space-time video super-resolution methods often fail to fully exploit
the abundant information existing within the spatio-temporal domain. To address
this problem, we tackle the issue by conceptualizing the input low-resolution
video as a cuboid structure. Drawing on this perspective, we introduce an
innovative methodology called ""Cuboid-Net,"" which incorporates a multi-branch
convolutional neural network. Cuboid-Net is designed to collectively enhance
the spatial and temporal resolutions of videos, enabling the extraction of rich
and meaningful information across both spatial and temporal dimensions.
Specifically, we take the input video as a cuboid to generate different
directional slices as input for different branches of the network. The proposed
network contains four modules, i.e., a multi-branch-based hybrid feature
extraction (MBFE) module, a multi-branch-based reconstruction (MBR) module, a
first stage quality enhancement (QE) module, and a second stage cross frame
quality enhancement (CFQE) module for interpolated frames only. Experimental
results demonstrate that the proposed method is not only effective for spatial
and temporal super-resolution of video but also for spatial and angular
super-resolution of light field.",2024-07-24 04:05:20+00:00,"['Congrui Fu', 'Hui Yuan', 'Hongji Xu', 'Hao Zhang', 'Liquan Shen']",http://arxiv.org/abs/2407.16986v1
VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges,"Recent advancements in large-scale video-language models have shown
significant potential for real-time planning and detailed interactions.
However, their high computational demands and the scarcity of annotated
datasets limit their practicality for academic researchers. In this work, we
introduce VideoLLaMB, a novel framework that utilizes temporal memory tokens
within bridge layers to allow for the encoding of entire video sequences
alongside historical visual data, effectively preserving semantic continuity
and enhancing model performance across various tasks. This approach includes
recurrent memory tokens and a SceneTilling algorithm, which segments videos
into independent semantic units to preserve semantic integrity. Empirically,
VideoLLaMB significantly outstrips existing video-language models,
demonstrating a 5.5 points improvement over its competitors across three
VideoQA benchmarks, and 2.06 points on egocentric planning. Comprehensive
results on the MVBench show that VideoLLaMB-7B achieves markedly better results
than previous 7B models of same LLM. Remarkably, it maintains robust
performance as PLLaVA even as video length increases up to 8 times. Besides,
the frame retrieval results on our specialized Needle in a Video Haystack
(NIAVH) benchmark, further validate VideoLLaMB's prowess in accurately
identifying specific frames within lengthy videos. Our SceneTilling algorithm
also enables the generation of streaming video captions directly, without
necessitating additional training. In terms of efficiency, VideoLLaMB, trained
on 16 frames, supports up to 320 frames on a single Nvidia A100 GPU with linear
GPU memory scaling, ensuring both high performance and cost-effectiveness,
thereby setting a new foundation for long-form video-language models in both
academic and practical applications.",2024-09-02 08:52:58+00:00,"['Yuxuan Wang', 'Cihang Xie', 'Yang Liu', 'Zilong Zheng']",http://arxiv.org/abs/2409.01071v1
A Low-Computational Video Synopsis Framework with a Standard Dataset,"Video synopsis is an efficient method for condensing surveillance videos.
This technique begins with the detection and tracking of objects, followed by
the creation of object tubes. These tubes consist of sequences, each containing
chronologically ordered bounding boxes of a unique object. To generate a
condensed video, the first step involves rearranging the object tubes to
maximize the number of non-overlapping objects in each frame. Then, these tubes
are stitched to a background image extracted from the source video. The lack of
a standard dataset for the video synopsis task hinders the comparison of
different video synopsis models. This paper addresses this issue by introducing
a standard dataset, called SynoClip, designed specifically for the video
synopsis task. SynoClip includes all the necessary features needed to evaluate
various models directly and effectively. Additionally, this work introduces a
video synopsis model, called FGS, with low computational cost. The model
includes an empty-frame object detector to identify frames empty of any
objects, facilitating efficient utilization of the deep object detector.
Moreover, a tube grouping algorithm is proposed to maintain relationships among
tubes in the synthesized video. This is followed by a greedy tube rearrangement
algorithm, which efficiently determines the start time of each tube. Finally,
the proposed model is evaluated using the proposed dataset. The source code,
fine-tuned object detection model, and tutorials are available at
https://github.com/Ramtin-ma/VideoSynopsis-FGS.",2024-09-08 22:08:36+00:00,"['Ramtin Malekpour', 'M. Mehrdad Morsali', 'Hoda Mohammadzade']",http://arxiv.org/abs/2409.05230v1
Deepfake detection in videos with multiple faces using geometric-fakeness features,"Due to the development of facial manipulation techniques in recent years
deepfake detection in video stream became an important problem for face
biometrics, brand monitoring or online video conferencing solutions. In case of
a biometric authentication, if you replace a real datastream with a deepfake,
you can bypass a liveness detection system. Using a deepfake in a video
conference, you can penetrate into a private meeting. Deepfakes of victims or
public figures can also be used by fraudsters for blackmailing, extorsion and
financial fraud. Therefore, the task of detecting deepfakes is relevant to
ensuring privacy and security. In existing approaches to a deepfake detection
their performance deteriorates when multiple faces are present in a video
simultaneously or when there are other objects erroneously classified as faces.
In our research we propose to use geometric-fakeness features (GFF) that
characterize a dynamic degree of a face presence in a video and its per-frame
deepfake scores. To analyze temporal inconsistencies in GFFs between the frames
we train a complex deep learning model that outputs a final deepfake
prediction. We employ our approach to analyze videos with multiple faces that
are simultaneously present in a video. Such videos often occur in practice
e.g., in an online video conference. In this case, real faces appearing in a
frame together with a deepfake face will significantly affect a deepfake
detection and our approach allows to counter this problem. Through extensive
experiments we demonstrate that our approach outperforms current
state-of-the-art methods on popular benchmark datasets such as FaceForensics++,
DFDC, Celeb-DF and WildDeepFake. The proposed approach remains accurate when
trained to detect multiple different deepfake generation techniques.",2024-10-10 13:10:34+00:00,"['Kirill Vyshegorodtsev', 'Dmitry Kudiyarov', 'Alexander Balashov', 'Alexander Kuzmin']",http://arxiv.org/abs/2410.07888v1
Free Video-LLM: Prompt-guided Visual Perception for Efficient Training-free Video LLMs,"Vision-language large models have achieved remarkable success in various
multi-modal tasks, yet applying them to video understanding remains challenging
due to the inherent complexity and computational demands of video data. While
training-based video-LLMs deliver high performance, they often require
substantial resources for training and inference. Conversely, training-free
approaches offer a more efficient alternative by adapting pre-trained
image-LLMs models for video tasks without additional training, but they face
inference efficiency bottlenecks due to the large number of visual tokens
generated from video frames. In this work, we present a novel prompt-guided
visual perception framework (abbreviated as Free Video-LLM) for efficient
inference of training-free video LLMs. The proposed framework decouples
spatial-temporal dimension and performs temporal frame sampling and spatial RoI
cropping respectively based on task-specific prompts. Our method effectively
reduces the number of visual tokens while maintaining high performance across
multiple video question-answering benchmarks. Extensive experiments demonstrate
that our approach achieves competitive results with significantly fewer tokens,
offering an optimal trade-off between accuracy and computational efficiency
compared to state-of-the-art video LLMs. The code will be available at
https://github.com/contrastive/FreeVideoLLM.",2024-10-14 12:35:12+00:00,"['Kai Han', 'Jianyuan Guo', 'Yehui Tang', 'Wei He', 'Enhua Wu', 'Yunhe Wang']",http://arxiv.org/abs/2410.10441v2
Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension,"Existing large video-language models (LVLMs) struggle to comprehend long
videos correctly due to limited context. To address this problem, fine-tuning
long-context LVLMs and employing GPT-based agents have emerged as promising
solutions. However, fine-tuning LVLMs would require extensive high-quality data
and substantial GPU resources, while GPT-based agents would rely on proprietary
models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented
Generation (Video-RAG), a training-free and cost-effective pipeline that
employs visually-aligned auxiliary texts to help facilitate cross-modality
alignment while providing additional information beyond the visual content.
Specifically, we leverage open-source external tools to extract
visually-aligned information from pure video data (e.g., audio, optical
character, and object detection), and incorporate the extracted information
into an existing LVLM as auxiliary texts, alongside video frames and queries,
in a plug-and-play manner. Our Video-RAG offers several key advantages: (i)
lightweight with low computing overhead due to single-turn retrieval; (ii) easy
implementation and compatibility with any LVLM; and (iii) significant,
consistent performance gains across long video understanding benchmarks,
including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates
superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o
when utilized with a 72B model.",2024-11-20 07:44:34+00:00,"['Yongdong Luo', 'Xiawu Zheng', 'Xiao Yang', 'Guilin Li', 'Haojia Lin', 'Jinfa Huang', 'Jiayi Ji', 'Fei Chao', 'Jiebo Luo', 'Rongrong Ji']",http://arxiv.org/abs/2411.13093v3
Neptune: The Long Orbit to Benchmarking Long Video Understanding,"We introduce Neptune, a benchmark for long video understanding that requires
reasoning over long time horizons and across different modalities. Many
existing video datasets and models are focused on short clips (10s-30s). While
some long video datasets do exist, they can often be solved by powerful image
models applied per frame (and often to very few frames) in a video, and are
usually manually annotated at high cost. In order to mitigate both these
problems, we propose a scalable dataset creation pipeline which leverages large
models (VLMs and LLMs), to automatically generate dense, time-aligned video
captions, as well as tough question answer decoy sets for video segments (up to
15 minutes in length). Our dataset Neptune covers a broad range of long video
reasoning abilities and consists of a subset that emphasizes multimodal
reasoning. Since existing metrics for open-ended question answering are either
rule-based or may rely on proprietary models, we provide a new open source
model-based metric GEM to score open-ended responses on Neptune. Benchmark
evaluations reveal that most current open-source long video models perform
poorly on Neptune, particularly on questions testing temporal ordering,
counting and state changes. Through Neptune, we aim to spur the development of
more advanced models capable of understanding long videos. The dataset is
available at https://github.com/google-deepmind/neptune",2024-12-12 18:54:48+00:00,"['Arsha Nagrani', 'Mingda Zhang', 'Ramin Mehran', 'Rachel Hornung', 'Nitesh Bharadwaj Gundavarapu', 'Nilpa Jha', 'Austin Myers', 'Xingyi Zhou', 'Boqing Gong', 'Cordelia Schmid', 'Mikhail Sirotenko', 'Yukun Zhu', 'Tobias Weyand']",http://arxiv.org/abs/2412.09582v2
Sample-efficient Unsupervised Policy Cloning from Ensemble Self-supervised Labeled Videos,"Current advanced policy learning methodologies have demonstrated the ability
to develop expert-level strategies when provided enough information. However,
their requirements, including task-specific rewards, expert-labeled
trajectories, and huge environmental interactions, can be expensive or even
unavailable in many scenarios. In contrast, humans can efficiently acquire
skills within a few trials and errors by imitating easily accessible internet
video, in the absence of any other supervision. In this paper, we try to let
machines replicate this efficient watching-and-learning process through
Unsupervised Policy from Ensemble Self-supervised labeled Videos (UPESV), a
novel framework to efficiently learn policies from videos without any other
expert supervision. UPESV trains a video labeling model to infer the expert
actions in expert videos, through several organically combined self-supervised
tasks. Each task performs its own duties, and they together enable the model to
make full use of both expert videos and reward-free interactions for advanced
dynamics understanding and robust prediction. Simultaneously, UPESV clones a
policy from the labeled expert videos, in turn collecting environmental
interactions for self-supervised tasks. After a sample-efficient and
unsupervised (i.e., reward-free) training process, an advanced video-imitated
policy is obtained. Extensive experiments in sixteen challenging
procedurally-generated environments demonstrate that the proposed UPESV
achieves state-of-the-art few-shot policy learning (outperforming five current
advanced baselines on 12/16 tasks) without exposure to any other supervision
except videos. Detailed analysis is also provided, verifying the necessity of
each self-supervised task employed in UPESV.",2024-12-14 10:12:22+00:00,"['Xin Liu', 'Yaran Chen']",http://arxiv.org/abs/2412.10778v1
3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos,"Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes
from multi-view videos remains a challenging endeavor. Despite the remarkable
advancements achieved by current neural rendering techniques, these methods
generally require complete video sequences for offline training and are not
capable of real-time rendering. To address these constraints, we introduce
3DGStream, a method designed for efficient FVV streaming of real-world dynamic
scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12
seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D
Gaussians (3DGs) to represent the scene. Instead of the na\""ive approach of
directly optimizing 3DGs per-frame, we employ a compact Neural Transformation
Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing
the training time and storage required for each FVV frame. Furthermore, we
propose an adaptive 3DG addition strategy to handle emerging objects in dynamic
scenes. Experiments demonstrate that 3DGStream achieves competitive performance
in terms of rendering speed, image quality, training time, and model storage
when compared with state-of-the-art methods.",2024-03-03 08:42:40+00:00,"['Jiakai Sun', 'Han Jiao', 'Guangyuan Li', 'Zhanjie Zhang', 'Lei Zhao', 'Wei Xing']",http://arxiv.org/abs/2403.01444v4
VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention,"Current video generation models excel at short clips but fail to produce
cohesive multi-shot narratives due to disjointed visual dynamics and fractured
storylines. Existing solutions either rely on extensive manual
scripting/editing or prioritize single-shot fidelity over cross-scene
continuity, limiting their practicality for movie-like content. We introduce
VideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot
video synthesis from a single sentence by systematically addressing three core
challenges: (1) Narrative Fragmentation: Existing methods lack structured
storytelling. We propose dynamic storyline modeling, which first converts the
user prompt into concise shot descriptions, then elaborates them into detailed,
cinematic specifications across five domains (character dynamics, background
continuity, relationship evolution, camera movements, HDR lighting), ensuring
logical narrative progression with self-validation. (2) Visual Inconsistency:
Existing approaches struggle with maintaining visual consistency across shots.
Our identity-aware cross-shot propagation generates identity-preserving
portrait (IPP) tokens that maintain character fidelity while allowing trait
variations (expressions, aging) dictated by the storyline. (3) Transition
Artifacts: Abrupt shot changes disrupt immersion. Our adjacent latent
transition mechanisms implement boundary-aware reset strategies that process
adjacent shots' features at transition points, enabling seamless visual flow
while preserving narrative continuity. VGoT generates multi-shot videos that
outperform state-of-the-art baselines by 20.4% in within-shot face consistency
and 17.4% in style consistency, while achieving over 100% better cross-shot
consistency and 10x fewer manual adjustments than alternatives.",2024-12-03 08:33:50+00:00,"['Mingzhe Zheng', 'Yongqi Xu', 'Haojian Huang', 'Xuran Ma', 'Yexin Liu', 'Wenjie Shu', 'Yatian Pang', 'Feilong Tang', 'Qifeng Chen', 'Harry Yang', 'Ser-Nam Lim']",http://arxiv.org/abs/2412.02259v2
MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations,"In this paper, we tackle the problem of how to build and benchmark a large
motion model (LMM). The ultimate goal of LMM is to serve as a foundation model
for versatile motion-related tasks, e.g., human motion generation, with
interpretability and generalizability. Though advanced, recent LMM-related
works are still limited by small-scale motion data and costly text
descriptions. Besides, previous motion benchmarks primarily focus on pure body
movements, neglecting the ubiquitous motions in context, i.e., humans
interacting with humans, objects, and scenes. To address these limitations, we
consolidate large-scale video action datasets as knowledge banks to build
MotionBank, which comprises 13 video action datasets, 1.24M motion sequences,
and 132.9M frames of natural and diverse human motions. Different from
laboratory-captured motions, in-the-wild human-centric videos contain abundant
motions in context. To facilitate better motion text alignment, we also
meticulously devise a motion caption generation algorithm to automatically
produce rule-based, unbiased, and disentangled text descriptions via the
kinematic characteristics for each motion. Extensive experiments show that our
MotionBank is beneficial for general motion-related tasks of human motion
generation, motion in-context generation, and motion understanding. Video
motions together with the rule-based text annotations could serve as an
efficient alternative for larger LMMs. Our dataset, codes, and benchmark will
be publicly available at https://github.com/liangxuy/MotionBank.",2024-10-17 17:31:24+00:00,"['Liang Xu', 'Shaoyang Hua', 'Zili Lin', 'Yifan Liu', 'Feipeng Ma', 'Yichao Yan', 'Xin Jin', 'Xiaokang Yang', 'Wenjun Zeng']",http://arxiv.org/abs/2410.13790v1
PEMF-VTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm,"Video Virtual Try-on aims to seamlessly transfer a reference garment onto a
target person in a video while preserving both visual fidelity and temporal
coherence. Existing methods typically rely on inpainting masks to define the
try-on area, enabling accurate garment transfer for simple scenes (e.g.,
in-shop videos). However, these mask-based approaches struggle with complex
real-world scenarios, as overly large and inconsistent masks often destroy
spatial-temporal information, leading to distorted results. Mask-free methods
alleviate this issue but face challenges in accurately determining the try-on
area, especially for videos with dynamic body movements. To address these
limitations, we propose PEMF-VTO, a novel Point-Enhanced Mask-Free Video
Virtual Try-On framework that leverages sparse point alignments to explicitly
guide garment transfer. Our key innovation is the introduction of
point-enhanced guidance, which provides flexible and reliable control over both
spatial-level garment transfer and temporal-level video coherence.
Specifically, we design a Point-Enhanced Transformer (PET) with two core
components: Point-Enhanced Spatial Attention (PSA), which uses frame-cloth
point alignments to precisely guide garment transfer, and Point-Enhanced
Temporal Attention (PTA), which leverages frame-frame point correspondences to
enhance temporal coherence and ensure smooth transitions across frames.
Extensive experiments demonstrate that our PEMF-VTO outperforms
state-of-the-art methods, generating more natural, coherent, and visually
appealing try-on videos, particularly for challenging in-the-wild scenarios.
The link to our paper's homepage is https://pemf-vto.github.io/.",2024-12-04 04:24:15+00:00,"['Tianyu Chang', 'Xiaohao Chen', 'Zhichao Wei', 'Xuanpu Zhang', 'Qing-Guo Chen', 'Weihua Luo', 'Peipei Song', 'Xun Yang']",http://arxiv.org/abs/2412.03021v4
Contextualized Diffusion Models for Text-Guided Image and Video Generation,"Conditional diffusion models have exhibited superior performance in
high-fidelity text-guided visual generation and editing. Nevertheless,
prevailing text-guided visual diffusion models primarily focus on incorporating
text-visual relationships exclusively into the reverse process, often
disregarding their relevance in the forward process. This inconsistency between
forward and reverse processes may limit the precise conveyance of textual
semantics in visual synthesis results. To address this issue, we propose a
novel and general contextualized diffusion model (ContextDiff) by incorporating
the cross-modal context encompassing interactions and alignments between text
condition and visual sample into forward and reverse processes. We propagate
this context to all timesteps in the two processes to adapt their trajectories,
thereby facilitating cross-modal conditional modeling. We generalize our
contextualized diffusion to both DDPMs and DDIMs with theoretical derivations,
and demonstrate the effectiveness of our model in evaluations with two
challenging tasks: text-to-image generation, and text-to-video editing. In each
task, our ContextDiff achieves new state-of-the-art performance, significantly
enhancing the semantic alignment between text condition and generated samples,
as evidenced by quantitative and qualitative evaluations. Our code is available
at https://github.com/YangLing0818/ContextDiff",2024-02-26 15:01:16+00:00,"['Ling Yang', 'Zhilong Zhang', 'Zhaochen Yu', 'Jingwei Liu', 'Minkai Xu', 'Stefano Ermon', 'Bin Cui']",http://arxiv.org/abs/2402.16627v3
Phy124: Fast Physics-Driven 4D Content Generation from a Single Image,"4D content generation focuses on creating dynamic 3D objects that change over
time. Existing methods primarily rely on pre-trained video diffusion models,
utilizing sampling processes or reference videos. However, these approaches
face significant challenges. Firstly, the generated 4D content often fails to
adhere to real-world physics since video diffusion models do not incorporate
physical priors. Secondly, the extensive sampling process and the large number
of parameters in diffusion models result in exceedingly time-consuming
generation processes. To address these issues, we introduce Phy124, a novel,
fast, and physics-driven method for controllable 4D content generation from a
single image. Phy124 integrates physical simulation directly into the 4D
generation process, ensuring that the resulting 4D content adheres to natural
physical laws. Phy124 also eliminates the use of diffusion models during the 4D
dynamics generation phase, significantly speeding up the process. Phy124 allows
for the control of 4D dynamics, including movement speed and direction, by
manipulating external forces. Extensive experiments demonstrate that Phy124
generates high-fidelity 4D content with significantly reduced inference times,
achieving stateof-the-art performance. The code and generated 4D content are
available at the provided link: https://anonymous.4open.science/r/BBF2/.",2024-09-11 10:41:46+00:00,"['Jiajing Lin', 'Zhenzhong Wang', 'Yongjie Hou', 'Yuzhou Tang', 'Min Jiang']",http://arxiv.org/abs/2409.07179v1
Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Video and Multi-view Diffusion Models,"Recent advancements in 3D generation are predominantly propelled by
improvements in 3D-aware image diffusion models. These models are pretrained on
Internet-scale image data and fine-tuned on massive 3D data, offering the
capability of producing highly consistent multi-view images. However, due to
the scarcity of synchronized multi-view video data, it remains challenging to
adapt this paradigm to 4D generation directly. Despite that, the available
video and 3D data are adequate for training video and multi-view diffusion
models separately that can provide satisfactory dynamic and geometric priors
respectively. To take advantage of both, this paper presents Diffusion$^2$, a
novel framework for dynamic 3D content creation that reconciles the knowledge
about geometric consistency and temporal smoothness from these models to
directly sample dense multi-view multi-frame images which can be employed to
optimize continuous 4D representation. Specifically, we design a simple yet
effective denoising strategy via score composition of pretrained video and
multi-view diffusion models based on the probability structure of the target
image array. To alleviate the potential conflicts between two heterogeneous
scores, we further introduce variance-reducing sampling via interpolated steps,
facilitating smooth and stable generation. Owing to the high parallelism of the
proposed image generation process and the efficiency of the modern 4D
reconstruction pipeline, our framework can generate 4D content within few
minutes. Notably, our method circumvents the reliance on expensive and
hard-to-scale 4D data, thereby having the potential to benefit from the scaling
of the foundation video and multi-view diffusion models. Extensive experiments
demonstrate the efficacy of our proposed framework in generating highly
seamless and consistent 4D assets under various types of conditions.",2024-04-02 17:58:03+00:00,"['Zeyu Yang', 'Zijie Pan', 'Chun Gu', 'Li Zhang']",http://arxiv.org/abs/2404.02148v4
STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians,"Recent progress in pre-trained diffusion models and 3D generation have
spurred interest in 4D content creation. However, achieving high-fidelity 4D
generation with spatial-temporal consistency remains a challenge. In this work,
we propose STAG4D, a novel framework that combines pre-trained diffusion models
with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing
inspiration from 3D generation techniques, we utilize a multi-view diffusion
model to initialize multi-view images anchoring on the input video frames,
where the video can be either real-world captured or generated by a video
diffusion model. To ensure the temporal consistency of the multi-view sequence
initialization, we introduce a simple yet effective fusion strategy to leverage
the first frame as a temporal anchor in the self-attention computation. With
the almost consistent multi-view sequences, we then apply the score
distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian
spatting is specially crafted for the generation task, where an adaptive
densification strategy is proposed to mitigate the unstable Gaussian gradient
for robust optimization. Notably, the proposed pipeline does not require any
pre-training or fine-tuning of diffusion networks, offering a more accessible
and practical solution for the 4D generation task. Extensive experiments
demonstrate that our method outperforms prior 4D generation works in rendering
quality, spatial-temporal consistency, and generation robustness, setting a new
state-of-the-art for 4D generation from diverse inputs, including text, image,
and video.",2024-03-22 04:16:33+00:00,"['Yifei Zeng', 'Yanqin Jiang', 'Siyu Zhu', 'Yuanxun Lu', 'Youtian Lin', 'Hao Zhu', 'Weiming Hu', 'Xun Cao', 'Yao Yao']",http://arxiv.org/abs/2403.14939v1
StyleMaster: Stylize Your Video with Artistic Generation and Translation,"Style control has been popular in video generation models. Existing methods
often generate videos far from the given style, cause content leakage, and
struggle to transfer one video to the desired style. Our first observation is
that the style extraction stage matters, whereas existing methods emphasize
global style but ignore local textures. In order to bring texture features
while preventing content leakage, we filter content-related patches while
retaining style ones based on prompt-patch similarity; for global style
extraction, we generate a paired style dataset through model illusion to
facilitate contrastive learning, which greatly enhances the absolute style
consistency. Moreover, to fill in the image-to-video gap, we train a
lightweight motion adapter on still videos, which implicitly enhances
stylization extent, and enables our image-trained model to be seamlessly
applied to videos. Benefited from these efforts, our approach, StyleMaster, not
only achieves significant improvement in both style resemblance and temporal
coherence, but also can easily generalize to video style transfer with a gray
tile ControlNet. Extensive experiments and visualizations demonstrate that
StyleMaster significantly outperforms competitors, effectively generating
high-quality stylized videos that align with textual content and closely
resemble the style of reference images. Our project page is at
https://zixuan-ye.github.io/stylemaster",2024-12-10 18:44:08+00:00,"['Zixuan Ye', 'Huijuan Huang', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Wenhan Luo']",http://arxiv.org/abs/2412.07744v1
Spherical World-Locking for Audio-Visual Localization in Egocentric Videos,"Egocentric videos provide comprehensive contexts for user and scene
understanding, spanning multisensory perception to behavioral interaction. We
propose Spherical World-Locking (SWL) as a general framework for egocentric
scene representation, which implicitly transforms multisensory streams with
respect to measurements of head orientation. Compared to conventional
head-locked egocentric representations with a 2D planar field-of-view, SWL
effectively offsets challenges posed by self-motion, allowing for improved
spatial synchronization between input modalities. Using a set of multisensory
embeddings on a worldlocked sphere, we design a unified encoder-decoder
transformer architecture that preserves the spherical structure of the scene
representation, without requiring expensive projections between image and world
coordinate systems. We evaluate the effectiveness of the proposed framework on
multiple benchmark tasks for egocentric video understanding, including
audio-visual active speaker localization, auditory spherical source
localization, and behavior anticipation in everyday activities.",2024-08-09 22:29:04+00:00,"['Heeseung Yun', 'Ruohan Gao', 'Ishwarya Ananthabhotla', 'Anurag Kumar', 'Jacob Donley', 'Chao Li', 'Gunhee Kim', 'Vamsi Krishna Ithapu', 'Calvin Murdock']",http://arxiv.org/abs/2408.05364v1
Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels,"Understanding and modeling pedestrian movements in the real world is crucial
for applications like motion forecasting and scene simulation. Many factors
influence pedestrian movements, such as scene context, individual
characteristics, and goals, which are often ignored by the existing human
generation methods. Web videos contain natural pedestrian behavior and rich
motion context, but annotating them with pre-trained predictors leads to noisy
labels. In this work, we propose learning diverse pedestrian movements from web
videos. We first curate a large-scale dataset called CityWalkers that captures
diverse real-world pedestrian movements in urban scenes. Then, based on
CityWalkers, we propose a generative model called PedGen for diverse pedestrian
movement generation. PedGen introduces automatic label filtering to remove the
low-quality labels and a mask embedding to train with partial labels. It also
contains a novel context encoder that lifts the 2D scene context to 3D and can
incorporate various context factors in generating realistic pedestrian
movements in urban scenes. Experiments show that PedGen outperforms existing
baseline methods for pedestrian movement generation by learning from noisy
labels and incorporating the context factors. In addition, PedGen achieves
zero-shot generalization in both real-world and simulated environments. The
code, model, and data will be made publicly available at
https://genforce.github.io/PedGen/ .",2024-10-10 00:25:17+00:00,"['Zhizheng Liu', 'Joe Lin', 'Wayne Wu', 'Bolei Zhou']",http://arxiv.org/abs/2410.07500v1
Motion Inversion for Video Customization,"In this work, we present a novel approach for motion customization in video
generation, addressing the widespread gap in the exploration of motion
representation within video generative models. Recognizing the unique
challenges posed by the spatiotemporal nature of video, our method introduces
Motion Embeddings, a set of explicit, temporally coherent embeddings derived
from a given video. These embeddings are designed to integrate seamlessly with
the temporal transformer modules of video diffusion models, modulating
self-attention computations across frames without compromising spatial
integrity. Our approach provides a compact and efficient solution to motion
representation, utilizing two types of embeddings: a Motion Query-Key Embedding
to modulate the temporal attention map and a Motion Value Embedding to modulate
the attention values. Additionally, we introduce an inference strategy that
excludes spatial dimensions from the Motion Query-Key Embedding and applies a
differential operation to the Motion Value Embedding, both designed to debias
appearance and ensure the embeddings focus solely on motion. Our contributions
include the introduction of a tailored motion embedding for customization tasks
and a demonstration of the practical advantages and effectiveness of our method
through extensive experiments.",2024-03-29 14:14:22+00:00,"['Luozhou Wang', 'Ziyang Mai', 'Guibao Shen', 'Yixun Liang', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Yijun Li', 'Yingcong Chen']",http://arxiv.org/abs/2403.20193v2
VideoDirector: Precise Video Editing via Text-to-Video Models,"Despite the typical inversion-then-editing paradigm using text-to-image (T2I)
models has demonstrated promising results, directly extending it to
text-to-video (T2V) models still suffers severe artifacts such as color
flickering and content distortion. Consequently, current video editing methods
primarily rely on T2I models, which inherently lack temporal-coherence
generative ability, often resulting in inferior editing results. In this paper,
we attribute the failure of the typical editing paradigm to: 1) Tightly
Spatial-temporal Coupling. The vanilla pivotal-based inversion strategy
struggles to disentangle spatial-temporal information in the video diffusion
model; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention
control is deficient in preserving the unedited content. To address these
limitations, we propose a spatial-temporal decoupled guidance (STDG) and
multi-frame null-text optimization strategy to provide pivotal temporal cues
for more precise pivotal inversion. Furthermore, we introduce a self-attention
control strategy to maintain higher fidelity for precise partial content
editing. Experimental results demonstrate that our method (termed
VideoDirector) effectively harnesses the powerful temporal generation
capabilities of T2V models, producing edited videos with state-of-the-art
performance in accuracy, motion smoothness, realism, and fidelity to unedited
content.",2024-11-26 16:56:53+00:00,"['Yukun Wang', 'Longguang Wang', 'Zhiyuan Ma', 'Qibin Hu', 'Kai Xu', 'Yulan Guo']",http://arxiv.org/abs/2411.17592v3
Enhancing Medical Imaging with GANs Synthesizing Realistic Images from Limited Data,"In this research, we introduce an innovative method for synthesizing medical
images using generative adversarial networks (GANs). Our proposed GANs method
demonstrates the capability to produce realistic synthetic images even when
trained on a limited quantity of real medical image data, showcasing
commendable generalization prowess. To achieve this, we devised a generator and
discriminator network architecture founded on deep convolutional neural
networks (CNNs), leveraging the adversarial training paradigm for model
optimization. Through extensive experimentation across diverse medical image
datasets, our method exhibits robust performance, consistently generating
synthetic images that closely emulate the structural and textural attributes of
authentic medical images.",2024-05-22 23:32:24+00:00,"['Yinqiu Feng', 'Bo Zhang', 'Lingxi Xiao', 'Yutian Yang', 'Tana Gegen', 'Zexi Chen']",http://arxiv.org/abs/2406.18547v1
Diffusion based multi-domain neuroimaging harmonization method with preservation of anatomical details,"Multi-center neuroimaging studies face technical variability due to batch
differences across sites, which potentially hinders data aggregation and
impacts study reliability.Recent efforts in neuroimaging harmonization have
aimed to minimize these technical gaps and reduce technical variability across
batches. While Generative Adversarial Networks (GAN) has been a prominent
method for addressing image harmonization tasks, GAN-harmonized images suffer
from artifacts or anatomical distortions. Given the advancements of denoising
diffusion probabilistic model which produces high-fidelity images, we have
assessed the efficacy of the diffusion model for neuroimaging harmonization. we
have demonstrated the diffusion model's superior capability in harmonizing
images from multiple domains, while GAN-based methods are limited to
harmonizing images between two domains per model. Our experiments highlight
that the learned domain invariant anatomical condition reinforces the model to
accurately preserve the anatomical details while differentiating batch
differences at each diffusion step. Our proposed method has been tested on two
public neuroimaging dataset ADNI1 and ABIDE II, yielding harmonization results
with consistent anatomy preservation and superior FID score compared to the
GAN-based methods. We have conducted multiple analysis including extensive
quantitative and qualitative evaluations against the baseline models, ablation
study showcasing the benefits of the learned conditions, and improvements in
the consistency of perivascular spaces (PVS) segmentation through
harmonization.",2024-09-01 18:54:00+00:00,"['Haoyu Lan', 'Bino A. Varghese', 'Nasim Sheikh-Bahaei', 'Farshid Sepehrband', 'Arthur W Toga', 'Jeiran Choupan']",http://arxiv.org/abs/2409.00807v1
CAS-GAN for Contrast-free Angiography Synthesis,"Iodinated contrast agents are widely utilized in numerous interventional
procedures, yet posing substantial health risks to patients. This paper
presents CAS-GAN, a novel GAN framework that serves as a ""virtual contrast
agent"" to synthesize X-ray angiographies via disentanglement representation
learning and vessel semantic guidance, thereby reducing the reliance on
iodinated contrast agents during interventional procedures. Specifically, our
approach disentangles X-ray angiographies into background and vessel
components, leveraging medical prior knowledge. A specialized predictor then
learns to map the interrelationships between these components. Additionally, a
vessel semantic-guided generator and a corresponding loss function are
introduced to enhance the visual fidelity of generated images. Experimental
results on the XCAD dataset demonstrate the state-of-the-art performance of our
CAS-GAN, achieving a FID of 5.87 and a MMD of 0.016. These promising results
highlight CAS-GAN's potential for clinical applications.",2024-10-11 03:31:40+00:00,"['De-Xing Huang', 'Xiao-Hu Zhou', 'Mei-Jiang Gui', 'Xiao-Liang Xie', 'Shi-Qi Liu', 'Shuang-Yi Wang', 'Hao Li', 'Tian-Yu Xiang', 'Zeng-Guang Hou']",http://arxiv.org/abs/2410.08490v3
Phy-Diff: Physics-guided Hourglass Diffusion Model for Diffusion MRI Synthesis,"Diffusion MRI (dMRI) is an important neuroimaging technique with high
acquisition costs. Deep learning approaches have been used to enhance dMRI and
predict diffusion biomarkers through undersampled dMRI. To generate more
comprehensive raw dMRI, generative adversarial network based methods are
proposed to include b-values and b-vectors as conditions, but they are limited
by unstable training and less desirable diversity. The emerging diffusion model
(DM) promises to improve generative performance. However, it remains
challenging to include essential information in conditioning DM for more
relevant generation, i.e., the physical principles of dMRI and white matter
tract structures. In this study, we propose a physics-guided diffusion model to
generate high-quality dMRI. Our model introduces the physical principles of
dMRI in the noise evolution in the diffusion process and introduce a
query-based conditional mapping within the difussion model. In addition, to
enhance the anatomical fine detials of the generation, we introduce the XTRACT
atlas as prior of white matter tracts by adopting an adapter technique. Our
experiment results show that our method outperforms other state-of-the-art
methods and has the potential to advance dMRI enhancement.",2024-06-05 07:09:19+00:00,"['Juanhua Zhang', 'Ruodan Yan', 'Alessandro Perelli', 'Xi Chen', 'Chao Li']",http://arxiv.org/abs/2406.03002v2
Rolling Diffusion Models,"Diffusion models have recently been increasingly applied to temporal data
such as video, fluid mechanics simulations, or climate data. These methods
generally treat subsequent frames equally regarding the amount of noise in the
diffusion process. This paper explores Rolling Diffusion: a new approach that
uses a sliding window denoising process. It ensures that the diffusion process
progressively corrupts through time by assigning more noise to frames that
appear later in a sequence, reflecting greater uncertainty about the future as
the generation process unfolds. Empirically, we show that when the temporal
dynamics are complex, Rolling Diffusion is superior to standard diffusion. In
particular, this result is demonstrated in a video prediction task using the
Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting
experiment.",2024-02-12 08:16:10+00:00,"['David Ruhe', 'Jonathan Heek', 'Tim Salimans', 'Emiel Hoogeboom']",http://arxiv.org/abs/2402.09470v3
Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network with Adversarial Temporal Reasoning,"Diffusion Probabilistic Models have recently attracted significant attention
in the community of computer vision due to their outstanding performance.
However, while a substantial amount of diffusion-based research has focused on
generative tasks, no work introduces diffusion models to advance the results of
polyp segmentation in videos, which is frequently challenged by polyps' high
camouflage and redundant temporal cues.In this paper, we present a novel
diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS.
We incorporate multi-task supervision into diffusion models to promote the
discrimination of diffusion models on pixel-by-pixel segmentation. This
integrates the contextual high-level information achieved by the joint
classification and detection tasks. To explore the temporal dependency,
Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the
target frame from the previous frames. We further equip TRM with a generative
adversarial self-supervised strategy to produce more realistic frames and thus
capture better dynamic cues. Extensive experiments are conducted on SUN-SEG,
and the results indicate that our proposed Diff-VPS significantly achieves
state-of-the-art performance. Code is available at
https://github.com/lydia-yllu/Diff-VPS.",2024-09-11 12:51:41+00:00,"['Yingling Lu', 'Yijun Yang', 'Zhaohu Xing', 'Qiong Wang', 'Lei Zhu']",http://arxiv.org/abs/2409.07238v1
VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation,"The recent years have witnessed great advances in video generation. However,
the development of automatic video metrics is lagging significantly behind.
None of the existing metric is able to provide reliable scores over generated
videos. The main barrier is the lack of large-scale human-annotated dataset. In
this paper, we release VideoFeedback, the first large-scale dataset containing
human-provided multi-aspect score over 37.6K synthesized videos from 11
existing video generative models. We train VideoScore (initialized from Mantis)
based on VideoFeedback to enable automatic video quality assessment.
Experiments show that the Spearman correlation between VideoScore and humans
can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about
50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and
VBench show that VideoScore has consistently much higher correlation with human
judges than other metrics. Due to these results, we believe VideoScore can
serve as a great proxy for human raters to (1) rate different video models to
track progress (2) simulate fine-grained human feedback in Reinforcement
Learning with Human Feedback (RLHF) to improve current video generation models.",2024-06-21 15:43:46+00:00,"['Xuan He', 'Dongfu Jiang', 'Ge Zhang', 'Max Ku', 'Achint Soni', 'Sherman Siu', 'Haonan Chen', 'Abhranil Chandra', 'Ziyan Jiang', 'Aaran Arulraj', 'Kai Wang', 'Quy Duc Do', 'Yuansheng Ni', 'Bohan Lyu', 'Yaswanth Narsupalli', 'Rongqi Fan', 'Zhiheng Lyu', 'Yuchen Lin', 'Wenhu Chen']",http://arxiv.org/abs/2406.15252v3
Text Prompting for Multi-Concept Video Customization by Autoregressive Generation,"We present a method for multi-concept customization of pretrained
text-to-video (T2V) models. Intuitively, the multi-concept customized video can
be derived from the (non-linear) intersection of the video manifolds of the
individual concepts, which is not straightforward to find. We hypothesize that
sequential and controlled walking towards the intersection of the video
manifolds, directed by text prompting, leads to the solution. To do so, we
generate the various concepts and their corresponding interactions,
sequentially, in an autoregressive manner. Our method can generate videos of
multiple custom concepts (subjects, action and background) such as a teddy bear
running towards a brown teapot, a dog playing violin and a teddy bear swimming
in the ocean. We quantitatively evaluate our method using videoCLIP and DINO
scores, in addition to human evaluation. Videos for results presented in this
paper can be found at https://github.com/divyakraman/MultiConceptVideo2024.",2024-05-22 19:35:00+00:00,"['Divya Kothandaraman', 'Kihyuk Sohn', 'Ruben Villegas', 'Paul Voigtlaender', 'Dinesh Manocha', 'Mohammad Babaeizadeh']",http://arxiv.org/abs/2405.13951v1
ExpertAF: Expert Actionable Feedback from Video,"Feedback is essential for learning a new skill or improving one's current
skill-level. However, current methods for skill-assessment from video only
provide scores or compare demonstrations, leaving the burden of knowing what to
do differently on the user. We introduce a novel method to generate actionable
feedback from video of a person doing a physical activity, such as basketball
or soccer. Our method takes a video demonstration and its accompanying 3D body
pose and generates (1) free-form expert commentary describing what the person
is doing well and what they could improve, and (2) a visual expert
demonstration that incorporates the required corrections. We show how to
leverage Ego-Exo4D's videos of skilled activity and expert commentary together
with a strong language model to create a weakly-supervised training dataset for
this task, and we devise a multimodal video-language model to infer coaching
feedback. Our method is able to reason across multi-modal input combinations to
output full-spectrum, actionable coaching -- expert commentary, expert video
retrieval, and expert pose generation -- outperforming strong vision-language
models on both established metrics and human preference studies. Code and data
will be publicly released.",2024-08-01 16:13:07+00:00,"['Kumar Ashutosh', 'Tushar Nagarajan', 'Georgios Pavlakos', 'Kris Kitani', 'Kristen Grauman']",http://arxiv.org/abs/2408.00672v2
HOTVCOM: Generating Buzzworthy Comments for Videos,"In the era of social media video platforms, popular ``hot-comments'' play a
crucial role in attracting user impressions of short-form videos, making them
vital for marketing and branding purpose. However, existing research
predominantly focuses on generating descriptive comments or ``danmaku'' in
English, offering immediate reactions to specific video moments. Addressing
this gap, our study introduces \textsc{HotVCom}, the largest Chinese video
hot-comment dataset, comprising 94k diverse videos and 137 million comments. We
also present the \texttt{ComHeat} framework, which synergistically integrates
visual, auditory, and textual data to generate influential hot-comments on the
Chinese video dataset. Empirical evaluations highlight the effectiveness of our
framework, demonstrating its excellence on both the newly constructed and
existing datasets.",2024-09-23 16:45:13+00:00,"['Yuyan Chen', 'Yiwen Qian', 'Songzhou Yan', 'Jiyuan Jia', 'Zhixu Li', 'Yanghua Xiao', 'Xiaobo Li', 'Ming Yang', 'Qingpei Guo']",http://arxiv.org/abs/2409.15196v1
Exploring Temporal Event Cues for Dense Video Captioning in Cyclic Co-learning,"Dense video captioning aims to detect and describe all events in untrimmed
videos. This paper presents a dense video captioning network called
Multi-Concept Cyclic Learning (MCCL), which aims to: (1) detect multiple
concepts at the frame level, using these concepts to enhance video features and
provide temporal event cues; and (2) design cyclic co-learning between the
generator and the localizer within the captioning network to promote semantic
perception and event localization. Specifically, we perform weakly supervised
concept detection for each frame, and the detected concept embeddings are
integrated into the video features to provide event cues. Additionally,
video-level concept contrastive learning is introduced to obtain more
discriminative concept embeddings. In the captioning network, we establish a
cyclic co-learning strategy where the generator guides the localizer for event
localization through semantic matching, while the localizer enhances the
generator's event semantic perception through location matching, making
semantic perception and event localization mutually beneficial. MCCL achieves
state-of-the-art performance on the ActivityNet Captions and YouCook2 datasets.
Extensive experiments demonstrate its effectiveness and interpretability.",2024-12-16 05:48:44+00:00,"['Zhuyang Xie', 'Yan Yang', 'Yankai Yu', 'Jie Wang', 'Yongquan Jiang', 'Xiao Wu']",http://arxiv.org/abs/2412.11467v1
VidTwin: Video VAE with Decoupled Structure and Dynamics,"Recent advancements in video autoencoders (Video AEs) have significantly
improved the quality and efficiency of video generation. In this paper, we
propose a novel and compact video autoencoder, VidTwin, that decouples video
into two distinct latent spaces: Structure latent vectors, which capture
overall content and global movement, and Dynamics latent vectors, which
represent fine-grained details and rapid movements. Specifically, our approach
leverages an Encoder-Decoder backbone, augmented with two submodules for
extracting these latent spaces, respectively. The first submodule employs a
Q-Former to extract low-frequency motion trends, followed by downsampling
blocks to remove redundant content details. The second averages the latent
vectors along the spatial dimension to capture rapid motion. Extensive
experiments show that VidTwin achieves a high compression rate of 0.20% with
high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and
performs efficiently and effectively in downstream generative tasks. Moreover,
our model demonstrates explainability and scalability, paving the way for
future research in video latent representation and generation. Our code has
been released at https://github.com/microsoft/VidTok/tree/main/vidtwin.",2024-12-23 17:16:58+00:00,"['Yuchi Wang', 'Junliang Guo', 'Xinyi Xie', 'Tianyu He', 'Xu Sun', 'Jiang Bian']",http://arxiv.org/abs/2412.17726v1
On the Content Bias in Frchet Video Distance,"Fr\'echet Video Distance (FVD), a prominent metric for evaluating video
generation models, is known to conflict with human perception occasionally. In
this paper, we aim to explore the extent of FVD's bias toward per-frame quality
over temporal realism and identify its sources. We first quantify the FVD's
sensitivity to the temporal axis by decoupling the frame and motion quality and
find that the FVD increases only slightly with large temporal corruption. We
then analyze the generated videos and show that via careful sampling from a
large set of generated videos that do not contain motions, one can drastically
decrease FVD without improving the temporal quality. Both studies suggest FVD's
bias towards the quality of individual frames. We further observe that the bias
can be attributed to the features extracted from a supervised video classifier
trained on the content-biased dataset. We show that FVD with features extracted
from the recent large-scale self-supervised video models is less biased toward
image quality. Finally, we revisit a few real-world examples to validate our
hypothesis.",2024-04-18 17:59:58+00:00,"['Songwei Ge', 'Aniruddha Mahapatra', 'Gaurav Parmar', 'Jun-Yan Zhu', 'Jia-Bin Huang']",http://arxiv.org/abs/2404.12391v1
Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation,"In recent developments, the Mamba architecture, known for its selective state
space approach, has shown potential in the efficient modeling of long
sequences. However, its application in image generation remains underexplored.
Traditional diffusion transformers (DiT), which utilize self-attention blocks,
are effective but their computational complexity scales quadratically with the
input length, limiting their use for high-resolution images. To address this
challenge, we introduce a novel diffusion architecture, Diffusion Mamba (DiM),
which foregoes traditional attention mechanisms in favor of a scalable
alternative. By harnessing the inherent efficiency of the Mamba architecture,
DiM achieves rapid inference times and reduced computational load, maintaining
linear complexity with respect to sequence length. Our architecture not only
scales effectively but also outperforms existing diffusion transformers in both
image and video generation tasks. The results affirm the scalability and
efficiency of DiM, establishing a new benchmark for image and video generation
techniques. This work advances the field of generative models and paves the way
for further applications of scalable architectures.",2024-05-24 18:50:27+00:00,"['Shentong Mo', 'Yapeng Tian']",http://arxiv.org/abs/2405.15881v1
Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model,"Multimodal emotion recognition is a task of great concern. However,
traditional data sets are based on fixed labels, resulting in models that often
focus on main emotions and ignore detailed emotional changes in complex scenes.
This report introduces the solution of using MLLMs technology to generate
open-vocabulary emotion labels from a video. The solution includes the use of
framework, data generation and processing, training methods, results generation
and multi-model co-judgment. In the MER-OV (Open-Word Emotion Recognition) of
the MER2024 challenge, our method achieved significant advantages, leading to
its superior capabilities in complex emotion computation.",2024-08-21 02:17:18+00:00,"['Mengying Ge', 'Dongkai Tang', 'Mingyang Li']",http://arxiv.org/abs/2408.11286v2
Enhancing Film Grain Coding in VVC: Improving Encoding Quality and Efficiency,"This paper presents an in-depth analysis of film grain handling in
open-source implementations of the Versatile Video Coding (VVC) standard. We
focus on two key components: the Film Grain Analysis (FGA) module implemented
in VVenC and the Film Grain Synthesis (FGS) module implemented in VVdeC. We
describe the methodologies used to implement these modules and discuss the
generation of Supplementary Enhancement Information (SEI) parameters to signal
film grain characteristics in the encoded video sequences. Additionally, we
conduct subjective and objective evaluations across Full HD videos to assess
the effectiveness of film grain handling. Our results demonstrate the
capability of the FGA and FGS techniques to accurately analyze and synthesize
film grain, thereby improving the visual quality of encoded video content.
Overall, our study contributes to advancing the understanding and
implementation of film grain handling techniques in VVC open-source
implementations, with implications for enhancing the viewing experience in
multimedia applications.",2024-07-17 10:34:49+00:00,"['Vignesh V Menon', 'Adam Wieckowski', 'Christian Stoffers', 'Jens Brandenburg', 'Christian Lehmann', 'Benjamin Bross', 'Thomas Schierl', 'Detlev Marpe']",http://arxiv.org/abs/2407.12465v1
TimeRewind: Rewinding Time with Image-and-Events Video Diffusion,"This paper addresses the novel challenge of ``rewinding'' time from a single
captured image to recover the fleeting moments missed just before the shutter
button is pressed. This problem poses a significant challenge in computer
vision and computational photography, as it requires predicting plausible
pre-capture motion from a single static frame, an inherently ill-posed task due
to the high degree of freedom in potential pixel movements. We overcome this
challenge by leveraging the emerging technology of neuromorphic event cameras,
which capture motion information with high temporal resolution, and integrating
this data with advanced image-to-video diffusion models. Our proposed framework
introduces an event motion adaptor conditioned on event camera data, guiding
the diffusion model to generate videos that are visually coherent and
physically grounded in the captured events. Through extensive experimentation,
we demonstrate the capability of our approach to synthesize high-quality videos
that effectively ``rewind'' time, showcasing the potential of combining event
camera technology with generative models. Our work opens new avenues for
research at the intersection of computer vision, computational photography, and
generative modeling, offering a forward-thinking solution to capturing missed
moments and enhancing future consumer cameras and smartphones. Please see the
project page at https://timerewind.github.io/ for video results and code
release.",2024-03-20 17:57:02+00:00,"['Jingxi Chen', 'Brandon Y. Feng', 'Haoming Cai', 'Mingyang Xie', 'Christopher Metzler', 'Cornelia Fermuller', 'Yiannis Aloimonos']",http://arxiv.org/abs/2403.13800v1
YingSound: Video-Guided Sound Effects Generation with Multi-modal Chain-of-Thought Controls,"Generating sound effects for product-level videos, where only a small amount
of labeled data is available for diverse scenes, requires the production of
high-quality sounds in few-shot settings. To tackle the challenge of limited
labeled data in real-world scenes, we introduce YingSound, a foundation model
designed for video-guided sound generation that supports high-quality audio
generation in few-shot settings. Specifically, YingSound consists of two major
modules. The first module uses a conditional flow matching transformer to
achieve effective semantic alignment in sound generation across audio and
visual modalities. This module aims to build a learnable audio-visual
aggregator (AVA) that integrates high-resolution visual features with
corresponding audio features at multiple stages. The second module is developed
with a proposed multi-modal visual-audio chain-of-thought (CoT) approach to
generate finer sound effects in few-shot settings. Finally, an
industry-standard video-to-audio (V2A) dataset that encompasses various
real-world scenarios is presented. We show that YingSound effectively generates
high-quality synchronized sounds across diverse conditional inputs through
automated evaluations and human studies. Project Page:
\url{https://giantailab.github.io/yingsound/}",2024-12-12 10:55:57+00:00,"['Zihao Chen', 'Haomin Zhang', 'Xinhan Di', 'Haoyu Wang', 'Sizhe Shan', 'Junjie Zheng', 'Yunming Liang', 'Yihan Fan', 'Xinfa Zhu', 'Wenjie Tian', 'Yihua Wang', 'Chaofan Ding', 'Lei Xie']",http://arxiv.org/abs/2412.09168v1
2S-ODIS: Two-Stage Omni-Directional Image Synthesis by Geometric Distortion Correction,"Omni-directional images have been increasingly used in various applications,
including virtual reality and SNS (Social Networking Services). However, their
availability is comparatively limited in contrast to normal field of view
(NFoV) images, since specialized cameras are required to take omni-directional
images. Consequently, several methods have been proposed based on generative
adversarial networks (GAN) to synthesize omni-directional images, but these
approaches have shown difficulties in training of the models, due to
instability and/or significant time consumption in the training. To address
these problems, this paper proposes a novel omni-directional image synthesis
method, 2S-ODIS (Two-Stage Omni-Directional Image Synthesis), which generated
high-quality omni-directional images but drastically reduced the training time.
This was realized by utilizing the VQGAN (Vector Quantized GAN) model
pre-trained on a large-scale NFoV image database such as ImageNet without
fine-tuning. Since this pre-trained model does not represent distortions of
omni-directional images in the equi-rectangular projection (ERP), it cannot be
applied directly to the omni-directional image synthesis in ERP. Therefore,
two-stage structure was adopted to first create a global coarse image in ERP
and then refine the image by integrating multiple local NFoV images in the
higher resolution to compensate the distortions in ERP, both of which are based
on the pre-trained VQGAN model. As a result, the proposed method, 2S-ODIS,
achieved the reduction of the training time from 14 days in OmniDreamer to four
days in higher image quality.",2024-09-16 04:01:10+00:00,"['Atsuya Nakata', 'Takao Yamanaka']",http://arxiv.org/abs/2409.09969v1
VideoSAVi: Self-Aligned Video Language Models without Human Supervision,"Recent advances in vision-language models (VLMs) have significantly enhanced
video understanding tasks. Instruction tuning (i.e., fine-tuning models on
datasets of instructions paired with desired outputs) has been key to improving
model performance. However, creating diverse instruction-tuning datasets is
challenging due to high annotation costs and the complexity of capturing
temporal information in videos. Existing approaches often rely on large
language models to generate instruction-output pairs, which can limit diversity
and lead to responses that lack grounding in the video content. To address
this, we propose VideoSAVi (Self-Aligned Video Language Model), a novel
self-training pipeline that enables VLMs to generate their own training data
without extensive manual annotation. The process involves three stages: (1)
generating diverse video-specific questions, (2) producing multiple candidate
answers, and (3) evaluating these responses for alignment with the video
content. This self-generated data is then used for direct preference
optimization (DPO), allowing the model to refine its own high-quality outputs
and improve alignment with video content. Our experiments demonstrate that even
smaller models (0.5B and 7B parameters) can effectively use this self-training
approach, outperforming previous methods and achieving results comparable to
those trained on proprietary preference data. VideoSAVi shows significant
improvements across multiple benchmarks: up to 28% on multi-choice QA, 8% on
zero-shot open-ended QA, and 12% on temporal reasoning benchmarks. These
results demonstrate the effectiveness of our self-training approach in
enhancing video understanding while reducing dependence on proprietary models.",2024-12-01 00:33:05+00:00,"['Yogesh Kulkarni', 'Pooyan Fazli']",http://arxiv.org/abs/2412.00624v1
G3FA: Geometry-guided GAN for Face Animation,"Animating human face images aims to synthesize a desired source identity in a
natural-looking way mimicking a driving video's facial movements. In this
context, Generative Adversarial Networks have demonstrated remarkable potential
in real-time face reenactment using a single source image, yet are constrained
by limited geometry consistency compared to graphic-based approaches. In this
paper, we introduce Geometry-guided GAN for Face Animation (G3FA) to tackle
this limitation. Our novel approach empowers the face animation model to
incorporate 3D information using only 2D images, improving the image generation
capabilities of the talking head synthesis model. We integrate inverse
rendering techniques to extract 3D facial geometry properties, improving the
feedback loop to the generator through a weighted average ensemble of
discriminators. In our face reenactment model, we leverage 2D motion warping to
capture motion dynamics along with orthogonal ray sampling and volume rendering
techniques to produce the ultimate visual output. To evaluate the performance
of our G3FA, we conducted comprehensive experiments using various evaluation
protocols on VoxCeleb2 and TalkingHead benchmarks to demonstrate the
effectiveness of our proposed framework compared to the state-of-the-art
real-time face animation methods.",2024-08-23 13:13:24+00:00,"['Alireza Javanmardi', 'Alain Pagani', 'Didier Stricker']",http://arxiv.org/abs/2408.13049v1
WorldSimBench: Towards Video Generation Models as World Simulators,"Recent advancements in predictive models have demonstrated exceptional
capabilities in predicting the future state of objects and scenes. However, the
lack of categorization based on inherent characteristics continues to hinder
the progress of predictive model development. Additionally, existing benchmarks
are unable to effectively evaluate higher-capability, highly embodied
predictive models from an embodied perspective. In this work, we classify the
functionalities of predictive models into a hierarchy and take the first step
in evaluating World Simulators by proposing a dual evaluation framework called
WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and
Implicit Manipulative Evaluation, encompassing human preference assessments
from the visual perspective and action-level evaluations in embodied tasks,
covering three representative embodied scenarios: Open-Ended Embodied
Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit
Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment
dataset based on fine-grained human feedback, which we use to train a Human
Preference Evaluator that aligns with human perception and explicitly assesses
the visual fidelity of World Simulators. In the Implicit Manipulative
Evaluation, we assess the video-action consistency of World Simulators by
evaluating whether the generated situation-aware video can be accurately
translated into the correct control signals in dynamic environments. Our
comprehensive evaluation offers key insights that can drive further innovation
in video generation models, positioning World Simulators as a pivotal
advancement toward embodied artificial intelligence.",2024-10-23 17:56:11+00:00,"['Yiran Qin', 'Zhelun Shi', 'Jiwen Yu', 'Xijun Wang', 'Enshen Zhou', 'Lijun Li', 'Zhenfei Yin', 'Xihui Liu', 'Lu Sheng', 'Jing Shao', 'Lei Bai', 'Wanli Ouyang', 'Ruimao Zhang']",http://arxiv.org/abs/2410.18072v1
Real-Time Video Generation with Pyramid Attention Broadcast,"We present Pyramid Attention Broadcast (PAB), a real-time, high quality and
training-free approach for DiT-based video generation. Our method is founded on
the observation that attention difference in the diffusion process exhibits a
U-shaped pattern, indicating significant redundancy. We mitigate this by
broadcasting attention outputs to subsequent steps in a pyramid style. It
applies different broadcast strategies to each attention based on their
variance for best efficiency. We further introduce broadcast sequence parallel
for more efficient distributed inference. PAB demonstrates up to 10.5x speedup
across three models compared to baselines, achieving real-time generation for
up to 720p videos. We anticipate that our simple yet effective method will
serve as a robust baseline and facilitate future research and application for
video generation.",2024-08-22 17:54:21+00:00,"['Xuanlei Zhao', 'Xiaolong Jin', 'Kai Wang', 'Yang You']",http://arxiv.org/abs/2408.12588v3
Shorts on the Rise: Assessing the Effects of YouTube Shorts on Long-Form Video Content,"Short form content has permeated into the video creator space over the past
few years, led by industry leading products such as TikTok, YouTube Shorts and
Instagram Reels. YouTube in particular was previously synonymous with being the
main hub for long form video content consumption. The monetization of long form
videos was easier as it allowed multiple advertisement placements during the
course of the video. This model also facilitated thematic brand partnerships.
However, since the introduction of short form content, creators have found it
more difficult to generate revenue as advertisement placements have decreased.
This leads to a unique situation where people are spending more time watching
shorter videos, and yet they generate less revenue for the creators. In this
paper, we perform a study of 250 creators with significant audiences to see if
the introduction of short form content has affected the view counts and
engagement of long form content. Our findings reveal a noteworthy trend: since
the advent of short-form content, there has been a significant decrease in both
view counts and engagement in long-form videos on these channels.",2024-02-28 09:59:31+00:00,"['Prajit T. Rajendran', 'Kevin Creusy', 'Vivien Garnes']",http://arxiv.org/abs/2402.18208v2
MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies,"Development of multimodal models has marked a significant step forward in how
machines understand videos. These models have shown promise in analyzing short
video clips. However, when it comes to longer formats like movies, they often
fall short. The main hurdles are the lack of high-quality, diverse video data
and the intensive work required to collect or annotate such data. In face of
these challenges, we propose MovieLLM, a novel framework designed to synthesize
consistent and high-quality video data for instruction tuning. The pipeline is
carefully designed to control the style of videos by improving textual
inversion technique with powerful text generation capability of GPT-4. As the
first framework to do such thing, our approach stands out for its flexibility
and scalability, empowering users to create customized movies with only one
description. This makes it a superior alternative to traditional data
collection methods. Our extensive experiments validate that the data produced
by MovieLLM significantly improves the performance of multimodal models in
understanding complex video narratives, overcoming the limitations of existing
datasets regarding scarcity and bias.",2024-03-03 07:43:39+00:00,"['Zhende Song', 'Chenchen Wang', 'Jiamu Sheng', 'Chi Zhang', 'Gang Yu', 'Jiayuan Fan', 'Tao Chen']",http://arxiv.org/abs/2403.01422v2
Video-Driven Animation of Neural Head Avatars,"We present a new approach for video-driven animation of high-quality neural
3D head models, addressing the challenge of person-independent animation from
video input. Typically, high-quality generative models are learned for specific
individuals from multi-view video footage, resulting in person-specific latent
representations that drive the generation process. In order to achieve
person-independent animation from video input, we introduce an LSTM-based
animation network capable of translating person-independent expression features
into personalized animation parameters of person-specific 3D head models. Our
approach combines the advantages of personalized head models (high quality and
realism) with the convenience of video-driven animation employing multi-person
facial performance capture. We demonstrate the effectiveness of our approach on
synthesized animations with high quality based on different source videos as
well as an ablation study.",2024-03-07 10:13:48+00:00,"['Wolfgang Paier', 'Paul Hinzer', 'Anna Hilsmann', 'Peter Eisert']",http://arxiv.org/abs/2403.04380v1
AQuA: Automated Question-Answering in Software Tutorial Videos with Visual Anchors,"Tutorial videos are a popular help source for learning feature-rich software.
However, getting quick answers to questions about tutorial videos is difficult.
We present an automated approach for responding to tutorial questions. By
analyzing 633 questions found in 5,944 video comments, we identified different
question types and observed that users frequently described parts of the video
in questions. We then asked participants (N=24) to watch tutorial videos and
ask questions while annotating the video with relevant visual anchors. Most
visual anchors referred to UI elements and the application workspace. Based on
these insights, we built AQuA, a pipeline that generates useful answers to
questions with visual anchors. We demonstrate this for Fusion 360, showing that
we can recognize UI elements in visual anchors and generate answers using GPT-4
augmented with that visual information and software documentation. An
evaluation study (N=16) demonstrates that our approach provides better answers
than baseline methods.",2024-03-08 10:55:43+00:00,"['Saelyne Yang', 'Jo Vermeulen', 'George Fitzmaurice', 'Justin Matejka']",http://arxiv.org/abs/2403.05213v1
Unveiling the Invisible: Captioning Videos with Metaphors,"Metaphors are a common communication tool used in our day-to-day life. The
detection and generation of metaphors in textual form have been studied
extensively but metaphors in other forms have been under-explored. Recent
studies have shown that Vision-Language (VL) models cannot understand visual
metaphors in memes and adverts. As of now, no probing studies have been done
that involve complex language phenomena like metaphors with videos. Hence, we
introduce a new VL task of describing the metaphors present in the videos in
our work. To facilitate this novel task, we construct and release a manually
created dataset with 705 videos and 2115 human-written captions, along with a
new metric called Average Concept Distance (ACD), to automatically evaluate the
creativity of the metaphors generated. We also propose a novel low-resource
video metaphor captioning system: GIT-LLaVA, which obtains comparable
performance to SoTA video language models on the proposed task. We perform a
comprehensive analysis of existing video language models on this task and
publish our dataset, models, and benchmark results to enable further research.",2024-06-07 12:32:44+00:00,"['Abisek Rajakumar Kalarani', 'Pushpak Bhattacharyya', 'Sumit Shekhar']",http://arxiv.org/abs/2406.04886v2
UVIS: Unsupervised Video Instance Segmentation,"Video instance segmentation requires classifying, segmenting, and tracking
every object across video frames. Unlike existing approaches that rely on
masks, boxes, or category labels, we propose UVIS, a novel Unsupervised Video
Instance Segmentation (UVIS) framework that can perform video instance
segmentation without any video annotations or dense label-based pretraining.
Our key insight comes from leveraging the dense shape prior from the
self-supervised vision foundation model DINO and the openset recognition
ability from the image-caption supervised vision-language model CLIP. Our UVIS
framework consists of three essential steps: frame-level pseudo-label
generation, transformer-based VIS model training, and query-based tracking. To
improve the quality of VIS predictions in the unsupervised setup, we introduce
a dual-memory design. This design includes a semantic memory bank for
generating accurate pseudo-labels and a tracking memory bank for maintaining
temporal consistency in object tracks. We evaluate our approach on three
standard VIS benchmarks, namely YoutubeVIS-2019, YoutubeVIS-2021, and Occluded
VIS. Our UVIS achieves 21.1 AP on YoutubeVIS-2019 without any video annotations
or dense pretraining, demonstrating the potential of our unsupervised VIS
framework.",2024-06-11 03:05:50+00:00,"['Shuaiyi Huang', 'Saksham Suri', 'Kamal Gupta', 'Sai Saketh Rambhatla', 'Ser-nam Lim', 'Abhinav Shrivastava']",http://arxiv.org/abs/2406.06908v1
Generative Outpainting To Enhance the Memorability of Short-Form Videos,"With the expanding use of the short-form video format in advertising, social
media, entertainment, education and more, there is a need for such media to
both captivate and be remembered. Video memorability indicates to us how likely
a video is to be remembered by a viewer who has no emotional or personal
connection with its content. This paper presents the results of using
generative outpainting to expand the screen size of a short-form video with a
view to improving its memorability. Advances in machine learning and deep
learning are compared and leveraged to understand how extending the borders of
video screensizes can affect their memorability to viewers. Using quantitative
evaluation we determine the best-performing model for outpainting and the
impact of outpainting based on image saliency on video memorability scores",2024-11-21 15:24:16+00:00,"['Alan Byju', 'Aman Sudhindra Ladwa', 'Lorin Sweeney', 'Alan F. Smeaton']",http://arxiv.org/abs/2411.14213v1
Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering,"This paper tackles the intricate challenge of video question-answering
(VideoQA). Despite notable progress, current methods fall short of effectively
integrating questions with video frames and semantic object-level abstractions
to create question-aware video representations. We introduce Local-Global
Question Aware Video Embedding (LGQAVE), which incorporates three major
innovations to integrate multi-modal knowledge better and emphasize semantic
visual concepts relevant to specific questions. LGQAVE moves beyond traditional
ad-hoc frame sampling by utilizing a cross-attention mechanism that precisely
identifies the most relevant frames concerning the questions. It captures the
dynamics of objects within these frames using distinct graphs, grounding them
in question semantics with the miniGPT model. These graphs are processed by a
question-aware dynamic graph transformer (Q-DGT), which refines the outputs to
develop nuanced global and local video representations. An additional
cross-attention module integrates these local and global embeddings to generate
the final video embeddings, which a language model uses to generate answers.
Extensive evaluations across multiple benchmarks demonstrate that LGQAVE
significantly outperforms existing models in delivering accurate multi-choice
and open-ended answers.",2024-12-12 12:39:07+00:00,"['Sai Bhargav Rongali', 'Mohamad Hassan N C', 'Ankit Jha', 'Neha Bhargava', 'Saurabh Prasad', 'Biplab Banerjee']",http://arxiv.org/abs/2412.09230v1
One-Shot Pose-Driving Face Animation Platform,"The objective of face animation is to generate dynamic and expressive talking
head videos from a single reference face, utilizing driving conditions derived
from either video or audio inputs. Current approaches often require fine-tuning
for specific identities and frequently fail to produce expressive videos due to
the limited effectiveness of Wav2Pose modules. To facilitate the generation of
one-shot and more consecutive talking head videos, we refine an existing
Image2Video model by integrating a Face Locator and Motion Frame mechanism. We
subsequently optimize the model using extensive human face video datasets,
significantly enhancing its ability to produce high-quality and expressive
talking head videos. Additionally, we develop a demo platform using the Gradio
framework, which streamlines the process, enabling users to quickly create
customized talking head videos.",2024-07-12 03:09:07+00:00,"['He Feng', 'Donglin Di', 'Yongjia Ma', 'Wei Chen', 'Tonghua Su']",http://arxiv.org/abs/2407.08949v1
Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis,"In the quest for artificial general intelligence, Multi-modal Large Language
Models (MLLMs) have emerged as a focal point in recent advancements. However,
the predominant focus remains on developing their capabilities in static image
understanding. The potential of MLLMs in processing sequential visual data is
still insufficiently explored, highlighting the absence of a comprehensive,
high-quality assessment of their performance. In this paper, we introduce
Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of
MLLMs in Video analysis. Our work distinguishes from existing benchmarks
through four key features: 1) Diversity in video types, spanning 6 primary
visual domains with 30 subfields to ensure broad scenario generalizability; 2)
Duration in temporal dimension, encompassing both short-, medium-, and
long-term videos, ranging from 11 seconds to 1 hour, for robust contextual
dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides
video frames, including subtitles and audios, to unveil the all-round
capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual
labeling by expert annotators to facilitate precise and reliable model
assessment. 900 videos with a total of 254 hours are manually selected and
annotated by repeatedly viewing all the video content, resulting in 2,700
question-answer pairs. With Video-MME, we extensively evaluate various
state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as
open-source image models like InternVL-Chat-V1.5 and video models like
LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the
best-performing commercial model, significantly outperforming the open-source
models. Our dataset along with these findings underscores the need for further
improvements in handling longer sequences and multi-modal data. Project Page:
https://video-mme.github.io",2024-05-31 17:59:47+00:00,"['Chaoyou Fu', 'Yuhan Dai', 'Yongdong Luo', 'Lei Li', 'Shuhuai Ren', 'Renrui Zhang', 'Zihan Wang', 'Chenyu Zhou', 'Yunhang Shen', 'Mengdan Zhang', 'Peixian Chen', 'Yanwei Li', 'Shaohui Lin', 'Sirui Zhao', 'Ke Li', 'Tong Xu', 'Xiawu Zheng', 'Enhong Chen', 'Rongrong Ji', 'Xing Sun']",http://arxiv.org/abs/2405.21075v2
Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis,"In this work, we introduce a single parameter $\omega$, to effectively
control granularity in diffusion-based synthesis. This parameter is
incorporated during the denoising steps of the diffusion model's reverse
process. Our approach does not require model retraining, architectural
modifications, or additional computational overhead during inference, yet
enables precise control over the level of details in the generated outputs.
Moreover, spatial masks or denoising schedules with varying $\omega$ values can
be applied to achieve region-specific or timestep-specific granularity control.
Prior knowledge of image composition from control signals or reference images
further facilitates the creation of precise $\omega$ masks for granularity
control on specific objects. To highlight the parameter's role in controlling
subtle detail variations, the technique is named Omegance, combining ""omega""
and ""nuance"". Our method demonstrates impressive performance across various
image and video synthesis tasks and is adaptable to advanced diffusion models.
The code is available at https://github.com/itsmag11/Omegance.",2024-11-26 08:23:16+00:00,"['Xinyu Hou', 'Zongsheng Yue', 'Xiaoming Li', 'Chen Change Loy']",http://arxiv.org/abs/2411.17769v1
Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers,"Solving image and video jigsaw puzzles poses the challenging task of
rearranging image fragments or video frames from unordered sequences to restore
meaningful images and video sequences. Existing approaches often hinge on
discriminative models tasked with predicting either the absolute positions of
puzzle elements or the permutation actions applied to the original data.
Unfortunately, these methods face limitations in effectively solving puzzles
with a large number of elements. In this paper, we propose JPDVT, an innovative
approach that harnesses diffusion transformers to address this challenge.
Specifically, we generate positional information for image patches or video
frames, conditioned on their underlying visual content. This information is
then employed to accurately assemble the puzzle pieces in their correct
positions, even in scenarios involving missing pieces. Our method achieves
state-of-the-art performance on several datasets.",2024-04-10 18:40:23+00:00,"['Jinyang Liu', 'Wondmgezahu Teshome', 'Sandesh Ghimire', 'Mario Sznaier', 'Octavia Camps']",http://arxiv.org/abs/2404.07292v1
Video Motion Transfer with Diffusion Transformers,"We propose DiTFlow, a method for transferring the motion of a reference video
to a newly synthesized one, designed specifically for Diffusion Transformers
(DiT). We first process the reference video with a pre-trained DiT to analyze
cross-frame attention maps and extract a patch-wise motion signal called the
Attention Motion Flow (AMF). We guide the latent denoising process in an
optimization-based, training-free, manner by optimizing latents with our AMF
loss to generate videos reproducing the motion of the reference one. We also
apply our optimization strategy to transformer positional embeddings, granting
us a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow
against recently published methods, outperforming all across multiple metrics
and human evaluation.",2024-12-10 18:59:58+00:00,"['Alexander Pondaven', 'Aliaksandr Siarohin', 'Sergey Tulyakov', 'Philip Torr', 'Fabio Pizzati']",http://arxiv.org/abs/2412.07776v1
Single Trajectory Distillation for Accelerating Image and Video Style Transfer,"Diffusion-based stylization methods typically denoise from a specific partial
noise state for image-to-image and video-to-video tasks. This multi-step
diffusion process is computationally expensive and hinders real-world
application. A promising solution to speed up the process is to obtain few-step
consistency models through trajectory distillation. However, current
consistency models only force the initial-step alignment between the
probability flow ODE (PF-ODE) trajectories of the student and the imperfect
teacher models. This training strategy can not ensure the consistency of whole
trajectories. To address this issue, we propose single trajectory distillation
(STD) starting from a specific partial noise state. We introduce a trajectory
bank to store the teacher model's trajectory states, mitigating the time cost
during training. Besides, we use an asymmetric adversarial loss to enhance the
style and quality of the generated images. Extensive experiments on image and
video stylization demonstrate that our method surpasses existing acceleration
models in terms of style similarity and aesthetic evaluations. Our code and
results will be available on the project page:
https://single-trajectory-distillation.github.io.",2024-12-25 16:40:23+00:00,"['Sijie Xu', 'Runqi Wang', 'Wei Zhu', 'Dejia Song', 'Nemo Chen', 'Xu Tang', 'Yao Hu']",http://arxiv.org/abs/2412.18945v1
VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation,"Human image animation involves generating a video from a static image by
following a specified pose sequence. Current approaches typically adopt a
multi-stage pipeline that separately learns appearance and motion, which often
leads to appearance degradation and temporal inconsistencies. To address these
issues, we propose VividPose, an innovative end-to-end pipeline based on Stable
Video Diffusion (SVD) that ensures superior temporal stability. To enhance the
retention of human identity, we propose an identity-aware appearance controller
that integrates additional facial information without compromising other
appearance details such as clothing texture and background. This approach
ensures that the generated videos maintain high fidelity to the identity of
human subject, preserving key facial features across various poses. To
accommodate diverse human body shapes and hand movements, we introduce a
geometry-aware pose controller that utilizes both dense rendering maps from
SMPL-X and sparse skeleton maps. This enables accurate alignment of pose and
shape in the generated videos, providing a robust framework capable of handling
a wide range of body shapes and dynamic hand movements. Extensive qualitative
and quantitative experiments on the UBCFashion and TikTok benchmarks
demonstrate that our method achieves state-of-the-art performance. Furthermore,
VividPose exhibits superior generalization capabilities on our proposed
in-the-wild dataset. Codes and models will be available.",2024-05-28 13:18:32+00:00,"['Qilin Wang', 'Zhengkai Jiang', 'Chengming Xu', 'Jiangning Zhang', 'Yabiao Wang', 'Xinyi Zhang', 'Yun Cao', 'Weijian Cao', 'Chengjie Wang', 'Yanwei Fu']",http://arxiv.org/abs/2405.18156v1
Animated Stickers: Bringing Stickers to Life with Video Diffusion,"We introduce animated stickers, a video diffusion model which generates an
animation conditioned on a text prompt and static sticker image. Our model is
built on top of the state-of-the-art Emu text-to-image model, with the addition
of temporal layers to model motion. Due to the domain gap, i.e. differences in
visual and motion style, a model which performed well on generating natural
videos can no longer generate vivid videos when applied to stickers. To bridge
this gap, we employ a two-stage finetuning pipeline: first with weakly
in-domain data, followed by human-in-the-loop (HITL) strategy which we term
ensemble-of-teachers. It distills the best qualities of multiple teachers into
a smaller student model. We show that this strategy allows us to specifically
target improvements to motion quality while maintaining the style from the
static image. With inference optimizations, our model is able to generate an
eight-frame video with high-quality, interesting, and relevant motion in under
one second.",2024-02-08 22:49:32+00:00,"['David Yan', 'Winnie Zhang', 'Luxin Zhang', 'Anmol Kalia', 'Dingkang Wang', 'Ankit Ramchandani', 'Miao Liu', 'Albert Pumarola', 'Edgar Schoenfeld', 'Elliot Blanchard', 'Krishna Narni', 'Yaqiao Luo', 'Lawrence Chen', 'Guan Pang', 'Ali Thabet', 'Peter Vajda', 'Amy Bearman', 'Licheng Yu']",http://arxiv.org/abs/2402.06088v1
HR-INR: Continuous Space-Time Video Super-Resolution via Event Camera,"Continuous space-time video super-resolution (C-STVSR) aims to simultaneously
enhance video resolution and frame rate at an arbitrary scale. Recently,
implicit neural representation (INR) has been applied to video restoration,
representing videos as implicit fields that can be decoded at an arbitrary
scale. However, the highly ill-posed nature of C-STVSR limits the effectiveness
of current INR-based methods: they assume linear motion between frames and use
interpolation or feature warping to generate features at arbitrary
spatiotemporal positions with two consecutive frames. This restrains C-STVSR
from capturing rapid and nonlinear motion and long-term dependencies (involving
more than two frames) in complex dynamic scenes. In this paper, we propose a
novel C-STVSR framework, called HR-INR, which captures both holistic
dependencies and regional motions based on INR. It is assisted by an event
camera, a novel sensor renowned for its high temporal resolution and low
latency. To fully utilize the rich temporal information from events, we design
a feature extraction consisting of (1) a regional event feature extractor -
taking events as inputs via the proposed event temporal pyramid representation
to capture the regional nonlinear motion and (2) a holistic event-frame feature
extractor for long-term dependence and continuity motion. We then propose a
novel INR-based decoder with spatiotemporal embeddings to capture long-term
dependencies with a larger temporal perception field. We validate the
effectiveness and generalization of our method on four datasets (both simulated
and real data), showing the superiority of our method.",2024-05-22 06:51:32+00:00,"['Yunfan Lu', 'Zipeng Wang', 'Yusheng Wang', 'Hui Xiong']",http://arxiv.org/abs/2405.13389v1
Global Spatial-Temporal Information-based Residual ConvLSTM for Video Space-Time Super-Resolution,"By converting low-frame-rate, low-resolution videos into high-frame-rate,
high-resolution ones, space-time video super-resolution techniques can enhance
visual experiences and facilitate more efficient information dissemination. We
propose a convolutional neural network (CNN) for space-time video
super-resolution, namely GIRNet. To generate highly accurate features and thus
improve performance, the proposed network integrates a feature-level temporal
interpolation module with deformable convolutions and a global spatial-temporal
information-based residual convolutional long short-term memory (convLSTM)
module. In the feature-level temporal interpolation module, we leverage
deformable convolution, which adapts to deformations and scale variations of
objects across different scene locations. This presents a more efficient
solution than conventional convolution for extracting features from moving
objects. Our network effectively uses forward and backward feature information
to determine inter-frame offsets, leading to the direct generation of
interpolated frame features. In the global spatial-temporal information-based
residual convLSTM module, the first convLSTM is used to derive global
spatial-temporal information from the input features, and the second convLSTM
uses the previously computed global spatial-temporal information feature as its
initial cell state. This second convLSTM adopts residual connections to
preserve spatial information, thereby enhancing the output features.
Experiments on the Vimeo90K dataset show that the proposed method outperforms
state-of-the-art techniques in peak signal-to-noise-ratio (by 1.45 dB, 1.14 dB,
and 0.02 dB over STARnet, TMNet, and 3DAttGAN, respectively), structural
similarity index(by 0.027, 0.023, and 0.006 over STARnet, TMNet, and 3DAttGAN,
respectively), and visually.",2024-07-11 13:01:44+00:00,"['Congrui Fu', 'Hui Yuan', 'Shiqi Jiang', 'Guanghui Zhang', 'Liquan Shen', 'Raouf Hamzaoui']",http://arxiv.org/abs/2407.08466v1
"One Shot, One Talk: Whole-body Talking Avatar from a Single Image","Building realistic and animatable avatars still requires minutes of
multi-view or monocular self-rotating videos, and most methods lack precise
control over gestures and expressions. To push this boundary, we address the
challenge of constructing a whole-body talking avatar from a single image. We
propose a novel pipeline that tackles two critical issues: 1) complex dynamic
modeling and 2) generalization to novel gestures and expressions. To achieve
seamless generalization, we leverage recent pose-guided image-to-video
diffusion models to generate imperfect video frames as pseudo-labels. To
overcome the dynamic modeling challenge posed by inconsistent and noisy
pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar
representation and apply several key regularizations to mitigate
inconsistencies caused by imperfect labels. Extensive experiments on diverse
subjects demonstrate that our method enables the creation of a photorealistic,
precisely animatable, and expressive whole-body talking avatar from just a
single image.",2024-12-02 04:27:41+00:00,"['Jun Xiang', 'Yudong Guo', 'Leipeng Hu', 'Boyang Guo', 'Yancheng Yuan', 'Juyong Zhang']",http://arxiv.org/abs/2412.01106v1
Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation,"Audio-driven talking head generation is a significant and challenging task
applicable to various fields such as virtual avatars, film production, and
online conferences. However, the existing GAN-based models emphasize generating
well-synchronized lip shapes but overlook the visual quality of generated
frames, while diffusion-based models prioritize generating high-quality frames
but neglect lip shape matching, resulting in jittery mouth movements. To
address the aforementioned problems, we introduce a two-stage diffusion-based
model. The first stage involves generating synchronized facial landmarks based
on the given speech. In the second stage, these generated landmarks serve as a
condition in the denoising process, aiming to optimize mouth jitter issues and
generate high-fidelity, well-synchronized, and temporally coherent talking head
videos. Extensive experiments demonstrate that our model yields the best
performance.",2024-08-03 10:19:38+00:00,"['Jintao Tan', 'Xize Cheng', 'Lingyu Xiong', 'Lei Zhu', 'Xiandong Li', 'Xianjia Wu', 'Kai Gong', 'Minglei Li', 'Yi Cai']",http://arxiv.org/abs/2408.01732v1
Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy for Temporal Sentence Grounding in Video,"Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias
issue, which is caused by the uneven temporal distribution of the target
moments for samples with similar semantic components in input videos or query
texts. Existing methods resort to utilizing prior knowledge about bias to
artificially break this uneven distribution, which only removes a limited
amount of significant language biases. In this work, we propose the
bias-conflict sample synthesis and adversarial removal debias strategy
(BSSARD), which dynamically generates bias-conflict samples by explicitly
leveraging potentially spurious correlations between single-modality features
and the temporal position of the target moments. Through adversarial training,
its bias generators continuously introduce biases and generate bias-conflict
samples to deceive its grounding model. Meanwhile, the grounding model
continuously eliminates the introduced biases, which requires it to model
multi-modality alignment information. BSSARD will cover most kinds of coupling
relationships and disrupt language and visual biases simultaneously. Extensive
experiments on Charades-CD and ActivityNet-CD demonstrate the promising
debiasing capability of BSSARD. Source codes are available at
https://github.com/qzhb/BSSARD.",2024-01-15 09:59:43+00:00,"['Zhaobo Qi', 'Yibo Yuan', 'Xiaowen Ruan', 'Shuhui Wang', 'Weigang Zhang', 'Qingming Huang']",http://arxiv.org/abs/2401.07567v2
Follow-Your-MultiPose: Tuning-Free Multi-Character Text-to-Video Generation via Pose Guidance,"Text-editable and pose-controllable character video generation is a
challenging but prevailing topic with practical applications. However, existing
approaches mainly focus on single-object video generation with pose guidance,
ignoring the realistic situation that multi-character appear concurrently in a
scenario. To tackle this, we propose a novel multi-character video generation
framework in a tuning-free manner, which is based on the separated text and
pose guidance. Specifically, we first extract character masks from the pose
sequence to identify the spatial position for each generating character, and
then single prompts for each character are obtained with LLMs for precise text
guidance. Moreover, the spatial-aligned cross attention and multi-branch
control module are proposed to generate fine grained controllable
multi-character video. The visualized results of generating video demonstrate
the precise controllability of our method for multi-character generation. We
also verify the generality of our method by applying it to various personalized
T2I models. Moreover, the quantitative results show that our approach achieves
superior performance compared with previous works.",2024-12-21 05:49:40+00:00,"['Beiyuan Zhang', 'Yue Ma', 'Chunlei Fu', 'Xinyang Song', 'Zhenan Sun', 'Ziqiang Li']",http://arxiv.org/abs/2412.16495v2
Modular Blind Video Quality Assessment,"Blind video quality assessment (BVQA) plays a pivotal role in evaluating and
improving the viewing experience of end-users across a wide range of
video-based platforms and services. Contemporary deep learning-based models
primarily analyze video content in its aggressively subsampled format, while
being blind to the impact of the actual spatial resolution and frame rate on
video quality. In this paper, we propose a modular BVQA model and a method of
training it to improve its modularity. Our model comprises a base quality
predictor, a spatial rectifier, and a temporal rectifier, responding to the
visual content and distortion, spatial resolution, and frame rate changes on
video quality, respectively. During training, spatial and temporal rectifiers
are dropped out with some probabilities to render the base quality predictor a
standalone BVQA model, which should work better with the rectifiers. Extensive
experiments on both professionally-generated content and user-generated content
video databases show that our quality model achieves superior or comparable
performance to current methods. Additionally, the modularity of our model
offers an opportunity to analyze existing video quality databases in terms of
their spatial and temporal complexity.",2024-02-29 15:44:00+00:00,"['Wen Wen', 'Mu Li', 'Yabin Zhang', 'Yiting Liao', 'Junlin Li', 'Li Zhang', 'Kede Ma']",http://arxiv.org/abs/2402.19276v4
Exploring Explainability in Video Action Recognition,"Image Classification and Video Action Recognition are perhaps the two most
foundational tasks in computer vision. Consequently, explaining the inner
workings of trained deep neural networks is of prime importance. While numerous
efforts focus on explaining the decisions of trained deep neural networks in
image classification, exploration in the domain of its temporal version, video
action recognition, has been scant. In this work, we take a deeper look at this
problem. We begin by revisiting Grad-CAM, one of the popular feature
attribution methods for Image Classification, and its extension to Video Action
Recognition tasks and examine the method's limitations. To address these, we
introduce Video-TCAV, by building on TCAV for Image Classification tasks, which
aims to quantify the importance of specific concepts in the decision-making
process of Video Action Recognition models. As the scalable generation of
concepts is still an open problem, we propose a machine-assisted approach to
generate spatial and spatiotemporal concepts relevant to Video Action
Recognition for testing Video-TCAV. We then establish the importance of
temporally-varying concepts by demonstrating the superiority of dynamic
spatiotemporal concepts over trivial spatial concepts. In conclusion, we
introduce a framework for investigating hypotheses in action recognition and
quantitatively testing them, thus advancing research in the explainability of
deep neural networks used in video action recognition.",2024-04-13 19:34:14+00:00,"['Avinab Saha', 'Shashank Gupta', 'Sravan Kumar Ankireddy', 'Karl Chahine', 'Joydeep Ghosh']",http://arxiv.org/abs/2404.09067v1
GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension,"There are substantial instructional videos on the Internet, which provide us
tutorials for completing various tasks. Existing instructional video datasets
only focus on specific steps at the video level, lacking experiential
guidelines at the task level, which can lead to beginners struggling to learn
new tasks due to the lack of relevant experience. Moreover, the specific steps
without guidelines are trivial and unsystematic, making it difficult to provide
a clear tutorial. To address these problems, we present the GUIDE
(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructional
tasks in 8 domains related to our daily life. Specifically, we annotate each
instructional task with a guideline, representing a common pattern shared by
all task-related videos. On this basis, we annotate systematic specific steps,
including their associated guideline steps, specific step descriptions and
timestamps. Our proposed benchmark consists of three sub-tasks to evaluate
comprehension ability of models: (1) Step Captioning: models have to generate
captions for specific steps from videos. (2) Guideline Summarization: models
have to mine the common pattern in task-related videos and summarize a
guideline from them. (3) Guideline-Guided Captioning: models have to generate
captions for specific steps under the guide of guideline. We evaluate plenty of
foundation models with GUIDE and perform in-depth analysis. Given the diversity
and practicality of GUIDE, we believe that it can be used as a better benchmark
for instructional video comprehension.",2024-06-26 10:24:00+00:00,"['Jiafeng Liang', 'Shixin Jiang', 'Zekun Wang', 'Haojie Pan', 'Zerui Chen', 'Zheng Chu', 'Ming Liu', 'Ruiji Fu', 'Zhongyuan Wang', 'Bing Qin']",http://arxiv.org/abs/2406.18227v1
Multimodal Language Models for Domain-Specific Procedural Video Summarization,"Videos serve as a powerful medium to convey ideas, tell stories, and provide
detailed instructions, especially through long-format tutorials. Such tutorials
are valuable for learning new skills at one's own pace, yet they can be
overwhelming due to their length and dense content. Viewers often seek specific
information, like precise measurements or step-by-step execution details,
making it essential to extract and summarize key segments efficiently. An
intelligent, time-sensitive video assistant capable of summarizing and
detecting highlights in long videos is highly sought after. Recent advancements
in Multimodal Large Language Models offer promising solutions to develop such
an assistant. Our research explores the use of multimodal models to enhance
video summarization and step-by-step instruction generation within specific
domains. These models need to understand temporal events and relationships
among actions across video frames. Our approach focuses on fine-tuning TimeChat
to improve its performance in specific domains: cooking and medical procedures.
By training the model on domain-specific datasets like Tasty for cooking and
MedVidQA for medical procedures, we aim to enhance its ability to generate
concise, accurate summaries of instructional videos. We curate and restructure
these datasets to create high-quality video-centric instruction data. Our
findings indicate that when finetuned on domain-specific procedural data,
TimeChat can significantly improve the extraction and summarization of key
instructional steps in long-format videos. This research demonstrates the
potential of specialized multimodal models to assist with practical tasks by
providing personalized, step-by-step guidance tailored to the unique aspects of
each domain.",2024-07-07 15:50:46+00:00,['Nafisa Hussain'],http://arxiv.org/abs/2407.05419v1
InterTrack: Tracking Human Object Interaction without Object Templates,"Tracking human object interaction from videos is important to understand
human behavior from the rapidly growing stream of video data. Previous
video-based methods require predefined object templates while
single-image-based methods are template-free but lack temporal consistency. In
this paper, we present a method to track human object interaction without any
object shape templates. We decompose the 4D tracking problem into per-frame
pose tracking and canonical shape optimization. We first apply a single-view
reconstruction method to obtain temporally-inconsistent per-frame interaction
reconstructions. Then, for the human, we propose an efficient autoencoder to
predict SMPL vertices directly from the per-frame reconstructions, introducing
temporally consistent correspondence. For the object, we introduce a pose
estimator that leverages temporal information to predict smooth object
rotations under occlusions. To train our model, we propose a method to generate
synthetic interaction videos and synthesize in total 10 hour videos of 8.5k
sequences with full 3D ground truth. Experiments on BEHAVE and InterCap show
that our method significantly outperforms previous template-based video
tracking and single-frame reconstruction methods. Our proposed synthetic video
dataset also allows training video-based methods that generalize to real-world
videos. Our code and dataset will be publicly released.",2024-08-25 22:26:46+00:00,"['Xianghui Xie', 'Jan Eric Lenssen', 'Gerard Pons-Moll']",http://arxiv.org/abs/2408.13953v1
VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos,"Fine-grained alignment between videos and text is challenging due to complex
spatial and temporal dynamics in videos. Existing video-based Large Multimodal
Models (LMMs) handle basic conversations but struggle with precise pixel-level
grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed
for fine-grained pixel-level grounding in videos based on user-provided textual
inputs. Our design seamlessly connects three key components: a Large Language
Model, a dual vision encoder that emphasizes both spatial and temporal details,
and a spatio-temporal decoder for accurate mask generation. This connection is
facilitated via tunable V-L and L-V adapters that enable close Vision-Language
(VL) alignment. The architecture is trained to synchronize both spatial and
temporal elements of video content with textual instructions. To enable
fine-grained grounding, we curate a multimodal dataset featuring detailed
visually-grounded conversations using a semiautomatic annotation pipeline,
resulting in a diverse set of 38k video-QA triplets along with 83k objects and
671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded
Conversation Generation, Visual Grounding, and Referring Video Segmentation.
Experimental results show that our model consistently outperforms existing
approaches across all three tasks.",2024-11-07 17:59:27+00:00,"['Shehan Munasinghe', 'Hanan Gani', 'Wenqi Zhu', 'Jiale Cao', 'Eric Xing', 'Fahad Shahbaz Khan', 'Salman Khan']",http://arxiv.org/abs/2411.04923v3
StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification,"Existing large vision-language models (LVLMs) are largely limited to
processing short, seconds-long videos and struggle with generating coherent
descriptions for extended video spanning minutes or more. Long video
description introduces new challenges, such as consistent character
identification and plot-level descriptions incorporating both visual and audio
information. To address these, we figure out audio-visual character
identification, matching character names to each dialogue, as a key factor. We
propose StoryTeller, a system for generating dense descriptions of long videos,
incorporating both low-level visual concepts and high-level plot information.
StoryTeller uses a multimodal large language model that integrates visual,
audio, and text modalities to perform audio-visual character identification on
minute-long video clips. The results are then fed into a LVLM to enhance
consistency of video description. We validate our approach on movie description
tasks and introduce MovieStory101, a dataset with dense descriptions for
three-minute movie clips. To evaluate long video descriptions, we create
StoryQA, a large set of multiple-choice questions for MovieStory101 test set.
We assess descriptions by inputting them into GPT-4 to answer these questions,
using accuracy as an automatic evaluation metric. Experiments show that
StoryTeller outperforms all open and closed-source baselines on StoryQA,
achieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and
demonstrating a +15.56% advantage in human side-by-side evaluations.
Additionally, incorporating audio-visual character identification from
StoryTeller improves the performance of all video description models, with
Gemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%,
respectively, in accuracy on StoryQA.",2024-11-11 15:51:48+00:00,"['Yichen He', 'Yuan Lin', 'Jianchao Wu', 'Hanchong Zhang', 'Yuchen Zhang', 'Ruicheng Le']",http://arxiv.org/abs/2411.07076v2
Grounding Video Models to Actions through Goal Conditioned Exploration,"Large video models, pretrained on massive amounts of Internet video, provide
a rich source of physical knowledge about the dynamics and motions of objects
and tasks. However, video models are not grounded in the embodiment of an
agent, and do not describe how to actuate the world to reach the visual states
depicted in a video. To tackle this problem, current methods use a separate
vision-based inverse dynamic model trained on embodiment-specific data to map
image states to actions. Gathering data to train such a model is often
expensive and challenging, and this model is limited to visual settings similar
to the ones in which data are available. In this paper, we investigate how to
directly ground video models to continuous actions through self-exploration in
the embodied environment -- using generated video states as visual goals for
exploration. We propose a framework that uses trajectory level action
generation in combination with video guidance to enable an agent to solve
complex tasks without any external supervision, e.g., rewards, action labels,
or segmentation masks. We validate the proposed approach on 8 tasks in Libero,
6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual
Navigation. We show how our approach is on par with or even surpasses multiple
behavior cloning baselines trained on expert demonstrations while without
requiring any action annotations.",2024-11-11 18:43:44+00:00,"['Yunhao Luo', 'Yilun Du']",http://arxiv.org/abs/2411.07223v2
VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding,"Recent advancements in video large multimodal models (LMMs) have
significantly improved their video understanding and reasoning capabilities.
However, their performance drops on out-of-distribution (OOD) tasks that are
underrepresented in training data. Traditional methods like fine-tuning on OOD
datasets are impractical due to high computational costs. While In-context
learning (ICL) with demonstration examples has shown promising generalization
performance in language tasks and image-language tasks without fine-tuning,
applying ICL to video-language tasks faces challenges due to the limited
context length in Video LMMs, as videos require longer token lengths. To
address these issues, we propose VideoICL, a novel video in-context learning
framework for OOD tasks that introduces a similarity-based relevant example
selection strategy and a confidence-based iterative inference approach. This
allows to select the most relevant examples and rank them based on similarity,
to be used for inference. If the generated response has low confidence, our
framework selects new examples and performs inference again, iteratively
refining the results until a high-confidence response is obtained. This
approach improves OOD video understanding performance by extending effective
context length without incurring high costs. The experimental results on
multiple benchmarks demonstrate significant performance gains, especially in
domain-specific scenarios, laying the groundwork for broader video
comprehension applications. Code will be released at
https://github.com/KangsanKim07/VideoICL",2024-12-03 05:54:43+00:00,"['Kangsan Kim', 'Geon Park', 'Youngwan Lee', 'Woongyeong Yeo', 'Sung Ju Hwang']",http://arxiv.org/abs/2412.02186v1
Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback,"Recent advancements in large language models have influenced the development
of video large multimodal models (VLMMs). The previous approaches for VLMMs
involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets,
integrating LLM with visual encoders, and adding additional learnable modules.
Video and text multimodal alignment remains challenging, primarily due to the
deficient volume and quality of multimodal instruction-tune data compared to
text-only data. We present a novel alignment strategy that employs multimodal
AI system to oversee itself called Reinforcement Learning from AI Feedback
(RLAIF), providing self-preference feedback to refine itself and facilitating
the alignment of video and text modalities. In specific, we propose
context-aware reward modeling by providing detailed video descriptions as
context during the generation of preference feedback in order to enrich the
understanding of video content. Demonstrating enhanced performance across
diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms
existing approaches, including the SFT model. We commit to open-sourcing our
code, models, and datasets to foster further research in this area.",2024-02-06 06:27:40+00:00,"['Daechul Ahn', 'Yura Choi', 'Youngjae Yu', 'Dongyeop Kang', 'Jonghyun Choi']",http://arxiv.org/abs/2402.03746v3
Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning,"Large Language Models (LLMs) demonstrate remarkable proficiency in
comprehending and handling text-based tasks. Many efforts are being made to
transfer these attributes to video modality, which are termed Video-LLMs.
However, existing Video-LLMs can only capture the coarse-grained semantics and
are unable to effectively handle tasks related to comprehension or localization
of specific video segments. In light of these challenges, we propose Momentor,
a Video-LLM capable of accomplishing fine-grained temporal understanding tasks.
To support the training of Momentor, we design an automatic data generation
engine to construct Moment-10M, a large-scale video instruction dataset with
segment-level instruction data. We train Momentor on Moment-10M, enabling it to
perform segment-level reasoning and localization. Zero-shot evaluations on
several tasks demonstrate that Momentor excels in fine-grained temporally
grounded comprehension and localization.",2024-02-18 03:04:38+00:00,"['Long Qian', 'Juncheng Li', 'Yu Wu', 'Yaobo Ye', 'Hao Fei', 'Tat-Seng Chua', 'Yueting Zhuang', 'Siliang Tang']",http://arxiv.org/abs/2402.11435v2
FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs,"Despite the remarkable performance of video-based large language models
(LLMs), their adversarial threat remains unexplored. To fill this gap, we
propose the first adversarial attack tailored for video-based LLMs by crafting
flow-based multi-modal adversarial perturbations on a small fraction of frames
within a video, dubbed FMM-Attack. Extensive experiments show that our attack
can effectively induce video-based LLMs to generate incorrect answers when
videos are added with imperceptible adversarial perturbations. Intriguingly,
our FMM-Attack can also induce garbling in the model output, prompting
video-based LLMs to hallucinate. Overall, our observations inspire a further
understanding of multi-modal robustness and safety-related feature alignment
across different modalities, which is of great importance for various large
multi-modal models. Our code is available at
https://github.com/THU-Kingmin/FMM-Attack.",2024-03-20 11:05:07+00:00,"['Jinmin Li', 'Kuofeng Gao', 'Yang Bai', 'Jingyun Zhang', 'Shu-tao Xia', 'Yisen Wang']",http://arxiv.org/abs/2403.13507v2
Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention,"Humans utilize their gaze to concentrate on essential information while
perceiving and interpreting intentions in videos. Incorporating human gaze into
computational algorithms can significantly enhance model performance in video
understanding tasks. In this work, we address a challenging and innovative task
in video understanding: predicting the actions of an agent in a video based on
a partial video. We introduce the Gaze-guided Action Anticipation algorithm,
which establishes a visual-semantic graph from the video input. Our method
utilizes a Graph Neural Network to recognize the agent's intention and predict
the action sequence to fulfill this intention. To assess the efficiency of our
approach, we collect a dataset containing household activities generated in the
VirtualHome environment, accompanied by human gaze data of viewing videos. Our
method outperforms state-of-the-art techniques, achieving a 7\% improvement in
accuracy for 18-class intention recognition. This highlights the efficiency of
our method in learning important features from human gaze data.",2024-04-10 21:03:23+00:00,"['Suleyman Ozdel', 'Yao Rong', 'Berat Mert Albaba', 'Yen-Ling Kuo', 'Xi Wang', 'Enkelejda Kasneci']",http://arxiv.org/abs/2404.07347v1
2nd Place Solution for PVUW Challenge 2024: Video Panoptic Segmentation,"Video Panoptic Segmentation (VPS) is a challenging task that is extends from
image panoptic segmentation.VPS aims to simultaneously classify, track, segment
all objects in a video, including both things and stuff. Due to its wide
application in many downstream tasks such as video understanding, video
editing, and autonomous driving. In order to deal with the task of video
panoptic segmentation in the wild, we propose a robust integrated video
panoptic segmentation solution. We use DVIS++ framework as our baseline to
generate the initial masks. Then,we add an additional image semantic
segmentation model to further improve the performance of semantic
classes.Finally, our method achieves state-of-the-art performance with a VPQ
score of 56.36 and 57.12 in the development and test phases, respectively, and
ultimately ranked 2nd in the VPS track of the PVUW Challenge at CVPR2024.",2024-06-01 17:03:16+00:00,"['Biao Wu', 'Diankai Zhang', 'Si Gao', 'Chengjian Zheng', 'Shaoli Liu', 'Ning Wang']",http://arxiv.org/abs/2406.00500v1
Training-Free Robust Interactive Video Object Segmentation,"Interactive video object segmentation is a crucial video task, having various
applications from video editing to data annotating. However, current approaches
struggle to accurately segment objects across diverse domains. Recently,
Segment Anything Model (SAM) introduces interactive visual prompts and
demonstrates impressive performance across different domains. In this paper, we
propose a training-free prompt tracking framework for interactive video object
segmentation (I-PT), leveraging the powerful generalization of SAM. Although
point tracking efficiently captures the pixel-wise information of objects in a
video, points tend to be unstable when tracked over a long period, resulting in
incorrect segmentation. Towards fast and robust interaction, we jointly adopt
sparse points and boxes tracking, filtering out unstable points and capturing
object-wise information. To better integrate reference information from
multiple interactions, we introduce a cross-round space-time module (CRSTM),
which adaptively aggregates mask features from previous rounds and frames,
enhancing the segmentation stability. Our framework has demonstrated robust
zero-shot video segmentation results on popular VOS datasets with interaction
types, including DAVIS 2017, YouTube-VOS 2018, and MOSE 2023, maintaining a
good tradeoff between performance and interaction time.",2024-06-08 14:25:57+00:00,"['Xiaoli Wei', 'Zhaoqing Wang', 'Yandong Guo', 'Chunxia Zhang', 'Tongliang Liu', 'Mingming Gong']",http://arxiv.org/abs/2406.05485v1
VcLLM: Video Codecs are Secretly Tensor Codecs,"As the parameter size of large language models (LLMs) continues to expand,
the need for a large memory footprint and high communication bandwidth have
become significant bottlenecks for the training and inference of LLMs. To
mitigate these bottlenecks, various tensor compression techniques have been
proposed to reduce the data size, thereby alleviating memory requirements and
communication pressure.
  Our research found that video codecs, despite being originally designed for
compressing videos, show excellent efficiency when compressing various types of
tensors. We demonstrate that video codecs can be versatile and general-purpose
tensor codecs while achieving the state-of-the-art compression efficiency in
various tasks. We further make use of the hardware video encoding and decoding
module available on GPUs to create a framework capable of both inference and
training with video codecs repurposed as tensor codecs. This greatly reduces
the requirement for memory capacity and communication bandwidth, enabling
training and inference of large models on consumer-grade GPUs.",2024-06-29 15:24:33+00:00,"['Ceyu Xu', 'Yongji Wu', 'Xinyu Yang', 'Beidi Chen', 'Matthew Lentz', 'Danyang Zhuo', 'Lisa Wu Wills']",http://arxiv.org/abs/2407.00467v1
VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool,"Multimodal large language models (MLLMs) are flourishing, but mainly focus on
images with less attention than videos, especially in sub-fields such as prompt
engineering, video chain-of-thought (CoT), and instruction tuning on videos.
Therefore, we try to explore the collection of CoT datasets in videos to lead
to video OpenQA and improve the reasoning ability of MLLMs. Unfortunately,
making such video CoT datasets is not an easy task. Given that human annotation
is too cumbersome and expensive, while machine-generated is not reliable due to
the hallucination issue, we develop an automatic annotation tool that combines
machine and human experts, under the active learning paradigm. Active learning
is an interactive strategy between the model and human experts, in this way,
the workload of human labeling can be reduced and the quality of the dataset
can be guaranteed. With the help of the automatic annotation tool, we strive to
contribute three datasets, namely VideoCoT, TopicQA, TopicCoT. Furthermore, we
propose a simple but effective benchmark based on the collected datasets, which
exploits CoT to maximize the complex reasoning capabilities of MLLMs. Extensive
experiments demonstrate the effectiveness our solution.",2024-07-07 13:10:23+00:00,"['Yan Wang', 'Yawen Zeng', 'Jingsheng Zheng', 'Xiaofen Xing', 'Jin Xu', 'Xiangmin Xu']",http://arxiv.org/abs/2407.05355v1
Bridging Information Asymmetry in Text-video Retrieval: A Data-centric Approach,"As online video content rapidly grows, the task of text-video retrieval (TVR)
becomes increasingly important. A key challenge in TVR is the information
asymmetry between video and text: videos are inherently richer in information,
while their textual descriptions often capture only fragments of this
complexity. This paper introduces a novel, data-centric framework to bridge
this gap by enriching textual representations to better match the richness of
video content. During training, videos are segmented into event-level clips and
captioned to ensure comprehensive coverage. During retrieval, a large language
model (LLM) generates semantically diverse queries to capture a broader range
of possible matches. To enhance retrieval efficiency, we propose a query
selection mechanism that identifies the most relevant and diverse queries,
reducing computational cost while improving accuracy. Our method achieves
state-of-the-art results across multiple benchmarks, demonstrating the power of
data-centric approaches in addressing information asymmetry in TVR. This work
paves the way for new research focused on leveraging data to improve
cross-modal retrieval.",2024-08-14 01:24:09+00:00,"['Zechen Bai', 'Tianjun Xiao', 'Tong He', 'Pichao Wang', 'Zheng Zhang', 'Thomas Brox', 'Mike Zheng Shou']",http://arxiv.org/abs/2408.07249v2
FBD-SV-2024: Flying Bird Object Detection Dataset in Surveillance Video,"A Flying Bird Dataset for Surveillance Videos (FBD-SV-2024) is introduced and
tailored for the development and performance evaluation of flying bird
detection algorithms in surveillance videos. This dataset comprises 483 video
clips, amounting to 28,694 frames in total. Among them, 23,833 frames contain
28,366 instances of flying birds. The proposed dataset of flying birds in
surveillance videos is collected from realistic surveillance scenarios, where
the birds exhibit characteristics such as inconspicuous features in single
frames (in some instances), generally small sizes, and shape variability during
flight. These attributes pose challenges that need to be addressed when
developing flying bird detection methods for surveillance videos. Finally,
advanced (video) object detection algorithms were selected for experimentation
on the proposed dataset, and the results demonstrated that this dataset remains
challenging for the algorithms above. The FBD-SV-2024 is now publicly
available: Please visit https://github.com/Ziwei89/FBD-SV-2024_github for the
dataset download link and related processing scripts.",2024-08-31 01:11:57+00:00,"['Zi-Wei Sun', 'Ze-Xi Hua', 'Heng-Chao Li', 'Zhi-Peng Qi', 'Xiang Li', 'Yan Li', 'Jin-Chi Zhang']",http://arxiv.org/abs/2409.00317v1
AirLetters: An Open Video Dataset of Characters Drawn in the Air,"We introduce AirLetters, a new video dataset consisting of real-world videos
of human-generated, articulated motions. Specifically, our dataset requires a
vision model to predict letters that humans draw in the air. Unlike existing
video datasets, accurate classification predictions for AirLetters rely
critically on discerning motion patterns and on integrating long-range
information in the video over time. An extensive evaluation of state-of-the-art
image and video understanding models on AirLetters shows that these methods
perform poorly and fall far behind a human baseline. Our work shows that,
despite recent progress in end-to-end video understanding, accurate
representations of complex articulated motions -- a task that is trivial for
humans -- remains an open problem for end-to-end learning.",2024-10-03 19:13:28+00:00,"['Rishit Dagli', 'Guillaume Berger', 'Joanna Materzynska', 'Ingo Bax', 'Roland Memisevic']",http://arxiv.org/abs/2410.02921v1
Video Summarization Techniques: A Comprehensive Review,"The rapid expansion of video content across a variety of industries,
including social media, education, entertainment, and surveillance, has made
video summarization an essential field of study. The current work is a survey
that explores the various approaches and methods created for video summarizing,
emphasizing both abstractive and extractive strategies. The process of
extractive summarization involves the identification of key frames or segments
from the source video, utilizing methods such as shot boundary recognition, and
clustering. On the other hand, abstractive summarization creates new content by
getting the essential content from the video, using machine learning models
like deep neural networks and natural language processing, reinforcement
learning, attention mechanisms, generative adversarial networks, and
multi-modal learning. We also include approaches that incorporate the two
methodologies, along with discussing the uses and difficulties encountered in
real-world implementations. The paper also covers the datasets used to
benchmark these techniques. This review attempts to provide a state-of-the-art
thorough knowledge of the current state and future directions of video
summarization research.",2024-10-06 11:17:54+00:00,"['Toqa Alaa', 'Ahmad Mongy', 'Assem Bakr', 'Mariam Diab', 'Walid Gomaa']",http://arxiv.org/abs/2410.04449v1
ChatVTG: Video Temporal Grounding via Chat with Video Dialogue Large Language Models,"Video Temporal Grounding (VTG) aims to ground specific segments within an
untrimmed video corresponding to the given natural language query. Existing VTG
methods largely depend on supervised learning and extensive annotated data,
which is labor-intensive and prone to human biases. To address these
challenges, we present ChatVTG, a novel approach that utilizes Video Dialogue
Large Language Models (LLMs) for zero-shot video temporal grounding. Our
ChatVTG leverages Video Dialogue LLMs to generate multi-granularity segment
captions and matches these captions with the given query for coarse temporal
grounding, circumventing the need for paired annotation data. Furthermore, to
obtain more precise temporal grounding results, we employ moment refinement for
fine-grained caption proposals. Extensive experiments on three mainstream VTG
datasets, including Charades-STA, ActivityNet-Captions, and TACoS, demonstrate
the effectiveness of ChatVTG. Our ChatVTG surpasses the performance of current
zero-shot methods.",2024-10-01 08:27:56+00:00,"['Mengxue Qu', 'Xiaodong Chen', 'Wu Liu', 'Alicia Li', 'Yao Zhao']",http://arxiv.org/abs/2410.12813v1
StableV2V: Stablizing Shape Consistency in Video-to-Video Editing,"Recent advancements of generative AI have significantly promoted content
creation and editing, where prevailing studies further extend this exciting
progress to video editing. In doing so, these studies mainly transfer the
inherent motion patterns from the source videos to the edited ones, where
results with inferior consistency to user prompts are often observed, due to
the lack of particular alignments between the delivered motions and edited
contents. To address this limitation, we present a shape-consistent video
editing method, namely StableV2V, in this paper. Our method decomposes the
entire editing pipeline into several sequential procedures, where it edits the
first video frame, then establishes an alignment between the delivered motions
and user prompts, and eventually propagates the edited contents to all other
frames based on such alignment. Furthermore, we curate a testing benchmark,
namely DAVIS-Edit, for a comprehensive evaluation of video editing, considering
various types of prompts and difficulties. Experimental results and analyses
illustrate the outperforming performance, visual consistency, and inference
efficiency of our method compared to existing state-of-the-art studies.",2024-11-17 11:48:01+00:00,"['Chang Liu', 'Rui Li', 'Kaidong Zhang', 'Yunwei Lan', 'Dong Liu']",http://arxiv.org/abs/2411.11045v1
Motion Free B-frame Coding for Neural Video Compression,"Typical deep neural video compression networks usually follow the hybrid
approach of classical video coding that contains two separate modules: motion
coding and residual coding. In addition, a symmetric auto-encoder is often used
as a normal architecture for both motion and residual coding. In this paper, we
propose a novel approach that handles the drawbacks of the two typical
above-mentioned architectures, we call it kernel-based motion-free video
coding. The advantages of the motion-free approach are twofold: it improves the
coding efficiency of the network and significantly reduces computational
complexity thanks to eliminating motion estimation, motion compensation, and
motion coding which are the most time-consuming engines. In addition, the
kernel-based auto-encoder alleviates blur artifacts that usually occur with the
conventional symmetric autoencoder. Consequently, it improves the visual
quality of the reconstructed frames. Experimental results show the proposed
framework outperforms the SOTA deep neural video compression networks on the
HEVC-class B dataset and is competitive on the UVG and MCL-JCV datasets. In
addition, it generates high-quality reconstructed frames in comparison with
conventional motion coding-based symmetric auto-encoder meanwhile its model
size is much smaller than that of the motion-based networks around three to
four times.",2024-11-26 07:03:11+00:00,['Van Thang Nguyen'],http://arxiv.org/abs/2411.17160v1
MCUCoder: Adaptive Bitrate Learned Video Compression for IoT Devices,"The rapid growth of camera-based IoT devices demands the need for efficient
video compression, particularly for edge applications where devices face
hardware constraints, often with only 1 or 2 MB of RAM and unstable internet
connections. Traditional and deep video compression methods are designed for
high-end hardware, exceeding the capabilities of these constrained devices.
Consequently, video compression in these scenarios is often limited to M-JPEG
due to its high hardware efficiency and low complexity. This paper introduces ,
an open-source adaptive bitrate video compression model tailored for
resource-limited IoT settings. MCUCoder features an ultra-lightweight encoder
with only 10.5K parameters and a minimal 350KB memory footprint, making it
well-suited for edge devices and MCUs. While MCUCoder uses a similar amount of
energy as M-JPEG, it reduces bitrate by 55.65% on the MCL-JCV dataset and
55.59% on the UVG dataset, measured in MS-SSIM. Moreover, MCUCoder supports
adaptive bitrate streaming by generating a latent representation that is sorted
by importance, allowing transmission based on available bandwidth. This ensures
smooth real-time video transmission even under fluctuating network conditions
on low-resource devices. Source code available at
https://github.com/ds-kiel/MCUCoder.",2024-11-29 03:00:21+00:00,"['Ali Hojjat', 'Janek Haberer', 'Olaf Landsiedel']",http://arxiv.org/abs/2411.19442v1
Progress-Aware Video Frame Captioning,"While image captioning provides isolated descriptions for individual images,
and video captioning offers one single narrative for an entire video clip, our
work explores an important middle ground: progress-aware video captioning at
the frame level. This novel task aims to generate temporally fine-grained
captions that not only accurately describe each frame but also capture the
subtle progression of actions throughout a video sequence. Despite the strong
capabilities of existing leading vision language models, they often struggle to
discern the nuances of frame-wise differences. To address this, we propose
ProgressCaptioner, a captioning model designed to capture the fine-grained
temporal dynamics within an action sequence. Alongside, we develop the FrameCap
dataset to support training and the FrameCapEval benchmark to assess caption
quality. The results demonstrate that ProgressCaptioner significantly surpasses
leading captioning models, producing precise captions that accurately capture
action progression and set a new standard for temporal precision in video
captioning. Finally, we showcase practical applications of our approach,
specifically in aiding keyframe selection and advancing video understanding,
highlighting its broad utility.",2024-12-03 01:21:28+00:00,"['Zihui Xue', 'Joungbin An', 'Xitong Yang', 'Kristen Grauman']",http://arxiv.org/abs/2412.02071v2
Towards Open-Vocabulary Video Semantic Segmentation,"Semantic segmentation in videos has been a focal point of recent research.
However, existing models encounter challenges when faced with unfamiliar
categories. To address this, we introduce the Open Vocabulary Video Semantic
Segmentation (OV-VSS) task, designed to accurately segment every pixel across a
wide range of open-vocabulary categories, including those that are novel or
previously unexplored. To enhance OV-VSS performance, we propose a robust
baseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing
the model to utilize temporal relationships across consecutive frames.
Additionally, we incorporate a random frame enhancement module, broadening the
model's understanding of semantic context throughout the entire video sequence.
Our approach also includes video text encoding, which strengthens the model's
capability to interpret textual information within the video context.
Comprehensive evaluations on benchmark datasets such as VSPW and Cityscapes
highlight OV-VSS's zero-shot generalization capabilities, especially in
handling novel categories. The results validate OV2VSS's effectiveness,
demonstrating improved performance in semantic segmentation tasks across
diverse video datasets.",2024-12-12 14:53:16+00:00,"['Xinhao Li', 'Yun Liu', 'Guolei Sun', 'Min Wu', 'Le Zhang', 'Ce Zhu']",http://arxiv.org/abs/2412.09329v1
VidTok: A Versatile and Open-Source Video Tokenizer,"Encoding video content into compact latent tokens has become a fundamental
step in video generation and understanding, driven by the need to address the
inherent redundancy in pixel-level representations. Consequently, there is a
growing demand for high-performance, open-source video tokenizers as
video-centric research gains prominence. We introduce VidTok, a versatile video
tokenizer that delivers state-of-the-art performance in both continuous and
discrete tokenizations. VidTok incorporates several key advancements over
existing approaches: 1) model architecture such as convolutional layers and
up/downsampling modules; 2) to address the training instability and codebook
collapse commonly associated with conventional Vector Quantization (VQ), we
integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)
improved training strategies, including a two-stage training process and the
use of reduced frame rates. By integrating these advancements, VidTok achieves
substantial improvements over existing methods, demonstrating superior
performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,
under standardized evaluation settings.",2024-12-17 16:27:11+00:00,"['Anni Tang', 'Tianyu He', 'Junliang Guo', 'Xinle Cheng', 'Li Song', 'Jiang Bian']",http://arxiv.org/abs/2412.13061v1
G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis,"We propose G-HOP, a denoising diffusion based generative prior for
hand-object interactions that allows modeling both the 3D object and a human
hand, conditioned on the object category. To learn a 3D spatial diffusion model
that can capture this joint distribution, we represent the human hand via a
skeletal distance field to obtain a representation aligned with the (latent)
signed distance field for the object. We show that this hand-object prior can
then serve as generic guidance to facilitate other tasks like reconstruction
from interaction clip and human grasp synthesis. We believe that our model,
trained by aggregating seven diverse real-world interaction datasets spanning
across 155 categories, represents a first approach that allows jointly
generating both hand and object. Our empirical evaluations demonstrate the
benefit of this joint prior in video-based reconstruction and human grasp
synthesis, outperforming current task-specific baselines.
  Project website: https://judyye.github.io/ghop-www",2024-04-18 17:59:28+00:00,"['Yufei Ye', 'Abhinav Gupta', 'Kris Kitani', 'Shubham Tulsiani']",http://arxiv.org/abs/2404.12383v1
"Audio-Agent: Leveraging LLMs For Audio Generation, Editing and Composition","We introduce Audio-Agent, a multimodal framework for audio generation,
editing and composition based on text or video inputs. Conventional approaches
for text-to-audio (TTA) tasks often make single-pass inferences from text
descriptions. While straightforward, this design struggles to produce
high-quality audio when given complex text conditions. In our method, we
utilize a pre-trained TTA diffusion network as the audio generation agent to
work in tandem with GPT-4, which decomposes the text condition into atomic,
specific instructions and calls the agent for audio generation. In doing so,
Audio-Agent can generate high-quality audio that is closely aligned with the
provided text or video exhibiting complex and multiple events, while supporting
variable-length and variable-volume generation. For video-to-audio (VTA) tasks,
most existing methods require training a timestamp detector to synchronize
video events with the generated audio, a process that can be tedious and
time-consuming. Instead, we propose a simpler approach by fine-tuning a
pre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both
semantic and temporal conditions that bridge the video and audio modality.
Consequently, our framework contributes a comprehensive solution for both TTA
and VTA tasks without substantial computational overhead in training.",2024-10-04 11:40:53+00:00,"['Zixuan Wang', 'Chi-Keung Tang', 'Yu-Wing Tai']",http://arxiv.org/abs/2410.03335v2
Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's Impact on Spatio-Temporal Cross-Attentions,"This paper investigates the role of CLIP image embeddings within the Stable
Video Diffusion (SVD) framework, focusing on their impact on video generation
quality and computational efficiency. Our findings indicate that CLIP
embeddings, while crucial for aesthetic quality, do not significantly
contribute towards the subject and background consistency of video outputs.
Moreover, the computationally expensive cross-attention mechanism can be
effectively replaced by a simpler linear layer. This layer is computed only
once at the first diffusion inference step, and its output is then cached and
reused throughout the inference process, thereby enhancing efficiency while
maintaining high-quality outputs. Building on these insights, we introduce the
VCUT, a training-free approach optimized for efficiency within the SVD
architecture. VCUT eliminates temporal cross-attention and replaces spatial
cross-attention with a one-time computed linear layer, significantly reducing
computational load. The implementation of VCUT leads to a reduction of up to
322T Multiple-Accumulate Operations (MACs) per video and a decrease in model
parameters by up to 50M, achieving a 20% reduction in latency compared to the
baseline. Our approach demonstrates that conditioning during the Semantic
Binding stage is sufficient, eliminating the need for continuous computation
across all inference steps and setting a new standard for efficient video
generation.",2024-07-27 08:21:14+00:00,"['Ashkan Taghipour', 'Morteza Ghahremani', 'Mohammed Bennamoun', 'Aref Miri Rekavandi', 'Zinuo Li', 'Hamid Laga', 'Farid Boussaid']",http://arxiv.org/abs/2407.19205v1
A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images,"Currently, medical image domain translation operations show a high demand
from researchers and clinicians. Amongst other capabilities, this task allows
the generation of new medical images with sufficiently high image quality,
making them clinically relevant. Deep Learning (DL) architectures, most
specifically deep generative models, are widely used to generate and translate
images from one domain to another. The proposed framework relies on an
adversarial Denoising Diffusion Model (DDM) to synthesize echocardiography
images and perform domain translation. Contrary to Generative Adversarial
Networks (GANs), DDMs are able to generate high quality image samples with a
large diversity. If a DDM is combined with a GAN, this ability to generate new
data is completed at an even faster sampling time. In this work we trained an
adversarial DDM combined with a GAN to learn the reverse denoising process,
relying on a guide image, making sure relevant anatomical structures of each
echocardiography image were kept and represented on the generated image
samples. For several domain translation operations, the results verified that
such generative model was able to synthesize high quality image samples: MSE:
11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposed
method showed high generalization ability, introducing a framework to create
echocardiography images suitable to be used for clinical research purposes.",2024-03-07 15:58:03+00:00,"['Cristiana Tiago', 'Sten Roar Snare', 'Jurica Sprem', 'Kristin McLeod']",http://arxiv.org/abs/2403.04612v1
VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation,"Recent advancements utilizing large-scale video data for learning video
generation models demonstrate significant potential in understanding complex
physical dynamics. It suggests the feasibility of leveraging diverse robot
trajectory data to develop a unified, dynamics-aware model to enhance robot
manipulation. However, given the relatively small amount of available robot
data, directly fitting data without considering the relationship between visual
observations and actions could lead to suboptimal data utilization. To this
end, we propose VidMan (Video Diffusion for Robot Manipulation), a novel
framework that employs a two-stage training mechanism inspired by dual-process
theory from neuroscience to enhance stability and improve data utilization
efficiency. Specifically, in the first stage, VidMan is pre-trained on the Open
X-Embodiment dataset (OXE) for predicting future visual trajectories in a video
denoising diffusion manner, enabling the model to develop a long horizontal
awareness of the environment's dynamics. In the second stage, a flexible yet
effective layer-wise self-attention adapter is introduced to transform VidMan
into an efficient inverse dynamics model that predicts action modulated by the
implicit dynamics knowledge via parameter sharing. Our VidMan framework
outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark,
achieving a 11.7% relative improvement, and demonstrates over 9% precision
gains on the OXE small-scale dataset. These results provide compelling evidence
that world models can significantly enhance the precision of robot action
prediction. Codes and models will be public.",2024-11-14 03:13:26+00:00,"['Youpeng Wen', 'Junfan Lin', 'Yi Zhu', 'Jianhua Han', 'Hang Xu', 'Shen Zhao', 'Xiaodan Liang']",http://arxiv.org/abs/2411.09153v1
Deformation-Recovery Diffusion Model (DRDM): Instance Deformation for Image Manipulation and Synthesis,"In medical imaging, the diffusion models have shown great potential in
synthetic image generation tasks. However, these models often struggle with the
interpretable connections between the generated and existing images and could
create illusions. To address these challenges, our research proposes a novel
diffusion-based generative model based on deformation diffusion and recovery.
This model, named Deformation-Recovery Diffusion Model (DRDM), diverges from
traditional score/intensity and latent feature-based approaches, emphasizing
morphological changes through deformation fields rather than direct image
synthesis. This is achieved by introducing a topological-preserving deformation
field generation method, which randomly samples and integrates a set of
multi-scale Deformation Vector Fields (DVF). DRDM is trained to learn to
recover unreasonable deformation components, thereby restoring each randomly
deformed image to a realistic distribution. These innovations facilitate the
generation of diverse and anatomically plausible deformations, enhancing data
augmentation and synthesis for further analysis in downstream tasks, such as
few-shot learning and image registration. Experimental results in cardiac MRI
and pulmonary CT show DRDM is capable of creating diverse, large (over 10\%
image size deformation scale), and high-quality (negative rate of the Jacobian
matrix's determinant is lower than 1\%) deformation fields. The further
experimental results in downstream tasks, 2D image segmentation and 3D image
registration, indicate significant improvements resulting from DRDM, showcasing
the potential of our model to advance image manipulation and synthesis in
medical imaging and beyond.
  Project page: https://jianqingzheng.github.io/def_diff_rec/",2024-07-10 01:26:48+00:00,"['Jian-Qing Zheng', 'Yuanhan Mo', 'Yang Sun', 'Jiahua Li', 'Fuping Wu', 'Ziyang Wang', 'Tonia Vincent', 'Bartomiej W. Papie']",http://arxiv.org/abs/2407.07295v2
VideoPhy: Evaluating Physical Commonsense for Video Generation,"Recent advances in internet-scale video data pretraining have led to the
development of text-to-video generative models that can create high-quality
videos across a broad range of visual concepts, synthesize realistic motions
and render complex objects. Hence, these generative models have the potential
to become general-purpose simulators of the physical world. However, it is
unclear how far we are from this goal with the existing text-to-video
generative models. To this end, we present VideoPhy, a benchmark designed to
assess whether the generated videos follow physical commonsense for real-world
activities (e.g. marbles will roll down when placed on a slanted surface).
Specifically, we curate diverse prompts that involve interactions between
various material types in the physical world (e.g., solid-solid, solid-fluid,
fluid-fluid). We then generate videos conditioned on these captions from
diverse state-of-the-art text-to-video generative models, including open models
(e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our human
evaluation reveals that the existing models severely lack the ability to
generate videos adhering to the given text prompts, while also lack physical
commonsense. Specifically, the best performing model, CogVideoX-5B, generates
videos that adhere to the caption and physical laws for 39.6% of the instances.
VideoPhy thus highlights that the video generative models are far from
accurately simulating the physical world. Finally, we propose an
auto-evaluator, VideoCon-Physics, to assess the performance reliably for the
newly released models.",2024-06-05 17:53:55+00:00,"['Hritik Bansal', 'Zongyu Lin', 'Tianyi Xie', 'Zeshun Zong', 'Michal Yarom', 'Yonatan Bitton', 'Chenfanfu Jiang', 'Yizhou Sun', 'Kai-Wei Chang', 'Aditya Grover']",http://arxiv.org/abs/2406.03520v2
Tiled Diffusion,"Image tiling -- the seamless connection of disparate images to create a
coherent visual field -- is crucial for applications such as texture creation,
video game asset development, and digital art. Traditionally, tiles have been
constructed manually, a method that poses significant limitations in
scalability and flexibility. Recent research has attempted to automate this
process using generative models. However, current approaches primarily focus on
tiling textures and manipulating models for single-image generation, without
inherently supporting the creation of multiple interconnected tiles across
diverse domains. This paper presents Tiled Diffusion, a novel approach that
extends the capabilities of diffusion models to accommodate the generation of
cohesive tiling patterns across various domains of image synthesis that require
tiling. Our method supports a wide range of tiling scenarios, from self-tiling
to complex many-to-many connections, enabling seamless integration of multiple
images. Tiled Diffusion automates the tiling process, eliminating the need for
manual intervention and enhancing creative possibilities in various
applications, such as seamlessly tiling of existing images, tiled texture
creation, and 360$^\circ$ synthesis.",2024-12-19 18:55:25+00:00,"['Or Madar', 'Ohad Fried']",http://arxiv.org/abs/2412.15185v4
Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation and Reconstruction,"Existing feedforward image-to-3D methods mainly rely on 2D multi-view
diffusion models that cannot guarantee 3D consistency. These methods easily
collapse when changing the prompt view direction and mainly handle
object-centric cases. In this paper, we propose a novel single-stage 3D
diffusion model, DiffusionGS, for object generation and scene reconstruction
from a single view. DiffusionGS directly outputs 3D Gaussian point clouds at
each timestep to enforce view consistency and allow the model to generate
robustly given prompt views of any directions, beyond object-centric inputs.
Plus, to improve the capability and generality of DiffusionGS, we scale up 3D
training data by developing a scene-object mixed training strategy. Experiments
show that DiffusionGS yields improvements of 2.20 dB/23.25 and 1.34 dB/19.16 in
PSNR/FID for objects and scenes than the state-of-the-art methods, without
depth estimator. Plus, our method enjoys over 5$\times$ faster speed ($\sim$6s
on an A100 GPU). Our Project page at
https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and
interactive results.",2024-11-21 18:21:24+00:00,"['Yuanhao Cai', 'He Zhang', 'Kai Zhang', 'Yixun Liang', 'Mengwei Ren', 'Fujun Luan', 'Qing Liu', 'Soo Ye Kim', 'Jianming Zhang', 'Zhifei Zhang', 'Yuqian Zhou', 'Yulun Zhang', 'Xiaokang Yang', 'Zhe Lin', 'Alan Yuille']",http://arxiv.org/abs/2411.14384v3
SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction,"Graph-based holistic scene representations facilitate surgical workflow
understanding and have recently demonstrated significant success. However, this
task is often hindered by the limited availability of densely annotated
surgical scene data. In this work, we introduce an end-to-end framework for the
generation and optimization of surgical scene graphs on a downstream task. Our
approach leverages the flexibility of graph-based spectral clustering and the
generalization capability of foundation models to generate unsupervised scene
graphs with learnable properties. We reinforce the initial spatial graph with
sparse temporal connections using local matches between consecutive frames to
predict temporally consistent clusters across a temporal neighborhood. By
jointly optimizing the spatiotemporal relations and node features of the
dynamic scene graph with the downstream task of phase segmentation, we address
the costly and annotation-burdensome task of semantic scene comprehension and
scene graph generation in surgical videos using only weak surgical phase
labels. Further, by incorporating effective intermediate scene representation
disentanglement steps within the pipeline, our solution outperforms the SOTA on
the CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflow
recognition",2024-07-29 17:44:34+00:00,"['ahan Kksal', 'Ghazal Ghazaei', 'Felix Holm', 'Azade Farshad', 'Nassir Navab']",http://arxiv.org/abs/2407.20214v2
Wolf: Dense Video Captioning with a World Summarization Framework,"We propose Wolf, a WOrLd summarization Framework for accurate video
captioning. Wolf is an automated captioning framework that adopts a
mixture-of-experts approach, leveraging complementary strengths of Vision
Language Models (VLMs). By utilizing both image and video models, our framework
captures different levels of information and summarizes them efficiently. Our
approach can be applied to enhance video understanding, auto-labeling, and
captioning. To evaluate caption quality, we introduce CapScore, an LLM-based
metric to assess the similarity and quality of generated captions compared to
the ground truth captions. We further build four human-annotated datasets in
three domains: autonomous driving, general scenes, and robotics, to facilitate
comprehensive comparisons. We show that Wolf achieves superior captioning
performance compared to state-of-the-art approaches from the research community
(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For
instance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise
by 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,
we establish a benchmark for video captioning and introduce a leaderboard,
aiming to accelerate advancements in video understanding, captioning, and data
alignment. Webpage: https://wolfv0.github.io/.",2024-07-26 17:59:09+00:00,"['Boyi Li', 'Ligeng Zhu', 'Ran Tian', 'Shuhan Tan', 'Yuxiao Chen', 'Yao Lu', 'Yin Cui', 'Sushant Veer', 'Max Ehrlich', 'Jonah Philion', 'Xinshuo Weng', 'Fuzhao Xue', 'Linxi Fan', 'Yuke Zhu', 'Jan Kautz', 'Andrew Tao', 'Ming-Yu Liu', 'Sanja Fidler', 'Boris Ivanovic', 'Trevor Darrell', 'Jitendra Malik', 'Song Han', 'Marco Pavone']",http://arxiv.org/abs/2407.18908v2
SnapCap: Efficient Snapshot Compressive Video Captioning,"Video Captioning (VC) is a challenging multi-modal task since it requires
describing the scene in language by understanding various and complex videos.
For machines, the traditional VC follows the
""imaging-compression-decoding-and-then-captioning"" pipeline, where compression
is pivot for storage and transmission. However, in such a pipeline, some
potential shortcomings are inevitable, i.e., information redundancy resulting
in low efficiency and information loss during the sampling process for
captioning. To address these problems, in this paper, we propose a novel VC
pipeline to generate captions directly from the compressed measurement, which
can be captured by a snapshot compressive sensing camera and we dub our model
SnapCap. To be more specific, benefiting from the signal simulation, we have
access to obtain abundant measurement-video-annotation data pairs for our
model. Besides, to better extract language-related visual representations from
the compressed measurement, we propose to distill the knowledge from videos via
a pre-trained CLIP with plentiful language-vision associations to guide the
learning of our SnapCap. To demonstrate the effectiveness of SnapCap, we
conduct experiments on two widely-used VC datasets. Both the qualitative and
quantitative results verify the superiority of our pipeline over conventional
VC pipelines. In particular, compared to the ""caption-after-reconstruction""
methods, our SnapCap can run at least 3$\times$ faster, and achieve better
caption results.",2024-01-10 03:11:21+00:00,"['Jianqiao Sun', 'Yudi Su', 'Hao Zhang', 'Ziheng Cheng', 'Zequn Zeng', 'Zhengjue Wang', 'Bo Chen', 'Xin Yuan']",http://arxiv.org/abs/2401.04903v1
EventHDR: from Event to High-Speed HDR Videos and Beyond,"Event cameras are innovative neuromorphic sensors that asynchronously capture
the scene dynamics. Due to the event-triggering mechanism, such cameras record
event streams with much shorter response latency and higher intensity
sensitivity compared to conventional cameras. On the basis of these features,
previous works have attempted to reconstruct high dynamic range (HDR) videos
from events, but have either suffered from unrealistic artifacts or failed to
provide sufficiently high frame rates. In this paper, we present a recurrent
convolutional neural network that reconstruct high-speed HDR videos from event
sequences, with a key frame guidance to prevent potential error accumulation
caused by the sparse event data. Additionally, to address the problem of
severely limited real dataset, we develop a new optical system to collect a
real-world dataset with paired high-speed HDR videos and event streams,
facilitating future research in this field. Our dataset provides the first real
paired dataset for event-to-HDR reconstruction, avoiding potential inaccuracies
from simulation strategies. Experimental results demonstrate that our method
can generate high-quality, high-speed HDR videos. We further explore the
potential of our work in cross-camera reconstruction and downstream computer
vision tasks, including object detection, panoramic segmentation, optical flow
estimation, and monocular depth estimation under HDR scenarios.",2024-09-25 15:32:07+00:00,"['Yunhao Zou', 'Ying Fu', 'Tsuyoshi Takatani', 'Yinqiang Zheng']",http://arxiv.org/abs/2409.17029v1
Learning Camera Movement Control from Real-World Drone Videos,"This study seeks to automate camera movement control for filming existing
subjects into attractive videos, contrasting with the creation of non-existent
content by directly generating the pixels. We select drone videos as our test
case due to their rich and challenging motion patterns, distinctive viewing
angles, and precise controls. Existing AI videography methods struggle with
limited appearance diversity in simulation training, high costs of recording
expert operations, and difficulties in designing heuristic-based goals to cover
all scenarios. To avoid these issues, we propose a scalable method that
involves collecting real-world training data to improve diversity, extracting
camera trajectories automatically to minimize annotation costs, and training an
effective architecture that does not rely on heuristics. Specifically, we
collect 99k high-quality trajectories by running 3D reconstruction on online
videos, connecting camera poses from consecutive frames to formulate 3D camera
paths, and using Kalman filter to identify and remove low-quality data.
Moreover, we introduce DVGFormer, an auto-regressive transformer that leverages
the camera path and images from all past frames to predict camera movement in
the next frame. We evaluate our system across 38 synthetic natural scenes and 7
real city 3D scans. We show that our system effectively learns to perform
challenging camera movements such as navigating through obstacles, maintaining
low altitude to increase perceived speed, and orbiting towers and buildings,
which are very useful for recording high-quality videos. Data and code are
available at dvgformer.github.io.",2024-12-12 18:59:54+00:00,"['Yunzhong Hou', 'Liang Zheng', 'Philip Torr']",http://arxiv.org/abs/2412.09620v1
Pandora: Towards General World Model with Natural Language Actions and Video States,"World models simulate future states of the world in response to different
actions. They facilitate interactive content creation and provides a foundation
for grounded, long-horizon reasoning. Current foundation models do not fully
meet the capabilities of general world models: large language models (LLMs) are
constrained by their reliance on language modality and their limited
understanding of the physical world, while video models lack interactive action
control over the world simulations. This paper makes a step towards building a
general world model by introducing Pandora, a hybrid autoregressive-diffusion
model that simulates world states by generating videos and allows real-time
control with free-text actions. Pandora achieves domain generality, video
consistency, and controllability through large-scale pretraining and
instruction tuning. Crucially, Pandora bypasses the cost of
training-from-scratch by integrating a pretrained LLM (7B) and a pretrained
video model, requiring only additional lightweight finetuning. We illustrate
extensive outputs by Pandora across diverse domains (indoor/outdoor,
natural/urban, human/robot, 2D/3D, etc.). The results indicate great potential
of building stronger general world models with larger-scale training.",2024-06-12 18:55:51+00:00,"['Jiannan Xiang', 'Guangyi Liu', 'Yi Gu', 'Qiyue Gao', 'Yuting Ning', 'Yuheng Zha', 'Zeyu Feng', 'Tianhua Tao', 'Shibo Hao', 'Yemin Shi', 'Zhengzhong Liu', 'Eric P. Xing', 'Zhiting Hu']",http://arxiv.org/abs/2406.09455v1
Unsupervised Video Summarization via Reinforcement Learning and a Trained Evaluator,"This paper presents a novel approach for unsupervised video summarization
using reinforcement learning. It aims to address the existing limitations of
current unsupervised methods, including unstable training of adversarial
generator-discriminator architectures and reliance on hand-crafted reward
functions for quality evaluation. The proposed method is based on the concept
that a concise and informative summary should result in a reconstructed video
that closely resembles the original. The summarizer model assigns an importance
score to each frame and generates a video summary. In the proposed scheme,
reinforcement learning, coupled with a unique reward generation pipeline, is
employed to train the summarizer model. The reward generation pipeline trains
the summarizer to create summaries that lead to improved reconstructions. It
comprises a generator model capable of reconstructing masked frames from a
partially masked video, along with a reward mechanism that compares the
reconstructed video from the summary against the original. The video generator
is trained in a self-supervised manner to reconstruct randomly masked frames,
enhancing its ability to generate accurate summaries. This training pipeline
results in a summarizer model that better mimics human-generated video
summaries compared to methods relying on hand-crafted rewards. The training
process consists of two stable and isolated training steps, unlike adversarial
architectures. Experimental results demonstrate promising performance, with
F-scores of 62.3 and 54.5 on TVSum and SumMe datasets, respectively.
Additionally, the inference stage is 300 times faster than our previously
reported state-of-the-art method.",2024-07-05 05:08:06+00:00,"['Mehryar Abbasi', 'Hadi Hadizadeh', 'Parvaneh Saeedi']",http://arxiv.org/abs/2407.04258v1
Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation,"The current state-of-the-art video generative models can produce
commercial-grade videos with highly realistic details. However, they still
struggle to coherently present multiple sequential events in the stories
specified by the prompts, which is foreseeable an essential capability for
future long video generation scenarios. For example, top T2V generative models
still fail to generate a video of the short simple story 'how to put an
elephant into a refrigerator.' While existing detail-oriented benchmarks
primarily focus on fine-grained metrics like aesthetic quality and
spatial-temporal consistency, they fall short of evaluating models' abilities
to handle event-level story presentation. To address this gap, we introduce
StoryEval, a story-oriented benchmark specifically designed to assess
text-to-video (T2V) models' story-completion capabilities. StoryEval features
423 prompts spanning 7 classes, each representing short stories composed of 2-4
consecutive events. We employ advanced vision-language models, such as GPT-4V
and LLaVA-OV-Chat-72B, to verify the completion of each event in the generated
videos, applying a unanimous voting method to enhance reliability. Our methods
ensure high alignment with human evaluations, and the evaluation of 11 models
reveals its challenge, with none exceeding an average story-completion rate of
50%. StoryEval provides a new benchmark for advancing T2V models and highlights
the challenges and opportunities in developing next-generation solutions for
coherent story-driven video generation.",2024-12-17 23:00:42+00:00,"['Yiping Wang', 'Xuehai He', 'Kuan Wang', 'Luyao Ma', 'Jianwei Yang', 'Shuohang Wang', 'Simon Shaolei Du', 'Yelong Shen']",http://arxiv.org/abs/2412.16211v1
Identity-Preserving Text-to-Video Generation by Frequency Decomposition,"Identity-preserving text-to-video (IPT2V) generation aims to create
high-fidelity videos with consistent human identity. It is an important task in
video generation but remains an open problem for generative models. This paper
pushes the technical frontier of IPT2V in two directions that have not been
resolved in literature: (1) A tuning-free pipeline without tedious case-by-case
finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based
control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V
model to keep human identity consistent in the generated video. Inspired by
prior findings in frequency analysis of diffusion transformers, it employs
identity-control signals in the frequency domain, where facial features can be
decomposed into low-frequency global features and high-frequency intrinsic
features. First, from a low-frequency perspective, we introduce a global facial
extractor, which encodes reference images and facial key points into a latent
space, generating features enriched with low-frequency information. These
features are then integrated into shallow layers of the network to alleviate
training challenges associated with DiT. Second, from a high-frequency
perspective, we design a local facial extractor to capture high-frequency
details and inject them into transformer blocks, enhancing the model's ability
to preserve fine-grained features. We propose a hierarchical training strategy
to leverage frequency information for identity preservation, transforming a
vanilla pre-trained video generation model into an IPT2V model. Extensive
experiments demonstrate that our frequency-aware heuristic scheme provides an
optimal control solution for DiT-based models. Thanks to this scheme, our
ConsisID generates high-quality, identity-preserving videos, making strides
towards more effective IPT2V. Code: https://github.com/PKU-YuanGroup/ConsisID.",2024-11-26 13:58:24+00:00,"['Shenghai Yuan', 'Jinfa Huang', 'Xianyi He', 'Yunyuan Ge', 'Yujun Shi', 'Liuhan Chen', 'Jiebo Luo', 'Li Yuan']",http://arxiv.org/abs/2411.17440v3
Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding,"Inspired by the activity-silent and persistent activity mechanisms in human
visual perception biology, we design a Unified Static and Dynamic Network
(UniSDNet), to learn the semantic association between the video and text/audio
queries in a cross-modal environment for efficient video grounding. For static
modeling, we devise a novel residual structure (ResMLP) to boost the global
comprehensive interaction between the video segments and queries, achieving
more effective semantic enhancement/supplement. For dynamic modeling, we
effectively exploit three characteristics of the persistent activity mechanism
in our network design for a better video context comprehension. Specifically,
we construct a diffusely connected video clip graph on the basis of 2D sparse
temporal masking to reflect the ""short-term effect"" relationship. We
innovatively consider the temporal distance and relevance as the joint
""auxiliary evidence clues"" and design a multi-kernel Temporal Gaussian Filter
to expand the context clue into high-dimensional space, simulating the ""complex
visual perception"", and then conduct element level filtering convolution
operations on neighbour clip nodes in message passing stage for finally
generating and ranking the candidate proposals. Our UniSDNet is applicable to
both Natural Language Video Grounding (NLVG) and Spoken Language Video
Grounding (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widely
used datasets for NLVG, as well as three datasets for SLVG, e.g., reporting new
records at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 on
TACoS. To facilitate this field, we collect two new datasets (Charades-STA
Speech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of our
UniSDNet is 1.56$\times$ faster than the strong multi-query benchmark. Code is
available at: https://github.com/xian-sh/UniSDNet.",2024-03-21 06:53:40+00:00,"['Jingjing Hu', 'Dan Guo', 'Kun Li', 'Zhan Si', 'Xun Yang', 'Xiaojun Chang', 'Meng Wang']",http://arxiv.org/abs/2403.14174v1
Turning Text and Imagery into Captivating Visual Video,"The ability to visualize a structure from multiple perspectives is crucial
for comprehensive planning and presentation. This paper introduces an advanced
application of generative models, akin to Stable Video Diffusion, tailored for
architectural visualization. We explore the potential of these models to create
consistent multi-perspective videos of buildings from single images and to
generate design videos directly from textual descriptions. The proposed method
enhances the design process by offering rapid prototyping, cost and time
efficiency, and an enriched creative space for architects and designers. By
harnessing the power of AI, our approach not only accelerates the visualization
of architectural concepts but also enables a more interactive and immersive
experience for clients and stakeholders. This advancement in architectural
visualization represents a significant leap forward, allowing for a deeper
exploration of design possibilities and a more effective communication of
complex architectural ideas.",2024-06-03 23:53:20+00:00,"['Mingming Wang', 'Elijah Miller']",http://arxiv.org/abs/2406.01851v1
ReferEverything: Towards Segmenting Everything We Can Speak of in Videos,"We present REM, a framework for segmenting a wide range of concepts in video
that can be described through natural language. Our method capitalizes on
visual-language representations learned by video diffusion models on
Internet-scale datasets. A key insight of our approach is preserving as much of
the generative model's original representation as possible, while fine-tuning
it on narrow-domain Referral Object Segmentation datasets. As a result, our
framework can accurately segment and track rare and unseen objects, despite
being trained on object masks from a limited set of categories. Additionally,
it can generalize to non-object dynamic concepts, such as waves crashing in the
ocean, as demonstrated in our newly introduced benchmark for Referral Video
Process Segmentation (Ref-VPS). Our experiments show that REM performs on par
with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while
outperforming them by up to twelve points in terms of region similarity on
out-of-domain data, leveraging the power of Internet-scale pre-training.",2024-10-30 17:59:26+00:00,"['Anurag Bagchi', 'Zhipeng Bao', 'Yu-Xiong Wang', 'Pavel Tokmakov', 'Martial Hebert']",http://arxiv.org/abs/2410.23287v1
Pathways on the Image Manifold: Image Editing via Video Generation,"Recent advances in image editing, driven by image diffusion models, have
shown remarkable progress. However, significant challenges remain, as these
models often struggle to follow complex edit instructions accurately and
frequently compromise fidelity by altering key elements of the original image.
Simultaneously, video generation has made remarkable strides, with models that
effectively function as consistent and continuous world simulators. In this
paper, we propose merging these two fields by utilizing image-to-video models
for image editing. We reformulate image editing as a temporal process, using
pretrained video models to create smooth transitions from the original image to
the desired edit. This approach traverses the image manifold continuously,
ensuring consistent edits while preserving the original image's key aspects.
Our approach achieves state-of-the-art results on text-based image editing,
demonstrating significant improvements in both edit accuracy and image
preservation. Visit our project page at
https://rotsteinnoam.github.io/Frame2Frame.",2024-11-25 16:41:45+00:00,"['Noam Rotstein', 'Gal Yona', 'Daniel Silver', 'Roy Velich', 'David Bensad', 'Ron Kimmel']",http://arxiv.org/abs/2411.16819v4
Anatomical feature-prioritized loss for enhanced MR to CT translation,"In medical image synthesis, the precision of localized structural details is
crucial, particularly when addressing specific clinical requirements such as
the identification and measurement of fine structures. Traditional methods for
image translation and synthesis are generally optimized for global image
reconstruction but often fall short in providing the finesse required for
detailed local analysis. This study represents a step toward addressing this
challenge by introducing a novel anatomical feature-prioritized (AFP) loss
function into the synthesis process. This method enhances reconstruction by
focusing on clinically significant structures, utilizing features from a
pre-trained model designed for a specific downstream task, such as the
segmentation of particular anatomical regions. The AFP loss function can
replace or complement global reconstruction methods, ensuring a balanced
emphasis on both global image fidelity and local structural details. Various
implementations of this loss function are explored, including its integration
into different synthesis networks such as GAN-based and CNN-based models. Our
approach is applied and evaluated in two contexts: lung MR to CT translation,
focusing on high-quality reconstruction of bronchial structures, using a
private dataset; and pelvis MR to CT synthesis, targeting the accurate
representation of organs and muscles, utilizing a public dataset from the
Synthrad2023 challenge. We leverage embeddings from pre-trained segmentation
models specific to these anatomical regions to demonstrate the capability of
the AFP loss to prioritize and accurately reconstruct essential features. This
tailored approach shows promising potential for enhancing the specificity and
practicality of medical image synthesis in clinical applications.",2024-10-14 09:40:52+00:00,"['Arthur Longuefosse', 'Baudouin Denis de Senneville', 'Gael Dournes', 'Ilyes Benlala', 'Pascal Desbarats', 'Fabien Baldacci']",http://arxiv.org/abs/2410.10328v2
Leveraging User-Generated Metadata of Online Videos for Cover Song Identification,"YouTube is a rich source of cover songs. Since the platform itself is
organized in terms of videos rather than songs, the retrieval of covers is not
trivial. The field of cover song identification addresses this problem and
provides approaches that usually rely on audio content. However, including the
user-generated video metadata available on YouTube promises improved
identification results. In this paper, we propose a multi-modal approach for
cover song identification on online video platforms. We combine the entity
resolution models with audio-based approaches using a ranking model. Our
findings implicate that leveraging user-generated metadata can stabilize cover
song identification performance on YouTube.",2024-12-16 14:35:32+00:00,"['Simon Hachmeier', 'Robert Jschke']",http://arxiv.org/abs/2412.11818v1
Spatial Decomposition and Temporal Fusion based Inter Prediction for Learned Video Compression,"Video compression performance is closely related to the accuracy of inter
prediction. It tends to be difficult to obtain accurate inter prediction for
the local video regions with inconsistent motion and occlusion. Traditional
video coding standards propose various technologies to handle motion
inconsistency and occlusion, such as recursive partitions, geometric
partitions, and long-term references. However, existing learned video
compression schemes focus on obtaining an overall minimized prediction error
averaged over all regions while ignoring the motion inconsistency and occlusion
in local regions. In this paper, we propose a spatial decomposition and
temporal fusion based inter prediction for learned video compression. To handle
motion inconsistency, we propose to decompose the video into structure and
detail (SDD) components first. Then we perform SDD-based motion estimation and
SDD-based temporal context mining for the structure and detail components to
generate short-term temporal contexts. To handle occlusion, we propose to
propagate long-term temporal contexts by recurrently accumulating the temporal
information of each historical reference feature and fuse them with short-term
temporal contexts. With the SDD-based motion model and long short-term temporal
contexts fusion, our proposed learned video codec can obtain more accurate
inter prediction. Comprehensive experimental results demonstrate that our codec
outperforms the reference software of H.266/VVC on all common test datasets for
both PSNR and MS-SSIM.",2024-01-29 03:30:21+00:00,"['Xihua Sheng', 'Li Li', 'Dong Liu', 'Houqiang Li']",http://arxiv.org/abs/2401.15864v1
Analysis of Neural Video Compression Networks for 360-Degree Video Coding,"With the increasing efforts of bringing high-quality virtual reality
technologies into the market, efficient 360-degree video compression gains in
importance. As such, the state-of-the-art H.266/VVC video coding standard
integrates dedicated tools for 360-degree video, and considerable efforts have
been put into designing 360-degree projection formats with improved compression
efficiency. For the fast-evolving field of neural video compression networks
(NVCs), the effects of different 360-degree projection formats on the overall
compression performance have not yet been investigated. It is thus unclear,
whether a resampling from the conventional equirectangular projection (ERP) to
other projection formats yields similar gains for NVCs as for hybrid video
codecs, and which formats perform best. In this paper, we analyze several
generations of NVCs and an extensive set of 360-degree projection formats with
respect to their compression performance for 360-degree video. Based on our
analysis, we find that projection format resampling yields significant
improvements in compression performance also for NVCs. The adjusted cubemap
projection (ACP) and equatorial cylindrical projection (ECP) show to perform
best and achieve rate savings of more than 55% compared to ERP based on WS-PSNR
for the most recent NVC. Remarkably, the observed rate savings are higher than
for H.266/VVC, emphasizing the importance of projection format resampling for
NVCs.",2024-02-15 17:15:54+00:00,"['Andy Regensky', 'Fabian Brand', 'Andr Kaup']",http://arxiv.org/abs/2402.10257v1
Edit3K: Universal Representation Learning for Video Editing Components,"This paper focuses on understanding the predominant video creation pipeline,
i.e., compositional video editing with six main types of editing components,
including video effects, animation, transition, filter, sticker, and text. In
contrast to existing visual representation learning of visual materials (i.e.,
images/videos), we aim to learn visual representations of editing
actions/components that are generally applied on raw materials. We start by
proposing the first large-scale dataset for editing components of video
creation, which covers about $3,094$ editing components with $618,800$ videos.
Each video in our dataset is rendered by various image/video materials with a
single editing component, which supports atomic visual understanding of
different editing components. It can also benefit several downstream tasks,
e.g., editing component recommendation, editing component
recognition/retrieval, etc. Existing visual representation methods perform
poorly because it is difficult to disentangle the visual appearance of editing
components from raw materials. To that end, we benchmark popular alternative
solutions and propose a novel method that learns to attend to the appearance of
editing components regardless of raw materials. Our method achieves favorable
results on editing component retrieval/recognition compared to the alternative
solutions. A user study is also conducted to show that our representations
cluster visually similar editing components better than other alternatives.
Furthermore, our learned representations used to transition recommendation
tasks achieve state-of-the-art results on the AutoTransition dataset. The code
and dataset are available at https://github.com/GX77/Edit3K .",2024-03-24 07:29:04+00:00,"['Xin Gu', 'Libo Zhang', 'Fan Chen', 'Longyin Wen', 'Yufei Wang', 'Tiejian Luo', 'Sijie Zhu']",http://arxiv.org/abs/2403.16048v2
Elysium: Exploring Object-level Perception in Videos via MLLM,"Multi-modal Large Language Models (MLLMs) have demonstrated their ability to
perceive objects in still images, but their application in video-related tasks,
such as object tracking, remains understudied. This lack of exploration is
primarily due to two key challenges. Firstly, extensive pretraining on
large-scale video datasets is required to equip MLLMs with the capability to
perceive objects across multiple frames and understand inter-frame
relationships. Secondly, processing a large number of frames within the context
window of Large Language Models (LLMs) can impose a significant computational
burden. To address the first challenge, we introduce ElysiumTrack-1M, a
large-scale video dataset supported for three tasks: Single Object Tracking
(SOT), Referring Single Object Tracking (RSOT), and Video Referring Expression
Generation (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video
frames with corresponding object boxes and descriptions. Leveraging this
dataset, we conduct training of MLLMs and propose a token-compression model
T-Selector to tackle the second challenge. Our proposed approach, Elysium:
Exploring Object-level Perception in Videos via MLLM, is an end-to-end
trainable MLLM that attempts to conduct object-level tasks in videos without
requiring any additional plug-in or expert models. All codes and datasets are
available at https://github.com/Hon-Wong/Elysium.",2024-03-25 09:17:15+00:00,"['Han Wang', 'Yanjie Wang', 'Yongjie Ye', 'Yuxiang Nie', 'Can Huang']",http://arxiv.org/abs/2403.16558v2
Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval,"The increasing prevalence of video clips has sparked growing interest in
text-video retrieval. Recent advances focus on establishing a joint embedding
space for text and video, relying on consistent embedding representations to
compute similarity. However, the text content in existing datasets is generally
short and concise, making it hard to fully describe the redundant semantics of
a video. Correspondingly, a single text embedding may be less expressive to
capture the video embedding and empower the retrieval. In this study, we
propose a new stochastic text modeling method T-MASS, i.e., text is modeled as
a stochastic embedding, to enrich text embedding with a flexible and resilient
semantic range, yielding a text mass. To be specific, we introduce a
similarity-aware radius module to adapt the scale of the text mass upon the
given text-video pairs. Plus, we design and develop a support text
regularization to further control the text mass during the training. The
inference pipeline is also tailored to fully exploit the text mass for accurate
retrieval. Empirical evidence suggests that T-MASS not only effectively
attracts relevant text-video pairs while distancing irrelevant ones, but also
enables the determination of precise text embeddings for relevant pairs. Our
experimental results show a substantial improvement of T-MASS over baseline (3%
to 6.3% by R@1). Also, T-MASS achieves state-of-the-art performance on five
benchmark datasets, including MSRVTT, LSMDC, DiDeMo, VATEX, and Charades.",2024-03-26 17:59:52+00:00,"['Jiamian Wang', 'Guohao Sun', 'Pichao Wang', 'Dongfang Liu', 'Sohail Dianat', 'Majid Rabbani', 'Raghuveer Rao', 'Zhiqiang Tao']",http://arxiv.org/abs/2403.17998v1
ST-LLM: Large Language Models Are Effective Temporal Learners,"Large Language Models (LLMs) have showcased impressive capabilities in text
comprehension and generation, prompting research efforts towards video LLMs to
facilitate human-AI interaction at the video level. However, how to effectively
encode and understand videos in video-based dialogue systems remains to be
solved. In this paper, we investigate a straightforward yet unexplored
question: Can we feed all spatial-temporal tokens into the LLM, thus delegating
the task of video sequence modeling to the LLMs? Surprisingly, this simple
approach yields significant improvements in video understanding. Based upon
this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal
sequence modeling inside LLM. Furthermore, to address the overhead and
stability issues introduced by uncompressed video tokens within LLMs, we
develop a dynamic masking strategy with tailor-made training objectives. For
particularly long videos, we have also designed a global-local input module to
balance efficiency and effectiveness. Consequently, we harness LLM for
proficient spatial-temporal modeling, while upholding efficiency and stability.
Extensive experimental results attest to the effectiveness of our method.
Through a more concise model and training pipeline, ST-LLM establishes a new
state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been
available at https://github.com/TencentARC/ST-LLM.",2024-03-30 10:11:26+00:00,"['Ruyang Liu', 'Chen Li', 'Haoran Tang', 'Yixiao Ge', 'Ying Shan', 'Ge Li']",http://arxiv.org/abs/2404.00308v1
Parameter-Efficient Instance-Adaptive Neural Video Compression,"Learning-based Neural Video Codecs (NVCs) have emerged as a compelling
alternative to standard video codecs, demonstrating promising performance, and
simple and easily maintainable pipelines. However, NVCs often fall short of
compression performance and occasionally exhibit poor generalization capability
due to inference-only compression scheme and their dependence on training data.
The instance-adaptive video compression techniques have recently been suggested
as a viable solution, fine-tuning the encoder or decoder networks for a
particular test instance video. However, fine-tuning all the model parameters
incurs high computational costs, increases the bitrates, and often leads to
unstable training. In this work, we propose a parameter-efficient
instance-adaptive video compression framework. Inspired by the remarkable
success of parameter-efficient fine-tuning on large-scale neural network
models, we propose to use a lightweight adapter module that can be easily
attached to the pretrained NVCs and fine-tuned for test video sequences. The
resulting algorithm significantly improves compression performance and reduces
the encoding time compared to the existing instant-adaptive video compression
algorithms. Furthermore, the suggested fine-tuning method enhances the
robustness of the training process, allowing for the proposed method to be
widely used in many practical settings. We conducted extensive experiments on
various standard benchmark datasets, including UVG, MCL-JVC, and HEVC
sequences, and the experimental results have shown a significant improvement in
rate-distortion (RD) curves (up to 5 dB PSNR) and BD rates compared to the
baselines NVC. Our code is available on https://github.com/ohsngjun/PEVC.",2024-05-14 12:14:58+00:00,"['Hyunmo Yang', 'Seungjun Oh', 'Eunbyung Park']",http://arxiv.org/abs/2405.08530v3
RMT-BVQA: Recurrent Memory Transformer-based Blind Video Quality Assessment for Enhanced Video Content,"With recent advances in deep learning, numerous algorithms have been
developed to enhance video quality, reduce visual artifacts, and improve
perceptual quality. However, little research has been reported on the quality
assessment of enhanced content - the evaluation of enhancement methods is often
based on quality metrics that were designed for compression applications. In
this paper, we propose a novel blind deep video quality assessment (VQA) method
specifically for enhanced video content. It employs a new Recurrent Memory
Transformer (RMT) based network architecture to obtain video quality
representations, which is optimized through a novel content-quality-aware
contrastive learning strategy based on a new database containing 13K training
patches with enhanced content. The extracted quality representations are then
combined through linear regression to generate video-level quality indices. The
proposed method, RMT-BVQA, has been evaluated on the VDPVE (VQA Dataset for
Perceptual Video Enhancement) database through a five-fold cross validation.
The results show its superior correlation performance when compared to ten
existing no-reference quality metrics.",2024-05-14 14:01:15+00:00,"['Tianhao Peng', 'Chen Feng', 'Duolikun Danier', 'Fan Zhang', 'Benoit Vallade', 'Alex Mackin', 'David Bull']",http://arxiv.org/abs/2405.08621v5
UBiSS: A Unified Framework for Bimodal Semantic Summarization of Videos,"With the surge in the amount of video data, video summarization techniques,
including visual-modal(VM) and textual-modal(TM) summarization, are attracting
more and more attention. However, unimodal summarization inevitably loses the
rich semantics of the video. In this paper, we focus on a more comprehensive
video summarization task named Bimodal Semantic Summarization of Videos
(BiSSV). Specifically, we first construct a large-scale dataset, BIDS, in
(video, VM-Summary, TM-Summary) triplet format. Unlike traditional processing
methods, our construction procedure contains a VM-Summary extraction algorithm
aiming to preserve the most salient content within long videos. Based on BIDS,
we propose a Unified framework UBiSS for the BiSSV task, which models the
saliency information in the video and generates a TM-summary and VM-summary
simultaneously. We further optimize our model with a list-wise ranking-based
objective to improve its capacity to capture highlights. Lastly, we propose a
metric, $NDCG_{MS}$, to provide a joint evaluation of the bimodal summary.
Experiments show that our unified framework achieves better performance than
multi-stage summarization pipelines. Code and data are available at
https://github.com/MeiYutingg/UBiSS.",2024-06-24 03:55:25+00:00,"['Yuting Mei', 'Linli Yao', 'Qin Jin']",http://arxiv.org/abs/2406.16301v1
MAMA: Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning,"Data quality stands at the forefront of deciding the effectiveness of
video-language representation learning. However, video-text pairs in previous
data typically do not align perfectly with each other, which might lead to
video-language representations that do not accurately reflect cross-modal
semantics. Moreover, previous data also possess an uneven distribution of
concepts, thereby hampering the downstream performance across unpopular
subjects. To address these problems, we propose MAMA, a new approach to
learning video-language representations by utilizing a contrastive objective
with a subtractive angular margin to regularize cross-modal representations in
their effort to reach perfect similarity. Furthermore, to adapt to the
non-uniform concept distribution, MAMA utilizes a multi-layer perceptron
(MLP)-parameterized weighting function that maps loss values to sample weights
which enable dynamic adjustment of the model's focus throughout the training.
With the training guided by a small amount of unbiased meta-data and augmented
by video-text data generated by large vision-language model, MAMA improves
video-language representations and achieve superior performances on commonly
used video question answering and text-video retrieval datasets. The code,
model, and data have been made available at
https://nguyentthong.github.io/MAMA.",2024-07-04 09:52:17+00:00,"['Thong Nguyen', 'Yi Bin', 'Xiaobao Wu', 'Xinshuai Dong', 'Zhiyuan Hu', 'Khoi Le', 'Cong-Duy Nguyen', 'See-Kiong Ng', 'Luu Anh Tuan']",http://arxiv.org/abs/2407.03788v4
VISA: Reasoning Video Object Segmentation via Large Language Models,"Existing Video Object Segmentation (VOS) relies on explicit user
instructions, such as categories, masks, or short phrases, restricting their
ability to perform complex video segmentation requiring reasoning with world
knowledge. In this paper, we introduce a new task, Reasoning Video Object
Segmentation (ReasonVOS). This task aims to generate a sequence of segmentation
masks in response to implicit text queries that require complex reasoning
abilities based on world knowledge and video contexts, which is crucial for
structured environment understanding and object-centric interactions, pivotal
in the development of embodied AI. To tackle ReasonVOS, we introduce VISA
(Video-based large language Instructed Segmentation Assistant), to leverage the
world knowledge reasoning capabilities of multi-modal LLMs while possessing the
ability to segment and track objects in videos with a mask decoder. Moreover,
we establish a comprehensive benchmark consisting of 35,074 instruction-mask
sequence pairs from 1,042 diverse videos, which incorporates complex world
knowledge reasoning into segmentation tasks for instruction-tuning and
evaluation purposes of ReasonVOS models. Experiments conducted on 8 datasets
demonstrate the effectiveness of VISA in tackling complex reasoning
segmentation and vanilla referring segmentation in both video and image
domains. The code and dataset are available at
https://github.com/cilinyan/VISA.",2024-07-16 02:29:29+00:00,"['Cilin Yan', 'Haochen Wang', 'Shilin Yan', 'Xiaolong Jiang', 'Yao Hu', 'Guoliang Kang', 'Weidi Xie', 'Efstratios Gavves']",http://arxiv.org/abs/2407.11325v1
Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video,"Weakly supervised video object segmentation (WSVOS) enables the
identification of segmentation maps without requiring an extensive training
dataset of object masks, relying instead on coarse video labels indicating
object presence. Current state-of-the-art methods either require multiple
independent stages of processing that employ motion cues or, in the case of
end-to-end trainable networks, lack in segmentation accuracy, in part due to
the difficulty of learning segmentation maps from videos with transient object
presence. This limits the application of WSVOS for semantic annotation of
surgical videos where multiple surgical tools frequently move in and out of the
field of view, a problem that is more difficult than typically encountered in
WSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks
(VDST-Net), a framework to disentangle spatiotemporal information using
semi-decoupled knowledge distillation to predict high-quality class activation
maps (CAMs). A teacher network designed to resolve temporal conflicts when
specifics about object location and timing in the video are not provided works
with a student network that integrates information over time by leveraging
temporal dependencies. We demonstrate the efficacy of our framework on a public
reference dataset and on a more challenging surgical video dataset where
objects are, on average, present in less than 60\% of annotated frames. Our
method outperforms state-of-the-art techniques and generates superior
segmentation masks under video-level weak supervision.",2024-07-22 16:52:32+00:00,"['Guiqiu Liao', 'Matjaz Jogan', 'Sai Koushik', 'Eric Eaton', 'Daniel A. Hashimoto']",http://arxiv.org/abs/2407.15794v4
Unlocking Exocentric Video-Language Data for Egocentric Video Representation Learning,"We present EMBED (Egocentric Models Built with Exocentric Data), a method
designed to transform exocentric video-language data for egocentric video
representation learning. Large-scale exocentric data covers diverse activities
with significant potential for egocentric learning, but inherent disparities
between egocentric and exocentric data pose challenges in utilizing one view
for the other seamlessly. Egocentric videos predominantly feature close-up
hand-object interactions, whereas exocentric videos offer a broader perspective
on human activities. Additionally, narratives in egocentric datasets are
typically more action-centric and closely linked with the visual content, in
contrast to the narrative styles found in exocentric datasets. To address these
challenges, we employ a data transformation framework to adapt exocentric data
for egocentric training, focusing on identifying specific video clips that
emphasize hand-object interactions and transforming narration styles to align
with egocentric perspectives. By applying both vision and language style
transfer, our framework creates a new egocentric dataset derived from
exocentric video-language data. Through extensive evaluations, we demonstrate
the effectiveness of EMBED, achieving state-of-the-art results across various
egocentric downstream tasks, including an absolute improvement of 4.7% on the
Epic-Kitchens-100 multi-instance retrieval and 6.2% on the EGTEA classification
benchmarks in zero-shot settings. Furthermore, EMBED enables egocentric
video-language models to perform competitively in exocentric tasks. Finally, we
showcase EMBED's application across various exocentric datasets, exhibiting
strong generalization capabilities when applied to different exocentric
datasets.",2024-08-07 06:10:45+00:00,"['Zi-Yi Dou', 'Xitong Yang', 'Tushar Nagarajan', 'Huiyu Wang', 'Jing Huang', 'Nanyun Peng', 'Kris Kitani', 'Fu-Jen Chu']",http://arxiv.org/abs/2408.03567v1
Temporal Divide-and-Conquer Anomaly Actions Localization in Semi-Supervised Videos with Hierarchical Transformer,"Anomaly action detection and localization play an essential role in security
and advanced surveillance systems. However, due to the tremendous amount of
surveillance videos, most of the available data for the task is unlabeled or
semi-labeled with the video class known, but the location of the anomaly event
is unknown. In this work, we target anomaly localization in semi-supervised
videos. While the mainstream direction in addressing this task is focused on
segment-level multi-instance learning and the generation of pseudo labels, we
aim to explore a promising yet unfulfilled direction to solve the problem by
learning the temporal relations within videos in order to locate anomaly
events. To this end, we propose a hierarchical transformer model designed to
evaluate the significance of observed actions in anomalous videos with a
divide-and-conquer strategy along the temporal axis. Our approach segments a
parent video hierarchically into multiple temporal children instances and
measures the influence of the children nodes in classifying the abnormality of
the parent video. Evaluating our model on two well-known anomaly detection
datasets, UCF-crime and ShanghaiTech, proves its ability to interpret the
observed actions within videos and localize the anomalous ones. Our proposed
approach outperforms previous works relying on segment-level multiple-instance
learning approaches while reaching a promising performance compared to the more
recent pseudo-labeling-based approaches.",2024-08-24 18:12:58+00:00,"['Nada Osman', 'Marwan Torki']",http://arxiv.org/abs/2408.13643v1
Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering,"Video Question Answering (VideoQA) represents a crucial intersection between
video understanding and language processing, requiring both discriminative
unimodal comprehension and sophisticated cross-modal interaction for accurate
inference. Despite advancements in multi-modal pre-trained models and
video-language foundation models, these systems often struggle with
domain-specific VideoQA due to their generalized pre-training objectives.
Addressing this gap necessitates bridging the divide between broad cross-modal
knowledge and the specific inference demands of VideoQA tasks. To this end, we
introduce HeurVidQA, a framework that leverages domain-specific entity-action
heuristics to refine pre-trained video-language foundation models. Our approach
treats these models as implicit knowledge engines, employing domain-specific
entity-action prompters to direct the model's focus toward precise cues that
enhance reasoning. By delivering fine-grained heuristics, we improve the
model's ability to identify and interpret key entities and actions, thereby
enhancing its reasoning capabilities. Extensive evaluations across multiple
VideoQA datasets demonstrate that our method significantly outperforms existing
models, underscoring the importance of integrating domain-specific knowledge
into video-language models for more accurate and context-aware VideoQA.",2024-10-12 06:22:23+00:00,"['Ting Yu', 'Kunhao Fu', 'Shuhui Wang', 'Qingming Huang', 'Jun Yu']",http://arxiv.org/abs/2410.09380v1
Pseudo-labeling with Keyword Refining for Few-Supervised Video Captioning,"Video captioning generate a sentence that describes the video content.
Existing methods always require a number of captions (\eg, 10 or 20) per video
to train the model, which is quite costly. In this work, we explore the
possibility of using only one or very few ground-truth sentences, and introduce
a new task named few-supervised video captioning. Specifically, we propose a
few-supervised video captioning framework that consists of lexically
constrained pseudo-labeling module and keyword-refined captioning module.
Unlike the random sampling in natural language processing that may cause
invalid modifications (\ie, edit words), the former module guides the model to
edit words using some actions (\eg, copy, replace, insert, and delete) by a
pretrained token-level classifier, and then fine-tunes candidate sentences by a
pretrained language model. Meanwhile, the former employs the repetition
penalized sampling to encourage the model to yield concise pseudo-labeled
sentences with less repetition, and selects the most relevant sentences upon a
pretrained video-text model. Moreover, to keep semantic consistency between
pseudo-labeled sentences and video content, we develop the transformer-based
keyword refiner with the video-keyword gated fusion strategy to emphasize more
on relevant words. Extensive experiments on several benchmarks demonstrate the
advantages of the proposed approach in both few-supervised and fully-supervised
scenarios. The code implementation is available at
https://github.com/mlvccn/PKG_VidCap",2024-11-06 17:11:44+00:00,"['Ping Li', 'Tao Wang', 'Xinkui Zhao', 'Xianghua Xu', 'Mingli Song']",http://arxiv.org/abs/2411.04059v1
Vid-Morp: Video Moment Retrieval Pretraining from Unlabeled Videos in the Wild,"Given a natural language query, video moment retrieval aims to localize the
described temporal moment in an untrimmed video. A major challenge of this task
is its heavy dependence on labor-intensive annotations for training. Unlike
existing works that directly train models on manually curated data, we propose
a novel paradigm to reduce annotation costs: pretraining the model on
unlabeled, real-world videos. To support this, we introduce Video Moment
Retrieval Pretraining (Vid-Morp), a large-scale dataset collected with minimal
human intervention, consisting of over 50K videos captured in the wild and 200K
pseudo annotations. Direct pretraining on these imperfect pseudo annotations,
however, presents significant challenges, including mismatched sentence-video
pairs and imprecise temporal boundaries. To address these issues, we propose
the ReCorrect algorithm, which comprises two main phases: semantics-guided
refinement and memory-consensus correction. The semantics-guided refinement
enhances the pseudo labels by leveraging semantic similarity with video frames
to clean out unpaired data and make initial adjustments to temporal boundaries.
In the following memory-consensus correction phase, a memory bank tracks the
model predictions, progressively correcting the temporal boundaries based on
consensus within the memory. Comprehensive experiments demonstrate ReCorrect's
strong generalization abilities across multiple downstream settings. Zero-shot
ReCorrect achieves over 75% and 80% of the best fully-supervised performance on
two benchmarks, while unsupervised ReCorrect reaches about 85% on both. The
code, dataset, and pretrained models are available at
https://github.com/baopj/Vid-Morp.",2024-12-01 13:49:21+00:00,"['Peijun Bao', 'Chenqi Kong', 'Zihao Shao', 'Boon Poh Ng', 'Meng Hwa Er', 'Alex C. Kot']",http://arxiv.org/abs/2412.00811v1
Semi-Supervised Contrastive Learning for Controllable Video-to-Music Retrieval,"Content creators often use music to enhance their videos, from soundtracks in
movies to background music in video blogs and social media content. However,
identifying the best music for a video can be a difficult and time-consuming
task. To address this challenge, we propose a novel framework for automatically
retrieving a matching music clip for a given video, and vice versa. Our
approach leverages annotated music labels, as well as the inherent artistic
correspondence between visual and music elements. Distinct from previous
cross-modal music retrieval works, our method combines both self-supervised and
supervised training objectives. We use self-supervised and label-supervised
contrastive learning to train a joint embedding space between music and video.
We show the effectiveness of our approach by using music genre labels for the
supervised training component, and our framework can be generalized to other
music annotations (e.g., emotion, instrument, etc.). Furthermore, our method
enables fine-grained control over how much the retrieval process focuses on
self-supervised vs. label information at inference time. We evaluate the
learned embeddings through a variety of video-to-music and music-to-video
retrieval tasks. Our experiments show that the proposed approach successfully
combines self-supervised and supervised objectives and is effective for
controllable music-video retrieval.",2024-12-08 06:37:27+00:00,"['Shanti Stewart', 'Gouthaman KV', 'Lie Lu', 'Andrea Fanelli']",http://arxiv.org/abs/2412.05831v2
Stitch Contrast and Segment_Learning a Human Action Segmentation Model Using Trimmed Skeleton Videos,"Existing skeleton-based human action classification models rely on
well-trimmed action-specific skeleton videos for both training and testing,
precluding their scalability to real-world applications where untrimmed videos
exhibiting concatenated actions are predominant. To overcome this limitation,
recently introduced skeleton action segmentation models involve un-trimmed
skeleton videos into end-to-end training. The model is optimized to provide
frame-wise predictions for any length of testing videos, simultaneously
realizing action localization and classification. Yet, achieving such an
improvement im-poses frame-wise annotated skeleton videos, which remains
time-consuming in practice. This paper features a novel framework for
skeleton-based action segmentation trained on short trimmed skeleton videos,
but that can run on longer un-trimmed videos. The approach is implemented in
three steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral
skeleton stitching scheme that treats trimmed skeleton videos as elementary
human motions that compose a semantic space and can be sampled to generate
multi-action stitched se-quences. Contrast learns contrastive representations
from stitched sequences with a novel discrimination pretext task that enables a
skeleton encoder to learn meaningful action-temporal contexts to improve action
segmentation. Finally, Segment relates the proposed method to action
segmentation by learning a segmentation layer while handling particular da-ta
availability. Experiments involve a trimmed source dataset and an untrimmed
target dataset in an adaptation formulation for real-world skeleton-based human
action segmentation to evaluate the effectiveness of the proposed method.",2024-12-19 16:00:10+00:00,"['Haitao Tian', 'Pierre Payeur']",http://arxiv.org/abs/2412.14988v2
FriendsQA: A New Large-Scale Deep Video Understanding Dataset with Fine-grained Topic Categorization for Story Videos,"Video question answering (VideoQA) aims to answer natural language questions
according to the given videos. Although existing models perform well in the
factoid VideoQA task, they still face challenges in deep video understanding
(DVU) task, which focuses on story videos. Compared to factoid videos, the most
significant feature of story videos is storylines, which are composed of
complex interactions and long-range evolvement of core story topics including
characters, actions and locations. Understanding these topics requires models
to possess DVU capability. However, existing DVU datasets rarely organize
questions according to these story topics, making them difficult to
comprehensively assess VideoQA models' DVU capability of complex storylines.
Additionally, the question quantity and video length of these dataset are
limited by high labor costs of handcrafted dataset building method. In this
paper, we devise a large language model based multi-agent collaboration
framework, StoryMind, to automatically generate a new large-scale DVU dataset.
The dataset, FriendsQA, derived from the renowned sitcom Friends with an
average episode length of 1,358 seconds, contains 44.6K questions evenly
distributed across 14 fine-grained topics. Finally, We conduct comprehensive
experiments on 10 state-of-the-art VideoQA models using the FriendsQA dataset.",2024-12-22 13:55:44+00:00,"['Zhengqian Wu', 'Ruizhe Li', 'Zijun Xu', 'Zhongyuan Wang', 'Chunxia Xiao', 'Chao Liang']",http://arxiv.org/abs/2412.17022v1
DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment,"Video-driven neural face reenactment aims to synthesize realistic facial
images that successfully preserve the identity and appearance of a source face,
while transferring the target head pose and facial expressions. Existing
GAN-based methods suffer from either distortions and visual artifacts or poor
reconstruction quality, i.e., the background and several important appearance
details, such as hair style/color, glasses and accessories, are not faithfully
reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable
the generation of high-quality realistic images. To this end, in this paper we
present DiffusionAct, a novel method that leverages the photo-realistic image
generation of diffusion models to perform neural face reenactment.
Specifically, we propose to control the semantic space of a Diffusion
Autoencoder (DiffAE), in order to edit the facial pose of the input images,
defined as the head pose orientation and the facial expressions. Our method
allows one-shot, self, and cross-subject reenactment, without requiring
subject-specific fine-tuning. We compare against state-of-the-art GAN-,
StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment
performance.",2024-03-25 21:46:53+00:00,"['Stella Bounareli', 'Christos Tzelepis', 'Vasileios Argyriou', 'Ioannis Patras', 'Georgios Tzimiropoulos']",http://arxiv.org/abs/2403.17217v2
Towards Generalizable Tumor Synthesis,"Tumor synthesis enables the creation of artificial tumors in medical images,
facilitating the training of AI models for tumor detection and segmentation.
However, success in tumor synthesis hinges on creating visually realistic
tumors that are generalizable across multiple organs and, furthermore, the
resulting AI models being capable of detecting real tumors in images sourced
from different domains (e.g., hospitals). This paper made a progressive stride
toward generalizable tumor synthesis by leveraging a critical observation:
early-stage tumors (< 2cm) tend to have similar imaging characteristics in
computed tomography (CT), whether they originate in the liver, pancreas, or
kidneys. We have ascertained that generative AI models, e.g., Diffusion Models,
can create realistic tumors generalized to a range of organs even when trained
on a limited number of tumor examples from only one organ. Moreover, we have
shown that AI models trained on these synthetic tumors can be generalized to
detect and segment real tumors from CT volumes, encompassing a broad spectrum
of patient demographics, imaging protocols, and healthcare facilities.",2024-02-29 18:57:39+00:00,"['Qi Chen', 'Xiaoxi Chen', 'Haorui Song', 'Zhiwei Xiong', 'Alan Yuille', 'Chen Wei', 'Zongwei Zhou']",http://arxiv.org/abs/2402.19470v2
Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level Blending and Spatiotemporal Adapter Tuning,"Three key challenges hinder the development of current deepfake video
detection: (1) Temporal features can be complex and diverse: how can we
identify general temporal artifacts to enhance model generalization? (2)
Spatiotemporal models often lean heavily on one type of artifact and ignore the
other: how can we ensure balanced learning from both? (3) Videos are naturally
resource-intensive: how can we tackle efficiency without compromising accuracy?
This paper attempts to tackle the three challenges jointly. First, inspired by
the notable generality of using image-level blending data for image forgery
detection, we investigate whether and how video-level blending can be effective
in video. We then perform a thorough analysis and identify a previously
underexplored temporal forgery artifact: Facial Feature Drift (FFD), which
commonly exists across different forgeries. To reproduce FFD, we then propose a
novel Video-level Blending data (VB), where VB is implemented by blending the
original image and its warped version frame-by-frame, serving as a hard
negative sample to mine more general artifacts. Second, we carefully design a
lightweight Spatiotemporal Adapter (StA) to equip a pretrained image model
(both ViTs and CNNs) with the ability to capture both spatial and temporal
features jointly and efficiently. StA is designed with two-stream 3D-Conv with
varying kernel sizes, allowing it to process spatial and temporal features
separately. Extensive experiments validate the effectiveness of the proposed
methods; and show our approach can generalize well to previously unseen forgery
videos, even the latest generation methods.",2024-08-30 07:49:57+00:00,"['Zhiyuan Yan', 'Yandan Zhao', 'Shen Chen', 'Mingyi Guo', 'Xinghe Fu', 'Taiping Yao', 'Shouhong Ding', 'Li Yuan']",http://arxiv.org/abs/2408.17065v2
ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way,"The text-to-video (T2V) generation models, offering convenient visual
creation, have recently garnered increasing attention. Despite their
substantial potential, the generated videos may present artifacts, including
structural implausibility, temporal inconsistency, and a lack of motion, often
resulting in near-static video. In this work, we have identified a correlation
between the disparity of temporal attention maps across different blocks and
the occurrence of temporal inconsistencies. Additionally, we have observed that
the energy contained within the temporal attention maps is directly related to
the magnitude of motion amplitude in the generated videos. Based on these
observations, we present ByTheWay, a training-free method to improve the
quality of text-to-video generation without introducing additional parameters,
augmenting memory or sampling time. Specifically, ByTheWay is composed of two
principal components: 1) Temporal Self-Guidance improves the structural
plausibility and temporal consistency of generated videos by reducing the
disparity between the temporal attention maps across various decoder blocks. 2)
Fourier-based Motion Enhancement enhances the magnitude and richness of motion
by amplifying the energy of the map. Extensive experiments demonstrate that
ByTheWay significantly improves the quality of text-to-video generation with
negligible additional cost.",2024-10-08 17:56:33+00:00,"['Jiazi Bu', 'Pengyang Ling', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang']",http://arxiv.org/abs/2410.06241v3
Pyramidal Flow Matching for Efficient Video Generative Modeling,"Video generation requires modeling a vast spatiotemporal space, which demands
significant computational resources and data usage. To reduce the complexity,
the prevailing approaches employ a cascaded architecture to avoid direct
training with full resolution latent. Despite reducing computational demands,
the separate optimization of each sub-stage hinders knowledge sharing and
sacrifices flexibility. This work introduces a unified pyramidal flow matching
algorithm. It reinterprets the original denoising trajectory as a series of
pyramid stages, where only the final stage operates at the full resolution,
thereby enabling more efficient video generative modeling. Through our
sophisticated design, the flows of different pyramid stages can be interlinked
to maintain continuity. Moreover, we craft autoregressive video generation with
a temporal pyramid to compress the full-resolution history. The entire
framework can be optimized in an end-to-end manner and with a single unified
Diffusion Transformer (DiT). Extensive experiments demonstrate that our method
supports generating high-quality 5-second (up to 10-second) videos at 768p
resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models
are open-sourced at https://pyramid-flow.github.io.",2024-10-08 12:10:37+00:00,"['Yang Jin', 'Zhicheng Sun', 'Ningyuan Li', 'Kun Xu', 'Kun Xu', 'Hao Jiang', 'Nan Zhuang', 'Quzhe Huang', 'Yang Song', 'Yadong Mu', 'Zhouchen Lin']",http://arxiv.org/abs/2410.05954v2
AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation,"The automatic generation of anchor-style product promotion videos presents
promising opportunities in online commerce, advertising, and consumer
engagement. However, this remains a challenging task despite significant
advancements in pose-guided human video generation. In addressing this
challenge, we identify the integration of human-object interactions (HOI) into
pose-guided human video generation as a core issue. To this end, we introduce
AnchorCrafter, a novel diffusion-based system designed to generate 2D videos
featuring a target human and a customized object, achieving high visual
fidelity and controllable interactions. Specifically, we propose two key
innovations: the HOI-appearance perception, which enhances object appearance
recognition from arbitrary multi-view perspectives and disentangles object and
human appearance, and the HOI-motion injection, which enables complex
human-object interactions by overcoming challenges in object trajectory
conditioning and inter-occlusion management. Additionally, we introduce the
HOI-region reweighting loss, a training objective that enhances the learning of
object details. Extensive experiments demonstrate that our proposed system
outperforms existing methods in preserving object appearance and shape
awareness, while simultaneously maintaining consistency in human appearance and
motion. Project page: https://cangcz.github.io/Anchor-Crafter/",2024-11-26 12:42:13+00:00,"['Ziyi Xu', 'Ziyao Huang', 'Juan Cao', 'Yong Zhang', 'Xiaodong Cun', 'Qing Shuai', 'Yuchen Wang', 'Linchao Bao', 'Jintao Li', 'Fan Tang']",http://arxiv.org/abs/2411.17383v1
DisPose: Disentangling Pose Guidance for Controllable Human Image Animation,"Controllable human image animation aims to generate videos from reference
images using driving videos. Due to the limited control signals provided by
sparse guidance (e.g., skeleton pose), recent works have attempted to introduce
additional dense conditions (e.g., depth map) to ensure motion alignment.
However, such strict dense guidance impairs the quality of the generated video
when the body shape of the reference character differs significantly from that
of the driving video. In this paper, we present DisPose to mine more
generalizable and effective control signals without additional dense input,
which disentangles the sparse skeleton pose in human image animation into
motion field guidance and keypoint correspondence. Specifically, we generate a
dense motion field from a sparse motion field and the reference image, which
provides region-level dense guidance while maintaining the generalization of
the sparse pose control. We also extract diffusion features corresponding to
pose keypoints from the reference image, and then these point features are
transferred to the target pose to provide distinct identity information. To
seamlessly integrate into existing models, we propose a plug-and-play hybrid
ControlNet that improves the quality and consistency of generated videos while
freezing the existing model parameters. Extensive qualitative and quantitative
experiments demonstrate the superiority of DisPose compared to current methods.
Project page:
\href{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.",2024-12-12 15:15:59+00:00,"['Hongxiang Li', 'Yaowei Li', 'Yuhang Yang', 'Junjie Cao', 'Zhihong Zhu', 'Xuxin Cheng', 'Long Chen']",http://arxiv.org/abs/2412.09349v3
A spatiotemporal style transfer algorithm for dynamic visual stimulus generation,"Understanding how visual information is encoded in biological and artificial
systems often requires vision scientists to generate appropriate stimuli to
test specific hypotheses. Although deep neural network models have
revolutionized the field of image generation with methods such as image style
transfer, available methods for video generation are scarce. Here, we introduce
the Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus
generation framework that allows powerful manipulation and synthesis of video
stimuli for vision research. It is based on a two-stream deep neural network
model that factorizes spatial and temporal features to generate dynamic visual
stimuli whose model layer activations are matched to those of input videos. As
an example, we show that our algorithm enables the generation of model
metamers, dynamic stimuli whose layer activations within our two-stream model
are matched to those of natural videos. We show that these generated stimuli
match the low-level spatiotemporal features of their natural counterparts but
lack their high-level semantic features, making it a powerful paradigm to study
object recognition. Late layer activations in deep vision models exhibited a
lower similarity between natural and metameric stimuli compared to early
layers, confirming the lack of high-level information in the generated stimuli.
Finally, we use our generated stimuli to probe the representational
capabilities of predictive coding deep networks. These results showcase
potential applications of our algorithm as a versatile tool for dynamic
stimulus generation in vision science.",2024-03-07 23:07:46+00:00,"['Antonino Greco', 'Markus Siegel']",http://arxiv.org/abs/2403.04940v1
Exploiting Style Latent Flows for Generalizing Deepfake Video Detection,"This paper presents a new approach for the detection of fake videos, based on
the analysis of style latent vectors and their abnormal behavior in temporal
changes in the generated videos. We discovered that the generated facial videos
suffer from the temporal distinctiveness in the temporal changes of style
latent vectors, which are inevitable during the generation of temporally stable
videos with various facial expressions and geometric transformations. Our
framework utilizes the StyleGRU module, trained by contrastive learning, to
represent the dynamic properties of style latent vectors. Additionally, we
introduce a style attention module that integrates StyleGRU-generated features
with content-based features, enabling the detection of visual and temporal
artifacts. We demonstrate our approach across various benchmark scenarios in
deepfake detection, showing its superiority in cross-dataset and
cross-manipulation scenarios. Through further analysis, we also validate the
importance of using temporal changes of style latent vectors to improve the
generality of deepfake video detection.",2024-03-11 10:35:58+00:00,"['Jongwook Choi', 'Taehoon Kim', 'Yonghyun Jeong', 'Seungryul Baek', 'Jongwon Choi']",http://arxiv.org/abs/2403.06592v3
"A Survey on Long Video Generation: Challenges, Methods, and Prospects","Video generation is a rapidly advancing research area, garnering significant
attention due to its broad range of applications. One critical aspect of this
field is the generation of long-duration videos, which presents unique
challenges and opportunities. This paper presents the first survey of recent
advancements in long video generation and summarises them into two key
paradigms: divide and conquer temporal autoregressive.
  We delve into the common models employed in each paradigm, including aspects
of network design and conditioning techniques. Furthermore, we offer a
comprehensive overview and classification of the datasets and evaluation
metrics which are crucial for advancing long video generation research.
Concluding with a summary of existing studies, we also discuss the emerging
challenges and future directions in this dynamic field. We hope that this
survey will serve as an essential reference for researchers and practitioners
in the realm of long video generation.",2024-03-25 03:47:53+00:00,"['Chengxuan Li', 'Di Huang', 'Zeyu Lu', 'Yang Xiao', 'Qingqi Pei', 'Lei Bai']",http://arxiv.org/abs/2403.16407v1
Animating the Past: Reconstruct Trilobite via Video Generation,"Paleontology, the study of past life, fundamentally relies on fossils to
reconstruct ancient ecosystems and understand evolutionary dynamics.
Trilobites, as an important group of extinct marine arthropods, offer valuable
insights into Paleozoic environments through their well-preserved fossil
records. Reconstructing trilobite behaviour from static fossils will set new
standards for dynamic reconstructions in scientific research and education.
Despite the potential, current computational methods for this purpose like
text-to-video (T2V) face significant challenges, such as maintaining visual
realism and consistency, which hinder their application in science contexts. To
overcome these obstacles, we introduce an automatic T2V prompt learning method.
Within this framework, prompts for a fine-tuned video generation model are
generated by a large language model, which is trained using rewards that
quantify the visual realism and smoothness of the generated video. The
fine-tuning of the video generation model, along with the reward calculations
make use of a collected dataset of 9,088 Eoredlichia intermedia fossil images,
which provides a common representative of visual details of all class of
trilobites. Qualitative and quantitative experiments show that our method can
generate trilobite videos with significantly higher visual realism compared to
powerful baselines, promising to boost both scientific understanding and public
engagement.",2024-10-10 02:54:58+00:00,"['Xiaoran Wu', 'Zien Huang', 'Chonghan Yu']",http://arxiv.org/abs/2410.14715v1
RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video Captioning,"Despite the significant progress of fully-supervised video captioning,
zero-shot methods remain much less explored. In this paper, we propose a novel
zero-shot video captioning framework named Retrieval-Enhanced Test-Time
Adaptation (RETTA), which takes advantage of existing pretrained large-scale
vision and language models to directly generate captions with test-time
adaptation. Specifically, we bridge video and text using four key models: a
general video-text retrieval model XCLIP, a general image-text matching model
CLIP, a text alignment model AnglE, and a text generation model GPT-2, due to
their source-code availability. The main challenge is how to enable the text
generation model to be sufficiently aware of the content in a given video so as
to generate corresponding captions. To address this problem, we propose using
learnable tokens as a communication medium among these four frozen models
GPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains
these tokens with training data, we propose to learn these tokens with soft
targets of the inference data under several carefully crafted loss functions,
which enable the tokens to absorb video information catered for GPT-2. This
procedure can be efficiently done in just a few iterations (we use 16
iterations in the experiments) and does not require ground truth data.
Extensive experimental results on three widely used datasets, MSR-VTT, MSVD,
and VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric
CIDEr compared to several state-of-the-art zero-shot video captioning methods.",2024-05-11 16:22:00+00:00,"['Yunchuan Ma', 'Laiyun Qing', 'Guorong Li', 'Yuankai Qi', 'Amin Beheshti', 'Quan Z. Sheng', 'Qingming Huang']",http://arxiv.org/abs/2405.07046v2
LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical Video Learning,"Multimodal large language models (LLMs) have achieved notable success across
various domains, while research in the medical field has largely focused on
unimodal images. Meanwhile, current general-domain multimodal models for videos
still lack the capabilities to understand and engage in conversations about
surgical videos. One major contributing factor is the absence of datasets in
the surgical field. In this paper, we create a new dataset, Surg-QA, consisting
of 102,000 surgical video-instruction pairs, the largest of its kind so far. To
build such a dataset, we propose a novel two-stage question-answer generation
pipeline with LLM to learn surgical knowledge in a structured manner from the
publicly available surgical lecture videos. The pipeline breaks down the
generation process into two stages to significantly reduce the task complexity,
allowing us to use a more affordable, locally deployed open-source LLM than the
premium paid LLM services. It also mitigates the risk of LLM hallucinations
during question-answer generation, thereby enhancing the overall quality of the
generated data. We further train LLaVA-Surg, a novel vision-language
conversational assistant capable of answering open-ended questions about
surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations
on zero-shot surgical video question-answering tasks. We show that LLaVA-Surg
significantly outperforms all previous general-domain models, demonstrating
exceptional multimodal conversational skills in answering open-ended questions
about surgical videos. We will release our code, model, and the
instruction-tuning dataset.",2024-08-15 07:00:20+00:00,"['Jiajie Li', 'Garrett Skinner', 'Gene Yang', 'Brian R Quaranto', 'Steven D Schwaitzberg', 'Peter C W Kim', 'Jinjun Xiong']",http://arxiv.org/abs/2408.07981v1
ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos,"Previous studies on question generation from videos have mostly focused on
generating questions about common objects and attributes and hence are not
entity-centric. In this work, we focus on the generation of entity-centric
information-seeking questions from videos. Such a system could be useful for
video-based learning, recommending ``People Also Ask'' questions, video-based
chatbots, and fact-checking. Our work addresses three key challenges:
identifying question-worthy information, linking it to entities, and
effectively utilizing multimodal signals. Further, to the best of our
knowledge, there does not exist a large-scale dataset for this task. Most video
question generation datasets are on TV shows, movies, or human activities or
lack entity-centric information-seeking questions. Hence, we contribute a
diverse dataset of YouTube videos, VideoQuestions, consisting of 411 videos
with 2265 manually annotated questions. We further propose a model architecture
combining Transformers, rich context signals (titles, transcripts, captions,
embeddings), and a combination of cross-entropy and contrastive loss function
to encourage entity-centric question generation. Our best method yields BLEU,
ROUGE, CIDEr, and METEOR scores of 71.3, 78.6, 7.31, and 81.9, respectively,
demonstrating practical usability. We make the code and dataset publicly
available. https://github.com/thePhukan/ECIS-VQG",2024-10-13 08:33:16+00:00,"['Arpan Phukan', 'Manish Gupta', 'Asif Ekbal']",http://arxiv.org/abs/2410.09776v1
PersonalVideo: High ID-Fidelity Video Customization without Dynamic and Semantic Degradation,"The current text-to-video (T2V) generation has made significant progress in
synthesizing realistic general videos, but it is still under-explored in
identity-specific human video generation with customized ID images. The key
challenge lies in maintaining high ID fidelity consistently while preserving
the original motion dynamic and semantic following after the identity
injection. Current video identity customization methods mainly rely on
reconstructing given identity images on text-to-image models, which have a
divergent distribution with the T2V model. This process introduces a
tuning-inference gap, leading to dynamic and semantic degradation. To tackle
this problem, we propose a novel framework, dubbed $\textbf{PersonalVideo}$,
that applies a mixture of reward supervision on synthesized videos instead of
the simple reconstruction objective on images. Specifically, we first
incorporate identity consistency reward to effectively inject the reference's
identity without the tuning-inference gap. Then we propose a novel semantic
consistency reward to align the semantic distribution of the generated videos
with the original T2V model, which preserves its dynamic and semantic following
capability during the identity injection. With the non-reconstructive reward
training, we further employ simulated prompt augmentation to reduce overfitting
by supervising generated results in more semantic scenarios, gaining good
robustness even with only a single reference image. Extensive experiments
demonstrate our method's superiority in delivering high identity faithfulness
while preserving the inherent video generation qualities of the original T2V
model, outshining prior methods.",2024-11-26 02:25:38+00:00,"['Hengjia Li', 'Haonan Qiu', 'Shiwei Zhang', 'Xiang Wang', 'Yujie Wei', 'Zekun Li', 'Yingya Zhang', 'Boxi Wu', 'Deng Cai']",http://arxiv.org/abs/2411.17048v2
Virtual avatar generation models as world navigators,"We introduce SABR-CLIMB, a novel video model simulating human movement in
rock climbing environments using a virtual avatar. Our diffusion transformer
predicts the sample instead of noise in each diffusion step and ingests entire
videos to output complete motion sequences. By leveraging a large proprietary
dataset, NAV-22M, and substantial computational resources, we showcase a proof
of concept for a system to train general-purpose virtual avatars for complex
tasks in robotics, sports, and healthcare.",2024-06-03 07:10:15+00:00,['Sai Mandava'],http://arxiv.org/abs/2406.01056v1
ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation,"We propose a novel text-to-video (T2V) generation benchmark,
ChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the
T2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast
to existing benchmarks that focus on visual quality and textual relevance of
generated videos, ChronoMagic-Bench focuses on the model's ability to generate
time-lapse videos with significant metamorphic amplitude and temporal
coherence. The benchmark probes T2V models for their physics, biology, and
chemistry capabilities, in a free-form text query. For these purposes,
ChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,
categorized into four major types of time-lapse videos: biological,
human-created, meteorological, and physical phenomena, which are further
divided into 75 subcategories. This categorization comprehensively evaluates
the model's capacity to handle diverse and complex transformations. To
accurately align human preference with the benchmark, we introduce two new
automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic
attributes and temporal coherence. MTScore measures the metamorphic amplitude,
reflecting the degree of change over time, while CHScore assesses the temporal
coherence, ensuring the generated videos maintain logical progression and
continuity. Based on ChronoMagic-Bench, we conduct comprehensive manual
evaluations of ten representative T2V models, revealing their strengths and
weaknesses across different categories of prompts, and providing a thorough
evaluation framework that addresses current gaps in video generation research.
Moreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k
high-quality pairs of 720p time-lapse videos and detailed captions ensuring
high physical pertinence and large metamorphic amplitude.
[Homepage](https://pku-yuangroup.github.io/ChronoMagic-Bench/).",2024-06-26 17:50:47+00:00,"['Shenghai Yuan', 'Jinfa Huang', 'Yongqi Xu', 'Yaoyang Liu', 'Shaofeng Zhang', 'Yujun Shi', 'Ruijie Zhu', 'Xinhua Cheng', 'Jiebo Luo', 'Li Yuan']",http://arxiv.org/abs/2406.18522v2
Towards Scene Graph Anticipation,"Spatio-temporal scene graphs represent interactions in a video by decomposing
scenes into individual objects and their pair-wise temporal relationships.
Long-term anticipation of the fine-grained pair-wise relationships between
objects is a challenging problem. To this end, we introduce the task of Scene
Graph Anticipation (SGA). We adapt state-of-the-art scene graph generation
methods as baselines to anticipate future pair-wise relationships between
objects and propose a novel approach SceneSayer. In SceneSayer, we leverage
object-centric representations of relationships to reason about the observed
video frames and model the evolution of relationships between objects. We take
a continuous time perspective and model the latent dynamics of the evolution of
object interactions using concepts of NeuralODE and NeuralSDE, respectively. We
infer representations of future relationships by solving an Ordinary
Differential Equation and a Stochastic Differential Equation, respectively.
Extensive experimentation on the Action Genome dataset validates the efficacy
of the proposed methods.",2024-03-07 21:08:51+00:00,"['Rohith Peddi', 'Saksham Singh', 'Saurabh', 'Parag Singla', 'Vibhav Gogate']",http://arxiv.org/abs/2403.04899v2
Adversarial Diffusion Compression for Real-World Image Super-Resolution,"Real-world image super-resolution (Real-ISR) aims to reconstruct
high-resolution images from low-resolution inputs degraded by complex, unknown
processes. While many Stable Diffusion (SD)-based Real-ISR methods have
achieved remarkable success, their slow, multi-step inference hinders practical
deployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate
this issue but still incur high computational costs due to their reliance on
large pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR,
by distilling the one-step diffusion network OSEDiff into a streamlined
diffusion-GAN model under our Adversarial Diffusion Compression (ADC)
framework. We meticulously examine the modules of OSEDiff, categorizing them
into two types: (1) Removable (VAE encoder, prompt extractor, text encoder,
etc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal
and pruning can degrade the model's generation capability, we pretrain our
pruned VAE decoder to restore its ability to decode images and employ
adversarial distillation to compensate for performance loss. This ADC-based
diffusion-GAN hybrid design effectively reduces complexity by 73% in inference
time, 78% in computation, and 74% in parameters, while preserving the model's
generation capability. Experiments manifest that our proposed AdcSR achieves
competitive recovery quality on both synthetic and real-world datasets,
offering up to 9.3$\times$ speedup over previous one-step diffusion-based
methods. Code and models are available at
https://github.com/Guaishou74851/AdcSR.",2024-11-20 15:13:36+00:00,"['Bin Chen', 'Gehui Li', 'Rongyuan Wu', 'Xindong Zhang', 'Jie Chen', 'Jian Zhang', 'Lei Zhang']",http://arxiv.org/abs/2411.13383v2
T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback,"Diffusion-based text-to-video (T2V) models have achieved significant success
but continue to be hampered by the slow sampling speed of their iterative
sampling processes. To address the challenge, consistency models have been
proposed to facilitate fast inference, albeit at the cost of sample quality. In
this work, we aim to break the quality bottleneck of a video consistency model
(VCM) to achieve $\textbf{both fast and high-quality video generation}$. We
introduce T2V-Turbo, which integrates feedback from a mixture of differentiable
reward models into the consistency distillation (CD) process of a pre-trained
T2V model. Notably, we directly optimize rewards associated with single-step
generations that arise naturally from computing the CD loss, effectively
bypassing the memory constraints imposed by backpropagating gradients through
an iterative sampling process. Remarkably, the 4-step generations from our
T2V-Turbo achieve the highest total score on VBench, even surpassing Gen-2 and
Pika. We further conduct human evaluations to corroborate the results,
validating that the 4-step generations from our T2V-Turbo are preferred over
the 50-step DDIM samples from their teacher models, representing more than a
tenfold acceleration while improving video generation quality.",2024-05-29 04:26:17+00:00,"['Jiachen Li', 'Weixi Feng', 'Tsu-Jui Fu', 'Xinyi Wang', 'Sugato Basu', 'Wenhu Chen', 'William Yang Wang']",http://arxiv.org/abs/2405.18750v2
LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models,"Patient data from real-world clinical practice often suffers from data
scarcity and long-tail imbalances, leading to biased outcomes or algorithmic
unfairness. This study addresses these challenges by generating
lesion-containing image-segmentation pairs from lesion-free images. Previous
efforts in medical imaging synthesis have struggled with separating lesion
information from background, resulting in low-quality backgrounds and limited
control over the synthetic output. Inspired by diffusion-based image
inpainting, we propose LeFusion, a lesion-focused diffusion model. By
redesigning the diffusion learning objectives to focus on lesion areas, we
simplify the learning process and improve control over the output while
preserving high-fidelity backgrounds by integrating forward-diffused background
contexts into the reverse diffusion process. Additionally, we tackle two major
challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class
lesions. We introduce two effective strategies: histogram-based texture control
and multi-channel decomposition, enabling the controlled generation of
high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion
mask diffusion, allowing control over lesion size, location, and boundary, thus
increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule
CT datasets, LeFusion-generated data significantly improves the performance of
state-of-the-art segmentation models, including nnUNet and SwinUNETR. Code and
model are available at https://github.com/M3DV/LeFusion.",2024-03-21 01:25:39+00:00,"['Hantao Zhang', 'Yuhe Liu', 'Jiancheng Yang', 'Shouhong Wan', 'Xinyuan Wang', 'Wei Peng', 'Pascal Fua']",http://arxiv.org/abs/2403.14066v2
Speech2UnifiedExpressions: Synchronous Synthesis of Co-Speech Affective Face and Body Expressions from Affordable Inputs,"We present a multimodal learning-based method to simultaneously synthesize
co-speech facial expressions and upper-body gestures for digital characters
using RGB video data captured using commodity cameras. Our approach learns from
sparse face landmarks and upper-body joints, estimated directly from video
data, to generate plausible emotive character motions. Given a speech audio
waveform and a token sequence of the speaker's face landmark motion and
body-joint motion computed from a video, our method synthesizes the motion
sequences for the speaker's face landmarks and body joints to match the content
and the affect of the speech. We design a generator consisting of a set of
encoders to transform all the inputs into a multimodal embedding space
capturing their correlations, followed by a pair of decoders to synthesize the
desired face and pose motions. To enhance the plausibility of synthesis, we use
an adversarial discriminator that learns to differentiate between the face and
pose motions computed from the original videos and our synthesized motions
based on their affective expressions. To evaluate our approach, we extend the
TED Gesture Dataset to include view-normalized, co-speech face landmarks in
addition to body gestures. We demonstrate the performance of our method through
thorough quantitative and qualitative experiments on multiple evaluation
metrics and via a user study. We observe that our method results in low
reconstruction error and produces synthesized samples with diverse facial
expressions and body gestures for digital characters.",2024-06-26 04:53:11+00:00,"['Uttaran Bhattacharya', 'Aniket Bera', 'Dinesh Manocha']",http://arxiv.org/abs/2406.18068v2
"MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation","Recently, the diffusion-based generative paradigm has achieved impressive
general image generation capabilities with text prompts due to its accurate
distribution modeling and stable training process. However, generating diverse
remote sensing (RS) images that are tremendously different from general images
in terms of scale and perspective remains a formidable challenge due to the
lack of a comprehensive remote sensing image generation dataset with various
modalities, ground sample distances (GSD), and scenes. In this paper, we
propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset
and benchmark for text-to-image generation in diverse remote sensing scenarios.
Specifically, we first collect nine publicly available RS datasets and conduct
standardization for all samples. To bridge RS images to textual semantic
information, we utilize a large-scale pretrained vision-language model to
automatically output text prompts and perform hand-crafted rectification,
resulting in information-rich text-image pairs (including multi-modal images).
In particular, we design some methods to obtain the images with different GSD
and various environments (e.g., low-light, foggy) in a single sample. With
extensive manual screening and refining annotations, we ultimately obtain a
MMM-RS dataset that comprises approximately 2.1 million text-image pairs.
Extensive experimental results verify that our proposed MMM-RS dataset allows
off-the-shelf diffusion models to generate diverse RS images across various
modalities, scenes, weather conditions, and GSD. The dataset is available at
https://github.com/ljl5261/MMM-RS.",2024-10-26 11:19:07+00:00,"['Jialin Luo', 'Yuanzhi Wang', 'Ziqi Gu', 'Yide Qiu', 'Shuaizhen Yao', 'Fuyun Wang', 'Chunyan Xu', 'Wenhua Zhang', 'Dan Wang', 'Zhen Cui']",http://arxiv.org/abs/2410.22362v1
Learning Temporally Consistent Video Depth from Video Diffusion Priors,"This work addresses the challenge of streamed video depth estimation, which
expects not only per-frame accuracy but, more importantly, cross-frame
consistency. We argue that sharing contextual information between frames or
clips is pivotal in fostering temporal consistency. Thus, instead of directly
developing a depth estimator from scratch, we reformulate this predictive task
into a conditional generation problem to provide contextual information within
a clip and across clips. Specifically, we propose a consistent context-aware
training and inference strategy for arbitrarily long videos to provide
cross-clip context. We sample independent noise levels for each frame within a
clip during training while using a sliding window strategy and initializing
overlapping frames with previously predicted frames without adding noise.
Moreover, we design an effective training strategy to provide context within a
clip. Extensive experimental results validate our design choices and
demonstrate the superiority of our approach, dubbed ChronoDepth. Project page:
https://xdimlab.github.io/ChronoDepth/.",2024-06-03 16:20:24+00:00,"['Jiahao Shao', 'Yuanbo Yang', 'Hongyu Zhou', 'Youmin Zhang', 'Yujun Shen', 'Vitor Guizilini', 'Yue Wang', 'Matteo Poggi', 'Yiyi Liao']",http://arxiv.org/abs/2406.01493v3
Sora Generates Videos with Stunning Geometrical Consistency,"The recently developed Sora model [1] has exhibited remarkable capabilities
in video generation, sparking intense discussions regarding its ability to
simulate real-world phenomena. Despite its growing popularity, there is a lack
of established metrics to evaluate its fidelity to real-world physics
quantitatively. In this paper, we introduce a new benchmark that assesses the
quality of the generated videos based on their adherence to real-world physics
principles. We employ a method that transforms the generated videos into 3D
models, leveraging the premise that the accuracy of 3D reconstruction is
heavily contingent on the video quality. From the perspective of 3D
reconstruction, we use the fidelity of the geometric constraints satisfied by
the constructed 3D models as a proxy to gauge the extent to which the generated
videos conform to real-world physics rules. Project page:
https://sora-geometrical-consistency.github.io/",2024-02-27 10:49:05+00:00,"['Xuanyi Li', 'Daquan Zhou', 'Chenxu Zhang', 'Shaodong Wei', 'Qibin Hou', 'Ming-Ming Cheng']",http://arxiv.org/abs/2402.17403v1
DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation,"Recent advancements in generative models have provided promising solutions
for synthesizing realistic driving videos, which are crucial for training
autonomous driving perception models. However, existing approaches often
struggle with multi-view video generation due to the challenges of integrating
3D information while maintaining spatial-temporal consistency and effectively
learning from a unified model. We propose DriveScape, an end-to-end framework
for multi-view, 3D condition-guided video generation, capable of producing 1024
x 576 high-resolution videos at 10Hz. Unlike other methods limited to 2Hz due
to the 3D box annotation frame rate, DriveScape overcomes this with its ability
to operate under sparse conditions. Our Bi-Directional Modulated Transformer
(BiMot) ensures precise alignment of 3D structural information, maintaining
spatial-temporal consistency. DriveScape excels in video generation
performance, achieving state-of-the-art results on the nuScenes dataset with an
FID score of 8.34 and an FVD score of 76.39. Our project homepage:
https://metadrivescape.github.io/papers_project/drivescapev1/index.html",2024-09-09 09:43:17+00:00,"['Wei Wu', 'Xi Guo', 'Weixuan Tang', 'Tingxuan Huang', 'Chiyu Wang', 'Dongyue Chen', 'Chenjing Ding']",http://arxiv.org/abs/2409.05463v4
Still-Moving: Customized Video Generation without Customized Video Data,"Customizing text-to-image (T2I) models has seen tremendous progress recently,
particularly in areas such as personalization, stylization, and conditional
generation. However, expanding this progress to video generation is still in
its infancy, primarily due to the lack of customized video data. In this work,
we introduce Still-Moving, a novel generic framework for customizing a
text-to-video (T2V) model, without requiring any customized video data. The
framework applies to the prominent T2V design where the video model is built
over a text-to-image (T2I) model (e.g., via inflation). We assume access to a
customized version of the T2I model, trained only on still image data (e.g.,
using DreamBooth or StyleDrop). Naively plugging in the weights of the
customized T2I model into the T2V model often leads to significant artifacts or
insufficient adherence to the customization data. To overcome this issue, we
train lightweight $\textit{Spatial Adapters}$ that adjust the features produced
by the injected T2I layers. Importantly, our adapters are trained on
$\textit{""frozen videos""}$ (i.e., repeated images), constructed from image
samples generated by the customized T2I model. This training is facilitated by
a novel $\textit{Motion Adapter}$ module, which allows us to train on such
static videos while preserving the motion prior of the video model. At test
time, we remove the Motion Adapter modules and leave in only the trained
Spatial Adapters. This restores the motion prior of the T2V model while
adhering to the spatial prior of the customized T2I model. We demonstrate the
effectiveness of our approach on diverse tasks including personalized,
stylized, and conditional generation. In all evaluated scenarios, our method
seamlessly integrates the spatial prior of the customized T2I model with a
motion prior supplied by the T2V model.",2024-07-11 17:06:53+00:00,"['Hila Chefer', 'Shiran Zada', 'Roni Paiss', 'Ariel Ephrat', 'Omer Tov', 'Michael Rubinstein', 'Lior Wolf', 'Tali Dekel', 'Tomer Michaeli', 'Inbar Mosseri']",http://arxiv.org/abs/2407.08674v1
Explainable Deepfake Video Detection using Convolutional Neural Network and CapsuleNet,"Deepfake technology, derived from deep learning, seamlessly inserts
individuals into digital media, irrespective of their actual participation. Its
foundation lies in machine learning and Artificial Intelligence (AI).
Initially, deepfakes served research, industry, and entertainment. While the
concept has existed for decades, recent advancements render deepfakes nearly
indistinguishable from reality. Accessibility has soared, empowering even
novices to create convincing deepfakes. However, this accessibility raises
security concerns.The primary deepfake creation algorithm, GAN (Generative
Adversarial Network), employs machine learning to craft realistic images or
videos. Our objective is to utilize CNN (Convolutional Neural Network) and
CapsuleNet with LSTM to differentiate between deepfake-generated frames and
originals. Furthermore, we aim to elucidate our model's decision-making process
through Explainable AI, fostering transparent human-AI relationships and
offering practical examples for real-life scenarios.",2024-04-19 12:21:27+00:00,"['Gazi Hasin Ishrak', 'Zalish Mahmud', 'MD. Zami Al Zunaed Farabe', 'Tahera Khanom Tinni', 'Tanzim Reza', 'Mohammad Zavid Parvez']",http://arxiv.org/abs/2404.12841v1
Unsupervised Sign Language Translation and Generation,"Motivated by the success of unsupervised neural machine translation (UNMT),
we introduce an unsupervised sign language translation and generation network
(USLNet), which learns from abundant single-modality (text and video) data
without parallel sign language data. USLNet comprises two main components:
single-modality reconstruction modules (text and video) that rebuild the input
from its noisy version in the same modality and cross-modality back-translation
modules (text-video-text and video-text-video) that reconstruct the input from
its noisy version in the different modality using back-translation
procedure.Unlike the single-modality back-translation procedure in text-based
UNMT, USLNet faces the cross-modality discrepancy in feature representation, in
which the length and the feature dimension mismatch between text and video
sequences. We propose a sliding window method to address the issues of aligning
variable-length text with video sequences. To our knowledge, USLNet is the
first unsupervised sign language translation and generation model capable of
generating both natural language text and sign language video in a unified
manner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL)
and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet
achieves competitive results compared to supervised baseline models, indicating
its effectiveness in sign language translation and generation.",2024-02-12 15:39:05+00:00,"['Zhengsheng Guo', 'Zhiwei He', 'Wenxiang Jiao', 'Xing Wang', 'Rui Wang', 'Kehai Chen', 'Zhaopeng Tu', 'Yong Xu', 'Min Zhang']",http://arxiv.org/abs/2402.07726v1
VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding,"Recent studies have demonstrated the effectiveness of Large Language Models
(LLMs) as reasoning modules that can deconstruct complex tasks into more
manageable sub-tasks, particularly when applied to visual reasoning tasks for
images. In contrast, this paper introduces a Video Understanding and Reasoning
Framework (VURF) based on the reasoning power of LLMs. Ours is a novel approach
to extend the utility of LLMs in the context of video tasks, leveraging their
capacity to generalize from minimal input and output demonstrations within a
contextual framework. We harness their contextual learning capabilities by
presenting LLMs with pairs of instructions and their corresponding high-level
programs to generate executable visual programs for video understanding. To
enhance the program's accuracy and robustness, we implement two important
strategies. \emph{Firstly,} we employ a feedback-generation approach, powered
by GPT-3.5, to rectify errors in programs utilizing unsupported functions.
\emph{Secondly}, taking motivation from recent works on self-refinement of LLM
outputs, we introduce an iterative procedure for improving the quality of the
in-context examples by aligning the initial outputs to the outputs that would
have been generated had the LLM not been bound by the structure of the
in-context examples. Our results on several video-specific tasks, including
visual QA, video anticipation, pose estimation, and multi-video QA, illustrate
these enhancements' efficacy in improving the performance of visual programming
approaches for video tasks.",2024-03-21 18:00:00+00:00,"['Ahmad Mahmood', 'Ashmal Vayani', 'Muzammal Naseer', 'Salman Khan', 'Fahad Shahbaz Khan']",http://arxiv.org/abs/2403.14743v3
Configurable Embodied Data Generation for Class-Agnostic RGB-D Video Segmentation,"This paper presents a method for generating large-scale datasets to improve
class-agnostic video segmentation across robots with different form factors.
Specifically, we consider the question of whether video segmentation models
trained on generic segmentation data could be more effective for particular
robot platforms if robot embodiment is factored into the data generation
process. To answer this question, a pipeline is formulated for using 3D
reconstructions (e.g. from HM3DSem) to generate segmented videos that are
configurable based on a robot's embodiment (e.g. sensor type, sensor placement,
and illumination source). A resulting massive RGB-D video panoptic segmentation
dataset (MVPd) is introduced for extensive benchmarking with foundation and
video segmentation models, as well as to support embodiment-focused research in
video segmentation. Our experimental findings demonstrate that using MVPd for
finetuning can lead to performance improvements when transferring foundation
models to certain robot embodiments, such as specific camera placements. These
experiments also show that using 3D modalities (depth images and camera pose)
can lead to improvements in video segmentation accuracy and consistency. The
project webpage is available at https://topipari.com/projects/MVPd",2024-10-16 19:43:23+00:00,"['Anthony Opipari', 'Aravindhan K Krishnan', 'Shreekant Gayaka', 'Min Sun', 'Cheng-Hao Kuo', 'Arnie Sen', 'Odest Chadwicke Jenkins']",http://arxiv.org/abs/2410.12995v1
Video-Guided Foley Sound Generation with Multimodal Controls,"Generating sound effects for videos often requires creating artistic sound
effects that diverge significantly from real-life sources and flexible control
in the sound design. To address this problem, we introduce MultiFoley, a model
designed for video-guided sound generation that supports multimodal
conditioning through text, audio, and video. Given a silent video and a text
prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels
spinning without wind noise) or more whimsical sounds (e.g., making a lion's
roar sound like a cat's meow). MultiFoley also allows users to choose reference
audio from sound effects (SFX) libraries or partial videos for conditioning. A
key novelty of our model lies in its joint training on both internet video
datasets with low-quality audio and professional SFX recordings, enabling
high-quality, full-bandwidth (48kHz) audio generation. Through automated
evaluations and human studies, we demonstrate that MultiFoley successfully
generates synchronized high-quality sounds across varied conditional inputs and
outperforms existing methods. Please see our project page for video results:
https://ificl.github.io/MultiFoley/",2024-11-26 18:59:58+00:00,"['Ziyang Chen', 'Prem Seetharaman', 'Bryan Russell', 'Oriol Nieto', 'David Bourgin', 'Andrew Owens', 'Justin Salamon']",http://arxiv.org/abs/2411.17698v4
EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing,"Current diffusion-based video editing primarily focuses on local editing
(\textit{e.g.,} object/background editing) or global style editing by utilizing
various dense correspondences. However, these methods often fail to accurately
edit the foreground and background simultaneously while preserving the original
layout. We find that the crux of the issue stems from the imprecise
distribution of attention weights across designated regions, including
inaccurate text-to-attribute control and attention leakage. To tackle this
issue, we introduce EVA, a \textbf{zero-shot} and \textbf{multi-attribute}
video editing framework tailored for human-centric videos with complex motions.
We incorporate a Spatial-Temporal Layout-Guided Attention mechanism that
leverages the intrinsic positive and negative correspondences of cross-frame
diffusion features. To avoid attention leakage, we utilize these
correspondences to boost the attention scores of tokens within the same
attribute across all video frames while limiting interactions between tokens of
different attributes in the self-attention layer. For precise text-to-attribute
manipulation, we use discrete text embeddings focused on specific layout areas
within the cross-attention layer. Benefiting from the precise attention weight
distribution, EVA can be easily generalized to multi-object editing scenarios
and achieves accurate identity mapping. Extensive experiments demonstrate EVA
achieves state-of-the-art results in real-world scenarios. Full results are
provided at https://knightyxp.github.io/EVA/",2024-03-24 12:04:06+00:00,"['Xiangpeng Yang', 'Linchao Zhu', 'Hehe Fan', 'Yi Yang']",http://arxiv.org/abs/2403.16111v1
Hiding Faces in Plain Sight: Defending DeepFakes by Disrupting Face Detection,"This paper investigates the feasibility of a proactive DeepFake defense
framework, {\em FacePosion}, to prevent individuals from becoming victims of
DeepFake videos by sabotaging face detection. The motivation stems from the
reliance of most DeepFake methods on face detectors to automatically extract
victim faces from videos for training or synthesis (testing). Once the face
detectors malfunction, the extracted faces will be distorted or incorrect,
subsequently disrupting the training or synthesis of the DeepFake model. To
achieve this, we adapt various adversarial attacks with a dedicated design for
this purpose and thoroughly analyze their feasibility. Based on FacePoison, we
introduce {\em VideoFacePoison}, a strategy that propagates FacePoison across
video frames rather than applying them individually to each frame. This
strategy can largely reduce the computational overhead while retaining the
favorable attack performance. Our method is validated on five face detectors,
and extensive experiments against eleven different DeepFake models demonstrate
the effectiveness of disrupting face detectors to hinder DeepFake generation.",2024-12-02 04:17:48+00:00,"['Delong Zhu', 'Yuezun Li', 'Baoyuan Wu', 'Jiaran Zhou', 'Zhibo Wang', 'Siwei Lyu']",http://arxiv.org/abs/2412.01101v1
ILDiff: Generate Transparent Animated Stickers by Implicit Layout Distillation,"High-quality animated stickers usually contain transparent channels, which
are often ignored by current video generation models. To generate fine-grained
animated transparency channels, existing methods can be roughly divided into
video matting algorithms and diffusion-based algorithms. The methods based on
video matting have poor performance in dealing with semi-open areas in
stickers, while diffusion-based methods are often used to model a single image,
which will lead to local flicker when modeling animated stickers. In this
paper, we firstly propose an ILDiff method to generate animated transparent
channels through implicit layout distillation, which solves the problems of
semi-open area collapse and no consideration of temporal information in
existing methods. Secondly, we create the Transparent Animated Sticker Dataset
(TASD), which contains 0.32M high-quality samples with transparent channel, to
provide data support for related fields. Extensive experiments demonstrate that
ILDiff can produce finer and smoother transparent channels compared to other
methods such as Matting Anything and Layer Diffusion. Our code and dataset will
be released at link https://xiaoyuan1996.github.io.",2024-12-30 12:27:35+00:00,"['Ting Zhang', 'Zhiqiang Yuan', 'Yeshuang Zhu', 'Jinchao Zhang']",http://arxiv.org/abs/2412.20901v1
Motion-aware Latent Diffusion Models for Video Frame Interpolation,"With the advancement of AIGC, video frame interpolation (VFI) has become a
crucial component in existing video generation frameworks, attracting
widespread research interest. For the VFI task, the motion estimation between
neighboring frames plays a crucial role in avoiding motion ambiguity. However,
existing VFI methods always struggle to accurately predict the motion
information between consecutive frames, and this imprecise estimation leads to
blurred and visually incoherent interpolated frames. In this paper, we propose
a novel diffusion framework, motion-aware latent diffusion models (MADiff),
which is specifically designed for the VFI task. By incorporating motion priors
between the conditional neighboring frames with the target interpolated frame
predicted throughout the diffusion sampling procedure, MADiff progressively
refines the intermediate outcomes, culminating in generating both visually
smooth and realistic results. Extensive experiments conducted on benchmark
datasets demonstrate that our method achieves state-of-the-art performance
significantly outperforming existing approaches, especially under challenging
scenarios involving dynamic textures with complex motion.",2024-04-21 05:09:56+00:00,"['Zhilin Huang', 'Yijie Yu', 'Ling Yang', 'Chujun Qin', 'Bing Zheng', 'Xiawu Zheng', 'Zikun Zhou', 'Yaowei Wang', 'Wenming Yang']",http://arxiv.org/abs/2404.13534v3
Style-NeRF2NeRF: 3D Style Transfer From Style-Aligned Multi-View Images,"We propose a simple yet effective pipeline for stylizing a 3D scene,
harnessing the power of 2D image diffusion models. Given a NeRF model
reconstructed from a set of multi-view images, we perform 3D style transfer by
refining the source NeRF model using stylized images generated by a
style-aligned image-to-image diffusion model. Given a target style prompt, we
first generate perceptually similar multi-view images by leveraging a
depth-conditioned diffusion model with an attention-sharing mechanism. Next,
based on the stylized multi-view images, we propose to guide the style transfer
process with the sliced Wasserstein loss based on the feature maps extracted
from a pre-trained CNN model. Our pipeline consists of decoupled steps,
allowing users to test various prompt ideas and preview the stylized 3D result
before proceeding to the NeRF fine-tuning stage. We demonstrate that our method
can transfer diverse artistic styles to real-world 3D scenes with competitive
quality. Result videos are also available on our project page:
https://haruolabs.github.io/style-n2n/",2024-06-19 09:36:18+00:00,"['Haruo Fujiwara', 'Yusuke Mukuta', 'Tatsuya Harada']",http://arxiv.org/abs/2406.13393v3
Music Consistency Models,"Consistency models have exhibited remarkable capabilities in facilitating
efficient image/video generation, enabling synthesis with minimal sampling
steps. It has proven to be advantageous in mitigating the computational burdens
associated with diffusion models. Nevertheless, the application of consistency
models in music generation remains largely unexplored. To address this gap, we
present Music Consistency Models (\texttt{MusicCM}), which leverages the
concept of consistency models to efficiently synthesize mel-spectrogram for
music clips, maintaining high quality while minimizing the number of sampling
steps. Building upon existing text-to-music diffusion models, the
\texttt{MusicCM} model incorporates consistency distillation and adversarial
discriminator training. Moreover, we find it beneficial to generate extended
coherent music by incorporating multiple diffusion processes with shared
constraints. Experimental results reveal the effectiveness of our model in
terms of computational efficiency, fidelity, and naturalness. Notable,
\texttt{MusicCM} achieves seamless music synthesis with a mere four sampling
steps, e.g., only one second per minute of the music clip, showcasing the
potential for real-time application.",2024-04-20 11:52:30+00:00,"['Zhengcong Fei', 'Mingyuan Fan', 'Junshi Huang']",http://arxiv.org/abs/2404.13358v1
"A Comprehensive Taxonomy and Analysis of Talking Head Synthesis: Techniques for Portrait Generation, Driving Mechanisms, and Editing","Talking head synthesis, an advanced method for generating portrait videos
from a still image driven by specific content, has garnered widespread
attention in virtual reality, augmented reality and game production. Recently,
significant breakthroughs have been made with the introduction of novel models
such as the transformer and the diffusion model. Current methods can not only
generate new content but also edit the generated material. This survey
systematically reviews the technology, categorizing it into three pivotal
domains: portrait generation, driven mechanisms, and editing techniques. We
summarize milestone studies and critically analyze their innovations and
shortcomings within each domain. Additionally, we organize an extensive
collection of datasets and provide a thorough performance analysis of current
methodologies based on various evaluation metrics, aiming to furnish a clear
framework and robust data support for future research. Finally, we explore
application scenarios of talking head synthesis, illustrate them with specific
cases, and examine potential future directions.",2024-06-15 08:14:59+00:00,"['Ming Meng', 'Yufei Zhao', 'Bo Zhang', 'Yonggui Zhu', 'Weimin Shi', 'Maxwell Wen', 'Zhaoxin Fan']",http://arxiv.org/abs/2406.10553v2
Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis,"Human motion generation is a long-standing problem, and scene-aware motion
synthesis has been widely researched recently due to its numerous applications.
Prevailing methods rely heavily on paired motion-scene data whose quantity is
limited. Meanwhile, it is difficult to generalize to diverse scenes when
trained only on a few specific ones. Thus, we propose a unified framework,
termed Diffusion Implicit Policy (DIP), for scene-aware motion synthesis, where
paired motion-scene data are no longer necessary. In this framework, we
disentangle human-scene interaction from motion synthesis during training and
then introduce an interaction-based implicit policy into motion diffusion
during inference. Synthesized motion can be derived through iterative diffusion
denoising and implicit policy optimization, thus motion naturalness and
interaction plausibility can be maintained simultaneously. The proposed
implicit policy optimizes the intermediate noised motion in a GAN Inversion
manner to maintain motion continuity and control keyframe poses though the
ControlNet branch and motion inpainting. For long-term motion synthesis, we
introduce motion blending for stable transitions between multiple sub-tasks,
where motions are fused in rotation power space and translation linear space.
The proposed method is evaluated on synthesized scenes with ShapeNet furniture,
and real scenes from PROX and Replica. Results show that our framework presents
better motion naturalness and interaction plausibility than cutting-edge
methods. This also indicates the feasibility of utilizing the DIP for motion
synthesis in more general tasks and versatile scenes.
https://jingyugong.github.io/DiffusionImplicitPolicy/",2024-12-03 08:34:41+00:00,"['Jingyu Gong', 'Chong Zhang', 'Fengqi Liu', 'Ke Fan', 'Qianyu Zhou', 'Xin Tan', 'Zhizhong Zhang', 'Yuan Xie', 'Lizhuang Ma']",http://arxiv.org/abs/2412.02261v1
AnimateZoo: Zero-shot Video Generation of Cross-Species Animation via Subject Alignment,"Recent video editing advancements rely on accurate pose sequences to animate
subjects. However, these efforts are not suitable for cross-species animation
due to pose misalignment between species (for example, the poses of a cat
differs greatly from that of a pig due to differences in body structure). In
this paper, we present AnimateZoo, a zero-shot diffusion-based video generator
to address this challenging cross-species animation issue, aiming to accurately
produce animal animations while preserving the background. The key technique
used in our AnimateZoo is subject alignment, which includes two steps. First,
we improve appearance feature extraction by integrating a Laplacian detail
booster and a prompt-tuning identity extractor. These components are
specifically designed to capture essential appearance information, including
identity and fine details. Second, we align shape features and address
conflicts from differing subjects by introducing a scale-information remover.
This ensures accurate cross-species animation. Moreover, we introduce two
high-quality animal video datasets featuring a wide variety of species. Trained
on these extensive datasets, our model is capable of generating videos
characterized by accurate movements, consistent appearance, and high-fidelity
frames, without the need for the pre-inference fine-tuning that prior arts
required. Extensive experiments showcase the outstanding performance of our
method in cross-species action following tasks, demonstrating exceptional shape
adaptation capability. The project page is available at
https://justinxu0.github.io/AnimateZoo/.",2024-04-07 12:57:41+00:00,"['Yuanfeng Xu', 'Yuhao Chen', 'Zhongzhan Huang', 'Zijian He', 'Guangrun Wang', 'Philip Torr', 'Liang Lin']",http://arxiv.org/abs/2404.04946v1
BaboonLand Dataset: Tracking Primates in the Wild and Automating Behaviour Recognition from Drone Videos,"Using drones to track multiple individuals simultaneously in their natural
environment is a powerful approach for better understanding group primate
behavior. Previous studies have demonstrated that it is possible to automate
the classification of primate behavior from video data, but these studies have
been carried out in captivity or from ground-based cameras. To understand group
behavior and the self-organization of a collective, the whole troop needs to be
seen at a scale where behavior can be seen in relation to the natural
environment in which ecological decisions are made. This study presents a novel
dataset from drone videos for baboon detection, tracking, and behavior
recognition. The baboon detection dataset was created by manually annotating
all baboons in drone videos with bounding boxes. A tiling method was
subsequently applied to create a pyramid of images at various scales from the
original 5.3K resolution images, resulting in approximately 30K images used for
baboon detection. The tracking dataset is derived from the detection dataset,
where all bounding boxes are assigned the same ID throughout the video. This
process resulted in half an hour of very dense tracking data. The behavior
recognition dataset was generated by converting tracks into mini-scenes, a
video subregion centered on each animal; each mini-scene was manually annotated
with 12 distinct behavior types, resulting in over 20 hours of data. Benchmark
results show mean average precision (mAP) of 92.62\% for the YOLOv8-X detection
model, multiple object tracking precision (MOTA) of 63.81\% for the BotSort
tracking algorithm, and micro top-1 accuracy of 63.97\% for the X3D behavior
recognition model. Using deep learning to classify wildlife behavior from drone
footage facilitates non-invasive insight into the collective behavior of an
entire group.",2024-05-27 23:09:37+00:00,"['Isla Duporge', 'Maksim Kholiavchenko', 'Roi Harel', 'Scott Wolf', 'Dan Rubenstein', 'Meg Crofoot', 'Tanya Berger-Wolf', 'Stephen Lee', 'Julie Barreau', 'Jenna Kline', 'Michelle Ramirez', 'Charles Stewart']",http://arxiv.org/abs/2405.17698v3
Histo-Diffusion: A Diffusion Super-Resolution Method for Digital Pathology with Comprehensive Quality Assessment,"Digital pathology has advanced significantly over the last decade, with Whole
Slide Images (WSIs) encompassing vast amounts of data essential for accurate
disease diagnosis. High-resolution WSIs are essential for precise diagnosis but
technical limitations in scanning equipment and variablity in slide preparation
can hinder obtaining these images. Super-resolution techniques can enhance
low-resolution images; while Generative Adversarial Networks (GANs) have been
effective in natural image super-resolution tasks, they often struggle with
histopathology due to overfitting and mode collapse. Traditional evaluation
metrics fall short in assessing the complex characteristics of histopathology
images, necessitating robust histology-specific evaluation methods.
  We introduce Histo-Diffusion, a novel diffusion-based method specially
designed for generating and evaluating super-resolution images in digital
pathology. It includes a restoration module for histopathology prior and a
controllable diffusion module for generating high-quality images. We have
curated two histopathology datasets and proposed a comprehensive evaluation
strategy which incorporates both full-reference and no-reference metrics to
thoroughly assess the quality of digital pathology images.
  Comparative analyses on multiple datasets with state-of-the-art methods
reveal that Histo-Diffusion outperforms GANs. Our method offers a versatile
solution for histopathology image super-resolution, capable of handling
multi-resolution generation from varied input sizes, providing valuable support
in diagnostic processes.",2024-08-27 17:31:00+00:00,"['Xuan Xu', 'Saarthak Kapse', 'Prateek Prasanna']",http://arxiv.org/abs/2408.15218v1
GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru,"TikTok is one of the largest and fastest-growing social media sites in the
world. TikTok features, however, such as voice transcripts, are often missing
and other important features, such as OCR or video descriptions, do not exist.
We introduce the Generative AI Enriched TikTok (GET-Tok) data, a pipeline for
collecting TikTok videos and enriched data by augmenting the TikTok Research
API with generative AI models. As a case study, we collect videos about the
attempted coup in Peru initiated by its former President, Pedro Castillo, and
its accompanying protests. The data includes information on 43,697 videos
published from November 20, 2022 to March 1, 2023 (102 days). Generative AI
augments the collected data via transcripts of TikTok videos, text descriptions
of what is shown in the videos, what text is displayed within the video, and
the stances expressed in the video. Overall, this pipeline will contribute to a
better understanding of online discussion in a multimodal setting with
applications of Generative AI, especially outlining the utility of this
pipeline in non-English-language social media. Our code used to produce the
pipeline is in a public Github repository:
https://github.com/gabbypinto/GET-Tok-Peru.",2024-02-08 18:16:47+00:00,"['Gabriela Pinto', 'Keith Burghardt', 'Kristina Lerman', 'Emilio Ferrara']",http://arxiv.org/abs/2402.05882v1
GVT2RPM: An Empirical Study for General Video Transformer Adaptation to Remote Physiological Measurement,"Remote physiological measurement (RPM) is an essential tool for healthcare
monitoring as it enables the measurement of physiological signs, e.g., heart
rate, in a remote setting via physical wearables. Recently, with facial videos,
we have seen rapid advancements in video-based RPMs. However, adopting facial
videos for RPM in the clinical setting largely depends on the accuracy and
robustness (work across patient populations). Fortunately, the capability of
the state-of-the-art transformer architecture in general (natural) video
understanding has resulted in marked improvements and has been translated to
facial understanding, including RPM. However, existing RPM methods usually need
RPM-specific modules, e.g., temporal difference convolution and handcrafted
feature maps. Although these customized modules can increase accuracy, they are
not demonstrated for their robustness across datasets. Further, due to their
customization of the transformer architecture, they cannot use the advancements
made in general video transformers (GVT). In this study, we interrogate the GVT
architecture and empirically analyze how the training designs, i.e., data
pre-processing and network configurations, affect the model performance applied
to RPM. Based on the structure of video transformers, we propose to configure
its spatiotemporal hierarchy to align with the dense temporal information
needed in RPM for signal feature extraction. We define several practical
guidelines and gradually adapt GVTs for RPM without introducing RPM-specific
modules. Our experiments demonstrate favorable results to existing RPM-specific
module counterparts. We conducted extensive experiments with five datasets
using intra-dataset and cross-dataset settings. We highlight that the proposed
guidelines GVT2RPM can be generalized to any video transformers and is robust
to various datasets.",2024-06-19 00:59:27+00:00,"['Hao Wang', 'Euijoon Ahn', 'Jinman Kim']",http://arxiv.org/abs/2406.13136v1
VCoME: Verbal Video Composition with Multimodal Editing Effects,"Verbal videos, featuring voice-overs or text overlays, provide valuable
content but present significant challenges in composition, especially when
incorporating editing effects to enhance clarity and visual appeal. In this
paper, we introduce the novel task of verbal video composition with editing
effects. This task aims to generate coherent and visually appealing verbal
videos by integrating multimodal editing effects across textual, visual, and
audio categories. To achieve this, we curate a large-scale dataset of video
effects compositions from publicly available sources. We then formulate this
task as a generative problem, involving the identification of appropriate
positions in the verbal content and the recommendation of editing effects for
these positions. To address this task, we propose VCoME, a general framework
that employs a large multimodal model to generate editing effects for video
composition. Specifically, VCoME takes in the multimodal video context and
autoregressively outputs where to apply effects within the verbal content and
which effects are most appropriate for each position. VCoME also supports
prompt-based control of composition density and style, providing substantial
flexibility for diverse applications. Through extensive quantitative and
qualitative evaluations, we clearly demonstrate the effectiveness of VCoME. A
comprehensive user study shows that our method produces videos of professional
quality while being 85$\times$ more efficient than professional editors.",2024-07-05 17:59:02+00:00,"['Weibo Gong', 'Xiaojie Jin', 'Xin Li', 'Dongliang He', 'Xinglong Wu']",http://arxiv.org/abs/2407.04697v1
"Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition","Open-world video recognition is challenging since traditional networks are
not generalized well on complex environment variations. Alternatively,
foundation models with rich knowledge have recently shown their generalization
power. However, how to apply such knowledge has not been fully explored for
open-world video recognition. To this end, we propose a generic knowledge
transfer pipeline, which progressively exploits and integrates external
multimodal knowledge from foundation models to boost open-world video
recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt.
First, we perform Percept process to reduce the video domain gap and obtain
external visual knowledge. Second, we generate rich linguistic semantics as
external textual knowledge in Chat stage. Finally, we blend external multimodal
knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules
into networks. We conduct extensive experiments on three challenging open-world
video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves
state-of-the-art performance on all three datasets.",2024-02-29 08:29:03+00:00,"['Boyu Chen', 'Siran Chen', 'Kunchang Li', 'Qinglin Xu', 'Yu Qiao', 'Yali Wang']",http://arxiv.org/abs/2402.18951v1
General surgery vision transformer: A video pre-trained foundation model for general surgery,"The absence of openly accessible data and specialized foundation models is a
major barrier for computational research in surgery. Toward this, (i) we
open-source the largest dataset of general surgery videos to-date, consisting
of 680 hours of surgical videos, including data from robotic and laparoscopic
techniques across 28 procedures; (ii) we propose a technique for video
pre-training a general surgery vision transformer (GSViT) on surgical videos
based on forward video prediction that can run in real-time for surgical
applications, toward which we open-source the code and weights of GSViT; (iii)
we also release code and weights for procedure-specific fine-tuned versions of
GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the
Cholec80 phase annotation task, displaying improved performance over
state-of-the-art single frame predictors.",2024-03-09 16:02:46+00:00,"['Samuel Schmidgall', 'Ji Woong Kim', 'Jeffrey Jopling', 'Axel Krieger']",http://arxiv.org/abs/2403.05949v3
AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM,"The rapid advancement of large multimodal models (LMMs) has led to the rapid
expansion of artificial intelligence generated videos (AIGVs), which highlights
the pressing need for effective video quality assessment (VQA) models designed
specifically for AIGVs. Current VQA models generally fall short in accurately
assessing the perceptual quality of AIGVs due to the presence of unique
distortions, such as unrealistic objects, unnatural movements, or inconsistent
visual elements. To address this challenge, we first present AIGVQA-DB, a
large-scale dataset comprising 36,576 AIGVs generated by 15 advanced
text-to-video models using 1,048 diverse prompts. With these AIGVs, a
systematic annotation pipeline including scoring and ranking processes is
devised, which collects 370k expert ratings to date. Based on AIGVQA-DB, we
further introduce AIGV-Assessor, a novel VQA model that leverages
spatiotemporal features and LMM frameworks to capture the intricate quality
attributes of AIGVs, thereby accurately predicting precise video quality scores
and video pair preferences. Through comprehensive experiments on both AIGVQA-DB
and existing AIGV databases, AIGV-Assessor demonstrates state-of-the-art
performance, significantly surpassing existing scoring or evaluation methods in
terms of multiple perceptual quality dimensions.",2024-11-26 08:43:15+00:00,"['Jiarui Wang', 'Huiyu Duan', 'Guangtao Zhai', 'Juntong Wang', 'Xiongkuo Min']",http://arxiv.org/abs/2411.17221v1
"Unveiling Deep Shadows: A Survey and Benchmark on Image and Video Shadow Detection, Removal, and Generation in the Deep Learning Era","Shadows are created when light encounters obstacles, resulting in regions of
reduced illumination. In computer vision, detecting, removing, and generating
shadows are critical tasks for improving scene understanding, enhancing image
quality, ensuring visual consistency in video editing, and optimizing virtual
environments. This paper offers a comprehensive survey and evaluation benchmark
on shadow detection, removal, and generation in both images and videos,
focusing on the deep learning approaches of the past decade. It covers key
aspects such as tasks, deep models, datasets, evaluation metrics, and
comparative results under consistent experimental settings. Our main
contributions include a thorough survey of shadow analysis, the standardization
of experimental comparisons, an exploration of the relationships between model
size, speed, and performance, a cross-dataset generalization study, the
identification of open challenges and future research directions, and the
provision of publicly available resources to support further research in this
field.",2024-09-03 17:59:05+00:00,"['Xiaowei Hu', 'Zhenghao Xing', 'Tianyu Wang', 'Chi-Wing Fu', 'Pheng-Ann Heng']",http://arxiv.org/abs/2409.02108v2
Generalized Deepfake Attribution,"The landscape of fake media creation changed with the introduction of
Generative Adversarial Networks (GAN s). Fake media creation has been on the
rise with the rapid advances in generation technology, leading to new
challenges in Detecting fake media. A fundamental characteristic of GAN s is
their sensitivity to parameter initialization, known as seeds. Each distinct
seed utilized during training leads to the creation of unique model instances,
resulting in divergent image outputs despite employing the same architecture.
This means that even if we have one GAN architecture, it can produce countless
variations of GAN models depending on the seed used. Existing methods for
attributing deepfakes work well only if they have seen the specific GAN model
during training. If the GAN architectures are retrained with a different seed,
these methods struggle to attribute the fakes. This seed dependency issue made
it difficult to attribute deepfakes with existing methods. We proposed a
generalized deepfake attribution network (GDA-N et) to attribute fake images to
their respective GAN architectures, even if they are generated from a retrained
version of the GAN architecture with a different seed (cross-seed) or from the
fine-tuned version of the existing GAN model. Extensive experiments on
cross-seed and fine-tuned data of GAN models show that our method is highly
effective compared to existing methods. We have provided the source code to
validate our results.",2024-06-26 12:04:09+00:00,"['Sowdagar Mahammad Shahid', 'Sudev Kumar Padhi', 'Umesh Kashyap', 'Sk. Subidh Ali']",http://arxiv.org/abs/2406.18278v1
Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative Framework,"Text-to-image (T2I) diffusion models are popular for introducing image
manipulation methods, such as editing, image fusion, inpainting, etc. At the
same time, image-to-video (I2V) and text-to-video (T2V) models are also built
on top of T2I models. We present Kandinsky 3, a novel T2I model based on latent
diffusion, achieving a high level of quality and photorealism. The key feature
of the new architecture is the simplicity and efficiency of its adaptation for
many types of generation tasks. We extend the base T2I model for various
applications and create a multifunctional generation system that includes
text-guided inpainting/outpainting, image fusion, text-image fusion, image
variations generation, I2V and T2V generation. We also present a distilled
version of the T2I model, evaluating inference in 4 steps of the reverse
process without reducing image quality and 3 times faster than the base model.
We deployed a user-friendly demo system in which all the features can be tested
in the public domain. Additionally, we released the source code and checkpoints
for the Kandinsky 3 and extended models. Human evaluations show that Kandinsky
3 demonstrates one of the highest quality scores among open source generation
systems.",2024-10-28 14:22:08+00:00,"['Vladimir Arkhipkin', 'Viacheslav Vasilev', 'Andrei Filatov', 'Igor Pavlov', 'Julia Agafonova', 'Nikolai Gerasimenko', 'Anna Averchenkova', 'Evelina Mironova', 'Anton Bukashkin', 'Konstantin Kulikov', 'Andrey Kuznetsov', 'Denis Dimitrov']",http://arxiv.org/abs/2410.21061v1
Mitigating analytical variability in fMRI results with style transfer,"We propose a novel approach to improve the reproducibility of neuroimaging
results by converting statistic maps across different functional MRI pipelines.
We make the assumption that pipelines used to compute fMRI statistic maps can
be considered as a style component and we propose to use different generative
models, among which, Generative Adversarial Networks (GAN) and Diffusion Models
(DM) to convert statistic maps across different pipelines. We explore the
performance of multiple GAN frameworks, and design a new DM framework for
unsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI
statistic maps using the latent space of an auxiliary classifier that
distinguishes statistic maps from different pipelines and extend traditional
sampling techniques used in DM to improve the transition performance. Our
experiments demonstrate that our proposed methods aresuccessful: pipelines can
indeed be transferred as a style component, providing animportant source of
data augmentation for future medical studies.",2024-04-04 07:49:39+00:00,"['Elodie Germani', 'Camille Maumet', 'Elisa Fromont']",http://arxiv.org/abs/2404.03703v3
EMAG: Ego-motion Aware and Generalizable 2D Hand Forecasting from Egocentric Videos,"Predicting future human behavior from egocentric videos is a challenging but
critical task for human intention understanding. Existing methods for
forecasting 2D hand positions rely on visual representations and mainly focus
on hand-object interactions. In this paper, we investigate the hand forecasting
task and tackle two significant issues that persist in the existing methods:
(1) 2D hand positions in future frames are severely affected by ego-motions in
egocentric videos; (2) prediction based on visual information tends to overfit
to background or scene textures, posing a challenge for generalization on novel
scenes or human behaviors. To solve the aforementioned problems, we propose
EMAG, an ego-motion-aware and generalizable 2D hand forecasting method. In
response to the first problem, we propose a method that considers ego-motion,
represented by a sequence of homography matrices of two consecutive frames. We
further leverage modalities such as optical flow, trajectories of hands and
interacting objects, and ego-motions, thereby alleviating the second issue.
Extensive experiments on two large-scale egocentric video datasets, Ego4D and
EPIC-Kitchens 55, verify the effectiveness of the proposed method. In
particular, our model outperforms prior methods by 1.7% and 7.0% on intra and
cross-dataset evaluations, respectively. Project page:
https://masashi-hatano.github.io/EMAG/",2024-05-30 13:15:18+00:00,"['Masashi Hatano', 'Ryo Hachiuma', 'Hideo Saito']",http://arxiv.org/abs/2405.20030v2
GUI Action Narrator: Where and When Did That Action Take Place?,"The advent of Multimodal LLMs has significantly enhanced image OCR
recognition capabilities, making GUI automation a viable reality for increasing
efficiency in digital tasks. One fundamental aspect of developing a GUI
automation system is understanding primitive GUI actions. This comprehension is
crucial as it enables agents to learn from user demonstrations, an essential
element of automation. To rigorously evaluate such capabilities, we developed a
video captioning benchmark for GUI actions, comprising 4,189 diverse video
captioning samples. This task presents unique challenges compared to natural
scene video captioning: 1) GUI screenshots typically contain denser information
than natural scenes, and 2) events within GUIs are subtler and occur more
rapidly, requiring precise attention to the appropriate time span and spatial
region for accurate understanding. To address these challenges, we introduce
our GUI action dataset \textbf{Act2Cap} as well as a simple yet effective
framework, \textbf{GUI Narrator}, for GUI video captioning that utilizes the
cursor as a visual prompt to enhance the interpretation of high-resolution
screenshots. Specifically, a cursor detector is trained on our dataset, and a
multimodal LLM model with mechanisms for selecting keyframes and key regions
generates the captions. Experimental results indicate that even for today's
most advanced multimodal models, such as GPT-4o, the task remains highly
challenging. Additionally, our evaluations show that our strategy effectively
enhances model performance, whether integrated into the fine-tuning of
open-source models or employed as a prompting strategy in closed-source models.",2024-06-19 17:22:11+00:00,"['Qinchen Wu', 'Difei Gao', 'Kevin Qinghong Lin', 'Zhuoyu Wu', 'Xiangwu Guo', 'Peiran Li', 'Weichen Zhang', 'Hengxu Wang', 'Mike Zheng Shou']",http://arxiv.org/abs/2406.13719v1
DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video,"Recent advancements in dynamic neural radiance field methods have yielded
remarkable outcomes. However, these approaches rely on the assumption of sharp
input images. When faced with motion blur, existing dynamic NeRF methods often
struggle to generate high-quality novel views. In this paper, we propose
DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views
from a monocular video affected by motion blur. To account for motion blur in
input images, we simultaneously capture the camera trajectory and object
Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we
employ a global cross-time rendering approach to ensure consistent temporal
coherence across the entire scene. We curate a dataset comprising diverse
dynamic scenes that are specifically tailored for our task. Experimental
results on our dataset demonstrate that our method outperforms existing
approaches in generating sharp novel views from motion-blurred inputs while
maintaining spatial-temporal consistency of the scene.",2024-03-15 08:48:37+00:00,"['Huiqiang Sun', 'Xingyi Li', 'Liao Shen', 'Xinyi Ye', 'Ke Xian', 'Zhiguo Cao']",http://arxiv.org/abs/2403.10103v2
GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis,"We present GaSpCT, a novel view synthesis and 3D scene representation method
used to generate novel projection views for Computer Tomography (CT) scans. We
adapt the Gaussian Splatting framework to enable novel view synthesis in CT
based on limited sets of 2D image projections and without the need for
Structure from Motion (SfM) methodologies. Therefore, we reduce the total
scanning duration and the amount of radiation dose the patient receives during
the scan. We adapted the loss function to our use-case by encouraging a
stronger background and foreground distinction using two sparsity promoting
regularizers: a beta loss and a total variation (TV) loss. Finally, we
initialize the Gaussian locations across the 3D space using a uniform prior
distribution of where the brain's positioning would be expected to be within
the field of view. We evaluate the performance of our model using brain CT
scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and
demonstrate that the rendered novel views closely match the original projection
views of the simulated scan, and have better performance than other implicit 3D
scene representations methodologies. Furthermore, we empirically observe
reduced training time compared to neural network based image synthesis for
sparse-view CT image reconstruction. Finally, the memory requirements of the
Gaussian Splatting representations are reduced by 17% compared to the
equivalent voxel grid image representations.",2024-04-04 00:28:50+00:00,"['Emmanouil Nikolakakis', 'Utkarsh Gupta', 'Jonathan Vengosh', 'Justin Bui', 'Razvan Marinescu']",http://arxiv.org/abs/2404.03126v1
T-FOLEY: A Controllable Waveform-Domain Diffusion Model for Temporal-Event-Guided Foley Sound Synthesis,"Foley sound, audio content inserted synchronously with videos, plays a
critical role in the user experience of multimedia content. Recently, there has
been active research in Foley sound synthesis, leveraging the advancements in
deep generative models. However, such works mainly focus on replicating a
single sound class or a textual sound description, neglecting temporal
information, which is crucial in the practical applications of Foley sound. We
present T-Foley, a Temporal-event-guided waveform generation model for Foley
sound synthesis. T-Foley generates high-quality audio using two conditions: the
sound class and temporal event feature. For temporal conditioning, we devise a
temporal event feature and a novel conditioning technique named Block-FiLM.
T-Foley achieves superior performance in both objective and subjective
evaluation metrics and generates Foley sound well-synchronized with the
temporal events. Additionally, we showcase T-Foley's practical applications,
particularly in scenarios involving vocal mimicry for temporal event control.
We show the demo on our companion website.",2024-01-17 15:54:36+00:00,"['Yoonjin Chung', 'Junwon Lee', 'Juhan Nam']",http://arxiv.org/abs/2401.09294v1
Motion Dreamer: Boundary Conditional Motion Reasoning for Physically Coherent Video Generation,"Recent advances in video generation have shown promise for generating future
scenarios, critical for planning and control in autonomous driving and embodied
intelligence. However, real-world applications demand more than visually
plausible predictions; they require reasoning about object motions based on
explicitly defined boundary conditions, such as initial scene image and partial
object motion. We term this capability Boundary Conditional Motion Reasoning.
Current approaches either neglect explicit user-defined motion constraints,
producing physically inconsistent motions, or conversely demand complete motion
inputs, which are rarely available in practice. Here we introduce Motion
Dreamer, a two-stage framework that explicitly separates motion reasoning from
visual synthesis, addressing these limitations. Our approach introduces
instance flow, a sparse-to-dense motion representation enabling effective
integration of partial user-defined motions, and the motion inpainting strategy
to robustly enable reasoning motions of other objects. Extensive experiments
demonstrate that Motion Dreamer significantly outperforms existing methods,
achieving superior motion plausibility and visual realism, thus bridging the
gap towards practical boundary conditional motion reasoning. Our webpage is
available: https://envision-research.github.io/MotionDreamer/.",2024-11-30 17:40:49+00:00,"['Tianshuo Xu', 'Zhifei Chen', 'Leyi Wu', 'Hao Lu', 'Yuying Chen', 'Lihui Jiang', 'Bingbing Liu', 'Yingcong Chen']",http://arxiv.org/abs/2412.00547v3
Lifelong Learning Using a Dynamically Growing Tree of Sub-networks for Domain Generalization in Video Object Segmentation,"Current state-of-the-art video object segmentation models have achieved great
success using supervised learning with massive labeled training datasets.
However, these models are trained using a single source domain and evaluated
using videos sampled from the same source domain. When these models are
evaluated using videos sampled from a different target domain, their
performance degrades significantly due to poor domain generalization, i.e.,
their inability to learn from multi-domain sources simultaneously using
traditional supervised learning. In this paper, We propose a dynamically
growing tree of sub-networks (DGT) to learn effectively from multi-domain
sources. DGT uses a novel lifelong learning technique that allows the model to
continuously and effectively learn from new domains without forgetting the
previously learned domains. Hence, the model can generalize to out-of-domain
videos. The proposed work is evaluated using single-source in-domain
(traditional video object segmentation), multi-source in-domain, and
multi-source out-of-domain video object segmentation. The results of DGT show a
single source in-domain performance gain of 0.2% and 3.5% on the DAVIS16 and
DAVIS17 datasets, respectively. However, when DGT is evaluated using in-domain
multi-sources, the results show superior performance compared to
state-of-the-art video object segmentation and other lifelong learning
techniques with an average performance increase in the F-score of 6.9% with
minimal catastrophic forgetting. Finally, in the out-of-domain experiment, the
performance of DGT is 2.7% and 4% better than state-of-the-art in 1 and
5-shots, respectively.",2024-05-29 21:01:27+00:00,"['Islam Osman', 'Mohamed S. Shehata']",http://arxiv.org/abs/2405.19525v1
EVC-MF: End-to-end Video Captioning Network with Multi-scale Features,"Conventional approaches for video captioning leverage a variety of
offline-extracted features to generate captions. Despite the availability of
various offline-feature-extractors that offer diverse information from
different perspectives, they have several limitations due to fixed parameters.
Concretely, these extractors are solely pre-trained on image/video
comprehension tasks, making them less adaptable to video caption datasets.
Additionally, most of these extractors only capture features prior to the
classifier of the pre-training task, ignoring a significant amount of valuable
shallow information. Furthermore, employing multiple offline-features may
introduce redundant information. To address these issues, we propose an
end-to-end encoder-decoder-based network (EVC-MF) for video captioning, which
efficiently utilizes multi-scale visual and textual features to generate video
descriptions. Specifically, EVC-MF consists of three modules. Firstly, instead
of relying on multiple feature extractors, we directly feed video frames into a
transformer-based network to obtain multi-scale visual features and update
feature extractor parameters. Secondly, we fuse the multi-scale features and
input them into a masked encoder to reduce redundancy and encourage learning
useful features. Finally, we utilize an enhanced transformer-based decoder,
which can efficiently leverage shallow textual information, to generate video
descriptions. To evaluate our proposed model, we conduct extensive experiments
on benchmark datasets. The results demonstrate that EVC-MF yields competitive
performance compared with the state-of-theart methods.",2024-10-22 02:16:02+00:00,"['Tian-Zi Niu', 'Zhen-Duo Chen', 'Xin Luo', 'Xin-Shun Xu']",http://arxiv.org/abs/2410.16624v1
ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models,"Recent advances in Large Multimodal Models (LMMs) have expanded their
capabilities to video understanding, with Text-to-Video (T2V) models excelling
in generating videos from textual prompts. However, they still frequently
produce hallucinated content, revealing AI-generated inconsistencies. We
introduce ViBe (https://vibe-t2v-bench.github.io/): a large-scale dataset of
hallucinated videos from open-source T2V models. We identify five major
hallucination types: Vanishing Subject, Omission Error, Numeric Variability,
Subject Dysmorphia, and Visual Incongruity. Using ten T2V models, we generated
and manually annotated 3,782 videos from 837 diverse MS COCO captions. Our
proposed benchmark includes a dataset of hallucinated videos and a
classification framework using video embeddings. ViBe serves as a critical
resource for evaluating T2V reliability and advancing hallucination detection.
We establish classification as a baseline, with the TimeSFormer + CNN ensemble
achieving the best performance (0.345 accuracy, 0.342 F1 score). While initial
baselines proposed achieve modest accuracy, this highlights the difficulty of
automated hallucination detection and the need for improved methods. Our
research aims to drive the development of more robust T2V models and evaluate
their outputs based on user preferences.",2024-11-16 19:23:12+00:00,"['Vipula Rawte', 'Sarthak Jain', 'Aarush Sinha', 'Garv Kaushik', 'Aman Bansal', 'Prathiksha Rumale Vishwanath', 'Samyak Rajesh Jain', 'Aishwarya Naresh Reganti', 'Vinija Jain', 'Aman Chadha', 'Amit P. Sheth', 'Amitava Das']",http://arxiv.org/abs/2411.10867v2
VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling,"In this work, we systematically study music generation conditioned solely on
the video. First, we present a large-scale dataset comprising 360K video-music
pairs, including various genres such as movie trailers, advertisements, and
documentaries. Furthermore, we propose VidMuse, a simple framework for
generating music aligned with video inputs. VidMuse stands out by producing
high-fidelity music that is both acoustically and semantically aligned with the
video. By incorporating local and global visual cues, VidMuse enables the
creation of musically coherent audio tracks that consistently match the video
content through Long-Short-Term modeling. Through extensive experiments,
VidMuse outperforms existing models in terms of audio quality, diversity, and
audio-visual alignment. The code and datasets will be available at
https://github.com/ZeyueT/VidMuse/.",2024-06-06 17:58:11+00:00,"['Zeyue Tian', 'Zhaoyang Liu', 'Ruibin Yuan', 'Jiahao Pan', 'Qifeng Liu', 'Xu Tan', 'Qifeng Chen', 'Wei Xue', 'Yike Guo']",http://arxiv.org/abs/2406.04321v2
Synthetic Human Memories: AI-Edited Images and Videos Can Implant False Memories and Distort Recollection,"AI is increasingly used to enhance images and videos, both intentionally and
unintentionally. As AI editing tools become more integrated into smartphones,
users can modify or animate photos into realistic videos. This study examines
the impact of AI-altered visuals on false memories--recollections of events
that didn't occur or deviate from reality. In a pre-registered study, 200
participants were divided into four conditions of 50 each. Participants viewed
original images, completed a filler task, then saw stimuli corresponding to
their assigned condition: unedited images, AI-edited images, AI-generated
videos, or AI-generated videos of AI-edited images. AI-edited visuals
significantly increased false recollections, with AI-generated videos of
AI-edited images having the strongest effect (2.05x compared to control).
Confidence in false memories was also highest for this condition (1.19x
compared to control). We discuss potential applications in HCI, such as
therapeutic memory reframing, and challenges in ethical, legal, political, and
societal domains.",2024-09-13 15:08:39+00:00,"['Pat Pataranutaporn', 'Chayapatr Archiwaranguprok', 'Samantha W. T. Chan', 'Elizabeth Loftus', 'Pattie Maes']",http://arxiv.org/abs/2409.08895v1
Temporal2Seq: A Unified Framework for Temporal Video Understanding Tasks,"With the development of video understanding, there is a proliferation of
tasks for clip-level temporal video analysis, including temporal action
detection (TAD), temporal action segmentation (TAS), and generic event boundary
detection (GEBD). While task-specific video understanding models have exhibited
outstanding performance in each task, there remains a dearth of a unified
framework capable of simultaneously addressing multiple tasks, which is a
promising direction for the next generation of AI. To this end, in this paper,
we propose a single unified framework, coined as Temporal2Seq, to formulate the
output of these temporal video understanding tasks as a sequence of discrete
tokens. With this unified token representation, Temporal2Seq can train a
generalist model within a single architecture on different video understanding
tasks. In the absence of multi-task learning (MTL) benchmarks, we compile a
comprehensive co-training dataset by borrowing the datasets from TAD, TAS, and
GEBD tasks. We evaluate our Temporal2Seq generalist model on the corresponding
test sets of three tasks, demonstrating that Temporal2Seq can produce
reasonable results on various tasks and achieve advantages compared with
single-task training on this framework. We also investigate the generalization
performance of our generalist model on new datasets from different tasks, which
yields superior performance to the specific model.",2024-09-27 06:37:47+00:00,"['Min Yang', 'Zichen Zhang', 'Limin Wang']",http://arxiv.org/abs/2409.18478v1
AnimateAnything: Consistent and Controllable Animation for Video Generation,"We present a unified controllable video generation approach AnimateAnything
that facilitates precise and consistent video manipulation across various
conditions, including camera trajectories, text prompts, and user motion
annotations. Specifically, we carefully design a multi-scale control feature
fusion network to construct a common motion representation for different
conditions. It explicitly converts all control information into frame-by-frame
optical flows. Then we incorporate the optical flows as motion priors to guide
final video generation. In addition, to reduce the flickering issues caused by
large-scale motion, we propose a frequency-based stabilization module. It can
enhance temporal coherence by ensuring the video's frequency domain
consistency. Experiments demonstrate that our method outperforms the
state-of-the-art approaches. For more details and videos, please refer to the
webpage: https://yu-shaonian.github.io/Animate_Anything/.",2024-11-16 16:36:49+00:00,"['Guojun Lei', 'Chi Wang', 'Hong Li', 'Rong Zhang', 'Yikai Wang', 'Weiwei Xu']",http://arxiv.org/abs/2411.10836v1
PTQ4ADM: Post-Training Quantization for Efficient Text Conditional Audio Diffusion Models,"Denoising diffusion models have emerged as state-of-the-art in generative
tasks across image, audio, and video domains, producing high-quality, diverse,
and contextually relevant data. However, their broader adoption is limited by
high computational costs and large memory footprints. Post-training
quantization (PTQ) offers a promising approach to mitigate these challenges by
reducing model complexity through low-bandwidth parameters. Yet, direct
application of PTQ to diffusion models can degrade synthesis quality due to
accumulated quantization noise across multiple denoising steps, particularly in
conditional tasks like text-to-audio synthesis. This work introduces PTQ4ADM, a
novel framework for quantizing audio diffusion models(ADMs). Our key
contributions include (1) a coverage-driven prompt augmentation method and (2)
an activation-aware calibration set generation algorithm for text-conditional
ADMs. These techniques ensure comprehensive coverage of audio aspects and
modalities while preserving synthesis fidelity. We validate our approach on
TANGO, Make-An-Audio, and AudioLDM models for text-conditional audio
generation. Extensive experiments demonstrate PTQ4ADM's capability to reduce
the model size by up to 70\% while achieving synthesis quality metrics
comparable to full-precision models($<$5\% increase in FD scores). We show that
specific layers in the backbone network can be quantized to 4-bit weights and
8-bit activations without significant quality loss. This work paves the way for
more efficient deployment of ADMs in resource-constrained environments.",2024-09-20 20:52:56+00:00,"['Jayneel Vora', 'Aditya Krishnan', 'Nader Bouacida', 'Prabhu RV Shankar', 'Prasant Mohapatra']",http://arxiv.org/abs/2409.13894v1
Controllable Talking Face Generation by Implicit Facial Keypoints Editing,"Audio-driven talking face generation has garnered significant interest within
the domain of digital human research. Existing methods are encumbered by
intricate model architectures that are intricately dependent on each other,
complicating the process of re-editing image or video inputs. In this work, we
present ControlTalk, a talking face generation method to control face
expression deformation based on driven audio, which can construct the head pose
and facial expression including lip motion for both single image or sequential
video inputs in a unified manner. By utilizing a pre-trained video synthesis
renderer and proposing the lightweight adaptation, ControlTalk achieves precise
and naturalistic lip synchronization while enabling quantitative control over
mouth opening shape. Our experiments show that our method is superior to
state-of-the-art performance on widely used benchmarks, including HDTF and
MEAD. The parameterized adaptation demonstrates remarkable generalization
capabilities, effectively handling expression deformation across same-ID and
cross-ID scenarios, and extending its utility to out-of-domain portraits,
regardless of languages. Code is available at
https://github.com/NetEase-Media/ControlTalk.",2024-06-05 02:54:46+00:00,"['Dong Zhao', 'Jiaying Shi', 'Wenjun Li', 'Shudong Wang', 'Shenghui Xu', 'Zhaoming Pan']",http://arxiv.org/abs/2406.02880v2
Style-Preserving Lip Sync via Audio-Aware Style Reference,"Audio-driven lip sync has recently drawn significant attention due to its
widespread application in the multimedia domain. Individuals exhibit distinct
lip shapes when speaking the same utterance, attributed to the unique speaking
styles of individuals, posing a notable challenge for audio-driven lip sync.
Earlier methods for such task often bypassed the modeling of personalized
speaking styles, resulting in sub-optimal lip sync conforming to the general
styles. Recent lip sync techniques attempt to guide the lip sync for arbitrary
audio by aggregating information from a style reference video, yet they can not
preserve the speaking styles well due to their inaccuracy in style aggregation.
This work proposes an innovative audio-aware style reference scheme that
effectively leverages the relationships between input audio and reference audio
from style reference video to address the style-preserving audio-driven lip
sync. Specifically, we first develop an advanced Transformer-based model adept
at predicting lip motion corresponding to the input audio, augmented by the
style information aggregated through cross-attention layers from style
reference video. Afterwards, to better render the lip motion into realistic
talking face video, we devise a conditional latent diffusion model, integrating
lip motion through modulated convolutional layers and fusing reference facial
images via spatial cross-attention layers. Extensive experiments validate the
efficacy of the proposed approach in achieving precise lip sync, preserving
speaking styles, and generating high-fidelity, realistic talking face videos.",2024-08-10 02:46:11+00:00,"['Weizhi Zhong', 'Jichang Li', 'Yinqi Cai', 'Liang Lin', 'Guanbin Li']",http://arxiv.org/abs/2408.05412v1
Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion,"Recent text-to-video diffusion models have achieved impressive progress. In
practice, users often desire the ability to control object motion and camera
movement independently for customized video creation. However, current methods
lack the focus on separately controlling object motion and camera movement in a
decoupled manner, which limits the controllability and flexibility of
text-to-video models. In this paper, we introduce Direct-a-Video, a system that
allows users to independently specify motions for multiple objects as well as
camera's pan and zoom movements, as if directing a video. We propose a simple
yet effective strategy for the decoupled control of object motion and camera
movement. Object motion is controlled through spatial cross-attention
modulation using the model's inherent priors, requiring no additional
optimization. For camera movement, we introduce new temporal cross-attention
layers to interpret quantitative camera movement parameters. We further employ
an augmentation-based approach to train these layers in a self-supervised
manner on a small-scale dataset, eliminating the need for explicit motion
annotation. Both components operate independently, allowing individual or
combined control, and can generalize to open-domain scenarios. Extensive
experiments demonstrate the superiority and effectiveness of our method.
Project page and code are available at https://direct-a-video.github.io/.",2024-02-05 16:30:57+00:00,"['Shiyuan Yang', 'Liang Hou', 'Haibin Huang', 'Chongyang Ma', 'Pengfei Wan', 'Di Zhang', 'Xiaodong Chen', 'Jing Liao']",http://arxiv.org/abs/2402.03162v2
DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control,"Recent advances in customized video generation have enabled users to create
videos tailored to both specific subjects and motion trajectories. However,
existing methods often require complicated test-time fine-tuning and struggle
with balancing subject learning and motion control, limiting their real-world
applications. In this paper, we present DreamVideo-2, a zero-shot video
customization framework capable of generating videos with a specific subject
and motion trajectory, guided by a single image and a bounding box sequence,
respectively, and without the need for test-time fine-tuning. Specifically, we
introduce reference attention, which leverages the model's inherent
capabilities for subject learning, and devise a mask-guided motion module to
achieve precise motion control by fully utilizing the robust motion signal of
box masks derived from bounding boxes. While these two components achieve their
intended functions, we empirically observe that motion control tends to
dominate over subject learning. To address this, we propose two key designs: 1)
the masked reference attention, which integrates a blended latent mask modeling
scheme into reference attention to enhance subject representations at the
desired positions, and 2) a reweighted diffusion loss, which differentiates the
contributions of regions inside and outside the bounding boxes to ensure a
balance between subject and motion control. Extensive experimental results on a
newly curated dataset demonstrate that DreamVideo-2 outperforms
state-of-the-art methods in both subject customization and motion control. The
dataset, code, and models will be made publicly available.",2024-10-17 17:52:57+00:00,"['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Xiang Wang', 'Haonan Qiu', 'Rui Zhao', 'Yutong Feng', 'Feng Liu', 'Zhizhong Huang', 'Jiaxin Ye', 'Yingya Zhang', 'Hongming Shan']",http://arxiv.org/abs/2410.13830v1
3D MRI Synthesis with Slice-Based Latent Diffusion Models: Improving Tumor Segmentation Tasks in Data-Scarce Regimes,"Despite the increasing use of deep learning in medical image segmentation,
the limited availability of annotated training data remains a major challenge
due to the time-consuming data acquisition and privacy regulations. In the
context of segmentation tasks, providing both medical images and their
corresponding target masks is essential. However, conventional data
augmentation approaches mainly focus on image synthesis. In this study, we
propose a novel slice-based latent diffusion architecture designed to address
the complexities of volumetric data generation in a slice-by-slice fashion.
This approach extends the joint distribution modeling of medical images and
their associated masks, allowing a simultaneous generation of both under
data-scarce regimes. Our approach mitigates the computational complexity and
memory expensiveness typically associated with diffusion models. Furthermore,
our architecture can be conditioned by tumor characteristics, including size,
shape, and relative position, thereby providing a diverse range of tumor
variations. Experiments on a segmentation task using the BRATS2022 confirm the
effectiveness of the synthesized volumes and masks for data augmentation.",2024-06-08 09:53:45+00:00,"['Aghiles Kebaili', 'Jrme Lapuyade-Lahorgue', 'Pierre Vera', 'Su Ruan']",http://arxiv.org/abs/2406.05421v1
DiffSR: Learning Radar Reflectivity Synthesis via Diffusion Model from Satellite Observations,"Weather radar data synthesis can fill in data for areas where ground
observations are missing. Existing methods often employ reconstruction-based
approaches with MSE loss to reconstruct radar data from satellite observation.
However, such methods lead to over-smoothing, which hinders the generation of
high-frequency details or high-value observation areas associated with
convective weather. To address this issue, we propose a two-stage
diffusion-based method called DiffSR. We first pre-train a reconstruction model
on global-scale data to obtain radar estimation and then synthesize radar
reflectivity by combining radar estimation results with satellite data as
conditions for the diffusion model. Extensive experiments show that our method
achieves state-of-the-art (SOTA) results, demonstrating the ability to generate
high-frequency details and high-value areas.",2024-11-11 04:50:34+00:00,"['Xuming He', 'Zhiwang Zhou', 'Wenlong Zhang', 'Xiangyu Zhao', 'Hao Chen', 'Shiqi Chen', 'Lei Bai']",http://arxiv.org/abs/2411.06714v1
Multi-task SAR Image Processing via GAN-based Unsupervised Manipulation,"Generative Adversarial Networks (GANs) have shown tremendous potential in
synthesizing a large number of realistic SAR images by learning patterns in the
data distribution. Some GANs can achieve image editing by introducing latent
codes, demonstrating significant promise in SAR image processing. Compared to
traditional SAR image processing methods, editing based on GAN latent space
control is entirely unsupervised, allowing image processing to be conducted
without any labeled data. Additionally, the information extracted from the data
is more interpretable. This paper proposes a novel SAR image processing
framework called GAN-based Unsupervised Editing (GUE), aiming to address the
following two issues: (1) disentangling semantic directions in the GAN latent
space and finding meaningful directions; (2) establishing a comprehensive SAR
image processing framework while achieving multiple image processing functions.
In the implementation of GUE, we decompose the entangled semantic directions in
the GAN latent space by training a carefully designed network. Moreover, we can
accomplish multiple SAR image processing tasks (including despeckling,
localization, auxiliary identification, and rotation editing) in a single
training process without any form of supervision. Extensive experiments
validate the effectiveness of the proposed method.",2024-08-02 19:49:30+00:00,"['Xuran Hu', 'Mingzhe Zhu', 'Ziqiang Xu', 'Zhenpeng Feng', 'Ljubisa Stankovic']",http://arxiv.org/abs/2408.01553v1
GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition,"Current datasets for action recognition tasks face limitations stemming from
traditional collection and generation methods, including the constrained range
of action classes, absence of multi-viewpoint recordings, limited diversity,
poor video quality, and labor-intensive manually collection. To address these
challenges, we introduce GTAutoAct, a innovative dataset generation framework
leveraging game engine technology to facilitate advancements in action
recognition. GTAutoAct excels in automatically creating large-scale,
well-annotated datasets with extensive action classes and superior video
quality. Our framework's distinctive contributions encompass: (1) it
innovatively transforms readily available coordinate-based 3D human motion into
rotation-orientated representation with enhanced suitability in multiple
viewpoints; (2) it employs dynamic segmentation and interpolation of rotation
sequences to create smooth and realistic animations of action; (3) it offers
extensively customizable animation scenes; (4) it implements an autonomous
video capture and processing pipeline, featuring a randomly navigating camera,
with auto-trimming and labeling functionalities. Experimental results
underscore the framework's robustness and highlights its potential to
significantly improve action recognition model training.",2024-01-24 12:18:31+00:00,"['Xingyu Song', 'Zhan Li', 'Shi Chen', 'Kazuyuki Demachi']",http://arxiv.org/abs/2401.13414v1
TC-PDM: Temporally Consistent Patch Diffusion Models for Infrared-to-Visible Video Translation,"Infrared imaging offers resilience against changing lighting conditions by
capturing object temperatures. Yet, in few scenarios, its lack of visual
details compared to daytime visible images, poses a significant challenge for
human and machine interpretation. This paper proposes a novel diffusion method,
dubbed Temporally Consistent Patch Diffusion Models (TC-DPM), for
infrared-to-visible video translation. Our method, extending the Patch
Diffusion Model, consists of two key components. Firstly, we propose a
semantic-guided denoising, leveraging the strong representations of
foundational models. As such, our method faithfully preserves the semantic
structure of generated visible images. Secondly, we propose a novel temporal
blending module to guide the denoising trajectory, ensuring the temporal
consistency between consecutive frames. Experiment shows that TC-PDM
outperforms state-of-the-art methods by 35.3% in FVD for infrared-to-visible
video translation and by 6.1% in AP50 for day-to-night object detection. Our
code is publicly available at https://github.com/dzungdoan6/tc-pdm",2024-08-26 12:43:48+00:00,"['Anh-Dzung Doan', 'Vu Minh Hieu Phan', 'Surabhi Gupta', 'Markus Wagner', 'Tat-Jun Chin', 'Ian Reid']",http://arxiv.org/abs/2408.14227v1
DiffSign: AI-Assisted Generation of Customizable Sign Language Videos With Enhanced Realism,"The proliferation of several streaming services in recent years has now made
it possible for a diverse audience across the world to view the same media
content, such as movies or TV shows. While translation and dubbing services are
being added to make content accessible to the local audience, the support for
making content accessible to people with different abilities, such as the Deaf
and Hard of Hearing (DHH) community, is still lagging. Our goal is to make
media content more accessible to the DHH community by generating sign language
videos with synthetic signers that are realistic and expressive. Using the same
signer for a given media content that is viewed globally may have limited
appeal. Hence, our approach combines parametric modeling and generative
modeling to generate realistic-looking synthetic signers and customize their
appearance based on user preferences. We first retarget human sign language
poses to 3D sign language avatars by optimizing a parametric model. The
high-fidelity poses from the rendered avatars are then used to condition the
poses of synthetic signers generated using a diffusion-based generative model.
The appearance of the synthetic signer is controlled by an image prompt
supplied through a visual adapter. Our results show that the sign language
videos generated using our approach have better temporal consistency and
realism than signing videos generated by a diffusion model conditioned only on
text prompts. We also support multimodal prompts to allow users to further
customize the appearance of the signer to accommodate diversity (e.g. skin
tone, gender). Our approach is also useful for signer anonymization.",2024-12-05 05:18:28+00:00,"['Sudha Krishnamurthy', 'Vimal Bhat', 'Abhinav Jain']",http://arxiv.org/abs/2412.03878v1
Utilizing Generative Adversarial Networks for Image Data Augmentation and Classification of Semiconductor Wafer Dicing Induced Defects,"In semiconductor manufacturing, the wafer dicing process is central yet
vulnerable to defects that significantly impair yield - the proportion of
defect-free chips. Deep neural networks are the current state of the art in
(semi-)automated visual inspection. However, they are notoriously known to
require a particularly large amount of data for model training. To address
these challenges, we explore the application of generative adversarial networks
(GAN) for image data augmentation and classification of semiconductor wafer
dicing induced defects to enhance the variety and balance of training data for
visual inspection systems. With this approach, synthetic yet realistic images
are generated that mimic real-world dicing defects. We employ three different
GAN variants for high-resolution image synthesis: Deep Convolutional GAN
(DCGAN), CycleGAN, and StyleGAN3. Our work-in-progress results demonstrate that
improved classification accuracies can be obtained, showing an average
improvement of up to 23.1 % from 65.1 % (baseline experiment) to 88.2 % (DCGAN
experiment) in balanced accuracy, which may enable yield optimization in
production.",2024-07-24 20:44:16+00:00,"['Zhining Hu', 'Tobias Schlosser', 'Michael Friedrich', 'Andr Luiz Vieira e Silva', 'Frederik Beuth', 'Danny Kowerko']",http://arxiv.org/abs/2407.20268v1
MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and 3D Editing,"Novel View Synthesis (NVS) and 3D generation have recently achieved prominent
improvements. However, these works mainly focus on confined categories or
synthetic 3D assets, which are discouraged from generalizing to challenging
in-the-wild scenes and fail to be employed with 2D synthesis directly.
Moreover, these methods heavily depended on camera poses, limiting their
real-world applications. To overcome these issues, we propose MVInpainter,
re-formulating the 3D editing as a multi-view 2D inpainting task. Specifically,
MVInpainter partially inpaints multi-view images with the reference guidance
rather than intractably generating an entirely novel view from scratch, which
largely simplifies the difficulty of in-the-wild NVS and leverages unmasked
clues instead of explicit pose conditions. To ensure cross-view consistency,
MVInpainter is enhanced by video priors from motion components and appearance
guidance from concatenated reference key&value attention. Furthermore,
MVInpainter incorporates slot attention to aggregate high-level optical flow
features from unmasked regions to control the camera movement with pose-free
training and inference. Sufficient scene-level experiments on both
object-centric and forward-facing datasets verify the effectiveness of
MVInpainter, including diverse tasks, such as multi-view object removal,
synthesis, insertion, and replacement. The project page is
https://ewrfcas.github.io/MVInpainter/.",2024-08-15 07:57:28+00:00,"['Chenjie Cao', 'Chaohui Yu', 'Fan Wang', 'Xiangyang Xue', 'Yanwei Fu']",http://arxiv.org/abs/2408.08000v3
Key-point Guided Deformable Image Manipulation Using Diffusion Model,"In this paper, we introduce a Key-point-guided Diffusion probabilistic Model
(KDM) that gains precise control over images by manipulating the object's
key-point. We propose a two-stage generative model incorporating an optical
flow map as an intermediate output. By doing so, a dense pixel-wise
understanding of the semantic relation between the image and sparse key point
is configured, leading to more realistic image generation. Additionally, the
integration of optical flow helps regulate the inter-frame variance of
sequential images, demonstrating an authentic sequential image generation. The
KDM is evaluated with diverse key-point conditioned image synthesis tasks,
including facial image generation, human pose synthesis, and echocardiography
video prediction, demonstrating the KDM is proving consistency enhanced and
photo-realistic images compared with state-of-the-art models.",2024-01-16 07:51:00+00:00,"['Seok-Hwan Oh', 'Guil Jung', 'Myeong-Gee Kim', 'Sang-Yun Kim', 'Young-Min Kim', 'Hyeon-Jik Lee', 'Hyuk-Sool Kwon', 'Hyeon-Min Bae']",http://arxiv.org/abs/2401.08178v3
Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model,"Diffusion models have obtained substantial progress in image-to-video
generation. However, in this paper, we find that these models tend to generate
videos with less motion than expected. We attribute this to the issue called
conditional image leakage, where the image-to-video diffusion models (I2V-DMs)
tend to over-rely on the conditional image at large time steps. We further
address this challenge from both inference and training aspects. First, we
propose to start the generation process from an earlier time step to avoid the
unreliable large-time steps of I2V-DMs, as well as an initial noise
distribution with optimal analytic expressions (Analytic-Init) by minimizing
the KL divergence between it and the actual marginal distribution to bridge the
training-inference gap. Second, we design a time-dependent noise distribution
(TimeNoise) for the conditional image during training, applying higher noise
levels at larger time steps to disrupt it and reduce the model's dependency on
it. We validate these general strategies on various I2V-DMs on our collected
open-domain image benchmark and the UCF101 dataset. Extensive results show that
our methods outperform baselines by producing higher motion scores with lower
errors while maintaining image alignment and temporal consistency, thereby
yielding superior overall performance and enabling more accurate motion
control. The project page: \url{https://cond-image-leak.github.io/}.",2024-06-22 04:56:16+00:00,"['Min Zhao', 'Hongzhou Zhu', 'Chendong Xiang', 'Kaiwen Zheng', 'Chongxuan Li', 'Jun Zhu']",http://arxiv.org/abs/2406.15735v3
SVP: Style-Enhanced Vivid Portrait Talking Head Diffusion Model,"Talking Head Generation (THG), typically driven by audio, is an important and
challenging task with broad application prospects in various fields such as
digital humans, film production, and virtual reality. While diffusion
model-based THG methods present high quality and stable content generation,
they often overlook the intrinsic style which encompasses personalized features
such as speaking habits and facial expressions of a video. As consequence, the
generated video content lacks diversity and vividness, thus being limited in
real life scenarios. To address these issues, we propose a novel framework
named Style-Enhanced Vivid Portrait (SVP) which fully leverages style-related
information in THG. Specifically, we first introduce the novel probabilistic
style prior learning to model the intrinsic style as a Gaussian distribution
using facial expressions and audio embedding. The distribution is learned
through the 'bespoked' contrastive objective, effectively capturing the dynamic
style information in each video. Then we finetune a pretrained Stable Diffusion
(SD) model to inject the learned intrinsic style as a controlling signal via
cross attention. Experiments show that our model generates diverse, vivid, and
high-quality videos with flexible control over intrinsic styles, outperforming
existing state-of-the-art methods.",2024-09-05 06:27:32+00:00,"['Weipeng Tan', 'Chuming Lin', 'Chengming Xu', 'Xiaozhong Ji', 'Junwei Zhu', 'Chengjie Wang', 'Yunsheng Wu', 'Yanwei Fu']",http://arxiv.org/abs/2409.03270v2
Towards Precise Scaling Laws for Video Diffusion Transformers,"Achieving optimal performance of video diffusion transformers within given
data and compute budget is crucial due to their high training costs. This
necessitates precisely determining the optimal model size and training
hyperparameters before large-scale training. While scaling laws are employed in
language models to predict performance, their existence and accurate derivation
in visual generation models remain underexplored. In this paper, we
systematically analyze scaling laws for video diffusion transformers and
confirm their presence. Moreover, we discover that, unlike language models,
video diffusion models are more sensitive to learning rate and batch size, two
hyperparameters often not precisely modeled. To address this, we propose a new
scaling law that predicts optimal hyperparameters for any model size and
compute budget. Under these optimal settings, we achieve comparable performance
and reduce inference costs by 40.1% compared to conventional scaling methods,
within a compute budget of 1e10 TFlops. Furthermore, we establish a more
generalized and precise relationship among validation loss, any model size, and
compute budget. This enables performance prediction for non-optimal model
sizes, which may also be appealed under practical inference cost constraints,
achieving a better trade-off.",2024-11-25 18:59:04+00:00,"['Yuanyang Yin', 'Yaqi Zhao', 'Mingwu Zheng', 'Ke Lin', 'Jiarong Ou', 'Rui Chen', 'Victor Shea-Jay Huang', 'Jiahao Wang', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Baoqun Yin', 'Wentao Zhang', 'Kun Gai']",http://arxiv.org/abs/2411.17470v2
Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era,"The rapid advancement of large language models (LLMs) and multimodal learning
has transformed digital content creation and manipulation. Traditional visual
editing tools require significant expertise, limiting accessibility. Recent
strides in instruction-based editing have enabled intuitive interaction with
visual content, using natural language as a bridge between user intent and
complex editing operations. This survey provides an overview of these
techniques, focusing on how LLMs and multimodal models empower users to achieve
precise visual modifications without deep technical knowledge. By synthesizing
over 100 publications, we explore methods from generative adversarial networks
to diffusion models, examining multimodal integration for fine-grained content
control. We discuss practical applications across domains such as fashion, 3D
scene manipulation, and video synthesis, highlighting increased accessibility
and alignment with human intuition. Our survey compares existing literature,
emphasizing LLM-empowered editing, and identifies key challenges to stimulate
further research. We aim to democratize powerful visual editing across various
industries, from entertainment to education. Interested readers are encouraged
to access our repository at
https://github.com/tamlhp/awesome-instruction-editing.",2024-11-15 05:18:15+00:00,"['Thanh Tam Nguyen', 'Zhao Ren', 'Trinh Pham', 'Thanh Trung Huynh', 'Phi Le Nguyen', 'Hongzhi Yin', 'Quoc Viet Hung Nguyen']",http://arxiv.org/abs/2411.09955v2
DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval,"Text-video retrieval is a critical multi-modal task to find the most relevant
video for a text query. Although pretrained models like CLIP have demonstrated
impressive potential in this area, the rising cost of fully finetuning these
models due to increasing model size continues to pose a problem. To address
this challenge, prompt tuning has emerged as an alternative. However, existing
works still face two problems when adapting pretrained image-text models to
downstream video-text tasks: (1) The visual encoder could only encode
frame-level features and failed to extract global-level general video
information. (2) Equipping the visual and text encoder with separated prompts
failed to mitigate the visual-text modality gap. To this end, we propose DGL, a
cross-modal Dynamic prompt tuning method with Global-Local video attention. In
contrast to previous prompt tuning methods, we employ the shared latent space
to generate local-level text and frame prompts that encourage inter-modal
interaction. Furthermore, we propose modeling video in a global-local attention
mechanism to capture global video information from the perspective of prompt
tuning. Extensive experiments reveal that when only 0.67% parameters are tuned,
our cross-modal prompt tuning strategy DGL outperforms or is comparable to
fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets.
Code will be available at https://github.com/knightyxp/DGL",2024-01-19 09:58:06+00:00,"['Xiangpeng Yang', 'Linchao Zhu', 'Xiaohan Wang', 'Yi Yang']",http://arxiv.org/abs/2401.10588v1
VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation,"Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate
through realistic 3D outdoor environments based on natural language
instructions. The performance of existing VLN methods is limited by
insufficient diversity in navigation environments and limited training data. To
address these issues, we propose VLN-Video, which utilizes the diverse outdoor
environments present in driving videos in multiple cities in the U.S. augmented
with automatically generated navigation instructions and actions to improve
outdoor VLN performance. VLN-Video combines the best of intuitive classical
approaches and modern deep learning techniques, using template infilling to
generate grounded navigation instructions, combined with an image rotation
similarity-based navigation action predictor to obtain VLN style data from
driving videos for pretraining deep learning VLN models. We pre-train the model
on the Touchdown dataset and our video-augmented dataset created from driving
videos with three proxy tasks: Masked Language Modeling, Instruction and
Trajectory Matching, and Next Action Prediction, so as to learn
temporally-aware and visually-aligned instruction representations. The learned
instruction representation is adapted to the state-of-the-art navigator when
fine-tuning on the Touchdown dataset. Empirical results demonstrate that
VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in
task completion rate, achieving a new state-of-the-art on the Touchdown
dataset.",2024-02-05 22:20:19+00:00,"['Jialu Li', 'Aishwarya Padmakumar', 'Gaurav Sukhatme', 'Mohit Bansal']",http://arxiv.org/abs/2402.03561v2
Orientation-conditioned Facial Texture Mapping for Video-based Facial Remote Photoplethysmography Estimation,"Camera-based remote photoplethysmography (rPPG) enables contactless
measurement of important physiological signals such as pulse rate (PR).
However, dynamic and unconstrained subject motion introduces significant
variability into the facial appearance in video, confounding the ability of
video-based methods to accurately extract the rPPG signal. In this study, we
leverage the 3D facial surface to construct a novel orientation-conditioned
facial texture video representation which improves the motion robustness of
existing video-based facial rPPG estimation methods. Our proposed method
achieves a significant 18.2% performance improvement in cross-dataset testing
on MMPD over our baseline using the PhysNet model trained on PURE, highlighting
the efficacy and generalization benefits of our designed video representation.
We demonstrate significant performance improvements of up to 29.6% in all
tested motion scenarios in cross-dataset testing on MMPD, even in the presence
of dynamic and unconstrained subject motion, emphasizing the benefits of
disentangling motion through modeling the 3D facial surface for motion robust
facial rPPG estimation. We validate the efficacy of our design decisions and
the impact of different video processing steps through an ablation study. Our
findings illustrate the potential strengths of exploiting the 3D facial surface
as a general strategy for addressing dynamic and unconstrained subject motion
in videos. The code is available at
https://samcantrill.github.io/orientation-uv-rppg/.",2024-04-14 23:30:35+00:00,"['Sam Cantrill', 'David Ahmedt-Aristizabal', 'Lars Petersson', 'Hanna Suominen', 'Mohammad Ali Armin']",http://arxiv.org/abs/2404.09378v3
"Contrast, Imitate, Adapt: Learning Robotic Skills From Raw Human Videos","Learning robotic skills from raw human videos remains a non-trivial
challenge. Previous works tackled this problem by leveraging behavior cloning
or learning reward functions from videos. Despite their remarkable
performances, they may introduce several issues, such as the necessity for
robot actions, requirements for consistent viewpoints and similar layouts
between human and robot videos, as well as low sample efficiency. To this end,
our key insight is to learn task priors by contrasting videos and to learn
action priors through imitating trajectories from videos, and to utilize the
task priors to guide trajectories to adapt to novel scenarios. We propose a
three-stage skill learning framework denoted as Contrast-Imitate-Adapt (CIA).
An interaction-aware alignment transformer is proposed to learn task priors by
temporally aligning video pairs. Then a trajectory generation model is used to
learn action priors. To adapt to novel scenarios different from human videos,
the Inversion-Interaction method is designed to initialize coarse trajectories
and refine them by limited interaction. In addition, CIA introduces an
optimization method based on semantic directions of trajectories for
interaction security and sample efficiency. The alignment distances computed by
IAAformer are used as the rewards. We evaluate CIA in six real-world everyday
tasks, and empirically demonstrate that CIA significantly outperforms previous
state-of-the-art works in terms of task success rate and generalization to
diverse novel scenarios layouts and object instances.",2024-08-10 08:27:43+00:00,"['Zhifeng Qian', 'Mingyu You', 'Hongjun Zhou', 'Xuanhui Xu', 'Hao Fu', 'Jinzhe Xue', 'Bin He']",http://arxiv.org/abs/2408.05485v1
One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos,"We introduce VideoLISA, a video-based multimodal large language model
designed to tackle the problem of language-instructed reasoning segmentation in
videos. Leveraging the reasoning capabilities and world knowledge of large
language models, and augmented by the Segment Anything Model, VideoLISA
generates temporally consistent segmentation masks in videos based on language
instructions. Existing image-based methods, such as LISA, struggle with video
tasks due to the additional temporal dimension, which requires temporal dynamic
understanding and consistent segmentation across frames. VideoLISA addresses
these challenges by integrating a Sparse Dense Sampling strategy into the
video-LLM, which balances temporal context and spatial detail within
computational constraints. Additionally, we propose a One-Token-Seg-All
approach using a specially designed <TRK> token, enabling the model to segment
and track objects across multiple frames. Extensive evaluations on diverse
benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate
VideoLISA's superior performance in video object segmentation tasks involving
complex reasoning, temporal understanding, and object tracking. While optimized
for videos, VideoLISA also shows promising generalization to image
segmentation, revealing its potential as a unified foundation model for
language-instructed object segmentation. Code and model will be available at:
https://github.com/showlab/VideoLISA.",2024-09-29 07:47:15+00:00,"['Zechen Bai', 'Tong He', 'Haiyang Mei', 'Pichao Wang', 'Ziteng Gao', 'Joya Chen', 'Lei Liu', 'Zheng Zhang', 'Mike Zheng Shou']",http://arxiv.org/abs/2409.19603v1
Grounding is All You Need? Dual Temporal Grounding for Video Dialog,"In the realm of video dialog response generation, the understanding of video
content and the temporal nuances of conversation history are paramount. While a
segment of current research leans heavily on large-scale pretrained
visual-language models and often overlooks temporal dynamics, another delves
deep into spatial-temporal relationships within videos but demands intricate
object trajectory pre-extractions and sidelines dialog temporal dynamics. This
paper introduces the Dual Temporal Grounding-enhanced Video Dialog model
(DTGVD), strategically designed to merge the strengths of both dominant
approaches. It emphasizes dual temporal relationships by predicting dialog
turn-specific temporal regions, filtering video content accordingly, and
grounding responses in both video and dialog contexts. One standout feature of
DTGVD is its heightened attention to chronological interplay. By recognizing
and acting upon the dependencies between different dialog turns, it captures
more nuanced conversational dynamics. To further bolster the alignment between
video and dialog temporal dynamics, we've implemented a list-wise contrastive
learning strategy. Within this framework, accurately grounded turn-clip
pairings are designated as positive samples, while less precise pairings are
categorized as negative. This refined classification is then funneled into our
holistic end-to-end response generation mechanism. Evaluations using
AVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our
methodology.",2024-10-08 07:48:34+00:00,"['You Qin', 'Wei Ji', 'Xinze Lan', 'Hao Fei', 'Xun Yang', 'Dan Guo', 'Roger Zimmermann', 'Lizi Liao']",http://arxiv.org/abs/2410.05767v2
Multi-Pair Temporal Sentence Grounding via Multi-Thread Knowledge Transfer Network,"Given some video-query pairs with untrimmed videos and sentence queries,
temporal sentence grounding (TSG) aims to locate query-relevant segments in
these videos. Although previous respectable TSG methods have achieved
remarkable success, they train each video-query pair separately and ignore the
relationship between different pairs. We observe that the similar video/query
content not only helps the TSG model better understand and generalize the
cross-modal representation but also assists the model in locating some complex
video-query pairs. Previous methods follow a single-thread framework that
cannot co-train different pairs and usually spends much time re-obtaining
redundant knowledge, limiting their real-world applications. To this end, in
this paper, we pose a brand-new setting: Multi-Pair TSG, which aims to co-train
these pairs. In particular, we propose a novel video-query co-training
approach, Multi-Thread Knowledge Transfer Network, to locate a variety of
video-query pairs effectively and efficiently. Firstly, we mine the spatial and
temporal semantics across different queries to cooperate with each other. To
learn intra- and inter-modal representations simultaneously, we design a
cross-modal contrast module to explore the semantic consistency by a
self-supervised strategy. To fully align visual and textual representations
between different pairs, we design a prototype alignment strategy to 1) match
object prototypes and phrase prototypes for spatial alignment, and 2) align
activity prototypes and sentence prototypes for temporal alignment. Finally, we
develop an adaptive negative selection module to adaptively generate a
threshold for cross-modal matching. Extensive experiments show the
effectiveness and efficiency of our proposed method.",2024-12-20 08:50:11+00:00,"['Xiang Fang', 'Wanlong Fang', 'Changshuo Wang', 'Daizong Liu', 'Keke Tang', 'Jianfeng Dong', 'Pan Zhou', 'Beibei Li']",http://arxiv.org/abs/2412.15678v1
Hierarchical Banzhaf Interaction for General Video-Language Representation Learning,"Multimodal representation learning, with contrastive learning, plays an
important role in the artificial intelligence domain. As an important subfield,
video-language representation learning focuses on learning representations
using global semantic interactions between pre-defined video-text pairs.
However, to enhance and refine such coarse-grained global interactions, more
detailed interactions are necessary for fine-grained multimodal learning. In
this study, we introduce a new approach that models video-text as game players
using multivariate cooperative game theory to handle uncertainty during
fine-grained semantic interactions with diverse granularity, flexible
combination, and vague intensity. Specifically, we design the Hierarchical
Banzhaf Interaction to simulate the fine-grained correspondence between video
clips and textual words from hierarchical perspectives. Furthermore, to
mitigate the bias in calculations within Banzhaf Interaction, we propose
reconstructing the representation through a fusion of single-modal and
cross-modal components. This reconstructed representation ensures fine
granularity comparable to that of the single-modal representation, while also
preserving the adaptive encoding characteristics of cross-modal representation.
Additionally, we extend our original structure into a flexible encoder-decoder
framework, enabling the model to adapt to various downstream tasks. Extensive
experiments on commonly used text-video retrieval, video-question answering,
and video captioning benchmarks, with superior performance, validate the
effectiveness and generalization of our method.",2024-12-30 14:09:15+00:00,"['Peng Jin', 'Hao Li', 'Li Yuan', 'Shuicheng Yan', 'Jie Chen']",http://arxiv.org/abs/2412.20964v1
EG4D: Explicit Generation of 4D Object without Score Distillation,"In recent years, the increasing demand for dynamic 3D assets in design and
gaming applications has given rise to powerful generative pipelines capable of
synthesizing high-quality 4D objects. Previous methods generally rely on score
distillation sampling (SDS) algorithm to infer the unseen views and motion of
4D objects, thus leading to unsatisfactory results with defects like
over-saturation and Janus problem. Therefore, inspired by recent progress of
video diffusion models, we propose to optimize a 4D representation by
explicitly generating multi-view videos from one input image. However, it is
far from trivial to handle practical challenges faced by such a pipeline,
including dramatic temporal inconsistency, inter-frame geometry and texture
diversity, and semantic defects brought by video generation results. To address
these issues, we propose DG4D, a novel multi-stage framework that generates
high-quality and consistent 4D assets without score distillation. Specifically,
collaborative techniques and solutions are developed, including an attention
injection strategy to synthesize temporal-consistent multi-view videos, a
robust and efficient dynamic reconstruction method based on Gaussian Splatting,
and a refinement stage with diffusion prior for semantic restoration. The
qualitative results and user preference study demonstrate that our framework
outperforms the baselines in generation quality by a considerable margin. Code
will be released at \url{https://github.com/jasongzy/EG4D}.",2024-05-28 12:47:22+00:00,"['Qi Sun', 'Zhiyang Guo', 'Ziyu Wan', 'Jing Nathan Yan', 'Shengming Yin', 'Wengang Zhou', 'Jing Liao', 'Houqiang Li']",http://arxiv.org/abs/2405.18132v1
Improving 3D deep learning segmentation with biophysically motivated cell synthesis,"Biomedical research increasingly relies on 3D cell culture models and
AI-based analysis can potentially facilitate a detailed and accurate feature
extraction on a single-cell level. However, this requires for a precise
segmentation of 3D cell datasets, which in turn demands high-quality ground
truth for training. Manual annotation, the gold standard for ground truth data,
is too time-consuming and thus not feasible for the generation of large 3D
training datasets. To address this, we present a novel framework for generating
3D training data, which integrates biophysical modeling for realistic cell
shape and alignment. Our approach allows the in silico generation of coherent
membrane and nuclei signals, that enable the training of segmentation models
utilizing both channels for improved performance. Furthermore, we present a new
GAN training scheme that generates not only image data but also matching
labels. Quantitative evaluation shows superior performance of biophysical
motivated synthetic training data, even outperforming manual annotation and
pretrained models. This underscores the potential of incorporating biophysical
modeling for enhancing synthetic training data quality.",2024-08-29 12:01:23+00:00,"['Roman Bruch', 'Mario Vitacolonna', 'Elina Nrnberg', 'Simeon Sauer', 'Rdiger Rudolf', 'Markus Reischl']",http://arxiv.org/abs/2408.16471v1
E2VIDiff: Perceptual Events-to-Video Reconstruction using Diffusion Priors,"Event cameras, mimicking the human retina, capture brightness changes with
unparalleled temporal resolution and dynamic range. Integrating events into
intensities poses a highly ill-posed challenge, marred by initial condition
ambiguities. Traditional regression-based deep learning methods fall short in
perceptual quality, offering deterministic and often unrealistic
reconstructions. In this paper, we introduce diffusion models to
events-to-video reconstruction, achieving colorful, realistic, and perceptually
superior video generation from achromatic events. Powered by the image
generation ability and knowledge of pretrained diffusion models, the proposed
method can achieve a better trade-off between the perception and distortion of
the reconstructed frame compared to previous solutions. Extensive experiments
on benchmark datasets demonstrate that our approach can produce diverse,
realistic frames with faithfulness to the given events.",2024-07-11 07:10:58+00:00,"['Jinxiu Liang', 'Bohan Yu', 'Yixin Yang', 'Yiming Han', 'Boxin Shi']",http://arxiv.org/abs/2407.08231v1
DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake Detection,"With the advancement of deepfake generation techniques, the importance of
deepfake detection in protecting multimedia content integrity has become
increasingly obvious. Recently, temporal inconsistency clues have been explored
to improve the generalizability of deepfake video detection. According to our
observation, the temporal artifacts of forged videos in terms of motion
information usually exhibits quite distinct inconsistency patterns along
horizontal and vertical directions, which could be leveraged to improve the
generalizability of detectors. In this paper, a transformer-based framework for
Diffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits
directional inconsistencies for deepfake video detection. Specifically, DIP
begins with a spatiotemporal encoder to represent spatiotemporal information. A
directional inconsistency decoder is adopted accordingly, where direction-aware
attention and inconsistency diffusion are incorporated to explore potential
inconsistency patterns and jointly learn the inherent relationships. In
addition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to
contrast spatiotemporally augmented sample pairs and prevent the model from
overfitting nonessential forgery artifacts. Extensive experiments on several
public datasets demonstrate that our method could effectively identify
directional forgery clues and achieve state-of-the-art performance.",2024-10-31 06:26:00+00:00,"['Fan Nie', 'Jiangqun Ni', 'Jian Zhang', 'Bin Zhang', 'Weizhe Zhang']",http://arxiv.org/abs/2410.23663v1
Physics-Informed Latent Diffusion for Multimodal Brain MRI Synthesis,"Recent advances in generative models for medical imaging have shown promise
in representing multiple modalities. However, the variability in modality
availability across datasets limits the general applicability of the synthetic
data they produce. To address this, we present a novel physics-informed
generative model capable of synthesizing a variable number of brain MRI
modalities, including those not present in the original dataset. Our approach
utilizes latent diffusion models and a two-step generative process: first,
unobserved physical tissue property maps are synthesized using a latent
diffusion model, and then these maps are combined with a physical signal model
to generate the final MRI scan. Our experiments demonstrate the efficacy of
this approach in generating unseen MR contrasts and preserving physical
plausibility. Furthermore, we validate the distributions of generated tissue
properties by comparing them to those measured in real brain tissue.",2024-09-20 14:21:34+00:00,"['Sven Lpke', 'Yousef Yeganeh', 'Ehsan Adeli', 'Nassir Navab', 'Azade Farshad']",http://arxiv.org/abs/2409.13532v2
Training-free Video Temporal Grounding using Large-scale Pre-trained Models,"Video temporal grounding aims to identify video segments within untrimmed
videos that are most relevant to a given natural language query. Existing video
temporal localization models rely on specific datasets for training and have
high data collection costs, but they exhibit poor generalization capability
under the across-dataset and out-of-distribution (OOD) settings. In this paper,
we propose a Training-Free Video Temporal Grounding (TFVTG) approach that
leverages the ability of pre-trained large models. A naive baseline is to
enumerate proposals in the video and use the pre-trained visual language models
(VLMs) to select the best proposal according to the vision-language alignment.
However, most existing VLMs are trained on image-text pairs or trimmed video
clip-text pairs, making it struggle to (1) grasp the relationship and
distinguish the temporal boundaries of multiple events within the same video;
(2) comprehend and be sensitive to the dynamic transition of events (the
transition from one event to another) in the video. To address these issues, we
propose leveraging large language models (LLMs) to analyze multiple sub-events
contained in the query text and analyze the temporal order and relationships
between these events. Secondly, we split a sub-event into dynamic transition
and static status parts and propose the dynamic and static scoring functions
using VLMs to better evaluate the relevance between the event and the
description. Finally, for each sub-event description, we use VLMs to locate the
top-k proposals and leverage the order and relationships between sub-events
provided by LLMs to filter and integrate these proposals. Our method achieves
the best performance on zero-shot video temporal grounding on Charades-STA and
ActivityNet Captions datasets without any training and demonstrates better
generalization capabilities in cross-dataset and OOD settings.",2024-08-29 02:25:12+00:00,"['Minghang Zheng', 'Xinhao Cai', 'Qingchao Chen', 'Yuxin Peng', 'Yang Liu']",http://arxiv.org/abs/2408.16219v1
OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog,"We present the Object Language Video Transformer (OLViT) - a novel model for
video dialog operating over a multi-modal attention-based dialog state tracker.
Existing video dialog models struggle with questions requiring both spatial and
temporal localization within videos, long-term temporal reasoning, and accurate
object tracking across multiple dialog turns. OLViT addresses these challenges
by maintaining a global dialog state based on the output of an Object State
Tracker (OST) and a Language State Tracker (LST): while the OST attends to the
most important objects within the video, the LST keeps track of the most
important linguistic co-references to previous dialog turns. In stark contrast
to previous works, our approach is generic by nature and is therefore capable
of learning continuous multi-modal dialog state representations of the most
relevant objects and rounds. As a result, they can be seamlessly integrated
into Large Language Models (LLMs) and offer high flexibility in dealing with
different datasets and tasks. Evaluations on the challenging DVD (response
classification) and SIMMC 2.1 (response generation) datasets show that OLViT
achieves new state-of-the-art performance across both datasets.",2024-02-20 17:00:59+00:00,"['Adnen Abdessaied', 'Manuel von Hochmeister', 'Andreas Bulling']",http://arxiv.org/abs/2402.13146v1
"CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility","Recent advancements in video generation have been remarkable, yet many
existing methods struggle with issues of consistency and poor text-video
alignment. Moreover, the field lacks effective techniques for text-guided video
inpainting, a stark contrast to the well-explored domain of text-guided image
inpainting. To this end, this paper proposes a novel text-guided video
inpainting model that achieves better consistency, controllability and
compatibility. Specifically, we introduce a simple but efficient motion capture
module to preserve motion consistency, and design an instance-aware region
selection instead of a random region selection to obtain better textual
controllability, and utilize a novel strategy to inject some personalized
models into our CoCoCo model and thus obtain better model compatibility.
Extensive experiments show that our model can generate high-quality video
clips. Meanwhile, our model shows better motion consistency, textual
controllability and model compatibility. More details are shown in
[cococozibojia.github.io](cococozibojia.github.io).",2024-03-18 17:59:27+00:00,"['Bojia Zi', 'Shihao Zhao', 'Xianbiao Qi', 'Jianan Wang', 'Yukai Shi', 'Qianyu Chen', 'Bin Liang', 'Kam-Fai Wong', 'Lei Zhang']",http://arxiv.org/abs/2403.12035v1
Learned Scanpaths Aid Blind Panoramic Video Quality Assessment,"Panoramic videos have the advantage of providing an immersive and interactive
viewing experience. Nevertheless, their spherical nature gives rise to various
and uncertain user viewing behaviors, which poses significant challenges for
panoramic video quality assessment (PVQA). In this work, we propose an
end-to-end optimized, blind PVQA method with explicit modeling of user viewing
patterns through visual scanpaths. Our method consists of two modules: a
scanpath generator and a quality assessor. The scanpath generator is initially
trained to predict future scanpaths by minimizing their expected code length
and then jointly optimized with the quality assessor for quality prediction.
Our blind PVQA method enables direct quality assessment of panoramic images by
treating them as videos composed of identical frames. Experiments on three
public panoramic image and video quality datasets, encompassing both synthetic
and authentic distortions, validate the superiority of our blind PVQA model
over existing methods.",2024-03-30 05:42:17+00:00,"['Kanglong Fan', 'Wen Wen', 'Mu Li', 'Yifan Peng', 'Kede Ma']",http://arxiv.org/abs/2404.00252v2
DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement,"We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for
dense video captioning (DVC), that elaborates on improving the quality of the
generated event captions and their associated pseudo event boundaries from
unlabeled videos. By leveraging the capabilities of diverse large language
models (LLMs), we generate rich DVC-oriented caption candidates and optimize
the corresponding pseudo boundaries under several meticulously designed
objectives, considering diversity, event-centricity, temporal ordering, and
coherence. Moreover, we further introduce a novel online boundary refinement
strategy that iteratively improves the quality of pseudo boundaries during
training. Comprehensive experiments have been conducted to examine the
effectiveness of the proposed technique components. By leveraging a substantial
amount of unlabeled video data, such as HowTo100M, we achieve a remarkable
advancement on standard DVC datasets like YouCook2 and ActivityNet. We
outperform the previous state-of-the-art Vid2Seq across a majority of metrics,
achieving this with just 0.4% of the unlabeled video data used for pre-training
by Vid2Seq.",2024-04-03 13:57:08+00:00,"['Hao Wu', 'Huabin Liu', 'Yu Qiao', 'Xiao Sun']",http://arxiv.org/abs/2404.02755v1
Video Enriched Retrieval Augmented Generation Using Aligned Video Captions,"In this work, we propose the use of ""aligned visual captions"" as a mechanism
for integrating information contained within videos into retrieval augmented
generation (RAG) based chat assistant systems. These captions are able to
describe the visual and audio content of videos in a large corpus while having
the advantage of being in a textual format that is both easy to reason about &
incorporate into large language model (LLM) prompts, but also typically require
less multimedia content to be inserted into the multimodal LLM context window,
where typical configurations can aggressively fill up the context window by
sampling video frames from the source video. Furthermore, visual captions can
be adapted to specific use cases by prompting the original foundational model /
captioner for particular visual details or fine tuning. In hopes of helping
advancing progress in this area, we curate a dataset and describe automatic
evaluation procedures on common RAG tasks.",2024-05-27 23:39:17+00:00,['Kevin Dela Rosa'],http://arxiv.org/abs/2405.17706v1
YouTube SFV+HDR Quality Dataset,"The popularity of Short form videos (SFV) has grown dramatically in the past
few years, and has become a phenomenal video category with billions of viewers.
Meanwhile, High Dynamic Range (HDR) as an advanced feature also becomes more
and more popular on video sharing platforms. As a hot topic with huge impact,
SFV and HDR bring new questions to video quality research: 1) is SFV+HDR
quality assessment significantly different from traditional User Generated
Content (UGC) quality assessment? 2) do objective quality metrics designed for
traditional UGC still work well for SFV+HDR? To answer the above questions, we
created the first large scale SFV+HDR dataset with reliable subjective quality
scores, covering 10 popular content categories. Further, we also introduce a
general sampling framework to maximize the representativeness of the dataset.
We provided a comprehensive analysis of subjective quality scores for Short
form SDR and HDR videos, and discuss the reliability of state-of-the-art UGC
quality metrics and potential improvements.",2024-06-08 00:15:37+00:00,"['Yilin Wang', 'Joong Gon Yim', 'Neil Birkbeck', 'Balu Adsumilli']",http://arxiv.org/abs/2406.05305v2
Context-Aware Temporal Embedding of Objects in Video Data,"In video analysis, understanding the temporal context is crucial for
recognizing object interactions, event patterns, and contextual changes over
time. The proposed model leverages adjacency and semantic similarities between
objects from neighboring video frames to construct context-aware temporal
object embeddings. Unlike traditional methods that rely solely on visual
appearance, our temporal embedding model considers the contextual relationships
between objects, creating a meaningful embedding space where temporally
connected object's vectors are positioned in proximity. Empirical studies
demonstrate that our context-aware temporal embeddings can be used in
conjunction with conventional visual embeddings to enhance the effectiveness of
downstream applications. Moreover, the embeddings can be used to narrate a
video using a Large Language Model (LLM). This paper describes the intricate
details of the proposed objective function to generate context-aware temporal
object embeddings for video data and showcases the potential applications of
the generated embeddings in video analysis and object classification tasks.",2024-08-23 01:44:10+00:00,"['Ahnaf Farhan', 'M. Shahriar Hossain']",http://arxiv.org/abs/2408.12789v1
Portrait Video Editing Empowered by Multimodal Generative Priors,"We introduce PortraitGen, a powerful portrait video editing method that
achieves consistent and expressive stylization with multimodal prompts.
Traditional portrait video editing methods often struggle with 3D and temporal
consistency, and typically lack in rendering quality and efficiency. To address
these issues, we lift the portrait video frames to a unified dynamic 3D
Gaussian field, which ensures structural and temporal coherence across frames.
Furthermore, we design a novel Neural Gaussian Texture mechanism that not only
enables sophisticated style editing but also achieves rendering speed over
100FPS. Our approach incorporates multimodal inputs through knowledge distilled
from large-scale 2D generative models. Our system also incorporates expression
similarity guidance and a face-aware portrait editing module, effectively
mitigating degradation issues associated with iterative dataset updates.
Extensive experiments demonstrate the temporal consistency, editing efficiency,
and superior rendering quality of our method. The broad applicability of the
proposed approach is demonstrated through various applications, including
text-driven editing, image-driven editing, and relighting, highlighting its
great potential to advance the field of video editing. Demo videos and released
code are provided in our project page: https://ustc3dv.github.io/PortraitGen/",2024-09-20 15:45:13+00:00,"['Xuan Gao', 'Haiyao Xiao', 'Chenglai Zhong', 'Shimin Hu', 'Yudong Guo', 'Juyong Zhang']",http://arxiv.org/abs/2409.13591v1
Bridging Vision and Language: Modeling Causality and Temporality in Video Narratives,"Video captioning is a critical task in the field of multimodal machine
learning, aiming to generate descriptive and coherent textual narratives for
video content. While large vision-language models (LVLMs) have shown
significant progress, they often struggle to capture the causal and temporal
dynamics inherent in complex video sequences. To address this limitation, we
propose an enhanced framework that integrates a Causal-Temporal Reasoning
Module (CTRM) into state-of-the-art LVLMs. CTRM comprises two key components:
the Causal Dynamics Encoder (CDE) and the Temporal Relational Learner (TRL),
which collectively encode causal dependencies and temporal consistency from
video frames. We further design a multi-stage learning strategy to optimize the
model, combining pre-training on large-scale video-text datasets, fine-tuning
on causally annotated data, and contrastive alignment for better embedding
coherence. Experimental results on standard benchmarks such as MSVD and MSR-VTT
demonstrate that our method outperforms existing approaches in both automatic
metrics (CIDEr, BLEU-4, ROUGE-L) and human evaluations, achieving more fluent,
coherent, and relevant captions. These results validate the effectiveness of
our approach in generating captions with enriched causal-temporal narratives.",2024-12-14 07:28:38+00:00,"['Ji-jun Park', 'Soo-joon Choi']",http://arxiv.org/abs/2412.10720v1
TeaserGen: Generating Teasers for Long Documentaries,"Teasers are an effective tool for promoting content in entertainment,
commercial and educational fields. However, creating an effective teaser for
long videos is challenging for it requires long-range multimodal modeling on
the input videos, while necessitating maintaining audiovisual alignments,
managing scene changes and preserving factual accuracy for the output teasers.
Due to the lack of a publicly-available dataset, progress along this research
direction has been hindered. In this work, we present DocumentaryNet, a
collection of 1,269 documentaries paired with their teasers, featuring
multimodal data streams of video, speech, music, sound effects and narrations.
With DocumentaryNet, we propose a new two-stage system for generating teasers
from long documentaries. The proposed TeaserGen system first generates the
teaser narration from the transcribed narration of the documentary using a
pretrained large language model, and then selects the most relevant visual
content to accompany the generated narration through language-vision models.
For narration-video matching, we explore two approaches: a pretraining-based
model using pretrained contrastive language-vision models and a deep sequential
model that learns the mapping between the narrations and visuals. Our
experimental results show that the pretraining-based approach is more effective
at identifying relevant visual content than directly trained deep
autoregressive models.",2024-10-08 01:00:09+00:00,"['Weihan Xu', 'Paul Pu Liang', 'Haven Kim', 'Julian McAuley', 'Taylor Berg-Kirkpatrick', 'Hao-Wen Dong']",http://arxiv.org/abs/2410.05586v2
Efficient4D: Fast Dynamic 3D Object Generation from a Single-view Video,"Generating dynamic 3D object from a single-view video is challenging due to
the lack of 4D labeled data. An intuitive approach is to extend previous
image-to-3D pipelines by transferring off-the-shelf image generation models
such as score distillation sampling.However, this approach would be slow and
expensive to scale due to the need for back-propagating the information-limited
supervision signals through a large pretrained model. To address this, we
propose an efficient video-to-4D object generation framework called
Efficient4D. It generates high-quality spacetime-consistent images under
different camera views, and then uses them as labeled data to directly
reconstruct the 4D content through a 4D Gaussian splatting model. Importantly,
our method can achieve real-time rendering under continuous camera
trajectories. To enable robust reconstruction under sparse views, we introduce
inconsistency-aware confidence-weighted loss design, along with a lightly
weighted score distillation loss. Extensive experiments on both synthetic and
real videos show that Efficient4D offers a remarkable 10-fold increase in speed
when compared to prior art alternatives while preserving the quality of novel
view synthesis. For example, Efficient4D takes only 10 minutes to model a
dynamic object, vs 120 minutes by the previous art model Consistent4D.",2024-01-16 18:58:36+00:00,"['Zijie Pan', 'Zeyu Yang', 'Xiatian Zhu', 'Li Zhang']",http://arxiv.org/abs/2401.08742v3
EPRecon: An Efficient Framework for Real-Time Panoptic 3D Reconstruction from Monocular Video,"Panoptic 3D reconstruction from a monocular video is a fundamental perceptual
task in robotic scene understanding. However, existing efforts suffer from
inefficiency in terms of inference speed and accuracy, limiting their practical
applicability. We present EPRecon, an efficient real-time panoptic 3D
reconstruction framework. Current volumetric-based reconstruction methods
usually utilize multi-view depth map fusion to obtain scene depth priors, which
is time-consuming and poses challenges to real-time scene reconstruction. To
address this issue, we propose a lightweight module to directly estimate scene
depth priors in a 3D volume for reconstruction quality improvement by
generating occupancy probabilities of all voxels. In addition, compared with
existing panoptic segmentation methods, EPRecon extracts panoptic features from
both voxel features and corresponding image features, obtaining more detailed
and comprehensive instance-level semantic information and achieving more
accurate segmentation results. Experimental results on the ScanNetV2 dataset
demonstrate the superiority of EPRecon over current state-of-the-art methods in
terms of both panoptic 3D reconstruction quality and real-time inference. Code
is available at https://github.com/zhen6618/EPRecon.",2024-09-03 11:40:31+00:00,"['Zhen Zhou', 'Yunkai Ma', 'Junfeng Fan', 'Shaolin Zhang', 'Fengshui Jing', 'Min Tan']",http://arxiv.org/abs/2409.01807v2
A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models,"Diffusion models have shown remarkable performance in generation problems
over various domains including images, videos, text, and audio. A practical
bottleneck of diffusion models is their sampling speed, due to the repeated
evaluation of score estimation networks during the inference. In this work, we
propose a novel framework capable of adaptively allocating compute required for
the score estimation, thereby reducing the overall sampling time of diffusion
models. We observe that the amount of computation required for the score
estimation may vary along the time step for which the score is estimated. Based
on this observation, we propose an early-exiting scheme, where we skip the
subset of parameters in the score estimation network during the inference,
based on a time-dependent exit schedule. Using the diffusion models for image
synthesis, we show that our method could significantly improve the sampling
throughput of the diffusion models without compromising image quality.
Furthermore, we also demonstrate that our method seamlessly integrates with
various types of solvers for faster sampling, capitalizing on their
compatibility to enhance overall efficiency. The source code and our
experiments are available at \url{https://github.com/taehong-moon/ee-diffusion}",2024-08-12 05:33:45+00:00,"['Taehong Moon', 'Moonseok Choi', 'EungGu Yun', 'Jongmin Yoon', 'Gayoung Lee', 'Jaewoong Cho', 'Juho Lee']",http://arxiv.org/abs/2408.05927v1
QID$^2$: An Image-Conditioned Diffusion Model for Q-space Up-sampling of DWI Data,"We propose an image-conditioned diffusion model to estimate high angular
resolution diffusion weighted imaging (DWI) from a low angular resolution
acquisition. Our model, which we call QID$^2$, takes as input a set of low
angular resolution DWI data and uses this information to estimate the DWI data
associated with a target gradient direction. We leverage a U-Net architecture
with cross-attention to preserve the positional information of the reference
images, further guiding the target image generation. We train and evaluate
QID$^2$ on single-shell DWI samples curated from the Human Connectome Project
(HCP) dataset. Specifically, we sub-sample the HCP gradient directions to
produce low angular resolution DWI data and train QID$^2$ to reconstruct the
missing high angular resolution samples. We compare QID$^2$ with two
state-of-the-art GAN models. Our results demonstrate that QID$^2$ not only
achieves higher-quality generated images, but it consistently outperforms the
GAN models in downstream tensor estimation across multiple metrics. Taken
together, this study highlights the potential of diffusion models, and QID$^2$
in particular, for q-space up-sampling, thus offering a promising toolkit for
clinical and research applications.",2024-09-03 21:39:58+00:00,"['Zijian Chen', 'Jueqi Wang', 'Archana Venkataraman']",http://arxiv.org/abs/2409.02309v1
GaRField++: Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene Reconstruction,"This paper proposes a novel framework for large-scale scene reconstruction
based on 3D Gaussian splatting (3DGS) and aims to address the scalability and
accuracy challenges faced by existing methods. For tackling the scalability
issue, we split the large scene into multiple cells, and the candidate
point-cloud and camera views of each cell are correlated through a
visibility-based camera selection and a progressive point-cloud extension. To
reinforce the rendering quality, three highlighted improvements are made in
comparison with vanilla 3DGS, which are a strategy of the ray-Gaussian
intersection and the novel Gaussians density control for learning efficiency,
an appearance decoupling module based on ConvKAN network to solve uneven
lighting conditions in large-scale scenes, and a refined final loss with the
color loss, the depth distortion loss, and the normal consistency loss.
Finally, the seamless stitching procedure is executed to merge the individual
Gaussian radiance field for novel view synthesis across different cells.
Evaluation of Mill19, Urban3D, and MatrixCity datasets shows that our method
consistently generates more high-fidelity rendering results than
state-of-the-art methods of large-scale scene reconstruction. We further
validate the generalizability of the proposed approach by rendering on
self-collected video clips recorded by a commercial drone.",2024-09-19 13:43:31+00:00,"['Hanyue Zhang', 'Zhiliu Yang', 'Xinhe Zuo', 'Yuxin Tong', 'Ying Long', 'Chen Liu']",http://arxiv.org/abs/2409.12774v3
Instruction-based Image Manipulation by Watching How Things Move,"This paper introduces a novel dataset construction pipeline that samples
pairs of frames from videos and uses multimodal large language models (MLLMs)
to generate editing instructions for training instruction-based image
manipulation models. Video frames inherently preserve the identity of subjects
and scenes, ensuring consistent content preservation during editing.
Additionally, video data captures diverse, natural dynamics-such as non-rigid
subject motion and complex camera movements-that are difficult to model
otherwise, making it an ideal source for scalable dataset construction. Using
this approach, we create a new dataset to train InstructMove, a model capable
of instruction-based complex manipulations that are difficult to achieve with
synthetically generated datasets. Our model demonstrates state-of-the-art
performance in tasks such as adjusting subject poses, rearranging elements, and
altering camera perspectives.",2024-12-16 18:56:17+00:00,"['Mingdeng Cao', 'Xuaner Zhang', 'Yinqiang Zheng', 'Zhihao Xia']",http://arxiv.org/abs/2412.12087v1
Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations,"Recent advancements in robotics have focused on developing generalist
policies capable of performing multiple tasks. Typically, these policies
utilize pre-trained vision encoders to capture crucial information from current
observations. However, previous vision encoders, which trained on two-image
contrastive learning or single-image reconstruction, can not perfectly capture
the sequential information essential for embodied tasks. Recently, video
diffusion models (VDMs) have demonstrated the capability to accurately predict
future image sequences, exhibiting a good understanding of physical dynamics.
Motivated by the strong visual prediction capabilities of VDMs, we hypothesize
that they inherently possess visual representations that reflect the evolution
of the physical world, which we term predictive visual representations.
Building on this hypothesis, we propose the Video Prediction Policy (VPP), a
generalist robotic policy conditioned on the predictive visual representations
from VDMs. To further enhance these representations, we incorporate diverse
human or robotic manipulation datasets, employing unified video-generation
training objectives. VPP consistently outperforms existing methods across two
simulated and two real-world benchmarks. Notably, it achieves a 28.1\% relative
improvement in the Calvin ABC-D benchmark compared to the previous
state-of-the-art and delivers a 28.8\% increase in success rates for complex
real-world dexterous manipulation tasks.",2024-12-19 12:48:40+00:00,"['Yucheng Hu', 'Yanjiang Guo', 'Pengchao Wang', 'Xiaoyu Chen', 'Yen-Jen Wang', 'Jianke Zhang', 'Koushil Sreenath', 'Chaochao Lu', 'Jianyu Chen']",http://arxiv.org/abs/2412.14803v1
Generative Inbetweening through Frame-wise Conditions-Driven Video Generation,"Generative inbetweening aims to generate intermediate frame sequences by
utilizing two key frames as input. Although remarkable progress has been made
in video generation models, generative inbetweening still faces challenges in
maintaining temporal stability due to the ambiguous interpolation path between
two key frames. This issue becomes particularly severe when there is a large
motion gap between input frames. In this paper, we propose a straightforward
yet highly effective Frame-wise Conditions-driven Video Generation (FCVG)
method that significantly enhances the temporal stability of interpolated video
frames. Specifically, our FCVG provides an explicit condition for each frame,
making it much easier to identify the interpolation path between two input
frames and thus ensuring temporally stable production of visually plausible
video frames. To achieve this, we suggest extracting matched lines from two
input frames that can then be easily interpolated frame by frame, serving as
frame-wise conditions seamlessly integrated into existing video generation
models. In extensive evaluations covering diverse scenarios such as natural
landscapes, complex human poses, camera movements and animations, existing
methods often exhibit incoherent transitions across frames. In contrast, our
FCVG demonstrates the capability to generate temporally stable videos using
both linear and non-linear interpolation curves. Our project page and code are
available at \url{https://fcvg-inbetween.github.io/}.",2024-12-16 13:19:41+00:00,"['Tianyi Zhu', 'Dongwei Ren', 'Qilong Wang', 'Xiaohe Wu', 'Wangmeng Zuo']",http://arxiv.org/abs/2412.11755v1
MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks,"The progress in generative models, particularly Generative Adversarial
Networks (GANs), opened new possibilities for image generation but raised
concerns about potential malicious uses, especially in sensitive areas like
medical imaging. This study introduces MITS-GAN, a novel approach to prevent
tampering in medical images, with a specific focus on CT scans. The approach
disrupts the output of the attacker's CT-GAN architecture by introducing finely
tuned perturbations that are imperceptible to the human eye. Specifically, the
proposed approach involves the introduction of appropriate Gaussian noise to
the input as a protective measure against various attacks. Our method aims to
enhance tamper resistance, comparing favorably to existing techniques.
Experimental results on a CT scan demonstrate MITS-GAN's superior performance,
emphasizing its ability to generate tamper-resistant images with negligible
artifacts. As image tampering in medical domains poses life-threatening risks,
our proactive approach contributes to the responsible and ethical use of
generative models. This work provides a foundation for future research in
countering cyber threats in medical imaging. Models and codes are publicly
available on https://iplab.dmi.unict.it/MITS-GAN-2024/.",2024-01-17 22:30:41+00:00,"['Giovanni Pasqualino', 'Luca Guarnera', 'Alessandro Ortis', 'Sebastiano Battiato']",http://arxiv.org/abs/2401.09624v2
Multi-scale Conditional Generative Modeling for Microscopic Image Restoration,"The advance of diffusion-based generative models in recent years has
revolutionized state-of-the-art (SOTA) techniques in a wide variety of image
analysis and synthesis tasks, whereas their adaptation on image restoration,
particularly within computational microscopy remains theoretically and
empirically underexplored. In this research, we introduce a multi-scale
generative model that enhances conditional image restoration through a novel
exploitation of the Brownian Bridge process within wavelet domain. By
initiating the Brownian Bridge diffusion process specifically at the
lowest-frequency subband and applying generative adversarial networks at
subsequent multi-scale high-frequency subbands in the wavelet domain, our
method provides significant acceleration during training and sampling while
sustaining a high image generation quality and diversity on par with SOTA
diffusion models. Experimental results on various computational microscopy and
imaging tasks confirm our method's robust performance and its considerable
reduction in its sampling steps and time. This pioneering technique offers an
efficient image restoration framework that harmonizes efficiency with quality,
signifying a major stride in incorporating cutting-edge generative models into
computational microscopy workflows.",2024-07-07 05:11:00+00:00,"['Luzhe Huang', 'Xiongye Xiao', 'Shixuan Li', 'Jiawen Sun', 'Yi Huang', 'Aydogan Ozcan', 'Paul Bogdan']",http://arxiv.org/abs/2407.05259v1
Scaling Laws For Diffusion Transformers,"Diffusion transformers (DiT) have already achieved appealing synthesis and
scaling properties in content recreation, e.g., image and video generation.
However, scaling laws of DiT are less explored, which usually offer precise
predictions regarding optimal model size and data requirements given a specific
compute budget. Therefore, experiments across a broad range of compute budgets,
from 1e17 to 6e18 FLOPs are conducted to confirm the existence of scaling laws
in DiT for the first time. Concretely, the loss of pretraining DiT also follows
a power-law relationship with the involved compute. Based on the scaling law,
we can not only determine the optimal model size and required data but also
accurately predict the text-to-image generation loss given a model with 1B
parameters and a compute budget of 1e21 FLOPs. Additionally, we also
demonstrate that the trend of pre-training loss matches the generation
performances (e.g., FID), even across various datasets, which complements the
mapping from compute to synthesis quality and thus provides a predictable
benchmark that assesses model performance and data quality at a reduced cost.",2024-10-10 17:56:03+00:00,"['Zhengyang Liang', 'Hao He', 'Ceyuan Yang', 'Bo Dai']",http://arxiv.org/abs/2410.08184v1
Accelerating Diffusion for SAR-to-Optical Image Translation via Adversarial Consistency Distillation,"Synthetic Aperture Radar (SAR) provides all-weather, high-resolution imaging
capabilities, but its unique imaging mechanism often requires expert
interpretation, limiting its widespread applicability. Translating SAR images
into more easily recognizable optical images using diffusion models helps
address this challenge. However, diffusion models suffer from high latency due
to numerous iterative inferences, while Generative Adversarial Networks (GANs)
can achieve image translation with just a single iteration but often at the
cost of image quality. To overcome these issues, we propose a new training
framework for SAR-to-optical image translation that combines the strengths of
both approaches. Our method employs consistency distillation to reduce
iterative inference steps and integrates adversarial learning to ensure image
clarity and minimize color shifts. Additionally, our approach allows for a
trade-off between quality and speed, providing flexibility based on application
requirements. We conducted experiments on SEN12 and GF3 datasets, performing
quantitative evaluations using Peak Signal-to-Noise Ratio (PSNR), Structural
Similarity Index (SSIM), and Frechet Inception Distance (FID), as well as
calculating the inference latency. The results demonstrate that our approach
significantly improves inference speed by 131 times while maintaining the
visual quality of the generated images, thus offering a robust and efficient
solution for SAR-to-optical image translation.",2024-07-08 16:36:12+00:00,"['Xinyu Bai', 'Feng Xu']",http://arxiv.org/abs/2407.06095v1
LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field,"Cinemagraph is a unique form of visual media that combines elements of still
photography and subtle motion to create a captivating experience. However, the
majority of videos generated by recent works lack depth information and are
confined to the constraints of 2D image space. In this paper, inspired by
significant progress in the field of novel view synthesis (NVS) achieved by 3D
Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from
2D image space to 3D space using 3D Gaussian modeling. To achieve this, we
first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from
multi-view images of static scenes,incorporating shape regularization terms to
prevent blurring or artifacts caused by object deformation. We then adopt an
autoencoder tailored for 3D Gaussian to project it into feature space. To
maintain the local continuity of the scene, we devise SuperGaussian for
clustering based on the acquired features. By calculating the similarity
between clusters and employing a two-stage estimation method, we derive an
Eulerian motion field to describe velocities across the entire scene. The 3D
Gaussian points then move within the estimated Eulerian motion field. Through
bidirectional animation techniques, we ultimately generate a 3D Cinemagraph
that exhibits natural and seamlessly loopable dynamics. Experiment results
validate the effectiveness of our approach, demonstrating high-quality and
visually appealing scene generation. The project is available at
https://pokerlishao.github.io/LoopGaussian/.",2024-04-13 11:07:53+00:00,"['Jiyang Li', 'Lechao Cheng', 'Zhangye Wang', 'Tingting Mu', 'Jingxuan He']",http://arxiv.org/abs/2404.08966v2
Diff4MMLiTS: Advanced Multimodal Liver Tumor Segmentation via Diffusion-Based Image Synthesis and Alignment,"Multimodal learning has been demonstrated to enhance performance across
various clinical tasks, owing to the diverse perspectives offered by different
modalities of data. However, existing multimodal segmentation methods rely on
well-registered multimodal data, which is unrealistic for real-world clinical
images, particularly for indistinct and diffuse regions such as liver tumors.
In this paper, we introduce Diff4MMLiTS, a four-stage multimodal liver tumor
segmentation pipeline: pre-registration of the target organs in multimodal CTs;
dilation of the annotated modality's mask and followed by its use in inpainting
to obtain multimodal normal CTs without tumors; synthesis of strictly aligned
multimodal CTs with tumors using the latent diffusion model based on multimodal
CT features and randomly generated tumor masks; and finally, training the
segmentation model, thus eliminating the need for strictly aligned multimodal
data. Extensive experiments on public and internal datasets demonstrate the
superiority of Diff4MMLiTS over other state-of-the-art multimodal segmentation
methods.",2024-12-29 09:55:00+00:00,"['Shiyun Chen', 'Li Lin', 'Pujin Cheng', 'ZhiCheng Jin', 'JianJian Chen', 'HaiDong Zhu', 'Kenneth K. Y. Wong', 'Xiaoying Tang']",http://arxiv.org/abs/2412.20418v1
Towards Accurate Lip-to-Speech Synthesis in-the-Wild,"In this paper, we introduce a novel approach to address the task of
synthesizing speech from silent videos of any in-the-wild speaker solely based
on lip movements. The traditional approach of directly generating speech from
lip videos faces the challenge of not being able to learn a robust language
model from speech alone, resulting in unsatisfactory outcomes. To overcome this
issue, we propose incorporating noisy text supervision using a state-of-the-art
lip-to-text network that instills language information into our model. The
noisy text is generated using a pre-trained lip-to-text model, enabling our
approach to work without text annotations during inference. We design a visual
text-to-speech network that utilizes the visual stream to generate accurate
speech, which is in-sync with the silent input video. We perform extensive
experiments and ablation studies, demonstrating our approach's superiority over
the current state-of-the-art methods on various benchmark datasets. Further, we
demonstrate an essential practical application of our method in assistive
technology by generating speech for an ALS patient who has lost the voice but
can make mouth movements. Our demo video, code, and additional details can be
found at
\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw}.",2024-03-02 04:07:24+00:00,"['Sindhu Hegde', 'Rudrabha Mukhopadhyay', 'C. V. Jawahar', 'Vinay Namboodiri']",http://arxiv.org/abs/2403.01087v1
High-fidelity and Lip-synced Talking Face Synthesis via Landmark-based Diffusion Model,"Audio-driven talking face video generation has attracted increasing attention
due to its huge industrial potential. Some previous methods focus on learning a
direct mapping from audio to visual content. Despite progress, they often
struggle with the ambiguity of the mapping process, leading to flawed results.
An alternative strategy involves facial structural representations (e.g.,
facial landmarks) as intermediaries. This multi-stage approach better preserves
the appearance details but suffers from error accumulation due to the
independent optimization of different stages. Moreover, most previous methods
rely on generative adversarial networks, prone to training instability and mode
collapse. To address these challenges, our study proposes a novel
landmark-based diffusion model for talking face generation, which leverages
facial landmarks as intermediate representations while enabling end-to-end
optimization. Specifically, we first establish the less ambiguous mapping from
audio to landmark motion of lip and jaw. Then, we introduce an innovative
conditioning module called TalkFormer to align the synthesized motion with the
motion represented by landmarks via differentiable cross-attention, which
enables end-to-end optimization for improved lip synchronization. Besides,
TalkFormer employs implicit feature warping to align the reference image
features with the target motion for preserving more appearance details.
Extensive experiments demonstrate that our approach can synthesize
high-fidelity and lip-synced talking face videos, preserving more subject
appearance details from the reference image.",2024-08-10 02:58:28+00:00,"['Weizhi Zhong', 'Junfan Lin', 'Peixin Chen', 'Liang Lin', 'Guanbin Li']",http://arxiv.org/abs/2408.05416v1
TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning,"Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in short video understanding. However, understanding long-form
videos still remains challenging for MLLMs. This paper proposes TimeSuite, a
collection of new designs to adapt the existing short-form video MLLMs for long
video understanding, including a simple yet efficient framework to process long
video sequence, a high-quality video dataset for grounded tuning of MLLMs, and
a carefully-designed instruction tuning task to explicitly incorporate the
grounding supervision in the traditional QA format. Specifically, based on
VideoChat, we propose our long-video MLLM, coined as VideoChat-T, by
implementing a token shuffling to compress long video tokens and introducing
Temporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of
visual representation. Meanwhile, we introduce the TimePro, a comprehensive
grounding-centric instruction tuning dataset composed of 9 tasks and 349k
high-quality grounded annotations. Notably, we design a new instruction tuning
task type, called Temporal Grounded Caption, to peform detailed video
descriptions with the corresponding time stamps prediction. This explicit
temporal location prediction will guide MLLM to correctly attend on the visual
content when generating description, and thus reduce the hallucination risk
caused by the LLMs. Experimental results demonstrate that our TimeSuite
provides a successful solution to enhance the long video understanding
capability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the
benchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T
exhibits robust zero-shot temporal grounding capabilities, significantly
outperforming the existing state-of-the-art MLLMs. After fine-tuning, it
performs on par with the traditional supervised expert models.",2024-10-25 17:19:55+00:00,"['Xiangyu Zeng', 'Kunchang Li', 'Chenting Wang', 'Xinhao Li', 'Tianxiang Jiang', 'Ziang Yan', 'Songze Li', 'Yansong Shi', 'Zhengrong Yue', 'Yi Wang', 'Yali Wang', 'Yu Qiao', 'Limin Wang']",http://arxiv.org/abs/2410.19702v2
Knowledge Guided Entity-aware Video Captioning and A Basketball Benchmark,"Despite the recent emergence of video captioning models, how to generate the
text description with specific entity names and fine-grained actions is far
from being solved, which however has great applications such as basketball live
text broadcast. In this paper, a new multimodal knowledge graph supported
basketball benchmark for video captioning is proposed. Specifically, we
construct a multimodal basketball game knowledge graph (KG_NBA_2022) to provide
additional knowledge beyond videos. Then, a multimodal basketball game video
captioning (VC_NBA_2022) dataset that contains 9 types of fine-grained shooting
events and 286 players' knowledge (i.e., images and names) is constructed based
on KG_NBA_2022. We develop a knowledge guided entity-aware video captioning
network (KEANet) based on a candidate player list in encoder-decoder form for
basketball live text broadcast. The temporal contextual information in video is
encoded by introducing the bi-directional GRU (Bi-GRU) module. And the
entity-aware module is designed to model the relationships among the players
and highlight the key players. Extensive experiments on multiple sports
benchmarks demonstrate that KEANet effectively leverages extera knowledge and
outperforms advanced video captioning models. The proposed dataset and
corresponding codes will be publicly available soon",2024-01-25 02:08:37+00:00,"['Zeyu Xi', 'Ge Shi', 'Xuefen Li', 'Junchi Yan', 'Zun Li', 'Lifang Wu', 'Zilin Liu', 'Liang Wang']",http://arxiv.org/abs/2401.13888v2
IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video Action Counting,"Video Action Counting (VAC) is crucial in analyzing sports, fitness, and
everyday activities by quantifying repetitive actions in videos. However,
traditional VAC methods have overlooked the complexity of action repetitions,
such as interruptions and the variability in cycle duration. Our research
addresses the shortfall by introducing a novel approach to VAC, called
Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular
repetition patterns in videos, which we define through two primary aspects:
Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle
Consistency ensures homogeneity in the spatial-temporal representations of
cycle segments, signifying action uniformity within cycles. Cycle-interval
inconsistency highlights the importance of distinguishing between cycle
segments and intervals based on their inherent content differences. To
encapsulate these principles, we propose a new methodology that includes
consistency and inconsistency modules, supported by a unique pull-push loss
(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence
among cycle segment features and a push loss to clearly distinguish features of
cycle segments from interval segments. Empirical evaluations conducted on the
RepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in
VAC task performance. Furthermore, the model demonstrates exceptional
adaptability and generalization across various video contents, outperforming
existing models on two additional datasets, UCFRep and Countix, without the
need for dataset-specific optimization. These results confirm the efficacy of
our approach in addressing irregular repetitions in videos and pave the way for
further advancements in video analysis and understanding.",2024-03-18 16:56:47+00:00,"['Hang Wang', 'Zhi-Qi Cheng', 'Youtian Du', 'Lei Zhang']",http://arxiv.org/abs/2403.11959v2
Fast Encoder-Based 3D from Casual Videos via Point Track Processing,"This paper addresses the long-standing challenge of reconstructing 3D
structures from videos with dynamic content. Current approaches to this problem
were not designed to operate on casual videos recorded by standard cameras or
require a long optimization time.
  Aiming to significantly improve the efficiency of previous approaches, we
present TracksTo4D, a learning-based approach that enables inferring 3D
structure and camera positions from dynamic content originating from casual
videos using a single efficient feed-forward pass. To achieve this, we propose
operating directly over 2D point tracks as input and designing an architecture
tailored for processing 2D point tracks. Our proposed architecture is designed
with two key principles in mind: (1) it takes into account the inherent
symmetries present in the input point tracks data, and (2) it assumes that the
movement patterns can be effectively represented using a low-rank
approximation. TracksTo4D is trained in an unsupervised way on a dataset of
casual videos utilizing only the 2D point tracks extracted from the videos,
without any 3D supervision. Our experiments show that TracksTo4D can
reconstruct a temporal point cloud and camera positions of the underlying video
with accuracy comparable to state-of-the-art methods, while drastically
reducing runtime by up to 95\%. We further show that TracksTo4D generalizes
well to unseen videos of unseen semantic categories at inference time.",2024-04-10 15:37:00+00:00,"['Yoni Kasten', 'Wuyue Lu', 'Haggai Maron']",http://arxiv.org/abs/2404.07097v2
Beyond Alignment: Blind Video Face Restoration via Parsing-Guided Temporal-Coherent Transformer,"Multiple complex degradations are coupled in low-quality video faces in the
real world. Therefore, blind video face restoration is a highly challenging
ill-posed problem, requiring not only hallucinating high-fidelity details but
also enhancing temporal coherence across diverse pose variations. Restoring
each frame independently in a naive manner inevitably introduces temporal
incoherence and artifacts from pose changes and keypoint localization errors.
To address this, we propose the first blind video face restoration approach
with a novel parsing-guided temporal-coherent transformer (PGTFormer) without
pre-alignment. PGTFormer leverages semantic parsing guidance to select optimal
face priors for generating temporally coherent artifact-free results.
Specifically, we pre-train a temporal-spatial vector quantized auto-encoder on
high-quality video face datasets to extract expressive context-rich priors.
Then, the temporal parse-guided codebook predictor (TPCP) restores faces in
different poses based on face parsing context cues without performing face
pre-alignment. This strategy reduces artifacts and mitigates jitter caused by
cumulative errors from face pre-alignment. Finally, the temporal fidelity
regulator (TFR) enhances fidelity through temporal feature interaction and
improves video temporal consistency. Extensive experiments on face videos show
that our method outperforms previous face restoration baselines. The code will
be released on
\href{https://github.com/kepengxu/PGTFormer}{https://github.com/kepengxu/PGTFormer}.",2024-04-21 12:33:07+00:00,"['Kepeng Xu', 'Li Xu', 'Gang He', 'Wenxin Yu', 'Yunsong Li']",http://arxiv.org/abs/2404.13640v1
Unveiling the Potential: Harnessing Deep Metric Learning to Circumvent Video Streaming Encryption,"Encryption on the internet with the shift to HTTPS has been an important step
to improve the privacy of internet users. However, there is an increasing body
of work about extracting information from encrypted internet traffic without
having to decrypt it. Such attacks bypass security guarantees assumed to be
given by HTTPS and thus need to be understood. Prior works showed that the
variable bitrates of video streams are sufficient to identify which video
someone is watching. These works generally have to make trade-offs in aspects
such as accuracy, scalability, robustness, etc. These trade-offs complicate the
practical use of these attacks. To that end, we propose a deep metric learning
framework based on the triplet loss method. Through this framework, we achieve
robust, generalisable, scalable and transferable encrypted video stream
detection. First, the triplet loss is better able to deal with video streams
not seen during training. Second, our approach can accurately classify videos
not seen during training. Third, we show that our method scales well to a
dataset of over 1000 videos. Finally, we show that a model trained on video
streams over Chrome can also classify streams over Firefox. Our results suggest
that this side-channel attack is more broadly applicable than originally
thought. We provide our code alongside a diverse and up-to-date dataset for
future research.",2024-05-16 08:49:05+00:00,"['Arwin Gansekoele', 'Tycho Bot', 'Rob van der Mei', 'Sandjai Bhulai', 'Mark Hoogendoorn']",http://arxiv.org/abs/2405.09902v1
video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models,"Speech understanding as an element of the more generic video understanding
using audio-visual large language models (av-LLMs) is a crucial yet
understudied aspect. This paper proposes video-SALMONN, a single end-to-end
av-LLM for video processing, which can understand not only visual frame
sequences, audio events and music, but speech as well. To obtain fine-grained
temporal information required by speech understanding, while keeping efficient
for other video elements, this paper proposes a novel multi-resolution causal
Q-Former (MRC Q-Former) structure to connect pre-trained audio-visual encoders
and the backbone large language model. Moreover, dedicated training approaches
including the diversity loss and the unpaired audio-visual mixed training
scheme are proposed to avoid frames or modality dominance. On the introduced
speech-audio-visual evaluation benchmark, video-SALMONN achieves more than 25\%
absolute accuracy improvements on the video-QA task and over 30\% absolute
accuracy improvements on audio-visual QA tasks with human speech. In addition,
video-SALMONN demonstrates remarkable video comprehension and reasoning
abilities on tasks that are unprecedented by other av-LLMs. Our training code
and model checkpoints are available at
\texttt{\url{https://github.com/bytedance/SALMONN/}}.",2024-06-22 01:36:11+00:00,"['Guangzhi Sun', 'Wenyi Yu', 'Changli Tang', 'Xianzhao Chen', 'Tian Tan', 'Wei Li', 'Lu Lu', 'Zejun Ma', 'Yuxuan Wang', 'Chao Zhang']",http://arxiv.org/abs/2406.15704v1
Hypergraph Multi-modal Large Language Model: Exploiting EEG and Eye-tracking Modalities to Evaluate Heterogeneous Responses for Video Understanding,"Understanding of video creativity and content often varies among individuals,
with differences in focal points and cognitive levels across different ages,
experiences, and genders. There is currently a lack of research in this area,
and most existing benchmarks suffer from several drawbacks: 1) a limited number
of modalities and answers with restrictive length; 2) the content and scenarios
within the videos are excessively monotonous, transmitting allegories and
emotions that are overly simplistic. To bridge the gap to real-world
applications, we introduce a large-scale Subjective Response Indicators for
Advertisement Videos dataset, namely SRI-ADV. Specifically, we collected real
changes in Electroencephalographic (EEG) and eye-tracking regions from
different demographics while they viewed identical video content. Utilizing
this multi-modal dataset, we developed tasks and protocols to analyze and
evaluate the extent of cognitive understanding of video content among different
users. Along with the dataset, we designed a Hypergraph Multi-modal Large
Language Model (HMLLM) to explore the associations among different
demographics, video elements, EEG, and eye-tracking indicators. HMLLM could
bridge semantic gaps across rich modalities and integrate information beyond
different modalities to perform logical reasoning. Extensive experimental
evaluations on SRI-ADV and other additional video-based generative performance
benchmarks demonstrate the effectiveness of our method. The codes and dataset
will be released at https://github.com/mininglamp-MLLM/HMLLM.",2024-07-11 03:00:26+00:00,"['Minghui Wu', 'Chenxu Zhao', 'Anyang Su', 'Donglin Di', 'Tianyu Fu', 'Da An', 'Min He', 'Ya Gao', 'Meng Ma', 'Kun Yan', 'Ping Wang']",http://arxiv.org/abs/2407.08150v3
SIGMA: Sinkhorn-Guided Masked Video Modeling,"Video-based pretraining offers immense potential for learning strong visual
representations on an unprecedented scale. Recently, masked video modeling
methods have shown promising scalability, yet fall short in capturing
higher-level semantics due to reconstructing predefined low-level targets such
as pixels. To tackle this, we present Sinkhorn-guided Masked Video Modelling
(SIGMA), a novel video pretraining method that jointly learns the video model
in addition to a target feature space using a projection network. However, this
simple modification means that the regular L2 reconstruction loss will lead to
trivial solutions as both networks are jointly optimized. As a solution, we
distribute features of space-time tubes evenly across a limited number of
learnable clusters. By posing this as an optimal transport problem, we enforce
high entropy in the generated features across the batch, infusing semantic and
temporal meaning into the feature space. The resulting cluster assignments are
used as targets for a symmetric prediction task where the video model predicts
cluster assignment of the projection network and vice versa. Experimental
results on ten datasets across three benchmarks validate the effectiveness of
SIGMA in learning more performant, temporally-aware, and robust video
representations improving upon state-of-the-art methods. Our project website
with code is available at: https://quva-lab.github.io/SIGMA.",2024-07-22 08:04:09+00:00,"['Mohammadreza Salehi', 'Michael Dorkenwald', 'Fida Mohammad Thoker', 'Efstratios Gavves', 'Cees G. M. Snoek', 'Yuki M. Asano']",http://arxiv.org/abs/2407.15447v1
Explore Cross-Codec Quality-Rate Convex Hulls Relation for Adaptive Streaming,"With the ongoing advancement of video technology and the emergence of new
video platforms, suppliers of video contents are striving to ensure that the
video quality meets the desire of consumers. Accessing a limited amount of
channel bandwidth, they are often looking for a novel approach to decrease the
use of data and thus the required energy and cost. This study evaluates the
Quality Rate performance of H.264, H.265, and VP9 codecs across resolutions
(960*544, 1920*1080, 3840*2160) to optimize video quality while minimizing
bitrate, crucial for energy and cost efficiency. At this approach, original
videos at native resolutions were encoded, decoded, and rescaled using FFmpeg.
For each resolution, encoding and decoding were performed at various
quantization levels. Quality Rate (QR) curves were generated using PSNR and
VMAF metric against bitrate. Convex Hull curves were then derived and
mathematically modelled for each resolution. The procedure was systematically
applied to H.264, H.265, and VP9 codecs. Results indicate that increasing CRF
values reduce bitrate, PSNR, and VMAF, with PSNR ranging between 20-40 dB.
Logarithmic polynomial modelling of convex hulls demonstrated high accuracy,
with low RMSE and high R-Squared values. These findings suggest that the convex
hull of one codec can predict the performance of others, aiding future
content-driven prediction methodologies and enhancing adaptive streaming
efficiency. Keywords: Video Codecs, Adaptive Streaming, Compression, Bitrate,
PSNR, VMAF, H.264, H.265, VP9",2024-08-16 22:52:00+00:00,['Masoumeh Farhadi Nia'],http://arxiv.org/abs/2408.09044v1
An Evaluation of Large Pre-Trained Models for Gesture Recognition using Synthetic Videos,"In this work, we explore the possibility of using synthetically generated
data for video-based gesture recognition with large pre-trained models. We
consider whether these models have sufficiently robust and expressive
representation spaces to enable ""training-free"" classification. Specifically,
we utilize various state-of-the-art video encoders to extract features for use
in k-nearest neighbors classification, where the training data points are
derived from synthetic videos only. We compare these results with another
training-free approach -- zero-shot classification using text descriptions of
each gesture. In our experiments with the RoCoG-v2 dataset, we find that using
synthetic training videos yields significantly lower classification accuracy on
real test videos compared to using a relatively small number of real training
videos. We also observe that video backbones that were fine-tuned on
classification tasks serve as superior feature extractors, and that the choice
of fine-tuning data has a substantial impact on k-nearest neighbors
performance. Lastly, we find that zero-shot text-based classification performs
poorly on the gesture recognition task, as gestures are not easily described
through natural language.",2024-10-03 02:31:14+00:00,"['Arun Reddy', 'Ketul Shah', 'Corban Rivera', 'William Paul', 'Celso M. De Melo', 'Rama Chellappa']",http://arxiv.org/abs/2410.02152v1
Data Playwright: Authoring Data Videos with Annotated Narration,"Creating data videos that effectively narrate stories with animated visuals
requires substantial effort and expertise. A promising research trend is
leveraging the easy-to-use natural language (NL) interaction to automatically
synthesize data video components from narrative content like text narrations,
or NL commands that specify user-required designs. Nevertheless, previous
research has overlooked the integration of narrative content and specific
design authoring commands, leading to generated results that lack customization
or fail to seamlessly fit into the narrative context. To address these issues,
we introduce a novel paradigm for creating data videos, which seamlessly
integrates users' authoring and narrative intents in a unified format called
annotated narration, allowing users to incorporate NL commands for design
authoring as inline annotations within the narration text. Informed by a
formative study on users' preference for annotated narration, we develop a
prototype system named Data Playwright that embodies this paradigm for
effective creation of data videos. Within Data Playwright, users can write
annotated narration based on uploaded visualizations. The system's interpreter
automatically understands users' inputs and synthesizes data videos with
narration-animation interplay, powered by large language models. Finally, users
can preview and fine-tune the video. A user study demonstrated that
participants can effectively create data videos with Data Playwright by
effortlessly articulating their desired outcomes through annotated narration.",2024-10-04 02:34:53+00:00,"['Leixian Shen', 'Haotian Li', 'Yun Wang', 'Tianqi Luo', 'Yuyu Luo', 'Huamin Qu']",http://arxiv.org/abs/2410.03093v1
Video Quality Assessment: A Comprehensive Survey,"Video quality assessment (VQA) is an important processing task, aiming at
predicting the quality of videos in a manner highly consistent with human
judgments of perceived quality. Traditional VQA models based on natural image
and/or video statistics, which are inspired both by models of projected images
of the real world and by dual models of the human visual system, deliver only
limited prediction performances on real-world user-generated content (UGC), as
exemplified in recent large-scale VQA databases containing large numbers of
diverse video contents crawled from the web. Fortunately, recent advances in
deep neural networks and Large Multimodality Models (LMMs) have enabled
significant progress in solving this problem, yielding better results than
prior handcrafted models. Numerous deep learning-based VQA models have been
developed, with progress in this direction driven by the creation of
content-diverse, large-scale human-labeled databases that supply ground truth
psychometric video quality data. Here, we present a comprehensive survey of
recent progress in the development of VQA algorithms and the benchmarking
studies and databases that make them possible. We also analyze open research
directions on study design and VQA algorithm architectures. Github link:
https://github.com/taco-group/Video-Quality-Assessment-A-Comprehensive-Survey.",2024-12-04 05:25:17+00:00,"['Qi Zheng', 'Yibo Fan', 'Leilei Huang', 'Tianyu Zhu', 'Jiaming Liu', 'Zhijian Hao', 'Shuo Xing', 'Chia-Ju Chen', 'Xiongkuo Min', 'Alan C. Bovik', 'Zhengzhong Tu']",http://arxiv.org/abs/2412.04508v2
BVI-CR: A Multi-View Human Dataset for Volumetric Video Compression,"The advances in immersive technologies and 3D reconstruction have enabled the
creation of digital replicas of real-world objects and environments with fine
details. These processes generate vast amounts of 3D data, requiring more
efficient compression methods to satisfy the memory and bandwidth constraints
associated with data storage and transmission. However, the development and
validation of efficient 3D data compression methods are constrained by the lack
of comprehensive and high-quality volumetric video datasets, which typically
require much more effort to acquire and consume increased resources compared to
2D image and video databases. To bridge this gap, we present an open multi-view
volumetric human dataset, denoted BVI-CR, which contains 18 multi-view RGB-D
captures and their corresponding textured polygonal meshes, depicting a range
of diverse human actions. Each video sequence contains 10 views in 1080p
resolution with durations between 10-15 seconds at 30FPS. Using BVI-CR, we
benchmarked three conventional and neural coordinate-based multi-view video
compression methods, following the MPEG MIV Common Test Conditions, and
reported their rate quality performance based on various quality metrics. The
results show the great potential of neural representation based methods in
volumetric video compression compared to conventional video coding methods
(with an up to 38\% average coding gain in PSNR). This dataset provides a
development and validation platform for a variety of tasks including volumetric
reconstruction, compression, and quality assessment. The database will be
shared publicly at \url{https://github.com/fan-aaron-zhang/bvi-cr}.",2024-11-17 23:22:48+00:00,"['Ge Gao', 'Adrian Azzarelli', 'Ho Man Kwan', 'Nantheera Anantrasirichai', 'Fan Zhang', 'Oliver Moolan-Feroze', 'David Bull']",http://arxiv.org/abs/2411.11199v1
LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment,"Recent advances in text-to-video (T2V) generative models have shown
impressive capabilities. However, these models are still inadequate in aligning
synthesized videos with human preferences (e.g., accurately reflecting text
descriptions), which is particularly difficult to address, as human preferences
are subjective and challenging to formalize as objective functions. Existing
studies train video quality assessment models that rely on human-annotated
ratings for video evaluation but overlook the reasoning behind evaluations,
limiting their ability to capture nuanced human criteria. Moreover, aligning
T2V model using video-based human feedback remains unexplored. Therefore, this
paper proposes LiFT, the first method designed to leverage human feedback for
T2V model alignment. Specifically, we first construct a Human Rating Annotation
dataset, LiFT-HRA, consisting of approximately 10k human annotations, each
including a score and its corresponding rationale. Based on this, we train a
reward model LiFT-Critic to learn reward function effectively, which serves as
a proxy for human judgment, measuring the alignment between given videos and
human expectations. Lastly, we leverage the learned reward function to align
the T2V model by maximizing the reward-weighted likelihood. As a case study, we
apply our pipeline to CogVideoX-2B, showing that the fine-tuned model
outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential
of human feedback in improving the alignment and quality of synthesized videos.",2024-12-06 07:16:14+00:00,"['Yibin Wang', 'Zhiyu Tan', 'Junyan Wang', 'Xiaomeng Yang', 'Cheng Jin', 'Hao Li']",http://arxiv.org/abs/2412.04814v3
DeblurGS: Gaussian Splatting for Camera Motion Blur,"Although significant progress has been made in reconstructing sharp 3D scenes
from motion-blurred images, a transition to real-world applications remains
challenging. The primary obstacle stems from the severe blur which leads to
inaccuracies in the acquisition of initial camera poses through
Structure-from-Motion, a critical aspect often overlooked by previous
approaches. To address this challenge, we propose DeblurGS, a method to
optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the
noisy camera pose initialization. We restore a fine-grained sharp scene by
leveraging the remarkable reconstruction capability of 3D Gaussian Splatting.
Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry
observation and synthesizes corresponding blurry renderings for the
optimization process. Furthermore, we propose Gaussian Densification Annealing
strategy to prevent the generation of inaccurate Gaussians at erroneous
locations during the early training stages when camera motion is still
imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves
state-of-the-art performance in deblurring and novel view synthesis for
real-world and synthetic benchmark datasets, as well as field-captured blurry
smartphone videos.",2024-04-17 13:14:52+00:00,"['Jeongtaek Oh', 'Jaeyoung Chung', 'Dongwoo Lee', 'Kyoung Mu Lee']",http://arxiv.org/abs/2404.11358v2
Pose Estimation from Camera Images for Underwater Inspection,"High-precision localization is pivotal in underwater reinspection missions.
Traditional localization methods like inertial navigation systems, Doppler
velocity loggers, and acoustic positioning face significant challenges and are
not cost-effective for some applications. Visual localization is a
cost-effective alternative in such cases, leveraging the cameras already
equipped on inspection vehicles to estimate poses from images of the
surrounding scene. Amongst these, machine learning-based pose estimation from
images shows promise in underwater environments, performing efficient
relocalization using models trained based on previously mapped scenes. We
explore the efficacy of learning-based pose estimators in both clear and turbid
water inspection missions, assessing the impact of image formats, model
architectures and training data diversity. We innovate by employing novel view
synthesis models to generate augmented training data, significantly enhancing
pose estimation in unexplored regions. Moreover, we enhance localization
accuracy by integrating pose estimator outputs with sensor data via an extended
Kalman filter, demonstrating improved trajectory smoothness and accuracy.",2024-07-24 03:00:53+00:00,"['Luyuan Peng', 'Hari Vishnu', 'Mandar Chitre', 'Yuen Min Too', 'Bharath Kalyan', 'Rajat Mishra', 'Soo Pieng Tan']",http://arxiv.org/abs/2407.16961v1
VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide,"Text-to-image (T2I) diffusion models have revolutionized visual content
creation, but extending these capabilities to text-to-video (T2V) generation
remains a challenge, particularly in preserving temporal consistency. Existing
methods that aim to improve consistency often cause trade-offs such as reduced
imaging quality and impractical computational time. To address these issues we
introduce VideoGuide, a novel framework that enhances the temporal consistency
of pretrained T2V models without the need for additional training or
fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model
(VDM) or itself as a guide during the early stages of inference, improving
temporal quality by interpolating the guiding model's denoised samples into the
sampling model's denoising process. The proposed method brings about
significant improvement in temporal consistency and image fidelity, providing a
cost-effective and practical solution that synergizes the strengths of various
video diffusion models. Furthermore, we demonstrate prior distillation,
revealing that base models can achieve enhanced text coherence by utilizing the
superior data prior of the guiding model through the proposed method. Project
Page: https://dohunlee1.github.io/videoguide.github.io/",2024-10-06 05:46:17+00:00,"['Dohun Lee', 'Bryan S Kim', 'Geon Yeong Park', 'Jong Chul Ye']",http://arxiv.org/abs/2410.04364v3
CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from Monocular Video,"The goal of our work is to generate high-quality novel views from monocular
videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have
shown impressive performance by leveraging time-varying dynamic radiation
fields. However, these methods have limitations when it comes to accurately
modeling the motion of complex objects, which can lead to inaccurate and blurry
renderings of details. To address this limitation, we propose a novel approach
that builds upon a recent generalization NeRF, which aggregates nearby views
onto new viewpoints. However, such methods are typically only effective for
static scenes. To overcome this challenge, we introduce a module that operates
in both the time and frequency domains to aggregate the features of object
motion. This allows us to learn the relationship between frames and generate
higher-quality images. Our experiments demonstrate significant improvements
over state-of-the-art methods on dynamic scene datasets. Specifically, our
approach outperforms existing methods in terms of both the accuracy and visual
quality of the synthesized views. Our code is available on
https://github.com/xingy038/CTNeRF.",2024-01-10 00:40:05+00:00,"['Xingyu Miao', 'Yang Bai', 'Haoran Duan', 'Yawen Huang', 'Fan Wan', 'Yang Long', 'Yefeng Zheng']",http://arxiv.org/abs/2401.04861v2
Medical Video Generation for Disease Progression Simulation,"Modeling disease progression is crucial for improving the quality and
efficacy of clinical diagnosis and prognosis, but it is often hindered by a
lack of longitudinal medical image monitoring for individual patients. To
address this challenge, we propose the first Medical Video Generation (MVG)
framework that enables controlled manipulation of disease-related image and
video features, allowing precise, realistic, and personalized simulations of
disease progression. Our approach begins by leveraging large language models
(LLMs) to recaption prompt for disease trajectory. Next, a controllable
multi-round diffusion model simulates the disease progression state for each
patient, creating realistic intermediate disease state sequence. Finally, a
diffusion-based video transition generation model interpolates disease
progression between these states. We validate our framework across three
medical imaging domains: chest X-ray, fundus photography, and skin image. Our
results demonstrate that MVG significantly outperforms baseline models in
generating coherent and clinically plausible disease trajectories. Two user
studies by veteran physicians, provide further validation and insights into the
clinical utility of the generated sequences. MVG has the potential to assist
healthcare providers in modeling disease trajectories, interpolating missing
medical image data, and enhancing medical education through realistic, dynamic
visualizations of disease progression.",2024-11-18 18:37:09+00:00,"['Xu Cao', 'Kaizhao Liang', 'Kuei-Da Liao', 'Tianren Gao', 'Wenqian Ye', 'Jintai Chen', 'Zhiguang Ding', 'Jianguo Cao', 'James M. Rehg', 'Jimeng Sun']",http://arxiv.org/abs/2411.11943v1
LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control,"Portrait Animation aims to synthesize a lifelike video from a single source
image, using it as an appearance reference, with motion (i.e., facial
expressions and head pose) derived from a driving video, audio, text, or
generation. Instead of following mainstream diffusion-based methods, we explore
and extend the potential of the implicit-keypoint-based framework, which
effectively balances computational efficiency and controllability. Building
upon this, we develop a video-driven portrait animation framework named
LivePortrait with a focus on better generalization, controllability, and
efficiency for practical usage. To enhance the generation quality and
generalization ability, we scale up the training data to about 69 million
high-quality frames, adopt a mixed image-video training strategy, upgrade the
network architecture, and design better motion transformation and optimization
objectives. Additionally, we discover that compact implicit keypoints can
effectively represent a kind of blendshapes and meticulously propose a
stitching and two retargeting modules, which utilize a small MLP with
negligible computational overhead, to enhance the controllability. Experimental
results demonstrate the efficacy of our framework even compared to
diffusion-based methods. The generation speed remarkably reaches 12.8ms on an
RTX 4090 GPU with PyTorch. The inference code and models are available at
https://github.com/KwaiVGI/LivePortrait",2024-07-03 14:41:39+00:00,"['Jianzhu Guo', 'Dingyun Zhang', 'Xiaoqiang Liu', 'Zhizhou Zhong', 'Yuan Zhang', 'Pengfei Wan', 'Di Zhang']",http://arxiv.org/abs/2407.03168v2
PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face Generation,"Audio-driven talking face generation is a challenging task in digital
communication. Despite significant progress in the area, most existing methods
concentrate on audio-lip synchronization, often overlooking aspects such as
visual quality, customization, and generalization that are crucial to producing
realistic talking faces. To address these limitations, we introduce a novel,
customizable one-shot audio-driven talking face generation framework, named
PortraitTalk. Our proposed method utilizes a latent diffusion framework
consisting of two main components: IdentityNet and AnimateNet. IdentityNet is
designed to preserve identity features consistently across the generated video
frames, while AnimateNet aims to enhance temporal coherence and motion
consistency. This framework also integrates an audio input with the reference
images, thereby reducing the reliance on reference-style videos prevalent in
existing approaches. A key innovation of PortraitTalk is the incorporation of
text prompts through decoupled cross-attention mechanisms, which significantly
expands creative control over the generated videos. Through extensive
experiments, including a newly developed evaluation metric, our model
demonstrates superior performance over the state-of-the-art methods, setting a
new standard for the generation of customizable realistic talking faces
suitable for real-world applications.",2024-12-10 18:51:31+00:00,"['Fatemeh Nazarieh', 'Zhenhua Feng', 'Diptesh Kanojia', 'Muhammad Awais', 'Josef Kittler']",http://arxiv.org/abs/2412.07754v1
"Neural-Network-Enhanced Metalens Camera for High-Definition, Dynamic Imaging in the Long-Wave Infrared Spectrum","To provide a lightweight and cost-effective solution for the long-wave
infrared imaging using a singlet, we develop a camera by integrating a
High-Frequency-Enhancing Cycle-GAN neural network into a metalens imaging
system. The High-Frequency-Enhancing Cycle-GAN improves the quality of the
original metalens images by addressing inherent frequency loss introduced by
the metalens. In addition to the bidirectional cyclic generative adversarial
network, it incorporates a high-frequency adversarial learning module. This
module utilizes wavelet transform to extract high-frequency components, and
then establishes a high-frequency feedback loop. It enables the generator to
enhance the camera outputs by integrating adversarial feedback from the
high-frequency discriminator. This ensures that the generator adheres to the
constraints imposed by the high-frequency adversarial loss, thereby effectively
recovering the camera's frequency loss. This recovery guarantees high-fidelity
image output from the camera, facilitating smooth video production. Our camera
is capable of achieving dynamic imaging at 125 frames per second with an End
Point Error value of 12.58. We also achieve 0.42 for Fr\'echet Inception
Distance, 30.62 for Peak Signal to Noise Ratio, and 0.69 for Structural
Similarity in the recorded videos.",2024-11-26 06:09:45+00:00,"['Jing-Yang Wei', 'Hao Huang', 'Xin Zhang', 'De-Mao Ye', 'Yi Li', 'Le Wang', 'Yao-Guang Ma', 'Yang-Hui Li']",http://arxiv.org/abs/2411.17139v1
Point-VOS: Pointing Up Video Object Segmentation,"Current state-of-the-art Video Object Segmentation (VOS) methods rely on
dense per-object mask annotations both during training and testing. This
requires time-consuming and costly video annotation mechanisms. We propose a
novel Point-VOS task with a spatio-temporally sparse point-wise annotation
scheme that substantially reduces the annotation effort. We apply our
annotation scheme to two large-scale video datasets with text descriptions and
annotate over 19M points across 133K objects in 32K videos. Based on our
annotations, we propose a new Point-VOS benchmark, and a corresponding
point-based training mechanism, which we use to establish strong baseline
results. We show that existing VOS methods can easily be adapted to leverage
our point annotations during training, and can achieve results close to the
fully-supervised performance when trained on pseudo-masks generated from these
points. In addition, we show that our data can be used to improve models that
connect vision and language, by evaluating it on the Video Narrative Grounding
(VNG) task. We will make our code and annotations available at
https://pointvos.github.io.",2024-02-08 18:52:23+00:00,"['Idil Esen Zulfikar', 'Sabarinath Mahadevan', 'Paul Voigtlaender', 'Bastian Leibe']",http://arxiv.org/abs/2402.05917v2
Don't Judge by the Look: Towards Motion Coherent Video Representation,"Current training pipelines in object recognition neglect Hue Jittering when
doing data augmentation as it not only brings appearance changes that are
detrimental to classification, but also the implementation is inefficient in
practice. In this study, we investigate the effect of hue variance in the
context of video understanding and find this variance to be beneficial since
static appearances are less important in videos that contain motion
information. Based on this observation, we propose a data augmentation method
for video understanding, named Motion Coherent Augmentation (MCA), that
introduces appearance variation in videos and implicitly encourages the model
to prioritize motion patterns, rather than static appearances. Concretely, we
propose an operation SwapMix to efficiently modify the appearance of video
samples, and introduce Variation Alignment (VA) to resolve the distribution
shift caused by SwapMix, enforcing the model to learn appearance invariant
representations. Comprehensive empirical evaluation across various
architectures and different datasets solidly validates the effectiveness and
generalization ability of MCA, and the application of VA in other augmentation
methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.",2024-03-14 15:53:04+00:00,"['Yitian Zhang', 'Yue Bai', 'Huan Wang', 'Yizhou Wang', 'Yun Fu']",http://arxiv.org/abs/2403.09506v2
VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding,"We explore how reconciling several foundation models (large language models
and vision-language models) with a novel unified memory mechanism could tackle
the challenging video understanding problem, especially capturing the long-term
temporal relations in lengthy videos. In particular, the proposed multimodal
agent VideoAgent: 1) constructs a structured memory to store both the generic
temporal event descriptions and object-centric tracking states of the video; 2)
given an input task query, it employs tools including video segment
localization and object memory querying along with other visual foundation
models to interactively solve the task, utilizing the zero-shot tool-use
ability of LLMs. VideoAgent demonstrates impressive performances on several
long-horizon video understanding benchmarks, an average increase of 6.6% on
NExT-QA and 26.0% on EgoSchema over baselines, closing the gap between
open-sourced models and private counterparts including Gemini 1.5 Pro.",2024-03-18 05:07:59+00:00,"['Yue Fan', 'Xiaojian Ma', 'Rujie Wu', 'Yuntao Du', 'Jiaqi Li', 'Zhi Gao', 'Qing Li']",http://arxiv.org/abs/2403.11481v2
Adaptive Cooperative Streaming of Holographic Video Over Wireless Networks: A Proximal Policy Optimization Solution,"Adapting holographic video streaming to fluctuating wireless channels is
essential to maintain consistent and satisfactory Quality of Experience (QoE)
for users, which, however, is a challenging task due to the dynamic and
uncertain characteristics of wireless networks. To address this issue, we
propose a holographic video cooperative streaming framework designed for a
generic wireless network in which multiple access points can cooperatively
transmit video with different bitrates to multiple users. Additionally, we
model a novel QoE metric tailored specifically for holographic video streaming,
which can effectively encapsulate the nuances of holographic video quality,
quality fluctuations, and rebuffering occurrences simultaneously. Furthermore,
we formulate a formidable QoE maximization problem, which is a non-convex mixed
integer nonlinear programming problem. Using proximal policy optimization
(PPO), a new class of reinforcement learning algorithms, we devise a joint
beamforming and bitrate control scheme, which can be wisely adapted to
fluctuations in the wireless channel. The numerical results demonstrate the
superiority of the proposed scheme over representative baselines.",2024-06-13 04:44:06+00:00,"['Wanli Wen', 'Jiping Yan', 'Yulu Zhang', 'Zhen Huang', 'Liang Liang', 'Yunjian Jia']",http://arxiv.org/abs/2406.08806v1
MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues,"Multimodal Large Language Models (MLLMs) have demonstrated remarkable
multimodal emotion recognition capabilities, integrating multimodal cues from
visual, acoustic, and linguistic contexts in the video to recognize human
emotional states. However, existing methods ignore capturing local facial
features of temporal dynamics of micro-expressions and do not leverage the
contextual dependencies of the utterance-aware temporal segments in the video,
thereby limiting their expected effectiveness to a certain extent. In this
work, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention
to the local facial micro-expression dynamics and the contextual dependencies
of utterance-aware video clips. Our model incorporates two key architectural
contributions: (1) a global-local attention visual encoder that integrates
global frame-level timestamp-bound image features with local facial features of
temporal dynamics of micro-expressions; (2) an utterance-aware video Q-Former
that captures multi-scale and contextual dependencies by generating visual
token sequences for each utterance segment and for the entire video then
combining them. Preliminary qualitative experiments demonstrate that in a new
Explainable Multimodal Emotion Recognition (EMER) task that exploits
multi-modal and multi-faceted clues to predict emotions in an open-vocabulary
(OV) manner, MicroEmo demonstrates its effectiveness compared with the latest
methods.",2024-07-23 15:05:55+00:00,['Liyun Zhang'],http://arxiv.org/abs/2407.16552v2
Kalman-Inspired Feature Propagation for Video Face Super-Resolution,"Despite the promising progress of face image super-resolution, video face
super-resolution remains relatively under-explored. Existing approaches either
adapt general video super-resolution networks to face datasets or apply
established face image super-resolution models independently on individual
video frames. These paradigms encounter challenges either in reconstructing
facial details or maintaining temporal consistency. To address these issues, we
introduce a novel framework called Kalman-inspired Feature Propagation (KEEP),
designed to maintain a stable face prior over time. The Kalman filtering
principles offer our method a recurrent ability to use the information from
previously restored frames to guide and regulate the restoration process of the
current frame. Extensive experiments demonstrate the effectiveness of our
method in capturing facial details consistently across video frames. Code and
video demo are available at https://jnjaby.github.io/projects/KEEP.",2024-08-09 17:57:12+00:00,"['Ruicheng Feng', 'Chongyi Li', 'Chen Change Loy']",http://arxiv.org/abs/2408.05205v1
Moment&Cross: Next-Generation Real-Time Cross-Domain CTR Prediction for Live-Streaming Recommendation at Kuaishou,"Kuaishou, is one of the largest short-video and live-streaming platform,
compared with short-video recommendations, live-streaming recommendation is
more complex because of: (1) temporarily-alive to distribution, (2) user may
watch for a long time with feedback delay, (3) content is unpredictable and
changes over time. Actually, even if a user is interested in the live-streaming
author, it still may be an negative watching (e.g., short-view < 3s) since the
real-time content is not attractive enough. Therefore, for live-streaming
recommendation, there exists a challenging task: how do we recommend the
live-streaming at right moment for users? Additionally, our platform's major
exposure content is short short-video, and the amount of exposed short-video is
9x more than exposed live-streaming. Thus users will leave more behaviors on
short-videos, which leads to a serious data imbalance problem making the
live-streaming data could not fully reflect user interests. In such case, there
raises another challenging task: how do we utilize users' short-video behaviors
to make live-streaming recommendation better?",2024-08-11 07:00:27+00:00,"['Jiangxia Cao', 'Shen Wang', 'Yue Li', 'Shenghui Wang', 'Jian Tang', 'Shiyao Wang', 'Shuang Yang', 'Zhaojie Liu', 'Guorui Zhou']",http://arxiv.org/abs/2408.05709v1
OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal Omni-Scale Feature Learning,"Recent Vision-Language Models (VLMs) \textit{e.g.} CLIP have made great
progress in video recognition. Despite the improvement brought by the strong
visual backbone in extracting spatial features, CLIP still falls short in
capturing and integrating spatial-temporal features which is essential for
video recognition. In this paper, we propose OmniCLIP, a framework that adapts
CLIP for video recognition by focusing on learning comprehensive features
encompassing spatial, temporal, and dynamic spatial-temporal scales, which we
refer to as omni-scale features. This is achieved through the design of
spatial-temporal blocks that include parallel temporal adapters (PTA), enabling
efficient temporal modeling. Additionally, we introduce a self-prompt generator
(SPG) module to capture dynamic object spatial features. The synergy between
PTA and SPG allows OmniCLIP to discern varying spatial information across
frames and assess object scales over time. We have conducted extensive
experiments in supervised video recognition, few-shot video recognition, and
zero-shot recognition tasks. The results demonstrate the effectiveness of our
method, especially with OmniCLIP achieving a top-1 accuracy of 74.30\% on
HMDB51 in a 16-shot setting, surpassing the recent MotionPrompt approach even
with full training data. The code is available at
\url{https://github.com/XiaoBuL/OmniCLIP}.",2024-08-12 13:55:46+00:00,"['Mushui Liu', 'Bozheng Li', 'Yunlong Yu']",http://arxiv.org/abs/2408.06158v1
PiTe: Pixel-Temporal Alignment for Large Video-Language Model,"Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models
(LVLMs) have emerged as a pivotal advancement, bridging the gap between image
and text. However, video making it challenging for LVLMs to perform adequately
due to the complexity of the relationship between language and spatial-temporal
data structure. Recent Large Video-Language Models (LVidLMs) align feature of
static visual data like image into latent space of language feature, by general
multi-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we
explore fine-grained alignment approach via object trajectory for different
modalities across both spatial and temporal dimensions simultaneously. Thus, we
propose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed
PiTe, that exhibits promising applicable model property. To achieve
fine-grained video-language alignment, we curate a multi-modal pre-training
dataset PiTe-143k, the dataset provision of moving trajectories in pixel level
for all individual objects, that appear and mention in the video and caption
both, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates
astounding capabilities on myriad video-related multi-modal tasks through beat
the state-of-the-art methods by a large margin.",2024-09-11 12:53:07+00:00,"['Yang Liu', 'Pengxiang Ding', 'Siteng Huang', 'Min Zhang', 'Han Zhao', 'Donglin Wang']",http://arxiv.org/abs/2409.07239v1
DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction,"Dynamic scene reconstruction from monocular video is critical for real-world
applications. This paper tackles the dual challenges of dynamic novel-view
synthesis and 3D geometry reconstruction by introducing a hybrid framework:
Deformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both
modules can leverage each other for both tasks. During training, depth maps
generated by the deformable Gaussian splatting module guide the ray sampling
for faster processing and provide depth supervision within the dynamic neural
surface module to improve geometry reconstruction. Simultaneously, the dynamic
neural surface directs the distribution of Gaussian primitives around the
surface, enhancing rendering quality. To further refine depth supervision, we
introduce a depth-filtering process on depth maps derived from Gaussian
rasterization. Extensive experiments on public datasets demonstrate that DGNS
achieves state-of-the-art performance in both novel-view synthesis and 3D
reconstruction.",2024-12-05 06:28:08+00:00,"['Xuesong Li', 'Jinguang Tong', 'Jie Hong', 'Vivien Rolland', 'Lars Petersson']",http://arxiv.org/abs/2412.03910v2
InstantDrag: Improving Interactivity in Drag-based Image Editing,"Drag-based image editing has recently gained popularity for its interactivity
and precision. However, despite the ability of text-to-image models to generate
samples within a second, drag editing still lags behind due to the challenge of
accurately reflecting user interaction while maintaining image content. Some
existing approaches rely on computationally intensive per-image optimization or
intricate guidance-based methods, requiring additional inputs such as masks for
movable regions and text prompts, thereby compromising the interactivity of the
editing process. We introduce InstantDrag, an optimization-free pipeline that
enhances interactivity and speed, requiring only an image and a drag
instruction as input. InstantDrag consists of two carefully designed networks:
a drag-conditioned optical flow generator (FlowGen) and an optical
flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion
dynamics for drag-based image editing in real-world video datasets by
decomposing the task into motion generation and motion-conditioned image
generation. We demonstrate InstantDrag's capability to perform fast,
photo-realistic edits without masks or text prompts through experiments on
facial video datasets and general scenes. These results highlight the
efficiency of our approach in handling drag-based image editing, making it a
promising solution for interactive, real-time applications.",2024-09-13 14:19:27+00:00,"['Joonghyuk Shin', 'Daehyeon Choi', 'Jaesik Park']",http://arxiv.org/abs/2409.08857v2
Generative Object Insertion in Gaussian Splatting with a Multi-View Diffusion Model,"Generating and inserting new objects into 3D content is a compelling approach
for achieving versatile scene recreation. Existing methods, which rely on SDS
optimization or single-view inpainting, often struggle to produce high-quality
results. To address this, we propose a novel method for object insertion in 3D
content represented by Gaussian Splatting. Our approach introduces a multi-view
diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable
video diffusion model to facilitate view-consistent object inpainting. Within
MVInpainter, we incorporate a ControlNet-based conditional injection module to
enable controlled and more predictable multi-view generation. After generating
the multi-view inpainted results, we further propose a mask-aware 3D
reconstruction technique to refine Gaussian Splatting reconstruction from these
sparse inpainted views. By leveraging these fabricate techniques, our approach
yields diverse results, ensures view-consistent and harmonious insertions, and
produces better object quality. Extensive experiments demonstrate that our
approach outperforms existing methods.",2024-09-25 13:52:50+00:00,"['Hongliang Zhong', 'Can Wang', 'Jingbo Zhang', 'Jing Liao']",http://arxiv.org/abs/2409.16938v1
Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior,"Recent methods for audio-driven talking head synthesis often optimize neural
radiance fields (NeRF) on a monocular talking portrait video, leveraging its
capability to render high-fidelity and 3D-consistent novel-view frames.
However, they often struggle to reconstruct complete face geometry due to the
absence of comprehensive 3D information in the input monocular videos. In this
paper, we introduce a novel audio-driven talking head synthesis framework,
called Talk3D, that can faithfully reconstruct its plausible facial geometries
by effectively adopting the pre-trained 3D-aware generative prior. Given the
personalized 3D generative model, we present a novel audio-guided attention
U-Net architecture that predicts the dynamic face variations in the NeRF space
driven by audio. Furthermore, our model is further modulated by audio-unrelated
conditioning tokens which effectively disentangle variations unrelated to audio
features. Compared to existing methods, our method excels in generating
realistic facial geometries even under extreme head poses. We also conduct
extensive experiments showing our approach surpasses state-of-the-art
benchmarks in terms of both quantitative and qualitative evaluations.",2024-03-29 12:49:40+00:00,"['Jaehoon Ko', 'Kyusun Cho', 'Joungbin Lee', 'Heeji Yoon', 'Sangmin Lee', 'Sangjun Ahn', 'Seungryong Kim']",http://arxiv.org/abs/2403.20153v1
Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation,"We present Mesh2NeRF, an approach to derive ground-truth radiance fields from
textured meshes for 3D generation tasks. Many 3D generative approaches
represent 3D scenes as radiance fields for training. Their ground-truth
radiance fields are usually fitted from multi-view renderings from a
large-scale synthetic 3D dataset, which often results in artifacts due to
occlusions or under-fitting issues. In Mesh2NeRF, we propose an analytic
solution to directly obtain ground-truth radiance fields from 3D meshes,
characterizing the density field with an occupancy function featuring a defined
surface thickness, and determining view-dependent color through a reflection
function considering both the mesh and environment lighting. Mesh2NeRF extracts
accurate radiance fields which provides direct supervision for training
generative NeRFs and single scene representation. We validate the effectiveness
of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in
PSNR for view synthesis in single scene representation on the ABO dataset, a
0.69 PSNR enhancement in the single-view conditional generation of ShapeNet
Cars, and notably improved mesh extraction from NeRF in the unconditional
generation of Objaverse Mugs.",2024-03-28 11:22:53+00:00,"['Yujin Chen', 'Yinyu Nie', 'Benjamin Ummenhofer', 'Reiner Birkl', 'Michael Paulitsch', 'Matthias Mller', 'Matthias Niener']",http://arxiv.org/abs/2403.19319v2
4K4DGen: Panoramic 4D Generation at 4K Resolution,"The blooming of virtual reality and augmented reality (VR/AR) technologies
has driven an increasing demand for the creation of high-quality, immersive,
and dynamic environments. However, existing generative techniques either focus
solely on dynamic objects or perform outpainting from a single perspective
image, failing to meet the requirements of VR/AR applications that need
free-viewpoint, 360$^{\circ}$ virtual views where users can move in all
directions. In this work, we tackle the challenging task of elevating a single
panorama to an immersive 4D experience. For the first time, we demonstrate the
capability to generate omnidirectional dynamic scenes with 360$^{\circ}$ views
at 4K (4096 $\times$ 2048) resolution, thereby providing an immersive user
experience. Our method introduces a pipeline that facilitates natural scene
animations and optimizes a set of dynamic Gaussians using efficient splatting
techniques for real-time exploration. To overcome the lack of scene-scale
annotated 4D data and models, especially in panoramic formats, we propose a
novel \textbf{Panoramic Denoiser} that adapts generic 2D diffusion priors to
animate consistently in 360$^{\circ}$ images, transforming them into panoramic
videos with dynamic scenes at targeted regions. Subsequently, we propose
\textbf{Dynamic Panoramic Lifting} to elevate the panoramic video into a 4D
immersive environment while preserving spatial and temporal consistency. By
transferring prior knowledge from 2D models in the perspective domain to the
panoramic domain and the 4D lifting with spatial appearance and geometry
regularization, we achieve high-quality Panorama-to-4D generation at a
resolution of 4K for the first time.",2024-06-19 13:11:02+00:00,"['Renjie Li', 'Panwang Pan', 'Bangbang Yang', 'Dejia Xu', 'Shijie Zhou', 'Xuanyang Zhang', 'Zeming Li', 'Achuta Kadambi', 'Zhangyang Wang', 'Zhengzhong Tu', 'Zhiwen Fan']",http://arxiv.org/abs/2406.13527v3
Exploiting Topological Priors for Boosting Point Cloud Generation,"This paper presents an innovative enhancement to the Sphere as Prior
Generative Adversarial Network (SP-GAN) model, a state-of-the-art GAN designed
for point cloud generation. A novel method is introduced for point cloud
generation that elevates the structural integrity and overall quality of the
generated point clouds by incorporating topological priors into the training
process of the generator. Specifically, this work utilizes the K-means
algorithm to segment a point cloud from the repository into clusters and
extract centroids, which are then used as priors in the generation process of
the SP-GAN. Furthermore, the discriminator component of the SP-GAN utilizes the
identical point cloud that contributed the centroids, ensuring a coherent and
consistent learning environment. This strategic use of centroids as intuitive
guides not only boosts the efficiency of global feature learning but also
substantially improves the structural coherence and fidelity of the generated
point clouds. By applying the K-means algorithm to generate centroids as the
prior, the work intuitively and experimentally demonstrates that such a prior
enhances the quality of generated point clouds.",2024-03-16 16:17:44+00:00,['Baiyuan Chen'],http://arxiv.org/abs/2403.10962v2
Motion-Adaptive Inference for Flexible Learned B-Frame Compression,"While the performance of recent learned intra and sequential video
compression models exceed that of respective traditional codecs, the
performance of learned B-frame compression models generally lag behind
traditional B-frame coding. The performance gap is bigger for complex scenes
with large motions. This is related to the fact that the distance between the
past and future references vary in hierarchical B-frame compression depending
on the level of hierarchy, which causes motion range to vary. The inability of
a single B-frame compression model to adapt to various motion ranges causes
loss of performance. As a remedy, we propose controlling the motion range for
flow prediction during inference (to approximately match the range of motions
in the training data) by downsampling video frames adaptively according to
amount of motion and level of hierarchy in order to compress all B-frames using
a single flexible-rate model. We present state-of-the-art BD rate results to
demonstrate the superiority of our proposed single-model motion-adaptive
inference approach to all existing learned B-frame compression models.",2024-02-13 15:54:31+00:00,"['M. Akin Yilmaz', 'O. Ugur Ulas', 'Ahmet Bilican', 'A. Murat Tekalp']",http://arxiv.org/abs/2402.08550v1
FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos,"Digitising the 3D world into a clean, CAD model-based representation has
important applications for augmented reality and robotics. Current
state-of-the-art methods are computationally intensive as they individually
encode each detected object and optimise CAD alignments in a second stage. In
this work, we propose FastCAD, a real-time method that simultaneously retrieves
and aligns CAD models for all objects in a given scene. In contrast to previous
works, we directly predict alignment parameters and shape embeddings. We
achieve high-quality shape retrievals by learning CAD embeddings in a
contrastive learning framework and distilling those into FastCAD. Our
single-stage method accelerates the inference time by a factor of 50 compared
to other methods operating on RGB-D scans while outperforming them on the
challenging Scan2CAD alignment benchmark. Further, our approach collaborates
seamlessly with online 3D reconstruction techniques. This enables the real-time
generation of precise CAD model-based reconstructions from videos at 10 FPS.
Doing so, we significantly improve the Scan2CAD alignment accuracy in the video
setting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to
29.6%.",2024-03-22 12:20:23+00:00,"['Florian Langer', 'Jihong Ju', 'Georgi Dikov', 'Gerhard Reitmayr', 'Mohsen Ghafoorian']",http://arxiv.org/abs/2403.15161v1
Deep Understanding of Soccer Match Videos,"Soccer is one of the most popular sport worldwide, with live broadcasts
frequently available for major matches. However, extracting detailed,
frame-by-frame information on player actions from these videos remains a
challenge. Utilizing state-of-the-art computer vision technologies, our system
can detect key objects such as soccer balls, players and referees. It also
tracks the movements of players and the ball, recognizes player numbers,
classifies scenes, and identifies highlights such as goal kicks. By analyzing
live TV streams of soccer matches, our system can generate highlight GIFs,
tactical illustrations, and diverse summary graphs of ongoing games. Through
these visual recognition techniques, we deliver a comprehensive understanding
of soccer game videos, enriching the viewer's experience with detailed and
insightful analysis.",2024-07-11 05:54:13+00:00,"['Shikun Xu', 'Yandong Zhu', 'Gen Li', 'Changhu Wang']",http://arxiv.org/abs/2407.08200v1
U-Motion: Learned Point Cloud Video Compression with U-Structured Temporal Context Generation,"Point cloud video (PCV) is a versatile 3D representation of dynamic scenes
with emerging applications. This paper introduces U-Motion, a learning-based
compression scheme for both PCV geometry and attributes. We propose a
U-Structured inter-frame prediction framework, U-Inter, which performs explicit
motion estimation and compensation (ME/MC) at different scales with varying
levels of detail. It integrates Top-Down (Fine-to-Coarse) Motion Propagation,
Bottom-Up Motion Predictive Coding and Multi-scale Group Motion Compensation to
enable accurate motion estimation and efficient motion compression at each
scale. In addition, we design a multi-scale spatial-temporal predictive coding
module to capture the cross-scale spatial redundancy remaining after U-Inter
prediction. We conduct experiments following the MPEG Common Test Condition for
dense dynamic point clouds and demonstrate that U-Motion can achieve
significant gains over MPEG G-PCC-GesTM v3.0 and recently published
learning-based methods for both geometry and attribute compression.",2024-11-21 07:17:01+00:00,"['Tingyu Fan', 'Yueyu Hu', 'Ran Gong', 'Yao Wang']",http://arxiv.org/abs/2411.14501v4
RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations,"We present RELOCATE, a simple training-free baseline designed to perform the
challenging task of visual query localization in long videos. To eliminate the
need for task-specific training and efficiently handle long videos, RELOCATE
leverages a region-based representation derived from pretrained vision models.
At a high level, it follows the classic object localization approach: (1)
identify all objects in each video frame, (2) compare the objects with the
given query and select the most similar ones, and (3) perform bidirectional
tracking to get a spatio-temporal response. However, we propose some key
enhancements to handle small objects, cluttered scenes, partial visibility, and
varying appearances. Notably, we refine the selected objects for accurate
localization and generate additional visual queries to capture visual
variations. We evaluate RELOCATE on the challenging Ego4D Visual Query 2D
Localization dataset, establishing a new baseline that outperforms prior
task-specific methods by 49% (relative improvement) in spatio-temporal average
precision.",2024-12-02 18:59:53+00:00,"['Savya Khosla', 'Sethuraman T V', 'Alexander Schwing', 'Derek Hoiem']",http://arxiv.org/abs/2412.01826v1
Learning Self-Supervised Audio-Visual Representations for Sound Recommendations,"We propose a novel self-supervised approach for learning audio and visual
representations from unlabeled videos, based on their correspondence. The
approach uses an attention mechanism to learn the relative importance of
convolutional features extracted at different resolutions from the audio and
visual streams and uses the attention features to encode the audio and visual
input based on their correspondence. We evaluated the representations learned
by the model to classify audio-visual correlation as well as to recommend sound
effects for visual scenes. Our results show that the representations generated
by the attention model improves the correlation accuracy compared to the
baseline, by 18% and the recommendation accuracy by 10% for VGG-Sound, which is
a public video dataset. Additionally, audio-visual representations learned by
training the attention model with cross-modal contrastive learning further
improves the recommendation performance, based on our evaluation using
VGG-Sound and a more challenging dataset consisting of gameplay video
recordings.",2024-12-10 10:56:02+00:00,['Sudha Krishnamurthy'],http://arxiv.org/abs/2412.07406v1
StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive Volume Visualization,"In volume visualization, visualization synthesis has attracted much attention
due to its ability to generate novel visualizations without following the
conventional rendering pipeline. However, existing solutions based on
generative adversarial networks often require many training images and take
significant training time. Still, issues such as low quality, consistency, and
flexibility persist. This paper introduces StyleRF-VolVis, an innovative style
transfer framework for expressive volume visualization (VolVis) via neural
radiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its
ability to accurately separate the underlying scene geometry (i.e., content)
and color appearance (i.e., style), conveniently modify color, opacity, and
lighting of the original rendering while maintaining visual content consistency
across the views, and effectively transfer arbitrary styles from reference
images to the reconstructed 3D scene. To achieve these, we design a base NeRF
model for scene geometry extraction, a palette color network to classify
regions of the radiance field for photorealistic editing, and an unrestricted
color network to lift the color palette constraint via knowledge distillation
for non-photorealistic editing. We demonstrate the superior quality,
consistency, and flexibility of StyleRF-VolVis by experimenting with various
volume rendering scenes and reference images and comparing StyleRF-VolVis
against other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF
and SNeRF) style rendering solutions.",2024-07-31 20:26:30+00:00,"['Kaiyuan Tang', 'Chaoli Wang']",http://arxiv.org/abs/2408.00150v1
FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait,"With the rapid advancement of diffusion-based generative models, portrait
image animation has achieved remarkable results. However, it still faces
challenges in temporally consistent video generation and fast sampling due to
its iterative sampling nature. This paper presents FLOAT, an audio-driven
talking portrait video generation method based on flow matching generative
model. We shift the generative modeling from the pixel-based latent space to a
learned motion latent space, enabling efficient design of temporally consistent
motion. To achieve this, we introduce a transformer-based vector field
predictor with a simple yet effective frame-wise conditioning mechanism.
Additionally, our method supports speech-driven emotion enhancement, enabling a
natural incorporation of expressive motions. Extensive experiments demonstrate
that our method outperforms state-of-the-art audio-driven talking portrait
methods in terms of visual quality, motion fidelity, and efficiency.",2024-12-02 02:50:07+00:00,"['Taekyung Ki', 'Dongchan Min', 'Gyeongsu Chae']",http://arxiv.org/abs/2412.01064v2
GIM: Learning Generalizable Image Matcher From Internet Videos,"Image matching is a fundamental computer vision problem. While learning-based
methods achieve state-of-the-art performance on existing benchmarks, they
generalize poorly to in-the-wild images. Such methods typically need to train
separate models for different scene types and are impractical when the scene
type is unknown in advance. One of the underlying problems is the limited
scalability of existing data construction pipelines, which limits the diversity
of standard image matching datasets. To address this problem, we propose GIM, a
self-training framework for learning a single generalizable model based on any
image matching architecture using internet videos, an abundant and diverse data
source. Given an architecture, GIM first trains it on standard domain-specific
datasets and then combines it with complementary matching methods to create
dense labels on nearby frames of novel videos. These labels are filtered by
robust fitting, and then enhanced by propagating them to distant frames. The
final model is trained on propagated data with strong augmentations. We also
propose ZEB, the first zero-shot evaluation benchmark for image matching. By
mixing data from diverse domains, ZEB can thoroughly assess the cross-domain
generalization performance of different methods. Applying GIM consistently
improves the zero-shot performance of 3 state-of-the-art image matching
architectures; with 50 hours of YouTube videos, the relative zero-shot
performance improves by 8.4%-18.1%. GIM also enables generalization to extreme
cross-domain data such as Bird Eye View (BEV) images of projected 3D point
clouds (Fig. 1(c)). More importantly, our single zero-shot model consistently
outperforms domain-specific baselines when evaluated on downstream tasks
inherent to their respective domains. The video presentation is available at
https://www.youtube.com/watch?v=FU_MJLD8LeY.",2024-02-16 21:48:17+00:00,"['Xuelun Shen', 'Zhipeng Cai', 'Wei Yin', 'Matthias Mller', 'Zijun Li', 'Kaixuan Wang', 'Xiaozhi Chen', 'Cheng Wang']",http://arxiv.org/abs/2402.11095v1
Anim-Director: A Large Multimodal Model Powered Agent for Controllable Animation Video Generation,"Traditional animation generation methods depend on training generative models
with human-labelled data, entailing a sophisticated multi-stage pipeline that
demands substantial human effort and incurs high training costs. Due to limited
prompting plans, these methods typically produce brief, information-poor, and
context-incoherent animations. To overcome these limitations and automate the
animation process, we pioneer the introduction of large multimodal models
(LMMs) as the core processor to build an autonomous animation-making agent,
named Anim-Director. This agent mainly harnesses the advanced understanding and
reasoning capabilities of LMMs and generative AI tools to create animated
videos from concise narratives or simple instructions. Specifically, it
operates in three main stages: Firstly, the Anim-Director generates a coherent
storyline from user inputs, followed by a detailed director's script that
encompasses settings of character profiles and interior/exterior descriptions,
and context-coherent scene descriptions that include appearing characters,
interiors or exteriors, and scene events. Secondly, we employ LMMs with the
image generation tool to produce visual images of settings and scenes. These
images are designed to maintain visual consistency across different scenes
using a visual-language prompting method that combines scene descriptions and
images of the appearing character and setting. Thirdly, scene images serve as
the foundation for producing animated videos, with LMMs generating prompts to
guide this process. The whole process is notably autonomous without manual
intervention, as the LMMs interact seamlessly with generative tools to generate
prompts, evaluate visual quality, and select the best one to optimize the final
output.",2024-08-19 08:27:31+00:00,"['Yunxin Li', 'Haoyuan Shi', 'Baotian Hu', 'Longyue Wang', 'Jiashun Zhu', 'Jinyi Xu', 'Zhen Zhao', 'Min Zhang']",http://arxiv.org/abs/2408.09787v1
Self-Supervised Video Desmoking for Laparoscopic Surgery,"Due to the difficulty of collecting real paired data, most existing desmoking
methods train the models by synthesizing smoke, generalizing poorly to real
surgical scenarios. Although a few works have explored single-image real-world
desmoking in unpaired learning manners, they still encounter challenges in
handling dense smoke. In this work, we address these issues together by
introducing the self-supervised surgery video desmoking (SelfSVD). On the one
hand, we observe that the frame captured before the activation of high-energy
devices is generally clear (named pre-smoke frame, PS frame), thus it can serve
as supervision for other smoky frames, making real-world self-supervised video
desmoking practically feasible. On the other hand, in order to enhance the
desmoking performance, we further feed the valuable information from PS frame
into models, where a masking strategy and a regularization term are presented
to avoid trivial solutions. In addition, we construct a real surgery video
dataset for desmoking, which covers a variety of smoky scenes. Extensive
experiments on the dataset show that our SelfSVD can remove smoke more
effectively and efficiently while recovering more photo-realistic details than
the state-of-the-art methods. The dataset, codes, and pre-trained models are
available at \url{https://github.com/ZcsrenlongZ/SelfSVD}.",2024-03-17 12:38:58+00:00,"['Renlong Wu', 'Zhilu Zhang', 'Shuohao Zhang', 'Longfei Gou', 'Haobin Chen', 'Lei Zhang', 'Hao Chen', 'Wangmeng Zuo']",http://arxiv.org/abs/2403.11192v2
VrdONE: One-stage Video Visual Relation Detection,"Video Visual Relation Detection (VidVRD) focuses on understanding how
entities interact over time and space in videos, a key step for gaining deeper
insights into video scenes beyond basic visual tasks. Traditional methods for
VidVRD, challenged by its complexity, typically split the task into two parts:
one for identifying what relation categories are present and another for
determining their temporal boundaries. This split overlooks the inherent
connection between these elements. Addressing the need to recognize entity
pairs' spatiotemporal interactions across a range of durations, we propose
VrdONE, a streamlined yet efficacious one-stage model. VrdONE combines the
features of subjects and objects, turning predicate detection into 1D instance
segmentation on their combined representations. This setup allows for both
relation category identification and binary mask generation in one go,
eliminating the need for extra steps like proposal generation or
post-processing. VrdONE facilitates the interaction of features across various
frames, adeptly capturing both short-lived and enduring relations.
Additionally, we introduce the Subject-Object Synergy (SOS) module, enhancing
how subjects and objects perceive each other before combining. VrdONE achieves
state-of-the-art performances on the VidOR benchmark and ImageNet-VidVRD,
showcasing its superior capability in discerning relations across different
temporal scales. The code is available at https://github.com/lucaspk512/vrdone.",2024-08-18 08:38:20+00:00,"['Xinjie Jiang', 'Chenxi Zheng', 'Xuemiao Xu', 'Bangzhen Liu', 'Weiying Zheng', 'Huaidong Zhang', 'Shengfeng He']",http://arxiv.org/abs/2408.09408v2
I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength,"Video generation technologies are developing rapidly and have broad potential
applications. Among these technologies, camera control is crucial for
generating professional-quality videos that accurately meet user expectations.
However, existing camera control methods still suffer from several limitations,
including control precision and the neglect of the control for subject motion
dynamics. In this work, we propose I2VControl-Camera, a novel camera control
method that significantly enhances controllability while providing
adjustability over the strength of subject motion. To improve control
precision, we employ point trajectory in the camera coordinate system instead
of only extrinsic matrix information as our control signal. To accurately
control and adjust the strength of subject motion, we explicitly model the
higher-order components of the video trajectory expansion, not merely the
linear terms, and design an operator that effectively represents the motion
strength. We use an adapter architecture that is independent of the base model
structure. Experiments on static and dynamic scenes show that our framework
outperformances previous methods both quantitatively and qualitatively. The
project page is: https://wanquanf.github.io/I2VControlCamera .",2024-11-10 16:59:39+00:00,"['Wanquan Feng', 'Jiawei Liu', 'Pengqi Tu', 'Tianhao Qi', 'Mingzhen Sun', 'Tianxiang Ma', 'Songtao Zhao', 'Siyu Zhou', 'Qian He']",http://arxiv.org/abs/2411.06525v3
Joint-Motion Mutual Learning for Pose Estimation in Videos,"Human pose estimation in videos has long been a compelling yet challenging
task within the realm of computer vision. Nevertheless, this task remains
difficult because of the complex video scenes, such as video defocus and
self-occlusion. Recent methods strive to integrate multi-frame visual features
generated by a backbone network for pose estimation. However, they often ignore
the useful joint information encoded in the initial heatmap, which is a
by-product of the backbone generation. Comparatively, methods that attempt to
refine the initial heatmap fail to consider any spatio-temporal motion
features. As a result, the performance of existing methods for pose estimation
falls short due to the lack of ability to leverage both local joint (heatmap)
information and global motion (feature) dynamics.
  To address this problem, we propose a novel joint-motion mutual learning
framework for pose estimation, which effectively concentrates on both local
joint dependency and global pixel-level motion dynamics. Specifically, we
introduce a context-aware joint learner that adaptively leverages initial
heatmaps and motion flow to retrieve robust local joint feature. Given that
local joint feature and global motion flow are complementary, we further
propose a progressive joint-motion mutual learning that synergistically
exchanges information and interactively learns between joint feature and motion
flow to improve the capability of the model. More importantly, to capture more
diverse joint and motion cues, we theoretically analyze and propose an
information orthogonality objective to avoid learning redundant information
from multi-cues. Empirical experiments show our method outperforms prior arts
on three challenging benchmarks.",2024-08-05 07:37:55+00:00,"['Sifan Wu', 'Haipeng Chen', 'Yifang Yin', 'Sihao Hu', 'Runyang Feng', 'Yingying Jiao', 'Ziqi Yang', 'Zhenguang Liu']",http://arxiv.org/abs/2408.02285v1
Comprehensive Generative Replay for Task-Incremental Segmentation with Concurrent Appearance and Semantic Forgetting,"Generalist segmentation models are increasingly favored for diverse tasks
involving various objects from different image sources. Task-Incremental
Learning (TIL) offers a privacy-preserving training paradigm using tasks
arriving sequentially, instead of gathering them due to strict data sharing
policies. However, the task evolution can span a wide scope that involves
shifts in both image appearance and segmentation semantics with intricate
correlation, causing concurrent appearance and semantic forgetting. To solve
this issue, we propose a Comprehensive Generative Replay (CGR) framework that
restores appearance and semantic knowledge by synthesizing image-mask pairs to
mimic past task data, which focuses on two aspects: modeling image-mask
correspondence and promoting scalability for diverse tasks. Specifically, we
introduce a novel Bayesian Joint Diffusion (BJD) model for high-quality
synthesis of image-mask pairs with their correspondence explicitly preserved by
conditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)
that recalibrates prompt embeddings to modulate the diffusion model, making the
data synthesis compatible with different tasks. Experiments on incremental
tasks (cardiac, fundus and prostate segmentation) show its clear advantage for
alleviating concurrent appearance and semantic forgetting. Code is available at
https://github.com/jingyzhang/CGR.",2024-06-28 10:05:58+00:00,"['Wei Li', 'Jingyang Zhang', 'Pheng-Ann Heng', 'Lixu Gu']",http://arxiv.org/abs/2406.19796v1
LM2D: Lyrics- and Music-Driven Dance Synthesis,"Dance typically involves professional choreography with complex movements
that follow a musical rhythm and can also be influenced by lyrical content. The
integration of lyrics in addition to the auditory dimension, enriches the
foundational tone and makes motion generation more amenable to its semantic
meanings. However, existing dance synthesis methods tend to model motions only
conditioned on audio signals. In this work, we make two contributions to bridge
this gap. First, we propose LM2D, a novel probabilistic architecture that
incorporates a multimodal diffusion model with consistency distillation,
designed to create dance conditioned on both music and lyrics in one diffusion
generation step. Second, we introduce the first 3D dance-motion dataset that
encompasses both music and lyrics, obtained with pose estimation technologies.
We evaluate our model against music-only baseline models with objective metrics
and human evaluations, including dancers and choreographers. The results
demonstrate LM2D is able to produce realistic and diverse dance matching both
lyrics and music. A video summary can be accessed at:
https://youtu.be/4XCgvYookvA.",2024-03-14 13:59:04+00:00,"['Wenjie Yin', 'Xuejiao Zhao', 'Yi Yu', 'Hang Yin', 'Danica Kragic', 'Mrten Bjrkman']",http://arxiv.org/abs/2403.09407v1
Align Your Steps: Optimizing Sampling Schedules in Diffusion Models,"Diffusion models (DMs) have established themselves as the state-of-the-art
generative modeling approach in the visual domain and beyond. A crucial
drawback of DMs is their slow sampling speed, relying on many sequential
function evaluations through large neural networks. Sampling from DMs can be
seen as solving a differential equation through a discretized set of noise
levels known as the sampling schedule. While past works primarily focused on
deriving efficient solvers, little attention has been given to finding optimal
sampling schedules, and the entire literature relies on hand-crafted
heuristics. In this work, for the first time, we propose a general and
principled approach to optimizing the sampling schedules of DMs for
high-quality outputs, called $\textit{Align Your Steps}$. We leverage methods
from stochastic calculus and find optimal schedules specific to different
solvers, trained DMs and datasets. We evaluate our novel approach on several
image, video as well as 2D toy data synthesis benchmarks, using a variety of
different samplers, and observe that our optimized schedules outperform
previous hand-crafted schedules in almost all experiments. Our method
demonstrates the untapped potential of sampling schedule optimization,
especially in the few-step synthesis regime.",2024-04-22 18:18:41+00:00,"['Amirmojtaba Sabour', 'Sanja Fidler', 'Karsten Kreis']",http://arxiv.org/abs/2404.14507v1
LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model Priors,"We aim to address sparse-view reconstruction of a 3D scene by leveraging
priors from large-scale vision models. While recent advancements such as 3D
Gaussian Splatting (3DGS) have demonstrated remarkable successes in 3D
reconstruction, these methods typically necessitate hundreds of input images
that densely capture the underlying scene, making them time-consuming and
impractical for real-world applications. However, sparse-view reconstruction is
inherently ill-posed and under-constrained, often resulting in inferior and
incomplete outcomes. This is due to issues such as failed initialization,
overfitting on input images, and a lack of details. To mitigate these
challenges, we introduce LM-Gaussian, a method capable of generating
high-quality reconstructions from a limited number of images. Specifically, we
propose a robust initialization module that leverages stereo priors to aid in
the recovery of camera poses and the reliable point clouds. Additionally, a
diffusion-based refinement is iteratively applied to incorporate image
diffusion priors into the Gaussian optimization process to preserve intricate
scene details. Finally, we utilize video diffusion priors to further enhance
the rendered images for realistic visual effects. Overall, our approach
significantly reduces the data acquisition requirements compared to previous
3DGS methods. We validate the effectiveness of our framework through
experiments on various public datasets, demonstrating its potential for
high-quality 360-degree scene reconstruction. Visual results are on our
website.",2024-09-05 12:09:02+00:00,"['Hanyang Yu', 'Xiaoxiao Long', 'Ping Tan']",http://arxiv.org/abs/2409.03456v2
In-Context Ensemble Learning from Pseudo Labels Improves Video-Language Models for Low-Level Workflow Understanding,"A Standard Operating Procedure (SOP) defines a low-level, step-by-step
written guide for a business software workflow. SOP generation is a crucial
step towards automating end-to-end software workflows. Manually creating SOPs
can be time-consuming. Recent advancements in large video-language models offer
the potential for automating SOP generation by analyzing recordings of human
demonstrations. However, current large video-language models face challenges
with zero-shot SOP generation. In this work, we first explore in-context
learning with video-language models for SOP generation. We then propose an
exploration-focused strategy called In-Context Ensemble Learning, to aggregate
pseudo labels of multiple possible paths of SOPs. The proposed in-context
ensemble learning as well enables the models to learn beyond its context window
limit with an implicit consistency regularisation. We report that in-context
learning helps video-language models to generate more temporally accurate SOP,
and the proposed in-context ensemble learning can consistently enhance the
capabilities of the video-language models in SOP generation.",2024-09-24 08:41:01+00:00,"['Moucheng Xu', 'Evangelos Chatzaroulas', 'Luc McCutcheon', 'Abdul Ahad', 'Hamzah Azeem', 'Janusz Marecki', 'Ammar Anwar']",http://arxiv.org/abs/2409.15867v5
DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT,"Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
https://github.com/YvanYin/DrivingWorld.",2024-12-27 07:44:07+00:00,"['Xiaotao Hu', 'Wei Yin', 'Mingkai Jia', 'Junyuan Deng', 'Xiaoyang Guo', 'Qian Zhang', 'Xiaoxiao Long', 'Ping Tan']",http://arxiv.org/abs/2412.19505v2
A New Lightweight Hybrid Graph Convolutional Neural Network -- CNN Scheme for Scene Classification using Object Detection Inference,"Scene understanding plays an important role in several high-level computer
vision applications, such as autonomous vehicles, intelligent video
surveillance, or robotics. However, too few solutions have been proposed for
indoor/outdoor scene classification to ensure scene context adaptability for
computer vision frameworks. We propose the first Lightweight Hybrid Graph
Convolutional Neural Network (LH-GCNN)-CNN framework as an add-on to object
detection models. The proposed approach uses the output of the CNN object
detection model to predict the observed scene type by generating a coherent
GCNN representing the semantic and geometric content of the observed scene.
This new method, applied to natural scenes, achieves an efficiency of over 90\%
for scene classification in a COCO-derived dataset containing a large number of
different scenes, while requiring fewer parameters than traditional CNN
methods. For the benefit of the scientific community, we will make the source
code publicly available: https://github.com/Aymanbegh/Hybrid-GCNN-CNN.",2024-07-19 20:34:40+00:00,"['Ayman Beghdadi', 'Azeddine Beghdadi', 'Mohib Ullah', 'Faouzi Alaya Cheikh', 'Malik Mallem']",http://arxiv.org/abs/2407.14658v1
Pancreatic Tumor Segmentation as Anomaly Detection in CT Images Using Denoising Diffusion Models,"Despite the advances in medicine, cancer has remained a formidable challenge.
Particularly in the case of pancreatic tumors, characterized by their diversity
and late diagnosis, early detection poses a significant challenge crucial for
effective treatment. The advancement of deep learning techniques, particularly
supervised algorithms, has significantly propelled pancreatic tumor detection
in the medical field. However, supervised deep learning approaches necessitate
extensive labeled medical images for training, yet acquiring such annotations
is both limited and costly. Conversely, weakly supervised anomaly detection
methods, requiring only image-level annotations, have garnered interest.
Existing methodologies predominantly hinge on generative adversarial networks
(GANs) or autoencoder models, which can pose complexity in training and, these
models may face difficulties in accurately preserving fine image details. This
research presents a novel approach to pancreatic tumor detection, employing
weak supervision anomaly detection through denoising diffusion algorithms. By
incorporating a deterministic iterative process of adding and removing noise
along with classifier guidance, the method enables seamless translation of
images between diseased and healthy subjects, resulting in detailed anomaly
maps without requiring complex training protocols and segmentation masks. This
study explores denoising diffusion models as a recent advancement over
traditional generative models like GANs, contributing to the field of
pancreatic tumor detection. Recognizing the low survival rates of pancreatic
cancer, this study emphasizes the need for continued research to leverage
diffusion models' efficiency in medical segmentation tasks.",2024-06-04 16:38:11+00:00,"['Reza Babaei', 'Samuel Cheng', 'Theresa Thai', 'Shangqing Zhao']",http://arxiv.org/abs/2406.02653v1
A General Framework for Jersey Number Recognition in Sports Video,"Jersey number recognition is an important task in sports video analysis,
partly due to its importance for long-term player tracking. It can be viewed as
a variant of scene text recognition. However, there is a lack of published
attempts to apply scene text recognition models on jersey number data. Here we
introduce a novel public jersey number recognition dataset for hockey and study
how scene text recognition methods can be adapted to this problem. We address
issues of occlusions and assess the degree to which training on one sport
(hockey) can be generalized to another (soccer). For the latter, we also
consider how jersey number recognition at the single-image level can be
aggregated across frames to yield tracklet-level jersey number labels. We
demonstrate high performance on image- and tracklet-level tasks, achieving
91.4% accuracy for hockey images and 87.4% for soccer tracklets. Code, models,
and data are available at https://github.com/mkoshkina/jersey-number-pipeline.",2024-05-22 18:08:26+00:00,"['Maria Koshkina', 'James H. Elder']",http://arxiv.org/abs/2405.13896v1
Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos,"Learning to understand dynamic 3D scenes from imagery is crucial for
applications ranging from robotics to scene reconstruction. Yet, unlike other
problems where large-scale supervised training has enabled rapid progress,
directly supervising methods for recovering 3D motion remains challenging due
to the fundamental difficulty of obtaining ground truth annotations. We present
a system for mining high-quality 4D reconstructions from internet stereoscopic,
wide-angle videos. Our system fuses and filters the outputs of camera pose
estimation, stereo depth estimation, and temporal tracking methods into
high-quality dynamic 3D reconstructions. We use this method to generate
large-scale data in the form of world-consistent, pseudo-metric 3D point clouds
with long-term motion trajectories. We demonstrate the utility of this data by
training a variant of DUSt3R to predict structure and 3D motion from real-world
image pairs, showing that training on our reconstructed data enables
generalization to diverse real-world scenes. Project page:
https://stereo4d.github.io",2024-12-12 18:59:54+00:00,"['Linyi Jin', 'Richard Tucker', 'Zhengqi Li', 'David Fouhey', 'Noah Snavely', 'Aleksander Holynski']",http://arxiv.org/abs/2412.09621v1
Non-Adversarial Learning: Vector-Quantized Common Latent Space for Multi-Sequence MRI,"Adversarial learning helps generative models translate MRI from source to
target sequence when lacking paired samples. However, implementing MRI
synthesis with adversarial learning in clinical settings is challenging due to
training instability and mode collapse. To address this issue, we leverage
intermediate sequences to estimate the common latent space among multi-sequence
MRI, enabling the reconstruction of distinct sequences from the common latent
space. We propose a generative model that compresses discrete representations
of each sequence to estimate the Gaussian distribution of vector-quantized
common (VQC) latent space between multiple sequences. Moreover, we improve the
latent space consistency with contrastive learning and increase model stability
by domain augmentation. Experiments using BraTS2021 dataset show that our
non-adversarial model outperforms other GAN-based methods, and VQC latent space
aids our model to achieve (1) anti-interference ability, which can eliminate
the effects of noise, bias fields, and artifacts, and (2) solid semantic
representation ability, with the potential of one-shot segmentation. Our code
is publicly available.",2024-07-03 08:37:01+00:00,"['Luyi Han', 'Tao Tan', 'Tianyu Zhang', 'Xin Wang', 'Yuan Gao', 'Chunyao Lu', 'Xinglong Liang', 'Haoran Dou', 'Yunzhi Huang', 'Ritse Mann']",http://arxiv.org/abs/2407.02911v1
cWDM: Conditional Wavelet Diffusion Models for Cross-Modality 3D Medical Image Synthesis,"This paper contributes to the ""BraTS 2024 Brain MR Image Synthesis Challenge""
and presents a conditional Wavelet Diffusion Model (cWDM) for directly solving
a paired image-to-image translation task on high-resolution volumes. While deep
learning-based brain tumor segmentation models have demonstrated clear clinical
utility, they typically require MR scans from various modalities (T1, T1ce, T2,
FLAIR) as input. However, due to time constraints or imaging artifacts, some of
these modalities may be missing, hindering the application of well-performing
segmentation algorithms in clinical routine. To address this issue, we propose
a method that synthesizes one missing modality image conditioned on three
available images, enabling the application of downstream segmentation models.
We treat this paired image-to-image translation task as a conditional
generation problem and solve it by combining a Wavelet Diffusion Model for
high-resolution 3D image synthesis with a simple conditioning strategy. This
approach allows us to directly apply our model to full-resolution volumes,
avoiding artifacts caused by slice- or patch-wise data processing. While this
work focuses on a specific application, the presented method can be applied to
all kinds of paired image-to-image translation problems, such as CT
$\leftrightarrow$ MR and MR $\leftrightarrow$ PET translation, or
mask-conditioned anatomically guided image generation.",2024-11-26 08:17:57+00:00,"['Paul Friedrich', 'Alicia Durrer', 'Julia Wolleb', 'Philippe C. Cattin']",http://arxiv.org/abs/2411.17203v1
Geometric Generative Models based on Morphological Equivariant PDEs and GANs,"Content and image generation consist in creating or generating data from
noisy information by extracting specific features such as texture, edges, and
other thin image structures. We are interested here in generative models, and
two main problems are addressed. Firstly, the improvements of specific feature
extraction while accounting at multiscale levels intrinsic geometric features;
and secondly, the equivariance of the network to reduce its complexity and
provide a geometric interpretability. To proceed, we propose a geometric
generative model based on an equivariant partial differential equation (PDE)
for group convolution neural networks (G-CNNs), so called PDE-G-CNNs, built on
morphology operators and generative adversarial networks (GANs). Equivariant
morphological PDE layers are composed of multiscale dilations and erosions
formulated in Riemannian manifolds, while group symmetries are defined on a Lie
group. We take advantage of the Lie group structure to properly integrate the
equivariance in layers, and are able to use the Riemannian metric to solve the
multiscale morphological operations. Each point of the Lie group is associated
with a unique point in the manifold, which helps us derive a metric on the
Riemannian manifold from a tensor field invariant under the Lie group so that
the induced metric has the same symmetries. The proposed geometric
morphological GAN (GM-GAN) is obtained by using the proposed morphological
equivariant convolutions in PDE-G-CNNs to bring nonlinearity in classical CNNs.
GM-GAN is evaluated on MNIST data and compared with GANs. Preliminary results
show that GM-GAN model outperforms classical GAN.",2024-03-22 01:02:09+00:00,"['El Hadji S. Diop', 'Thierno Fall', 'Alioune Mbengue', 'Mohamed Daoudi']",http://arxiv.org/abs/2403.14897v3
GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details,"Traditional 3D garment creation is labor-intensive, involving sketching,
modeling, UV mapping, and texturing, which are time-consuming and costly.
Recent advances in diffusion-based generative models have enabled new
possibilities for 3D garment generation from text prompts, images, and videos.
However, existing methods either suffer from inconsistencies among multi-view
images or require additional processes to separate cloth from the underlying
human model. In this paper, we propose GarmentDreamer, a novel method that
leverages 3D Gaussian Splatting (GS) as guidance to generate wearable,
simulation-ready 3D garment meshes from text prompts. In contrast to using
multi-view images directly predicted by generative models as guidance, our 3DGS
guidance ensures consistent optimization in both garment deformation and
texture synthesis. Our method introduces a novel garment augmentation module,
guided by normal and RGBA information, and employs implicit Neural Texture
Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate
diverse geometric and texture details. We validate the effectiveness of our
approach through comprehensive qualitative and quantitative experiments,
showcasing the superior performance of GarmentDreamer over state-of-the-art
alternatives. Our project page is available at:
https://xuan-li.github.io/GarmentDreamerDemo/.",2024-05-20 23:54:28+00:00,"['Boqian Li', 'Xuan Li', 'Ying Jiang', 'Tianyi Xie', 'Feng Gao', 'Huamin Wang', 'Yin Yang', 'Chenfanfu Jiang']",http://arxiv.org/abs/2405.12420v1
Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance,"The video-to-audio (V2A) generation task has drawn attention in the field of
multimedia due to the practicality in producing Foley sound. Semantic and
temporal conditions are fed to the generation model to indicate sound events
and temporal occurrence. Recent studies on synthesizing immersive and
synchronized audio are faced with challenges on videos with moving visual
presence. The temporal condition is not accurate enough while low-resolution
semantic condition exacerbates the problem. To tackle these challenges, we
propose Smooth-Foley, a V2A generative model taking semantic guidance from the
textual label across the generation to enhance both semantic and temporal
alignment in audio. Two adapters are trained to leverage pre-trained
text-to-audio generation models. A frame adapter integrates high-resolution
frame-wise video features while a temporal adapter integrates temporal
conditions obtained from similarities of visual frames and textual labels. The
incorporation of semantic guidance from textual labels achieves precise
audio-video alignment. We conduct extensive quantitative and qualitative
experiments. Results show that Smooth-Foley performs better than existing
models on both continuous sound scenarios and general scenarios. With semantic
guidance, the audio generated by Smooth-Foley exhibits higher quality and
better adherence to physical laws.",2024-12-24 04:29:46+00:00,"['Yaoyun Zhang', 'Xuenan Xu', 'Mengyue Wu']",http://arxiv.org/abs/2412.18157v1
Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with Motion and Appearance Disentanglement,"We aim to edit the lip movements in talking video according to the given
speech while preserving the personal identity and visual details. The task can
be decomposed into two sub-problems: (1) speech-driven lip motion generation
and (2) visual appearance synthesis. Current solutions handle the two
sub-problems within a single generative model, resulting in a challenging
trade-off between lip-sync quality and visual details preservation. Instead, we
propose to disentangle the motion and appearance, and then generate them one by
one with a speech-to-motion diffusion model and a motion-conditioned appearance
generation model. However, there still remain challenges in each stage, such as
motion-aware identity preservation in (1) and visual details preservation in
(2). Therefore, to preserve personal identity, we adopt landmarks to represent
the motion, and further employ a landmark-based identity loss. To capture
motion-agnostic visual details, we use separate encoders to encode the lip,
non-lip appearance and motion, and then integrate them with a learned fusion
module. We train MyTalk on a large-scale and diverse dataset. Experiments show
that our method generalizes well to the unknown, even out-of-domain person, in
terms of both lip sync and visual detail preservation. We encourage the readers
to watch the videos on our project page (https://Ingrid789.github.io/MyTalk/).",2024-06-12 11:22:03+00:00,"['Runyi Yu', 'Tianyu He', 'Ailing Zhang', 'Yuchi Wang', 'Junliang Guo', 'Xu Tan', 'Chang Liu', 'Jie Chen', 'Jiang Bian']",http://arxiv.org/abs/2406.08096v2
Pix2Gif: Motion-Guided Diffusion for GIF Generation,"We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)
generation. We tackle this problem differently by formulating the task as an
image translation problem steered by text and motion magnitude prompts, as
shown in teaser fig. To ensure that the model adheres to motion guidance, we
propose a new motion-guided warping module to spatially transform the features
of the source image conditioned on the two types of prompts. Furthermore, we
introduce a perceptual loss to ensure the transformed feature map remains
within the same space as the target image, ensuring content consistency and
coherence. In preparation for the model training, we meticulously curated data
by extracting coherent image frames from the TGIF video-caption dataset, which
provides rich information about the temporal changes of subjects. After
pretraining, we apply our model in a zero-shot manner to a number of video
datasets. Extensive qualitative and quantitative experiments demonstrate the
effectiveness of our model -- it not only captures the semantic prompt from
text but also the spatial ones from motion guidance. We train all our models
using a single node of 16xV100 GPUs. Code, dataset and models are made public
at: https://hiteshk03.github.io/Pix2Gif/.",2024-03-07 16:18:28+00:00,"['Hitesh Kandala', 'Jianfeng Gao', 'Jianwei Yang']",http://arxiv.org/abs/2403.04634v2
LegoPET: Hierarchical Feature Guided Conditional Diffusion for PET Image Reconstruction,"Positron emission tomography (PET) is widely utilized for cancer detection
due to its ability to visualize functional and biological processes in vivo.
PET images are usually reconstructed from histogrammed raw data (sinograms)
using traditional iterative techniques (e.g., OSEM, MLEM). Recently, deep
learning (DL) methods have shown promise by directly mapping raw sinogram data
to PET images. However, DL approaches that are regression-based or GAN-based
often produce overly smoothed images or introduce various artifacts
respectively. Image-conditioned diffusion probabilistic models (cDPMs) are
another class of likelihood-based DL techniques capable of generating highly
realistic and controllable images. While cDPMs have notable strengths, they
still face challenges such as maintain correspondence and consistency between
input and output images when they are from different domains (e.g., sinogram
vs. image domain) as well as slow convergence rates. To address these
limitations, we introduce LegoPET, a hierarchical feature guided conditional
diffusion model for high-perceptual quality PET image reconstruction from
sinograms. We conducted several experiments demonstrating that LegoPET not only
improves the performance of cDPMs but also surpasses recent DL-based PET image
reconstruction techniques in terms of visual quality and pixel-level PSNR/SSIM
metrics. Our code is available at https://github.com/yransun/LegoPET.",2024-11-25 18:05:34+00:00,"['Yiran Sun', 'Osama Mawlawi']",http://arxiv.org/abs/2411.16629v1
Generation and Detection of Sign Language Deepfakes - A Linguistic and Visual Analysis,"This research explores the positive application of deepfake technology for
upper body generation, specifically sign language for the Deaf and Hard of
Hearing (DHoH) community. Given the complexity of sign language and the
scarcity of experts, the generated videos are vetted by a sign language expert
for accuracy. We construct a reliable deepfake dataset, evaluating its
technical and visual credibility using computer vision and natural language
processing models. The dataset, consisting of over 1200 videos featuring both
seen and unseen individuals, is also used to detect deepfake videos targeting
vulnerable individuals. Expert annotations confirm that the generated videos
are comparable to real sign language content. Linguistic analysis, using
textual similarity scores and interpreter evaluations, shows that the
interpretation of generated videos is at least 90% similar to authentic sign
language. Visual analysis demonstrates that convincingly realistic deepfakes
can be produced, even for new subjects. Using a pose/style transfer model, we
pay close attention to detail, ensuring hand movements are accurate and align
with the driving video. We also apply machine learning algorithms to establish
a baseline for deepfake detection on this dataset, contributing to the
detection of fraudulent sign language videos.",2024-04-01 19:22:43+00:00,"['Shahzeb Naeem', 'Muhammad Riyyan Khan', 'Usman Tariq', 'Abhinav Dhall', 'Carlos Ivan Colon', 'Hasan Al-Nashash']",http://arxiv.org/abs/2404.01438v2
Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection,"Weakly supervised video anomaly detection (WSVAD) is a challenging task.
Generating fine-grained pseudo-labels based on weak-label and then
self-training a classifier is currently a promising solution. However, since
the existing methods use only RGB visual modality and the utilization of
category text information is neglected, thus limiting the generation of more
accurate pseudo-labels and affecting the performance of self-training. Inspired
by the manual labeling process based on the event description, in this paper,
we propose a novel pseudo-label generation and self-training framework based on
Text Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer
the rich language-visual knowledge of the contrastive language-image
pre-training (CLIP) model for aligning the video event description text and
corresponding video frames to generate pseudo-labels. Specifically, We first
fine-tune the CLIP for domain adaptation by designing two ranking losses and a
distributional inconsistency loss. Further, we propose a learnable text prompt
mechanism with the assist of a normality visual prompt to further improve the
matching accuracy of video event description text and video frames. Then, we
design a pseudo-label generation module based on the normality guidance to
infer reliable frame-level pseudo-labels. Finally, we introduce a temporal
context self-adaptive learning module to learn the temporal dependencies of
different video events more flexibly and accurately. Extensive experiments show
that our method achieves state-of-the-art performance on two benchmark
datasets, UCF-Crime and XD-Viole",2024-04-12 15:18:25+00:00,"['Zhiwei Yang', 'Jing Liu', 'Peng Wu']",http://arxiv.org/abs/2404.08531v1
NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative,"Existing video captioning benchmarks and models lack causal-temporal
narrative, which is sequences of events linked through cause and effect,
unfolding over time and driven by characters or agents. This lack of narrative
restricts models' ability to generate text descriptions that capture the causal
and temporal dynamics inherent in video content. To address this gap, we
propose NarrativeBridge, an approach comprising of: (1) a novel Causal-Temporal
Narrative (CTN) captions benchmark generated using a large language model and
few-shot prompting, explicitly encoding cause-effect temporal relationships in
video descriptions; and (2) a Cause-Effect Network (CEN) with separate encoders
for capturing cause and effect dynamics, enabling effective learning and
generation of captions with causal-temporal narrative. Extensive experiments
demonstrate that CEN significantly outperforms state-of-the-art models in
articulating the causal and temporal aspects of video content: 17.88 and 17.44
CIDEr on the MSVD-CTN and MSRVTT-CTN datasets, respectively. Cross-dataset
evaluations further showcase CEN's strong generalization capabilities. The
proposed framework understands and generates nuanced text descriptions with
intricate causal-temporal narrative structures present in videos, addressing a
critical limitation in video captioning. For project details, visit
https://narrativebridge.github.io/.",2024-06-10 17:34:24+00:00,"['Asmar Nadeem', 'Faegheh Sardari', 'Robert Dawes', 'Syed Sameed Husain', 'Adrian Hilton', 'Armin Mustafa']",http://arxiv.org/abs/2406.06499v3
Implicit Location-Caption Alignment via Complementary Masking for Weakly-Supervised Dense Video Captioning,"Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and
describe all events of interest in a video without requiring annotations of
event boundaries. This setting poses a great challenge in accurately locating
the temporal location of event, as the relevant supervision is unavailable.
Existing methods rely on explicit alignment constraints between event locations
and captions, which involve complex event proposal procedures during both
training and inference. To tackle this problem, we propose a novel implicit
location-caption alignment paradigm by complementary masking, which simplifies
the complex event proposal and localization process while maintaining
effectiveness. Specifically, our model comprises two components: a dual-mode
video captioning module and a mask generation module. The dual-mode video
captioning module captures global event information and generates descriptive
captions, while the mask generation module generates differentiable positive
and negative masks for localizing the events. These masks enable the implicit
alignment of event locations and captions by ensuring that captions generated
from positively and negatively masked videos are complementary, thereby forming
a complete video description. In this way, even under weak supervision, the
event location and event caption can be aligned implicitly. Extensive
experiments on the public datasets demonstrate that our method outperforms
existing weakly-supervised methods and achieves competitive results compared to
fully-supervised methods.",2024-12-17 10:52:50+00:00,"['Shiping Ge', 'Qiang Chen', 'Zhiwei Jiang', 'Yafeng Yin', 'Liu Qin', 'Ziyao Chen', 'Qing Gu']",http://arxiv.org/abs/2412.12791v2
Semantics Guided Disentangled GAN for Chest X-ray Image Rib Segmentation,"The label annotations for chest X-ray image rib segmentation are time
consuming and laborious, and the labeling quality heavily relies on medical
knowledge of annotators. To reduce the dependency on annotated data, existing
works often utilize generative adversarial network (GAN) to generate training
data. However, GAN-based methods overlook the nuanced information specific to
individual organs, which degrades the generation quality of chest X-ray image.
Hence, we propose a novel Semantics guided Disentangled GAN (SD-GAN), which can
generate the high-quality training data by fully utilizing the semantic
information of different organs, for chest X-ray image rib segmentation. In
particular, we use three ResNet50 branches to disentangle features of different
organs, then use a decoder to combine features and generate corresponding
images. To ensure that the generated images correspond to the input organ
labels in semantics tags, we employ a semantics guidance module to perform
semantic guidance on the generated images. To evaluate the efficacy of SD-GAN
in generating high-quality samples, we introduce modified TransUNet(MTUNet), a
specialized segmentation network designed for multi-scale contextual
information extracting and multi-branch decoding, effectively tackling the
challenge of organ overlap. We also propose a new chest X-ray image dataset
(CXRS). It includes 1250 samples from various medical institutions. Lungs,
clavicles, and 24 ribs are simultaneously annotated on each chest X-ray image.
The visualization and quantitative results demonstrate the efficacy of SD-GAN
in generating high-quality chest X-ray image-mask pairs. Using generated data,
our trained MTUNet overcomes the limitations of the data scale and outperforms
other segmentation networks.",2024-07-22 12:13:02+00:00,"['Lili Huang', 'Dexin Ma', 'Xiaowei Zhao', 'Chenglong Li', 'Haifeng Zhao', 'Jin Tang', 'Chuanfu Li']",http://arxiv.org/abs/2407.15903v1
InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation,"Recent talking avatar generation models have made strides in achieving
realistic and accurate lip synchronization with the audio, but often fall short
in controlling and conveying detailed expressions and emotions of the avatar,
making the generated video less vivid and controllable. In this paper, we
propose a novel text-guided approach for generating emotionally expressive 2D
avatars, offering fine-grained control, improved interactivity, and
generalizability to the resulting video. Our framework, named InstructAvatar,
leverages a natural language interface to control the emotion as well as the
facial motion of avatars. Technically, we design an automatic annotation
pipeline to construct an instruction-video paired training dataset, equipped
with a novel two-branch diffusion-based generator to predict avatars with audio
and text instructions at the same time. Experimental results demonstrate that
InstructAvatar produces results that align well with both conditions, and
outperforms existing methods in fine-grained emotion control, lip-sync quality,
and naturalness. Our project page is
https://wangyuchi369.github.io/InstructAvatar/.",2024-05-24 17:53:54+00:00,"['Yuchi Wang', 'Junliang Guo', 'Jianhong Bai', 'Runyi Yu', 'Tianyu He', 'Xu Tan', 'Xu Sun', 'Jiang Bian']",http://arxiv.org/abs/2405.15758v1
Stable Video Portraits,"Rapid advances in the field of generative AI and text-to-image methods in
particular have transformed the way we interact with and perceive
computer-generated imagery today. In parallel, much progress has been made in
3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we
present SVP, a novel hybrid 2D/3D generation method that outputs photorealistic
videos of talking faces leveraging a large pre-trained text-to-image prior
(2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific
fine-tuning of a general 2D stable diffusion model which we lift to a video
model by providing temporal 3DMM sequences as conditioning and by introducing a
temporal denoising procedure. As an output, this model generates temporally
smooth imagery of a person with 3DMM-based controls, i.e., a person-specific
avatar. The facial appearance of this person-specific avatar can be edited and
morphed to text-defined celebrities, without any fine-tuning at test time. The
method is analyzed quantitatively and qualitatively, and we show that our
method outperforms state-of-the-art monocular head avatar methods.",2024-09-26 17:26:18+00:00,"['Mirela Ostrek', 'Justus Thies']",http://arxiv.org/abs/2409.18083v1
EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion,"Diffusion models have revolutionized the field of talking head generation,
yet still face challenges in expressiveness, controllability, and stability in
long-time generation. In this research, we propose an EmotiveTalk framework to
address these issues. Firstly, to realize better control over the generation of
lip movement and facial expression, a Vision-guided Audio Information
Decoupling (V-AID) approach is designed to generate audio-based decoupled
representations aligned with lip movements and expression. Specifically, to
achieve alignment between audio and facial expression representation spaces, we
present a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within
V-AID to generate expression-related representations under multi-source emotion
condition constraints. Then we propose a well-designed Emotional Talking Head
Diffusion (ETHD) backbone to efficiently generate highly expressive talking
head videos, which contains an Expression Decoupling Injection (EDI) module to
automatically decouple the expressions from reference portraits while
integrating the target expression information, achieving more expressive
generation performance. Experimental results show that EmotiveTalk can generate
expressive talking head videos, ensuring the promised controllability of
emotions and stability during long-time generation, yielding state-of-the-art
performance compared to existing methods.",2024-11-23 04:38:51+00:00,"['Haotian Wang', 'Yuzhe Weng', 'Yueyan Li', 'Zilu Guo', 'Jun Du', 'Shutong Niu', 'Jiefeng Ma', 'Shan He', 'Xiaoyan Wu', 'Qiming Hu', 'Bing Yin', 'Cong Liu', 'Qingfeng Liu']",http://arxiv.org/abs/2411.16726v2
Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases,"Anatomical atlases are widely used for population analysis. Conditional
atlases target a particular sub-population defined via certain conditions (e.g.
demographics or pathologies) and allow for the investigation of fine-grained
anatomical differences - such as morphological changes correlated with age.
Existing approaches use either registration-based methods that are unable to
handle large anatomical variations or generative models, which can suffer from
training instabilities and hallucinations. To overcome these limitations, we
use latent diffusion models to generate deformation fields, which transform a
general population atlas into one representing a specific sub-population. By
generating a deformation field and registering the conditional atlas to a
neighbourhood of images, we ensure structural plausibility and avoid
hallucinations, which can occur during direct image synthesis. We compare our
method to several state-of-the-art atlas generation methods in experiments
using 5000 brain as well as whole-body MR images from UK Biobank. Our method
generates highly realistic atlases with smooth transformations and high
anatomical fidelity, outperforming the baselines.",2024-03-25 13:52:48+00:00,"['Sophie Starck', 'Vasiliki Sideri-Lampretsa', 'Bernhard Kainz', 'Martin Menten', 'Tamara Mueller', 'Daniel Rueckert']",http://arxiv.org/abs/2403.16776v1
"SDS -- See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration","In this paper, we present SDS (``See it. Do it. Sorted.''), a novel pipeline
for intuitive quadrupedal skill learning from a single demonstration video.
Leveraging the Visual capabilities of GPT-4o, SDS processes input videos
through our novel chain-of-thought promoting technique (SUS) and generates
executable reward functions (RFs) that drive the imitation of locomotion
skills, through learning a Proximal Policy Optimization (PPO)-based
Reinforcement Learning (RL) policy, using environment information from the
NVIDIA IsaacGym simulator. SDS autonomously evaluates the RFs by monitoring the
individual reward components and supplying training footage and fitness metrics
back into GPT-4o, which is then prompted to evolve the RFs to achieve higher
task fitness at each iteration. We validate our method on the Unitree Go1
robot, demonstrating its ability to execute variable skills such as trotting,
bounding, pacing and hopping, achieving high imitation fidelity and locomotion
stability. SDS shows improvements over SOTA methods in task adaptability,
reduced dependence on domain-specific knowledge, and bypassing the need for
labor-intensive reward engineering and large-scale training datasets.
Additional information and the open-sourced code can be found in:
https://rpl-cs-ucl.github.io/SDSweb",2024-10-15 13:04:11+00:00,"['Jeffrey Li', 'Maria Stamatopoulou', 'Dimitrios Kanoulas']",http://arxiv.org/abs/2410.11571v1
Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection,"Skeleton-based video anomaly detection (SVAD) is a crucial task in computer
vision. Accurately identifying abnormal patterns or events enables operators to
promptly detect suspicious activities, thereby enhancing safety. Achieving this
demands a comprehensive understanding of human motions, both at body and region
levels, while also accounting for the wide variations of performing a single
action. However, existing studies fail to simultaneously address these crucial
properties. This paper introduces a novel, practical and lightweight framework,
namely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video
Anomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD.
GiCiSAD consists of three novel modules: the Graph Attention-based Forecasting
module to capture the spatio-temporal dependencies inherent in the data, the
Graph-level Jigsaw Puzzle Maker module to distinguish subtle region-level
discrepancies between normal and abnormal motions, and the Graph-based
Conditional Diffusion model to generate a wide spectrum of human motions.
Extensive experiments on four widely used skeleton-based video datasets show
that GiCiSAD outperforms existing methods with significantly fewer training
parameters, establishing it as the new state-of-the-art.",2024-03-18 18:42:32+00:00,"['Ali Karami', 'Thi Kieu Khanh Ho', 'Narges Armanfard']",http://arxiv.org/abs/2403.12172v2
MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization,"Generating music that aligns with the visual content of a video has been a
challenging task, as it requires a deep understanding of visual semantics and
involves generating music whose melody, rhythm, and dynamics harmonize with the
visual narratives. This paper presents MuVi, a novel framework that effectively
addresses these challenges to enhance the cohesion and immersive experience of
audio-visual content. MuVi analyzes video content through a specially designed
visual adaptor to extract contextually and temporally relevant features. These
features are used to generate music that not only matches the video's mood and
theme but also its rhythm and pacing. We also introduce a contrastive
music-visual pre-training scheme to ensure synchronization, based on the
periodicity nature of music phrases. In addition, we demonstrate that our
flow-matching-based music generator has in-context learning ability, allowing
us to control the style and genre of the generated music. Experimental results
show that MuVi demonstrates superior performance in both audio quality and
temporal synchronization. The generated music video samples are available at
https://muvi-v2m.github.io.",2024-10-16 18:44:56+00:00,"['Ruiqi Li', 'Siqi Zheng', 'Xize Cheng', 'Ziang Zhang', 'Shengpeng Ji', 'Zhou Zhao']",http://arxiv.org/abs/2410.12957v1
A Chinese Continuous Sign Language Dataset Based on Complex Environments,"The current bottleneck in continuous sign language recognition (CSLR)
research lies in the fact that most publicly available datasets are limited to
laboratory environments or television program recordings, resulting in a single
background environment with uniform lighting, which significantly deviates from
the diversity and complexity found in real-life scenarios. To address this
challenge, we have constructed a new, large-scale dataset for Chinese
continuous sign language (CSL) based on complex environments, termed the
complex environment - chinese sign language dataset (CE-CSL). This dataset
encompasses 5,988 continuous CSL video clips collected from daily life scenes,
featuring more than 70 different complex backgrounds to ensure
representativeness and generalization capability. To tackle the impact of
complex backgrounds on CSLR performance, we propose a time-frequency network
(TFNet) model for continuous sign language recognition. This model extracts
frame-level features and then utilizes both temporal and spectral information
to separately derive sequence features before fusion, aiming to achieve
efficient and accurate CSLR. Experimental results demonstrate that our approach
achieves significant performance improvements on the CE-CSL, validating its
effectiveness under complex background conditions. Additionally, our proposed
method has also yielded highly competitive results when applied to three
publicly available CSL datasets.",2024-09-18 13:11:15+00:00,"['Qidan Zhu', 'Jing Li', 'Fei Yuan', 'Jiaojiao Fan', 'Quan Gan']",http://arxiv.org/abs/2409.11960v1
DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion,"In extreme scenarios such as nighttime or low-visibility environments,
achieving reliable perception is critical for applications like autonomous
driving, robotics, and surveillance. Multi-modality image fusion, particularly
integrating infrared imaging, offers a robust solution by combining
complementary information from different modalities to enhance scene
understanding and decision-making. However, current methods face significant
limitations: GAN-based approaches often produce blurry images that lack
fine-grained details, while AE-based methods may introduce bias toward specific
modalities, leading to unnatural fusion results. To address these challenges,
we propose DAE-Fuse, a novel two-phase discriminative autoencoder framework
that generates sharp and natural fused images. Furthermore, We pioneer the
extension of image fusion techniques from static images to the video domain
while preserving temporal consistency across frames, thus advancing the
perceptual capabilities required for autonomous navigation. Extensive
experiments on public datasets demonstrate that DAE-Fuse achieves
state-of-the-art performance on multiple benchmarks, with superior
generalizability to tasks like medical image fusion.",2024-09-16 08:37:09+00:00,"['Yuchen Guo', 'Ruoxiang Xu', 'Rongcheng Li', 'Zhenghao Wu', 'Weifeng Su']",http://arxiv.org/abs/2409.10080v2
VM-DDPM: Vision Mamba Diffusion for Medical Image Synthesis,"In the realm of smart healthcare, researchers enhance the scale and diversity
of medical datasets through medical image synthesis. However, existing methods
are limited by CNN local perception and Transformer quadratic complexity,
making it difficult to balance structural texture consistency. To this end, we
propose the Vision Mamba DDPM (VM-DDPM) based on State Space Model (SSM), fully
combining CNN local perception and SSM global modeling capabilities, while
maintaining linear computational complexity. Specifically, we designed a
multi-level feature extraction module called Multi-level State Space Block
(MSSBlock), and a basic unit of encoder-decoder structure called State Space
Layer (SSLayer) for medical pathological images. Besides, we designed a simple,
Plug-and-Play, zero-parameter Sequence Regeneration strategy for the Cross-Scan
Module (CSM), which enabled the S6 module to fully perceive the spatial
features of the 2D image and stimulate the generalization potential of the
model. To our best knowledge, this is the first medical image synthesis model
based on the SSM-CNN hybrid architecture. Our experimental evaluation on three
datasets of different scales, i.e., ACDC, BraTS2018, and ChestXRay, as well as
qualitative evaluation by radiologists, demonstrate that VM-DDPM achieves
state-of-the-art performance.",2024-05-09 10:41:18+00:00,"['Zhihan Ju', 'Wanting Zhou']",http://arxiv.org/abs/2405.05667v1
Inverse Painting: Reconstructing The Painting Process,"Given an input painting, we reconstruct a time-lapse video of how it may have
been painted. We formulate this as an autoregressive image generation problem,
in which an initially blank ""canvas"" is iteratively updated. The model learns
from real artists by training on many painting videos. Our approach
incorporates text and region understanding to define a set of painting
""instructions"" and updates the canvas with a novel diffusion-based renderer.
The method extrapolates beyond the limited, acrylic style paintings on which it
has been trained, showing plausible results for a wide range of artistic styles
and genres.",2024-09-30 17:56:52+00:00,"['Bowei Chen', 'Yifan Wang', 'Brian Curless', 'Ira Kemelmacher-Shlizerman', 'Steven M. Seitz']",http://arxiv.org/abs/2409.20556v2
Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation,"We introduce Infinigen Indoors, a Blender-based procedural generator of
photorealistic indoor scenes. It builds upon the existing Infinigen system,
which focuses on natural scenes, but expands its coverage to indoor scenes by
introducing a diverse library of procedural indoor assets, including furniture,
architecture elements, appliances, and other day-to-day objects. It also
introduces a constraint-based arrangement system, which consists of a
domain-specific language for expressing diverse constraints on scene
composition, and a solver that generates scene compositions that maximally
satisfy the constraints. We provide an export tool that allows the generated 3D
objects and scenes to be directly used for training embodied agents in
real-time simulators such as Omniverse and Unreal. Infinigen Indoors is
open-sourced under the BSD license. Please visit https://infinigen.org for code
and videos.",2024-06-17 17:57:50+00:00,"['Alexander Raistrick', 'Lingjie Mei', 'Karhan Kayan', 'David Yan', 'Yiming Zuo', 'Beining Han', 'Hongyu Wen', 'Meenal Parakh', 'Stamatis Alexandropoulos', 'Lahav Lipson', 'Zeyu Ma', 'Jia Deng']",http://arxiv.org/abs/2406.11824v1
Diffusion Models in 3D Vision: A Survey,"In recent years, 3D vision has become a crucial field within computer vision,
powering a wide range of applications such as autonomous driving, robotics,
augmented reality (AR), and medical imaging. This field relies on the accurate
perception, understanding, and reconstruction of 3D scenes from 2D data sources
like images and videos. Diffusion models, originally designed for 2D generative
tasks, offer the potential for more flexible, probabilistic approaches that can
better capture the variability and uncertainty present in real-world 3D data.
However, traditional methods often struggle with efficiency and scalability. In
this paper, we review the state-of-the-art approaches that leverage diffusion
models for 3D visual tasks, including but not limited to 3D object generation,
shape completion, point cloud reconstruction, and scene understanding. We
provide an in-depth discussion of the underlying mathematical principles of
diffusion models, outlining their forward and reverse processes, as well as the
various architectural advancements that enable these models to work with 3D
datasets. We also discuss the key challenges in applying diffusion models to 3D
vision, such as handling occlusions and varying point densities, and the
computational demands of high-dimensional data. Finally, we discuss potential
solutions, including improving computational efficiency, enhancing multimodal
fusion, and exploring the use of large-scale pretraining for better
generalization across 3D tasks. This paper serves as a foundation for future
exploration and development in this rapidly evolving field.",2024-10-07 04:12:23+00:00,"['Zhen Wang', 'Dongyuan Li', 'Renhe Jiang']",http://arxiv.org/abs/2410.04738v2
Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN Efficiency via Knowledge Distillation,"In this paper, we address the challenge of compressing generative adversarial
networks (GANs) for deployment in resource-constrained environments by
proposing two novel methodologies: Distribution Matching for Efficient
compression (DiME) and Network Interactive Compression via Knowledge Exchange
and Learning (NICKEL). DiME employs foundation models as embedding kernels for
efficient distribution matching, leveraging maximum mean discrepancy to
facilitate effective knowledge distillation. Simultaneously, NICKEL employs an
interactive compression method that enhances the communication between the
student generator and discriminator, achieving a balanced and stable
compression process. Our comprehensive evaluation on the StyleGAN2 architecture
with the FFHQ dataset shows the effectiveness of our approach, with NICKEL &
DiME achieving FID scores of 10.45 and 15.93 at compression rates of 95.73% and
98.92%, respectively. Remarkably, our methods sustain generative quality even
at an extreme compression rate of 99.69%, surpassing the previous
state-of-the-art performance by a large margin. These findings not only
demonstrate our methodologies' capacity to significantly lower GANs'
computational demands but also pave the way for deploying high-quality GAN
models in settings with limited resources. Our code will be released soon.",2024-05-19 17:09:43+00:00,"['Sangyeop Yeo', 'Yoojin Jang', 'Jaejun Yoo']",http://arxiv.org/abs/2405.11614v2
Enhancing Alzheimer's Disease Prediction: A Novel Approach to Leveraging GAN-Augmented Data for Improved CNN Model Accuracy,"Alzheimer's Disease (AD) is a neurodegenerative disease affecting millions of
individuals across the globe. As the prevalence of this disease continues to
rise, early diagnosis is crucial to improve clinical outcomes. Neural networks,
specifically Convolutional Neural Networks (CNNs), are promising tools for
diagnosing individuals with Alzheimer's. However, neural networks such as ANNs
and CNNs typically yield lower validation accuracies when fed lower quantities
of data. Hence, Generative Adversarial Networks (GANs) can be utilized to
synthesize data to augment these existing MRI datasets, potentially yielding
higher validation accuracies. In this study, we use this principle while
examining a novel application of the SSMI metric in selecting high-quality
synthetic data generated by our GAN to compare its accuracies with shuffled
data generated by our GAN. We observed that incorporating GANs with an SSMI
metric returned the highest accuracies when compared to a traditional dataset.",2024-09-03 19:34:22+00:00,"['Akshay Sunkara', 'Rajiv Morthala', 'Anav Jain', 'Srinjoy Ghose', 'Santosh Morthala']",http://arxiv.org/abs/2409.02961v1
The Value of AI-Generated Metadata for UGC Platforms: Evidence from a Large-scale Field Experiment,"AI-generated content (AIGC), such as advertisement copy, product
descriptions, and social media posts, is becoming ubiquitous in business
practices. However, the value of AI-generated metadata, such as titles, remains
unclear on user-generated content (UGC) platforms. To address this gap, we
conducted a large-scale field experiment on a leading short-video platform in
Asia to provide about 1 million users access to AI-generated titles for their
uploaded videos. Our findings show that the provision of AI-generated titles
significantly boosted content consumption, increasing valid watches by 1.6% and
watch duration by 0.9%. When producers adopted these titles, these increases
jumped to 7.1% and 4.1%, respectively. This viewership-boost effect was largely
attributed to the use of this generative AI (GAI) tool increasing the
likelihood of videos having a title by 41.4%. The effect was more pronounced
for groups more affected by metadata sparsity. Mechanism analysis revealed that
AI-generated metadata improved user-video matching accuracy in the platform's
recommender system. Interestingly, for a video for which the producer would
have posted a title anyway, adopting the AI-generated title decreased its
viewership on average, implying that AI-generated titles may be of lower
quality than human-generated ones. However, when producers chose to co-create
with GAI and significantly revised the AI-generated titles, the videos
outperformed their counterparts with either fully AI-generated or
human-generated titles, showcasing the benefits of human-AI co-creation. This
study highlights the value of AI-generated metadata and human-AI metadata
co-creation in enhancing user-content matching and content consumption for UGC
platforms.",2024-12-24 10:47:27+00:00,"['Xinyi Zhang', 'Chenshuo Sun', 'Renyu Zhang', 'Khim-Yong Goh']",http://arxiv.org/abs/2412.18337v1
3D$^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic Gaussian Avatar Modeling,"Advancements in neural implicit representations and differentiable rendering
have markedly improved the ability to learn animatable 3D avatars from sparse
multi-view RGB videos. However, current methods that map observation space to
canonical space often face challenges in capturing pose-dependent details and
generalizing to novel poses. While diffusion models have demonstrated
remarkable zero-shot capabilities in 2D image generation, their potential for
creating animatable 3D avatars from 2D inputs remains underexplored. In this
work, we introduce 3D$^2$-Actor, a novel approach featuring a pose-conditioned
3D-aware human modeling pipeline that integrates iterative 2D denoising and 3D
rectifying steps. The 2D denoiser, guided by pose cues, generates detailed
multi-view images that provide the rich feature set necessary for high-fidelity
3D reconstruction and pose rendering. Complementing this, our Gaussian-based 3D
rectifier renders images with enhanced 3D consistency through a two-stage
projection strategy and a novel local coordinate representation. Additionally,
we propose an innovative sampling strategy to ensure smooth temporal continuity
across frames in video synthesis. Our method effectively addresses the
limitations of traditional numerical solutions in handling ill-posed mappings,
producing realistic and animatable 3D human avatars. Experimental results
demonstrate that 3D$^2$-Actor excels in high-fidelity avatar modeling and
robustly generalizes to novel poses. Code is available at:
https://github.com/silence-tang/GaussianActor.",2024-12-16 09:37:52+00:00,"['Zichen Tang', 'Hongyu Yang', 'Hanchen Zhang', 'Jiaxin Chen', 'Di Huang']",http://arxiv.org/abs/2412.11599v1
Towards Generalist Robot Learning from Internet Video: A Survey,"Scaling deep learning to massive, diverse internet data has yielded
remarkably general capabilities in visual and natural language understanding
and generation. However, data has remained scarce and challenging to collect in
robotics, seeing robot learning struggle to obtain similarly general
capabilities. Promising Learning from Videos (LfV) methods aim to address the
robotics data bottleneck by augmenting traditional robot data with large-scale
internet video data. This video data offers broad foundational information
regarding physical behaviour and the underlying physics of the world, and thus
can be highly informative for a generalist robot.
  In this survey, we present a thorough overview of the emerging field of LfV.
We outline fundamental concepts, including the benefits and challenges of LfV.
We provide a comprehensive review of current methods for extracting knowledge
from large-scale internet video, addressing key challenges in LfV, and boosting
downstream robot and reinforcement learning via the use of video data. The
survey concludes with a critical discussion of challenges and opportunities in
LfV. Here, we advocate for scalable foundation model approaches that can
leverage the full range of available internet video to improve the learning of
robot policies and dynamics models. We hope this survey can inform and catalyse
further LfV research, driving progress towards the development of
general-purpose robots.",2024-04-30 15:57:41+00:00,"['Robert McCarthy', 'Daniel C. H. Tan', 'Dominik Schmidt', 'Fernando Acero', 'Nathan Herr', 'Yilun Du', 'Thomas G. Thuruthel', 'Zhibin Li']",http://arxiv.org/abs/2404.19664v4
FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds,"We study Neural Foley, the automatic generation of high-quality sound effects
synchronizing with videos, enabling an immersive audio-visual experience.
Despite its wide range of applications, existing approaches encounter
limitations when it comes to simultaneously synthesizing high-quality and
video-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To
overcome these limitations, we propose FoleyCrafter, a novel framework that
leverages a pre-trained text-to-audio model to ensure high-quality audio
generation. FoleyCrafter comprises two key components: the semantic adapter for
semantic alignment and the temporal controller for precise audio-video
synchronization. The semantic adapter utilizes parallel cross-attention layers
to condition audio generation on video features, producing realistic sound
effects that are semantically relevant to the visual content. Meanwhile, the
temporal controller incorporates an onset detector and a timestampbased adapter
to achieve precise audio-video alignment. One notable advantage of FoleyCrafter
is its compatibility with text prompts, enabling the use of text descriptions
to achieve controllable and diverse video-to-audio generation according to user
intents. We conduct extensive quantitative and qualitative experiments on
standard benchmarks to verify the effectiveness of FoleyCrafter. Models and
codes are available at https://github.com/open-mmlab/FoleyCrafter.",2024-07-01 17:35:56+00:00,"['Yiming Zhang', 'Yicheng Gu', 'Yanhong Zeng', 'Zhening Xing', 'Yuancheng Wang', 'Zhizheng Wu', 'Kai Chen']",http://arxiv.org/abs/2407.01494v1
Standardizing Generative Face Video Compression using Supplemental Enhancement Information,"This paper proposes a Generative Face Video Compression (GFVC) approach using
Supplemental Enhancement Information (SEI), where a series of compact spatial
and temporal representations of a face video signal (i.e., 2D/3D key-points,
facial semantics and compact features) can be coded using SEI message and
inserted into the coded video bitstream. At the time of writing, the proposed
GFVC approach using SEI messages has been adopted into the official working
draft of Versatile Supplemental Enhancement Information (VSEI) standard by the
Joint Video Experts Team (JVET) of ISO/IEC JTC 1/SC 29 and ITU-T SG16, which
will be standardized as a new version for ""ITU-T H.274 | ISO/IEC 23002-7"". To
the best of the authors' knowledge, the JVET work on the proposed SEI-based
GFVC approach is the first standardization activity for generative video
compression. The proposed SEI approach has not only advanced the reconstruction
quality of early-day Model-Based Coding (MBC) via the state-of-the-art
generative technique, but also established a new SEI definition for future GFVC
applications and deployment. Experimental results illustrate that the proposed
SEI-based GFVC approach can achieve remarkable rate-distortion performance
compared with the latest Versatile Video Coding (VVC) standard, whilst also
potentially enabling a wide variety of functionalities including user-specified
animation/filtering and metaverse-related applications.",2024-10-19 13:37:24+00:00,"['Bolin Chen', 'Yan Ye', 'Jie Chen', 'Ru-Ling Liao', 'Shanzhi Yin', 'Shiqi Wang', 'Kaifa Yang', 'Yue Li', 'Yiling Xu', 'Ye-Kui Wang', 'Shiv Gehlot', 'Guan-Ming Su', 'Peng Yin', 'Sean McCarthy', 'Gary J. Sullivan']",http://arxiv.org/abs/2410.15105v2
Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level,"In this paper, we introduce Motion-Grounded Video Reasoning, a new motion
understanding task that requires generating visual answers (video segmentation
masks) according to the input question, and hence needs implicit spatiotemporal
reasoning and grounding. This task extends existing spatiotemporal grounding
work focusing on explicit action/motion grounding, to a more general format by
enabling implicit reasoning via questions. To facilitate the development of the
new task, we collect a large-scale dataset called GROUNDMORE, which comprises
1,715 video clips, 249K object masks that are deliberately designed with 4
question types (Causal, Sequential, Counterfactual, and Descriptive) for
benchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE
uniquely requires models to generate visual answers, providing a more concrete
and visually interpretable response than plain texts. It evaluates models on
both spatiotemporal grounding and reasoning, fostering to address complex
challenges in motion-related video reasoning, temporal perception, and
pixel-level understanding. Furthermore, we introduce a novel baseline model
named Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the
multimodal reasoning ability from the Multimodal LLM, the pixel-level
perception capability from the grounding model (SAM), and the temporal
perception ability from a lightweight localization head. MORA achieves
respectable performance on GROUNDMORE outperforming the best existing visual
grounding baseline model by an average of 21.5% relatively. We hope this novel
and challenging task will pave the way for future advancements in robust and
general motion understanding via video reasoning segmentation",2024-11-15 03:45:09+00:00,"['Andong Deng', 'Tongjia Chen', 'Shoubin Yu', 'Taojiannan Yang', 'Lincoln Spencer', 'Yapeng Tian', 'Ajmal Saeed Mian', 'Mohit Bansal', 'Chen Chen']",http://arxiv.org/abs/2411.09921v1
Text-Driven Traffic Anomaly Detection with Temporal High-Frequency Modeling in Driving Videos,"Traffic anomaly detection (TAD) in driving videos is critical for ensuring
the safety of autonomous driving and advanced driver assistance systems.
Previous single-stage TAD methods primarily rely on frame prediction, making
them vulnerable to interference from dynamic backgrounds induced by the rapid
movement of the dashboard camera. While two-stage TAD methods appear to be a
natural solution to mitigate such interference by pre-extracting
background-independent features (such as bounding boxes and optical flow) using
perceptual algorithms, they are susceptible to the performance of first-stage
perceptual algorithms and may result in error propagation. In this paper, we
introduce TTHF, a novel single-stage method aligning video clips with text
prompts, offering a new perspective on traffic anomaly detection. Unlike
previous approaches, the supervised signal of our method is derived from
languages rather than orthogonal one-hot vectors, providing a more
comprehensive representation. Further, concerning visual representation, we
propose to model the high frequency of driving videos in the temporal domain.
This modeling captures the dynamic changes of driving scenes, enhances the
perception of driving behavior, and significantly improves the detection of
traffic anomalies. In addition, to better perceive various types of traffic
anomalies, we carefully design an attentive anomaly focusing mechanism that
visually and linguistically guides the model to adaptively focus on the visual
context of interest, thereby facilitating the detection of traffic anomalies.
It is shown that our proposed TTHF achieves promising performance,
outperforming state-of-the-art competitors by +5.4% AUC on the DoTA dataset and
achieving high generalization on the DADA dataset.",2024-01-07 15:47:19+00:00,"['Rongqin Liang', 'Yuanman Li', 'Jiantao Zhou', 'Xia Li']",http://arxiv.org/abs/2401.03522v2
Matching Anything by Segmenting Anything,"The robust association of the same objects across video frames in complex
scenes is crucial for many applications, especially Multiple Object Tracking
(MOT). Current methods predominantly rely on labeled domain-specific video
datasets, which limits the cross-domain generalization of learned similarity
embeddings. We propose MASA, a novel method for robust instance association
learning, capable of matching any objects within videos across diverse domains
without tracking labels. Leveraging the rich object segmentation from the
Segment Anything Model (SAM), MASA learns instance-level correspondence through
exhaustive data transformations. We treat the SAM outputs as dense object
region proposals and learn to match those regions from a vast image collection.
We further design a universal MASA adapter which can work in tandem with
foundational segmentation or detection models and enable them to track any
detected objects. Those combinations present strong zero-shot tracking ability
in complex domains. Extensive tests on multiple challenging MOT and MOTS
benchmarks indicate that the proposed method, using only unlabeled static
images, achieves even better performance than state-of-the-art methods trained
with fully annotated in-domain video sequences, in zero-shot association.
Project Page: https://matchinganything.github.io/",2024-06-06 16:20:07+00:00,"['Siyuan Li', 'Lei Ke', 'Martin Danelljan', 'Luigi Piccinelli', 'Mattia Segu', 'Luc Van Gool', 'Fisher Yu']",http://arxiv.org/abs/2406.04221v1
Combining Pre- and Post-Demosaicking Noise Removal for RAW Video,"Denoising is one of the fundamental steps of the processing pipeline that
converts data captured by a camera sensor into a display-ready image or video.
It is generally performed early in the pipeline, usually before demosaicking,
although studies swapping their order or even conducting them jointly have been
proposed. With the advent of deep learning, the quality of denoising algorithms
has steadily increased. Even so, modern neural networks still have a hard time
adapting to new noise levels and scenes, which is indispensable for real-world
applications. With those in mind, we propose a self-similarity-based denoising
scheme that weights both a pre- and a post-demosaicking denoiser for
Bayer-patterned CFA video data. We show that a balance between the two leads to
better image quality, and we empirically find that higher noise levels benefit
from a higher influence pre-demosaicking. We also integrate temporal trajectory
prefiltering steps before each denoiser, which further improve texture
reconstruction. The proposed method only requires an estimation of the noise
model at the sensor, accurately adapts to any noise level, and is competitive
with the state of the art, making it suitable for real-world videography.",2024-10-03 15:20:19+00:00,"['Marco Snchez-Beeckman', 'Antoni Buades', 'Nicola Brandonisio', 'Bilel Kanoun']",http://arxiv.org/abs/2410.02572v2
EgoMimic: Scaling Imitation Learning via Egocentric Video,"The scale and diversity of demonstration data required for imitation learning
is a significant challenge. We present EgoMimic, a full-stack framework which
scales manipulation via human embodiment data, specifically egocentric human
videos paired with 3D hand tracking. EgoMimic achieves this through: (1) a
system to capture human embodiment data using the ergonomic Project Aria
glasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap
to human data, (3) cross-domain data alignment techniques, and (4) an imitation
learning architecture that co-trains on human and robot data. Compared to prior
works that only extract high-level intent from human videos, our approach
treats human and robot data equally as embodied demonstration data and learns a
unified policy from both data sources. EgoMimic achieves significant
improvement on a diverse set of long-horizon, single-arm and bimanual
manipulation tasks over state-of-the-art imitation learning methods and enables
generalization to entirely new scenes. Finally, we show a favorable scaling
trend for EgoMimic, where adding 1 hour of additional hand data is
significantly more valuable than 1 hour of additional robot data. Videos and
additional information can be found at https://egomimic.github.io/",2024-10-31 17:59:55+00:00,"['Simar Kareer', 'Dhruv Patel', 'Ryan Punamiya', 'Pranay Mathur', 'Shuo Cheng', 'Chen Wang', 'Judy Hoffman', 'Danfei Xu']",http://arxiv.org/abs/2410.24221v1
Sync4D: Video Guided Controllable Dynamics for Physics-Based 4D Generation,"In this work, we introduce a novel approach for creating controllable
dynamics in 3D-generated Gaussians using casually captured reference videos.
Our method transfers the motion of objects from reference videos to a variety
of generated 3D Gaussians across different categories, ensuring precise and
customizable motion transfer. We achieve this by employing blend skinning-based
non-parametric shape reconstruction to extract the shape and motion of
reference objects. This process involves segmenting the reference objects into
motion-related parts based on skinning weights and establishing shape
correspondences with generated target shapes. To address shape and temporal
inconsistencies prevalent in existing methods, we integrate physical
simulation, driving the target shapes with matched motion. This integration is
optimized through a displacement loss to ensure reliable and genuine dynamics.
Our approach supports diverse reference inputs, including humans, quadrupeds,
and articulated objects, and can generate dynamics of arbitrary length,
providing enhanced fidelity and applicability. Unlike methods heavily reliant
on diffusion video generation models, our technique offers specific and
high-quality motion transfer, maintaining both shape integrity and temporal
consistency.",2024-05-27 05:49:12+00:00,"['Zhoujie Fu', 'Jiacheng Wei', 'Wenhao Shen', 'Chaoyue Song', 'Xiaofeng Yang', 'Fayao Liu', 'Xulei Yang', 'Guosheng Lin']",http://arxiv.org/abs/2405.16849v3
Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching,"Video-to-audio (V2A) generation aims to synthesize content-matching audio
from silent video, and it remains challenging to build V2A models with high
generation quality, efficiency, and visual-audio temporal synchrony. We propose
Frieren, a V2A model based on rectified flow matching. Frieren regresses the
conditional transport vector field from noise to spectrogram latent with
straight paths and conducts sampling by solving ODE, outperforming
autoregressive and score-based models in terms of audio quality. By employing a
non-autoregressive vector field estimator based on a feed-forward transformer
and channel-level cross-modal feature fusion with strong temporal alignment,
our model generates audio that is highly synchronized with the input video.
Furthermore, through reflow and one-step distillation with guided vector field,
our model can generate decent audio in a few, or even only one sampling step.
Experiments indicate that Frieren achieves state-of-the-art performance in both
generation quality and temporal alignment on VGGSound, with alignment accuracy
reaching 97.22%, and 6.2% improvement in inception score over the strong
diffusion-based baseline. Audio samples are available at
http://frieren-v2a.github.io.",2024-06-01 06:40:22+00:00,"['Yongqi Wang', 'Wenxiang Guo', 'Rongjie Huang', 'Jiawei Huang', 'Zehan Wang', 'Fuming You', 'Ruiqi Li', 'Zhou Zhao']",http://arxiv.org/abs/2406.00320v4
Synthetic Brain Images: Bridging the Gap in Brain Mapping With Generative Adversarial Model,"Magnetic Resonance Imaging (MRI) is a vital modality for gaining precise
anatomical information, and it plays a significant role in medical imaging for
diagnosis and therapy planning. Image synthesis problems have seen a revolution
in recent years due to the introduction of deep learning techniques,
specifically Generative Adversarial Networks (GANs). This work investigates the
use of Deep Convolutional Generative Adversarial Networks (DCGAN) for producing
high-fidelity and realistic MRI image slices. The suggested approach uses a
dataset with a variety of brain MRI scans to train a DCGAN architecture. While
the discriminator network discerns between created and real slices, the
generator network learns to synthesise realistic MRI image slices. The
generator refines its capacity to generate slices that closely mimic real MRI
data through an adversarial training approach. The outcomes demonstrate that
the DCGAN promise for a range of uses in medical imaging research, since they
show that it can effectively produce MRI image slices if we train them for a
consequent number of epochs. This work adds to the expanding corpus of research
on the application of deep learning techniques for medical image synthesis. The
slices that are could be produced possess the capability to enhance datasets,
provide data augmentation in the training of deep learning models, as well as a
number of functions are made available to make MRI data cleaning easier, and a
three ready to use and clean dataset on the major anatomical plans.",2024-04-11 05:06:51+00:00,"['Drici Mourad', 'Kazeem Oluwakemi Oseni']",http://arxiv.org/abs/2404.08703v1
Motion Modes: What Could Happen Next?,"Predicting diverse object motions from a single static image remains
challenging, as current video generation models often entangle object movement
with camera motion and other scene changes. While recent methods can predict
specific motions from motion arrow input, they rely on synthetic data and
predefined motions, limiting their application to complex scenes. We introduce
Motion Modes, a training-free approach that explores a pre-trained
image-to-video generator's latent distribution to discover various distinct and
plausible motions focused on selected objects in static images. We achieve this
by employing a flow generator guided by energy functions designed to
disentangle object and camera motion. Additionally, we use an energy inspired
by particle guidance to diversify the generated motions, without requiring
explicit training data. Experimental results demonstrate that Motion Modes
generates realistic and varied object animations, surpassing previous methods
and even human predictions regarding plausibility and diversity. Project
Webpage: https://motionmodes.github.io/",2024-11-29 01:51:08+00:00,"['Karran Pandey', 'Matheus Gadelha', 'Yannick Hold-Geoffroy', 'Karan Singh', 'Niloy J. Mitra', 'Paul Guerrero']",http://arxiv.org/abs/2412.00148v1
Electrooptical Image Synthesis from SAR Imagery Using Generative Adversarial Networks,"The utility of Synthetic Aperture Radar (SAR) imagery in remote sensing and
satellite image analysis is well established, offering robustness under various
weather and lighting conditions. However, SAR images, characterized by their
unique structural and texture characteristics, often pose interpretability
challenges for analysts accustomed to electrooptical (EO) imagery. This
application compares state-of-the-art Generative Adversarial Networks (GANs)
including Pix2Pix, CycleGan, S-CycleGan, and a novel dual?generator GAN
utilizing partial convolutions and a novel dual-generator architecture
utilizing transformers. These models are designed to progressively refine the
realism in the translated optical images, thereby enhancing the visual
interpretability of SAR data. We demonstrate the efficacy of our approach
through qualitative and quantitative evaluations, comparing the synthesized EO
images with actual EO images in terms of visual fidelity and feature
preservation. The results show significant improvements in interpretability,
making SAR data more accessible for analysts familiar with EO imagery.
Furthermore, we explore the potential of this technology in various
applications, including environmental monitoring, urban planning, and military
reconnaissance, where rapid, accurate interpretation of SAR data is crucial.
Our research contributes to the field of remote sensing by bridging the gap
between SAR and EO imagery, offering a novel tool for enhanced data
interpretation and broader application of SAR technology in various domains.",2024-09-07 14:31:46+00:00,"['Grant Rosario', 'David Noever']",http://arxiv.org/abs/2409.15331v1
Self-Supervised Learning of Deviation in Latent Representation for Co-speech Gesture Video Generation,"Gestures are pivotal in enhancing co-speech communication. While recent works
have mostly focused on point-level motion transformation or fully supervised
motion representations through data-driven approaches, we explore the
representation of gestures in co-speech, with a focus on self-supervised
representation and pixel-level motion deviation, utilizing a diffusion model
which incorporates latent motion features. Our approach leverages
self-supervised deviation in latent representation to facilitate hand gestures
generation, which are crucial for generating realistic gesture videos. Results
of our first experiment demonstrate that our method enhances the quality of
generated videos, with an improvement from 2.7 to 4.5% for FGD, DIV, and FVD,
and 8.1% for PSNR, 2.5% for SSIM over the current state-of-the-art methods.",2024-09-26 09:33:20+00:00,"['Huan Yang', 'Jiahui Chen', 'Chaofan Ding', 'Runhua Shi', 'Siyu Xiong', 'Qingqi Hong', 'Xiaoqi Mo', 'Xinhan Di']",http://arxiv.org/abs/2409.17674v1
AtomoVideo: High Fidelity Image-to-Video Generation,"Recently, video generation has achieved significant rapid development based
on superior text-to-image generation techniques. In this work, we propose a
high fidelity framework for image-to-video generation, named AtomoVideo. Based
on multi-granularity image injection, we achieve higher fidelity of the
generated video to the given image. In addition, thanks to high quality
datasets and training strategies, we achieve greater motion intensity while
maintaining superior temporal consistency and stability. Our architecture
extends flexibly to the video frame prediction task, enabling long sequence
prediction through iterative generation. Furthermore, due to the design of
adapter training, our approach can be well combined with existing personalized
models and controllable modules. By quantitatively and qualitatively
evaluation, AtomoVideo achieves superior results compared to popular methods,
more examples can be found on our project website:
https://atomo-video.github.io/.",2024-03-04 07:41:50+00:00,"['Litong Gong', 'Yiran Zhu', 'Weijie Li', 'Xiaoyang Kang', 'Biao Wang', 'Tiezheng Ge', 'Bo Zheng']",http://arxiv.org/abs/2403.01800v2
Semantically consistent Video-to-Audio Generation using Multimodal Language Large Model,"Existing works have made strides in video generation, but the lack of sound
effects (SFX) and background music (BGM) hinders a complete and immersive
viewer experience. We introduce a novel semantically consistent v ideo-to-audio
generation framework, namely SVA, which automatically generates audio
semantically consistent with the given video content. The framework harnesses
the power of multimodal large language model (MLLM) to understand video
semantics from a key frame and generate creative audio schemes, which are then
utilized as prompts for text-to-audio models, resulting in video-to-audio
generation with natural language as an interface. We show the satisfactory
performance of SVA through case study and discuss the limitations along with
the future research direction. The project page is available at
https://huiz-a.github.io/audio4video.github.io/.",2024-04-25 03:14:49+00:00,"['Gehui Chen', ""Guan'an Wang"", 'Xiaowen Huang', 'Jitao Sang']",http://arxiv.org/abs/2404.16305v2
Elevating Flow-Guided Video Inpainting with Reference Generation,"Video inpainting (VI) is a challenging task that requires effective
propagation of observable content across frames while simultaneously generating
new content not present in the original video. In this study, we propose a
robust and practical VI framework that leverages a large generative model for
reference generation in combination with an advanced pixel propagation
algorithm. Powered by a strong generative model, our method not only
significantly enhances frame-level quality for object removal but also
synthesizes new content in the missing areas based on user-provided text
prompts. For pixel propagation, we introduce a one-shot pixel pulling method
that effectively avoids error accumulation from repeated sampling while
maintaining sub-pixel precision. To evaluate various VI methods in realistic
scenarios, we also propose a high-quality VI benchmark, HQVI, comprising
carefully generated videos using alpha matte composition. On public benchmarks
and the HQVI dataset, our method demonstrates significantly higher visual
quality and metric scores compared to existing solutions. Furthermore, it can
process high-resolution videos exceeding 2K resolution with ease, underscoring
its superiority for real-world applications.",2024-12-12 06:13:00+00:00,"['Suhwan Cho', 'Seoung Wug Oh', 'Sangyoun Lee', 'Joon-Young Lee']",http://arxiv.org/abs/2412.08975v1
RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction,"Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach
for high-fidelity image synthesis, operating diffusion processes on continuous
VAE latent, which significantly differ from the text generation methods
employed by Large Language Models (LLMs). In this paper, we introduce a novel
generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which
enhances the diffusion process through a recurrent token prediction mechanism,
thereby pioneering the field of Discrete Diffusion. By progressively
introducing Gaussian noise into the latent representations of images and
encoding them into vector-quantized tokens in a recurrent manner, RDPM
facilitates a unique diffusion process on discrete-value domains. This process
iteratively predicts the token codes for subsequent timesteps, transforming the
initial standard Gaussian noise into the source data distribution, aligning
with GPT-style models in terms of the loss function. RDPM demonstrates superior
performance while benefiting from the speed advantage of requiring only a few
inference steps. This model not only leverages the diffusion process to ensure
high-quality generation but also converts continuous signals into a series of
high-fidelity discrete tokens, thereby maintaining a unified optimization
strategy with other discrete tokens, such as text. We anticipate that this work
will contribute to the development of a unified model for multimodal
generation, specifically by integrating continuous signal domains such as
images, videos, and audio with text. We will release the code and model weights
to the open-source community.",2024-12-24 12:28:19+00:00,"['Xiaoping Wu', 'Jie Hu', 'Xiaoming Wei']",http://arxiv.org/abs/2412.18390v2
"Controlling Rate, Distortion, and Realism: Towards a Single Comprehensive Neural Image Compression Model","In recent years, neural network-driven image compression (NIC) has gained
significant attention. Some works adopt deep generative models such as GANs and
diffusion models to enhance perceptual quality (realism). A critical obstacle
of these generative NIC methods is that each model is optimized for a single
bit rate. Consequently, multiple models are required to compress images to
different bit rates, which is impractical for real-world applications. To
tackle this issue, we propose a variable-rate generative NIC model.
Specifically, we explore several discriminator designs tailored for the
variable-rate approach and introduce a novel adversarial loss. Moreover, by
incorporating the newly proposed multi-realism technique, our method allows the
users to adjust the bit rate, distortion, and realism with a single model,
achieving ultra-controllability. Unlike existing variable-rate generative NIC
models, our method matches or surpasses the performance of state-of-the-art
single-rate generative NIC models while covering a wide range of bit rates
using just one model. Code will be available at https://github.com/iwa-shi/CRDR",2024-05-27 04:22:25+00:00,"['Shoma Iwai', 'Tomo Miyazaki', 'Shinichiro Omachi']",http://arxiv.org/abs/2405.16817v1
VASE: Object-Centric Appearance and Shape Manipulation of Real Videos,"Recently, several works tackled the video editing task fostered by the
success of large-scale text-to-image generative models. However, most of these
methods holistically edit the frame using the text, exploiting the prior given
by foundation diffusion models and focusing on improving the temporal
consistency across frames. In this work, we introduce a framework that is
object-centric and is designed to control both the object's appearance and,
notably, to execute precise and explicit structural modifications on the
object. We build our framework on a pre-trained image-conditioned diffusion
model, integrate layers to handle the temporal dimension, and propose training
strategies and architectural modifications to enable shape control. We evaluate
our method on the image-driven video editing task showing similar performance
to the state-of-the-art, and showcasing novel shape-editing capabilities.
Further details, code and examples are available on our project page:
https://helia95.github.io/vase-website/",2024-01-04 18:59:24+00:00,"['Elia Peruzzo', 'Vidit Goel', 'Dejia Xu', 'Xingqian Xu', 'Yifan Jiang', 'Zhangyang Wang', 'Humphrey Shi', 'Nicu Sebe']",http://arxiv.org/abs/2401.02473v1
StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN,"We propose a method that can generate cinemagraphs automatically from a still
landscape image using a pre-trained StyleGAN. Inspired by the success of recent
unconditional video generation, we leverage a powerful pre-trained image
generator to synthesize high-quality cinemagraphs. Unlike previous approaches
that mainly utilize the latent space of a pre-trained StyleGAN, our approach
utilizes its deep feature space for both GAN inversion and cinemagraph
generation. Specifically, we propose multi-scale deep feature warping (MSDFW),
which warps the intermediate features of a pre-trained StyleGAN at different
resolutions. By using MSDFW, the generated cinemagraphs are of high resolution
and exhibit plausible looping animation. We demonstrate the superiority of our
method through user studies and quantitative comparisons with state-of-the-art
cinemagraph generation methods and a video generation method that uses a
pre-trained StyleGAN.",2024-03-21 07:21:51+00:00,"['Jongwoo Choi', 'Kwanggyoon Seo', 'Amirsaman Ashtari', 'Junyong Noh']",http://arxiv.org/abs/2403.14186v1
LidarDM: Generative LiDAR Simulation in a Generated World,"We present LidarDM, a novel LiDAR generative model capable of producing
realistic, layout-aware, physically plausible, and temporally coherent LiDAR
videos. LidarDM stands out with two unprecedented capabilities in LiDAR
generative modeling: (i) LiDAR generation guided by driving scenarios, offering
significant potential for autonomous driving simulations, and (ii) 4D LiDAR
point cloud generation, enabling the creation of realistic and temporally
coherent sequences. At the heart of our model is a novel integrated 4D world
generation framework. Specifically, we employ latent diffusion models to
generate the 3D scene, combine it with dynamic actors to form the underlying 4D
world, and subsequently produce realistic sensory observations within this
virtual environment. Our experiments indicate that our approach outperforms
competing algorithms in realism, temporal coherency, and layout consistency. We
additionally show that LidarDM can be used as a generative world model
simulator for training and testing perception models.",2024-04-03 17:59:28+00:00,"['Vlas Zyrianov', 'Henry Che', 'Zhijian Liu', 'Shenlong Wang']",http://arxiv.org/abs/2404.02903v1
Macro2Micro: Cross-modal Magnetic Resonance Imaging Synthesis Leveraging Multi-scale Brain Structures,"Spanning multiple scales-from macroscopic anatomy down to intricate
microscopic architecture-the human brain exemplifies a complex system that
demands integrated approaches to fully understand its complexity. Yet, mapping
nonlinear relationships between these scales remains challenging due to
technical limitations and the high cost of multimodal Magnetic Resonance
Imaging (MRI) acquisition. Here, we introduce Macro2Micro, a deep learning
framework that predicts brain microstructure from macrostructure using a
Generative Adversarial Network (GAN). Grounded in the scale-free, self-similar
nature of brain organization-where microscale information can be inferred from
macroscale patterns-Macro2Micro explicitly encodes multiscale brain
representations into distinct processing branches. To further enhance image
fidelity and suppress artifacts, we propose a simple yet effective auxiliary
discriminator and learning objective. Our results show that Macro2Micro
faithfully translates T1-weighted MRIs into corresponding Fractional Anisotropy
(FA) images, achieving a 6.8% improvement in the Structural Similarity Index
Measure (SSIM) compared to previous methods, while preserving the individual
neurobiological characteristics.",2024-12-15 18:49:20+00:00,"['Sooyoung Kim', 'Joonwoo Kwon', 'Junbeom Kwon', 'Sangyoon Bae', 'Yuewei Lin', 'Shinjae Yoo', 'Jiook Cha']",http://arxiv.org/abs/2412.11277v1
Learning Long-form Video Prior via Generative Pre-Training,"Concepts involved in long-form videos such as people, objects, and their
interactions, can be viewed as following an implicit prior. They are notably
complex and continue to pose challenges to be comprehensively learned. In
recent years, generative pre-training (GPT) has exhibited versatile capacities
in modeling any kind of text content even visual locations. Can this manner
work for learning long-form video prior? Instead of operating on pixel space,
it is efficient to employ visual locations like bounding boxes and keypoints to
represent key information in videos, which can be simply discretized and then
tokenized for consumption by GPT. Due to the scarcity of suitable data, we
create a new dataset called \textbf{Storyboard20K} from movies to serve as a
representative. It includes synopses, shot-by-shot keyframes, and fine-grained
annotations of film sets and characters with consistent IDs, bounding boxes,
and whole body keypoints. In this way, long-form videos can be represented by a
set of tokens and be learned via generative pre-training. Experimental results
validate that our approach has great potential for learning long-form video
prior. Code and data will be released at
\url{https://github.com/showlab/Long-form-Video-Prior}.",2024-04-24 15:03:53+00:00,"['Jinheng Xie', 'Jiajun Feng', 'Zhaoxu Tian', 'Kevin Qinghong Lin', 'Yawen Huang', 'Xi Xia', 'Nanxu Gong', 'Xu Zuo', 'Jiaqi Yang', 'Yefeng Zheng', 'Mike Zheng Shou']",http://arxiv.org/abs/2404.15909v1
BVI-UGC: A Video Quality Database for User-Generated Content Transcoding,"In recent years, user-generated content (UGC) has become one of the major
video types consumed via streaming networks. Numerous research contributions
have focused on assessing its visual quality through subjective tests and
objective modeling. In most cases, objective assessments are based on a
no-reference scenario, where the corresponding reference content is assumed not
to be available. However, full-reference video quality assessment is also
important for UGC in the delivery pipeline, particularly associated with the
video transcoding process. In this context, we present a new UGC video quality
database, BVI-UGC, for user-generated content transcoding, which contains 60
(non-pristine) reference videos and 1,080 test sequences. In this work, we
simulated the creation of non-pristine reference sequences (with a wide range
of compression distortions), typical of content uploaded to UGC platforms for
transcoding. A comprehensive crowdsourced subjective study was then conducted
involving more than 3,500 human participants. Based on this collected
subjective data, we benchmarked the performance of 10 full-reference and 11
no-reference quality metrics. Our results demonstrate the poor performance
(SROCC values are lower than 0.6) of these metrics in predicting the perceptual
quality of UGC in two different scenarios (with or without a reference).",2024-08-13 19:30:12+00:00,"['Zihao Qi', 'Chen Feng', 'Fan Zhang', 'Xiaozhong Xu', 'Shan Liu', 'David Bull']",http://arxiv.org/abs/2408.07171v1
Towards a Universal Synthetic Video Detector: From Face or Background Manipulations to Fully AI-Generated Content,"Existing DeepFake detection techniques primarily focus on facial
manipulations, such as face-swapping or lip-syncing. However, advancements in
text-to-video (T2V) and image-to-video (I2V) generative models now allow fully
AI-generated synthetic content and seamless background alterations, challenging
face-centric detection methods and demanding more versatile approaches.
  To address this, we introduce the \underline{U}niversal \underline{N}etwork
for \underline{I}dentifying \underline{T}ampered and synth\underline{E}tic
videos (\texttt{UNITE}) model, which, unlike traditional detectors, captures
full-frame manipulations. \texttt{UNITE} extends detection capabilities to
scenarios without faces, non-human subjects, and complex background
modifications. It leverages a transformer-based architecture that processes
domain-agnostic features extracted from videos via the SigLIP-So400M foundation
model. Given limited datasets encompassing both facial/background alterations
and T2V/I2V content, we integrate task-irrelevant data alongside standard
DeepFake datasets in training. We further mitigate the model's tendency to
over-focus on faces by incorporating an attention-diversity (AD) loss, which
promotes diverse spatial attention across video frames. Combining AD loss with
cross-entropy improves detection performance across varied contexts.
Comparative evaluations demonstrate that \texttt{UNITE} outperforms
state-of-the-art detectors on datasets (in cross-data settings) featuring
face/background manipulations and fully synthetic T2V/I2V videos, showcasing
its adaptability and generalizable detection capabilities.",2024-12-16 19:00:19+00:00,"['Rohit Kundu', 'Hao Xiong', 'Vishal Mohanty', 'Athula Balachandran', 'Amit K. Roy-Chowdhury']",http://arxiv.org/abs/2412.12278v1
"M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset","Publishing open-source academic video recordings is an emergent and prevalent
approach to sharing knowledge online. Such videos carry rich multimodal
information including speech, the facial and body movements of the speakers, as
well as the texts and pictures in the slides and possibly even the papers.
Although multiple academic video datasets have been constructed and released,
few of them support both multimodal content recognition and understanding
tasks, which is partially due to the lack of high-quality human annotations. In
this paper, we propose a novel multimodal, multigenre, and multipurpose
audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of
videos from five sources covering computer science, mathematics, and medical
and biology topics. With high-quality human annotations of the slide text and
spoken words, in particular high-valued name entities, the dataset can be used
for multiple audio-visual recognition and understanding tasks. Evaluations
performed on contextual speech recognition, speech synthesis, and slide and
script generation tasks demonstrate that the diversity of M$^3$AV makes it a
challenging dataset.",2024-03-21 06:43:59+00:00,"['Zhe Chen', 'Heyang Liu', 'Wenyi Yu', 'Guangzhi Sun', 'Hongcheng Liu', 'Ji Wu', 'Chao Zhang', 'Yu Wang', 'Yanfeng Wang']",http://arxiv.org/abs/2403.14168v3
DINTR: Tracking via Diffusion-based Interpolation,"Object tracking is a fundamental task in computer vision, requiring the
localization of objects of interest across video frames. Diffusion models have
shown remarkable capabilities in visual generation, making them well-suited for
addressing several requirements of the tracking problem. This work proposes a
novel diffusion-based methodology to formulate the tracking task. Firstly,
their conditional process allows for injecting indications of the target object
into the generation process. Secondly, diffusion mechanics can be developed to
inherently model temporal correspondences, enabling the reconstruction of
actual frames in video. However, existing diffusion models rely on extensive
and unnecessary mapping to a Gaussian noise domain, which can be replaced by a
more efficient and stable interpolation process. Our proposed interpolation
mechanism draws inspiration from classic image-processing techniques, offering
a more interpretable, stable, and faster approach tailored specifically for the
object tracking task. By leveraging the strengths of diffusion models while
circumventing their limitations, our Diffusion-based INterpolation TrackeR
(DINTR) presents a promising new paradigm and achieves a superior multiplicity
on seven benchmarks across five indicator representations.",2024-10-14 00:41:58+00:00,"['Pha Nguyen', 'Ngan Le', 'Jackson Cothren', 'Alper Yilmaz', 'Khoa Luu']",http://arxiv.org/abs/2410.10053v1
AI-Generated Video Detection via Spatio-Temporal Anomaly Learning,"The advancement of generation models has led to the emergence of highly
realistic artificial intelligence (AI)-generated videos. Malicious users can
easily create non-existent videos to spread false information. This letter
proposes an effective AI-generated video detection (AIGVDet) scheme by
capturing the forensic traces with a two-branch spatio-temporal convolutional
neural network (CNN). Specifically, two ResNet sub-detectors are learned
separately for identifying the anomalies in spatical and optical flow domains,
respectively. Results of such sub-detectors are fused to further enhance the
discrimination ability. A large-scale generated video dataset (GVD) is
constructed as a benchmark for model training and evaluation. Extensive
experimental results verify the high generalization and robustness of our
AIGVDet scheme. Code and dataset will be available at
https://github.com/multimediaFor/AIGVDet.",2024-03-25 11:26:18+00:00,"['Jianfa Bai', 'Man Lin', 'Gang Cao']",http://arxiv.org/abs/2403.16638v1
3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement,"Despite advances in neural rendering, due to the scarcity of high-quality 3D
datasets and the inherent limitations of multi-view diffusion models, view
synthesis and 3D model generation are restricted to low resolutions with
suboptimal multi-view consistency. In this study, we present a novel 3D
enhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent
diffusion model to enhance coarse 3D inputs while preserving multi-view
consistency. Our method includes a pose-aware encoder and a diffusion-based
denoiser to refine low-quality multi-view images, along with data augmentation
and a multi-view attention module with epipolar aggregation to maintain
consistent, high-quality 3D outputs across views. Unlike existing video-based
approaches, our model supports seamless multi-view enhancement with improved
coherence across diverse viewing angles. Extensive evaluations show that
3DEnhancer significantly outperforms existing methods, boosting both multi-view
enhancement and per-instance 3D optimization tasks.",2024-12-24 17:36:34+00:00,"['Yihang Luo', 'Shangchen Zhou', 'Yushi Lan', 'Xingang Pan', 'Chen Change Loy']",http://arxiv.org/abs/2412.18565v1
Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN,"In the 6G era, real-time radio resource monitoring and management are urged
to support diverse wireless-empowered applications. This calls for fast and
accurate estimation on the distribution of the radio resources, which is
usually represented by the spatial signal power strength over the geographical
environment, known as a radio map. In this paper, we present a cooperative
radio map estimation (CRME) approach enabled by the generative adversarial
network (GAN), called as GAN-CRME, which features fast and accurate radio map
estimation without the transmitters' information. The radio map is inferred by
exploiting the interaction between distributed received signal strength (RSS)
measurements at mobile users and the geographical map using a deep neural
network estimator, resulting in low data-acquisition cost and computational
complexity. Moreover, a GAN-based learning algorithm is proposed to boost the
inference capability of the deep neural network estimator by exploiting the
power of generative AI. Simulation results showcase that the proposed GAN-CRME
is even capable of coarse error-correction when the geographical map
information is inaccurate.",2024-02-05 05:01:28+00:00,"['Zezhong Zhang', 'Guangxu Zhu', 'Junting Chen', 'Shuguang Cui']",http://arxiv.org/abs/2402.02729v1
JoyVASA: Portrait and Animal Image Animation with Diffusion-Based Audio-Driven Facial Dynamics and Head Motion Generation,"Audio-driven portrait animation has made significant advances with
diffusion-based models, improving video quality and lipsync accuracy. However,
the increasing complexity of these models has led to inefficiencies in training
and inference, as well as constraints on video length and inter-frame
continuity. In this paper, we propose JoyVASA, a diffusion-based method for
generating facial dynamics and head motion in audio-driven facial animation.
Specifically, in the first stage, we introduce a decoupled facial
representation framework that separates dynamic facial expressions from static
3D facial representations. This decoupling allows the system to generate longer
videos by combining any static 3D facial representation with dynamic motion
sequences. Then, in the second stage, a diffusion transformer is trained to
generate motion sequences directly from audio cues, independent of character
identity. Finally, a generator trained in the first stage uses the 3D facial
representation and the generated motion sequences as inputs to render
high-quality animations. With the decoupled facial representation and the
identity-independent motion generation process, JoyVASA extends beyond human
portraits to animate animal faces seamlessly. The model is trained on a hybrid
dataset of private Chinese and public English data, enabling multilingual
support. Experimental results validate the effectiveness of our approach.
Future work will focus on improving real-time performance and refining
expression control, further expanding the applications in portrait animation.
The code is available at: https://github.com/jdh-algo/JoyVASA.",2024-11-14 06:13:05+00:00,"['Xuyang Cao', 'Guoxin Wang', 'Sheng Shi', 'Jun Zhao', 'Yang Yao', 'Jintao Fei', 'Minyu Gao']",http://arxiv.org/abs/2411.09209v4
Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation,"The field of portrait image animation, driven by speech audio input, has
experienced significant advancements in the generation of realistic and dynamic
portraits. This research delves into the complexities of synchronizing facial
movements and creating visually appealing, temporally consistent animations
within the framework of diffusion-based methodologies. Moving away from
traditional paradigms that rely on parametric models for intermediate facial
representations, our innovative approach embraces the end-to-end diffusion
paradigm and introduces a hierarchical audio-driven visual synthesis module to
enhance the precision of alignment between audio inputs and visual outputs,
encompassing lip, expression, and pose motion. Our proposed network
architecture seamlessly integrates diffusion-based generative models, a
UNet-based denoiser, temporal alignment techniques, and a reference network.
The proposed hierarchical audio-driven visual synthesis offers adaptive control
over expression and pose diversity, enabling more effective personalization
tailored to different identities. Through a comprehensive evaluation that
incorporates both qualitative and quantitative analyses, our approach
demonstrates obvious enhancements in image and video quality, lip
synchronization precision, and motion diversity. Further visualization and
access to the source code can be found at:
https://fudan-generative-vision.github.io/hallo.",2024-06-13 04:33:20+00:00,"['Mingwang Xu', 'Hui Li', 'Qingkun Su', 'Hanlin Shang', 'Liwei Zhang', 'Ce Liu', 'Jingdong Wang', 'Yao Yao', 'Siyu Zhu']",http://arxiv.org/abs/2406.08801v2
Gotta Hear Them All: Sound Source Aware Vision to Audio Generation,"Vision-to-audio (V2A) synthesis has broad applications in multimedia. Recent
advancements of V2A methods have made it possible to generate relevant audios
from inputs of videos or still images. However, the immersiveness and
expressiveness of the generation are limited. One possible problem is that
existing methods solely rely on the global scene and overlook details of local
sounding objects (i.e., sound sources). To address this issue, we propose a
Sound Source-Aware V2A (SSV2A) generator. SSV2A is able to locally perceive
multimodal sound sources from a scene with visual detection and cross-modality
translation. It then contrastively learns a Cross-Modal Sound Source (CMSS)
Manifold to semantically disambiguate each source. Finally, we attentively mix
their CMSS semantics into a rich audio representation, from which a pretrained
audio generator outputs the sound. To model the CMSS manifold, we curate a
novel single-sound-source visual-audio dataset VGGS3 from VGGSound. We also
design a Sound Source Matching Score to measure localized audio relevance. By
addressing V2A generation at the sound-source level, SSV2A surpasses
state-of-the-art methods in both generation fidelity and relevance as evidenced
by extensive experiments. We further demonstrate SSV2A's ability to achieve
intuitive V2A control by compositing vision, text, and audio conditions. Our
generation can be tried and heard at https://ssv2a.github.io/SSV2A-demo .",2024-11-23 04:27:19+00:00,"['Wei Guo', 'Heng Wang', 'Jianbo Ma', 'Weidong Cai']",http://arxiv.org/abs/2411.15447v3
PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation,"Text-to-video (T2V) generation has been recently enabled by transformer-based
diffusion models, but current T2V models lack capabilities in adhering to the
real-world common knowledge and physical rules, due to their limited
understanding of physical realism and deficiency in temporal modeling. Existing
solutions are either data-driven or require extra model inputs, but cannot be
generalizable to out-of-distribution domains. In this paper, we present PhyT2V,
a new data-independent T2V technique that expands the current T2V model's
capability of video generation to out-of-distribution domains, by enabling
chain-of-thought and step-back reasoning in T2V prompting. Our experiments show
that PhyT2V improves existing T2V models' adherence to real-world physical
rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.
The source codes are available at: https://github.com/pittisl/PhyT2V.",2024-11-30 22:02:12+00:00,"['Qiyao Xue', 'Xiangyu Yin', 'Boyuan Yang', 'Wei Gao']",http://arxiv.org/abs/2412.00596v1
"Quark: Real-time, High-resolution, and General Neural View Synthesis","We present a novel neural algorithm for performing high-quality,
high-resolution, real-time novel view synthesis. From a sparse set of input RGB
images or videos streams, our network both reconstructs the 3D scene and
renders novel views at 1080p resolution at 30fps on an NVIDIA A100. Our
feed-forward network generalizes across a wide variety of datasets and scenes
and produces state-of-the-art quality for a real-time method. Our quality
approaches, and in some cases surpasses, the quality of some of the top offline
methods. In order to achieve these results we use a novel combination of
several key concepts, and tie them together into a cohesive and effective
algorithm. We build on previous works that represent the scene using
semi-transparent layers and use an iterative learned render-and-refine approach
to improve those layers. Instead of flat layers, our method reconstructs
layered depth maps (LDMs) that efficiently represent scenes with complex depth
and occlusions. The iterative update steps are embedded in a multi-scale,
UNet-style architecture to perform as much compute as possible at reduced
resolution. Within each update step, to better aggregate the information from
multiple input views, we use a specialized Transformer-based network component.
This allows the majority of the per-input image processing to be performed in
the input image space, as opposed to layer space, further increasing
efficiency. Finally, due to the real-time nature of our reconstruction and
rendering, we dynamically create and discard the internal 3D geometry for each
frame, generating the LDM for each view. Taken together, this produces a novel
and effective algorithm for view synthesis. Through extensive evaluation, we
demonstrate that we achieve state-of-the-art quality at real-time rates.
Project page: https://quark-3d.github.io/",2024-11-25 18:59:50+00:00,"['John Flynn', 'Michael Broxton', 'Lukas Murmann', 'Lucy Chai', 'Matthew DuVall', 'Clment Godard', 'Kathryn Heal', 'Srinivas Kaza', 'Stephen Lombardi', 'Xuan Luo', 'Supreeth Achar', 'Kira Prabhu', 'Tiancheng Sun', 'Lynn Tsai', 'Ryan Overbeck']",http://arxiv.org/abs/2411.16680v1
LightIt: Illumination Modeling and Control for Diffusion Models,"We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.",2024-03-15 18:26:33+00:00,"['Peter Kocsis', 'Julien Philip', 'Kalyan Sunkavalli', 'Matthias Niener', 'Yannick Hold-Geoffroy']",http://arxiv.org/abs/2403.10615v2
Shotit: compute-efficient image-to-video search engine for the cloud,"With the rapid growth of information technology, users are exposed to a
massive amount of data online, including image, music, and video. This has led
to strong needs to provide effective corresponsive search services such as
image, music, and video search services. Most of them are operated based on
keywords, namely using keywords to find related image, music, and video.
Additionally, there are image-to-image search services that enable users to
find similar images using one input image. Given that videos are essentially
composed of image frames, then similar videos can be searched by one input
image or screenshot. We want to target this scenario and provide an efficient
method and implementation in this paper.
  We present Shotit, a cloud-native image-to-video search engine that tailors
this search scenario in a compute-efficient approach. One main limitation faced
in this scenario is the scale of its dataset. A typical image-to-image search
engine only handles one-to-one relationships, colloquially, one image
corresponds to another single image. But image-to-video proliferates. Take a
24-min length video as an example, it will generate roughly 20,000 image
frames. As the number of videos grows, the scale of the dataset explodes
exponentially. In this case, a compute-efficient approach ought to be
considered, and the system design should cater to the cloud-native trend.
Choosing an emerging technology - vector database as its backbone, Shotit fits
these two metrics performantly. Experiments for two different datasets, a 50
thousand-scale Blender Open Movie dataset, and a 50 million-scale proprietary
TV genre dataset at a 4 Core 32GB RAM Intel Xeon Gold 6271C cloud machine with
object storage reveal the effectiveness of Shotit. A demo regarding the Blender
Open Movie dataset is illustrated within this paper.",2024-04-18 13:23:05+00:00,['Leslie Wong'],http://arxiv.org/abs/2404.12169v1
Human Action CLIPS: Detecting AI-generated Human Motion,"Full-blown AI-generated video generation continues its journey through the
uncanny valley to produce content that is perceptually indistinguishable from
reality. Intermixed with many exciting and creative applications are malicious
applications that harm individuals, organizations, and democracies. We describe
an effective and robust technique for distinguishing real from AI-generated
human motion. This technique leverages a multi-modal semantic embedding, making
it robust to the types of laundering that typically confound more low- to
mid-level approaches. This method is evaluated against a custom-built dataset
of video clips with human actions generated by seven text-to-video AI models
and matching real footage.",2024-11-30 16:20:58+00:00,"['Matyas Bohacek', 'Hany Farid']",http://arxiv.org/abs/2412.00526v1
Fight Scene Detection for Movie Highlight Generation System,"In this paper of a research based project, using Bidirectional Long
Short-Term Memory (BiLSTM) networks, we provide a novel Fight Scene Detection
(FSD) model which can be used for Movie Highlight Generation Systems (MHGS)
based on deep learning and Neural Networks . Movies usually have Fight Scenes
to keep the audience amazed. For trailer generation, or any other application
of Highlight generation, it is very tidious to first identify all such scenes
manually and then compile them to generate a highlight serving the purpose. Our
proposed FSD system utilises temporal characteristics of the movie scenes and
thus is capable to automatically identify fight scenes. Thereby helping in the
effective production of captivating movie highlights. We observe that the
proposed solution features 93.5% accuracy and is higher than 2D CNN with Hough
Forests which being 92% accurate and is significantly higher than 3D CNN which
features an accuracy of 65%.",2024-06-04 08:18:05+00:00,['Aryan Mathur'],http://arxiv.org/abs/2406.05152v1
Deepfake Generation and Detection: A Benchmark and Survey,"Deepfake is a technology dedicated to creating highly realistic facial images
and videos under specific conditions, which has significant application
potential in fields such as entertainment, movie production, digital human
creation, to name a few. With the advancements in deep learning, techniques
primarily represented by Variational Autoencoders and Generative Adversarial
Networks have achieved impressive generation results. More recently, the
emergence of diffusion models with powerful generation capabilities has sparked
a renewed wave of research. In addition to deepfake generation, corresponding
detection technologies continuously evolve to regulate the potential misuse of
deepfakes, such as for privacy invasion and phishing attacks. This survey
comprehensively reviews the latest developments in deepfake generation and
detection, summarizing and analyzing current state-of-the-arts in this rapidly
evolving field. We first unify task definitions, comprehensively introduce
datasets and metrics, and discuss developing technologies. Then, we discuss the
development of several related sub-fields and focus on researching four
representative deepfake fields: face swapping, face reenactment, talking face
generation, and facial attribute editing, as well as forgery detection.
Subsequently, we comprehensively benchmark representative methods on popular
datasets for each field, fully evaluating the latest and influential published
works. Finally, we analyze challenges and future research directions of the
discussed fields.",2024-03-26 17:12:34+00:00,"['Gan Pei', 'Jiangning Zhang', 'Menghan Hu', 'Zhenyu Zhang', 'Chengjie Wang', 'Yunsheng Wu', 'Guangtao Zhai', 'Jian Yang', 'Chunhua Shen', 'Dacheng Tao']",http://arxiv.org/abs/2403.17881v4
Instance Brownian Bridge as Texts for Open-vocabulary Video Instance Segmentation,"Temporally locating objects with arbitrary class texts is the primary pursuit
of open-vocabulary Video Instance Segmentation (VIS). Because of the
insufficient vocabulary of video data, previous methods leverage image-text
pretraining model for recognizing object instances by separately aligning each
frame and class texts, ignoring the correlation between frames. As a result,
the separation breaks the instance movement context of videos, causing inferior
alignment between video and text. To tackle this issue, we propose to link
frame-level instance representations as a Brownian Bridge to model instance
dynamics and align bridge-level instance representation to class texts for more
precisely open-vocabulary VIS (BriVIS). Specifically, we build our system upon
a frozen video segmentor to generate frame-level instance queries, and design
Temporal Instance Resampler (TIR) to generate queries with temporal context
from frame queries. To mold instance queries to follow Brownian bridge and
accomplish alignment with class texts, we design Bridge-Text Alignment (BTA) to
learn discriminative bridge-level representations of instances via contrastive
objectives. Setting MinVIS as the basic video segmentor, BriVIS surpasses the
Open-vocabulary SOTA (OV2Seg) by a clear margin. For example, on the
challenging large-vocabulary VIS dataset (BURST), BriVIS achieves 7.43 mAP and
exhibits 49.49% improvement compared to OV2Seg (4.97 mAP).",2024-01-18 05:20:07+00:00,"['Zesen Cheng', 'Kehan Li', 'Hao Li', 'Peng Jin', 'Chang Liu', 'Xiawu Zheng', 'Rongrong Ji', 'Jie Chen']",http://arxiv.org/abs/2401.09732v1
SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding,"Temporal video grounding (TVG) is a critical task in video content
understanding, requiring precise alignment between video content and natural
language instructions. Despite significant advancements, existing methods face
challenges in managing confidence bias towards salient objects and capturing
long-term dependencies in video sequences. To address these issues, we
introduce SpikeMba: a multi-modal spiking saliency mamba for temporal video
grounding. Our approach integrates Spiking Neural Networks (SNNs) with state
space models (SSMs) to leverage their unique advantages in handling different
aspects of the task. Specifically, we use SNNs to develop a spiking saliency
detector that generates the proposal set. The detector emits spike signals when
the input signal exceeds a predefined threshold, resulting in a dynamic and
binary saliency proposal set. To enhance the model's capability to retain and
infer contextual information, we introduce relevant slots which learnable
tensors that encode prior knowledge. These slots work with the contextual
moment reasoner to maintain a balance between preserving contextual information
and exploring semantic relevance dynamically. The SSMs facilitate selective
information propagation, addressing the challenge of long-term dependency in
video content. By combining SNNs for proposal generation and SSMs for effective
contextual reasoning, SpikeMba addresses confidence bias and long-term
dependencies, thereby significantly enhancing fine-grained multimodal
relationship capture. Our experiments demonstrate the effectiveness of
SpikeMba, which consistently outperforms state-of-the-art methods across
mainstream benchmarks.",2024-04-01 15:26:44+00:00,"['Wenrui Li', 'Xiaopeng Hong', 'Ruiqin Xiong', 'Xiaopeng Fan']",http://arxiv.org/abs/2404.01174v2
Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM,"Towards open-ended Video Anomaly Detection (VAD), existing methods often
exhibit biased detection when faced with challenging or unseen events and lack
interpretability. To address these drawbacks, we propose Holmes-VAD, a novel
framework that leverages precise temporal supervision and rich multimodal
instructions to enable accurate anomaly localization and comprehensive
explanations. Firstly, towards unbiased and explainable VAD system, we
construct the first large-scale multimodal VAD instruction-tuning benchmark,
i.e., VAD-Instruct50k. This dataset is created using a carefully designed
semi-automatic labeling paradigm. Efficient single-frame annotations are
applied to the collected untrimmed videos, which are then synthesized into
high-quality analyses of both abnormal and normal video clips using a robust
off-the-shelf video captioner and a large language model (LLM). Building upon
the VAD-Instruct50k dataset, we develop a customized solution for interpretable
video anomaly detection. We train a lightweight temporal sampler to select
frames with high anomaly response and fine-tune a multimodal large language
model (LLM) to generate explanatory content. Extensive experimental results
validate the generality and interpretability of the proposed Holmes-VAD,
establishing it as a novel interpretable technique for real-world video anomaly
analysis. To support the community, our benchmark and model will be publicly
available at https://holmesvad.github.io.",2024-06-18 03:19:24+00:00,"['Huaxin Zhang', 'Xiaohao Xu', 'Xiang Wang', 'Jialong Zuo', 'Chuchu Han', 'Xiaonan Huang', 'Changxin Gao', 'Yuehuan Wang', 'Nong Sang']",http://arxiv.org/abs/2406.12235v2
MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation,"In the context of escalating safety concerns across various domains, the
tasks of Video Anomaly Detection (VAD) and Video Anomaly Recognition (VAR) have
emerged as critically important for applications in intelligent surveillance,
evidence investigation, violence alerting, etc. These tasks, aimed at
identifying and classifying deviations from normal behavior in video data, face
significant challenges due to the rarity of anomalies which leads to extremely
imbalanced data and the impracticality of extensive frame-level data annotation
for supervised learning. This paper introduces a novel hierarchical graph
neural network (GNN) based model MissionGNN that addresses these challenges by
leveraging a state-of-the-art large language model and a comprehensive
knowledge graph for efficient weakly supervised learning in VAR. Our approach
circumvents the limitations of previous methods by avoiding heavy gradient
computations on large multimodal models and enabling fully frame-level training
without fixed video segmentation. Utilizing automated, mission-specific
knowledge graph generation, our model provides a practical and efficient
solution for real-time video analysis without the constraints of previous
segmentation-based or multimodal approaches. Experimental validation on
benchmark datasets demonstrates our model's performance in VAD and VAR,
highlighting its potential to redefine the landscape of anomaly detection and
recognition in video surveillance systems.",2024-06-27 01:09:07+00:00,"['Sanggeon Yun', 'Ryozo Masukawa', 'Minhyoung Na', 'Mohsen Imani']",http://arxiv.org/abs/2406.18815v2
VideoEval: Comprehensive Benchmark Suite for Low-Cost Evaluation of Video Foundation Model,"With the growth of high-quality data and advancement in visual pre-training
paradigms, Video Foundation Models (VFMs) have made significant progress
recently, demonstrating their remarkable performance on traditional video
understanding benchmarks. However, the existing benchmarks (e.g. Kinetics) and
their evaluation protocols are often limited by relatively poor diversity, high
evaluation costs, and saturated performance metrics. In this paper, we build a
comprehensive benchmark suite to address these issues, namely VideoEval.
Specifically, we establish the Video Task Adaption Benchmark (VidTAB) and the
Video Embedding Benchmark (VidEB) from two perspectives: evaluating the task
adaptability of VFMs under few-shot conditions and assessing their
representation power by directly applying to downstream tasks. With VideoEval,
we conduct a large-scale study on 20 popular open-source vision foundation
models. Our study reveals some insightful findings on VFMs: 1) overall, current
VFMs exhibit weak generalization across diverse tasks, 2) increasing video
data, whether labeled or weakly-labeled video-text pairs, does not necessarily
improve task performance, 3) the effectiveness of some pre-training paradigms
may not be fully validated in previous benchmarks, and 4) combining different
pre-training paradigms can help improve the generalization capabilities. We
believe this study serves as an important complement to the current evaluation
for VFMs and offers valuable insights for the future research.",2024-07-09 01:49:08+00:00,"['Xinhao Li', 'Zhenpeng Huang', 'Jing Wang', 'Kunchang Li', 'Limin Wang']",http://arxiv.org/abs/2407.06491v1
Learning Natural Consistency Representation for Face Forgery Video Detection,"Face Forgery videos have elicited critical social public concerns and various
detectors have been proposed. However, fully-supervised detectors may lead to
easily overfitting to specific forgery methods or videos, and existing
self-supervised detectors are strict on auxiliary tasks, such as requiring
audio or multi-modalities, leading to limited generalization and robustness. In
this paper, we examine whether we can address this issue by leveraging
visual-only real face videos. To this end, we propose to learn the Natural
Consistency representation (NACO) of real face videos in a self-supervised
manner, which is inspired by the observation that fake videos struggle to
maintain the natural spatiotemporal consistency even under unknown forgery
methods and different perturbations. Our NACO first extracts spatial features
of each frame by CNNs then integrates them into Transformer to learn the
long-range spatiotemporal representation, leveraging the advantages of CNNs and
Transformer on local spatial receptive field and long-term memory respectively.
Furthermore, a Spatial Predictive Module~(SPM) and a Temporal Contrastive
Module~(TCM) are introduced to enhance the natural consistency representation
learning. The SPM aims to predict random masked spatial features from
spatiotemporal representation, and the TCM regularizes the latent distance of
spatiotemporal representation by shuffling the natural order to disturb the
consistency, which could both force our NACO more sensitive to the natural
spatiotemporal consistency. After the representation learning stage, a MLP head
is fine-tuned to perform the usual forgery video classification task. Extensive
experiments show that our method outperforms other state-of-the-art competitors
with impressive generalization and robustness.",2024-07-15 09:00:02+00:00,"['Daichi Zhang', 'Zihao Xiao', 'Shikun Li', 'Fanzhao Lin', 'Jianmin Li', 'Shiming Ge']",http://arxiv.org/abs/2407.10550v1
Query-Efficient Video Adversarial Attack with Stylized Logo,"Video classification systems based on Deep Neural Networks (DNNs) have
demonstrated excellent performance in accurately verifying video content.
However, recent studies have shown that DNNs are highly vulnerable to
adversarial examples. Therefore, a deep understanding of adversarial attacks
can better respond to emergency situations. In order to improve attack
performance, many style-transfer-based attacks and patch-based attacks have
been proposed. However, the global perturbation of the former will bring
unnatural global color, while the latter is difficult to achieve success in
targeted attacks due to the limited perturbation space. Moreover, compared to a
plethora of methods targeting image classifiers, video adversarial attacks are
still not that popular. Therefore, to generate adversarial examples with a low
budget and to provide them with a higher verisimilitude, we propose a novel
black-box video attack framework, called Stylized Logo Attack (SLA). SLA is
conducted through three steps. The first step involves building a style
references set for logos, which can not only make the generated examples more
natural, but also carry more target class features in the targeted attacks.
Then, reinforcement learning (RL) is employed to determine the style reference
and position parameters of the logo within the video, which ensures that the
stylized logo is placed in the video with optimal attributes. Finally,
perturbation optimization is designed to optimize perturbations to improve the
fooling rate in a step-by-step manner. Sufficient experimental results indicate
that, SLA can achieve better performance than state-of-the-art methods and
still maintain good deception effects when facing various defense methods.",2024-08-22 03:19:09+00:00,"['Duoxun Tang', 'Yuxin Cao', 'Xi Xiao', 'Derui Wang', 'Sheng Wen', 'Tianqing Zhu']",http://arxiv.org/abs/2408.12099v1
Personalized Video Summarization using Text-Based Queries and Conditional Modeling,"The proliferation of video content on platforms like YouTube and Vimeo
presents significant challenges in efficiently locating relevant information.
Automatic video summarization aims to address this by extracting and presenting
key content in a condensed form. This thesis explores enhancing video
summarization by integrating text-based queries and conditional modeling to
tailor summaries to user needs. Traditional methods often produce fixed
summaries that may not align with individual requirements. To overcome this, we
propose a multi-modal deep learning approach that incorporates both textual
queries and visual information, fusing them at different levels of the model
architecture. Evaluation metrics such as accuracy and F1-score assess the
quality of the generated summaries. The thesis also investigates improving
text-based query representations using contextualized word embeddings and
specialized attention networks. This enhances the semantic understanding of
queries, leading to better video summaries. To emulate human-like
summarization, which accounts for both visual coherence and abstract factors
like storyline consistency, we introduce a conditional modeling approach. This
method uses multiple random variables and joint distributions to capture key
summarization components, resulting in more human-like and explainable
summaries. Addressing data scarcity in fully supervised learning, the thesis
proposes a segment-level pseudo-labeling approach. This self-supervised method
generates additional data, improving model performance even with limited
human-labeled datasets. In summary, this research aims to enhance automatic
video summarization by incorporating text-based queries, improving query
representations, introducing conditional modeling, and addressing data
scarcity, thereby creating more effective and personalized video summaries.",2024-08-27 02:43:40+00:00,['Jia-Hong Huang'],http://arxiv.org/abs/2408.14743v1
Realizing Video Summarization from the Path of Language-based Semantic Understanding,"The recent development of Video-based Large Language Models (VideoLLMs), has
significantly advanced video summarization by aligning video features and, in
some cases, audio features with Large Language Models (LLMs). Each of these
VideoLLMs possesses unique strengths and weaknesses. Many recent methods have
required extensive fine-tuning to overcome the limitations of these models,
which can be resource-intensive. In this work, we observe that the strengths of
one VideoLLM can complement the weaknesses of another. Leveraging this insight,
we propose a novel video summarization framework inspired by the Mixture of
Experts (MoE) paradigm, which operates as an inference-time algorithm without
requiring any form of fine-tuning. Our approach integrates multiple VideoLLMs
to generate comprehensive and coherent textual summaries. It effectively
combines visual and audio content, provides detailed background descriptions,
and excels at identifying keyframes, which enables more semantically meaningful
retrieval compared to traditional computer vision approaches that rely solely
on visual information, all without the need for additional fine-tuning.
Moreover, the resulting summaries enhance performance in downstream tasks such
as summary video generation, either through keyframe selection or in
combination with text-to-image models. Our language-driven approach offers a
semantically rich alternative to conventional methods and provides flexibility
to incorporate newer VideoLLMs, enhancing adaptability and performance in video
summarization tasks.",2024-10-06 15:03:22+00:00,"['Kuan-Chen Mu', 'Zhi-Yi Chin', 'Wei-Chen Chiu']",http://arxiv.org/abs/2410.04511v1
VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding,"Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained
understanding, which hinders precise video moment localization when given
fine-grained queries. In this paper, we propose a more challenging fine-grained
VCMR benchmark requiring methods to localize the best-matched moment from the
corpus with other partially matched candidates. To improve the dataset
construction efficiency and guarantee high-quality data annotations, we propose
VERIFIED, an automatic \underline{V}id\underline{E}o-text annotation pipeline
to generate captions with \underline{R}el\underline{I}able
\underline{FI}n\underline{E}-grained statics and \underline{D}ynamics.
Specifically, we resort to large language models (LLM) and large multimodal
models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules
to generate diverse fine-grained captions for each video. To filter out the
inaccurate annotations caused by the LLM hallucination, we propose a
Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation
model with disturbed hard-negatives augmented contrastive and matching losses.
With VERIFIED, we construct a more challenging fine-grained VCMR benchmark
containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a
high level of annotation quality. We evaluate several state-of-the-art VCMR
models on the proposed dataset, revealing that there is still significant scope
for fine-grained video understanding in VCMR. Code and Datasets are in
\href{https://github.com/hlchen23/VERIFIED}{https://github.com/hlchen23/VERIFIED}.",2024-10-11 07:42:36+00:00,"['Houlun Chen', 'Xin Wang', 'Hong Chen', 'Zeyang Zhang', 'Wei Feng', 'Bin Huang', 'Jia Jia', 'Wenwu Zhu']",http://arxiv.org/abs/2410.08593v1
From Dashcam Videos to Driving Simulations: Stress Testing Automated Vehicles against Rare Events,"Testing Automated Driving Systems (ADS) in simulation with realistic driving
scenarios is important for verifying their performance. However, converting
real-world driving videos into simulation scenarios is a significant challenge
due to the complexity of interpreting high-dimensional video data and the
time-consuming nature of precise manual scenario reconstruction. In this work,
we propose a novel framework that automates the conversion of real-world car
crash videos into detailed simulation scenarios for ADS testing. Our approach
leverages prompt-engineered Video Language Models(VLM) to transform dashcam
footage into SCENIC scripts, which define the environment and driving behaviors
in the CARLA simulator, enabling the generation of realistic simulation
scenarios. Importantly, rather than solely aiming for one-to-one scenario
reconstruction, our framework focuses on capturing the essential driving
behaviors from the original video while offering flexibility in parameters such
as weather or road conditions to facilitate search-based testing. Additionally,
we introduce a similarity metric that helps iteratively refine the generated
scenario through feedback by comparing key features of driving behaviors
between the real and simulated videos. Our preliminary results demonstrate
substantial time efficiency, finishing the real-to-sim conversion in minutes
with full automation and no human intervention, while maintaining high fidelity
to the original driving events.",2024-11-25 01:01:54+00:00,"['Yan Miao', 'Georgios Fainekos', 'Bardh Hoxha', 'Hideki Okamoto', 'Danil Prokhorov', 'Sayan Mitra']",http://arxiv.org/abs/2411.16027v2
Overview of TREC 2024 Medical Video Question Answering (MedVidQA) Track,"One of the key goals of artificial intelligence (AI) is the development of a
multimodal system that facilitates communication with the visual world (image
and video) using a natural language query. Earlier works on medical question
answering primarily focused on textual and visual (image) modalities, which may
be inefficient in answering questions requiring demonstration. In recent years,
significant progress has been achieved due to the introduction of large-scale
language-vision datasets and the development of efficient deep neural
techniques that bridge the gap between language and visual understanding.
Improvements have been made in numerous vision-and-language tasks, such as
visual captioning visual question answering, and natural language video
localization. Most of the existing work on language vision focused on creating
datasets and developing solutions for open-domain applications. We believe
medical videos may provide the best possible answers to many first aid, medical
emergency, and medical education questions. With increasing interest in AI to
support clinical decision-making and improve patient engagement, there is a
need to explore such challenges and develop efficient algorithms for medical
language-video understanding and generation. Toward this, we introduced new
tasks to foster research toward designing systems that can understand medical
videos to provide visual answers to natural language questions, and are
equipped with multimodal capability to generate instruction steps from the
medical video. These tasks have the potential to support the development of
sophisticated downstream applications that can benefit the public and medical
professionals.",2024-12-15 05:18:01+00:00,"['Deepak Gupta', 'Dina Demner-Fushman']",http://arxiv.org/abs/2412.11056v1
"COVID-19 on YouTube: A Data-Driven Analysis of Sentiment, Toxicity, and Content Recommendations","This study presents a data-driven analysis of COVID-19 discourse on YouTube,
examining the sentiment, toxicity, and thematic patterns of video content
published between January 2023 and October 2024. The analysis involved applying
advanced natural language processing (NLP) techniques: sentiment analysis with
VADER, toxicity detection with Detoxify, and topic modeling using Latent
Dirichlet Allocation (LDA). The sentiment analysis revealed that 49.32% of
video descriptions were positive, 36.63% were neutral, and 14.05% were
negative, indicating a generally informative and supportive tone in
pandemic-related content. Toxicity analysis identified only 0.91% of content as
toxic, suggesting minimal exposure to toxic content. Topic modeling revealed
two main themes, with 66.74% of the videos covering general health information
and pandemic-related impacts and 33.26% focused on news and real-time updates,
highlighting the dual informational role of YouTube. A recommendation system
was also developed using TF-IDF vectorization and cosine similarity, refined by
sentiment, toxicity, and topic filters to ensure relevant and context-aligned
video recommendations. This system achieved 69% aggregate coverage, with
monthly coverage rates consistently above 85%, demonstrating robust performance
and adaptability over time. Evaluation across recommendation sizes showed
coverage reaching 69% for five video recommendations and 79% for ten video
recommendations per video. In summary, this work presents a framework for
understanding COVID-19 discourse on YouTube and a recommendation system that
supports user engagement while promoting responsible and relevant content
related to COVID-19.",2024-12-22 22:43:36+00:00,"['Vanessa Su', 'Nirmalya Thakur']",http://arxiv.org/abs/2412.17180v1
X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization,"Lately, there has been growing interest in adapting vision-language models
(VLMs) to image and third-person video classification due to their success in
zero-shot recognition. However, the adaptation of these models to egocentric
videos has been largely unexplored. To address this gap, we propose a simple
yet effective cross-modal adaptation framework, which we call X-MIC. Using a
video adapter, our pipeline learns to align frozen text embeddings to each
egocentric video directly in the shared embedding space. Our novel adapter
architecture retains and improves generalization of the pre-trained VLMs by
disentangling learnable temporal modeling and frozen visual encoder. This
results in an enhanced alignment of text embeddings to each egocentric video,
leading to a significant improvement in cross-dataset generalization. We
evaluate our approach on the Epic-Kitchens, Ego4D, and EGTEA datasets for
fine-grained cross-dataset action generalization, demonstrating the
effectiveness of our method. Code is available at
https://github.com/annusha/xmic",2024-03-28 19:45:35+00:00,"['Anna Kukleva', 'Fadime Sener', 'Edoardo Remelli', 'Bugra Tekin', 'Eric Sauser', 'Bernt Schiele', 'Shugao Ma']",http://arxiv.org/abs/2403.19811v1
Evaluation of Text-to-Video Generation Models: A Dynamics Perspective,"Comprehensive and constructive evaluation protocols play an important role in
the development of sophisticated text-to-video (T2V) generation models.
Existing evaluation protocols primarily focus on temporal consistency and
content continuity, yet largely ignore the dynamics of video content. Dynamics
are an essential dimension for measuring the visual vividness and the honesty
of video content to text prompts. In this study, we propose an effective
evaluation protocol, termed DEVIL, which centers on the dynamics dimension to
evaluate T2V models. For this purpose, we establish a new benchmark comprising
text prompts that fully reflect multiple dynamics grades, and define a set of
dynamics scores corresponding to various temporal granularities to
comprehensively evaluate the dynamics of each generated video. Based on the new
benchmark and the dynamics scores, we assess T2V models with the design of
three metrics: dynamics range, dynamics controllability, and dynamics-based
quality. Experiments show that DEVIL achieves a Pearson correlation exceeding
90% with human ratings, demonstrating its potential to advance T2V generation
models. Code is available at https://github.com/MingXiangL/DEVIL.",2024-07-01 08:51:22+00:00,"['Mingxiang Liao', 'Hannan Lu', 'Xinyu Zhang', 'Fang Wan', 'Tianyu Wang', 'Yuzhong Zhao', 'Wangmeng Zuo', 'Qixiang Ye', 'Jingdong Wang']",http://arxiv.org/abs/2407.01094v1
General and Task-Oriented Video Segmentation,"We present GvSeg, a general video segmentation framework for addressing four
different video segmentation tasks (i.e., instance, semantic, panoptic, and
exemplar-guided) while maintaining an identical architectural design.
Currently, there is a trend towards developing general video segmentation
solutions that can be applied across multiple tasks. This streamlines research
endeavors and simplifies deployment. However, such a highly homogenized
framework in current design, where each element maintains uniformity, could
overlook the inherent diversity among different tasks and lead to suboptimal
performance. To tackle this, GvSeg: i) provides a holistic disentanglement and
modeling for segment targets, thoroughly examining them from the perspective of
appearance, position, and shape, and on this basis, ii) reformulates the query
initialization, matching and sampling strategies in alignment with the
task-specific requirement. These architecture-agnostic innovations empower
GvSeg to effectively address each unique task by accommodating the specific
properties that characterize them. Extensive experiments on seven gold-standard
benchmark datasets demonstrate that GvSeg surpasses all existing
specialized/general solutions by a significant margin on four different video
segmentation tasks.",2024-07-09 04:21:38+00:00,"['Mu Chen', 'Liulei Li', 'Wenguan Wang', 'Ruijie Quan', 'Yi Yang']",http://arxiv.org/abs/2407.06540v1
3DAttGAN: A 3D Attention-based Generative Adversarial Network for Joint Space-Time Video Super-Resolution,"In many applications, including surveillance, entertainment, and restoration,
there is a need to increase both the spatial resolution and the frame rate of a
video sequence. The aim is to improve visual quality, refine details, and
create a more realistic viewing experience. Existing space-time video
super-resolution methods do not effectively use spatio-temporal information. To
address this limitation, we propose a generative adversarial network for joint
space-time video super-resolution. The generative network consists of three
operations: shallow feature extraction, deep feature extraction, and
reconstruction. It uses three-dimensional (3D) convolutions to process temporal
and spatial information simultaneously and includes a novel 3D attention
mechanism to extract the most important channel and spatial information. The
discriminative network uses a two-branch structure to handle details and motion
information, making the generated results more accurate. Experimental results
on the Vid4, Vimeo-90K, and REDS datasets demonstrate the effectiveness of the
proposed method. The source code is publicly available at
https://github.com/FCongRui/3DAttGan.git.",2024-07-24 03:11:47+00:00,"['Congrui Fu', 'Hui Yuan', 'Liquan Shen', 'Raouf Hamzaoui', 'Hao Zhang']",http://arxiv.org/abs/2407.16965v1
Dormant: Defending against Pose-driven Human Image Animation,"Pose-driven human image animation has achieved tremendous progress, enabling
the generation of vivid and realistic human videos from just one single photo.
However, it conversely exacerbates the risk of image misuse, as attackers may
use one available image to create videos involving politics, violence, and
other illegal content. To counter this threat, we propose Dormant, a novel
protection approach tailored to defend against pose-driven human image
animation techniques. Dormant applies protective perturbation to one human
image, preserving the visual similarity to the original but resulting in
poor-quality video generation. The protective perturbation is optimized to
induce misextraction of appearance features from the image and create
incoherence among the generated video frames. Our extensive evaluation across 8
animation methods and 4 datasets demonstrates the superiority of Dormant over 6
baseline protection methods, leading to misaligned identities, visual
distortions, noticeable artifacts, and inconsistent frames in the generated
videos. Moreover, Dormant shows effectiveness on 6 real-world commercial
services, even with fully black-box access.",2024-09-22 12:51:32+00:00,"['Jiachen Zhou', 'Mingsi Wang', 'Tianlin Li', 'Guozhu Meng', 'Kai Chen']",http://arxiv.org/abs/2409.14424v2
SimTube: Generating Simulated Video Comments through Multimodal AI and User Personas,"Audience feedback is crucial for refining video content, yet it typically
comes after publication, limiting creators' ability to make timely adjustments.
To bridge this gap, we introduce SimTube, a generative AI system designed to
simulate audience feedback in the form of video comments before a video's
release. SimTube features a computational pipeline that integrates multimodal
data from the video-such as visuals, audio, and metadata-with user personas
derived from a broad and diverse corpus of audience demographics, generating
varied and contextually relevant feedback. Furthermore, the system's UI allows
creators to explore and customize the simulated comments. Through a
comprehensive evaluation-comprising quantitative analysis, crowd-sourced
assessments, and qualitative user studies-we show that SimTube's generated
comments are not only relevant, believable, and diverse but often more detailed
and informative than actual audience comments, highlighting its potential to
help creators refine their content before release.",2024-11-14 16:35:17+00:00,"['Yu-Kai Hung', 'Yun-Chien Huang', 'Ting-Yu Su', 'Yen-Ting Lin', 'Lung-Pan Cheng', 'Bryan Wang', 'Shao-Hua Sun']",http://arxiv.org/abs/2411.09577v2
ARCON: Advancing Auto-Regressive Continuation for Driving Videos,"Recent advancements in auto-regressive large language models (LLMs) have led
to their application in video generation. This paper explores the use of Large
Vision Models (LVMs) for video continuation, a task essential for building
world models and predicting future frames. We introduce ARCON, a scheme that
alternates between generating semantic and RGB tokens, allowing the LVM to
explicitly learn high-level structural video information. We find high
consistency in the RGB images and semantic maps generated without special
design. Moreover, we employ an optical flow-based texture stitching method to
enhance visual quality. Experiments in autonomous driving scenarios show that
our model can consistently generate long videos.",2024-12-04 22:53:56+00:00,"['Ruibo Ming', 'Jingwei Wu', 'Zhewei Huang', 'Zhuoxuan Ju', 'Jianming HU', 'Lihui Peng', 'Shuchang Zhou']",http://arxiv.org/abs/2412.03758v3
ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping,"In this paper, we introduce ManiVideo, a novel method for generating
consistent and temporally coherent bimanual hand-object manipulation videos
from given motion sequences of hands and objects. The core idea of ManiVideo is
the construction of a multi-layer occlusion (MLO) representation that learns 3D
occlusion relationships from occlusion-free normal maps and occlusion
confidence maps. By embedding the MLO structure into the UNet in two forms, the
model enhances the 3D consistency of dexterous hand-object manipulation. To
further achieve the generalizable grasping of objects, we integrate Objaverse,
a large-scale 3D object dataset, to address the scarcity of video data, thereby
facilitating the learning of extensive object consistency. Additionally, we
propose an innovative training strategy that effectively integrates multiple
datasets, supporting downstream tasks such as human-centric hand-object
manipulation video generation. Through extensive experiments, we demonstrate
that our approach not only achieves video generation with plausible hand-object
interaction and generalizable objects, but also outperforms existing SOTA
methods.",2024-12-18 00:37:55+00:00,"['Youxin Pang', 'Ruizhi Shao', 'Jiajun Zhang', 'Hanzhang Tu', 'Yun Liu', 'Boyao Zhou', 'Hongwen Zhang', 'Yebin Liu']",http://arxiv.org/abs/2412.16212v1
Gender Bias in Text-to-Video Generation Models: A case study of Sora,"The advent of text-to-video generation models has revolutionized content
creation as it produces high-quality videos from textual prompts. However,
concerns regarding inherent biases in such models have prompted scrutiny,
particularly regarding gender representation. Our study investigates the
presence of gender bias in OpenAI's Sora, a state-of-the-art text-to-video
generation model. We uncover significant evidence of bias by analyzing the
generated videos from a diverse set of gender-neutral and stereotypical
prompts. The results indicate that Sora disproportionately associates specific
genders with stereotypical behaviors and professions, which reflects societal
prejudices embedded in its training data.",2024-12-30 18:08:13+00:00,"['Mohammad Nadeem', 'Shahab Saquib Sohail', 'Erik Cambria', 'Bjrn W. Schuller', 'Amir Hussain']",http://arxiv.org/abs/2501.01987v2
JPEG-LM: LLMs as Image Generators with Canonical Codec Representations,"Recent work in image and video generation has been adopting the
autoregressive LLM architecture due to its generality and potentially easy
integration into multi-modal systems. The crux of applying autoregressive
training in language generation to visual generation is discretization --
representing continuous data like images and videos as discrete tokens. Common
methods of discretizing images and videos include modeling raw pixel values,
which are prohibitively lengthy, or vector quantization, which requires
convoluted pre-hoc training. In this work, we propose to directly model images
and videos as compressed files saved on computers via canonical codecs (e.g.,
JPEG, AVC/H.264). Using the default Llama architecture without any
vision-specific modifications, we pretrain JPEG-LM from scratch to generate
images (and AVC-LM to generate videos as a proof of concept), by directly
outputting compressed file bytes in JPEG and AVC formats. Evaluation of image
generation shows that this simple and straightforward approach is more
effective than pixel-based modeling and sophisticated vector quantization
baselines (on which our method yields a 31% reduction in FID). Our analysis
shows that JPEG-LM has an especial advantage over vector quantization models in
generating long-tail visual elements. Overall, we show that using canonical
codec representations can help lower the barriers between language generation
and visual generation, facilitating future research on multi-modal
language/image/video LLMs.",2024-08-15 23:57:02+00:00,"['Xiaochuang Han', 'Marjan Ghazvininejad', 'Pang Wei Koh', 'Yulia Tsvetkov']",http://arxiv.org/abs/2408.08459v2
Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities,"The advent of AI has influenced many aspects of human life, from self-driving
cars and intelligent chatbots to text-based image and video generation models
capable of creating realistic images and videos based on user prompts
(text-to-image, image-to-image, and image-to-video). AI-based methods for image
and video super resolution, video frame interpolation, denoising, and
compression have already gathered significant attention and interest in the
industry and some solutions are already being implemented in real-world
products and services. However, to achieve widespread integration and
acceptance, AI-generated and enhanced content must be visually accurate, adhere
to intended use, and maintain high visual quality to avoid degrading the end
user's quality of experience (QoE).
  One way to monitor and control the visual ""quality"" of AI-generated and
-enhanced content is by deploying Image Quality Assessment (IQA) and Video
Quality Assessment (VQA) models. However, most existing IQA and VQA models
measure visual fidelity in terms of ""reconstruction"" quality against a pristine
reference content and were not designed to assess the quality of ""generative""
artifacts. To address this, newer metrics and models have recently been
proposed, but their performance evaluation and overall efficacy have been
limited by datasets that were too small or otherwise lack representative
content and/or distortion capacity; and by performance measures that can
accurately report the success of an IQA/VQA model for ""GenAI"". This paper
examines the current shortcomings and possibilities presented by AI-generated
and enhanced image and video content, with a particular focus on end-user
perceived quality. Finally, we discuss open questions and make recommendations
for future work on the ""GenAI"" quality assessment problems, towards further
progressing on this interesting and relevant field of research.",2024-10-11 05:08:44+00:00,"['Abhijay Ghildyal', 'Yuanhan Chen', 'Saman Zadtootaghaj', 'Nabajeet Barman', 'Alan C. Bovik']",http://arxiv.org/abs/2410.08534v2
Generative Enhancement for 3D Medical Images,"The limited availability of 3D medical image datasets, due to privacy
concerns and high collection or annotation costs, poses significant challenges
in the field of medical imaging. While a promising alternative is the use of
synthesized medical data, there are few solutions for realistic 3D medical
image synthesis due to difficulties in backbone design and fewer 3D training
samples compared to 2D counterparts. In this paper, we propose GEM-3D, a novel
generative approach to the synthesis of 3D medical images and the enhancement
of existing datasets using conditional diffusion models. Our method begins with
a 2D slice, noted as the informed slice to serve the patient prior, and
propagates the generation process using a 3D segmentation mask. By decomposing
the 3D medical images into masks and patient prior information, GEM-3D offers a
flexible yet effective solution for generating versatile 3D images from
existing datasets. GEM-3D can enable dataset enhancement by combining informed
slice selection and generation at random positions, along with editable mask
volumes to introduce large variations in diffusion sampling. Moreover, as the
informed slice contains patient-wise information, GEM-3D can also facilitate
counterfactual image synthesis and dataset-level de-enhancement with desired
control. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D
is capable of synthesizing high-quality 3D medical images with volumetric
consistency, offering a straightforward solution for dataset enhancement during
inference. The code is available at https://github.com/HKU-MedAI/GEM-3D.",2024-03-19 15:57:04+00:00,"['Lingting Zhu', 'Noel Codella', 'Dongdong Chen', 'Zhenchao Jin', 'Lu Yuan', 'Lequan Yu']",http://arxiv.org/abs/2403.12852v2
ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis,"Gestures play a key role in human communication. Recent methods for co-speech
gesture generation, while managing to generate beat-aligned motions, struggle
generating gestures that are semantically aligned with the utterance. Compared
to beat gestures that align naturally to the audio signal, semantically
coherent gestures require modeling the complex interactions between the
language and human motion, and can be controlled by focusing on certain words.
Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal
gesture synthesis, which can not only generate gestures based on multi-modal
speech inputs, but can also facilitate controllability in gesture synthesis.
Our method proposes two guidance objectives that allow the users to modulate
the impact of different conditioning modalities (e.g. audio vs text) as well as
to choose certain words to be emphasized during gesturing. Our method is
versatile in that it can be trained either for generating monologue gestures or
even the conversational gestures. To further advance the research on
multi-party interactive gestures, the DnD Group Gesture dataset is released,
which contains 6 hours of gesture data showing 5 people interacting with one
another. We compare our method with several recent works and demonstrate
effectiveness of our method on a variety of tasks. We urge the reader to watch
our supplementary video at our website.",2024-03-26 17:59:52+00:00,"['Muhammad Hamza Mughal', 'Rishabh Dabral', 'Ikhsanul Habibie', 'Lucia Donatelli', 'Marc Habermann', 'Christian Theobalt']",http://arxiv.org/abs/2403.17936v1
LaSe-E2V: Towards Language-guided Semantic-Aware Event-to-Video Reconstruction,"Event cameras harness advantages such as low latency, high temporal
resolution, and high dynamic range (HDR), compared to standard cameras. Due to
the distinct imaging paradigm shift, a dominant line of research focuses on
event-to-video (E2V) reconstruction to bridge event-based and standard computer
vision. However, this task remains challenging due to its inherently ill-posed
nature: event cameras only detect the edge and motion information locally.
Consequently, the reconstructed videos are often plagued by artifacts and
regional blur, primarily caused by the ambiguous semantics of event data. In
this paper, we find language naturally conveys abundant semantic information,
rendering it stunningly superior in ensuring semantic consistency for E2V
reconstruction. Accordingly, we propose a novel framework, called LaSe-E2V,
that can achieve semantic-aware high-quality E2V reconstruction from a
language-guided perspective, buttressed by the text-conditional diffusion
models. However, due to diffusion models' inherent diversity and randomness, it
is hardly possible to directly apply them to achieve spatial and temporal
consistency for E2V reconstruction. Thus, we first propose an Event-guided
Spatiotemporal Attention (ESA) module to condition the event data to the
denoising pipeline effectively. We then introduce an event-aware mask loss to
ensure temporal coherence and a noise initialization strategy to enhance
spatial consistency. Given the absence of event-text-video paired data, we
aggregate existing E2V datasets and generate textual descriptions using the
tagging models for training and evaluation. Extensive experiments on three
datasets covering diverse challenging scenarios (e.g., fast motion, low light)
demonstrate the superiority of our method.",2024-07-08 01:40:32+00:00,"['Kanghao Chen', 'Hangyu Li', 'JiaZhou Zhou', 'Zeyu Wang', 'Lin Wang']",http://arxiv.org/abs/2407.05547v3
Noise2Image: Noise-Enabled Static Scene Recovery for Event Cameras,"Event cameras, also known as dynamic vision sensors, are an emerging modality
for measuring fast dynamics asynchronously. Event cameras capture changes of
log-intensity over time as a stream of 'events' and generally cannot measure
intensity itself; hence, they are only used for imaging dynamic scenes.
However, fluctuations due to random photon arrival inevitably trigger noise
events, even for static scenes. While previous efforts have been focused on
filtering out these undesirable noise events to improve signal quality, we find
that, in the photon-noise regime, these noise events are correlated with the
static scene intensity. We analyze the noise event generation and model its
relationship to illuminance. Based on this understanding, we propose a method,
called Noise2Image, to leverage the illuminance-dependent noise characteristics
to recover the static parts of a scene, which are otherwise invisible to event
cameras. We experimentally collect a dataset of noise events on static scenes
to train and validate Noise2Image. Our results provide a novel approach for
capturing static scenes in event cameras, solely from noise events, without
additional hardware.",2024-04-01 17:59:53+00:00,"['Ruiming Cao', 'Dekel Galor', 'Amit Kohli', 'Jacob L Yates', 'Laura Waller']",http://arxiv.org/abs/2404.01298v2
From Sim-to-Real: Toward General Event-based Low-light Frame Interpolation with Per-scene Optimization,"Video Frame Interpolation (VFI) is important for video enhancement, frame
rate up-conversion, and slow-motion generation. The introduction of event
cameras, which capture per-pixel brightness changes asynchronously, has
significantly enhanced VFI capabilities, particularly for high-speed, nonlinear
motions. However, these event-based methods encounter challenges in low-light
conditions, notably trailing artifacts and signal latency, which hinder their
direct applicability and generalization. Addressing these issues, we propose a
novel per-scene optimization strategy tailored for low-light conditions. This
approach utilizes the internal statistics of a sequence to handle degraded
event data under low-light conditions, improving the generalizability to
different lighting and camera settings. To evaluate its robustness in low-light
condition, we further introduce EVFI-LL, a unique RGB+Event dataset captured
under low-light conditions. Our results demonstrate state-of-the-art
performance in low-light environments. Project page:
https://naturezhanghn.github.io/sim2real.",2024-06-12 11:15:59+00:00,"['Ziran Zhang', 'Yongrui Ma', 'Yueting Chen', 'Feng Zhang', 'Jinwei Gu', 'Tianfan Xue', 'Shi Guo']",http://arxiv.org/abs/2406.08090v2
DEGSTalk: Decomposed Per-Embedding Gaussian Fields for Hair-Preserving Talking Face Synthesis,"Accurately synthesizing talking face videos and capturing fine facial
features for individuals with long hair presents a significant challenge. To
tackle these challenges in existing methods, we propose a decomposed
per-embedding Gaussian fields (DEGSTalk), a 3D Gaussian Splatting (3DGS)-based
talking face synthesis method for generating realistic talking faces with long
hairs. Our DEGSTalk employs Deformable Pre-Embedding Gaussian Fields, which
dynamically adjust pre-embedding Gaussian primitives using implicit expression
coefficients. This enables precise capture of dynamic facial regions and subtle
expressions. Additionally, we propose a Dynamic Hair-Preserving Portrait
Rendering technique to enhance the realism of long hair motions in the
synthesized videos. Results show that DEGSTalk achieves improved realism and
synthesis quality compared to existing approaches, particularly in handling
complex facial dynamics and hair preservation. Our code will be publicly
available at https://github.com/CVI-SZU/DEGSTalk.",2024-12-28 13:23:31+00:00,"['Kaijun Deng', 'Dezhi Zheng', 'Jindong Xie', 'Jinbao Wang', 'Weicheng Xie', 'Linlin Shen', 'Siyang Song']",http://arxiv.org/abs/2412.20148v1
Cascaded Multi-path Shortcut Diffusion Model for Medical Image Translation,"Image-to-image translation is a vital component in medical imaging
processing, with many uses in a wide range of imaging modalities and clinical
scenarios. Previous methods include Generative Adversarial Networks (GANs) and
Diffusion Models (DMs), which offer realism but suffer from instability and
lack uncertainty estimation. Even though both GAN and DM methods have
individually exhibited their capability in medical image translation tasks, the
potential of combining a GAN and DM to further improve translation performance
and to enable uncertainty estimation remains largely unexplored. In this work,
we address these challenges by proposing a Cascade Multi-path Shortcut
Diffusion Model (CMDM) for high-quality medical image translation and
uncertainty estimation. To reduce the required number of iterations and ensure
robust performance, our method first obtains a conditional GAN-generated prior
image that will be used for the efficient reverse translation with a DM in the
subsequent step. Additionally, a multi-path shortcut diffusion strategy is
employed to refine translation results and estimate uncertainty. A cascaded
pipeline further enhances translation quality, incorporating residual averaging
between cascades. We collected three different medical image datasets with two
sub-tasks for each dataset to test the generalizability of our approach. Our
experimental results found that CMDM can produce high-quality translations
comparable to state-of-the-art methods while providing reasonable uncertainty
estimations that correlate well with the translation error.",2024-04-06 03:02:47+00:00,"['Yinchi Zhou', 'Tianqi Chen', 'Jun Hou', 'Huidong Xie', 'Nicha C. Dvornek', 'S. Kevin Zhou', 'David L. Wilson', 'James S. Duncan', 'Chi Liu', 'Bo Zhou']",http://arxiv.org/abs/2405.12223v3
Conditional Controllable Image Fusion,"Image fusion aims to integrate complementary information from multiple input
images acquired through various sources to synthesize a new fused image.
Existing methods usually employ distinct constraint designs tailored to
specific scenes, forming fixed fusion paradigms. However, this data-driven
fusion approach is challenging to deploy in varying scenarios, especially in
rapidly changing environments. To address this issue, we propose a conditional
controllable fusion (CCF) framework for general image fusion tasks without
specific training. Due to the dynamic differences of different samples, our CCF
employs specific fusion constraints for each individual in practice. Given the
powerful generative capabilities of the denoising diffusion model, we first
inject the specific constraints into the pre-trained DDPM as adaptive fusion
conditions. The appropriate conditions are dynamically selected to ensure the
fusion process remains responsive to the specific requirements in each reverse
diffusion stage. Thus, CCF enables conditionally calibrating the fused images
step by step. Extensive experiments validate our effectiveness in general
fusion tasks across diverse scenarios against the competing methods without
additional training.",2024-11-03 13:56:15+00:00,"['Bing Cao', 'Xingxin Xu', 'Pengfei Zhu', 'Qilong Wang', 'Qinghua Hu']",http://arxiv.org/abs/2411.01573v1
"CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis","Neural Radiance Fields (NeRF) have achieved huge success in effectively
capturing and representing 3D objects and scenes. However, to establish a
ubiquitous presence in everyday media formats, such as images and videos, we
need to fulfill three key objectives: 1. fast encoding and decoding time, 2.
compact model sizes, and 3. high-quality renderings. Despite recent
advancements, a comprehensive algorithm that adequately addresses all
objectives has yet to be fully realized. In this work, we present CodecNeRF, a
neural codec for NeRF representations, consisting of an encoder and decoder
architecture that can generate a NeRF representation in a single forward pass.
Furthermore, inspired by the recent parameter-efficient finetuning approaches,
we propose a finetuning method to efficiently adapt the generated NeRF
representations to a new test instance, leading to high-quality image
renderings and compact code sizes. The proposed CodecNeRF, a newly suggested
encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented
compression performance of more than 100x and remarkable reduction in encoding
time while maintaining (or improving) the image quality on widely used 3D
object datasets.",2024-04-07 10:49:59+00:00,"['Gyeongjin Kang', 'Younggeun Lee', 'Seungjun Oh', 'Eunbyung Park']",http://arxiv.org/abs/2404.04913v3
RDSinger: Reference-based Diffusion Network for Singing Voice Synthesis,"Singing voice synthesis (SVS) aims to produce high-fidelity singing audio
from music scores, requiring a detailed understanding of notes, pitch, and
duration, unlike text-to-speech tasks. Although diffusion models have shown
exceptional performance in various generative tasks like image and video
creation, their application in SVS is hindered by time complexity and the
challenge of capturing acoustic features, particularly during pitch
transitions. Some networks learn from the prior distribution and use the
compressed latent state as a better start in the diffusion model, but the
denoising step doesn't consistently improve quality over the entire duration.
We introduce RDSinger, a reference-based denoising diffusion network that
generates high-quality audio for SVS tasks. Our approach is inspired by Animate
Anyone, a diffusion image network that maintains intricate appearance features
from reference images. RDSinger utilizes FastSpeech2 mel-spectrogram as a
reference to mitigate denoising step artifacts. Additionally, existing models
could be influenced by misleading information on the compressed latent state
during pitch transitions. We address this issue by applying Gaussian blur on
partial reference mel-spectrogram and adjusting loss weights in these regions.
Extensive ablation studies demonstrate the efficiency of our method.
Evaluations on OpenCpop, a Chinese singing dataset, show that RDSinger
outperforms current state-of-the-art SVS methods in performance.",2024-10-29 01:01:18+00:00,"['Kehan Sui', 'Jinxu Xiang', 'Fang Jin']",http://arxiv.org/abs/2410.21641v1
GameIR: A Large-Scale Synthesized Ground-Truth Dataset for Image Restoration over Gaming Content,"Image restoration methods like super-resolution and image synthesis have been
successfully used in commercial cloud gaming products like NVIDIA's DLSS.
However, restoration over gaming content is not well studied by the general
public. The discrepancy is mainly caused by the lack of ground-truth gaming
training data that match the test cases. Due to the unique characteristics of
gaming content, the common approach of generating pseudo training data by
degrading the original HR images results in inferior restoration performance.
In this work, we develop GameIR, a large-scale high-quality
computer-synthesized ground-truth dataset to fill in the blanks, targeting at
two different applications. The first is super-resolution with deferred
rendering, to support the gaming solution of rendering and transferring LR
images only and restoring HR images on the client side. We provide 19200 LR-HR
paired ground-truth frames coming from 640 videos rendered at 720p and 1440p
for this task. The second is novel view synthesis (NVS), to support the
multiview gaming solution of rendering and transferring part of the multiview
frames and generating the remaining frames on the client side. This task has
57,600 HR frames from 960 videos of 160 scenes with 6 camera views. In addition
to the RGB frames, the GBuffers during the deferred rendering stage are also
provided, which can be used to help restoration. Furthermore, we evaluate
several SOTA super-resolution algorithms and NeRF-based NVS algorithms over our
dataset, which demonstrates the effectiveness of our ground-truth GameIR data
in improving restoration performance for gaming content. Also, we test the
method of incorporating the GBuffers as additional input information for
helping super-resolution and NVS. We release our dataset and models to the
general public to facilitate research on restoration methods over gaming
content.",2024-08-29 19:11:46+00:00,"['Lebin Zhou', 'Kun Han', 'Nam Ling', 'Wei Wang', 'Wei Jiang']",http://arxiv.org/abs/2408.16866v1
Network Bending of Diffusion Models for Audio-Visual Generation,"In this paper we present the first steps towards the creation of a tool which
enables artists to create music visualizations using pre-trained, generative,
machine learning models. First, we investigate the application of network
bending, the process of applying transforms within the layers of a generative
network, to image generation diffusion models by utilizing a range of
point-wise, tensor-wise, and morphological operators. We identify a number of
visual effects that result from various operators, including some that are not
easily recreated with standard image editing tools. We find that this process
allows for continuous, fine-grain control of image generation which can be
helpful for creative applications. Next, we generate music-reactive videos
using Stable Diffusion by passing audio features as parameters to network
bending operators. Finally, we comment on certain transforms which radically
shift the image and the possibilities of learning more about the latent space
of Stable Diffusion based on these transforms.",2024-06-28 00:39:17+00:00,"['Luke Dzwonczyk', 'Carmine Emanuele Cella', 'David Ban']",http://arxiv.org/abs/2406.19589v1
Disentangled Multimodal Brain MR Image Translation via Transformer-based Modality Infuser,"Multimodal Magnetic Resonance (MR) Imaging plays a crucial role in disease
diagnosis due to its ability to provide complementary information by analyzing
a relationship between multimodal images on the same subject. Acquiring all MR
modalities, however, can be expensive, and, during a scanning session, certain
MR images may be missed depending on the study protocol. The typical solution
would be to synthesize the missing modalities from the acquired images such as
using generative adversarial networks (GANs). Yet, GANs constructed with
convolutional neural networks (CNNs) are likely to suffer from a lack of global
relationships and mechanisms to condition the desired modality. To address
this, in this work, we propose a transformer-based modality infuser designed to
synthesize multimodal brain MR images. In our method, we extract
modality-agnostic features from the encoder and then transform them into
modality-specific features using the modality infuser. Furthermore, the
modality infuser captures long-range relationships among all brain structures,
leading to the generation of more realistic images. We carried out experiments
on the BraTS 2018 dataset, translating between four MR modalities, and our
experimental results demonstrate the superiority of our proposed method in
terms of synthesis quality. In addition, we conducted experiments on a brain
tumor segmentation task and different conditioning methods.",2024-02-01 06:34:35+00:00,"['Jihoon Cho', 'Xiaofeng Liu', 'Fangxu Xing', 'Jinsong Ouyang', 'Georges El Fakhri', 'Jinah Park', 'Jonghye Woo']",http://arxiv.org/abs/2402.00375v1
GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation,"Creating 4D fields of Gaussian Splatting from images or videos is a
challenging task due to its under-constrained nature. While the optimization
can draw photometric reference from the input videos or be regulated by
generative models, directly supervising Gaussian motions remains underexplored.
In this paper, we introduce a novel concept, Gaussian flow, which connects the
dynamics of 3D Gaussians and pixel velocities between consecutive frames. The
Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into
the image space. This differentiable process enables direct dynamic supervision
from optical flow. Our method significantly benefits 4D dynamic content
generation and 4D novel view synthesis with Gaussian Splatting, especially for
contents with rich motions that are hard to be handled by existing methods. The
common color drifting issue that happens in 4D generation is also resolved with
improved Guassian dynamics. Superior visual quality on extensive experiments
demonstrates our method's effectiveness. Quantitative and qualitative
evaluations show that our method achieves state-of-the-art results on both
tasks of 4D generation and 4D novel view synthesis. Project page:
https://zerg-overmind.github.io/GaussianFlow.github.io/",2024-03-19 02:22:21+00:00,"['Quankai Gao', 'Qiangeng Xu', 'Zhe Cao', 'Ben Mildenhall', 'Wenchao Ma', 'Le Chen', 'Danhang Tang', 'Ulrich Neumann']",http://arxiv.org/abs/2403.12365v2
Novel Hybrid Integrated Pix2Pix and WGAN Model with Gradient Penalty for Binary Images Denoising,"This paper introduces a novel approach to image denoising that leverages the
advantages of Generative Adversarial Networks (GANs). Specifically, we propose
a model that combines elements of the Pix2Pix model and the Wasserstein GAN
(WGAN) with Gradient Penalty (WGAN-GP). This hybrid framework seeks to
capitalize on the denoising capabilities of conditional GANs, as demonstrated
in the Pix2Pix model, while mitigating the need for an exhaustive search for
optimal hyperparameters that could potentially ruin the stability of the
learning process. In the proposed method, the GAN's generator is employed to
produce denoised images, harnessing the power of a conditional GAN for noise
reduction. Simultaneously, the implementation of the Lipschitz continuity
constraint during updates, as featured in WGAN-GP, aids in reducing
susceptibility to mode collapse. This innovative design allows the proposed
model to benefit from the strong points of both Pix2Pix and WGAN-GP, generating
superior denoising results while ensuring training stability. Drawing on
previous work on image-to-image translation and GAN stabilization techniques,
the proposed research highlights the potential of GANs as a general-purpose
solution for denoising. The paper details the development and testing of this
model, showcasing its effectiveness through numerical experiments. The dataset
was created by adding synthetic noise to clean images. Numerical results based
on real-world dataset validation underscore the efficacy of this approach in
image-denoising tasks, exhibiting significant enhancements over traditional
techniques. Notably, the proposed model demonstrates strong generalization
capabilities, performing effectively even when trained with synthetic noise.",2024-07-16 15:50:45+00:00,"['Luca Tirel', 'Ali Mohamed Ali', 'Hashim A. Hashim']",http://arxiv.org/abs/2407.11865v2
Advancing Weakly-Supervised Audio-Visual Video Parsing via Segment-wise Pseudo Labeling,"The Audio-Visual Video Parsing task aims to identify and temporally localize
the events that occur in either or both the audio and visual streams of audible
videos. It often performs in a weakly-supervised manner, where only video event
labels are provided, \ie, the modalities and the timestamps of the labels are
unknown. Due to the lack of densely annotated labels, recent work attempts to
leverage pseudo labels to enrich the supervision. A commonly used strategy is
to generate pseudo labels by categorizing the known video event labels for each
modality. However, the labels are still confined to the video level, and the
temporal boundaries of events remain unlabeled. In this paper, we propose a new
pseudo label generation strategy that can explicitly assign labels to each
video segment by utilizing prior knowledge learned from the open world.
Specifically, we exploit the large-scale pretrained models, namely CLIP and
CLAP, to estimate the events in each video segment and generate segment-level
visual and audio pseudo labels, respectively. We then propose a new loss
function to exploit these pseudo labels by taking into account their
category-richness and segment-richness. A label denoising strategy is also
adopted to further improve the visual pseudo labels by flipping them whenever
abnormally large forward losses occur. We perform extensive experiments on the
LLP dataset and demonstrate the effectiveness of each proposed design and we
achieve state-of-the-art video parsing performance on all types of event
parsing, \ie, audio event, visual event, and audio-visual event. We also
examine the proposed pseudo label generation strategy on a relevant
weakly-supervised audio-visual event localization task and the experimental
results again verify the benefits and generalization of our method.",2024-06-03 01:09:15+00:00,"['Jinxing Zhou', 'Dan Guo', 'Yiran Zhong', 'Meng Wang']",http://arxiv.org/abs/2406.00919v1
GAN-Based Architecture for Low-dose Computed Tomography Imaging Denoising,"Generative Adversarial Networks (GANs) have surfaced as a revolutionary
element within the domain of low-dose computed tomography (LDCT) imaging,
providing an advanced resolution to the enduring issue of reconciling radiation
exposure with image quality. This comprehensive review synthesizes the rapid
advancements in GAN-based LDCT denoising techniques, examining the evolution
from foundational architectures to state-of-the-art models incorporating
advanced features such as anatomical priors, perceptual loss functions, and
innovative regularization strategies. We critically analyze various GAN
architectures, including conditional GANs (cGANs), CycleGANs, and
Super-Resolution GANs (SRGANs), elucidating their unique strengths and
limitations in the context of LDCT denoising. The evaluation provides both
qualitative and quantitative results related to the improvements in performance
in benchmark and clinical datasets with metrics such as PSNR, SSIM, and LPIPS.
After highlighting the positive results, we discuss some of the challenges
preventing a wider clinical use, including the interpretability of the images
generated by GANs, synthetic artifacts, and the need for clinically relevant
metrics. The review concludes by highlighting the essential significance of
GAN-based methodologies in the progression of precision medicine via tailored
LDCT denoising models, underlining the transformative possibilities presented
by artificial intelligence within contemporary radiological practice.",2024-11-14 15:26:10+00:00,"['Yunuo Wang', 'Ningning Yang', 'Jialin Li']",http://arxiv.org/abs/2411.09512v2
VDMA: Video Question Answering with Dynamically Generated Multi-Agents,"This technical report provides a detailed description of our approach to the
EgoSchema Challenge 2024. The EgoSchema Challenge aims to identify the most
appropriate responses to questions regarding a given video clip. In this paper,
we propose Video Question Answering with Dynamically Generated Multi-Agents
(VDMA). This method is a complementary approach to existing response generation
systems by employing a multi-agent system with dynamically generated expert
agents. This method aims to provide the most accurate and contextually
appropriate responses. This report details the stages of our approach, the
tools employed, and the results of our experiments.",2024-07-04 03:40:36+00:00,"['Noriyuki Kugo', 'Tatsuya Ishibashi', 'Kosuke Ono', 'Yuji Sato']",http://arxiv.org/abs/2407.03610v1
Embodiment-Agnostic Action Planning via Object-Part Scene Flow,"Observing that the key for robotic action planning is to understand the
target-object motion when its associated part is manipulated by the end
effector, we propose to generate the 3D object-part scene flow and extract its
transformations to solve the action trajectories for diverse embodiments. The
advantage of our approach is that it derives the robot action explicitly from
object motion prediction, yielding a more robust policy by understanding the
object motions. Also, beyond policies trained on embodiment-centric data, our
method is embodiment-agnostic, generalizable across diverse embodiments, and
being able to learn from human demonstrations. Our method comprises three
components: an object-part predictor to locate the part for the end effector to
manipulate, an RGBD video generator to predict future RGBD videos, and a
trajectory planner to extract embodiment-agnostic transformation sequences and
solve the trajectory for diverse embodiments. Trained on videos even without
trajectory data, our method still outperforms existing works significantly by
27.7% and 26.2% on the prevailing virtual environments MetaWorld and
Franka-Kitchen, respectively. Furthermore, we conducted real-world experiments,
showing that our policy, trained only with human demonstration, can be deployed
to various embodiments.",2024-09-16 06:46:08+00:00,"['Weiliang Tang', 'Jia-Hui Pan', 'Wei Zhan', 'Jianshu Zhou', 'Huaxiu Yao', 'Yun-Hui Liu', 'Masayoshi Tomizuka', 'Mingyu Ding', 'Chi-Wing Fu']",http://arxiv.org/abs/2409.10032v1
Enhancing Video Transformers for Action Understanding with VLM-aided Training,"Owing to their ability to extract relevant spatio-temporal video embeddings,
Vision Transformers (ViTs) are currently the best performing models in video
action understanding. However, their generalization over domains or datasets is
somewhat limited. In contrast, Visual Language Models (VLMs) have demonstrated
exceptional generalization performance, but are currently unable to process
videos. Consequently, they cannot extract spatio-temporal patterns that are
crucial for action understanding. In this paper, we propose the Four-tiered
Prompts (FTP) framework that takes advantage of the complementary strengths of
ViTs and VLMs. We retain ViTs' strong spatio-temporal representation ability
but improve the visual encodings to be more comprehensive and general by
aligning them with VLM outputs. The FTP framework adds four feature processors
that focus on specific aspects of human action in videos: action category,
action components, action description, and context information. The VLMs are
only employed during training, and inference incurs a minimal computation cost.
Our approach consistently yields state-of-the-art performance. For instance, we
achieve remarkable top-1 accuracy of 93.8% on Kinetics-400 and 83.4% on
Something-Something V2, surpassing VideoMAEv2 by 2.8% and 2.6%, respectively.",2024-03-24 12:55:50+00:00,"['Hui Lu', 'Hu Jian', 'Ronald Poppe', 'Albert Ali Salah']",http://arxiv.org/abs/2403.16128v1
Investigating Event-Based Cameras for Video Frame Interpolation in Sports,"Slow-motion replays provide a thrilling perspective on pivotal moments within
sports games, offering a fresh and captivating visual experience. However,
capturing slow-motion footage typically demands high-tech, expensive cameras
and infrastructures. Deep learning Video Frame Interpolation (VFI) techniques
have emerged as a promising avenue, capable of generating high-speed footage
from regular camera feeds. Moreover, the utilization of event-based cameras has
recently gathered attention as they provide valuable motion information between
frames, further enhancing the VFI performances. In this work, we present a
first investigation of event-based VFI models for generating sports slow-motion
videos. Particularly, we design and implement a bi-camera recording setup,
including an RGB and an event-based camera to capture sports videos, to
temporally align and spatially register both cameras. Our experimental
validation demonstrates that TimeLens, an off-the-shelf event-based VFI model,
can effectively generate slow-motion footage for sports videos. This first
investigation underscores the practical utility of event-based cameras in
producing sports slow-motion content and lays the groundwork for future
research endeavors in this domain.",2024-07-02 15:39:08+00:00,"['Antoine Deckyvere', 'Anthony Cioppa', 'Silvio Giancola', 'Bernard Ghanem', 'Marc Van Droogenbroeck']",http://arxiv.org/abs/2407.02370v2
HeadsetOff: Enabling Photorealistic Video Conferencing on Economical VR Headsets,"Virtual Reality (VR) has become increasingly popular for remote
collaboration, but video conferencing poses challenges when the user's face is
covered by the headset. Existing solutions have limitations in terms of
accessibility. In this paper, we propose HeadsetOff, a novel system that
achieves photorealistic video conferencing on economical VR headsets by
leveraging voice-driven face reconstruction. HeadsetOff consists of three main
components: a multimodal predictor, a generator, and an adaptive controller.
The predictor effectively predicts user future behavior based on different
modalities. The generator employs voice, head motion, and eye blink to animate
the human face. The adaptive controller dynamically selects the appropriate
generator model based on the trade-off between video quality and delay.
Experimental results demonstrate the effectiveness of HeadsetOff in achieving
high-quality, low-latency video conferencing on economical VR headsets.",2024-07-29 13:20:22+00:00,"['Yili Jin', 'Xize Duan', 'Fangxin Wang', 'Xue Liu']",http://arxiv.org/abs/2407.19988v2
Self-Enhancing Video Data Management System for Compositional Events with Large Language Models [Technical Report],"Complex video queries can be answered by decomposing them into modular
subtasks. However, existing video data management systems assume the existence
of predefined modules for each subtask. We introduce VOCAL-UDF, a novel
self-enhancing system that supports compositional queries over videos without
the need for predefined modules. VOCAL-UDF automatically identifies and
constructs missing modules and encapsulates them as user-defined functions
(UDFs), thus expanding its querying capabilities. To achieve this, we formulate
a unified UDF model that leverages large language models (LLMs) to aid in new
UDF generation. VOCAL-UDF handles a wide range of concepts by supporting both
program-based UDFs (i.e., Python functions generated by LLMs) and
distilled-model UDFs (lightweight vision models distilled from strong
pretrained models). To resolve the inherent ambiguity in user intent, VOCAL-UDF
generates multiple candidate UDFs and uses active learning to efficiently
select the best one. With the self-enhancing capability, VOCAL-UDF
significantly improves query performance across three video datasets.",2024-08-05 05:27:52+00:00,"['Enhao Zhang', 'Nicole Sullivan', 'Brandon Haynes', 'Ranjay Krishna', 'Magdalena Balazinska']",http://arxiv.org/abs/2408.02243v2
Physical Informed Driving World Model,"Autonomous driving requires robust perception models trained on high-quality,
large-scale multi-view driving videos for tasks like 3D object detection,
segmentation and trajectory prediction. While world models provide a
cost-effective solution for generating realistic driving videos, challenges
remain in ensuring these videos adhere to fundamental physical principles, such
as relative and absolute motion, spatial relationship like occlusion and
spatial consistency, and temporal consistency. To address these, we propose
DrivePhysica, an innovative model designed to generate realistic multi-view
driving videos that accurately adhere to essential physical principles through
three key advancements: (1) a Coordinate System Aligner module that integrates
relative and absolute motion features to enhance motion interpretation, (2) an
Instance Flow Guidance module that ensures precise temporal consistency via
efficient 3D flow extraction, and (3) a Box Coordinate Guidance module that
improves spatial relationship understanding and accurately resolves occlusion
hierarchies. Grounded in physical principles, we achieve state-of-the-art
performance in driving video generation quality (3.96 FID and 38.06 FVD on the
Nuscenes dataset) and downstream perception tasks. Our project homepage:
https://metadrivescape.github.io/papers_project/DrivePhysica/page.html",2024-12-11 14:29:35+00:00,"['Zhuoran Yang', 'Xi Guo', 'Chenjing Ding', 'Chiyu Wang', 'Wei Wu']",http://arxiv.org/abs/2412.08410v2
Future Research Avenues for Artificial Intelligence in Digital Gaming: An Exploratory Report,"Video games are a natural and synergistic application domain for artificial
intelligence (AI) systems, offering both the potential to enhance player
experience and immersion, as well as providing valuable benchmarks and virtual
environments to advance AI technologies in general. This report presents a
high-level overview of five promising research pathways for applying
state-of-the-art AI methods, particularly deep learning, to digital gaming
within the context of the current research landscape. The objective of this
work is to outline a curated, non-exhaustive list of encouraging research
directions at the intersection of AI and video games that may serve to inspire
more rigorous and comprehensive research efforts in the future. We discuss (i)
investigating large language models as core engines for game agent modelling,
(ii) using neural cellular automata for procedural game content generation,
(iii) accelerating computationally expensive in-game simulations via deep
surrogate modelling, (iv) leveraging self-supervised learning to obtain useful
video game state embeddings, and (v) training generative models of interactive
worlds using unlabelled video data. We also briefly address current technical
challenges associated with the integration of advanced deep learning systems
into video game development, and indicate key areas where further progress is
likely to be beneficial.",2024-12-18 17:32:27+00:00,['Markus Dablander'],http://arxiv.org/abs/2412.14085v1
ParaGAN: A Scalable Distributed Training Framework for Generative Adversarial Networks,"Recent advances in Generative Artificial Intelligence have fueled numerous
applications, particularly those involving Generative Adversarial Networks
(GANs), which are essential for synthesizing realistic photos and videos.
However, efficiently training GANs remains a critical challenge due to their
computationally intensive and numerically unstable nature. Existing methods
often require days or even weeks for training, posing significant resource and
time constraints.
  In this work, we introduce ParaGAN, a scalable distributed GAN training
framework that leverages asynchronous training and an asymmetric optimization
policy to accelerate GAN training. ParaGAN employs a congestion-aware data
pipeline and hardware-aware layout transformation to enhance accelerator
utilization, resulting in over 30% improvements in throughput. With ParaGAN, we
reduce the training time of BigGAN from 15 days to 14 hours while achieving 91%
scaling efficiency. Additionally, ParaGAN enables unprecedented high-resolution
image generation using BigGAN.",2024-11-06 15:40:46+00:00,"['Ziji Shi', 'Jialin Li', 'Yang You']",http://arxiv.org/abs/2411.03999v1
Evaluating the Effectiveness of Attack-Agnostic Features for Morphing Attack Detection,"Morphing attacks have diversified significantly over the past years, with new
methods based on generative adversarial networks (GANs) and diffusion models
posing substantial threats to face recognition systems. Recent research has
demonstrated the effectiveness of features extracted from large vision models
pretrained on bonafide data only (attack-agnostic features) for detecting deep
generative images. Building on this, we investigate the potential of these
image representations for morphing attack detection (MAD). We develop
supervised detectors by training a simple binary linear SVM on the extracted
features and one-class detectors by modeling the distribution of bonafide
features with a Gaussian Mixture Model (GMM). Our method is evaluated across a
comprehensive set of attacks and various scenarios, including generalization to
unseen attacks, different source datasets, and print-scan data. Our results
indicate that attack-agnostic features can effectively detect morphing attacks,
outperforming traditional supervised and one-class detectors from the
literature in most scenarios. Additionally, we provide insights into the
strengths and limitations of each considered representation and discuss
potential future research directions to further enhance the robustness and
generalizability of our approach.",2024-10-22 08:27:43+00:00,"['Laurent Colbois', 'Sbastien Marcel']",http://arxiv.org/abs/2410.16802v1
DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and Precise Editing with Diffusion Models,"Generating and editing dynamic 3D head avatars are crucial tasks in virtual
reality and film production. However, existing methods often suffer from facial
distortions, inaccurate head movements, and limited fine-grained editing
capabilities. To address these challenges, we present DynamicAvatars, a dynamic
model that generates photorealistic, moving 3D head avatars from video clips
and parameters associated with facial positions and expressions. Our approach
enables precise editing through a novel prompt-based editing model, which
integrates user-provided prompts with guiding parameters derived from large
language models (LLMs). To achieve this, we propose a dual-tracking framework
based on Gaussian Splatting and introduce a prompt preprocessing module to
enhance editing stability. By incorporating a specialized GAN algorithm and
connecting it to our control module, which generates precise guiding parameters
from LLMs, we successfully address the limitations of existing methods.
Additionally, we develop a dynamic editing strategy that selectively utilizes
specific training datasets to improve the efficiency and adaptability of the
model for dynamic editing tasks.",2024-11-24 06:22:30+00:00,"['Yangyang Qian', 'Yuan Sun', 'Yu Guo']",http://arxiv.org/abs/2411.15732v1
Video-to-Audio Generation with Hidden Alignment,"Generating semantically and temporally aligned audio content in accordance
with video input has become a focal point for researchers, particularly
following the remarkable breakthrough in text-to-video generation. In this
work, we aim to offer insights into the video-to-audio generation paradigm,
focusing on three crucial aspects: vision encoders, auxiliary embeddings, and
data augmentation techniques. Beginning with a foundational model built on a
simple yet surprisingly effective intuition, we explore various vision encoders
and auxiliary embeddings through ablation studies. Employing a comprehensive
evaluation pipeline that emphasizes generation quality and video-audio
synchronization alignment, we demonstrate that our model exhibits
state-of-the-art video-to-audio generation capabilities. Furthermore, we
provide critical insights into the impact of different data augmentation
methods on enhancing the generation framework's overall capacity. We showcase
possibilities to advance the challenge of generating synchronized audio from
semantic and temporal perspectives. We hope these insights will serve as a
stepping stone toward developing more realistic and accurate audio-visual
generation models.",2024-07-10 08:40:39+00:00,"['Manjie Xu', 'Chenxing Li', 'Xinyi Tu', 'Yong Ren', 'Rilin Chen', 'Yu Gu', 'Wei Liang', 'Dong Yu']",http://arxiv.org/abs/2407.07464v3
NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics Evaluation,"Neural radiance fields (NeRF) are a groundbreaking computer vision technology
that enables the generation of high-quality, immersive visual content from
multiple viewpoints. This capability has significant advantages for
applications such as virtual/augmented reality, 3D modelling, and content
creation for the film and entertainment industry. However, the evaluation of
NeRF methods poses several challenges, including a lack of comprehensive
datasets, reliable assessment methodologies, and objective quality metrics.
This paper addresses the problem of NeRF view synthesis (NVS) quality
assessment thoroughly, by conducting a rigorous subjective quality assessment
test that considers several scene classes and recently proposed NVS methods.
Additionally, the performance of a wide range of state-of-the-art conventional
and learning-based full-reference 2D image and video quality assessment metrics
is evaluated against the subjective scores of the subjective study. This study
found that errors in camera pose estimation can result in spatial misalignments
between synthesized and reference images, which need to be corrected before
applying an objective quality metric. The experimental results are analyzed in
depth, providing a comparative evaluation of several NVS methods and objective
quality metrics, across different classes of visual scenes, including real and
synthetic content for front-face and 360-degree camera trajectories.",2024-05-30 14:08:09+00:00,"['Pedro Martin', 'Antonio Rodrigues', 'Joao Ascenso', 'Maria Paula Queluz']",http://arxiv.org/abs/2405.20078v3
Choroidal thinning assessment through facial video analysis,"Different features of skin are associated with various medical conditions and
provide opportunities to evaluate and monitor body health. This study created a
strategy to assess choroidal thinning through the video analysis of facial
skin. Videos capturing the entire facial skin were collected from 48
participants with age-related macular degeneration (AMD) and 12 healthy
individuals. These facial videos were analyzed using video-based
trans-angiosomes imaging photoplethysmography (TaiPPG) to generate facial
imaging biomarkers that were correlated with choroidal thickness (CT)
measurements. The CT of all patients was determined using swept-source optical
coherence tomography (SS-OCT). The results revealed the relationship between
relative blood pulsation amplitude (BPA) in three typical facial angiosomes
(cheek, side-forehead and mid-forehead) and the average macular CT (r = 0.48, p
< 0.001; r = -0.56, p < 0.001; r = -0.40, p < 0.01). When considering a
diagnostic threshold of 200{\mu}m, the newly developed facial video analysis
tool effectively distinguished between cases of choroidal thinning and normal
cases, yielding areas under the curve of 0.75, 0.79 and 0.69. These findings
shed light on the connection between choroidal blood flow and facial skin
hemodynamics, which suggests the potential for predicting vascular diseases
through widely accessible skin imaging data.",2024-01-29 09:13:12+00:00,"['Qinghua He', 'Yi Zhang', 'Mengxi Shen', 'Giovanni Gregori', 'Philip J. Rosenfeld', 'Ruikang K. Wang']",http://arxiv.org/abs/2401.15984v1
Slot-VLM: SlowFast Slots for Video-Language Modeling,"Video-Language Models (VLMs), powered by the advancements in Large Language
Models (LLMs), are charting new frontiers in video understanding. A pivotal
challenge is the development of an efficient method to encapsulate video
content into a set of representative tokens to align with LLMs. In this work,
we introduce Slot-VLM, a novel framework designed to generate semantically
decomposed video tokens, in terms of object-wise and event-wise visual
representations, to facilitate LLM inference. Particularly, we design a
SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense
video tokens from the CLIP vision encoder to a set of representative slots. In
order to take into account both the spatial object details and the varied
temporal dynamics, SF-Slots is built with a dual-branch structure. The
Slow-Slots branch focuses on extracting object-centric slots from features at
high spatial resolution but low (slow) frame sample rate, emphasizing detailed
object information. Conversely, Fast-Slots branch is engineered to learn
event-centric slots from high temporal sample rate but low spatial resolution
features. These complementary slots are combined to form the vision context,
serving as the input to the LLM for efficient question answering. Our
experimental results demonstrate the effectiveness of our Slot-VLM, which
achieves the state-of-the-art performance on video question-answering.",2024-02-20 15:30:09+00:00,"['Jiaqi Xu', 'Cuiling Lan', 'Wenxuan Xie', 'Xuejin Chen', 'Yan Lu']",http://arxiv.org/abs/2402.13088v1
Boosting Neural Representations for Videos with a Conditional Decoder,"Implicit neural representations (INRs) have emerged as a promising approach
for video storage and processing, showing remarkable versatility across various
video tasks. However, existing methods often fail to fully leverage their
representation capabilities, primarily due to inadequate alignment of
intermediate features during target frame decoding. This paper introduces a
universal boosting framework for current implicit video representation
approaches. Specifically, we utilize a conditional decoder with a
temporal-aware affine transform module, which uses the frame index as a prior
condition to effectively align intermediate features with target frames.
Besides, we introduce a sinusoidal NeRV-like block to generate diverse
intermediate features and achieve a more balanced parameter distribution,
thereby enhancing the model's capacity. With a high-frequency
information-preserving reconstruction loss, our approach successfully boosts
multiple baseline INRs in the reconstruction quality and convergence speed for
video regression, and exhibits superior inpainting and interpolation results.
Further, we integrate a consistent entropy minimization technique and develop
video codecs based on these boosted INRs. Experiments on the UVG dataset
confirm that our enhanced codecs significantly outperform baseline INRs and
offer competitive rate-distortion performance compared to traditional and
learning-based codecs. Code is available at
https://github.com/Xinjie-Q/Boosting-NeRV.",2024-02-28 08:32:19+00:00,"['Xinjie Zhang', 'Ren Yang', 'Dailan He', 'Xingtong Ge', 'Tongda Xu', 'Yan Wang', 'Hongwei Qin', 'Jun Zhang']",http://arxiv.org/abs/2402.18152v3
LITA: Language Instructed Temporal-Localization Assistant,"There has been tremendous progress in multimodal Large Language Models
(LLMs). Recent works have extended these models to video input with promising
instruction following capabilities. However, an important missing piece is
temporal localization. These models cannot accurately answer the ""When?""
questions. We identify three key aspects that limit their temporal localization
capabilities: (i) time representation, (ii) architecture, and (iii) data. We
address these shortcomings by proposing Language Instructed
Temporal-Localization Assistant (LITA) with the following features: (1) We
introduce time tokens that encode timestamps relative to the video length to
better represent time in videos. (2) We introduce SlowFast tokens in the
architecture to capture temporal information at fine temporal resolution. (3)
We emphasize temporal localization data for LITA. In addition to leveraging
existing video datasets with timestamps, we propose a new task, Reasoning
Temporal Localization (RTL), along with the dataset, ActivityNet-RTL, for
learning and evaluating this task. Reasoning temporal localization requires
both the reasoning and temporal localization of Video LLMs. LITA demonstrates
strong performance on this challenging task, nearly doubling the temporal mean
intersection-over-union (mIoU) of baselines. In addition, we show that our
emphasis on temporal localization also substantially improves video-based text
generation compared to existing Video LLMs, including a 36% relative
improvement of Temporal Understanding. Code is available at:
https://github.com/NVlabs/LITA",2024-03-27 22:50:48+00:00,"['De-An Huang', 'Shijia Liao', 'Subhashree Radhakrishnan', 'Hongxu Yin', 'Pavlo Molchanov', 'Zhiding Yu', 'Jan Kautz']",http://arxiv.org/abs/2403.19046v1
Arena: A Patch-of-Interest ViT Inference Acceleration System for Edge-Assisted Video Analytics,"The advent of edge computing has made real-time intelligent video analytics
feasible. Previous works, based on traditional model architecture (e.g., CNN,
RNN, etc.), employ various strategies to filter out non-region-of-interest
content to minimize bandwidth and computation consumption but show inferior
performance in adverse environments. Recently, visual foundation models based
on transformers have shown great performance in adverse environments due to
their amazing generalization capability. However, they require a large amount
of computation power, which limits their applications in real-time intelligent
video analytics. In this paper, we find visual foundation models like Vision
Transformer (ViT) also have a dedicated acceleration mechanism for video
analytics. To this end, we introduce Arena, an end-to-end edge-assisted video
inference acceleration system based on ViT. We leverage the capability of ViT
that can be accelerated through token pruning by only offloading and feeding
Patches-of-Interest to the downstream models. Additionally, we design an
adaptive keyframe inference switching algorithm tailored to different videos,
capable of adapting to the current video content to jointly optimize accuracy
and bandwidth. Through extensive experiments, our findings reveal that Arena
can boost inference speeds by up to 1.58\(\times\) and 1.82\(\times\) on
average while consuming only 47\% and 31\% of the bandwidth, respectively, all
with high inference accuracy.",2024-04-14 13:14:13+00:00,"['Haosong Peng', 'Wei Feng', 'Hao Li', 'Yufeng Zhan', 'Ren Jin', 'Yuanqing Xia']",http://arxiv.org/abs/2404.09245v2
Group-aware Parameter-efficient Updating for Content-Adaptive Neural Video Compression,"Content-adaptive compression is crucial for enhancing the adaptability of the
pre-trained neural codec for various contents. Although these methods have been
very practical in neural image compression (NIC), their application in neural
video compression (NVC) is still limited due to two main aspects: 1), video
compression relies heavily on temporal redundancy, therefore updating just one
or a few frames can lead to significant errors accumulating over time; 2), NVC
frameworks are generally more complex, with many large components that are not
easy to update quickly during encoding. To address the previously mentioned
challenges, we have developed a content-adaptive NVC technique called
Group-aware Parameter-Efficient Updating (GPU). Initially, to minimize error
accumulation, we adopt a group-aware approach for updating encoder parameters.
This involves adopting a patch-based Group of Pictures (GoP) training strategy
to segment a video into patch-based GoPs, which will be updated to facilitate a
globally optimized domain-transferable solution. Subsequently, we introduce a
parameter-efficient delta-tuning strategy, which is achieved by integrating
several light-weight adapters into each coding component of the encoding
process by both serial and parallel configuration. Such architecture-agnostic
modules stimulate the components with large parameters, thereby reducing both
the update cost and the encoding time. We incorporate our GPU into the latest
NVC framework and conduct comprehensive experiments, whose results showcase
outstanding video compression efficiency across four video benchmarks and
adaptability of one medical image benchmark.",2024-05-07 12:42:23+00:00,"['Zhenghao Chen', 'Luping Zhou', 'Zhihao Hu', 'Dong Xu']",http://arxiv.org/abs/2405.04274v2
ToxVidLM: A Multimodal Framework for Toxicity Detection in Code-Mixed Videos,"In an era of rapidly evolving internet technology, the surge in multimodal
content, including videos, has expanded the horizons of online communication.
However, the detection of toxic content in this diverse landscape, particularly
in low-resource code-mixed languages, remains a critical challenge. While
substantial research has addressed toxic content detection in textual data, the
realm of video content, especially in non-English languages, has been
relatively underexplored. This paper addresses this research gap by introducing
a benchmark dataset, the first of its kind, consisting of 931 videos with 4021
code-mixed Hindi-English utterances collected from YouTube. Each utterance
within this dataset has been meticulously annotated for toxicity, severity, and
sentiment labels. We have developed an advanced Multimodal Multitask framework
built for Toxicity detection in Video Content by leveraging Language Models
(LMs), crafted for the primary objective along with the additional tasks of
conducting sentiment and severity analysis. ToxVidLM incorporates three key
modules - the Encoder module, Cross-Modal Synchronization module, and Multitask
module - crafting a generic multimodal LM customized for intricate video
classification tasks. Our experiments reveal that incorporating multiple
modalities from the videos substantially enhances the performance of toxic
content detection by achieving an Accuracy and Weighted F1 score of 94.29% and
94.35%, respectively.",2024-05-31 05:40:56+00:00,"['Krishanu Maity', 'A. S. Poornash', 'Sriparna Saha', 'Pushpak Bhattacharyya']",http://arxiv.org/abs/2405.20628v2
Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization,"Video is an increasingly prominent and information-dense medium, yet it poses
substantial challenges for language models. A typical video consists of a
sequence of shorter segments, or shots, that collectively form a coherent
narrative. Each shot is analogous to a word in a sentence where multiple data
streams of information (such as visual and auditory data) must be processed
simultaneously. Comprehension of the entire video requires not only
understanding the visual-audio information of each shot but also requires that
the model links the ideas between each shot to generate a larger,
all-encompassing story. Despite significant progress in the field, current
works often overlook videos' more granular shot-by-shot semantic information.
In this project, we propose a family of efficient large language vision models
(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By
leveraging better pretraining and data collection strategies, we extend the
abilities of existing small LLVMs from being able to understand a picture to
being able to understand a sequence of frames. Specifically, we show that
Shotluck Holmes achieves better performance than state-of-the-art results on
the Shot2Story video captioning and summary task with significantly smaller and
more computationally efficient models.",2024-05-31 07:30:24+00:00,"['Richard Luo', 'Austin Peng', 'Adithya Vasudev', 'Rishabh Jain']",http://arxiv.org/abs/2405.20648v2
Thoracic Surgery Video Analysis for Surgical Phase Recognition,"This paper presents an approach for surgical phase recognition using video
data, aiming to provide a comprehensive understanding of surgical procedures
for automated workflow analysis. The advent of robotic surgery, digitized
operating rooms, and the generation of vast amounts of data have opened doors
for the application of machine learning and computer vision in the analysis of
surgical videos. Among these advancements, Surgical Phase Recognition(SPR)
stands out as an emerging technology that has the potential to recognize and
assess the ongoing surgical scenario, summarize the surgery, evaluate surgical
skills, offer surgical decision support, and facilitate medical training. In
this paper, we analyse and evaluate both frame-based and video clipping-based
phase recognition on thoracic surgery dataset consisting of 11 classes of
phases. Specifically, we utilize ImageNet ViT for image-based classification
and VideoMAE as the baseline model for video-based classification. We show that
Masked Video Distillation(MVD) exhibits superior performance, achieving a top-1
accuracy of 72.9%, compared to 52.31% achieved by ImageNet ViT. These findings
underscore the efficacy of video-based classifiers over their image-based
counterparts in surgical phase recognition tasks.",2024-06-13 14:47:57+00:00,"['Syed Abdul Mateen', 'Niharika Malvia', 'Syed Abdul Khader', 'Danny Wang', 'Deepti Srinivasan', 'Chi-Fu Jeffrey Yang', 'Lana Schumacher', 'Sandeep Manjanna']",http://arxiv.org/abs/2406.09185v1
SSTFB: Leveraging self-supervised pretext learning and temporal self-attention with feature branching for real-time video polyp segmentation,"Polyps are early cancer indicators, so assessing occurrences of polyps and
their removal is critical. They are observed through a colonoscopy screening
procedure that generates a stream of video frames. Segmenting polyps in their
natural video screening procedure has several challenges, such as the
co-existence of imaging artefacts, motion blur, and floating debris. Most
existing polyp segmentation algorithms are developed on curated still image
datasets that do not represent real-world colonoscopy. Their performance often
degrades on video data. We propose a video polyp segmentation method that
performs self-supervised learning as an auxiliary task and a spatial-temporal
self-attention mechanism for improved representation learning. Our end-to-end
configuration and joint optimisation of losses enable the network to learn more
discriminative contextual features in videos. Our experimental results
demonstrate an improvement with respect to several state-of-the-art (SOTA)
methods. Our ablation study also confirms that the choice of the proposed joint
end-to-end training improves network accuracy by over 3% and nearly 10% on both
the Dice similarity coefficient and intersection-over-union compared to the
recently proposed method PNS+ and Polyp-PVT, respectively. Results on
previously unseen video data indicate that the proposed method generalises.",2024-06-14 17:33:11+00:00,"['Ziang Xu', 'Jens Rittscher', 'Sharib Ali']",http://arxiv.org/abs/2406.10200v1
AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding,"AI personal assistants deployed via robots or wearables require embodied
understanding to collaborate with humans effectively. However, current
Vision-Language Models (VLMs) primarily focus on third-person view videos,
neglecting the richness of egocentric perceptual experience. To address this
gap, we propose three key contributions. First, we introduce the Egocentric
Video Understanding Dataset (EVUD) for training VLMs on video captioning and
question answering tasks specific to egocentric videos. Second, we present
AlanaVLM, a 7B parameter VLM trained using parameter-efficient methods on EVUD.
Finally, we evaluate AlanaVLM's capabilities on OpenEQA, a challenging
benchmark for embodied video question answering. Our model achieves
state-of-the-art performance, outperforming open-source models including strong
Socratic models using GPT-4 as a planner by 3.6%. Additionally, we outperform
Claude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared to
Gemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning.
This research paves the way for building efficient VLMs that can be deployed in
robots or wearables, leveraging embodied video understanding to collaborate
seamlessly with humans in everyday tasks, contributing to the next generation
of Embodied AI.",2024-06-19 20:14:14+00:00,"['Alessandro Suglia', 'Claudio Greco', 'Katie Baker', 'Jose L. Part', 'Ioannis Papaioannou', 'Arash Eshghi', 'Ioannis Konstas', 'Oliver Lemon']",http://arxiv.org/abs/2406.13807v2
Harnessing LLMs for Automated Video Content Analysis: An Exploratory Workflow of Short Videos on Depression,"Despite the growing interest in leveraging Large Language Models (LLMs) for
content analysis, current studies have primarily focused on text-based content.
In the present work, we explored the potential of LLMs in assisting video
content analysis by conducting a case study that followed a new workflow of
LLM-assisted multimodal content analysis. The workflow encompasses codebook
design, prompt engineering, LLM processing, and human evaluation. We
strategically crafted annotation prompts to get LLM Annotations in structured
form and explanation prompts to generate LLM Explanations for a better
understanding of LLM reasoning and transparency. To test LLM's video annotation
capabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos
about depression. We compared the LLM Annotations with those of two human
coders and found that LLM has higher accuracy in object and activity
Annotations than emotion and genre Annotations. Moreover, we identified the
potential and limitations of LLM's capabilities in annotating videos. Based on
the findings, we explore opportunities and challenges for future research and
improvements to the workflow. We also discuss ethical concerns surrounding
future studies based on LLM-assisted video analysis.",2024-06-27 21:03:56+00:00,"['Jiaying Lizzy Liu', 'Yunlong Wang', 'Yao Lyu', 'Yiheng Su', 'Shuo Niu', 'Xuhai Orson Xu', 'Yan Zhang']",http://arxiv.org/abs/2406.19528v3
Open Vocabulary Multi-Label Video Classification,"Pre-trained vision-language models (VLMs) have enabled significant progress
in open vocabulary computer vision tasks such as image classification, object
detection and image segmentation. Some recent works have focused on extending
VLMs to open vocabulary single label action classification in videos. However,
previous methods fall short in holistic video understanding which requires the
ability to simultaneously recognize multiple actions and entities e.g., objects
in the video in an open vocabulary setting. We formulate this problem as open
vocabulary multilabel video classification and propose a method to adapt a
pre-trained VLM such as CLIP to solve this task. We leverage large language
models (LLMs) to provide semantic guidance to the VLM about class labels to
improve its open vocabulary performance with two key contributions. First, we
propose an end-to-end trainable architecture that learns to prompt an LLM to
generate soft attributes for the CLIP text-encoder to enable it to recognize
novel classes. Second, we integrate a temporal modeling module into CLIP's
vision encoder to effectively model the spatio-temporal dynamics of video
concepts as well as propose a novel regularized finetuning technique to ensure
strong open vocabulary classification performance in the video domain. Our
extensive experimentation showcases the efficacy of our approach on multiple
benchmark datasets.",2024-07-12 07:53:54+00:00,"['Rohit Gupta', 'Mamshad Nayeem Rizve', 'Jayakrishnan Unnikrishnan', 'Ashish Tawari', 'Son Tran', 'Mubarak Shah', 'Benjamin Yao', 'Trishul Chilimbi']",http://arxiv.org/abs/2407.09073v1
LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding,"Large multimodal models (LMMs) are processing increasingly longer and richer
inputs. Albeit the progress, few public benchmark is available to measure such
development. To mitigate this gap, we introduce LongVideoBench, a
question-answering benchmark that features video-language interleaved inputs up
to an hour long. Our benchmark includes 3,763 varying-length web-collected
videos with their subtitles across diverse themes, designed to comprehensively
evaluate LMMs on long-term multimodal understanding. To achieve this, we
interpret the primary challenge as to accurately retrieve and reason over
detailed multimodal information from long inputs. As such, we formulate a novel
video question-answering task termed referring reasoning. Specifically, as part
of the question, it contains a referring query that references related video
contexts, called referred context. The model is then required to reason over
relevant video details from the referred context. Following the paradigm of
referring reasoning, we curate 6,678 human-annotated multiple-choice questions
in 17 fine-grained categories, establishing one of the most comprehensive
benchmarks for long-form video understanding. Evaluations suggest that the
LongVideoBench presents significant challenges even for the most advanced
proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their
open-source counterparts show an even larger performance gap. In addition, our
results indicate that model performance on the benchmark improves only when
they are capable of processing more frames, positioning LongVideoBench as a
valuable benchmark for evaluating future-generation long-context LMMs.",2024-07-22 16:00:55+00:00,"['Haoning Wu', 'Dongxu Li', 'Bei Chen', 'Junnan Li']",http://arxiv.org/abs/2407.15754v1
Accelerating Learned Video Compression via Low-Resolution Representation Learning,"In recent years, the field of learned video compression has witnessed rapid
advancement, exemplified by the latest neural video codecs DCVC-DC that has
outperformed the upcoming next-generation codec ECM in terms of compression
ratio. Despite this, learned video compression frameworks often exhibit low
encoding and decoding speeds primarily due to their increased computational
complexity and unnecessary high-resolution spatial operations, which hugely
hinder their applications in reality. In this work, we introduce an
efficiency-optimized framework for learned video compression that focuses on
low-resolution representation learning, aiming to significantly enhance the
encoding and decoding speeds. Firstly, we diminish the computational load by
reducing the resolution of inter-frame propagated features obtained from reused
features of decoded frames, including I-frames. We implement a joint training
strategy for both the I-frame and P-frame models, further improving the
compression ratio. Secondly, our approach efficiently leverages multi-frame
priors for parameter prediction, minimizing computation at the decoding end.
Thirdly, we revisit the application of the Online Encoder Update (OEU) strategy
for high-resolution sequences, achieving notable improvements in compression
ratio without compromising decoding efficiency. Our efficiency-optimized
framework has significantly improved the balance between compression ratio and
speed for learned video compression. In comparison to traditional codecs, our
method achieves performance levels on par with the low-decay P configuration of
the H.266 reference software VTM. Furthermore, when contrasted with DCVC-HEM,
our approach delivers a comparable compression ratio while boosting encoding
and decoding speeds by a factor of 3 and 7, respectively. On RTX 2080Ti, our
method can decode each 1080p frame under 100ms.",2024-07-23 12:02:57+00:00,"['Zidian Qiu', 'Zongyao He', 'Zhi Jin']",http://arxiv.org/abs/2407.16418v1
Performance and Non-adversarial Robustness of the Segment Anything Model 2 in Surgical Video Segmentation,"Fully supervised deep learning (DL) models for surgical video segmentation
have been shown to struggle with non-adversarial, real-world corruptions of
image quality including smoke, bleeding, and low illumination. Foundation
models for image segmentation, such as the segment anything model (SAM) that
focuses on interactive prompt-based segmentation, move away from semantic
classes and thus can be trained on larger and more diverse data, which offers
outstanding zero-shot generalization with appropriate user prompts. Recently,
building upon this success, SAM-2 has been proposed to further extend the
zero-shot interactive segmentation capabilities from independent frame-by-frame
to video segmentation. In this paper, we present a first experimental study
evaluating SAM-2's performance on surgical video data. Leveraging the
SegSTRONG-C MICCAI EndoVIS 2024 sub-challenge dataset, we assess SAM-2's
effectiveness on uncorrupted endoscopic sequences and evaluate its
non-adversarial robustness on videos with corrupted image quality simulating
smoke, bleeding, and low brightness conditions under various prompt strategies.
Our experiments demonstrate that SAM-2, in zero-shot manner, can achieve
competitive or even superior performance compared to fully-supervised deep
learning models on surgical video data, including under non-adversarial
corruptions of image quality. Additionally, SAM-2 consistently outperforms the
original SAM and its medical variants across all conditions. Finally,
frame-sparse prompting can consistently outperform frame-wise prompting for
SAM-2, suggesting that allowing SAM-2 to leverage its temporal modeling
capabilities leads to more coherent and accurate segmentation compared to
frequent prompting.",2024-08-07 21:33:07+00:00,"['Yiqing Shen', 'Hao Ding', 'Xinyuan Shao', 'Mathias Unberath']",http://arxiv.org/abs/2408.04098v2
High-Efficiency Neural Video Compression via Hierarchical Predictive Learning,"The enhanced Deep Hierarchical Video Compression-DHVC 2.0-has been
introduced. This single-model neural video codec operates across a broad range
of bitrates, delivering not only superior compression performance to
representative methods but also impressive complexity efficiency, enabling
real-time processing with a significantly smaller memory footprint on standard
GPUs. These remarkable advancements stem from the use of hierarchical
predictive coding. Each video frame is uniformly transformed into multiscale
representations through hierarchical variational autoencoders. For a specific
scale's feature representation of a frame, its corresponding latent residual
variables are generated by referencing lower-scale spatial features from the
same frame and then conditionally entropy-encoded using a probabilistic model
whose parameters are predicted using same-scale temporal reference from
previous frames and lower-scale spatial reference of the current frame. This
feature-space processing operates from the lowest to the highest scale of each
frame, completely eliminating the need for the complexity-intensive motion
estimation and compensation techniques that have been standard in video codecs
for decades. The hierarchical approach facilitates parallel processing,
accelerating both encoding and decoding, and supports transmission-friendly
progressive decoding, making it particularly advantageous for networked video
applications in the presence of packet loss. Source codes will be made
available.",2024-10-03 15:40:58+00:00,"['Ming Lu', 'Zhihao Duan', 'Wuyang Cong', 'Dandan Ding', 'Fengqing Zhu', 'Zhan Ma']",http://arxiv.org/abs/2410.02598v1
MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval,"Efficiently retrieving and synthesizing information from large-scale
multimodal collections has become a critical challenge. However, existing video
retrieval datasets suffer from scope limitations, primarily focusing on
matching descriptive but vague queries with small collections of professionally
edited, English-centric videos. To address this gap, we introduce
$\textbf{MultiVENT 2.0}$, a large-scale, multilingual event-centric video
retrieval benchmark featuring a collection of more than 218,000 news videos and
3,906 queries targeting specific world events. These queries specifically
target information found in the visual content, audio, embedded text, and text
metadata of the videos, requiring systems leverage all these sources to succeed
at the task. Preliminary results show that state-of-the-art vision-language
models struggle significantly with this task, and while alternative approaches
show promise, they are still insufficient to adequately address this problem.
These findings underscore the need for more robust multimodal retrieval
systems, as effective video retrieval is a crucial step towards multimodal
content understanding and generation.",2024-10-15 13:56:34+00:00,"['Reno Kriz', 'Kate Sanders', 'David Etter', 'Kenton Murray', 'Cameron Carpenter', 'Kelly Van Ochten', 'Hannah Recknor', 'Jimena Guallar-Blasco', 'Alexander Martin', 'Ronald Colaianni', 'Nolan King', 'Eugene Yang', 'Benjamin Van Durme']",http://arxiv.org/abs/2410.11619v2
When Spatial meets Temporal in Action Recognition,"Video action recognition has made significant strides, but challenges remain
in effectively using both spatial and temporal information. While existing
methods often focus on either spatial features (e.g., object appearance) or
temporal dynamics (e.g., motion), they rarely address the need for a
comprehensive integration of both. Capturing the rich temporal evolution of
video frames, while preserving their spatial details, is crucial for improving
accuracy. In this paper, we introduce the Temporal Integration and Motion
Enhancement (TIME) layer, a novel preprocessing technique designed to
incorporate temporal information. The TIME layer generates new video frames by
rearranging the original sequence, preserving temporal order while embedding
$N^2$ temporally evolving frames into a single spatial grid of size $N \times
N$. This transformation creates new frames that balance both spatial and
temporal information, making them compatible with existing video models. When
$N=1$, the layer captures rich spatial details, similar to existing methods. As
$N$ increases ($N\geq2$), temporal information becomes more prominent, while
the spatial information decreases to ensure compatibility with model inputs. We
demonstrate the effectiveness of the TIME layer by integrating it into popular
action recognition models, such as ResNet-50, Vision Transformer, and Video
Masked Autoencoders, for both RGB and depth video data. Our experiments show
that the TIME layer enhances recognition accuracy, offering valuable insights
for video processing tasks.",2024-11-22 16:39:45+00:00,"['Huilin Chen', 'Lei Wang', 'Yifan Chen', 'Tom Gedeon', 'Piotr Koniusz']",http://arxiv.org/abs/2411.15284v1
OphCLIP: Hierarchical Retrieval-Augmented Learning for Ophthalmic Surgical Video-Language Pretraining,"Surgical practice involves complex visual interpretation, procedural skills,
and advanced medical knowledge, making surgical vision-language pretraining
(VLP) particularly challenging due to this complexity and the limited
availability of annotated data. To address the gap, we propose OphCLIP, a
hierarchical retrieval-augmented vision-language pretraining framework
specifically designed for ophthalmic surgical workflow understanding. OphCLIP
leverages the OphVL dataset we constructed, a large-scale and comprehensive
collection of over 375K hierarchically structured video-text pairs with tens of
thousands of different combinations of attributes (surgeries,
phases/operations/actions, instruments, medications, as well as more advanced
aspects like the causes of eye diseases, surgical objectives, and postoperative
recovery recommendations, etc). These hierarchical video-text correspondences
enable OphCLIP to learn both fine-grained and long-term visual representations
by aligning short video clips with detailed narrative descriptions and full
videos with structured titles, capturing intricate surgical details and
high-level procedural insights, respectively. Our OphCLIP also designs a
retrieval-augmented pretraining framework to leverage the underexplored
large-scale silent surgical procedure videos, automatically retrieving
semantically relevant content to enhance the representation learning of
narrative videos. Evaluation across 11 datasets for phase recognition and
multi-instrument identification shows OphCLIP's robust generalization and
superior performance.",2024-11-23 02:53:08+00:00,"['Ming Hu', 'Kun Yuan', 'Yaling Shen', 'Feilong Tang', 'Xiaohao Xu', 'Lin Zhou', 'Wei Li', 'Ying Chen', 'Zhongxing Xu', 'Zelin Peng', 'Siyuan Yan', 'Vinkle Srivastav', 'Diping Song', 'Tianbin Li', 'Danli Shi', 'Jin Ye', 'Nicolas Padoy', 'Nassir Navab', 'Junjun He', 'Zongyuan Ge']",http://arxiv.org/abs/2411.15421v2
Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation,"Using generative models to synthesize new data has become a de-facto standard
in autonomous driving to address the data scarcity issue. Though existing
approaches are able to boost perception models, we discover that these
approaches fail to improve the performance of planning of end-to-end autonomous
driving models as the generated videos are usually less than 8 frames and the
spatial and temporal inconsistencies are not negligible. To this end, we
propose Delphi, a novel diffusion-based long video generation method with a
shared noise modeling mechanism across the multi-views to increase spatial
consistency, and a feature-aligned module to achieves both precise
controllability and temporal consistency. Our method can generate up to 40
frames of video without loss of consistency which is about 5 times longer
compared with state-of-the-art methods. Instead of randomly generating new
data, we further design a sampling policy to let Delphi generate new data that
are similar to those failure cases to improve the sample efficiency. This is
achieved by building a failure-case driven framework with the help of
pre-trained visual language models. Our extensive experiment demonstrates that
our Delphi generates a higher quality of long videos surpassing previous
state-of-the-art methods. Consequentially, with only generating 4% of the
training dataset size, our framework is able to go beyond perception and
prediction tasks, for the first time to the best of our knowledge, boost the
planning performance of the end-to-end autonomous driving model by a margin of
25%.",2024-06-03 14:13:13+00:00,"['Enhui Ma', 'Lijun Zhou', 'Tao Tang', 'Zhan Zhang', 'Dong Han', 'Junpeng Jiang', 'Kun Zhan', 'Peng Jia', 'Xianpeng Lang', 'Haiyang Sun', 'Di Lin', 'Kaicheng Yu']",http://arxiv.org/abs/2406.01349v3
Bi-Directional MS Lesion Filling and Synthesis Using Denoising Diffusion Implicit Model-based Lesion Repainting,"Automatic magnetic resonance (MR) image processing pipelines are widely used
to study people with multiple sclerosis (PwMS), encompassing tasks such as
lesion segmentation and brain parcellation. However, the presence of lesion
often complicates these analysis, particularly in brain parcellation. Lesion
filling is commonly used to mitigate this issue, but existing lesion filling
algorithms often fall short in accurately reconstructing realistic lesion-free
images, which are vital for consistent downstream analysis. Additionally, the
performance of lesion segmentation algorithms is often limited by insufficient
data with lesion delineation as training labels. In this paper, we propose a
novel approach leveraging Denoising Diffusion Implicit Models (DDIMs) for both
MS lesion filling and synthesis based on image inpainting. Our modified DDIM
architecture, once trained, enables both MS lesion filing and synthesis.
Specifically, it can generate lesion-free T1-weighted or FLAIR images from
those containing lesions; Or it can add lesions to T1-weighted or FLAIR images
of healthy subjects. The former is essential for downstream analyses that
require lesion-free images, while the latter is valuable for augmenting
training datasets for lesion segmentation tasks. We validate our approach
through initial experiments in this paper and demonstrate promising results in
both lesion filling and synthesis, paving the way for future work.",2024-10-07 13:27:41+00:00,"['Jinwei Zhang', 'Lianrui Zuo', 'Yihao Liu', 'Samuel Remedios', 'Bennett A. Landman', 'Jerry L. Prince', 'Aaron Carass']",http://arxiv.org/abs/2410.05027v1
A comparative study of generative adversarial networks for image recognition algorithms based on deep learning and traditional methods,"In this paper, an image recognition algorithm based on the combination of
deep learning and generative adversarial network (GAN) is studied, and compared
with traditional image recognition methods. The purpose of this study is to
evaluate the advantages and application prospects of deep learning technology,
especially GAN, in the field of image recognition. Firstly, this paper reviews
the basic principles and techniques of traditional image recognition methods,
including the classical algorithms based on feature extraction such as SIFT,
HOG and their combination with support vector machine (SVM), random forest, and
other classifiers. Then, the working principle, network structure, and unique
advantages of GAN in image generation and recognition are introduced. In order
to verify the effectiveness of GAN in image recognition, a series of
experiments are designed and carried out using multiple public image data sets
for training and testing. The experimental results show that compared with
traditional methods, GAN has excellent performance in processing complex
images, recognition accuracy, and anti-noise ability. Specifically, Gans are
better able to capture high-dimensional features and details of images,
significantly improving recognition performance. In addition, Gans shows unique
advantages in dealing with image noise, partial missing information, and
generating high-quality images.",2024-08-07 06:11:25+00:00,"['Yihao Zhong', 'Yijing Wei', 'Yingbin Liang', 'Xiqing Liu', 'Rongwei Ji', 'Yiru Cang']",http://arxiv.org/abs/2408.03568v1
Text-to-3D Gaussian Splatting with Physics-Grounded Motion Generation,"Text-to-3D generation is a valuable technology in virtual reality and digital
content creation. While recent works have pushed the boundaries of text-to-3D
generation, producing high-fidelity 3D objects with inefficient prompts and
simulating their physics-grounded motion accurately still remain unsolved
challenges. To address these challenges, we present an innovative framework
that utilizes the Large Language Model (LLM)-refined prompts and diffusion
priors-guided Gaussian Splatting (GS) for generating 3D models with accurate
appearances and geometric structures. We also incorporate a continuum
mechanics-based deformation map and color regularization to synthesize vivid
physics-grounded motion for the generated 3D Gaussians, adhering to the
conservation of mass and momentum. By integrating text-to-3D generation with
physics-grounded motion synthesis, our framework renders photo-realistic 3D
objects that exhibit physics-aware motion, accurately reflecting the behaviors
of the objects under various forces and constraints across different materials.
Extensive experiments demonstrate that our approach achieves high-quality 3D
generations with realistic physics-grounded motion.",2024-12-07 06:48:16+00:00,"['Wenqing Wang', 'Yun Fu']",http://arxiv.org/abs/2412.05560v1
Beyond Generation: Unlocking Universal Editing via Self-Supervised Fine-Tuning,"Recent advances in video generation have outpaced progress in video editing,
which remains constrained by several limiting factors, namely: (a) the task's
dependency on supervision severely limits generality, (b) an unnecessary
artificial separation between the generation and editing task, and (c) the high
computational costs of training a video model. In this work, we propose UES
(Unlocking Universal Editing via Self-Supervision), a lightweight
self-supervised fine-tuning strategy that transforms generation models into
unified generation-editing systems through self-supervised semantic alignment.
Our approach establishes a dual-conditioning mechanism where original
video-text pairs jointly provide visual and textual semantics, enabling
structured learning of intrinsic spatiotemporal correspondences. Key advantages
include: (i) Universality through supervision-free adaptation to diverse
editing tasks, (ii) Unification of generation and editing applicable to most
text(+image)-to-video model, and (iii) Efficiency via lightweight fine-tune
that reduces tunable parameters by 92.67%. To enable systematic evaluation, we
introduce OmniBench-99, a comprehensive benchmark spanning 99 videos across
humans/animals, environments, and objects, comprising 4 editing types and 8
scenarios. Extensive experiments show UES enables models without inherent
editing capability to perform powerful and universal editing while preserving
or even enhancing their original generation performance.",2024-12-03 03:10:19+00:00,"['Harold Haodong Chen', 'Harry Yang', 'Ser-Nam Lim']",http://arxiv.org/abs/2412.02114v2
Bi-KVIL: Keypoints-based Visual Imitation Learning of Bimanual Manipulation Tasks,"Visual imitation learning has achieved impressive progress in learning
unimanual manipulation tasks from a small set of visual observations, thanks to
the latest advances in computer vision. However, learning bimanual coordination
strategies and complex object relations from bimanual visual demonstrations, as
well as generalizing them to categorical objects in novel cluttered scenes
remain unsolved challenges. In this paper, we extend our previous work on
keypoints-based visual imitation learning (\mbox{K-VIL})~\cite{gao_kvil_2023}
to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called
\emph{Hybrid Master-Slave Relationships} (HMSR) among objects and hands,
bimanual coordination strategies, and sub-symbolic task representations. Our
bimanual task representation is object-centric, embodiment-independent, and
viewpoint-invariant, thus generalizing well to categorical objects in novel
scenes. We evaluate our approach in various real-world applications, showcasing
its ability to learn fine-grained bimanual manipulation tasks from a small
number of human demonstration videos. Videos and source code are available at
https://sites.google.com/view/bi-kvil.",2024-03-05 19:11:17+00:00,"['Jianfeng Gao', 'Xiaoshu Jin', 'Franziska Krebs', 'Nomie Jaquier', 'Tamim Asfour']",http://arxiv.org/abs/2403.03270v2
LN-Gen: Rectal Lymph Nodes Generation via Anatomical Features,"Accurate segmentation of rectal lymph nodes is crucial for the staging and
treatment planning of rectal cancer. However, the complexity of the surrounding
anatomical structures and the scarcity of annotated data pose significant
challenges. This study introduces a novel lymph node synthesis technique aimed
at generating diverse and realistic synthetic rectal lymph node samples to
mitigate the reliance on manual annotation. Unlike direct diffusion methods,
which often produce masks that are discontinuous and of suboptimal quality, our
approach leverages an implicit SDF-based method for mask generation, ensuring
the production of continuous, stable, and morphologically diverse masks.
Experimental results demonstrate that our synthetic data significantly improves
segmentation performance. Our work highlights the potential of diffusion model
for accurately synthesizing structurally complex lesions, such as lymph nodes
in rectal cancer, alleviating the challenge of limited annotated data in this
field and aiding in advancements in rectal cancer diagnosis and treatment.",2024-08-27 11:40:23+00:00,"['Weidong Guo', 'Hantao Zhang', 'Shouhong Wan', 'Bingbing Zou', 'Wanqin Wang', 'Peiquan Jin']",http://arxiv.org/abs/2408.14977v1
DRDM: A Disentangled Representations Diffusion Model for Synthesizing Realistic Person Images,"Person image synthesis with controllable body poses and appearances is an
essential task owing to the practical needs in the context of virtual try-on,
image editing and video production. However, existing methods face significant
challenges with details missing, limbs distortion and the garment style
deviation. To address these issues, we propose a Disentangled Representations
Diffusion Model (DRDM) to generate photo-realistic images from source portraits
in specific desired poses and appearances. First, a pose encoder is responsible
for encoding pose features into a high-dimensional space to guide the
generation of person images. Second, a body-part subspace decoupling block
(BSDB) disentangles features from the different body parts of a source figure
and feeds them to the various layers of the noise prediction block, thereby
supplying the network with rich disentangled features for generating a
realistic target image. Moreover, during inference, we develop a parsing
map-based disentangled classifier-free guided sampling method, which amplifies
the conditional signals of texture and pose. Extensive experimental results on
the Deepfashion dataset demonstrate the effectiveness of our approach in
achieving pose transfer and appearance control.",2024-12-25 06:36:24+00:00,"['Enbo Huang', 'Yuan Zhang', 'Faliang Huang', 'Guangyu Zhang', 'Yang Liu']",http://arxiv.org/abs/2412.18797v1
G3R: Generating Rich and Fine-grained mmWave Radar Data from 2D Videos for Generalized Gesture Recognition,"Millimeter wave radar is gaining traction recently as a promising modality
for enabling pervasive and privacy-preserving gesture recognition. However, the
lack of rich and fine-grained radar datasets hinders progress in developing
generalized deep learning models for gesture recognition across various user
postures (e.g., standing, sitting), positions, and scenes. To remedy this, we
resort to designing a software pipeline that exploits wealthy 2D videos to
generate realistic radar data, but it needs to address the challenge of
simulating diversified and fine-grained reflection properties of user gestures.
To this end, we design G3R with three key components: (i) a gesture reflection
point generator expands the arm's skeleton points to form human reflection
points; (ii) a signal simulation model simulates the multipath reflection and
attenuation of radar signals to output the human intensity map; (iii) an
encoder-decoder model combines a sampling module and a fitting module to
address the differences in number and distribution of points between generated
and real-world radar data for generating realistic radar data. We implement and
evaluate G3R using 2D videos from public data sources and self-collected
real-world radar data, demonstrating its superiority over other
state-of-the-art approaches for gesture recognition.",2024-04-23 11:22:59+00:00,"['Kaikai Deng', 'Dong Zhao', 'Wenxin Zheng', 'Yue Ling', 'Kangwen Yin', 'Huadong Ma']",http://arxiv.org/abs/2404.14934v1
Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents,"Scene simulation in autonomous driving has gained significant attention
because of its huge potential for generating customized data. However, existing
editable scene simulation approaches face limitations in terms of user
interaction efficiency, multi-camera photo-realistic rendering and external
digital assets integration. To address these challenges, this paper introduces
ChatSim, the first system that enables editable photo-realistic 3D driving
scene simulations via natural language commands with external digital assets.
To enable editing with high command flexibility,~ChatSim leverages a large
language model (LLM) agent collaboration framework. To generate photo-realistic
outcomes, ChatSim employs a novel multi-camera neural radiance field method.
Furthermore, to unleash the potential of extensive high-quality digital assets,
ChatSim employs a novel multi-camera lighting estimation method to achieve
scene-consistent assets' rendering. Our experiments on Waymo Open Dataset
demonstrate that ChatSim can handle complex language commands and generate
corresponding photo-realistic scene videos.",2024-02-08 15:26:28+00:00,"['Yuxi Wei', 'Zi Wang', 'Yifan Lu', 'Chenxin Xu', 'Changxing Liu', 'Hao Zhao', 'Siheng Chen', 'Yanfeng Wang']",http://arxiv.org/abs/2402.05746v3
"T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design","In this paper, we focus on enhancing a diffusion-based text-to-video (T2V)
model during the post-training phase by distilling a highly capable consistency
model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2,
introduces a significant advancement by integrating various supervision
signals, including high-quality training data, reward model feedback, and
conditional guidance, into the consistency distillation process. Through
comprehensive ablation studies, we highlight the crucial importance of
tailoring datasets to specific learning objectives and the effectiveness of
learning from diverse reward models for enhancing both the visual quality and
text-video alignment. Additionally, we highlight the vast design space of
conditional guidance strategies, which centers on designing an effective energy
function to augment the teacher ODE solver. We demonstrate the potential of
this approach by extracting motion guidance from the training datasets and
incorporating it into the ODE solver, showcasing its effectiveness in improving
the motion quality of the generated videos with the improved motion-related
metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2
establishes a new state-of-the-art result on VBench, with a Total score of
85.13, surpassing proprietary systems such as Gen-3 and Kling.",2024-10-08 04:30:06+00:00,"['Jiachen Li', 'Qian Long', 'Jian Zheng', 'Xiaofeng Gao', 'Robinson Piramuthu', 'Wenhu Chen', 'William Yang Wang']",http://arxiv.org/abs/2410.05677v2
Automating Video Thumbnails Selection and Generation with Multimodal and Multistage Analysis,"This thesis presents an innovative approach to automate video thumbnail
selection for traditional broadcast content. Our methodology establishes
stringent criteria for diverse, representative, and aesthetically pleasing
thumbnails, considering factors like logo placement space, incorporation of
vertical aspect ratios, and accurate recognition of facial identities and
emotions. We introduce a sophisticated multistage pipeline that can select
candidate frames or generate novel images by blending video elements or using
diffusion models. The pipeline incorporates state-of-the-art models for various
tasks, including downsampling, redundancy reduction, automated cropping, face
recognition, closed-eye and emotion detection, shot scale and aesthetic
prediction, segmentation, matting, and harmonization. It also leverages large
language models and visual transformers for semantic consistency. A GUI tool
facilitates rapid navigation of the pipeline's output. To evaluate our method,
we conducted comprehensive experiments. In a study of 69 videos, 53.6% of our
proposed sets included thumbnails chosen by professional designers, with 73.9%
containing similar images. A survey of 82 participants showed a 45.77%
preference for our method, compared to 37.99% for manually chosen thumbnails
and 16.36% for an alternative method. Professional designers reported a
3.57-fold increase in valid candidates compared to the alternative method,
confirming that our approach meets established criteria. In conclusion, our
findings affirm that the proposed method accelerates thumbnail creation while
maintaining high-quality standards and fostering greater user engagement.",2024-10-18 16:04:15+00:00,['Elia Fantini'],http://arxiv.org/abs/2410.19825v1
TextCraftor: Your Text Encoder Can be Image Quality Controller,"Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have
revolutionized the field of content generation, enabling significant
advancements in areas like image editing and video synthesis. Despite their
formidable capabilities, these models are not without their limitations. It is
still challenging to synthesize an image that aligns well with the input text,
and multiple runs with carefully crafted prompts are required to achieve
satisfactory results. To mitigate these limitations, numerous studies have
endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing
various technologies. Yet, amidst these efforts, a pivotal question of
text-to-image diffusion model training has remained largely unexplored: Is it
possible and feasible to fine-tune the text encoder to improve the performance
of text-to-image diffusion models? Our findings reveal that, instead of
replacing the CLIP text encoder used in Stable Diffusion with other large
language models, we can enhance it through our proposed fine-tuning approach,
TextCraftor, leading to substantial improvements in quantitative benchmarks and
human assessments. Interestingly, our technique also empowers controllable
image generation through the interpolation of different text encoders
fine-tuned with various rewards. We also demonstrate that TextCraftor is
orthogonal to UNet finetuning, and can be combined to further improve
generative quality.",2024-03-27 19:52:55+00:00,"['Yanyu Li', 'Xian Liu', 'Anil Kag', 'Ju Hu', 'Yerlan Idelbayev', 'Dhritiman Sagar', 'Yanzhi Wang', 'Sergey Tulyakov', 'Jian Ren']",http://arxiv.org/abs/2403.18978v1
Optical Generative Models,"Generative models cover various application areas, including image, video and
music synthesis, natural language processing, and molecular design, among many
others. As digital generative models become larger, scalable inference in a
fast and energy-efficient manner becomes a challenge. Here, we present optical
generative models inspired by diffusion models, where a shallow and fast
digital encoder first maps random noise into phase patterns that serve as
optical generative seeds for a desired data distribution; a jointly-trained
free-space-based reconfigurable decoder all-optically processes these
generative seeds to create novel images (never seen before) following the
target data distribution. Except for the illumination power and the random seed
generation through a shallow encoder, these optical generative models do not
consume computing power during the synthesis of novel images. We report the
optical generation of monochrome and multi-color novel images of handwritten
digits, fashion products, butterflies, and human faces, following the data
distributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets,
respectively, achieving an overall performance comparable to digital neural
network-based generative models. To experimentally demonstrate optical
generative models, we used visible light to generate, in a snapshot, novel
images of handwritten digits and fashion products. These optical generative
models might pave the way for energy-efficient, scalable and rapid inference
tasks, further exploiting the potentials of optics and photonics for artificial
intelligence-generated content.",2024-10-23 15:36:08+00:00,"['Shiqi Chen', 'Yuhang Li', 'Hanlong Chen', 'Aydogan Ozcan']",http://arxiv.org/abs/2410.17970v1
Non-rigid Relative Placement through 3D Dense Diffusion,"The task of ""relative placement"" is to predict the placement of one object in
relation to another, e.g. placing a mug onto a mug rack. Through explicit
object-centric geometric reasoning, recent methods for relative placement have
made tremendous progress towards data-efficient learning for robot manipulation
while generalizing to unseen task variations. However, they have yet to
represent deformable transformations, despite the ubiquity of non-rigid bodies
in real world settings. As a first step towards bridging this gap, we propose
``cross-displacement"" - an extension of the principles of relative placement to
geometric relationships between deformable objects - and present a novel
vision-based method to learn cross-displacement through dense diffusion. To
this end, we demonstrate our method's ability to generalize to unseen object
instances, out-of-distribution scene configurations, and multimodal goals on
multiple highly deformable tasks (both in simulation and in the real world)
beyond the scope of prior works. Supplementary information and videos can be
found at https://sites.google.com/view/tax3d-corl-2024 .",2024-10-25 01:54:17+00:00,"['Eric Cai', 'Octavian Donca', 'Ben Eisner', 'David Held']",http://arxiv.org/abs/2410.19247v2
SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs,"The advancement of autonomous driving is increasingly reliant on high-quality
annotated datasets, especially in the task of 3D occupancy prediction, where
the occupancy labels require dense 3D annotation with significant human effort.
In this paper, we propose SyntheOcc, which denotes a diffusion model that
Synthesize photorealistic and geometric-controlled images by conditioning
Occupancy labels in driving scenarios. This yields an unlimited amount of
diverse, annotated, and controllable datasets for applications like training
perception models and simulation. SyntheOcc addresses the critical challenge of
how to efficiently encode 3D geometric information as conditional input to a 2D
diffusion model. Our approach innovatively incorporates 3D semantic multi-plane
images (MPIs) to provide comprehensive and spatially aligned 3D scene
descriptions for conditioning. As a result, SyntheOcc can generate
photorealistic multi-view images and videos that faithfully align with the
given geometric labels (semantics in 3D voxel space). Extensive qualitative and
quantitative evaluations of SyntheOcc on the nuScenes dataset prove its
effectiveness in generating controllable occupancy datasets that serve as an
effective data augmentation to perception models.",2024-10-01 02:29:24+00:00,"['Leheng Li', 'Weichao Qiu', 'Yingjie Cai', 'Xu Yan', 'Qing Lian', 'Bingbing Liu', 'Ying-Cong Chen']",http://arxiv.org/abs/2410.00337v1
Panonut360: A Head and Eye Tracking Dataset for Panoramic Video,"With the rapid development and widespread application of VR/AR technology,
maximizing the quality of immersive panoramic video services that match users'
personal preferences and habits has become a long-standing challenge.
Understanding the saliency region where users focus, based on data collected
with HMDs, can promote multimedia encoding, transmission, and quality
assessment. At the same time, large-scale datasets are essential for
researchers and developers to explore short/long-term user behavior patterns
and train AI models related to panoramic videos. However, existing panoramic
video datasets often include low-frequency user head or eye movement data
through short-term videos only, lacking sufficient data for analyzing users'
Field of View (FoV) and generating video saliency regions.
  Driven by these practical factors, in this paper, we present a head and eye
tracking dataset involving 50 users (25 males and 25 females) watching 15
panoramic videos. The dataset provides details on the viewport and gaze
attention locations of users. Besides, we present some statistics samples
extracted from the dataset. For example, the deviation between head and eye
movements challenges the widely held assumption that gaze attention decreases
from the center of the FoV following a Gaussian distribution. Our analysis
reveals a consistent downward offset in gaze fixations relative to the FoV in
experimental settings involving multiple users and videos. That's why we name
the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also
provide a script that generates saliency distributions based on given head or
eye coordinates and pre-generated saliency distribution map sets of each video
from the collected eye tracking data.
  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.",2024-03-26 13:54:52+00:00,"['Yutong Xu', 'Junhao Du', 'Jiahe Wang', 'Yuwei Ning', 'Sihan Zhou Yang Cao']",http://arxiv.org/abs/2403.17708v1
VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT,"Video temporal grounding (VTG) aims to locate specific temporal segments from
an untrimmed video based on a linguistic query. Most existing VTG models are
trained on extensive annotated video-text pairs, a process that not only
introduces human biases from the queries but also incurs significant
computational costs. To tackle these challenges, we propose VTG-GPT, a
GPT-based method for zero-shot VTG without training or fine-tuning. To reduce
prejudice in the original query, we employ Baichuan2 to generate debiased
queries. To lessen redundant information in videos, we apply MiniGPT-v2 to
transform visual content into more precise captions. Finally, we devise the
proposal generator and post-processing to produce accurate segments from
debiased queries and image captions. Extensive experiments demonstrate that
VTG-GPT significantly outperforms SOTA methods in zero-shot settings and
surpasses unsupervised approaches. More notably, it achieves competitive
performance comparable to supervised methods. The code is available on
https://github.com/YoucanBaby/VTG-GPT",2024-03-04 14:22:02+00:00,"['Yifang Xu', 'Yunzhuo Sun', 'Zien Xie', 'Benxiang Zhai', 'Sidan Du']",http://arxiv.org/abs/2403.02076v1
Patch Spatio-Temporal Relation Prediction for Video Anomaly Detection,"Video Anomaly Detection (VAD), aiming to identify abnormalities within a
specific context and timeframe, is crucial for intelligent Video Surveillance
Systems. While recent deep learning-based VAD models have shown promising
results by generating high-resolution frames, they often lack competence in
preserving detailed spatial and temporal coherence in video frames. To tackle
this issue, we propose a self-supervised learning approach for VAD through an
inter-patch relationship prediction task. Specifically, we introduce a
two-branch vision transformer network designed to capture deep visual features
of video frames, addressing spatial and temporal dimensions responsible for
modeling appearance and motion patterns, respectively. The inter-patch
relationship in each dimension is decoupled into inter-patch similarity and the
order information of each patch. To mitigate memory consumption, we convert the
order information prediction task into a multi-label learning problem, and the
inter-patch similarity prediction task into a distance matrix regression
problem. Comprehensive experiments demonstrate the effectiveness of our method,
surpassing pixel-generation-based methods by a significant margin across three
public benchmarks. Additionally, our approach outperforms other self-supervised
learning-based methods.",2024-03-28 03:07:16+00:00,"['Hao Shen', 'Lu Shi', 'Wanru Xu', 'Yigang Cen', 'Linna Zhang', 'Gaoyun An']",http://arxiv.org/abs/2403.19111v1
OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and Captioning,"We propose the new task 'open-world video instance segmentation and
captioning'. It requires to detect, segment, track and describe with rich
captions never before seen objects. This challenging task can be addressed by
developing ""abstractors"" which connect a vision model and a language foundation
model. Concretely, we connect a multi-scale visual feature extractor and a
large language model (LLM) by developing an object abstractor and an
object-to-text abstractor. The object abstractor, consisting of a prompt
encoder and transformer blocks, introduces spatially-diverse open-world object
queries to discover never before seen objects in videos. An inter-query
contrastive loss further encourages the diversity of object queries. The
object-to-text abstractor is augmented with masked cross-attention and acts as
a bridge between the object queries and a frozen LLM to generate rich and
descriptive object-centric captions for each detected object. Our generalized
approach surpasses the baseline that jointly addresses the tasks of open-world
video instance segmentation and dense video object captioning by 13% on never
before seen objects, and by 10% on object-centric captions.",2024-04-04 17:59:58+00:00,"['Anwesa Choudhuri', 'Girish Chowdhary', 'Alexander G. Schwing']",http://arxiv.org/abs/2404.03657v2
THQA: A Perceptual Quality Assessment Database for Talking Heads,"In the realm of media technology, digital humans have gained prominence due
to rapid advancements in computer technology. However, the manual modeling and
control required for the majority of digital humans pose significant obstacles
to efficient development. The speech-driven methods offer a novel avenue for
manipulating the mouth shape and expressions of digital humans. Despite the
proliferation of driving methods, the quality of many generated talking head
(TH) videos remains a concern, impacting user visual experiences. To tackle
this issue, this paper introduces the Talking Head Quality Assessment (THQA)
database, featuring 800 TH videos generated through 8 diverse speech-driven
methods. Extensive experiments affirm the THQA database's richness in character
and speech features. Subsequent subjective quality assessment experiments
analyze correlations between scoring results and speech-driven methods, ages,
and genders. In addition, experimental results show that mainstream image and
video quality assessment methods have limitations for the THQA database,
underscoring the imperative for further research to enhance TH video quality
assessment. The THQA database is publicly accessible at
https://github.com/zyj-2000/THQA.",2024-04-13 13:08:57+00:00,"['Yingjie Zhou', 'Zicheng Zhang', 'Wei Sun', 'Xiaohong Liu', 'Xiongkuo Min', 'Zhihua Wang', 'Xiao-Ping Zhang', 'Guangtao Zhai']",http://arxiv.org/abs/2404.09003v1
Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment,"Learning 3D human motion from 2D inputs is a fundamental task in the realms
of computer vision and computer graphics. Many previous methods grapple with
this inherently ambiguous task by introducing motion priors into the learning
process. However, these approaches face difficulties in defining the complete
configurations of such priors or training a robust model. In this paper, we
present the Video-to-Motion Generator (VTM), which leverages motion priors
through cross-modal latent feature space alignment between 3D human motion and
2D inputs, namely videos and 2D keypoints. To reduce the complexity of modeling
motion priors, we model the motion data separately for the upper and lower body
parts. Additionally, we align the motion data with a scale-invariant virtual
skeleton to mitigate the interference of human skeleton variations to the
motion priors. Evaluated on AIST++, the VTM showcases state-of-the-art
performance in reconstructing 3D human motion from monocular videos. Notably,
our VTM exhibits the capabilities for generalization to unseen view angles and
in-the-wild videos.",2024-04-15 06:38:09+00:00,"['Shuaiying Hou', 'Hongyu Tao', 'Junheng Fang', 'Changqing Zou', 'Hujun Bao', 'Weiwei Xu']",http://arxiv.org/abs/2404.09499v1
"Enhancing Inertial Hand based HAR through Joint Representation of Language, Pose and Synthetic IMUs","Due to the scarcity of labeled sensor data in HAR, prior research has turned
to video data to synthesize Inertial Measurement Units (IMU) data, capitalizing
on its rich activity annotations. However, generating IMU data from videos
presents challenges for HAR in real-world settings, attributed to the poor
quality of synthetic IMU data and its limited efficacy in subtle, fine-grained
motions. In this paper, we propose Multi$^3$Net, our novel multi-modal,
multitask, and contrastive-based framework approach to address the issue of
limited data. Our pretraining procedure uses videos from online repositories,
aiming to learn joint representations of text, pose, and IMU simultaneously. By
employing video data and contrastive learning, our method seeks to enhance
wearable HAR performance, especially in recognizing subtle activities.Our
experimental findings validate the effectiveness of our approach in improving
HAR performance with IMU data. We demonstrate that models trained with
synthetic IMU data generated from videos using our method surpass existing
approaches in recognizing fine-grained activities.",2024-06-03 13:28:42+00:00,"['Vitor Fortes Rey', 'Lala Shakti Swarup Ray', 'Xia Qingxin', 'Kaishun Wu', 'Paul Lukowicz']",http://arxiv.org/abs/2406.01316v2
Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake News Detection,"News media, especially video news media, have penetrated into every aspect of
daily life, which also brings the risk of fake news. Therefore, multimodal fake
news detection has recently garnered increased attention. However, the existing
datasets are comprised of user-uploaded videos and contain an excess amounts of
superfluous data, which introduces noise into the model training process. To
address this issue, we construct a dataset named Official-NV, comprising
officially published news videos. The crawl officially published videos are
augmented through the use of LLMs-based generation and manual verification,
thereby expanding the dataset. We also propose a new baseline model called
OFNVD, which captures key information from multimodal features through a GLU
attention mechanism and performs feature enhancement and modal aggregation via
a cross-modal Transformer. Benchmarking the dataset and baselines demonstrates
the effectiveness of our model in multimodal news detection.",2024-07-28 13:23:43+00:00,"['Yihao Wang', 'Lizhi Chen', 'Zhong Qian', 'Peifeng Li']",http://arxiv.org/abs/2407.19493v3
Sportoonizer: Augmenting Sports Highlights' Narration and Visual Impact via Automatic Manga B-Roll Generation,"Sports highlights are becoming increasingly popular on video-sharing
platforms. Yet, crafting sport highlight videos is challenging, which requires
producing engaging narratives from different angles, and conforming to
different platform affordances with constantly changing audiences. Many content
creators therefore create derivative work of the original sports video through
manga styles to enhance its expressiveness. But manually creating and inserting
tailored manga-style content can still be time-consuming. We introduce
Sportoonizer, a system embedding the pipeline for automatic generation of
manga-style animations for highlights in sports videos and insertion into
original videos. It seamlessly merges dynamic manga sequences with live-action
footage, enriching the visual tapestry and deepening narrative scope. By
leveraging genAIs, Sportoonizer crafts compelling storylines encapsulating the
intensity of sports moments and athletes' personal journeys. Our evaluation
study demonstrates that integrating manga B-rolls significantly enhances viewer
engagement, visual interest, and emotional connection towards athletes' stories
in the viewing experience.",2024-09-20 12:15:43+00:00,"['Siying Hu', 'Xiangzhe Yuan', 'Jiajun Wang', 'Piaohong Wang', 'Jian Ma', 'Zhiyang Wu', 'Qian Wan', 'Zhicong Lu']",http://arxiv.org/abs/2409.13443v1
GEM-VPC: A dual Graph-Enhanced Multimodal integration for Video Paragraph Captioning,"Video Paragraph Captioning (VPC) aims to generate paragraph captions that
summarises key events within a video. Despite recent advancements, challenges
persist, notably in effectively utilising multimodal signals inherent in videos
and addressing the long-tail distribution of words. The paper introduces a
novel multimodal integrated caption generation framework for VPC that leverages
information from various modalities and external knowledge bases. Our framework
constructs two graphs: a 'video-specific' temporal graph capturing major events
and interactions between multimodal information and commonsense knowledge, and
a 'theme graph' representing correlations between words of a specific theme.
These graphs serve as input for a transformer network with a shared
encoder-decoder architecture. We also introduce a node selection module to
enhance decoding efficiency by selecting the most relevant nodes from the
graphs. Our results demonstrate superior performance across benchmark datasets.",2024-10-12 06:01:00+00:00,"['Eileen Wang', 'Caren Han', 'Josiah Poon']",http://arxiv.org/abs/2410.09377v1
OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation,"We study the problem of teaching humanoid robots manipulation skills by
imitating from single video demonstrations. We introduce OKAMI, a method that
generates a manipulation plan from a single RGB-D video and derives a policy
for execution. At the heart of our approach is object-aware retargeting, which
enables the humanoid robot to mimic the human motions in an RGB-D video while
adjusting to different object locations during deployment. OKAMI uses
open-world vision models to identify task-relevant objects and retarget the
body motions and hand poses separately. Our experiments show that OKAMI
achieves strong generalizations across varying visual and spatial conditions,
outperforming the state-of-the-art baseline on open-world imitation from
observation. Furthermore, OKAMI rollout trajectories are leveraged to train
closed-loop visuomotor policies, which achieve an average success rate of 79.2%
without the need for labor-intensive teleoperation. More videos can be found on
our website https://ut-austin-rpl.github.io/OKAMI/.",2024-10-15 17:17:54+00:00,"['Jinhan Li', 'Yifeng Zhu', 'Yuqi Xie', 'Zhenyu Jiang', 'Mingyo Seo', 'Georgios Pavlakos', 'Yuke Zhu']",http://arxiv.org/abs/2410.11792v1
VidHal: Benchmarking Temporal Hallucinations in Vision LLMs,"Vision Large Language Models (VLLMs) are widely acknowledged to be prone to
hallucinations. Existing research addressing this problem has primarily been
confined to image inputs, with limited exploration of video-based
hallucinations. Furthermore, current evaluation methods fail to capture nuanced
errors in generated responses, which are often exacerbated by the rich
spatiotemporal dynamics of videos. To address this, we introduce VidHal, a
benchmark specially designed to evaluate video-based hallucinations in VLLMs.
VidHal is constructed by bootstrapping video instances across a wide range of
common temporal aspects. A defining feature of our benchmark lies in the
careful creation of captions which represent varying levels of hallucination
associated with each video. To enable fine-grained evaluation, we propose a
novel caption ordering task requiring VLLMs to rank captions by hallucinatory
extent. We conduct extensive experiments on VidHal and comprehensively evaluate
a broad selection of models. Our results uncover significant limitations in
existing VLLMs regarding hallucination generation. Through our benchmark, we
aim to inspire further research on 1) holistic understanding of VLLM
capabilities, particularly regarding hallucination, and 2) extensive
development of advanced VLLMs to alleviate this problem.",2024-11-25 06:17:23+00:00,"['Wey Yeh Choong', 'Yangyang Guo', 'Mohan Kankanhalli']",http://arxiv.org/abs/2411.16771v2
Temporal Contrastive Learning for Video Temporal Reasoning in Large Vision-Language Models,"Temporal reasoning is a critical challenge in video-language understanding,
as it requires models to align semantic concepts consistently across time.
While existing large vision-language models (LVLMs) and large language models
(LLMs) excel at static tasks, they struggle to capture dynamic interactions and
temporal dependencies in video sequences. In this work, we propose Temporal
Semantic Alignment via Dynamic Prompting (TSADP), a novel framework that
enhances temporal reasoning capabilities through dynamic task-specific prompts
and temporal contrastive learning. TSADP leverages a Dynamic Prompt Generator
(DPG) to encode fine-grained temporal relationships and a Temporal Contrastive
Loss (TCL) to align visual and textual embeddings across time. We evaluate our
method on the VidSitu dataset, augmented with enriched temporal annotations,
and demonstrate significant improvements over state-of-the-art models in tasks
such as Intra-Video Entity Association, Temporal Relationship Understanding,
and Chronology Prediction. Human evaluations further confirm TSADP's ability to
generate coherent and semantically accurate descriptions. Our analysis
highlights the robustness, efficiency, and practical utility of TSADP, making
it a step forward in the field of video-language understanding.",2024-12-16 02:37:58+00:00,"['Rafael Souza', 'Jia-Hao Lim', 'Alexander Davis']",http://arxiv.org/abs/2412.11391v1
An Ensemble Approach to Short-form Video Quality Assessment Using Multimodal LLM,"The rise of short-form videos, characterized by diverse content, editing
styles, and artifacts, poses substantial challenges for learning-based blind
video quality assessment (BVQA) models. Multimodal large language models
(MLLMs), renowned for their superior generalization capabilities, present a
promising solution. This paper focuses on effectively leveraging a pretrained
MLLM for short-form video quality assessment, regarding the impacts of
pre-processing and response variability, and insights on combining the MLLM
with BVQA models. We first investigated how frame pre-processing and sampling
techniques influence the MLLM's performance. Then, we introduced a lightweight
learning-based ensemble method that adaptively integrates predictions from the
MLLM and state-of-the-art BVQA models. Our results demonstrated superior
generalization performance with the proposed ensemble approach. Furthermore,
the analysis of content-aware ensemble weights highlighted that some video
characteristics are not fully represented by existing BVQA models, revealing
potential directions to improve BVQA models further.",2024-12-24 00:13:10+00:00,"['Wen Wen', 'Yilin Wang', 'Neil Birkbeck', 'Balu Adsumilli']",http://arxiv.org/abs/2412.18060v1
Scaling Rectified Flow Transformers for High-Resolution Image Synthesis,"Diffusion models create data from noise by inverting the forward paths of
data towards noise and have emerged as a powerful generative modeling technique
for high-dimensional, perceptual data such as images and videos. Rectified flow
is a recent generative model formulation that connects data and noise in a
straight line. Despite its better theoretical properties and conceptual
simplicity, it is not yet decisively established as standard practice. In this
work, we improve existing noise sampling techniques for training rectified flow
models by biasing them towards perceptually relevant scales. Through a
large-scale study, we demonstrate the superior performance of this approach
compared to established diffusion formulations for high-resolution
text-to-image synthesis. Additionally, we present a novel transformer-based
architecture for text-to-image generation that uses separate weights for the
two modalities and enables a bidirectional flow of information between image
and text tokens, improving text comprehension, typography, and human preference
ratings. We demonstrate that this architecture follows predictable scaling
trends and correlates lower validation loss to improved text-to-image synthesis
as measured by various metrics and human evaluations. Our largest models
outperform state-of-the-art models, and we will make our experimental data,
code, and model weights publicly available.",2024-03-05 18:45:39+00:00,"['Patrick Esser', 'Sumith Kulal', 'Andreas Blattmann', 'Rahim Entezari', 'Jonas Mller', 'Harry Saini', 'Yam Levi', 'Dominik Lorenz', 'Axel Sauer', 'Frederic Boesel', 'Dustin Podell', 'Tim Dockhorn', 'Zion English', 'Kyle Lacey', 'Alex Goodwin', 'Yannik Marek', 'Robin Rombach']",http://arxiv.org/abs/2403.03206v1
Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos,"Generating realistic audio for human actions is important for many
applications, such as creating sound effects for films or virtual reality
games. Existing approaches implicitly assume total correspondence between the
video and audio during training, yet many sounds happen off-screen and have
weak to no correspondence with the visuals -- resulting in uncontrolled ambient
sounds or hallucinations at test time. We propose a novel ambient-aware audio
generation model, AV-LDM. We devise a novel audio-conditioning mechanism to
learn to disentangle foreground action sounds from the ambient background
sounds in in-the-wild training videos. Given a novel silent video, our model
uses retrieval-augmented generation to create audio that matches the visual
content both semantically and temporally. We train and evaluate our model on
two in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we
introduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence.
Our model outperforms an array of existing methods, allows controllable
generation of the ambient sound, and even shows promise for generalizing to
computer graphics game clips. Overall, our approach is the first to focus
video-to-audio generation faithfully on the observed visual content despite
training from uncurated clips with natural background sounds.",2024-06-13 16:10:19+00:00,"['Changan Chen', 'Puyuan Peng', 'Ami Baid', 'Zihui Xue', 'Wei-Ning Hsu', 'David Harwath', 'Kristen Grauman']",http://arxiv.org/abs/2406.09272v3
Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks for Talking Head Video Generation,"Talking head video generation aims to generate a realistic talking head video
that preserves the person's identity from a source image and the motion from a
driving video. Despite the promising progress made in the field, it remains a
challenging and critical problem to generate videos with accurate poses and
fine-grained facial details simultaneously. Essentially, facial motion is often
highly complex to model precisely, and the one-shot source face image cannot
provide sufficient appearance guidance during generation due to dynamic pose
changes. To tackle the problem, we propose to jointly learn motion and
appearance codebooks and perform multi-scale codebook compensation to
effectively refine both the facial motion conditions and appearance features
for talking face image decoding. Specifically, the designed multi-scale motion
and appearance codebooks are learned simultaneously in a unified framework to
store representative global facial motion flow and appearance patterns. Then,
we present a novel multi-scale motion and appearance compensation module, which
utilizes a transformer-based codebook retrieval strategy to query complementary
information from the two codebooks for joint motion and appearance
compensation. The entire process produces motion flows of greater flexibility
and appearance features with fewer distortions across different scales,
resulting in a high-quality talking head video generation framework. Extensive
experiments on various benchmarks validate the effectiveness of our approach
and demonstrate superior generation results from both qualitative and
quantitative perspectives when compared to state-of-the-art competitors.",2024-12-01 07:54:07+00:00,"['Shuling Zhao', 'Fa-Ting Hong', 'Xiaoshui Huang', 'Dan Xu']",http://arxiv.org/abs/2412.00719v2
"Comprehensive Review of EEG-to-Output Research: Decoding Neural Signals into Images, Videos, and Audio","Electroencephalography (EEG) is an invaluable tool in neuroscience, offering
insights into brain activity with high temporal resolution. Recent advancements
in machine learning and generative modeling have catalyzed the application of
EEG in reconstructing perceptual experiences, including images, videos, and
audio. This paper systematically reviews EEG-to-output research, focusing on
state-of-the-art generative methods, evaluation metrics, and data challenges.
Using PRISMA guidelines, we analyze 1800 studies and identify key trends,
challenges, and opportunities in the field. The findings emphasize the
potential of advanced models such as Generative Adversarial Networks (GANs),
Variational Autoencoders (VAEs), and Transformers, while highlighting the
pressing need for standardized datasets and cross-subject generalization. A
roadmap for future research is proposed that aims to improve decoding accuracy
and broadening real-world applications.",2024-12-28 03:50:56+00:00,"['Yashvir Sabharwal', 'Balaji Rama']",http://arxiv.org/abs/2412.19999v1
In-Loop Filtering via Trained Look-Up Tables,"In-loop filtering (ILF) is a key technology for removing the artifacts in
image/video coding standards. Recently, neural network-based in-loop filtering
methods achieve remarkable coding gains beyond the capability of advanced video
coding standards, which becomes a powerful coding tool candidate for future
video coding standards. However, the utilization of deep neural networks brings
heavy time and computational complexity, and high demands of high-performance
hardware, which is challenging to apply to the general uses of coding scene. To
address this limitation, inspired by explorations in image restoration, we
propose an efficient and practical in-loop filtering scheme by adopting the
Look-up Table (LUT). We train the DNN of in-loop filtering within a fixed
filtering reference range, and cache the output values of the DNN into a LUT
via traversing all possible inputs. At testing time in the coding process, the
filtered pixel is generated by locating input pixels (to-be-filtered pixel with
reference pixels) and interpolating cached filtered pixel values. To further
enable the large filtering reference range with the limited storage cost of
LUT, we introduce the enhanced indexing mechanism in the filtering process, and
clipping/finetuning mechanism in the training. The proposed method is
implemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.
Experimental results show that the ultrafast, very fast, and fast mode of the
proposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%
BD-rate reduction, under the all intra (AI) and random access (RA)
configurations. Especially, our method has friendly time and computational
complexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,
and only 164-1148 KB storage cost for a single model. Our solution may shed
light on the journey of practical neural network-based coding tool evolution.",2024-07-15 17:25:42+00:00,"['Zhuoyuan Li', 'Jiacheng Li', 'Yao Li', 'Li Li', 'Dong Liu', 'Feng Wu']",http://arxiv.org/abs/2407.10926v2
Temporal Evolution of Knee Osteoarthritis: A Diffusion-based Morphing Model for X-ray Medical Image Synthesis,"Knee Osteoarthritis (KOA) is a common musculoskeletal disorder that
significantly affects the mobility of older adults. In the medical domain,
images containing temporal data are frequently utilized to study temporal
dynamics and statistically monitor disease progression. While deep
learning-based generative models for natural images have been widely
researched, there are comparatively few methods available for synthesizing
temporal knee X-rays. In this work, we introduce a novel deep-learning model
designed to synthesize intermediate X-ray images between a specific patient's
healthy knee and severe KOA stages. During the testing phase, based on a
healthy knee X-ray, the proposed model can produce a continuous and effective
sequence of KOA X-ray images with varying degrees of severity. Specifically, we
introduce a Diffusion-based Morphing Model by modifying the Denoising Diffusion
Probabilistic Model. Our approach integrates diffusion and morphing modules,
enabling the model to capture spatial morphing details between source and
target knee X-ray images and synthesize intermediate frames along a geodesic
path. A hybrid loss consisting of diffusion loss, morphing loss, and
supervision loss was employed. We demonstrate that our proposed approach
achieves the highest temporal frame synthesis performance, effectively
augmenting data for classification models and simulating the progression of
KOA.",2024-08-01 20:00:18+00:00,"['Zhe Wang', 'Aladine Chetouani', 'Rachid Jennane', 'Yuhua Ru', 'Wasim Issa', 'Mohamed Jarraya']",http://arxiv.org/abs/2408.00891v1
Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation,"We seek to learn a generalizable goal-conditioned policy that enables
zero-shot robot manipulation: interacting with unseen objects in novel scenes
without test-time adaptation. While typical approaches rely on a large amount
of demonstration data for such generalization, we propose an approach that
leverages web videos to predict plausible interaction plans and learns a
task-agnostic transformation to obtain robot actions in the real world. Our
framework,Track2Act predicts tracks of how points in an image should move in
future time-steps based on a goal, and can be trained with diverse videos on
the web including those of humans and robots manipulating everyday objects. We
use these 2D track predictions to infer a sequence of rigid transforms of the
object to be manipulated, and obtain robot end-effector poses that can be
executed in an open-loop manner. We then refine this open-loop plan by
predicting residual actions through a closed loop policy trained with a few
embodiment-specific demonstrations. We show that this approach of combining
scalably learned track prediction with a residual policy requiring minimal
in-domain robot-specific data enables diverse generalizable robot manipulation,
and present a wide array of real-world robot manipulation results across unseen
tasks, objects, and scenes. https://homangab.github.io/track2act/",2024-05-02 17:56:55+00:00,"['Homanga Bharadhwaj', 'Roozbeh Mottaghi', 'Abhinav Gupta', 'Shubham Tulsiani']",http://arxiv.org/abs/2405.01527v2
Evaluation of strategies for efficient rate-distortion NeRF streaming,"Neural Radiance Fields (NeRF) have revolutionized the field of 3D visual
representation by enabling highly realistic and detailed scene reconstructions
from a sparse set of images. NeRF uses a volumetric functional representation
that maps 3D points to their corresponding colors and opacities, allowing for
photorealistic view synthesis from arbitrary viewpoints. Despite its
advancements, the efficient streaming of NeRF content remains a significant
challenge due to the large amount of data involved. This paper investigates the
rate-distortion performance of two NeRF streaming strategies: pixel-based and
neural network (NN) parameter-based streaming. While in the former, images are
coded and then transmitted throughout the network, in the latter, the
respective NeRF model parameters are coded and transmitted instead. This work
also highlights the trade-offs in complexity and performance, demonstrating
that the NN parameter-based strategy generally offers superior efficiency,
making it suitable for one-to-many streaming scenarios.",2024-10-25 10:40:03+00:00,"['Pedro Martin', 'Antnio Rodrigues', 'Joo Ascenso', 'Maria Paula Queluz']",http://arxiv.org/abs/2410.19459v1
CoroNetGAN: Controlled Pruning of GANs via Hypernetworks,"Generative Adversarial Networks (GANs) have proven to exhibit remarkable
performance and are widely used across many generative computer vision
applications. However, the unprecedented demand for the deployment of GANs on
resource-constrained edge devices still poses a challenge due to huge number of
parameters involved in the generation process. This has led to focused
attention on the area of compressing GANs. Most of the existing works use
knowledge distillation with the overhead of teacher dependency. Moreover, there
is no ability to control the degree of compression in these methods. Hence, we
propose CoroNet-GAN for compressing GAN using the combined strength of
differentiable pruning method via hypernetworks. The proposed method provides
the advantage of performing controllable compression while training along with
reducing training time by a substantial factor. Experiments have been done on
various conditional GAN architectures (Pix2Pix and CycleGAN) to signify the
effectiveness of our approach on multiple benchmark datasets such as
Edges-to-Shoes, Horse-to-Zebra and Summer-to-Winter. The results obtained
illustrate that our approach succeeds to outperform the baselines on
Zebra-to-Horse and Summer-to-Winter achieving the best FID score of 32.3 and
72.3 respectively, yielding high-fidelity images across all the datasets.
Additionally, our approach also outperforms the state-of-the-art methods in
achieving better inference time on various smart-phone chipsets and data-types
making it a feasible solution for deployment on edge devices.",2024-03-13 05:24:28+00:00,"['Aman Kumar', 'Khushboo Anand', 'Shubham Mandloi', 'Ashutosh Mishra', 'Avinash Thakur', 'Neeraj Kasera', 'Prathosh A P']",http://arxiv.org/abs/2403.08261v1
NeuroCine: Decoding Vivid Video Sequences from Human Brain Activties,"In the pursuit to understand the intricacies of human brain's visual
processing, reconstructing dynamic visual experiences from brain activities
emerges as a challenging yet fascinating endeavor. While recent advancements
have achieved success in reconstructing static images from non-invasive brain
recordings, the domain of translating continuous brain activities into video
format remains underexplored. In this work, we introduce NeuroCine, a novel
dual-phase framework to targeting the inherent challenges of decoding fMRI
data, such as noises, spatial redundancy and temporal lags. This framework
proposes spatial masking and temporal interpolation-based augmentation for
contrastive learning fMRI representations and a diffusion model enhanced by
dependent prior noise for video generation. Tested on a publicly available fMRI
dataset, our method shows promising results, outperforming the previous
state-of-the-art models by a notable margin of ${20.97\%}$, ${31.00\%}$ and
${12.30\%}$ respectively on decoding the brain activities of three subjects in
the fMRI dataset, as measured by SSIM. Additionally, our attention analysis
suggests that the model aligns with existing brain structures and functions,
indicating its biological plausibility and interpretability.",2024-02-02 17:34:25+00:00,"['Jingyuan Sun', 'Mingxiao Li', 'Zijiao Chen', 'Marie-Francine Moens']",http://arxiv.org/abs/2402.01590v2
Efficient Digital Twin Data Processing for Low-Latency Multicast Short Video Streaming,"In this paper, we propose a novel efficient digital twin (DT) data processing
scheme to reduce service latency for multicast short video streaming.
Particularly, DT is constructed to emulate and analyze user status for
multicast group update and swipe feature abstraction. Then, a precise
measurement model of DT data processing is developed to characterize the
relationship among DT model size, user dynamics, and user clustering accuracy.
A service latency model, consisting of DT data processing delay, video
transcoding delay, and multicast transmission delay, is constructed by
incorporating the impact of user clustering accuracy. Finally, a joint
optimization problem of DT model size selection and bandwidth allocation is
formulated to minimize the service latency. To efficiently solve this problem,
a diffusion-based resource management algorithm is proposed, which utilizes the
denoising technique to improve the action-generation process in the deep
reinforcement learning algorithm. Simulation results based on the real-world
dataset demonstrate that the proposed DT data processing scheme outperforms
benchmark schemes in terms of service latency.",2024-04-21 19:12:22+00:00,"['Xinyu Huang', 'Shisheng Hu', 'Mushu Li', 'Cheng Huang', 'Xuemin Shen']",http://arxiv.org/abs/2404.13749v1
HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos,"Recently, implicit neural representation has been widely used to generate
animatable human avatars. However, the materials and geometry of those
representations are coupled in the neural network and hard to edit, which
hinders their application in traditional graphics engines. We present a
framework for acquiring human avatars that are attached with high-resolution
physically-based material textures and triangular mesh from monocular video.
Our method introduces a novel information fusion strategy to combine the
information from the monocular video and synthesize virtual multi-view images
to tackle the sparsity of the input view. We reconstruct humans as deformable
neural implicit surfaces and extract triangle mesh in a well-behaved pose as
the initial mesh of the next stage. In addition, we introduce an approach to
correct the bias for the boundary and size of the coarse mesh extracted.
Finally, we adapt prior knowledge of the latent diffusion model at
super-resolution in multi-view to distill the decomposed texture. Experiments
show that our approach outperforms previous representations in terms of high
fidelity, and this explicit result supports deployment on common renderers.",2024-05-18 11:49:09+00:00,"['Qifeng Chen', 'Rengan Xie', 'Kai Huang', 'Qi Wang', 'Wenting Zheng', 'Rong Li', 'Yuchi Huo']",http://arxiv.org/abs/2405.11270v1
Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection,"Video anomaly detection is an essential yet challenging open-set task in
computer vision, often addressed by leveraging reconstruction as a proxy task.
However, existing reconstruction-based methods encounter challenges in two main
aspects: (1) limited model robustness for open-set scenarios, (2) and an
overemphasis on, but restricted capacity for, detailed motion reconstruction.
To this end, we propose a novel frequency-guided diffusion model with
perturbation training, which enhances the model robustness by perturbation
training and emphasizes the principal motion components guided by motion
frequencies. Specifically, we first use a trainable generator to produce
perturbative samples for perturbation training of the diffusion model. During
the perturbation training phase, the model robustness is enhanced and the
domain of the reconstructed model is broadened by training against this
generator. Subsequently, perturbative samples are introduced for inference,
which impacts the reconstruction of normal and abnormal motions differentially,
thereby enhancing their separability. Considering that motion details originate
from high-frequency information, we propose a masking method based on 2D
discrete cosine transform to separate high-frequency information and
low-frequency information. Guided by the high-frequency information from
observed motion, the diffusion model can focus on generating low-frequency
information, and thus reconstructing the motion accurately. Experimental
results on five video anomaly detection datasets, including human-related and
open-set benchmarks, demonstrate the effectiveness of the proposed method. Our
code is available at https://github.com/Xiaofeng-Tan/FGDMAD-Code.",2024-12-04 05:43:53+00:00,"['Xiaofeng Tan', 'Hongsong Wang', 'Xin Geng']",http://arxiv.org/abs/2412.03044v1
MedEdit: Counterfactual Diffusion-based Image Editing on Brain MRI,"Denoising diffusion probabilistic models enable high-fidelity image synthesis
and editing. In biomedicine, these models facilitate counterfactual image
editing, producing pairs of images where one is edited to simulate hypothetical
conditions. For example, they can model the progression of specific diseases,
such as stroke lesions. However, current image editing techniques often fail to
generate realistic biomedical counterfactuals, either by inadequately modeling
indirect pathological effects like brain atrophy or by excessively altering the
scan, which disrupts correspondence to the original images. Here, we propose
MedEdit, a conditional diffusion model for medical image editing. MedEdit
induces pathology in specific areas while balancing the modeling of disease
effects and preserving the integrity of the original scan. We evaluated MedEdit
on the Atlas v2.0 stroke dataset using Frechet Inception Distance and Dice
scores, outperforming state-of-the-art diffusion-based methods such as Palette
(by 45%) and SDEdit (by 61%). Additionally, clinical evaluations by a
board-certified neuroradiologist confirmed that MedEdit generated realistic
stroke scans indistinguishable from real ones. We believe this work will enable
counterfactual image editing research to further advance the development of
realistic and clinically useful imaging tools.",2024-07-21 21:19:09+00:00,"['Malek Ben Alaya', 'Daniel M. Lang', 'Benedikt Wiestler', 'Julia A. Schnabel', 'Cosmin I. Bercea']",http://arxiv.org/abs/2407.15270v1
Tutorial on Diffusion Models for Imaging and Vision,"The astonishing growth of generative tools in recent years has empowered many
exciting applications in text-to-image generation and text-to-video generation.
The underlying principle behind these generative tools is the concept of
diffusion, a particular sampling mechanism that has overcome some shortcomings
that were deemed difficult in the previous approaches. The goal of this
tutorial is to discuss the essential ideas underlying the diffusion models. The
target audience of this tutorial includes undergraduate and graduate students
who are interested in doing research on diffusion models or applying these
models to solve other problems.",2024-03-26 21:01:41+00:00,['Stanley H. Chan'],http://arxiv.org/abs/2403.18103v3
GAIA: Rethinking Action Quality Assessment for AI-Generated Videos,"Assessing action quality is both imperative and challenging due to its
significant impact on the quality of AI-generated videos, further complicated
by the inherently ambiguous nature of actions within AI-generated video (AIGV).
Current action quality assessment (AQA) algorithms predominantly focus on
actions from real specific scenarios and are pre-trained with normative action
features, thus rendering them inapplicable in AIGVs. To address these problems,
we construct GAIA, a Generic AI-generated Action dataset, by conducting a
large-scale subjective evaluation from a novel causal reasoning-based
perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based
on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their
ability to generate visually rational actions, revealing their pros and cons on
different categories of actions. We also extend GAIA as a testbed to benchmark
the AQA capacity of existing automatic evaluation methods. Results show that
traditional AQA methods, action-related metrics in recent T2V benchmarks, and
mainstream video quality methods perform poorly with an average SRCC of 0.454,
0.191, and 0.519, respectively, indicating a sizable gap between current models
and human action perception patterns in AIGVs. Our findings underscore the
significance of action quality as a unique perspective for studying AIGVs and
can catalyze progress towards methods with enhanced capacities for AQA in
AIGVs.",2024-06-10 08:18:07+00:00,"['Zijian Chen', 'Wei Sun', 'Yuan Tian', 'Jun Jia', 'Zicheng Zhang', 'Jiarui Wang', 'Ru Huang', 'Xiongkuo Min', 'Guangtao Zhai', 'Wenjun Zhang']",http://arxiv.org/abs/2406.06087v2
LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior,"We present LARP, a novel video tokenizer designed to overcome limitations in
current video tokenization methods for autoregressive (AR) generative models.
Unlike traditional patchwise tokenizers that directly encode local visual
patches into discrete tokens, LARP introduces a holistic tokenization scheme
that gathers information from the visual content using a set of learned
holistic queries. This design allows LARP to capture more global and semantic
representations, rather than being limited to local patch-level information.
Furthermore, it offers flexibility by supporting an arbitrary number of
discrete tokens, enabling adaptive and efficient tokenization based on the
specific requirements of the task. To align the discrete token space with
downstream AR generation tasks, LARP integrates a lightweight AR transformer as
a training-time prior model that predicts the next token on its discrete latent
space. By incorporating the prior model during training, LARP learns a latent
space that is not only optimized for video reconstruction but is also
structured in a way that is more conducive to autoregressive generation.
Moreover, this process defines a sequential order for the discrete tokens,
progressively pushing them toward an optimal configuration during training,
ensuring smoother and more accurate AR generation at inference time.
Comprehensive experiments demonstrate LARP's strong performance, achieving
state-of-the-art FVD on the UCF101 class-conditional video generation
benchmark. LARP enhances the compatibility of AR models with videos and opens
up the potential to build unified high-fidelity multimodal large language
models (MLLMs).",2024-10-28 17:57:07+00:00,"['Hanyu Wang', 'Saksham Suri', 'Yixuan Ren', 'Hao Chen', 'Abhinav Shrivastava']",http://arxiv.org/abs/2410.21264v1
Open-Sora Plan: Open-Source Large Video Generation Model,"We introduce Open-Sora Plan, an open-source project that aims to contribute a
large generation model for generating desired high-resolution videos with long
durations based on various user inputs. Our project comprises multiple
components for the entire video generation process, including a Wavelet-Flow
Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various
condition controllers. Moreover, many assistant strategies for efficient
training and inference are designed, and a multi-dimensional data curation
pipeline is proposed for obtaining desired high-quality data. Benefiting from
efficient thoughts, our Open-Sora Plan achieves impressive video generation
results in both qualitative and quantitative evaluations. We hope our careful
design and practical experience can inspire the video generation research
community. All our codes and model weights are publicly available at
\url{https://github.com/PKU-YuanGroup/Open-Sora-Plan}.",2024-11-28 14:07:45+00:00,"['Bin Lin', 'Yunyang Ge', 'Xinhua Cheng', 'Zongjian Li', 'Bin Zhu', 'Shaodong Wang', 'Xianyi He', 'Yang Ye', 'Shenghai Yuan', 'Liuhan Chen', 'Tanghui Jia', 'Junwu Zhang', 'Zhenyu Tang', 'Yatian Pang', 'Bin She', 'Cen Yan', 'Zhiheng Hu', 'Xiaoyi Dong', 'Lin Chen', 'Zhang Pan', 'Xing Zhou', 'Shaoling Dong', 'Yonghong Tian', 'Li Yuan']",http://arxiv.org/abs/2412.00131v1
DSplats: 3D Generation by Denoising Splats-Based Multiview Diffusion Models,"Generating high-quality 3D content requires models capable of learning robust
distributions of complex scenes and the real-world objects within them. Recent
Gaussian-based 3D reconstruction techniques have achieved impressive results in
recovering high-fidelity 3D assets from sparse input images by predicting 3D
Gaussians in a feed-forward manner. However, these techniques often lack the
extensive priors and expressiveness offered by Diffusion Models. On the other
hand, 2D Diffusion Models, which have been successfully applied to denoise
multiview images, show potential for generating a wide range of photorealistic
3D outputs but still fall short on explicit 3D priors and consistency. In this
work, we aim to bridge these two approaches by introducing DSplats, a novel
method that directly denoises multiview images using Gaussian Splat-based
Reconstructors to produce a diverse array of realistic 3D assets. To harness
the extensive priors of 2D Diffusion Models, we incorporate a pretrained Latent
Diffusion Model into the reconstructor backbone to predict a set of 3D
Gaussians. Additionally, the explicit 3D representation embedded in the
denoising network provides a strong inductive bias, ensuring geometrically
consistent novel view generation. Our qualitative and quantitative experiments
demonstrate that DSplats not only produces high-quality, spatially consistent
outputs, but also sets a new standard in single-image to 3D reconstruction.
When evaluated on the Google Scanned Objects dataset, DSplats achieves a PSNR
of 20.38, an SSIM of 0.842, and an LPIPS of 0.109.",2024-12-11 07:32:17+00:00,"['Kevin Miao', 'Harsh Agrawal', 'Qihang Zhang', 'Federico Semeraro', 'Marco Cavallo', 'Jiatao Gu', 'Alexander Toshev']",http://arxiv.org/abs/2412.09648v1
TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models,"Understanding fine-grained temporal dynamics is crucial for multimodal video
comprehension and generation. Due to the lack of fine-grained temporal
annotations, existing video benchmarks mostly resemble static image benchmarks
and are incompetent at evaluating models for temporal understanding. In this
paper, we introduce TemporalBench, a new benchmark dedicated to evaluating
fine-grained temporal understanding in videos. TemporalBench consists of ~10K
video question-answer pairs, derived from ~2K high-quality human annotations
detailing the temporal dynamics in video clips. As a result, our benchmark
provides a unique testbed for evaluating various temporal understanding and
reasoning abilities such as action frequency, motion magnitude, event order,
etc. Moreover, it enables evaluations on various tasks like both video question
answering and captioning, both short and long video understanding, as well as
different models such as multimodal video embedding models and text generation
models. Results show that state-of-the-art models like GPT-4o achieve only
38.5% question answering accuracy on TemporalBench, demonstrating a significant
gap (~30%) between humans and AI in temporal understanding. Furthermore, we
notice a critical pitfall for multi-choice QA where LLMs can detect the subtle
changes in negative captions and find a centralized description as a cue for
its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such
bias. We hope that TemporalBench can foster research on improving models'
temporal reasoning capabilities. Both dataset and evaluation code will be made
available.",2024-10-14 17:59:58+00:00,"['Mu Cai', 'Reuben Tan', 'Jianrui Zhang', 'Bocheng Zou', 'Kai Zhang', 'Feng Yao', 'Fangrui Zhu', 'Jing Gu', 'Yiwu Zhong', 'Yuzhang Shang', 'Yao Dou', 'Jaden Park', 'Jianfeng Gao', 'Yong Jae Lee', 'Jianwei Yang']",http://arxiv.org/abs/2410.10818v2
DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation,"Closed-loop simulation is essential for advancing end-to-end autonomous
driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS,
rely predominantly on conditions closely aligned with training data
distributions, which are largely confined to forward-driving scenarios.
Consequently, these methods face limitations when rendering complex maneuvers
(e.g., lane change, acceleration, deceleration). Recent advancements in
autonomous-driving world models have demonstrated the potential to generate
diverse driving videos. However, these approaches remain constrained to 2D
video generation, inherently lacking the spatiotemporal coherence required to
capture intricacies of dynamic driving environments. In this paper, we
introduce DriveDreamer4D, which enhances 4D driving scene representation
leveraging world model priors. Specifically, we utilize the world model as a
data machine to synthesize novel trajectory videos, where structured conditions
are explicitly leveraged to control the spatial-temporal consistency of traffic
elements. Besides, the cousin data training strategy is proposed to facilitate
merging real and synthetic data for optimizing 4DGS. To our knowledge,
DriveDreamer4D is the first to utilize video generation models for improving 4D
reconstruction in driving scenarios. Experimental results reveal that
DriveDreamer4D significantly enhances generation quality under novel trajectory
views, achieving a relative improvement in FID by 32.1%, 46.4%, and 16.3%
compared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D
markedly enhances the spatiotemporal coherence of driving agents, which is
verified by a comprehensive user study and the relative increases of 22.6%,
43.5%, and 15.6% in the NTA-IoU metric.",2024-10-17 14:07:46+00:00,"['Guosheng Zhao', 'Chaojun Ni', 'Xiaofeng Wang', 'Zheng Zhu', 'Xueyang Zhang', 'Yida Wang', 'Guan Huang', 'Xinze Chen', 'Boyuan Wang', 'Youyi Zhang', 'Wenjun Mei', 'Xingang Wang']",http://arxiv.org/abs/2410.13571v3
A Volumetric Saliency Guided Image Summarization for RGB-D Indoor Scene Classification,"Image summary, an abridged version of the original visual content, can be
used to represent the scene. Thus, tasks such as scene classification,
identification, indexing, etc., can be performed efficiently using the unique
summary. Saliency is the most commonly used technique for generating the
relevant image summary. However, the definition of saliency is subjective in
nature and depends upon the application. Existing saliency detection methods
using RGB-D data mainly focus on color, texture, and depth features.
Consequently, the generated summary contains either foreground objects or
non-stationary objects. However, applications such as scene identification
require stationary characteristics of the scene, unlike state-of-the-art
methods. This paper proposes a novel volumetric saliency-guided framework for
indoor scene classification. The results highlight the efficacy of the proposed
method.",2024-01-19 09:48:48+00:00,"['Preeti Meena', 'Himanshu Kumar', 'Sandeep Yadav']",http://arxiv.org/abs/2401.16227v1
Towards Unbiased and Robust Spatio-Temporal Scene Graph Generation and Anticipation,"Spatio-Temporal Scene Graphs (STSGs) provide a concise and expressive
representation of dynamic scenes by modeling objects and their evolving
relationships over time. However, real-world visual relationships often exhibit
a long-tailed distribution, causing existing methods for tasks like Video Scene
Graph Generation (VidSGG) and Scene Graph Anticipation (SGA) to produce biased
scene graphs. To this end, we propose ImparTail, a novel training framework
that leverages loss masking and curriculum learning to mitigate bias in the
generation and anticipation of spatio-temporal scene graphs. Unlike prior
methods that add extra architectural components to learn unbiased estimators,
we propose an impartial training objective that reduces the dominance of head
classes during learning and focuses on underrepresented tail relationships. Our
curriculum-driven mask generation strategy further empowers the model to
adaptively adjust its bias mitigation strategy over time, enabling more
balanced and robust estimations. To thoroughly assess performance under various
distribution shifts, we also introduce two new tasks Robust Spatio-Temporal
Scene Graph Generation and Robust Scene Graph Anticipation offering a
challenging benchmark for evaluating the resilience of STSG models. Extensive
experiments on the Action Genome dataset demonstrate the superior unbiased
performance and robustness of our method compared to existing baselines.",2024-11-20 06:15:28+00:00,"['Rohith Peddi', 'Saurabh', 'Ayush Abhay Shrivastava', 'Parag Singla', 'Vibhav Gogate']",http://arxiv.org/abs/2411.13059v2
Mechanisms of Generative Image-to-Image Translation Networks,"Generative Adversarial Networks (GANs) are a class of neural networks that
have been widely used in the field of image-to-image translation. In this
paper, we propose a streamlined image-to-image translation network with a
simpler architecture compared to existing models. We investigate the
relationship between GANs and autoencoders and provide an explanation for the
efficacy of employing only the GAN component for tasks involving image
translation. We show that adversarial for GAN models yields results comparable
to those of existing methods without additional complex loss penalties.
Subsequently, we elucidate the rationale behind this phenomenon. We also
incorporate experimental results to demonstrate the validity of our findings.",2024-11-15 17:17:46+00:00,"['Guangzong Chen', 'Mingui Sun', 'Zhi-Hong Mao', 'Kangni Liu', 'Wenyan Jia']",http://arxiv.org/abs/2411.10368v1
DREAM: Improving Video-Text Retrieval Through Relevance-Based Augmentation Using Large Foundation Models,"Recent progress in video-text retrieval has been driven largely by
advancements in model architectures and training strategies. However, the
representation learning capabilities of videotext retrieval models remain
constrained by lowquality and limited training data annotations. To address
this issue, we present a novel ViDeoText Retrieval Paradigm with
RElevance-based AugMentation, namely DREAM, which enhances video and text data
using large foundation models to learn more generalized features. Specifically,
we first adopt a simple augmentation method, which generates self-similar data
by randomly duplicating or dropping subwords and frames. In addition, inspired
by the recent advancement in visual and language generative models, we propose
a more robust augmentation method through textual paraphrasing and video
stylization using large language models (LLMs) and visual generative models
(VGMs). To further enrich video and text information, we propose a
relevance-based augmentation method, where LLMs and VGMs generate and integrate
new relevant information into the original data. Leveraging this enriched data,
extensive experiments on several video-text retrieval benchmarks demonstrate
the superiority of DREAM over existing methods.",2024-04-07 21:46:47+00:00,"['Yimu Wang', 'Shuai Yuan', 'Bo Xue', 'Xiangru Jian', 'Wei Pang', 'Mushi Wang', 'Ning Yu']",http://arxiv.org/abs/2404.05083v2
Learning Online Scale Transformation for Talking Head Video Generation,"One-shot talking head video generation uses a source image and driving video
to create a synthetic video where the source person's facial movements imitate
those of the driving video. However, differences in scale between the source
and driving images remain a challenge for face reenactment. Existing methods
attempt to locate a frame in the driving video that aligns best with the source
image, but imprecise alignment can result in suboptimal outcomes.
  To this end, we introduce a scale transformation module that can
automatically adjust the scale of the driving image to fit that of the source
image, by using the information of scale difference maintained in the detected
keypoints of the source image and the driving frame. Furthermore, to keep
perceiving the scale information of faces during the generation process, we
incorporate the scale information learned from the scale transformation module
into each layer of the generation process to produce a final result with an
accurate scale. Our method can perform accurate motion transfer between the two
images without any anchor frame, achieved through the contributions of the
proposed online scale transformation facial reenactment network. Extensive
experiments have demonstrated that our proposed method adjusts the scale of the
driving face automatically according to the source face, and generates
high-quality faces with an accurate scale in the cross-identity facial
reenactment.",2024-07-13 18:08:46+00:00,"['Fa-Ting Hong', 'Dan Xu']",http://arxiv.org/abs/2407.09965v1
Towards Open Domain Text-Driven Synthesis of Multi-Person Motions,"This work aims to generate natural and diverse group motions of multiple
humans from textual descriptions. While single-person text-to-motion generation
is extensively studied, it remains challenging to synthesize motions for more
than one or two subjects from in-the-wild prompts, mainly due to the lack of
available datasets. In this work, we curate human pose and motion datasets by
estimating pose information from large-scale image and video datasets. Our
models use a transformer-based diffusion framework that accommodates multiple
datasets with any number of subjects or frames. Experiments explore both
generation of multi-person static poses and generation of multi-person motion
sequences. To our knowledge, our method is the first to generate multi-subject
motion sequences with high diversity and fidelity from a large variety of
textual prompts.",2024-05-28 18:00:06+00:00,"['Mengyi Shan', 'Lu Dong', 'Yutao Han', 'Yuan Yao', 'Tao Liu', 'Ifeoma Nwogu', 'Guo-Jun Qi', 'Mitch Hill']",http://arxiv.org/abs/2405.18483v2
Upsample Guidance: Scale Up Diffusion Models without Training,"Diffusion models have demonstrated superior performance across various
generative tasks including images, videos, and audio. However, they encounter
difficulties in directly generating high-resolution samples. Previously
proposed solutions to this issue involve modifying the architecture, further
training, or partitioning the sampling process into multiple stages. These
methods have the limitation of not being able to directly utilize pre-trained
models as-is, requiring additional work. In this paper, we introduce upsample
guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to
generate higher-resolution images (e.g., $1536^2$) by adding only a single term
in the sampling process. Remarkably, this technique does not necessitate any
additional training or relying on external models. We demonstrate that upsample
guidance can be applied to various models, such as pixel-space, latent space,
and video diffusion models. We also observed that the proper selection of
guidance scale can improve image quality, fidelity, and prompt alignment.",2024-04-02 07:49:08+00:00,"['Juno Hwang', 'Yong-Hyun Park', 'Junghyo Jo']",http://arxiv.org/abs/2404.01709v1
Pseudo-MRI-Guided PET Image Reconstruction Method Based on a Diffusion Probabilistic Model,"Anatomically guided PET reconstruction using MRI information has been shown
to have the potential to improve PET image quality. However, these improvements
are limited to PET scans with paired MRI information. In this work we employed
a diffusion probabilistic model (DPM) to infer T1-weighted-MRI (deep-MRI)
images from FDG-PET brain images. We then use the DPM-generated T1w-MRI to
guide the PET reconstruction. The model was trained with brain FDG scans, and
tested in datasets containing multiple levels of counts. Deep-MRI images
appeared somewhat degraded than the acquired MRI images. Regarding PET image
quality, volume of interest analysis in different brain regions showed that
both PET reconstructed images using the acquired and the deep-MRI images
improved image quality compared to OSEM. Same conclusions were found analysing
the decimated datasets. A subjective evaluation performed by two physicians
confirmed that OSEM scored consistently worse than the MRI-guided PET images
and no significant differences were observed between the MRI-guided PET images.
This proof of concept shows that it is possible to infer DPM-based MRI imagery
to guide the PET reconstruction, enabling the possibility of changing
reconstruction parameters such as the strength of the prior on anatomically
guided PET reconstruction in the absence of MRI.",2024-03-26 22:50:36+00:00,"['Weijie Gan', 'Huidong Xie', 'Carl von Gall', 'Gnther Platsch', 'Michael T. Jurkiewicz', 'Andrea Andrade', 'Udunna C. Anazodo', 'Ulugbek S. Kamilov', 'Hongyu An', 'Jorge Cabello']",http://arxiv.org/abs/2403.18139v1
Reading Between the Frames: Multi-Modal Depression Detection in Videos from Non-Verbal Cues,"Depression, a prominent contributor to global disability, affects a
substantial portion of the population. Efforts to detect depression from social
media texts have been prevalent, yet only a few works explored depression
detection from user-generated video content. In this work, we address this
research gap by proposing a simple and flexible multi-modal temporal model
capable of discerning non-verbal depression cues from diverse modalities in
noisy, real-world videos. We show that, for in-the-wild videos, using
additional high-level non-verbal cues is crucial to achieving good performance,
and we extracted and processed audio speech embeddings, face emotion
embeddings, face, body and hand landmarks, and gaze and blinking information.
Through extensive experiments, we show that our model achieves state-of-the-art
results on three key benchmark datasets for depression detection from video by
a substantial margin. Our code is publicly available on GitHub.",2024-01-05 10:47:42+00:00,"['David Gimeno-Gmez', 'Ana-Maria Bucur', 'Adrian Cosma', 'Carlos-David Martnez-Hinarejos', 'Paolo Rosso']",http://arxiv.org/abs/2401.02746v1
A Network for structural dense displacement based on 3D deformable mesh model and optical flow,"This study proposes a Network to recognize displacement of a RC frame
structure from a video by a monocular camera. The proposed Network consists of
two modules which is FlowNet2 and POFRN-Net. FlowNet2 is used to generate dense
optical flow as well as POFRN-Net is to extract pose parameter H. FlowNet2
convert two video frames into dense optical flow. POFRN-Net is inputted dense
optical flow from FlowNet2 to output the pose parameter H. The displacement of
any points of structure can be calculated from parameter H. The Fast Fourier
Transform (FFT) is applied to obtain frequency domain signal from corresponding
displacement signal. Furthermore, the comparison of the truth displacement on
the First floor of the First video is shown in this study. Finally, the
predicted displacements on four floors of RC frame structure of given three
videos are exhibited in the last of this study.",2024-02-09 11:09:52+00:00,"['Peimian Du', 'Qicheng Guo', 'Yanru Li']",http://arxiv.org/abs/2402.06329v1
Depth-aware Test-Time Training for Zero-shot Video Object Segmentation,"Zero-shot Video Object Segmentation (ZSVOS) aims at segmenting the primary
moving object without any human annotations. Mainstream solutions mainly focus
on learning a single model on large-scale video datasets, which struggle to
generalize to unseen videos. In this work, we introduce a test-time training
(TTT) strategy to address the problem. Our key insight is to enforce the model
to predict consistent depth during the TTT process. In detail, we first train a
single network to perform both segmentation and depth prediction tasks. This
can be effectively learned with our specifically designed depth modulation
layer. Then, for the TTT process, the model is updated by predicting consistent
depth maps for the same frame under different data augmentations. In addition,
we explore different TTT weight updating strategies. Our empirical results
suggest that the momentum-based weight initialization and looping-based
training scheme lead to more stable improvements. Experiments show that the
proposed method achieves clear improvements on ZSVOS. Our proposed video TTT
strategy provides significant superiority over state-of-the-art TTT methods.
Our code is available at: https://nifangbaage.github.io/DATTT.",2024-03-07 06:40:53+00:00,"['Weihuang Liu', 'Xi Shen', 'Haolun Li', 'Xiuli Bi', 'Bo Liu', 'Chi-Man Pun', 'Xiaodong Cun']",http://arxiv.org/abs/2403.04258v1
Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation,"We propose a novel approach to the action segmentation task for long,
untrimmed videos, based on solving an optimal transport problem. By encoding a
temporal consistency prior into a Gromov-Wasserstein problem, we are able to
decode a temporally consistent segmentation from a noisy affinity/matching cost
matrix between video frames and action classes. Unlike previous approaches, our
method does not require knowing the action order for a video to attain temporal
consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can
be efficiently solved on GPUs using a few iterations of projected mirror
descent. We demonstrate the effectiveness of our method in an unsupervised
learning setting, where our method is used to generate pseudo-labels for
self-training. We evaluate our segmentation approach and unsupervised learning
pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly
datasets, yielding state-of-the-art results for the unsupervised video action
segmentation task.",2024-04-01 22:53:47+00:00,"['Ming Xu', 'Stephen Gould']",http://arxiv.org/abs/2404.01518v3
A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos,"Eye-tracking applications that utilize the human gaze in video understanding
tasks have become increasingly important. To effectively automate the process
of video analysis based on eye-tracking data, it is important to accurately
replicate human gaze behavior. However, this task presents significant
challenges due to the inherent complexity and ambiguity of human gaze patterns.
In this work, we introduce a novel method for simulating human gaze behavior.
Our approach uses a transformer-based reinforcement learning algorithm to train
an agent that acts as a human observer, with the primary role of watching
videos and simulating human gaze behavior. We employed an eye-tracking dataset
gathered from videos generated by the VirtualHome simulator, with a primary
focus on activity recognition. Our experimental results demonstrate the
effectiveness of our gaze prediction method by highlighting its capability to
replicate human gaze behavior and its applicability for downstream tasks where
real human-gaze is used as input.",2024-04-10 21:14:33+00:00,"['Suleyman Ozdel', 'Yao Rong', 'Berat Mert Albaba', 'Yen-Ling Kuo', 'Xi Wang', 'Enkelejda Kasneci']",http://arxiv.org/abs/2404.07351v1
Leveraging Temporal Contextualization for Video Action Recognition,"We propose a novel framework for video understanding, called Temporally
Contextualized CLIP (TC-CLIP), which leverages essential temporal information
through global interactions in a spatio-temporal domain within a video. To be
specific, we introduce Temporal Contextualization (TC), a layer-wise temporal
information infusion mechanism for videos, which 1) extracts core information
from each frame, 2) connects relevant information across frames for the
summarization into context tokens, and 3) leverages the context tokens for
feature encoding. Furthermore, the Video-conditional Prompting (VP) module
processes context tokens to generate informative prompts in the text modality.
Extensive experiments in zero-shot, few-shot, base-to-novel, and
fully-supervised action recognition validate the effectiveness of our model.
Ablation studies for TC and VP support our design choices. Our project page
with the source code is available at https://github.com/naver-ai/tc-clip",2024-04-15 06:24:56+00:00,"['Minji Kim', 'Dongyoon Han', 'Taekyung Kim', 'Bohyung Han']",http://arxiv.org/abs/2404.09490v2
SC-HVPPNet: Spatial and Channel Hybrid-Attention Video Post-Processing Network with CNN and Transformer,"Convolutional Neural Network (CNN) and Transformer have attracted much
attention recently for video post-processing (VPP). However, the interaction
between CNN and Transformer in existing VPP methods is not fully explored,
leading to inefficient communication between the local and global extracted
features. In this paper, we explore the interaction between CNN and Transformer
in the task of VPP, and propose a novel Spatial and Channel Hybrid-Attention
Video Post-Processing Network (SC-HVPPNet), which can cooperatively exploit the
image priors in both spatial and channel domains. Specifically, in the spatial
domain, a novel spatial attention fusion module is designed, in which two
attention weights are generated to fuse the local and global representations
collaboratively. In the channel domain, a novel channel attention fusion module
is developed, which can blend the deep representations at the channel dimension
dynamically. Extensive experiments show that SC-HVPPNet notably boosts video
restoration quality, with average bitrate savings of 5.29%, 12.42%, and 13.09%
for Y, U, and V components in the VTM-11.0-NNVC RA configuration.",2024-04-23 03:35:27+00:00,"['Tong Zhang', 'Wenxue Cui', 'Shaohui Liu', 'Feng Jiang']",http://arxiv.org/abs/2404.14709v1
CSTA: CNN-based Spatiotemporal Attention for Video Summarization,"Video summarization aims to generate a concise representation of a video,
capturing its essential content and key moments while reducing its overall
length. Although several methods employ attention mechanisms to handle
long-term dependencies, they often fail to capture the visual significance
inherent in frames. To address this limitation, we propose a CNN-based
SpatioTemporal Attention (CSTA) method that stacks each feature of frames from
a single video to form image-like frame representations and applies 2D CNN to
these frame features. Our methodology relies on CNN to comprehend the inter and
intra-frame relations and to find crucial attributes in videos by exploiting
its ability to learn absolute positions within images. In contrast to previous
work compromising efficiency by designing additional modules to focus on
spatial importance, CSTA requires minimal computational overhead as it uses CNN
as a sliding window. Extensive experiments on two benchmark datasets (SumMe and
TVSum) demonstrate that our proposed approach achieves state-of-the-art
performance with fewer MACs compared to previous methods. Codes are available
at https://github.com/thswodnjs3/CSTA.",2024-05-20 09:38:37+00:00,"['Jaewon Son', 'Jaehun Park', 'Kwangsu Kim']",http://arxiv.org/abs/2405.11905v2
Context-Enhanced Video Moment Retrieval with Large Language Models,"Current methods for Video Moment Retrieval (VMR) struggle to align complex
situations involving specific environmental details, character descriptions,
and action narratives. To tackle this issue, we propose a Large Language
Model-guided Moment Retrieval (LMR) approach that employs the extensive
knowledge of Large Language Models (LLMs) to improve video context
representation as well as cross-modal alignment, facilitating accurate
localization of target moments. Specifically, LMR introduces a context
enhancement technique with LLMs to generate crucial target-related context
semantics. These semantics are integrated with visual features for producing
discriminative video representations. Finally, a language-conditioned
transformer is designed to decode free-form language queries, on the fly, using
aligned video representations for moment retrieval. Extensive experiments
demonstrate that LMR achieves state-of-the-art results, outperforming the
nearest competitor by up to 3.28\% and 4.06\% on the challenging QVHighlights
and Charades-STA benchmarks, respectively. More importantly, the performance
gains are significantly higher for localization of complex queries.",2024-05-21 07:12:27+00:00,"['Weijia Liu', 'Bo Miao', 'Jiuxin Cao', 'Xuelin Zhu', 'Bo Liu', 'Mehwish Nasim', 'Ajmal Mian']",http://arxiv.org/abs/2405.12540v1
Compressed Video Quality Enhancement with Temporal Group Alignment and Fusion,"In this paper, we propose a temporal group alignment and fusion network to
enhance the quality of compressed videos by using the long-short term
correlations between frames. The proposed model consists of the intra-group
feature alignment (IntraGFA) module, the inter-group feature fusion (InterGFF)
module, and the feature enhancement (FE) module. We form the group of pictures
(GoP) by selecting frames from the video according to their temporal distances
to the target enhanced frame. With this grouping, the composed GoP can contain
either long- or short-term correlated information of neighboring frames. We
design the IntraGFA module to align the features of frames of each GoP to
eliminate the motion existing between frames. We construct the InterGFF module
to fuse features belonging to different GoPs and finally enhance the fused
features with the FE module to generate high-quality video frames. The
experimental results show that our proposed method achieves up to 0.05dB gain
and lower complexity compared to the state-of-the-art method.",2024-06-14 03:36:55+00:00,"['Qiang Zhu', 'Yajun Qiu', 'Yu Liu', 'Shuyuan Zhu', 'Bing Zeng']",http://arxiv.org/abs/2406.09693v1
Computational Thinking through Design Patterns in Video Games,"Prior research has explored potential applications of video games in
programming education to elicit computational thinking skills. However,
existing approaches are often either too general, not taking into account the
diversity of genres and mechanisms between video games, or too narrow,
selecting tools that were specifically designed for educational purposes. In
this paper we propose a more fundamental approach, defining beneficial
connections between individual design patterns present in video games and
computational thinking skills. We argue that video games have the capacity to
elicit these skills and even to potentially train them. This could be an
effective method to solidify a conceptual base which would make programming
education more effective.",2024-07-04 11:44:31+00:00,"['Giulio Barbero', 'Marcello A. Gmez-Maureira', 'Felienne F. J. Hermans']",http://arxiv.org/abs/2407.03860v1
Audio-driven High-resolution Seamless Talking Head Video Editing via StyleGAN,"The existing methods for audio-driven talking head video editing have the
limitations of poor visual effects. This paper tries to tackle this problem
through editing talking face images seamless with different emotions based on
two modules: (1) an audio-to-landmark module, consisting of the
CrossReconstructed Emotion Disentanglement and an alignment network module. It
bridges the gap between speech and facial motions by predicting corresponding
emotional landmarks from speech; (2) a landmark-based editing module edits face
videos via StyleGAN. It aims to generate the seamless edited video consisting
of the emotion and content components from the input audio. Extensive
experiments confirm that compared with state-of-the-arts methods, our method
provides high-resolution videos with high visual quality.",2024-07-08 03:17:10+00:00,"['Jiacheng Su', 'Kunhong Liu', 'Liyan Chen', 'Junfeng Yao', 'Qingsong Liu', 'Dongdong Lv']",http://arxiv.org/abs/2407.05577v1
Orthogonal Hyper-category Guided Multi-interest Elicitation for Micro-video Matching,"Watching micro-videos is becoming a part of public daily life. Usually, user
watching behaviors are thought to be rooted in their multiple different
interests. In the paper, we propose a model named OPAL for micro-video
matching, which elicits a user's multiple heterogeneous interests by
disentangling multiple soft and hard interest embeddings from user
interactions. Moreover, OPAL employs a two-stage training strategy, in which
the pre-train is to generate soft interests from historical interactions under
the guidance of orthogonal hyper-categories of micro-videos and the fine-tune
is to reinforce the degree of disentanglement among the interests and learn the
temporal evolution of each interest of each user. We conduct extensive
experiments on two real-world datasets. The results show that OPAL not only
returns diversified micro-videos but also outperforms six state-of-the-art
models in terms of recall and hit rate.",2024-07-20 03:41:57+00:00,"['Beibei Li', 'Beihong Jin', 'Yisong Yu', 'Yiyuan Zheng', 'Jiageng Song', 'Wei Zhuo', 'Tao Xiang']",http://arxiv.org/abs/2407.14741v1
MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos,"Detection of anomaly events is relevant for public safety and requires a
combination of fine-grained motion information and contextual events at
variable time-scales. To this end, we propose a Multi-Timescale Feature
Learning (MTFL) method to enhance the representation of anomaly features.
Short, medium, and long temporal tubelets are employed to extract
spatio-temporal video features using a Video Swin Transformer. Experimental
results demonstrate that MTFL outperforms state-of-the-art methods on the
UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC.
Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech
and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended
dataset of the UCF-Crime for development and evaluation on a wider range of
anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591
videos in 18 classes with extensive coverage of realistic anomalies.",2024-10-08 10:57:33+00:00,"['Yiling Zhang', 'Erkut Akdag', 'Egor Bondarev', 'Peter H. N. De With']",http://arxiv.org/abs/2410.05900v1
ElasticTok: Adaptive Tokenization for Image and Video,"Efficient video tokenization remains a key bottleneck in learning general
purpose vision models that are capable of processing long video sequences.
Prevailing approaches are restricted to encoding videos to a fixed number of
tokens, where too few tokens will result in overly lossy encodings, and too
many tokens will result in prohibitively long sequence lengths. In this work,
we introduce ElasticTok, a method that conditions on prior frames to adaptively
encode a frame into a variable number of tokens. To enable this in a
computationally scalable way, we propose a masking technique that drops a
random number of tokens at the end of each frames's token encoding. During
inference, ElasticTok can dynamically allocate tokens when needed -- more
complex data can leverage more tokens, while simpler data only needs a few
tokens. Our empirical evaluations on images and video demonstrate the
effectiveness of our approach in efficient token usage, paving the way for
future development of more powerful multimodal models, world models, and
agents.",2024-10-10 20:54:15+00:00,"['Wilson Yan', 'Volodymyr Mnih', 'Aleksandra Faust', 'Matei Zaharia', 'Pieter Abbeel', 'Hao Liu']",http://arxiv.org/abs/2410.08368v2
Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos,"We present Agent-to-Sim (ATS), a framework for learning interactive behavior
models of 3D agents from casual longitudinal video collections. Different from
prior works that rely on marker-based tracking and multiview cameras, ATS
learns natural behaviors of animal and human agents non-invasively through
video observations recorded over a long time-span (e.g., a month) in a single
environment. Modeling 3D behavior of an agent requires persistent 3D tracking
(e.g., knowing which point corresponds to which) over a long time period. To
obtain such data, we develop a coarse-to-fine registration method that tracks
the agent and the camera over time through a canonical 3D space, resulting in a
complete and persistent spacetime 4D representation. We then train a generative
model of agent behaviors using paired data of perception and motion of an agent
queried from the 4D reconstruction. ATS enables real-to-sim transfer from video
recordings of an agent to an interactive behavior simulator. We demonstrate
results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos
captured by a smartphone.",2024-10-21 17:57:50+00:00,"['Gengshan Yang', 'Andrea Bajcsy', 'Shunsuke Saito', 'Angjoo Kanazawa']",http://arxiv.org/abs/2410.16259v1
Enhancing Multimodal Affective Analysis with Learned Live Comment Features,"Live comments, also known as Danmaku, are user-generated messages that are
synchronized with video content. These comments overlay directly onto streaming
videos, capturing viewer emotions and reactions in real-time. While prior work
has leveraged live comments in affective analysis, its use has been limited due
to the relative rarity of live comments across different video platforms. To
address this, we first construct the Live Comment for Affective Analysis
(LCAffect) dataset which contains live comments for English and Chinese videos
spanning diverse genres that elicit a wide spectrum of emotions. Then, using
this dataset, we use contrastive learning to train a video encoder to produce
synthetic live comment features for enhanced multimodal affective content
analysis. Through comprehensive experimentation on a wide range of affective
analysis tasks (sentiment, emotion recognition, and sarcasm detection) in both
English and Chinese, we demonstrate that these synthetic live comment features
significantly improve performance over state-of-the-art methods.",2024-10-21 18:19:09+00:00,"['Zhaoyuan Deng', 'Amith Ananthram', 'Kathleen McKeown']",http://arxiv.org/abs/2410.16407v1
Event-guided Low-light Video Semantic Segmentation,"Recent video semantic segmentation (VSS) methods have demonstrated promising
results in well-lit environments. However, their performance significantly
drops in low-light scenarios due to limited visibility and reduced contextual
details. In addition, unfavorable low-light conditions make it harder to
incorporate temporal consistency across video frames and thus, lead to video
flickering effects. Compared with conventional cameras, event cameras can
capture motion dynamics, filter out temporal-redundant information, and are
robust to lighting conditions. To this end, we propose EVSNet, a lightweight
framework that leverages event modality to guide the learning of a unified
illumination-invariant representation. Specifically, we leverage a Motion
Extraction Module to extract short-term and long-term temporal motions from
event modality and a Motion Fusion Module to integrate image features and
motion features adaptively. Furthermore, we use a Temporal Decoder to exploit
video contexts and generate segmentation predictions. Such designs in EVSNet
result in a lightweight architecture while achieving SOTA performance.
Experimental results on 3 large-scale datasets demonstrate our proposed EVSNet
outperforms SOTA methods with up to 11x higher parameter efficiency.",2024-11-01 14:54:34+00:00,"['Zhen Yao', 'Mooi Choo Chuah']",http://arxiv.org/abs/2411.00639v1
Referring Video Object Segmentation via Language-aligned Track Selection,"Referring video object segmentation (RVOS) requires tracking and segmenting
an object throughout a video according to a given natural language expression,
demanding both complex motion understanding and the alignment of visual
representations with language descriptions. Given these challenges, the
recently proposed Segment Anything Model 2 (SAM2) emerges as a potential
candidate due to its ability to generate coherent segmentation mask tracks
across video frames, and provide an inherent spatio-temporal objectness in its
object token representations. In this paper, we introduce SOLA (Selection by
Object Language Alignment), a novel framework that leverages SAM2 object tokens
as compact video-level object representations, which are aligned with language
features through a lightweight track selection module. To effectively
facilitate this alignment, we propose an IoU-based pseudo-labeling strategy,
which bridges the modality gap between SAM2 representations with language
features. Extensive experiments show that SOLA achieves state-of-the-art
performance on the MeViS dataset and demonstrate that SOLA offers an effective
solution for RVOS. Our project page is available at:
https://cvlab-kaist.github.io/SOLA.",2024-12-02 05:20:35+00:00,"['Seongchan Kim', 'Woojeong Jin', 'Sangbeom Lim', 'Heeji Yoon', 'Hyunwook Choi', 'Seungryong Kim']",http://arxiv.org/abs/2412.01136v2
Experimental Study of Low-Latency Video Streaming in an ORAN Setup with Generative AI,"Video streaming services depend on the underlying communication
infrastructure and available network resources to offer ultra-low latency,
high-quality content delivery. Open Radio Access Network (ORAN) provides a
dynamic, programmable, and flexible RAN architecture that can be configured to
support the requirements of time-critical applications. This work considers a
setup in which the constrained network resources are supplemented by \gls{GAI}
and \gls{MEC} {techniques} in order to reach a satisfactory video quality.
Specifically, we implement a novel semantic control channel that enables
\gls{MEC} to support low-latency applications by tight coupling among the ORAN
xApp, \gls{MEC}, and the control channel. The proposed concepts are
experimentally verified with an actual ORAN setup that supports video
streaming. The performance evaluation includes the \gls{PSNR} metric and
end-to-end latency. Our findings reveal that latency adjustments can yield
gains in image \gls{PSNR}, underscoring the trade-off potential for optimized
video quality in resource-limited environments.",2024-12-17 10:15:46+00:00,"['Andreas Casparsen', 'Van-Phuc Bui', 'Shashi Raj Pandey', 'Jimmy Jessen Nielsen', 'Petar Popovski']",http://arxiv.org/abs/2412.12751v1
ESVQA: Perceptual Quality Assessment of Egocentric Spatial Videos,"With the rapid development of eXtended Reality (XR), egocentric spatial
shooting and display technologies have further enhanced immersion and
engagement for users. Assessing the quality of experience (QoE) of egocentric
spatial videos is crucial to ensure a high-quality viewing experience. However,
the corresponding research is still lacking. In this paper, we use the embodied
experience to highlight this more immersive experience and study the new
problem, i.e., embodied perceptual quality assessment for egocentric spatial
videos. Specifically, we introduce the first Egocentric Spatial Video Quality
Assessment Database (ESVQAD), which comprises 600 egocentric spatial videos and
their mean opinion scores (MOSs). Furthermore, we propose a novel
multi-dimensional binocular feature fusion model, termed ESVQAnet, which
integrates binocular spatial, motion, and semantic features to predict the
perceptual quality. Experimental results demonstrate the ESVQAnet outperforms
16 state-of-the-art VQA models on the embodied perceptual quality assessment
task, and exhibits strong generalization capability on traditional VQA tasks.
The database and codes will be released upon the publication.",2024-12-29 10:13:30+00:00,"['Xilei Zhu', 'Huiyu Duan', 'Liu Yang', 'Yucheng Zhu', 'Xiongkuo Min', 'Guangtao Zhai', 'Patrick Le Callet']",http://arxiv.org/abs/2412.20423v1
GenXD: Generating Any 3D and 4D Scenes,"Recent developments in 2D visual generation have been remarkably successful.
However, 3D and 4D generation remain challenging in real-world applications due
to the lack of large-scale 4D data and effective model design. In this paper,
we propose to jointly investigate general 3D and 4D generation by leveraging
camera and object movements commonly observed in daily life. Due to the lack of
real-world 4D data in the community, we first propose a data curation pipeline
to obtain camera poses and object motion strength from videos. Based on this
pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.
By leveraging all the 3D and 4D data, we develop our framework, GenXD, which
allows us to produce any 3D or 4D scene. We propose multiview-temporal modules,
which disentangle camera and object movements, to seamlessly learn from both 3D
and 4D data. Additionally, GenXD employs masked latent conditions to support a
variety of conditioning views. GenXD can generate videos that follow the camera
trajectory as well as consistent 3D views that can be lifted into 3D
representations. We perform extensive evaluations across various real-world and
synthetic datasets, demonstrating GenXD's effectiveness and versatility
compared to previous methods in 3D and 4D generation.",2024-11-04 17:45:44+00:00,"['Yuyang Zhao', 'Chung-Ching Lin', 'Kevin Lin', 'Zhiwen Yan', 'Linjie Li', 'Zhengyuan Yang', 'Jianfeng Wang', 'Gim Hee Lee', 'Lijuan Wang']",http://arxiv.org/abs/2411.02319v2
Nonlinear Inverse Design of Mechanical Multi-Material Metamaterials Enabled by Video Denoising Diffusion and Structure Identifier,"Metamaterials, synthetic materials with customized properties, have emerged
as a promising field due to advancements in additive manufacturing. These
materials derive unique mechanical properties from their internal lattice
structures, which are often composed of multiple materials that repeat
geometric patterns. While traditional inverse design approaches have shown
potential, they struggle to map nonlinear material behavior to multiple
possible structural configurations. This paper presents a novel framework
leveraging video diffusion models, a type of generative artificial Intelligence
(AI), for inverse multi-material design based on nonlinear stress-strain
responses. Our approach consists of two key components: (1) a fields generator
using a video diffusion model to create solution fields based on target
nonlinear stress-strain responses, and (2) a structure identifier employing two
UNet models to determine the corresponding multi-material 2D design. By
incorporating multiple materials, plasticity, and large deformation, our
innovative design method allows for enhanced control over the highly nonlinear
mechanical behavior of metamaterials commonly seen in real-world applications.
It offers a promising solution for generating next-generation metamaterials
with finely tuned mechanical characteristics.",2024-09-20 21:26:15+00:00,"['Jaewan Park', 'Shashank Kushwaha', 'Junyan He', 'Seid Koric', 'Qibang Liu', 'Iwona Jasiuk', 'Diab Abueidda']",http://arxiv.org/abs/2409.13908v2
Emu3: Next-Token Prediction is All You Need,"While next-token prediction is considered a promising path towards artificial
general intelligence, it has struggled to excel in multimodal tasks, which are
still dominated by diffusion models (e.g., Stable Diffusion) and compositional
approaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a
new suite of state-of-the-art multimodal models trained solely with next-token
prediction. By tokenizing images, text, and videos into a discrete space, we
train a single transformer from scratch on a mixture of multimodal sequences.
Emu3 outperforms several well-established task-specific models in both
generation and perception tasks, surpassing flagship models such as SDXL and
LLaVA-1.6, while eliminating the need for diffusion or compositional
architectures. Emu3 is also capable of generating high-fidelity video via
predicting the next token in a video sequence. We simplify complex multimodal
model designs by converging on a singular focus: tokens, unlocking great
potential for scaling both during training and inference. Our results
demonstrate that next-token prediction is a promising path towards building
general multimodal intelligence beyond language. We open-source key techniques
and models to support further research in this direction.",2024-09-27 16:06:11+00:00,"['Xinlong Wang', 'Xiaosong Zhang', 'Zhengxiong Luo', 'Quan Sun', 'Yufeng Cui', 'Jinsheng Wang', 'Fan Zhang', 'Yueze Wang', 'Zhen Li', 'Qiying Yu', 'Yingli Zhao', 'Yulong Ao', 'Xuebin Min', 'Tao Li', 'Boya Wu', 'Bo Zhao', 'Bowen Zhang', 'Liangdong Wang', 'Guang Liu', 'Zheqi He', 'Xi Yang', 'Jingjing Liu', 'Yonghua Lin', 'Tiejun Huang', 'Zhongyuan Wang']",http://arxiv.org/abs/2409.18869v1
CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation,"Despite the significant success achieved by deep learning methods in medical
image segmentation, researchers still struggle in the computer-aided diagnosis
of abdominal lymph nodes due to the complex abdominal environment, small and
indistinguishable lesions, and limited annotated data. To address these
problems, we present a pipeline that integrates the conditional diffusion model
for lymph node generation and the nnU-Net model for lymph node segmentation to
improve the segmentation performance of abdominal lymph nodes through
synthesizing a diversity of realistic abdominal lymph node data. We propose
LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph
node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical
structure masks as model conditions. These conditions work in two conditioning
mechanisms: global structure conditioning and local detail conditioning, to
distinguish between lymph nodes and their surroundings and better capture lymph
node characteristics. The obtained paired abdominal lymph node images and masks
are used for the downstream segmentation task. Experimental results on the
abdominal lymph node datasets demonstrate that LN-DDPM outperforms other
generative methods in the abdominal lymph node image synthesis and better
assists the downstream abdominal lymph node segmentation task.",2024-03-26 14:59:11+00:00,"['Yongrui Yu', 'Hanyu Chen', 'Zitian Zhang', 'Qiong Xiao', 'Wenhui Lei', 'Linrui Dai', 'Yu Fu', 'Hui Tan', 'Guan Wang', 'Peng Gao', 'Xiaofan Zhang']",http://arxiv.org/abs/2403.17770v1
PEGASUS: Personalized Generative 3D Avatars with Composable Attributes,"We present PEGASUS, a method for constructing a personalized generative 3D
face avatar from monocular video sources. Our generative 3D avatar enables
disentangled controls to selectively alter the facial attributes (e.g., hair or
nose) while preserving the identity. Our approach consists of two stages:
synthetic database generation and constructing a personalized generative
avatar. We generate a synthetic video collection of the target identity with
varying facial attributes, where the videos are synthesized by borrowing the
attributes from monocular videos of diverse identities. Then, we build a
person-specific generative 3D avatar that can modify its attributes
continuously while preserving its identity. Through extensive experiments, we
demonstrate that our method of generating a synthetic database and creating a
3D generative avatar is the most effective in preserving identity while
achieving high realism. Subsequently, we introduce a zero-shot approach to
achieve the same goal of generative modeling more efficiently by leveraging a
previously constructed personalized generative model.",2024-02-16 12:35:35+00:00,"['Hyunsoo Cha', 'Byungjun Kim', 'Hanbyul Joo']",http://arxiv.org/abs/2402.10636v2
RefDrop: Controllable Consistency in Image or Video Generation via Reference Feature Guidance,"There is a rapidly growing interest in controlling consistency across
multiple generated images using diffusion models. Among various methods, recent
works have found that simply manipulating attention modules by concatenating
features from multiple reference images provides an efficient approach to
enhancing consistency without fine-tuning. Despite its popularity and success,
few studies have elucidated the underlying mechanisms that contribute to its
effectiveness. In this work, we reveal that the popular approach is a linear
interpolation of image self-attention and cross-attention between synthesized
content and reference features, with a constant rank-1 coefficient. Motivated
by this observation, we find that a rank-1 coefficient is not necessary and
simplifies the controllable generation mechanism. The resulting algorithm,
which we coin as RefDrop, allows users to control the influence of reference
context in a direct and precise manner. Besides further enhancing consistency
in single-subject image generation, our method also enables more interesting
applications, such as the consistent generation of multiple subjects,
suppressing specific features to encourage more diverse content, and
high-quality personalized video generation by boosting temporal consistency.
Even compared with state-of-the-art image-prompt-based generators, such as
IP-Adapter, RefDrop is competitive in terms of controllability and quality
while avoiding the need to train a separate image encoder for feature injection
from reference images, making it a versatile plug-and-play solution for any
image or video diffusion model.",2024-05-27 21:23:20+00:00,"['Jiaojiao Fan', 'Haotian Xue', 'Qinsheng Zhang', 'Yongxin Chen']",http://arxiv.org/abs/2405.17661v1
Improving text-conditioned latent diffusion for cancer pathology,"The development of generative models in the past decade has allowed for
hyperrealistic data synthesis. While potentially beneficial, this synthetic
data generation process has been relatively underexplored in cancer
histopathology. One algorithm for synthesising a realistic image is diffusion;
it iteratively converts an image to noise and learns the recovery process from
this noise [Wang and Vastola, 2023]. While effective, it is highly
computationally expensive for high-resolution images, rendering it infeasible
for histopathology. The development of Variational Autoencoders (VAEs) has
allowed us to learn the representation of complex high-resolution images in a
latent space. A vital by-product of this is the ability to compress
high-resolution images to space and recover them lossless. The marriage of
diffusion and VAEs allows us to carry out diffusion in the latent space of an
autoencoder, enabling us to leverage the realistic generative capabilities of
diffusion while maintaining reasonable computational requirements. Rombach et
al. [2021b] and Yellapragada et al. [2023] build foundational models for this
task, paving the way to generate realistic histopathology images. In this
paper, we discuss the pitfalls of current methods, namely [Yellapragada et al.,
2023] and resolve critical errors while proposing improvements along the way.
Our methods achieve an FID score of 21.11, beating its SOTA counterparts in
[Yellapragada et al., 2023] by 1.2 FID, while presenting a train-time GPU
memory usage reduction of 7%.",2024-12-09 13:38:19+00:00,"['Aakash Madhav Rao', 'Debayan Gupta']",http://arxiv.org/abs/2412.06487v1
AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production,"The Agent and AIGC (Artificial Intelligence Generated Content) technologies
have recently made significant progress. We propose AesopAgent, an Agent-driven
Evolutionary System on Story-to-Video Production. AesopAgent is a practical
application of agent technology for multimodal content generation. The system
integrates multiple generative capabilities within a unified framework, so that
individual users can leverage these modules easily. This innovative system
would convert user story proposals into scripts, images, and audio, and then
integrate these multimodal contents into videos. Additionally, the animating
units (e.g., Gen-2 and Sora) could make the videos more infectious. The
AesopAgent system could orchestrate task workflow for video generation,
ensuring that the generated video is both rich in content and coherent. This
system mainly contains two layers, i.e., the Horizontal Layer and the Utility
Layer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary
system that optimizes the whole video generation workflow and the steps within
the workflow. It continuously evolves and iteratively optimizes workflow by
accumulating expert experience and professional knowledge, including optimizing
the LLM prompts and utilities usage. The Utility Layer provides multiple
utilities, leading to consistent image generation that is visually coherent in
terms of composition, characters, and style. Meanwhile, it provides audio and
special effects, integrating them into expressive and logically arranged
videos. Overall, our AesopAgent achieves state-of-the-art performance compared
with many previous works in visual storytelling. Our AesopAgent is designed for
convenient service for individual users, which is available on the following
page: https://aesopai.github.io/.",2024-03-12 02:30:50+00:00,"['Jiuniu Wang', 'Zehua Du', 'Yuyuan Zhao', 'Bo Yuan', 'Kexiang Wang', 'Jian Liang', 'Yaxi Zhao', 'Yihen Lu', 'Gengliang Li', 'Junlong Gao', 'Xin Tu', 'Zhenyu Guo']",http://arxiv.org/abs/2403.07952v1
The NES Video-Music Database: A Dataset of Symbolic Video Game Music Paired with Gameplay Videos,"Neural models are one of the most popular approaches for music generation,
yet there aren't standard large datasets tailored for learning music directly
from game data. To address this research gap, we introduce a novel dataset
named NES-VMDB, containing 98,940 gameplay videos from 389 NES games, each
paired with its original soundtrack in symbolic format (MIDI). NES-VMDB is
built upon the Nintendo Entertainment System Music Database (NES-MDB),
encompassing 5,278 music pieces from 397 NES games. Our approach involves
collecting long-play videos for 389 games of the original dataset, slicing them
into 15-second-long clips, and extracting the audio from each clip.
Subsequently, we apply an audio fingerprinting algorithm (similar to Shazam) to
automatically identify the corresponding piece in the NES-MDB dataset.
Additionally, we introduce a baseline method based on the Controllable Music
Transformer to generate NES music conditioned on gameplay clips. We evaluated
this approach with objective metrics, and the results showed that the
conditional CMT improves musical structural quality when compared to its
unconditional counterpart. Moreover, we used a neural classifier to predict the
game genre of the generated pieces. Results showed that the CMT generator can
learn correlations between gameplay videos and game genres, but further
research has to be conducted to achieve human-level performance.",2024-04-05 21:41:20+00:00,"['Igor Cardoso', 'Rubens O. Moraes', 'Lucas N. Ferreira']",http://arxiv.org/abs/2404.04420v1
AniClipart: Clipart Animation with Text-to-Video Priors,"Clipart, a pre-made art form, offers a convenient and efficient way of
creating visual content. However, traditional workflows for animating static
clipart are laborious and time-consuming, involving steps like rigging,
keyframing, and inbetweening. Recent advancements in text-to-video generation
hold great potential in resolving this challenge. Nevertheless, direct
application of text-to-video models often struggles to preserve the visual
identity of clipart or generate cartoon-style motion, resulting in subpar
animation outcomes. In this paper, we introduce AniClipart, a computational
system that converts static clipart into high-quality animations guided by
text-to-video priors. To generate natural, smooth, and coherent motion, we
first parameterize the motion trajectories of the keypoints defined over the
initial clipart image by cubic B\'ezier curves. We then align these motion
trajectories with a given text prompt by optimizing a video Score Distillation
Sampling (SDS) loss and a skeleton fidelity loss. By incorporating
differentiable As-Rigid-As-Possible (ARAP) shape deformation and differentiable
rendering, AniClipart can be end-to-end optimized while maintaining deformation
rigidity. Extensive experimental results show that the proposed AniClipart
consistently outperforms the competing methods, in terms of text-video
alignment, visual identity preservation, and temporal consistency.
Additionally, we showcase the versatility of AniClipart by adapting it to
generate layered animations, which allow for topological changes.",2024-04-18 17:24:28+00:00,"['Ronghuan Wu', 'Wanchao Su', 'Kede Ma', 'Jing Liao']",http://arxiv.org/abs/2404.12347v2
Sentiment-oriented Transformer-based Variational Autoencoder Network for Live Video Commenting,"Automatic live video commenting is with increasing attention due to its
significance in narration generation, topic explanation, etc. However, the
diverse sentiment consideration of the generated comments is missing from the
current methods. Sentimental factors are critical in interactive commenting,
and lack of research so far. Thus, in this paper, we propose a
Sentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network
which consists of a sentiment-oriented diversity encoder module and a batch
attention module, to achieve diverse video commenting with multiple sentiments
and multiple semantics. Specifically, our sentiment-oriented diversity encoder
elegantly combines VAE and random mask mechanism to achieve semantic diversity
under sentiment guidance, which is then fused with cross-modal features to
generate live video comments. Furthermore, a batch attention module is also
proposed in this paper to alleviate the problem of missing sentimental samples,
caused by the data imbalance, which is common in live videos as the popularity
of videos varies. Extensive experiments on Livebot and VideoIC datasets
demonstrate that the proposed So-TVAE outperforms the state-of-the-art methods
in terms of the quality and diversity of generated comments. Related code is
available at https://github.com/fufy1024/So-TVAE.",2024-04-19 10:43:25+00:00,"['Fengyi Fu', 'Shancheng Fang', 'Weidong Chen', 'Zhendong Mao']",http://arxiv.org/abs/2404.12782v1
FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance,"Synthesizing motion-rich and temporally consistent videos remains a challenge
in artificial intelligence, especially when dealing with extended durations.
Existing text-to-video (T2V) models commonly employ spatial cross-attention for
text control, equivalently guiding different frame generations without
frame-specific textual guidance. Thus, the model's capacity to comprehend the
temporal logic conveyed in prompts and generate videos with coherent motion is
restricted. To tackle this limitation, we introduce FancyVideo, an innovative
video generator that improves the existing text-control mechanism with the
well-designed Cross-frame Textual Guidance Module (CTGM). Specifically, CTGM
incorporates the Temporal Information Injector (TII), Temporal Affinity Refiner
(TAR), and Temporal Feature Booster (TFB) at the beginning, middle, and end of
cross-attention, respectively, to achieve frame-specific textual guidance.
Firstly, TII injects frame-specific information from latent features into text
conditions, thereby obtaining cross-frame textual conditions. Then, TAR refines
the correlation matrix between cross-frame textual conditions and latent
features along the time dimension. Lastly, TFB boosts the temporal consistency
of latent features. Extensive experiments comprising both quantitative and
qualitative evaluations demonstrate the effectiveness of FancyVideo. Our video
demo, code and model are available at https://360cvgroup.github.io/FancyVideo/.",2024-08-15 14:47:44+00:00,"['Jiasong Feng', 'Ao Ma', 'Jing Wang', 'Bo Cheng', 'Xiaodan Liang', 'Dawei Leng', 'Yuhui Yin']",http://arxiv.org/abs/2408.08189v2
Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes,"In recent years, DeepFake technology has achieved unprecedented success in
high-quality video synthesis, but these methods also pose potential and severe
security threats to humanity. DeepFake can be bifurcated into entertainment
applications like face swapping and illicit uses such as lip-syncing fraud.
However, lip-forgery videos, which neither change identity nor have discernible
visual artifacts, present a formidable challenge to existing DeepFake detection
methods. Our preliminary experiments have shown that the effectiveness of the
existing methods often drastically decrease or even fail when tackling
lip-syncing videos. In this paper, for the first time, we propose a novel
approach dedicated to lip-forgery identification that exploits the
inconsistency between lip movements and audio signals. We also mimic human
natural cognition by capturing subtle biological links between lips and head
regions to boost accuracy. To better illustrate the effectiveness and advances
of our proposed method, we create a high-quality LipSync dataset, AVLips, by
employing the state-of-the-art lip generators. We hope this high-quality and
diverse dataset could be well served the further research on this challenging
and interesting field. Experimental results show that our approach gives an
average accuracy of more than 95.3% in spotting lip-syncing videos,
significantly outperforming the baselines. Extensive experiments demonstrate
the capability to tackle deepfakes and the robustness in surviving diverse
input transformations. Our method achieves an accuracy of up to 90.2% in
real-world scenarios (e.g., WeChat video call) and shows its powerful
capabilities in real scenario deployment. To facilitate the progress of this
research community, we release all resources at
https://github.com/AaronComo/LipFD.",2024-01-28 14:22:11+00:00,"['Weifeng Liu', 'Tianyi She', 'Jiawei Liu', 'Boheng Li', 'Dongyu Yao', 'Ziyou Liang', 'Run Wang']",http://arxiv.org/abs/2401.15668v2
Sequential Posterior Sampling with Diffusion Models,"Diffusion models have quickly risen in popularity for their ability to model
complex distributions and perform effective posterior sampling. Unfortunately,
the iterative nature of these generative models makes them computationally
expensive and unsuitable for real-time sequential inverse problems such as
ultrasound imaging. Considering the strong temporal structure across sequences
of frames, we propose a novel approach that models the transition dynamics to
improve the efficiency of sequential diffusion posterior sampling in
conditional image synthesis. Through modeling sequence data using a video
vision transformer (ViViT) transition model based on previous diffusion
outputs, we can initialize the reverse diffusion trajectory at a lower noise
scale, greatly reducing the number of iterations required for convergence. We
demonstrate the effectiveness of our approach on a real-world dataset of high
frame rate cardiac ultrasound images and show that it achieves the same
performance as a full diffusion trajectory while accelerating inference
25$\times$, enabling real-time posterior sampling. Furthermore, we show that
the addition of a transition model improves the PSNR up to 8\% in cases with
severe motion. Our method opens up new possibilities for real-time applications
of diffusion models in imaging and other domains requiring real-time inference.",2024-09-09 07:55:59+00:00,"['Tristan S. W. Stevens', 'Oisn Nolan', 'Jean-Luc Robert', 'Ruud J. G. van Sloun']",http://arxiv.org/abs/2409.05399v1
Stochastic Deep Restoration Priors for Imaging Inverse Problems,"Deep neural networks trained as image denoisers are widely used as priors for
solving imaging inverse problems. While Gaussian denoising is thought
sufficient for learning image priors, we show that priors from deep models
pre-trained as more general restoration operators can perform better. We
introduce Stochastic deep Restoration Priors (ShaRP), a novel method that
leverages an ensemble of such restoration models to regularize inverse
problems. ShaRP improves upon methods using Gaussian denoiser priors by better
handling structured artifacts and enabling self-supervised training even
without fully sampled data. We prove ShaRP minimizes an objective function
involving a regularizer derived from the score functions of minimum mean square
error (MMSE) restoration operators, and theoretically analyze its convergence.
Empirically, ShaRP achieves state-of-the-art performance on tasks such as
magnetic resonance imaging reconstruction and single-image super-resolution,
surpassing both denoiser-and diffusion-model-based methods without requiring
retraining.",2024-10-02 22:06:49+00:00,"['Yuyang Hu', 'Albert Peng', 'Weijie Gan', 'Peyman Milanfar', 'Mauricio Delbracio', 'Ulugbek S. Kamilov']",http://arxiv.org/abs/2410.02057v1
Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Image Synthesis: T1 MRI to Tau-PET,"Alzheimer's Disease (AD) is the most common form of dementia, characterised
by cognitive decline and biomarkers such as tau-proteins. Tau-positron emission
tomography (tau-PET), which employs a radiotracer to selectively bind, detect,
and visualise tau protein aggregates within the brain, is valuable for early AD
diagnosis but is less accessible due to high costs, limited availability, and
its invasive nature. Image synthesis with neural networks enables the
generation of tau-PET images from more accessible T1-weighted magnetic
resonance imaging (MRI) images. To ensure high-quality image synthesis, we
propose a cyclic 2.5D perceptual loss combined with mean squared error and
structural similarity index measure (SSIM) losses. The cyclic 2.5D perceptual
loss sequentially calculates the axial 2D average perceptual loss for a
specified number of epochs, followed by the coronal and sagittal planes for the
same number of epochs. This sequence is cyclically performed, with intervals
reducing as the cycles repeat. We conduct supervised synthesis of tau-PET
images from T1w MRI images using 516 paired T1w MRI and tau-PET 3D images from
the ADNI database. For the collected data, we perform preprocessing, including
intensity standardisation for tau-PET images from each manufacturer. The
proposed loss, applied to generative 3D U-Net and its variants, outperformed
those with 2.5D and 3D perceptual losses in SSIM and peak signal-to-noise ratio
(PSNR). In addition, including the cyclic 2.5D perceptual loss to the original
losses of GAN-based image synthesis models such as CycleGAN and Pix2Pix
improves SSIM and PSNR by at least 2% and 3%. Furthermore, by-manufacturer PET
standardisation helps the models in synthesising high-quality images than
min-max PET normalisation.",2024-06-18 13:59:10+00:00,"['Symac Kim', 'Junho Moon', 'Haejun Chung', 'Ikbeom Jang']",http://arxiv.org/abs/2406.12632v1
Accelerating Vision Diffusion Transformers with Skip Branches,"Diffusion Transformers (DiT), an emerging image and video generation model
architecture, has demonstrated great potential because of its high generation
quality and scalability properties. Despite the impressive performance, its
practical deployment is constrained by computational complexity and redundancy
in the sequential denoising process. While feature caching across timesteps has
proven effective in accelerating diffusion models, its application to DiT is
limited by fundamental architectural differences from U-Net-based approaches.
Through empirical analysis of DiT feature dynamics, we identify that
significant feature variation between DiT blocks presents a key challenge for
feature reusability. To address this, we convert standard DiT into Skip-DiT
with skip branches to enhance feature smoothness. Further, we introduce
Skip-Cache which utilizes the skip branches to cache DiT features across
timesteps at the inference time. We validated effectiveness of our proposal on
different DiT backbones for video and image generation, showcasing skip
branches to help preserve generation quality and achieve higher speedup.
Experimental results indicate that Skip-DiT achieves a 1.5x speedup almost for
free and a 2.2x speedup with only a minor reduction in quantitative metrics.
Code is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",2024-11-26 17:28:10+00:00,"['Guanjie Chen', 'Xinyu Zhao', 'Yucheng Zhou', 'Tianlong Chen', 'Yu Cheng']",http://arxiv.org/abs/2411.17616v2
Physics-based Scene Layout Generation from Human Motion,"Creating scenes for captured motions that achieve realistic human-scene
interaction is crucial for 3D animation in movies or video games. As character
motion is often captured in a blue-screened studio without real furniture or
objects in place, there may be a discrepancy between the planned motion and the
captured one. This gives rise to the need for automatic scene layout generation
to relieve the burdens of selecting and positioning furniture and objects.
Previous approaches cannot avoid artifacts like penetration and floating due to
the lack of physical constraints. Furthermore, some heavily rely on specific
data to learn the contact affordances, restricting the generalization ability
to different motions. In this work, we present a physics-based approach that
simultaneously optimizes a scene layout generator and simulates a moving human
in a physics simulator. To attain plausible and realistic interaction motions,
our method explicitly introduces physical constraints. To automatically recover
and generate the scene layout, we minimize the motion tracking errors to
identify the objects that can afford interaction. We use reinforcement learning
to perform a dual-optimization of both the character motion imitation
controller and the scene layout generator. To facilitate the optimization, we
reshape the tracking rewards and devise pose prior guidance obtained from our
estimated pseudo-contact labels. We evaluate our method using motions from SAMP
and PROX, and demonstrate physically plausible scene layout reconstruction
compared with the previous kinematics-based method.",2024-05-21 02:36:37+00:00,"['Jianan Li', 'Tao Huang', 'Qingxu Zhu', 'Tien-Tsin Wong']",http://arxiv.org/abs/2405.12460v1
Expression-aware video inpainting for HMD removal in XR applications,"Head-mounted displays (HMDs) serve as indispensable devices for observing
extended reality (XR) environments and virtual content. However, HMDs present
an obstacle to external recording techniques as they block the upper face of
the user. This limitation significantly affects social XR applications,
specifically teleconferencing, where facial features and eye gaze information
play a vital role in creating an immersive user experience. In this study, we
propose a new network for expression-aware video inpainting for HMD removal
(EVI-HRnet) based on generative adversarial networks (GANs). Our model
effectively fills in missing information with regard to facial landmarks and a
single occlusion-free reference image of the user. The framework and its
components ensure the preservation of the user's identity across frames using
the reference frame. To further improve the level of realism of the inpainted
output, we introduce a novel facial expression recognition (FER) loss function
for emotion preservation. Our results demonstrate the remarkable capability of
the proposed framework to remove HMDs from facial videos while maintaining the
subject's facial expression and identity. Moreover, the outputs exhibit
temporal consistency along the inpainted frames. This lightweight framework
presents a practical approach for HMD occlusion removal, with the potential to
enhance various collaborative XR applications without the need for additional
hardware.",2024-01-25 12:32:21+00:00,"['Fatemeh Ghorbani Lohesara', 'Karen Egiazarian', 'Sebastian Knorr']",http://arxiv.org/abs/2401.14136v1
Functional Imaging Constrained Diffusion for Brain PET Synthesis from Structural MRI,"Magnetic resonance imaging (MRI) and positron emission tomography (PET) are
increasingly used in multimodal analysis of neurodegenerative disorders. While
MRI is broadly utilized in clinical settings, PET is less accessible. Many
studies have attempted to use deep generative models to synthesize PET from MRI
scans. However, they often suffer from unstable training and inadequately
preserve brain functional information conveyed by PET. To this end, we propose
a functional imaging constrained diffusion (FICD) framework for 3D brain PET
image synthesis with paired structural MRI as input condition, through a new
constrained diffusion model (CDM). The FICD introduces noise to PET and then
progressively removes it with CDM, ensuring high output fidelity throughout a
stable training phase. The CDM learns to predict denoised PET with a functional
imaging constraint introduced to ensure voxel-wise alignment between each
denoised PET and its ground truth. Quantitative and qualitative analyses
conducted on 293 subjects with paired T1-weighted MRI and
18F-fluorodeoxyglucose (FDG)-PET scans suggest that FICD achieves superior
performance in generating FDG-PET data compared to state-of-the-art methods. We
further validate the effectiveness of the proposed FICD on data from a total of
1,262 subjects through three downstream tasks, with experimental results
suggesting its utility and generalizability.",2024-05-03 22:33:46+00:00,"['Minhui Yu', 'Mengqi Wu', 'Ling Yue', 'Andrea Bozoki', 'Mingxia Liu']",http://arxiv.org/abs/2405.02504v3
OpFlowTalker: Realistic and Natural Talking Face Generation via Optical Flow Guidance,"Creating realistic, natural, and lip-readable talking face videos remains a
formidable challenge. Previous research primarily concentrated on generating
and aligning single-frame images while overlooking the smoothness of
frame-to-frame transitions and temporal dependencies. This often compromised
visual quality and effects in practical settings, particularly when handling
complex facial data and audio content, which frequently led to semantically
incongruent visual illusions. Specifically, synthesized videos commonly
featured disorganized lip movements, making them difficult to understand and
recognize. To overcome these limitations, this paper introduces the application
of optical flow to guide facial image generation, enhancing inter-frame
continuity and semantic consistency. We propose ""OpFlowTalker"", a novel
approach that utilizes predicted optical flow changes from audio inputs rather
than direct image predictions. This method smooths image transitions and aligns
changes with semantic content. Moreover, it employs a sequence fusion technique
to replace the independent generation of single frames, thus preserving
contextual information and maintaining temporal coherence. We also developed an
optical flow synchronization module that regulates both full-face and lip
movements, optimizing visual synthesis by balancing regional dynamics.
Furthermore, we introduce a Visual Text Consistency Score (VTCS) that
accurately measures lip-readability in synthesized videos. Extensive empirical
evidence validates the effectiveness of our approach.",2024-05-23 15:42:34+00:00,"['Shuheng Ge', 'Haoyu Xing', 'Li Zhang', 'Xiangqian Wu']",http://arxiv.org/abs/2405.14709v2
MCGAN: Enhancing GAN Training with Regression-Based Generator Loss,"Generative adversarial networks (GANs) have emerged as a powerful tool for
generating high-fidelity data. However, the main bottleneck of existing
approaches is the lack of supervision on the generator training, which often
results in undamped oscillation and unsatisfactory performance. To address this
issue, we propose an algorithm called Monte Carlo GAN (MCGAN). This approach,
utilizing an innovative generative loss function, termly the regression loss,
reformulates the generator training as a regression task and enables the
generator training by minimizing the mean squared error between the
discriminator's output of real data and the expected discriminator of fake
data. We demonstrate the desirable analytic properties of the regression loss,
including discriminability and optimality, and show that our method requires a
weaker condition on the discriminator for effective generator training. These
properties justify the strength of this approach to improve the training
stability while retaining the optimality of GAN by leveraging strong
supervision of the regression loss. Extensive experiments on diverse datasets,
including image data (CIFAR-10/100, FFHQ256, ImageNet, and LSUN Bedroom), time
series data (VAR and stock data) and video data, are conducted to demonstrate
the flexibility and effectiveness of our proposed MCGAN. Numerical results show
that the proposed MCGAN is versatile in enhancing a variety of backbone GAN
models and achieves consistent and significant improvement in terms of quality,
accuracy, training stability, and learned latent space.",2024-05-27 14:15:52+00:00,"['Baoren Xiao', 'Hao Ni', 'Weixin Yang']",http://arxiv.org/abs/2405.17191v3
Self-supervised learning of video representations from a child's perspective,"Children learn powerful internal models of the world around them from a few
years of egocentric visual experience. Can such internal models be learned from
a child's visual experience with highly generic learning algorithms or do they
require strong inductive biases? Recent advances in collecting large-scale,
longitudinal, developmentally realistic video datasets and generic
self-supervised learning (SSL) algorithms are allowing us to begin to tackle
this nature vs. nurture question. However, existing work typically focuses on
image-based SSL algorithms and visual capabilities that can be learned from
static images (e.g. object recognition), thus ignoring temporal aspects of the
world. To close this gap, here we train self-supervised video models on
longitudinal, egocentric headcam recordings collected from a child over a two
year period in their early development (6-31 months). The resulting models are
highly effective at facilitating the learning of action concepts from a small
number of labeled examples; they have favorable data size scaling properties;
and they display emergent video interpolation capabilities. Video models also
learn more accurate and more robust object representations than image-based
models trained with the exact same data. These results suggest that important
temporal aspects of a child's internal model of the world may be learnable from
their visual experience using highly generic learning algorithms and without
strong inductive biases.",2024-02-01 03:27:26+00:00,"['A. Emin Orhan', 'Wentao Wang', 'Alex N. Wang', 'Mengye Ren', 'Brenden M. Lake']",http://arxiv.org/abs/2402.00300v3
Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue,"Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which
aims to generate a natural language explanation for the given sarcastic
dialogue that involves multiple modalities (\ie utterance, video, and audio).
Although existing studies have achieved great success based on the generative
pretrained language model BART, they overlook exploiting the sentiments
residing in the utterance, video and audio, which play important roles in
reflecting sarcasm that essentially involves subtle sentiment contrasts.
Nevertheless, it is non-trivial to incorporate sentiments for boosting SED
performance, due to three main challenges: 1) diverse effects of utterance
tokens on sentiments; 2) gap between video-audio sentiment signals and the
embedding space of BART; and 3) various relations among utterances, utterance
sentiments, and video-audio sentiments. To tackle these challenges, we propose
a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation
framework, named EDGE. In particular, we first propose a lexicon-guided
utterance sentiment inference module, where a heuristic utterance sentiment
refinement strategy is devised. We then develop a module named Joint Cross
Attention-based Sentiment Inference (JCA-SI) by extending the multimodal
sentiment analysis model JCA to derive the joint sentiment label for each
video-audio clip. Thereafter, we devise a context-sentiment graph to
comprehensively model the semantic relations among the utterances, utterance
sentiments, and video-audio sentiments, to facilitate sarcasm explanation
generation. Extensive experiments on the publicly released dataset WITS verify
the superiority of our model over cutting-edge methods.",2024-02-06 03:14:46+00:00,"['Kun Ouyang', 'Liqiang Jing', 'Xuemeng Song', 'Meng Liu', 'Yupeng Hu', 'Liqiang Nie']",http://arxiv.org/abs/2402.03658v2
A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance,"Much research has highlighted the impressive capabilities of large language
models (LLMs), like GPT and Bard, for solving introductory programming
exercises. Recent work has shown that LLMs can effectively solve a range of
more complex object-oriented programming (OOP) exercises with text-based
specifications. This raises concerns about academic integrity, as students
might use these models to complete assignments unethically, neglecting the
development of important skills such as program design, problem-solving, and
computational thinking. To address this, we propose an innovative approach to
formulating OOP tasks using diagrams and videos, as a way to foster
problem-solving and deter students from a copy-and-prompt approach in OOP
courses. We introduce a novel notation system for specifying OOP assignments,
encompassing structural and behavioral requirements, and assess its use in a
classroom setting over a semester. Student perceptions of this approach are
explored through a survey (n=56). Generally, students responded positively to
diagrams and videos, with video-based projects being better received than
diagram-based exercises. This notation appears to have several benefits, with
students investing more effort in understanding the diagrams and feeling more
motivated to engage with the video-based projects. Furthermore, students
reported being less inclined to rely on LLM-based code generation tools for
these diagram and video-based exercises. Experiments with GPT-4 and Bard's
vision abilities revealed that they currently fall short in interpreting these
diagrams to generate accurate code solutions.",2024-03-13 10:21:29+00:00,"['Bruno Pereira Cipriano', 'Pedro Alves', 'Paul Denny']",http://arxiv.org/abs/2403.08396v1
RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos,"Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets. In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges, we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.",2024-03-27 14:22:40+00:00,"['Ali Zare', 'Yulei Niu', 'Hammad Ayyubi', 'Shih-fu Chang']",http://arxiv.org/abs/2403.18600v2
How Video Meetings Change Your Expression,"Do our facial expressions change when we speak over video calls? Given two
unpaired sets of videos of people, we seek to automatically find
spatio-temporal patterns that are distinctive of each set. Existing methods use
discriminative approaches and perform post-hoc explainability analysis. Such
methods are insufficient as they are unable to provide insights beyond obvious
dataset biases, and the explanations are useful only if humans themselves are
good at the task. Instead, we tackle the problem through the lens of generative
domain translation: our method generates a detailed report of learned,
input-dependent spatio-temporal features and the extent to which they vary
between the domains. We demonstrate that our method can discover behavioral
differences between conversing face-to-face (F2F) and on video-calls (VCs). We
also show the applicability of our method on discovering differences in
presidential communication styles. Additionally, we are able to predict
temporal change-points in videos that decouple expressions in an unsupervised
way, and increase the interpretability and usefulness of our model. Finally,
our method, being generative, can be used to transform a video call to appear
as if it were recorded in a F2F setting. Experiments and visualizations show
our approach is able to discover a range of behaviors, taking a step towards
deeper understanding of human behaviors.",2024-06-03 03:15:02+00:00,"['Sumit Sarin', 'Utkarsh Mall', 'Purva Tendulkar', 'Carl Vondrick']",http://arxiv.org/abs/2406.00955v1
POPCat: Propagation of particles for complex annotation tasks,"Novel dataset creation for all multi-object tracking, crowd-counting, and
industrial-based videos is arduous and time-consuming when faced with a unique
class that densely populates a video sequence. We propose a time efficient
method called POPCat that exploits the multi-target and temporal features of
video data to produce a semi-supervised pipeline for segmentation or box-based
video annotation. The method retains the accuracy level associated with human
level annotation while generating a large volume of semi-supervised annotations
for greater generalization. The method capitalizes on temporal features through
the use of a particle tracker to expand the domain of human-provided target
points. This is done through the use of a particle tracker to reassociate the
initial points to a set of images that follow the labeled frame. A YOLO model
is then trained with this generated data, and then rapidly infers on the target
video. Evaluations are conducted on GMOT-40, AnimalTrack, and Visdrone-2019
benchmarks. These multi-target video tracking/detection sets contain multiple
similar-looking targets, camera movements, and other features that would
commonly be seen in ""wild"" situations. We specifically choose these difficult
datasets to demonstrate the efficacy of the pipeline and for comparison
purposes. The method applied on GMOT-40, AnimalTrack, and Visdrone shows a
margin of improvement on recall/mAP50/mAP over the best results by a value of
24.5%/9.6%/4.8%, -/43.1%/27.8%, and 7.5%/9.4%/7.5% where metrics were
collected.",2024-06-24 23:43:08+00:00,"['Adam Srebrnjak Yang', 'Dheeraj Khanna', 'John S. Zelek']",http://arxiv.org/abs/2406.17183v1
Adapt2Reward: Adapting Video-Language Models to Generalizable Robotic Rewards via Failure Prompts,"For a general-purpose robot to operate in reality, executing a broad range of
instructions across various environments is imperative. Central to the
reinforcement learning and planning for such robotic agents is a generalizable
reward function. Recent advances in vision-language models, such as CLIP, have
shown remarkable performance in the domain of deep learning, paving the way for
open-domain visual recognition. However, collecting data on robots executing
various language instructions across multiple environments remains a challenge.
This paper aims to transfer video-language models with robust generalization
into a generalizable language-conditioned reward function, only utilizing robot
video data from a minimal amount of tasks in a singular environment. Unlike
common robotic datasets used for training reward functions, human
video-language datasets rarely contain trivial failure videos. To enhance the
model's ability to distinguish between successful and failed robot executions,
we cluster failure video features to enable the model to identify patterns
within. For each cluster, we integrate a newly trained failure prompt into the
text encoder to represent the corresponding failure mode. Our
language-conditioned reward function shows outstanding generalization to new
environments and new instructions for robot planning and reinforcement
learning.",2024-07-20 13:22:59+00:00,"['Yanting Yang', 'Minghao Chen', 'Qibo Qiu', 'Jiahao Wu', 'Wenxiao Wang', 'Binbin Lin', 'Ziyu Guan', 'Xiaofei He']",http://arxiv.org/abs/2407.14872v1
Everything is a Video: Unifying Modalities through Next-Frame Prediction,"Multimodal learning, which involves integrating information from various
modalities such as text, images, audio, and video, is pivotal for numerous
complex tasks like visual question answering, cross-modal retrieval, and
caption generation. Traditional approaches rely on modality-specific encoders
and late fusion techniques, which can hinder scalability and flexibility when
adapting to new tasks or modalities. To address these limitations, we introduce
a novel framework that extends the concept of task reformulation beyond natural
language processing (NLP) to multimodal learning. We propose to reformulate
diverse multimodal tasks into a unified next-frame prediction problem, allowing
a single model to handle different modalities without modality-specific
components. This method treats all inputs and outputs as sequential frames in a
video, enabling seamless integration of modalities and effective knowledge
transfer across tasks. Our approach is evaluated on a range of tasks, including
text-to-text, image-to-text, video-to-video, video-to-text, and audio-to-text,
demonstrating the model's ability to generalize across modalities with minimal
adaptation. We show that task reformulation can significantly simplify
multimodal model design across various tasks, laying the groundwork for more
generalized multimodal foundation models.",2024-11-15 12:59:37+00:00,"['G. Thomas Hudson', 'Dean Slack', 'Thomas Winterbottom', 'Jamie Sterling', 'Chenghao Xiao', 'Junjie Shentu', 'Noura Al Moubayed']",http://arxiv.org/abs/2411.10503v1
BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering,"The development of multi-modal models has been rapidly advancing, with some
demonstrating remarkable capabilities. However, annotating video-text pairs
remains expensive and insufficient. Take video question answering (VideoQA)
tasks as an example, human annotated questions and answers often cover only
part of the video, and similar semantics can also be expressed through
different text forms, leading to underutilization of video. To address this, we
propose BoViLA, a self-training framework that augments question samples during
training through LLM-based self-questioning and answering, which help model
exploit video information and the internal knowledge of LLMs more thoroughly to
improve modality alignment. To filter bad self-generated questions, we
introduce Evidential Deep Learning (EDL) to estimate uncertainty and assess the
quality of self-generated questions by evaluating the modality alignment within
the context. To the best of our knowledge, this work is the first to explore
LLM-based self-training frameworks for modality alignment. We evaluate BoViLA
on five strong VideoQA benchmarks, where it outperforms several
state-of-the-art methods and demonstrate its effectiveness and generality.
Additionally, we provide extensive analyses of the self-training framework and
the EDL-based uncertainty filtering mechanism. The code will be made available
at https://github.com/dunknsabsw/BoViLA.",2024-09-17 05:17:37+00:00,"['Jin Chen', 'Kaijing Ma', 'Haojian Huang', 'Jiayu Shen', 'Han Fang', 'Xianghao Zang', 'Chao Ban', 'Zhongjiang He', 'Hao Sun', 'Yanmei Kang']",http://arxiv.org/abs/2410.02768v1
Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts,"Despite recent advances in image-to-video generation, better controllability
and local animation are less explored. Most existing image-to-video methods are
not locally aware and tend to move the entire scene. However, human artists may
need to control the movement of different objects or regions. Additionally,
current I2V methods require users not only to describe the target motion but
also to provide redundant detailed descriptions of frame contents. These two
issues hinder the practical utilization of current I2V tools. In this paper, we
propose a practical framework, named Follow-Your-Click, to achieve image
animation with a simple user click (for specifying what to move) and a short
motion prompt (for specifying how to move). Technically, we propose the
first-frame masking strategy, which significantly improves the video generation
quality, and a motion-augmented module equipped with a short motion prompt
dataset to improve the short prompt following abilities of our model. To
further control the motion speed, we propose flow-based motion magnitude
control to control the speed of target movement more precisely. Our framework
has simpler yet precise user control and better generation performance than
previous methods. Extensive experiments compared with 7 baselines, including
both commercial tools and research methods on 8 metrics, suggest the
superiority of our approach. Project Page: https://follow-your-click.github.io/",2024-03-13 05:44:37+00:00,"['Yue Ma', 'Yingqing He', 'Hongfa Wang', 'Andong Wang', 'Chenyang Qi', 'Chengfei Cai', 'Xiu Li', 'Zhifeng Li', 'Heung-Yeung Shum', 'Wei Liu', 'Qifeng Chen']",http://arxiv.org/abs/2403.08268v1
Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection,"Video Anomaly Detection (VAD) is essential for computer vision research.
Existing VAD methods utilize either reconstruction-based or prediction-based
frameworks. The former excels at detecting irregular patterns or structures,
whereas the latter is capable of spotting abnormal deviations or trends. We
address pose-based video anomaly detection and introduce a novel framework
called Dual Conditioned Motion Diffusion (DCMD), which enjoys the advantages of
both approaches. The DCMD integrates conditioned motion and conditioned
embedding to comprehensively utilize the pose characteristics and latent
semantics of observed movements, respectively. In the reverse diffusion
process, a motion transformer is proposed to capture potential correlations
from multi-layered characteristics within the spectrum space of human motion.
To enhance the discriminability between normal and abnormal instances, we
design a novel United Association Discrepancy (UAD) regularization that
primarily relies on a Gaussian kernel-based time association and a
self-attention-based global association. Finally, a mask completion strategy is
introduced during the inference stage of the reverse diffusion process to
enhance the utilization of conditioned motion for the prediction branch of
anomaly detection. Extensive experiments on four datasets demonstrate that our
method dramatically outperforms state-of-the-art methods and exhibits superior
generalization performance.",2024-12-23 01:31:39+00:00,"['Hongsong Wang', 'Andi Xu', 'Pinle Ding', 'Jie Gui']",http://arxiv.org/abs/2412.17210v2
"Weakly-Supervised PET Anomaly Detection using Implicitly-Guided Attention-Conditional Counterfactual Diffusion Modeling: a Multi-Center, Multi-Cancer, and Multi-Tracer Study","Minimizing the need for pixel-level annotated data to train PET lesion
detection and segmentation networks is highly desired and can be
transformative, given time and cost constraints associated with expert
annotations. Current un-/weakly-supervised anomaly detection methods rely on
autoencoder or generative adversarial networks trained only on healthy data;
however GAN-based networks are more challenging to train due to issues with
simultaneous optimization of two competing networks, mode collapse, etc. In
this paper, we present the weakly-supervised Implicitly guided COuNterfactual
diffusion model for Detecting Anomalies in PET images (IgCONDA-PET). The
solution is developed and validated using PET scans from six retrospective
cohorts consisting of a total of 2652 cases containing both local and public
datasets. The training is conditioned on image class labels (healthy vs.
unhealthy) via attention modules, and we employ implicit diffusion guidance. We
perform counterfactual generation which facilitates ""unhealthy-to-healthy""
domain translation by generating a synthetic, healthy version of an unhealthy
input image, enabling the detection of anomalies through the calculated
differences. The performance of our method was compared against several other
deep learning based weakly-supervised or unsupervised methods as well as
traditional methods like 41% SUVmax thresholding. We also highlight the
importance of incorporating attention modules in our network for the detection
of small anomalies. The code is publicly available at:
https://github.com/ahxmeds/IgCONDA-PET.git.",2024-04-30 23:09:54+00:00,"['Shadab Ahamed', 'Arman Rahmim']",http://arxiv.org/abs/2405.00239v2
InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions,"We introduce $\textit{InteractiveVideo}$, a user-centric framework for video
generation. Different from traditional generative approaches that operate based
on user-provided images or text, our framework is designed for dynamic
interaction, allowing users to instruct the generative model through various
intuitive mechanisms during the whole generation process, e.g. text and image
prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal
Instruction mechanism, designed to seamlessly integrate users' multimodal
instructions into generative models, thus facilitating a cooperative and
responsive interaction between user inputs and the generative process. This
approach enables iterative and fine-grained refinement of the generation result
through precise and effective user instructions. With
$\textit{InteractiveVideo}$, users are given the flexibility to meticulously
tailor key aspects of a video. They can paint the reference image, edit
semantics, and adjust video motions until their requirements are fully met.
Code, models, and demo are available at
https://github.com/invictus717/InteractiveVideo",2024-02-05 14:24:46+00:00,"['Yiyuan Zhang', 'Yuhao Kang', 'Zhixin Zhang', 'Xiaohan Ding', 'Sanyuan Zhao', 'Xiangyu Yue']",http://arxiv.org/abs/2402.03040v1
Generating Seamless Virtual Immunohistochemical Whole Slide Images with Content and Color Consistency,"Immunohistochemical (IHC) stains play a vital role in a pathologist's
analysis of medical images, providing crucial diagnostic information for
various diseases. Virtual staining from hematoxylin and eosin (H&E)-stained
whole slide images (WSIs) allows the automatic production of other useful IHC
stains without the expensive physical staining process. However, current
virtual WSI generation methods based on tile-wise processing often suffer from
inconsistencies in content, texture, and color at tile boundaries. These
inconsistencies lead to artifacts that compromise image quality and potentially
hinder accurate clinical assessment and diagnoses. To address this limitation,
we propose a novel consistent WSI synthesis network, CC-WSI-Net, that extends
GAN models to produce seamless synthetic whole slide images. Our CC-WSI-Net
integrates a content- and color-consistency supervisor, ensuring consistency
across tiles and facilitating the generation of seamless synthetic WSIs while
ensuring Sox10 immunohistochemistry accuracy in melanocyte detection. We
validate our method through extensive image-quality analyses, objective
detection assessments, and a subjective survey with pathologists. By generating
high-quality synthetic WSIs, our method opens doors for advanced virtual
staining techniques with broader applications in research and clinical care.",2024-10-01 21:02:16+00:00,"['Sitong Liu', 'Kechun Liu', 'Samuel Margolis', 'Wenjun Wu', 'Stevan R. Knezevich', 'David E Elder', 'Megan M. Eguchi', 'Joann G Elmore', 'Linda Shapiro']",http://arxiv.org/abs/2410.01072v1
Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion,"In recent years, there has been rapid development in 3D generation models,
opening up new possibilities for applications such as simulating the dynamic
movements of 3D objects and customizing their behaviors. However, current 3D
generative models tend to focus only on surface features such as color and
shape, neglecting the inherent physical properties that govern the behavior of
objects in the real world. To accurately simulate physics-aligned dynamics, it
is essential to predict the physical properties of materials and incorporate
them into the behavior prediction process. Nonetheless, predicting the diverse
materials of real-world objects is still challenging due to the complex nature
of their physical attributes. In this paper, we propose \textbf{Physics3D}, a
novel method for learning various physical properties of 3D objects through a
video diffusion model. Our approach involves designing a highly generalizable
physical simulation system based on a viscoelastic material model, which
enables us to simulate a wide range of materials with high-fidelity
capabilities. Moreover, we distill the physical priors from a video diffusion
model that contains more understanding of realistic object materials. Extensive
experiments demonstrate the effectiveness of our method with both elastic and
plastic materials. Physics3D shows great potential for bridging the gap between
the physical world and virtual neural space, providing a better integration and
application of realistic physical principles in virtual environments. Project
page: https://liuff19.github.io/Physics3D.",2024-06-06 17:59:47+00:00,"['Fangfu Liu', 'Hanyang Wang', 'Shunyu Yao', 'Shengjun Zhang', 'Jie Zhou', 'Yueqi Duan']",http://arxiv.org/abs/2406.04338v3
MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space,"We introduce a novel framework for 3D human avatar generation and
personalization, leveraging text prompts to enhance user engagement and
customization. Central to our approach are key innovations aimed at overcoming
the challenges in photo-realistic avatar synthesis. Firstly, we utilize a
conditional Neural Radiance Fields (NeRF) model, trained on a large-scale
unannotated multi-view dataset, to create a versatile initial solution space
that accelerates and diversifies avatar generation. Secondly, we develop a
geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models,
to ensure superior view invariance and enable direct optimization of avatar
geometry. These foundational ideas are complemented by our optimization
pipeline built on Variational Score Distillation (VSD), which mitigates texture
loss and over-saturation issues. As supported by our extensive experiments,
these strategies collectively enable the creation of custom avatars with
unparalleled visual quality and better adherence to input text prompts. You can
find more results and videos in our website:
https://syntec-research.github.io/MagicMirror",2024-04-01 17:59:11+00:00,"['Armand Comas-Massagu', 'Di Qiu', 'Menglei Chai', 'Marcel Bhler', 'Amit Raj', 'Ruiqi Gao', 'Qiangeng Xu', 'Mark Matthews', 'Paulo Gotardo', 'Octavia Camps', 'Sergio Orts-Escolano', 'Thabo Beeler']",http://arxiv.org/abs/2404.01296v1
Temporal and Spatial Super Resolution with Latent Diffusion Model in Medical MRI images,"Super Resolution (SR) plays a critical role in computer vision, particularly
in medical imaging, where hardware and acquisition time constraints often
result in low spatial and temporal resolution. While diffusion models have been
applied for both spatial and temporal SR, few studies have explored their use
for joint spatial and temporal SR, particularly in medical imaging. In this
work, we address this gap by proposing to use a Latent Diffusion Model (LDM)
combined with a Vector Quantised GAN (VQGAN)-based encoder-decoder architecture
for joint super resolution. We frame SR as an image denoising problem, focusing
on improving both spatial and temporal resolution in medical images. Using the
cardiac MRI dataset from the Data Science Bowl Cardiac Challenge, consisting of
2D cine images with a spatial resolution of 256x256 and 8-14 slices per
time-step, we demonstrate the effectiveness of our approach. Our LDM model
achieves Peak Signal to Noise Ratio (PSNR) of 30.37, Structural Similarity
Index (SSIM) of 0.7580, and Learned Perceptual Image Patch Similarity (LPIPS)
of 0.2756, outperforming simple baseline method by 5% in PSNR, 6.5% in SSIM,
39% in LPIPS. Our LDM model generates images with high fidelity and perceptual
quality with 15 diffusion steps. These results suggest that LDMs hold promise
for advancing super resolution in medical imaging, potentially enhancing
diagnostic accuracy and patient outcomes. Code link is also shared.",2024-10-29 20:13:00+00:00,['Vishal Dubey'],http://arxiv.org/abs/2410.23898v1
DiTFastAttn: Attention Compression for Diffusion Transformer Models,"Diffusion Transformers (DiT) excel at image and video generation but face
computational challenges due to the quadratic complexity of self-attention
operators. We propose DiTFastAttn, a post-training compression method to
alleviate the computational bottleneck of DiT. We identify three key
redundancies in the attention computation during DiT inference: (1) spatial
redundancy, where many attention heads focus on local information; (2) temporal
redundancy, with high similarity between the attention outputs of neighboring
steps; (3) conditional redundancy, where conditional and unconditional
inferences exhibit significant similarity. We propose three techniques to
reduce these redundancies: (1) Window Attention with Residual Sharing to reduce
spatial redundancy; (2) Attention Sharing across Timesteps to exploit the
similarity between steps; (3) Attention Sharing across CFG to skip redundant
computations during conditional generation. We apply DiTFastAttn to DiT,
PixArt-Sigma for image generation tasks, and OpenSora for video generation
tasks. Our results show that for image generation, our method reduces up to 76%
of the attention FLOPs and achieves up to 1.8x end-to-end speedup at
high-resolution (2k x 2k) generation.",2024-06-12 18:00:08+00:00,"['Zhihang Yuan', 'Hanling Zhang', 'Pu Lu', 'Xuefei Ning', 'Linfeng Zhang', 'Tianchen Zhao', 'Shengen Yan', 'Guohao Dai', 'Yu Wang']",http://arxiv.org/abs/2406.08552v2
Time Weaver: A Conditional Time Series Generation Model,"Imagine generating a city's electricity demand pattern based on weather, the
presence of an electric vehicle, and location, which could be used for capacity
planning during a winter freeze. Such real-world time series are often enriched
with paired heterogeneous contextual metadata (weather, location, etc.).
Current approaches to time series generation often ignore this paired metadata,
and its heterogeneity poses several practical challenges in adapting existing
conditional generation approaches from the image, audio, and video domains to
the time series domain. To address this gap, we introduce Time Weaver, a novel
diffusion-based model that leverages the heterogeneous metadata in the form of
categorical, continuous, and even time-variant variables to significantly
improve time series generation. Additionally, we show that naive extensions of
standard evaluation metrics from the image to the time series domain are
insufficient. These metrics do not penalize conditional generation approaches
for their poor specificity in reproducing the metadata-specific features in the
generated time series. Thus, we innovate a novel evaluation metric that
accurately captures the specificity of conditional generation and the realism
of the generated time series. We show that Time Weaver outperforms
state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by
up to 27% in downstream classification tasks on real-world energy, medical, air
quality, and traffic data sets.",2024-03-05 06:10:22+00:00,"['Sai Shankar Narasimhan', 'Shubhankar Agarwal', 'Oguzhan Akcin', 'Sujay Sanghavi', 'Sandeep Chinchali']",http://arxiv.org/abs/2403.02682v1
"Synthesising Handwritten Music with GANs: A Comprehensive Evaluation of CycleWGAN, ProGAN, and DCGAN","The generation of handwritten music sheets is a crucial step toward enhancing
Optical Music Recognition (OMR) systems, which rely on large and diverse
datasets for optimal performance. However, handwritten music sheets, often
found in archives, present challenges for digitisation due to their fragility,
varied handwriting styles, and image quality. This paper addresses the data
scarcity problem by applying Generative Adversarial Networks (GANs) to
synthesise realistic handwritten music sheets. We provide a comprehensive
evaluation of three GAN models - DCGAN, ProGAN, and CycleWGAN - comparing their
ability to generate diverse and high-quality handwritten music images. The
proposed CycleWGAN model, which enhances style transfer and training stability,
significantly outperforms DCGAN and ProGAN in both qualitative and quantitative
evaluations. CycleWGAN achieves superior performance, with an FID score of
41.87, an IS of 2.29, and a KID of 0.05, making it a promising solution for
improving OMR systems.",2024-11-25 14:10:43+00:00,"['Elona Shatri', 'Kalikidhar Palavala', 'George Fazekas']",http://arxiv.org/abs/2411.16405v1
DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction,"Audio-visual saliency prediction can draw support from diverse modality
complements, but further performance enhancement is still challenged by
customized architectures as well as task-specific loss functions. In recent
studies, denoising diffusion models have shown more promising in unifying task
frameworks owing to their inherent ability of generalization. Following this
motivation, a novel Diffusion architecture for generalized audio-visual
Saliency prediction (DiffSal) is proposed in this work, which formulates the
prediction problem as a conditional generative task of the saliency map by
utilizing input audio and video as the conditions. Based on the spatio-temporal
audio-visual features, an extra network Saliency-UNet is designed to perform
multi-modal attention modulation for progressive refinement of the ground-truth
saliency map from the noisy map. Extensive experiments demonstrate that the
proposed DiffSal can achieve excellent performance across six challenging
audio-visual benchmarks, with an average relative improvement of 6.3\% over the
previous state-of-the-art results by six metrics.",2024-03-02 14:52:58+00:00,"['Junwen Xiong', 'Peng Zhang', 'Tao You', 'Chuanyue Li', 'Wei Huang', 'Yufei Zha']",http://arxiv.org/abs/2403.01226v1
BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering,"Developing blind video deflickering (BVD) algorithms to enhance video
temporal consistency, is gaining importance amid the flourish of image
processing and video generation. However, the intricate nature of video data
complicates the training of deep learning methods, leading to high resource
consumption and instability, notably under severe lighting flicker. This
underscores the critical need for a compact representation beyond pixel values
to advance BVD research and applications. Inspired by the classic scale-time
equalization (STE), our work introduces the histogram-assisted solution, called
BlazeBVD, for high-fidelity and rapid BVD. Compared with STE, which directly
corrects pixel values by temporally smoothing color histograms, BlazeBVD
leverages smoothed illumination histograms within STE filtering to ease the
challenge of learning temporal data using neural networks. In technique,
BlazeBVD begins by condensing pixel values into illumination histograms that
precisely capture flickering and local exposure variations. These histograms
are then smoothed to produce singular frames set, filtered illumination maps,
and exposure maps. Resorting to these deflickering priors, BlazeBVD utilizes a
2D network to restore faithful and consistent texture impacted by lighting
changes or localized exposure issues. BlazeBVD also incorporates a lightweight
3D network to amend slight temporal inconsistencies, avoiding the resource
consumption issue. Comprehensive experiments on synthetic, real-world and
generated videos, showcase the superior qualitative and quantitative results of
BlazeBVD, achieving inference speeds up to 10x faster than state-of-the-arts.",2024-03-10 15:56:55+00:00,"['Xinmin Qiu', 'Congying Han', 'Zicheng Zhang', 'Bonan Li', 'Tiande Guo', 'Pingyu Wang', 'Xuecheng Nie']",http://arxiv.org/abs/2403.06243v1
FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders,"In recent years, automated Gallbladder Cancer (GBC) detection has gained the
attention of researchers. Current state-of-the-art (SOTA) methodologies relying
on ultrasound sonography (US) images exhibit limited generalization,
emphasizing the need for transformative approaches. We observe that individual
US frames may lack sufficient information to capture disease manifestation.
This study advocates for a paradigm shift towards video-based GBC detection,
leveraging the inherent advantages of spatiotemporal representations. Employing
the Masked Autoencoder (MAE) for representation learning, we address
shortcomings in conventional image-based methods. We propose a novel design
called FocusMAE to systematically bias the selection of masking tokens from
high-information regions, fostering a more refined representation of
malignancy. Additionally, we contribute the most extensive US video dataset for
GBC detection. We also note that, this is the first study on US video-based GBC
detection. We validate the proposed methods on the curated dataset, and report
a new state-of-the-art (SOTA) accuracy of 96.4% for the GBC detection problem,
against an accuracy of 84% by current Image-based SOTA - GBCNet, and RadFormer,
and 94.7% by Video-based SOTA - AdaMAE. We further demonstrate the generality
of the proposed FocusMAE on a public CT-based Covid detection dataset,
reporting an improvement in accuracy by 3.3% over current baselines. The source
code and pretrained models are available at:
https://gbc-iitd.github.io/focusmae",2024-03-13 16:57:04+00:00,"['Soumen Basu', 'Mayuna Gupta', 'Chetan Madan', 'Pankaj Gupta', 'Chetan Arora']",http://arxiv.org/abs/2403.08848v2
AutoTVG: A New Vision-language Pre-training Paradigm for Temporal Video Grounding,"Temporal Video Grounding (TVG) aims to localize a moment from an untrimmed
video given the language description. Since the annotation of TVG is
labor-intensive, TVG under limited supervision has accepted attention in recent
years. The great success of vision-language pre-training guides TVG to follow
the traditional ""pre-training + fine-tuning"" paradigm, however, the
pre-training process would suffer from a lack of temporal modeling and
fine-grained alignment due to the difference of data nature between pre-train
and test. Besides, the large gap between pretext and downstream tasks makes
zero-shot testing impossible for the pre-trained model. To avoid the drawbacks
of the traditional paradigm, we propose AutoTVG, a new vision-language
pre-training paradigm for TVG that enables the model to learn semantic
alignment and boundary regression from automatically annotated untrimmed
videos. To be specific, AutoTVG consists of a novel Captioned Moment Generation
(CMG) module to generate captioned moments from untrimmed videos, and TVGNet
with a regression head to predict localization results. Experimental results on
Charades-STA and ActivityNet Captions show that, regarding zero-shot temporal
video grounding, AutoTVG achieves highly competitive performance with
in-distribution methods under out-of-distribution testing, and is superior to
existing pre-training frameworks with much less training data.",2024-06-11 09:31:37+00:00,"['Xing Zhang', 'Jiaxi Gu', 'Haoyu Zhao', 'Shicong Wang', 'Hang Xu', 'Renjing Pei', 'Songcen Xu', 'Zuxuan Wu', 'Yu-Gang Jiang']",http://arxiv.org/abs/2406.07091v1
Secure Video Quality Assessment Resisting Adversarial Attacks,"The exponential surge in video traffic has intensified the imperative for
Video Quality Assessment (VQA). Leveraging cutting-edge architectures, current
VQA models have achieved human-comparable accuracy. However, recent studies
have revealed the vulnerability of existing VQA models against adversarial
attacks. To establish a reliable and practical assessment system, a secure VQA
model capable of resisting such malicious attacks is urgently demanded.
Unfortunately, no attempt has been made to explore this issue. This paper first
attempts to investigate general adversarial defense principles, aiming at
endowing existing VQA models with security. Specifically, we first introduce
random spatial grid sampling on the video frame for intra-frame defense. Then,
we design pixel-wise randomization through a guardian map, globally
neutralizing adversarial perturbations. Meanwhile, we extract temporal
information from the video sequence as compensation for inter-frame defense.
Building upon these principles, we present a novel VQA framework from the
security-oriented perspective, termed SecureVQA. Extensive experiments indicate
that SecureVQA sets a new benchmark in security while achieving competitive VQA
performance compared with state-of-the-art models. Ablation studies delve
deeper into analyzing the principles of SecureVQA, demonstrating their
generalization and contributions to the security of leading VQA models.",2024-10-09 13:27:06+00:00,"['Ao-Xiang Zhang', 'Yu Ran', 'Weixuan Tang', 'Yuan-Gen Wang', 'Qingxiao Guan', 'Chunsheng Yang']",http://arxiv.org/abs/2410.06866v1
Exploring Efficient Foundational Multi-modal Models for Video Summarization,"Foundational models are able to generate text outputs given prompt
instructions and text, audio, or image inputs. Recently these models have been
combined to perform tasks on video, such as video summarization. Such video
foundation models perform pre-training by aligning outputs from each
modality-specific model into the same embedding space. Then the embeddings from
each model are used within a language model, which is fine-tuned on a desired
instruction set. Aligning each modality during pre-training is computationally
expensive and prevents rapid testing of different base modality models. During
fine-tuning, evaluation is carried out within in-domain videos where it is hard
to understand the generalizability and data efficiency of these methods. To
alleviate these issues we propose a plug-and-play video language model. It
directly uses the texts generated from each input modality into the language
model, avoiding pre-training alignment overhead. Instead of fine-tuning we
leverage few-shot instruction adaptation strategies. We compare the performance
versus the computational costs for our plug-and-play style method and baseline
tuning methods. Finally, we explore the generalizability of each method during
domain shift and present insights on what data is useful when training data is
limited. Through this analysis, we present practical insights on how to
leverage multi-modal foundational models for effective results given realistic
compute and data limitations.",2024-10-09 20:07:06+00:00,"['Karan Samel', 'Apoorva Beedu', 'Nitish Sontakke', 'Irfan Essa']",http://arxiv.org/abs/2410.07405v1
Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models,"Our brains represent the ever-changing environment with neurons in a highly
dynamic fashion. The temporal features of visual pixels in dynamic natural
scenes are entrapped in the neuronal responses of the retina. It is crucial to
establish the intrinsic temporal relationship between visual pixels and
neuronal responses. Recent foundation vision models have paved an advanced way
of understanding image pixels. Yet, neuronal coding in the brain largely lacks
a deep understanding of its alignment with pixels. Most previous studies employ
static images or artificial videos derived from static images for emulating
more real and complicated stimuli. Despite these simple scenarios effectively
help to separate key factors influencing visual coding, complex temporal
relationships receive no consideration. To decompose the temporal features of
visual coding in natural scenes, here we propose Vi-ST, a spatiotemporal
convolutional neural network fed with a self-supervised Vision Transformer
(ViT) prior, aimed at unraveling the temporal-based encoding patterns of
retinal neuronal populations. The model demonstrates robust predictive
performance in generalization tests. Furthermore, through detailed ablation
experiments, we demonstrate the significance of each temporal module.
Furthermore, we introduce a visual coding evaluation metric designed to
integrate temporal considerations and compare the impact of different numbers
of neuronal populations on complementary coding. In conclusion, our proposed
Vi-ST demonstrates a novel modeling framework for neuronal coding of dynamic
visual scenes in the brain, effectively aligning our brain representation of
video with neuronal activity. The code is available at
https://github.com/wurining/Vi-ST.",2024-07-15 14:06:13+00:00,"['Rining Wu', 'Feixiang Zhou', 'Ziwei Yin', 'Jian K. Liu']",http://arxiv.org/abs/2407.10737v1
From Sora What We Can See: A Survey of Text-to-Video Generation,"With impressive achievements made, artificial intelligence is on the path
forward to artificial general intelligence. Sora, developed by OpenAI, which is
capable of minute-level world-simulative abilities can be considered as a
milestone on this developmental path. However, despite its notable successes,
Sora still encounters various obstacles that need to be resolved. In this
survey, we embark from the perspective of disassembling Sora in text-to-video
generation, and conducting a comprehensive review of literature, trying to
answer the question, \textit{From Sora What We Can See}. Specifically, after
basic preliminaries regarding the general algorithms are introduced, the
literature is categorized from three mutually perpendicular dimensions:
evolutionary generators, excellent pursuit, and realistic panorama.
Subsequently, the widely used datasets and metrics are organized in detail.
Last but more importantly, we identify several challenges and open problems in
this domain and propose potential future directions for research and
development.",2024-05-17 10:09:09+00:00,"['Rui Sun', 'Yumin Zhang', 'Tejal Shah', 'Jiahao Sun', 'Shuoying Zhang', 'Wenqi Li', 'Haoran Duan', 'Bo Wei', 'Rajiv Ranjan']",http://arxiv.org/abs/2405.10674v1
Augmenting Efficient Real-time Surgical Instrument Segmentation in Video with Point Tracking and Segment Anything,"The Segment Anything Model (SAM) is a powerful vision foundation model that
is revolutionizing the traditional paradigm of segmentation. Despite this, a
reliance on prompting each frame and large computational cost limit its usage
in robotically assisted surgery. Applications, such as augmented reality
guidance, require little user intervention along with efficient inference to be
usable clinically. In this study, we address these limitations by adopting
lightweight SAM variants to meet the efficiency requirement and employing
fine-tuning techniques to enhance their generalization in surgical scenes.
Recent advancements in Tracking Any Point (TAP) have shown promising results in
both accuracy and efficiency, particularly when points are occluded or leave
the field of view. Inspired by this progress, we present a novel framework that
combines an online point tracker with a lightweight SAM model that is
fine-tuned for surgical instrument segmentation. Sparse points within the
region of interest are tracked and used to prompt SAM throughout the video
sequence, providing temporal consistency. The quantitative results surpass the
state-of-the-art semi-supervised video object segmentation method XMem on the
EndoVis 2015 dataset with 84.8 IoU and 91.0 Dice. Our method achieves promising
performance that is comparable to XMem and transformer-based fully supervised
segmentation methods on ex vivo UCL dVRK and in vivo CholecSeg8k datasets. In
addition, the proposed method shows promising zero-shot generalization ability
on the label-free STIR dataset. In terms of efficiency, we tested our method on
a single GeForce RTX 4060/4090 GPU respectively, achieving an over 25/90 FPS
inference speed. Code is available at:
https://github.com/wuzijian1997/SIS-PT-SAM",2024-03-12 18:12:42+00:00,"['Zijian Wu', 'Adam Schmidt', 'Peter Kazanzides', 'Septimiu E. Salcudean']",http://arxiv.org/abs/2403.08003v2
Learning Multiple Object States from Actions via Large Language Models,"Recognizing the states of objects in a video is crucial in understanding the
scene beyond actions and objects. For instance, an egg can be raw, cracked, and
whisked while cooking an omelet, and these states can coexist simultaneously
(an egg can be both raw and whisked). However, most existing research assumes a
single object state change (e.g., uncracked -> cracked), overlooking the
coexisting nature of multiple object states and the influence of past states on
the current state. We formulate object state recognition as a multi-label
classification task that explicitly handles multiple states. We then propose to
learn multiple object states from narrated videos by leveraging large language
models (LLMs) to generate pseudo-labels from the transcribed narrations,
capturing the influence of past states. The challenge is that narrations mostly
describe human actions in the video but rarely explain object states.
Therefore, we use the LLMs knowledge of the relationship between actions and
states to derive the missing object states. We further accumulate the derived
object states to consider past state contexts to infer current object state
pseudo-labels. We newly collect a dataset called the Multiple Object States
Transition (MOST) dataset, which includes manual multi-label annotation for
evaluation purposes, covering 60 object states across six object categories.
Experimental results show that our model trained on LLM-generated pseudo-labels
significantly outperforms strong vision-language models, demonstrating the
effectiveness of our pseudo-labeling framework that considers past context via
LLMs.",2024-05-02 08:43:16+00:00,"['Masatoshi Tateno', 'Takuma Yagi', 'Ryosuke Furuta', 'Yoichi Sato']",http://arxiv.org/abs/2405.01090v2
DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion,"Leveraging pretrained 2D diffusion models and score distillation sampling
(SDS), recent methods have shown promising results for text-to-3D avatar
generation. However, generating high-quality 3D avatars capable of expressive
animation remains challenging. In this work, we present DreamWaltz-G, a novel
learning framework for animatable 3D avatar generation from text. The core of
this framework lies in Skeleton-guided Score Distillation and Hybrid 3D
Gaussian Avatar representation. Specifically, the proposed skeleton-guided
score distillation integrates skeleton controls from 3D human templates into 2D
diffusion models, enhancing the consistency of SDS supervision in terms of view
and human pose. This facilitates the generation of high-quality avatars,
mitigating issues such as multiple faces, extra limbs, and blurring. The
proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D
Gaussians, combining neural implicit fields and parameterized 3D meshes to
enable real-time rendering, stable SDS optimization, and expressive animation.
Extensive experiments demonstrate that DreamWaltz-G is highly effective in
generating and animating 3D avatars, outperforming existing methods in both
visual quality and animation expressiveness. Our framework further supports
diverse applications, including human video reenactment and multi-subject scene
composition.",2024-09-25 17:59:45+00:00,"['Yukun Huang', 'Jianan Wang', 'Ailing Zeng', 'Zheng-Jun Zha', 'Lei Zhang', 'Xihui Liu']",http://arxiv.org/abs/2409.17145v1
DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion,"Story visualization aims to create visually compelling images or videos
corresponding to textual narratives. Despite recent advances in diffusion
models yielding promising results, existing methods still struggle to create a
coherent sequence of subject-consistent frames based solely on a story. To this
end, we propose DreamStory, an automatic open-domain story visualization
framework by leveraging the LLMs and a novel multi-subject consistent diffusion
model. DreamStory consists of (1) an LLM acting as a story director and (2) an
innovative Multi-Subject consistent Diffusion model (MSD) for generating
consistent multi-subject across the images. First, DreamStory employs the LLM
to generate descriptive prompts for subjects and scenes aligned with the story,
annotating each scene's subjects for subsequent subject-consistent generation.
Second, DreamStory utilizes these detailed subject descriptions to create
portraits of the subjects, with these portraits and their corresponding textual
information serving as multimodal anchors (guidance). Finally, the MSD uses
these multimodal anchors to generate story scenes with consistent
multi-subject. Specifically, the MSD includes Masked Mutual Self-Attention
(MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA and MMCA modules
ensure appearance and semantic consistency with reference images and text,
respectively. Both modules employ masking mechanisms to prevent subject
blending. To validate our approach and promote progress in story visualization,
we established a benchmark, DS-500, which can assess the overall performance of
the story visualization framework, subject-identification accuracy, and the
consistency of the generation model. Extensive experiments validate the
effectiveness of DreamStory in both subjective and objective evaluations.
Please visit our project homepage at https://dream-xyz.github.io/dreamstory.",2024-07-17 17:54:12+00:00,"['Huiguo He', 'Huan Yang', 'Zixi Tuo', 'Yuan Zhou', 'Qiuyue Wang', 'Yuhang Zhang', 'Zeyu Liu', 'Wenhao Huang', 'Hongyang Chao', 'Jian Yin']",http://arxiv.org/abs/2407.12899v2
Frame Interpolation with Consecutive Brownian Bridge Diffusion,"Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a
diffusion-based conditional image generation problem, synthesizing the
intermediate frame given a random noise and neighboring frames. Due to the
relatively high resolution of videos, Latent Diffusion Models (LDMs) are
employed as the conditional generation model, where the autoencoder compresses
images into latent representations for diffusion and then reconstructs images
from these latent representations. Such a formulation poses a crucial
challenge: VFI expects that the output is deterministically equal to the ground
truth intermediate frame, but LDMs randomly generate a diverse set of different
images when the model runs multiple times. The reason for the diverse
generation is that the cumulative variance (variance accumulated at each step
of generation) of generated latent representations in LDMs is large. This makes
the sampling trajectory random, resulting in diverse rather than deterministic
generations. To address this problem, we propose our unique solution: Frame
Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we
propose consecutive Brownian Bridge diffusion that takes a deterministic
initial value as input, resulting in a much smaller cumulative variance of
generated latent representations. Our experiments suggest that our method can
improve together with the improvement of the autoencoder and achieve
state-of-the-art performance in VFI, leaving strong potential for further
enhancement.",2024-05-09 17:46:22+00:00,"['Zonglin Lyu', 'Ming Li', 'Jianbo Jiao', 'Chen Chen']",http://arxiv.org/abs/2405.05953v7
MotionDreamer: Exploring Semantic Video Diffusion features for Zero-Shot 3D Mesh Animation,"Animation techniques bring digital 3D worlds and characters to life. However,
manual animation is tedious and automated techniques are often specialized to
narrow shape classes. In our work, we propose a technique for automatic
re-animation of various 3D shapes based on a motion prior extracted from a
video diffusion model. Unlike existing 4D generation methods, we focus solely
on the motion, and we leverage an explicit mesh-based representation compatible
with existing computer-graphics pipelines. Furthermore, our utilization of
diffusion features enhances accuracy of our motion fitting. We analyze efficacy
of these features for animation fitting and we experimentally validate our
approach for two different diffusion models and four animation models. Finally,
we demonstrate that our time-efficient zero-shot method achieves a superior
performance re-animating a diverse set of 3D shapes when compared to existing
techniques in a user study. The project website is located at
https://lukas.uzolas.com/MotionDreamer.",2024-05-30 15:30:38+00:00,"['Lukas Uzolas', 'Elmar Eisemann', 'Petr Kellnhofer']",http://arxiv.org/abs/2405.20155v2
LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models,"The explosive growth of videos on streaming media platforms has underscored
the urgent need for effective video quality assessment (VQA) algorithms to
monitor and perceptually optimize the quality of streaming videos. However, VQA
remains an extremely challenging task due to the diverse video content and the
complex spatial and temporal distortions, thus necessitating more advanced
methods to address these issues. Nowadays, large multimodal models (LMMs), such
as GPT-4V, have exhibited strong capabilities for various visual understanding
tasks, motivating us to leverage the powerful multimodal representation ability
of LMMs to solve the VQA task. Therefore, we propose the first Large
Multi-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel
spatiotemporal visual modeling strategy for quality-aware feature extraction.
Specifically, we first reformulate the quality regression problem into a
question and answering (Q&A) task and construct Q&A prompts for VQA instruction
tuning. Then, we design a spatiotemporal vision encoder to extract spatial and
temporal features to represent the quality characteristics of videos, which are
subsequently mapped into the language space by the spatiotemporal projector for
modality alignment. Finally, the aligned visual tokens and the quality-inquired
text tokens are aggregated as inputs for the large language model (LLM) to
generate the quality score and level. Extensive experiments demonstrate that
LMM-VQA achieves state-of-the-art performance across five VQA benchmarks,
exhibiting an average improvement of $5\%$ in generalization ability over
existing methods. Furthermore, due to the advanced design of the spatiotemporal
encoder and projector, LMM-VQA also performs exceptionally well on general
video understanding tasks, further validating its effectiveness. Our code will
be released at https://github.com/Sueqk/LMM-VQA.",2024-08-26 04:29:52+00:00,"['Qihang Ge', 'Wei Sun', 'Yu Zhang', 'Yunhao Li', 'Zhongpeng Ji', 'Fengyu Sun', 'Shangling Jui', 'Xiongkuo Min', 'Guangtao Zhai']",http://arxiv.org/abs/2408.14008v1
Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback,"Large text-to-video models hold immense potential for a wide range of
downstream applications. However, these models struggle to accurately depict
dynamic object interactions, often resulting in unrealistic movements and
frequent violations of real-world physics. One solution inspired by large
language models is to align generated outputs with desired outcomes using
external feedback. This enables the model to refine its responses autonomously,
eliminating extensive manual data collection. In this work, we investigate the
use of feedback to enhance the object dynamics in text-to-video models. We aim
to answer a critical question: what types of feedback, paired with which
specific self-improvement algorithms, can most effectively improve text-video
alignment and realistic object interactions? We begin by deriving a unified
probabilistic objective for offline RL finetuning of text-to-video models. This
perspective highlights how design elements in existing algorithms like KL
regularization and policy projection emerge as specific choices within a
unified framework. We then use derived methods to optimize a set of text-video
alignment metrics (e.g., CLIP scores, optical flow), but notice that they often
fail to align with human perceptions of generation quality. To address this
limitation, we propose leveraging vision-language models to provide more
nuanced feedback specifically tailored to object dynamics in videos. Our
experiments demonstrate that our method can effectively optimize a wide variety
of rewards, with binary AI feedback driving the most significant improvements
in video quality for dynamic interactions, as confirmed by both AI and human
evaluations. Notably, we observe substantial gains when using reward signals
derived from AI feedback, particularly in scenarios involving complex
interactions between multiple objects and realistic depictions of objects
falling.",2024-12-03 17:44:23+00:00,"['Hiroki Furuta', 'Heiga Zen', 'Dale Schuurmans', 'Aleksandra Faust', 'Yutaka Matsuo', 'Percy Liang', 'Sherry Yang']",http://arxiv.org/abs/2412.02617v1
Memory-efficient High-resolution OCT Volume Synthesis with Cascaded Amortized Latent Diffusion Models,"Optical coherence tomography (OCT) image analysis plays an important role in
the field of ophthalmology. Current successful analysis models rely on
available large datasets, which can be challenging to be obtained for certain
tasks. The use of deep generative models to create realistic data emerges as a
promising approach. However, due to limitations in hardware resources, it is
still difficulty to synthesize high-resolution OCT volumes. In this paper, we
introduce a cascaded amortized latent diffusion model (CA-LDM) that can
synthesis high-resolution OCT volumes in a memory-efficient way. First, we
propose non-holistic autoencoders to efficiently build a bidirectional mapping
between high-resolution volume space and low-resolution latent space. In tandem
with autoencoders, we propose cascaded diffusion processes to synthesize
high-resolution OCT volumes with a global-to-local refinement process,
amortizing the memory and computational demands. Experiments on a public
high-resolution OCT dataset show that our synthetic data have realistic
high-resolution and global features, surpassing the capabilities of existing
methods. Moreover, performance gains on two down-stream fine-grained
segmentation tasks demonstrate the benefit of the proposed method in training
deep learning models for medical imaging tasks. The code is public available
at: https://github.com/nicetomeetu21/CA-LDM.",2024-05-26 10:58:22+00:00,"['Kun Huang', 'Xiao Ma', 'Yuhan Zhang', 'Na Su', 'Songtao Yuan', 'Yong Liu', 'Qiang Chen', 'Huazhu Fu']",http://arxiv.org/abs/2405.16516v1
Non-Invasive to Invasive: Enhancing FFA Synthesis from CFP with a Benchmark Dataset and a Novel Network,"Fundus imaging is a pivotal tool in ophthalmology, and different imaging
modalities are characterized by their specific advantages. For example, Fundus
Fluorescein Angiography (FFA) uniquely provides detailed insights into retinal
vascular dynamics and pathology, surpassing Color Fundus Photographs (CFP) in
detecting microvascular abnormalities and perfusion status. However, the
conventional invasive FFA involves discomfort and risks due to fluorescein dye
injection, and it is meaningful but challenging to synthesize FFA images from
non-invasive CFP. Previous studies primarily focused on FFA synthesis in a
single disease category. In this work, we explore FFA synthesis in multiple
diseases by devising a Diffusion-guided generative adversarial network, which
introduces an adaptive and dynamic diffusion forward process into the
discriminator and adds a category-aware representation enhancer. Moreover, to
facilitate this research, we collect the first multi-disease CFP and FFA paired
dataset, named the Multi-disease Paired Ocular Synthesis (MPOS) dataset, with
four different fundus diseases. Experimental results show that our FFA
synthesis network can generate better FFA images compared to state-of-the-art
methods. Furthermore, we introduce a paired-modal diagnostic network to
validate the effectiveness of synthetic FFA images in the diagnosis of multiple
fundus diseases, and the results show that our synthesized FFA images with the
real CFP images have higher diagnosis accuracy than that of the compared FFA
synthesizing methods. Our research bridges the gap between non-invasive imaging
and FFA, thereby offering promising prospects to enhance ophthalmic diagnosis
and patient care, with a focus on reducing harm to patients through
non-invasive procedures. Our dataset and code will be released to support
further research in this field (https://github.com/whq-xxh/FFA-Synthesis).",2024-10-19 03:53:25+00:00,"['Hongqiu Wang', 'Zhaohu Xing', 'Weitong Wu', 'Yijun Yang', 'Qingqing Tang', 'Meixia Zhang', 'Yanwu Xu', 'Lei Zhu']",http://arxiv.org/abs/2410.14965v1
RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation,"Bimanual manipulation is essential in robotics, yet developing foundation
models is extremely challenging due to the inherent complexity of coordinating
two robot arms (leading to multi-modal action distributions) and the scarcity
of training data. In this paper, we present the Robotics Diffusion Transformer
(RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT
builds on diffusion models to effectively represent multi-modality, with
innovative designs of a scalable Transformer to deal with the heterogeneity of
multi-modal inputs and to capture the nonlinearity and high frequency of
robotic data. To address data scarcity, we further introduce a Physically
Interpretable Unified Action Space, which can unify the action representations
of various robots while preserving the physical meanings of original actions,
facilitating learning transferrable physical knowledge. With these designs, we
managed to pre-train RDT on the largest collection of multi-robot datasets to
date and scaled it up to 1.2B parameters, which is the largest diffusion-based
foundation model for robotic manipulation. We finally fine-tuned RDT on a
self-created multi-task bimanual dataset with over 6K+ episodes to refine its
manipulation capabilities. Experiments on real robots demonstrate that RDT
significantly outperforms existing methods. It exhibits zero-shot
generalization to unseen objects and scenes, understands and follows language
instructions, learns new skills with just 1~5 demonstrations, and effectively
handles complex, dexterous tasks. We refer to
https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.",2024-10-10 12:33:46+00:00,"['Songming Liu', 'Lingxuan Wu', 'Bangguo Li', 'Hengkai Tan', 'Huayu Chen', 'Zhengyi Wang', 'Ke Xu', 'Hang Su', 'Jun Zhu']",http://arxiv.org/abs/2410.07864v2
Reviewing FID and SID Metrics on Generative Adversarial Networks,"The growth of generative adversarial network (GAN) models has increased the
ability of image processing and provides numerous industries with the
technology to produce realistic image transformations. However, with the field
being recently established there are new evaluation metrics that can further
this research. Previous research has shown the Fr\'echet Inception Distance
(FID) to be an effective metric when testing these image-to-image GANs in
real-world applications. Signed Inception Distance (SID), a founded metric in
2023, expands on FID by allowing unsigned distances. This paper uses public
datasets that consist of fa\c{c}ades, cityscapes, and maps within Pix2Pix and
CycleGAN models. After training these models are evaluated on both inception
distance metrics which measure the generating performance of the trained
models. Our findings indicate that usage of the metric SID incorporates an
efficient and effective metric to complement, or even exceed the ability shown
using the FID for the image-to-image GANs",2024-02-06 03:02:39+00:00,"['Ricardo de Deijn', 'Aishwarya Batra', 'Brandon Koch', 'Naseef Mansoor', 'Hema Makkena']",http://arxiv.org/abs/2402.03654v1
Generative Adversarial Network on Motion-Blur Image Restoration,"In everyday life, photographs taken with a camera often suffer from motion
blur due to hand vibrations or sudden movements. This phenomenon can
significantly detract from the quality of the images captured, making it an
interesting challenge to develop a deep learning model that utilizes the
principles of adversarial networks to restore clarity to these blurred pixels.
In this project, we will focus on leveraging Generative Adversarial Networks
(GANs) to effectively deblur images affected by motion blur. A GAN-based
Tensorflow model is defined, training and evaluating by GoPro dataset which
comprises paired street view images featuring both clear and blurred versions.
This adversarial training process between Discriminator and Generator helps to
produce increasingly realistic images over time. Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation
metrics used to provide quantitative measures of image quality, allowing us to
evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and
mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in
this project. The blurry pixels are sharper in the output of GAN model shows a
good image restoration effect in real world applications.",2024-12-27 06:12:50+00:00,['Zhengdong Li'],http://arxiv.org/abs/2412.19479v1
EchoNet-Synthetic: Privacy-preserving Video Generation for Safe Medical Data Sharing,"To make medical datasets accessible without sharing sensitive patient
information, we introduce a novel end-to-end approach for generative
de-identification of dynamic medical imaging data. Until now, generative
methods have faced constraints in terms of fidelity, spatio-temporal coherence,
and the length of generation, failing to capture the complete details of
dataset distributions. We present a model designed to produce high-fidelity,
long and complete data samples with near-real-time efficiency and explore our
approach on a challenging task: generating echocardiogram videos. We develop
our generation method based on diffusion models and introduce a protocol for
medical video dataset anonymization. As an exemplar, we present
EchoNet-Synthetic, a fully synthetic, privacy-compliant echocardiogram dataset
with paired ejection fraction labels. As part of our de-identification
protocol, we evaluate the quality of the generated dataset and propose to use
clinical downstream tasks as a measurement on top of widely used but
potentially biased image quality metrics. Experimental outcomes demonstrate
that EchoNet-Synthetic achieves comparable dataset fidelity to the actual
dataset, effectively supporting the ejection fraction regression task. Code,
weights and dataset are available at
https://github.com/HReynaud/EchoNet-Synthetic.",2024-06-02 17:18:06+00:00,"['Hadrien Reynaud', 'Qingjie Meng', 'Mischa Dombrowski', 'Arijit Ghosh', 'Thomas Day', 'Alberto Gomez', 'Paul Leeson', 'Bernhard Kainz']",http://arxiv.org/abs/2406.00808v1
Zero-to-Hero: Enhancing Zero-Shot Novel View Synthesis via Attention Map Filtering,"Generating realistic images from arbitrary views based on a single source
image remains a significant challenge in computer vision, with broad
applications ranging from e-commerce to immersive virtual experiences. Recent
advancements in diffusion models, particularly the Zero-1-to-3 model, have been
widely adopted for generating plausible views, videos, and 3D models. However,
these models still struggle with inconsistencies and implausibility in new
views generation, especially for challenging changes in viewpoint. In this
work, we propose Zero-to-Hero, a novel test-time approach that enhances view
synthesis by manipulating attention maps during the denoising process of
Zero-1-to-3. By drawing an analogy between the denoising process and stochastic
gradient descent (SGD), we implement a filtering mechanism that aggregates
attention maps, enhancing generation reliability and authenticity. This process
improves geometric consistency without requiring retraining or significant
computational resources. Additionally, we modify the self-attention mechanism
to integrate information from the source view, reducing shape distortions.
These processes are further supported by a specialized sampling schedule.
Experimental results demonstrate substantial improvements in fidelity and
consistency, validated on a diverse set of out-of-distribution objects.
Additionally, we demonstrate the general applicability and effectiveness of
Zero-to-Hero in multi-view, and image generation conditioned on semantic maps
and pose.",2024-05-29 00:58:22+00:00,"['Ido Sobol', 'Chenfeng Xu', 'Or Litany']",http://arxiv.org/abs/2405.18677v2
HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models,"The goal of Arbitrary Style Transfer (AST) is injecting the artistic features
of a style reference into a given image/video. Existing methods usually focus
on pursuing the balance between style and content, whereas ignoring the
significant demand for flexible and customized stylization results and thereby
limiting their practical application. To address this critical issue, a novel
AST approach namely HiCAST is proposed, which is capable of explicitly
customizing the stylization results according to various source of semantic
clues. In the specific, our model is constructed based on Latent Diffusion
Model (LDM) and elaborately designed to absorb content and style instance as
conditions of LDM. It is characterized by introducing of \textit{Style
Adapter}, which allows user to flexibly manipulate the output results by
aligning multi-level style information and intrinsic knowledge in LDM. Lastly,
we further extend our model to perform video AST. A novel learning objective is
leveraged for video diffusion model training, which significantly improve
cross-frame temporal consistency in the premise of maintaining stylization
strength. Qualitative and quantitative comparisons as well as comprehensive
user studies demonstrate that our HiCAST outperforms the existing SoTA methods
in generating visually plausible stylization results.",2024-01-11 12:26:23+00:00,"['Hanzhang Wang', 'Haoran Wang', 'Jinze Yang', 'Zhongrui Yu', 'Zeke Xie', 'Lei Tian', 'Xinyan Xiao', 'Junjun Jiang', 'Xianming Liu', 'Mingming Sun']",http://arxiv.org/abs/2401.05870v1
Motion Diffusion-Guided 3D Global HMR from a Dynamic Camera,"Motion capture technologies have transformed numerous fields, from the film
and gaming industries to sports science and healthcare, by providing a tool to
capture and analyze human movement in great detail. The holy grail in the topic
of monocular global human mesh and motion reconstruction (GHMR) is to achieve
accuracy on par with traditional multi-view capture on any monocular videos
captured with a dynamic camera, in-the-wild. This is a challenging task as the
monocular input has inherent depth ambiguity, and the moving camera adds
additional complexity as the rendered human motion is now a product of both
human and camera movement. Not accounting for this confusion, existing GHMR
methods often output motions that are unrealistic, e.g. unaccounted root
translation of the human causes foot sliding. We present DiffOpt, a novel 3D
global HMR method using Diffusion Optimization. Our key insight is that recent
advances in human motion generation, such as the motion diffusion model (MDM),
contain a strong prior of coherent human motion. The core of our method is to
optimize the initial motion reconstruction using the MDM prior. This step can
lead to more globally coherent human motion. Our optimization jointly optimizes
the motion prior loss and reprojection loss to correctly disentangle the human
and camera motions. We validate DiffOpt with video sequences from the
Electromagnetic Database of Global 3D Human Pose and Shape in the Wild (EMDB)
and Egobody, and demonstrate superior global human motion recovery capability
over other state-of-the-art global HMR methods most prominently in long video
settings.",2024-11-15 21:09:40+00:00,"['Jaewoo Heo', 'Kuan-Chieh Wang', 'Karen Liu', 'Serena Yeung-Levy']",http://arxiv.org/abs/2411.10582v1
"Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers","Sora unveils the potential of scaling Diffusion Transformer for generating
photorealistic images and videos at arbitrary resolutions, aspect ratios, and
durations, yet it still lacks sufficient implementation details. In this
technical report, we introduce the Lumina-T2X family - a series of Flow-based
Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized
attention, as a unified framework designed to transform noise into images,
videos, multi-view 3D objects, and audio clips conditioned on text
instructions. By tokenizing the latent spatial-temporal space and incorporating
learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X
seamlessly unifies the representations of different modalities across various
spatial-temporal resolutions. This unified approach enables training within a
single framework for different modalities and allows for flexible generation of
multimodal data at any resolution, aspect ratio, and length during inference.
Advanced techniques like RoPE, RMSNorm, and flow matching enhance the
stability, flexibility, and scalability of Flag-DiT, enabling models of
Lumina-T2X to scale up to 7 billion parameters and extend the context window to
128K tokens. This is particularly beneficial for creating ultra-high-definition
images with our Lumina-T2I model and long 720p videos with our Lumina-T2V
model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT,
requires only 35% of the training computational costs of a
600-million-parameter naive DiT. Our further comprehensive analysis underscores
Lumina-T2X's preliminary capability in resolution extrapolation,
high-resolution editing, generating consistent 3D views, and synthesizing
videos with seamless transitions. We expect that the open-sourcing of
Lumina-T2X will further foster creativity, transparency, and diversity in the
generative AI community.",2024-05-09 17:35:16+00:00,"['Peng Gao', 'Le Zhuo', 'Dongyang Liu', 'Ruoyi Du', 'Xu Luo', 'Longtian Qiu', 'Yuhang Zhang', 'Chen Lin', 'Rongjie Huang', 'Shijie Geng', 'Renrui Zhang', 'Junlin Xi', 'Wenqi Shao', 'Zhengkai Jiang', 'Tianshuo Yang', 'Weicai Ye', 'He Tong', 'Jingwen He', 'Yu Qiao', 'Hongsheng Li']",http://arxiv.org/abs/2405.05945v3
URCDM: Ultra-Resolution Image Synthesis in Histopathology,"Diagnosing medical conditions from histopathology data requires a thorough
analysis across the various resolutions of Whole Slide Images (WSI). However,
existing generative methods fail to consistently represent the hierarchical
structure of WSIs due to a focus on high-fidelity patches. To tackle this, we
propose Ultra-Resolution Cascaded Diffusion Models (URCDMs) which are capable
of synthesising entire histopathology images at high resolutions whilst
authentically capturing the details of both the underlying anatomy and
pathology at all magnification levels. We evaluate our method on three separate
datasets, consisting of brain, breast and kidney tissue, and surpass existing
state-of-the-art multi-resolution models. Furthermore, an expert evaluation
study was conducted, demonstrating that URCDMs consistently generate outputs
across various resolutions that trained evaluators cannot distinguish from real
images. All code and additional examples can be found on GitHub.",2024-07-18 08:31:55+00:00,"['Sarah Cechnicka', 'James Ball', 'Matthew Baugh', 'Hadrien Reynaud', 'Naomi Simmonds', 'Andrew P. T. Smith', 'Catherine Horsfield', 'Candice Roufosse', 'Bernhard Kainz']",http://arxiv.org/abs/2407.13277v1
Conditional Generative Models for Contrast-Enhanced Synthesis of T1w and T1 Maps in Brain MRI,"Contrast enhancement by Gadolinium-based contrast agents (GBCAs) is a vital
tool for tumor diagnosis in neuroradiology. Based on brain MRI scans of
glioblastoma before and after Gadolinium administration, we address enhancement
prediction by neural networks with two new contributions. Firstly, we study the
potential of generative models, more precisely conditional diffusion and flow
matching, for uncertainty quantification in virtual enhancement. Secondly, we
examine the performance of T1 scans from quantitive MRI versus T1-weighted
scans. In contrast to T1-weighted scans, these scans have the advantage of a
physically meaningful and thereby comparable voxel range. To compare network
prediction performance of these two modalities with incompatible gray-value
scales, we propose to evaluate segmentations of contrast-enhanced regions of
interest using Dice and Jaccard scores. Across models, we observe better
segmentations with T1 scans than with T1-weighted scans.",2024-10-11 15:11:24+00:00,"['Moritz Piening', 'Fabian Altekrger', 'Gabriele Steidl', 'Elke Hattingen', 'Eike Steidl']",http://arxiv.org/abs/2410.08894v1
Dialogue Director: Bridging the Gap in Dialogue Visualization for Multimodal Storytelling,"Recent advances in AI-driven storytelling have enhanced video generation and
story visualization. However, translating dialogue-centric scripts into
coherent storyboards remains a significant challenge due to limited script
detail, inadequate physical context understanding, and the complexity of
integrating cinematic principles. To address these challenges, we propose
Dialogue Visualization, a novel task that transforms dialogue scripts into
dynamic, multi-view storyboards. We introduce Dialogue Director, a
training-free multimodal framework comprising a Script Director,
Cinematographer, and Storyboard Maker. This framework leverages large
multimodal models and diffusion-based architectures, employing techniques such
as Chain-of-Thought reasoning, Retrieval-Augmented Generation, and multi-view
synthesis to improve script understanding, physical context comprehension, and
cinematic knowledge integration. Experimental results demonstrate that Dialogue
Director outperforms state-of-the-art methods in script interpretation,
physical world understanding, and cinematic principle application,
significantly advancing the quality and controllability of dialogue-based story
visualization.",2024-12-30 05:54:23+00:00,"['Min Zhang', 'Zilin Wang', 'Liyan Chen', 'Kunhong Liu', 'Juncong Lin']",http://arxiv.org/abs/2412.20725v1
Multimodal Latent Language Modeling with Next-Token Diffusion,"Multimodal generative models require a unified approach to handle both
discrete data (e.g., text and code) and continuous data (e.g., image, audio,
video). In this work, we propose Latent Language Modeling (LatentLM), which
seamlessly integrates continuous and discrete data using causal Transformers.
Specifically, we employ a variational autoencoder (VAE) to represent continuous
data as latent vectors and introduce next-token diffusion for autoregressive
generation of these vectors. Additionally, we develop $\sigma$-VAE to address
the challenges of variance collapse, which is crucial for autoregressive
modeling. Extensive experiments demonstrate the effectiveness of LatentLM
across various modalities. In image generation, LatentLM surpasses Diffusion
Transformers in both performance and scalability. When integrated into
multimodal large language models, LatentLM provides a general-purpose interface
that unifies multimodal generation and understanding. Experimental results show
that LatentLM achieves favorable performance compared to Transfusion and vector
quantized models in the setting of scaling up training tokens. In
text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2
model in speaker similarity and robustness, while requiring 10x fewer decoding
steps. The results establish LatentLM as a highly effective and scalable
approach to advance large multimodal models.",2024-12-11 18:57:32+00:00,"['Yutao Sun', 'Hangbo Bao', 'Wenhui Wang', 'Zhiliang Peng', 'Li Dong', 'Shaohan Huang', 'Jianyong Wang', 'Furu Wei']",http://arxiv.org/abs/2412.08635v1
GoMatching: A Simple Baseline for Video Text Spotting via Long and Short Term Matching,"Beyond the text detection and recognition tasks in image text spotting, video
text spotting presents an augmented challenge with the inclusion of tracking.
While advanced end-to-end trainable methods have shown commendable performance,
the pursuit of multi-task optimization may pose the risk of producing
sub-optimal outcomes for individual tasks. In this paper, we identify a main
bottleneck in the state-of-the-art video text spotter: the limited recognition
capability. In response to this issue, we propose to efficiently turn an
off-the-shelf query-based image text spotter into a specialist on video and
present a simple baseline termed GoMatching, which focuses the training efforts
on tracking while maintaining strong recognition performance. To adapt the
image text spotter to video datasets, we add a rescoring head to rescore each
detected instance's confidence via efficient tuning, leading to a better
tracking candidate pool. Additionally, we design a long-short term matching
module, termed LST-Matcher, to enhance the spotter's tracking capability by
integrating both long- and short-term matching results via Transformer. Based
on the above simple designs, GoMatching delivers new records on ICDAR15-video,
DSText, BOVText, and our proposed novel test with arbitrary-shaped text termed
ArTVideo, which demonstrates GoMatching's capability to accommodate general,
dense, small, arbitrary-shaped, Chinese and English text scenarios while saving
considerable training budgets.",2024-01-13 13:59:15+00:00,"['Haibin He', 'Maoyuan Ye', 'Jing Zhang', 'Juhua Liu', 'Bo Du', 'Dacheng Tao']",http://arxiv.org/abs/2401.07080v2
UniAV: Unified Audio-Visual Perception for Multi-Task Video Event Localization,"Video localization tasks aim to temporally locate specific instances in
videos, including temporal action localization (TAL), sound event detection
(SED) and audio-visual event localization (AVEL). Existing methods
over-specialize on each task, overlooking the fact that these instances often
occur in the same video to form the complete video content. In this work, we
present UniAV, a Unified Audio-Visual perception network, to achieve joint
learning of TAL, SED and AVEL tasks for the first time. UniAV can leverage
diverse data available in task-specific datasets, allowing the model to learn
and share mutually beneficial knowledge across tasks and modalities. To tackle
the challenges posed by substantial variations in datasets
(size/domain/duration) and distinct task characteristics, we propose to
uniformly encode visual and audio modalities of all videos to derive generic
representations, while also designing task-specific experts to capture unique
knowledge for each task. Besides, we develop a unified language-aware
classifier by utilizing a pre-trained text encoder, enabling the model to
flexibly detect various types of instances and previously unseen ones by simply
changing prompts during inference. UniAV outperforms its single-task
counterparts by a large margin with fewer parameters, achieving on-par or
superior performances compared to state-of-the-art task-specific methods across
ActivityNet 1.3, DESED and UnAV-100 benchmarks.",2024-04-04 03:28:57+00:00,"['Tiantian Geng', 'Teng Wang', 'Yanfu Zhang', 'Jinming Duan', 'Weili Guan', 'Feng Zheng', 'Ling shao']",http://arxiv.org/abs/2404.03179v2
Unmasking Illusions: Understanding Human Perception of Audiovisual Deepfakes,"The emergence of contemporary deepfakes has attracted significant attention
in machine learning research, as artificial intelligence (AI) generated
synthetic media increases the incidence of misinterpretation and is difficult
to distinguish from genuine content. Currently, machine learning techniques
have been extensively studied for automatically detecting deepfakes. However,
human perception has been less explored. Malicious deepfakes could ultimately
cause public and social problems. Can we humans correctly perceive the
authenticity of the content of the videos we watch? The answer is obviously
uncertain; therefore, this paper aims to evaluate the human ability to discern
deepfake videos through a subjective study. We present our findings by
comparing human observers to five state-ofthe-art audiovisual deepfake
detection models. To this end, we used gamification concepts to provide 110
participants (55 native English speakers and 55 non-native English speakers)
with a webbased platform where they could access a series of 40 videos (20 real
and 20 fake) to determine their authenticity. Each participant performed the
experiment twice with the same 40 videos in different random orders. The videos
are manually selected from the FakeAVCeleb dataset. We found that all AI models
performed better than humans when evaluated on the same 40 videos. The study
also reveals that while deception is not impossible, humans tend to
overestimate their detection capabilities. Our experimental results may help
benchmark human versus machine performance, advance forensics analysis, and
enable adaptive countermeasures.",2024-05-07 07:57:15+00:00,"['Ammarah Hashmi', 'Sahibzada Adil Shahzad', 'Chia-Wen Lin', 'Yu Tsao', 'Hsin-Min Wang']",http://arxiv.org/abs/2405.04097v2
MADRL-Based Rate Adaptation for 360 Video Streaming with Multi-Viewpoint Prediction,"Over the last few years, 360{\deg} video traffic on the network has grown
significantly. A key challenge of 360{\deg} video playback is ensuring a high
quality of experience (QoE) with limited network bandwidth. Currently, most
studies focus on tile-based adaptive bitrate (ABR) streaming based on single
viewport prediction to reduce bandwidth consumption. However, the performance
of models for single-viewpoint prediction is severely limited by the inherent
uncertainty in head movement, which can not cope with the sudden movement of
users very well. This paper first presents a multimodal spatial-temporal
attention transformer to generate multiple viewpoint trajectories with their
probabilities given a historical trajectory. The proposed method models
viewpoint prediction as a classification problem and uses attention mechanisms
to capture the spatial and temporal characteristics of input video frames and
viewpoint trajectories for multi-viewpoint prediction. After that, a
multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing
multi-viewpoint prediction for 360{\deg} video streaming is proposed for
maximizing different QoE objectives under various network conditions. We
formulate the ABR problem as a decentralized partially observable Markov
decision process (Dec-POMDP) problem and present a MAPPO algorithm based on
centralized training and decentralized execution (CTDE) framework to solve the
problem. The experimental results show that our proposed method improves the
defined QoE metric by up to 85.5% compared to existing ABR methods.",2024-05-13 13:59:59+00:00,"['Haopeng Wang', 'Zijian Long', 'Haiwei Dong', 'Abdulmotaleb El Saddik']",http://arxiv.org/abs/2405.07759v2
One-shot Training for Video Object Segmentation,"Video Object Segmentation (VOS) aims to track objects across frames in a
video and segment them based on the initial annotated frame of the target
objects. Previous VOS works typically rely on fully annotated videos for
training. However, acquiring fully annotated training videos for VOS is
labor-intensive and time-consuming. Meanwhile, self-supervised VOS methods have
attempted to build VOS systems through correspondence learning and label
propagation. Still, the absence of mask priors harms their robustness to
complex scenarios, and the label propagation paradigm makes them impractical in
terms of efficiency. To address these issues, we propose, for the first time, a
general one-shot training framework for VOS, requiring only a single labeled
frame per training video and applicable to a majority of state-of-the-art VOS
networks. Specifically, our algorithm consists of: i) Inferring object masks
time-forward based on the initial labeled frame. ii) Reconstructing the initial
object mask time-backward using the masks from step i). Through this
bi-directional training, a satisfactory VOS network can be obtained. Notably,
our approach is extremely simple and can be employed end-to-end. Finally, our
approach uses a single labeled frame of YouTube-VOS and DAVIS datasets to
achieve comparable results to those trained on fully labeled datasets. The code
will be released.",2024-05-22 21:37:08+00:00,"['Baiyu Chen', 'Sixian Chan', 'Xiaoqin Zhang']",http://arxiv.org/abs/2405.14010v1
"Too Many Frames, Not All Useful: Efficient Strategies for Long-Form Video QA","Long-form videos that span across wide temporal intervals are highly
information redundant and contain multiple distinct events or entities that are
often loosely related. Therefore, when performing long-form video question
answering (LVQA), all information necessary to generate a correct response can
often be contained within a small subset of frames. Recent literature explore
use of large language models (LLMs) in LVQA benchmarks, achieving exceptional
performance, while relying on vision language models (VLMs) to convert all
visual content within videos into natural language. Such VLMs often
independently caption a large number of frames uniformly sampled from long
videos, which is not efficient and can mostly be redundant. Questioning these
decision choices, we explore optimal strategies for key-frame selection that
can significantly reduce these redundancies, namely Hierarchical Keyframe
Selector. Our proposed framework, LVNet, achieves state-of-the-art performance
at a comparable caption scale across three benchmark LVQA datasets: EgoSchema,
NExT-QA, and IntentQA, while also demonstrating a strong performance on videos
up to an hour long in VideoMME. Our code will be released publicly. The code
can be found at https://github.com/jongwoopark7978/LVNet.",2024-06-13 17:59:16+00:00,"['Jongwoo Park', 'Kanchana Ranasinghe', 'Kumara Kahatapitiya', 'Wonjeong Ryu', 'Donghyun Kim', 'Michael S. Ryoo']",http://arxiv.org/abs/2406.09396v5
DaBiT: Depth and Blur informed Transformer for Video Focal Deblurring,"In many real-world scenarios, recorded videos suffer from accidental focus
blur, and while video deblurring methods exist, most specifically target motion
blur or spatial-invariant blur. This paper introduces a framework optimized for
the as yet unattempted task of video focal deblurring (refocusing). The
proposed method employs novel map-guided transformers, in addition to image
propagation, to effectively leverage the continuous spatial variance of focal
blur and restore the footage. We also introduce a flow re-focusing module
designed to efficiently align relevant features between blurry and sharp
domains. Additionally, we propose a novel technique for generating synthetic
focal blur data, broadening the model's learning capabilities and robustness to
include a wider array of content. We have made a new benchmark dataset,
DAVIS-Blur, available. This dataset, a modified extension of the popular DAVIS
video segmentation set, provides realistic focal blur degradations as well as
the corresponding blur maps. Comprehensive experiments demonstrate the
superiority of our approach. We achieve state-of-the-art results with an
average PSNR performance over 1.9dB greater than comparable existing video
restoration methods. Our source code and the developed databases will be made
available at https://github.com/crispianm/DaBiT",2024-07-01 12:22:16+00:00,"['Crispian Morris', 'Nantheera Anantrasirichai', 'Fan Zhang', 'David Bull']",http://arxiv.org/abs/2407.01230v3
Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design,"Deep neural networks (DNNs) are frequently employed in a variety of computer
vision applications. Nowadays, an emerging trend in the current video
distribution system is to take advantage of DNN's overfitting properties to
perform video resolution upscaling. By splitting videos into chunks and
applying a super-resolution (SR) model to overfit each chunk, this scheme of SR
models plus video chunks is able to replace traditional video transmission to
enhance video quality and transmission efficiency. However, many models and
chunks are needed to guarantee high performance, which leads to tremendous
overhead on model switching and memory footprints at the user end. To resolve
such problems, we propose a Dynamic Deep neural network assisted by a
Content-Aware data processing pipeline to reduce the model number down to one
(Dy-DCA), which helps promote performance while conserving computational
resources. Additionally, to achieve real acceleration on the user end, we
designed a framework that optimizes dynamic features (e.g., dynamic shapes,
sizes, and control flow) in Dy-DCA to enable a series of compilation
optimizations, including fused code generation, static execution planning, etc.
By employing such techniques, our method achieves better PSNR and real-time
performance (33 FPS) on an off-the-shelf mobile phone. Meanwhile, assisted by
our compilation optimization, we achieve a 1.7$\times$ speedup while saving up
to 1.61$\times$ memory consumption. Code available in
https://github.com/coulsonlee/Dy-DCA-ECCV2024.",2024-07-03 05:17:26+00:00,"['Gen Li', 'Zhihao Shu', 'Jie Ji', 'Minghai Qin', 'Fatemeh Afghah', 'Wei Niu', 'Xiaolong Ma']",http://arxiv.org/abs/2407.02813v2
ReLaX-VQA: Residual Fragment and Layer Stack Extraction for Enhancing Video Quality Assessment,"With the rapid growth of User-Generated Content (UGC) exchanged between users
and sharing platforms, the need for video quality assessment in the wild is
increasingly evident. UGC is typically acquired using consumer devices and
undergoes multiple rounds of compression (transcoding) before reaching the end
user. Therefore, traditional quality metrics that employ the original content
as a reference are not suitable. In this paper, we propose ReLaX-VQA, a novel
No-Reference Video Quality Assessment (NR-VQA) model that aims to address the
challenges of evaluating the quality of diverse video content without reference
to the original uncompressed videos. ReLaX-VQA uses frame differences to select
spatio-temporal fragments intelligently together with different expressions of
spatial features associated with the sampled frames. These are then used to
better capture spatial and temporal variabilities in the quality of
neighbouring frames. Furthermore, the model enhances abstraction by employing
layer-stacking techniques in deep neural network features from Residual
Networks and Vision Transformers. Extensive testing across four UGC datasets
demonstrates that ReLaX-VQA consistently outperforms existing NR-VQA methods,
achieving an average SRCC of 0.8658 and PLCC of 0.8873. Open-source code and
trained models that will facilitate further research and applications of NR-VQA
can be found at https://github.com/xinyiW915/ReLaX-VQA.",2024-07-16 08:33:55+00:00,"['Xinyi Wang', 'Angeliki Katsenou', 'David Bull']",http://arxiv.org/abs/2407.11496v3
VidLPRO: A $\underline{Vid}$eo-$\underline{L}$anguage $\underline{P}$re-training Framework for $\underline{Ro}$botic and Laparoscopic Surgery,"We introduce VidLPRO, a novel video-language (VL) pre-training framework
designed specifically for robotic and laparoscopic surgery. While existing
surgical VL models primarily rely on contrastive learning, we propose a more
comprehensive approach to capture the intricate temporal dynamics and align
video with language. VidLPRO integrates video-text contrastive learning,
video-text matching, and masked language modeling objectives to learn rich VL
representations. To support this framework, we present GenSurg+, a carefully
curated dataset derived from GenSurgery, comprising 17k surgical video clips
paired with captions generated by GPT-4 using transcripts extracted by the
Whisper model. This dataset addresses the need for large-scale, high-quality VL
data in the surgical domain. Extensive experiments on benchmark datasets,
including Cholec80 and AutoLaparo, demonstrate the efficacy of our approach.
VidLPRO achieves state-of-the-art performance in zero-shot surgical phase
recognition, significantly outperforming existing surgical VL models such as
SurgVLP and HecVL. Our model demonstrates improvements of up to 21.5\% in
accuracy and 15.7% in F1 score, setting a new benchmark in the field. Notably,
VidLPRO exhibits robust performance even with single-frame inference, while
effectively scaling with increased temporal context. Ablation studies reveal
the impact of frame sampling strategies on model performance and computational
efficiency. These results underscore VidLPRO's potential as a foundation model
for surgical video understanding.",2024-09-07 06:33:12+00:00,"['Mohammadmahdi Honarmand', 'Muhammad Abdullah Jamal', 'Omid Mohareri']",http://arxiv.org/abs/2409.04732v2
VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying Misinformation of Short Videos,"Short video platforms have become important channels for news dissemination,
offering a highly engaging and immediate way for users to access current events
and share information. However, these platforms have also emerged as
significant conduits for the rapid spread of misinformation, as fake news and
rumors can leverage the visual appeal and wide reach of short videos to
circulate extensively among audiences. Existing fake news detection methods
mainly rely on single-modal information, such as text or images, or apply only
basic fusion techniques, limiting their ability to handle the complex,
multi-layered information inherent in short videos. To address these
limitations, this paper presents a novel fake news detection method based on
multimodal information, designed to identify misinformation through a
multi-level analysis of video content. This approach effectively utilizes
different modal representations to generate a unified textual description,
which is then fed into a large language model for comprehensive evaluation. The
proposed framework successfully integrates multimodal features within videos,
significantly enhancing the accuracy and reliability of fake news detection.
Experimental results demonstrate that the proposed approach outperforms
existing models in terms of accuracy, robustness, and utilization of multimodal
information, achieving an accuracy of 90.93%, which is significantly higher
than the best baseline model (SV-FEND) at 81.05%. Furthermore, case studies
provide additional evidence of the effectiveness of the approach in accurately
distinguishing between fake news, debunking content, and real incidents,
highlighting its reliability and robustness in real-world applications.",2024-11-15 08:20:26+00:00,"['Weihao Zhong', 'Yinhao Xiao', 'Minghui Xu', 'Xiuzhen Cheng']",http://arxiv.org/abs/2411.10032v1
VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection,"The advancement of Large Vision Language Models (LVLMs) has significantly
improved multimodal understanding, yet challenges remain in video reasoning
tasks due to the scarcity of high-quality, large-scale datasets. Existing video
question-answering (VideoQA) datasets often rely on costly manual annotations
with insufficient granularity or automatic construction methods with redundant
frame-by-frame analysis, limiting their scalability and effectiveness for
complex reasoning. To address these challenges, we introduce VideoEspresso, a
novel dataset that features VideoQA pairs preserving essential spatial details
and temporal coherence, along with multimodal annotations of intermediate
reasoning steps. Our construction pipeline employs a semantic-aware method to
reduce redundancy, followed by generating QA pairs using GPT-4o. We further
develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,
guiding GPT-4o in extracting logical relationships from QA pairs and video
content. To exploit the potential of high-quality VideoQA pairs, we propose a
Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a
two-stage instruction fine-tuned reasoning LVLM. This framework adaptively
selects core frames and performs CoT reasoning using multimodal evidence.
Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our
method outperforms existing baselines on most tasks, demonstrating superior
video reasoning capabilities. Our code and dataset will be released at:
https://github.com/hshjerry/VideoEspresso",2024-11-22 08:33:36+00:00,"['Songhao Han', 'Wei Huang', 'Hairong Shi', 'Le Zhuo', 'Xiu Su', 'Shifeng Zhang', 'Xu Zhou', 'Xiaojuan Qi', 'Yue Liao', 'Si Liu']",http://arxiv.org/abs/2411.14794v1
Real-Time Anomaly Detection in Video Streams,"This thesis is part of a CIFRE agreement between the company Othello and the
LIASD laboratory. The objective is to develop an artificial intelligence system
that can detect real-time dangers in a video stream. To achieve this, a novel
approach combining temporal and spatial analysis has been proposed. Several
avenues have been explored to improve anomaly detection by integrating object
detection, human pose detection, and motion analysis. For result
interpretability, techniques commonly used for image analysis, such as
activation and saliency maps, have been extended to videos, and an original
method has been proposed. The proposed architecture performs binary or
multiclass classification depending on whether an alert or the cause needs to
be identified. Numerous neural networkmodels have been tested, and three of
them have been selected. You Only Looks Once (YOLO) has been used for spatial
analysis, a Convolutional Recurrent Neuronal Network (CRNN) composed of VGG19
and a Gated Recurrent Unit (GRU) for temporal analysis, and a multi-layer
perceptron for classification. These models handle different types of data and
can be combined in parallel or in series. Although the parallel mode is faster,
the serial mode is generally more reliable. For training these models,
supervised learning was chosen, and two proprietary datasets were created. The
first dataset focuses on objects that may play a potential role in anomalies,
while the second consists of videos containing anomalies or non-anomalies. This
approach allows for the processing of both continuous video streams and finite
videos, providing greater flexibility in detection.",2024-11-29 14:24:33+00:00,['Fabien Poirier'],http://arxiv.org/abs/2411.19731v1
VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval,"Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video
analysis. Recent joint prediction transformer models often overlook their
cross-task dynamics and video-text alignment and refinement. Moreover, most
models typically use limited, uni-directional attention mechanisms, resulting
in weakly integrated representations and suboptimal performance in capturing
the interdependence between video and text modalities. Although large-language
and vision-language models (LLM/LVLMs) have gained prominence across various
domains, their application in this field remains relatively underexplored. Here
we propose VideoLights, a novel HD/MR framework addressing these limitations
through (i) Convolutional Projection and Feature Refinement modules with an
alignment loss for better video-text feature alignment, (ii) Bi-Directional
Cross-Modal Fusion network for strongly coupled query-aware clip
representations, and (iii) Uni-directional joint-task feedback mechanism
enhancing both tasks through correlation. In addition, (iv) we introduce hard
positive/negative losses for adaptive error penalization and improved learning,
and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration
and intelligent pretraining using synthetic data generated from LVLMs.
Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks
demonstrate state-of-the-art performance. Codes and models are available at
https://github.com/dpaul06/VideoLights .",2024-12-02 14:45:53+00:00,"['Dhiman Paul', 'Md Rizwan Parvez', 'Nabeel Mohammed', 'Shafin Rahman']",http://arxiv.org/abs/2412.01558v1
SweetTok: Semantic-Aware Spatial-Temporal Tokenizer for Compact Video Discretization,"This paper presents the \textbf{S}emantic-a\textbf{W}ar\textbf{E}
spatial-t\textbf{E}mporal \textbf{T}okenizer (SweetTok), a novel video
tokenizer to overcome the limitations in current video tokenization methods for
compacted yet effective discretization. Unlike previous approaches that process
flattened local visual patches via direct discretization or adaptive query
tokenization, SweetTok proposes a decoupling framework, compressing visual
inputs through distinct spatial and temporal queries via \textbf{D}ecoupled
\textbf{Q}uery \textbf{A}uto\textbf{E}ncoder (DQAE). This design allows
SweetTok to efficiently compress video token count while achieving superior
fidelity by capturing essential information across spatial and temporal
dimensions. Furthermore, we design a \textbf{M}otion-enhanced \textbf{L}anguage
\textbf{C}odebook (MLC) tailored for spatial and temporal compression to
address the differences in semantic representation between appearance and
motion information. SweetTok significantly improves video reconstruction
results by \textbf{42.8\%} w.r.t rFVD on UCF-101 dataset. With a better token
compression strategy, it also boosts downstream video generation results by
\textbf{15.1\%} w.r.t gFVD. Additionally, the compressed decoupled tokens are
imbued with semantic information, enabling few-shot recognition capabilities
powered by LLMs in downstream applications.",2024-12-11 13:48:06+00:00,"['Zhentao Tan', 'Ben Xue', 'Jian Jia', 'Junhao Wang', 'Wencai Ye', 'Shaoyun Shi', 'Mingjie Sun', 'Wenjin Wu', 'Quan Chen', 'Peng Jiang']",http://arxiv.org/abs/2412.10443v3
BlendScape: Enabling End-User Customization of Video-Conferencing Environments through Generative AI,"Today's video-conferencing tools support a rich range of professional and
social activities, but their generic meeting environments cannot be dynamically
adapted to align with distributed collaborators' needs. To enable end-user
customization, we developed BlendScape, a rendering and composition system for
video-conferencing participants to tailor environments to their meeting context
by leveraging AI image generation techniques. BlendScape supports flexible
representations of task spaces by blending users' physical or digital
backgrounds into unified environments and implements multimodal interaction
techniques to steer the generation. Through an exploratory study with 15
end-users, we investigated whether and how they would find value in using
generative AI to customize video-conferencing environments. Participants
envisioned using a system like BlendScape to facilitate collaborative
activities in the future, but required further controls to mitigate distracting
or unrealistic visual elements. We implemented scenarios to demonstrate
BlendScape's expressiveness for supporting environment design strategies from
prior work and propose composition techniques to improve the quality of
environments.",2024-03-20 19:41:05+00:00,"['Shwetha Rajaram', 'Nels Numan', 'Balasaravanan Thoravi Kumaravel', 'Nicolai Marquardt', 'Andrew D. Wilson']",http://arxiv.org/abs/2403.13947v2
Action-conditioned video data improves predictability,"Long-term video generation and prediction remain challenging tasks in
computer vision, particularly in partially observable scenarios where cameras
are mounted on moving platforms. The interaction between observed image frames
and the motion of the recording agent introduces additional complexities. To
address these issues, we introduce the Action-Conditioned Video Generation
(ACVG) framework, a novel approach that investigates the relationship between
actions and generated image frames through a deep dual Generator-Actor
architecture. ACVG generates video sequences conditioned on the actions of
robots, enabling exploration and analysis of how vision and action mutually
influence one another in dynamic environments. We evaluate the framework's
effectiveness on an indoor robot motion dataset which consists of sequences of
image frames along with the sequences of actions taken by the robotic agent,
conducting a comprehensive empirical study comparing ACVG to other
state-of-the-art frameworks along with a detailed ablation study.",2024-04-08 12:18:01+00:00,"['Meenakshi Sarkar', 'Debasish Ghose']",http://arxiv.org/abs/2404.05439v1
MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation,"Recent advancements in personalized Text-to-Video (T2V) generation highlight
the importance of integrating character-specific identities and actions.
However, previous T2V models struggle with identity consistency and
controllable motion dynamics, mainly due to limited fine-grained facial and
action-based textual prompts, and datasets that overlook key human attributes
and actions. To address these challenges, we propose MotionCharacter, an
efficient and high-fidelity human video generation framework designed for
identity preservation and fine-grained motion control. We introduce an
ID-preserving module to maintain identity fidelity while allowing flexible
attribute modifications, and further integrate ID-consistency and region-aware
loss mechanisms, significantly enhancing identity consistency and detail
fidelity. Additionally, our approach incorporates a motion control module that
prioritizes action-related text while maintaining subject consistency, along
with a dataset, Human-Motion, which utilizes large language models to generate
detailed motion descriptions. For simplify user control during inference, we
parameterize motion intensity through a single coefficient, allowing for easy
adjustments. Extensive experiments highlight the effectiveness of
MotionCharacter, demonstrating significant improvements in ID-preserving,
high-quality video generation.",2024-11-27 12:15:52+00:00,"['Haopeng Fang', 'Di Qiu', 'Binjie Mao', 'Pengfei Yan', 'He Tang']",http://arxiv.org/abs/2411.18281v2
UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics,"We introduce UniReal, a unified framework designed to address various image
generation and editing tasks. Existing solutions often vary by tasks, yet share
fundamental principles: preserving consistency between inputs and outputs while
capturing visual variations. Inspired by recent video generation models that
effectively balance consistency and variation across frames, we propose a
unifying approach that treats image-level tasks as discontinuous video
generation. Specifically, we treat varying numbers of input and output images
as frames, enabling seamless support for tasks such as image generation,
editing, customization, composition, etc. Although designed for image-level
tasks, we leverage videos as a scalable source for universal supervision.
UniReal learns world dynamics from large-scale videos, demonstrating advanced
capability in handling shadows, reflections, pose variation, and object
interaction, while also exhibiting emergent capability for novel applications.",2024-12-10 18:59:55+00:00,"['Xi Chen', 'Zhifei Zhang', 'He Zhang', 'Yuqian Zhou', 'Soo Ye Kim', 'Qing Liu', 'Yijun Li', 'Jianming Zhang', 'Nanxuan Zhao', 'Yilin Wang', 'Hui Ding', 'Zhe Lin', 'Hengshuang Zhao']",http://arxiv.org/abs/2412.07774v2
X-Ray: A Sequential 3D Representation For Generation,"We introduce X-Ray, a novel 3D sequential representation inspired by the
penetrability of x-ray scans. X-Ray transforms a 3D object into a series of
surface frames at different layers, making it suitable for generating 3D models
from images. Our method utilizes ray casting from the camera center to capture
geometric and textured details, including depth, normal, and color, across all
intersected surfaces. This process efficiently condenses the whole 3D object
into a multi-frame video format, motivating the utilize of a network
architecture similar to those in video diffusion models. This design ensures an
efficient 3D representation by focusing solely on surface information. Also, we
propose a two-stage pipeline to generate 3D objects from X-Ray Diffusion Model
and Upsampler. We demonstrate the practicality and adaptability of our X-Ray
representation by synthesizing the complete visible and hidden surfaces of a 3D
object from a single input image. Experimental results reveal the
state-of-the-art superiority of our representation in enhancing the accuracy of
3D generation, paving the way for new 3D representation research and practical
applications.",2024-04-22 16:40:11+00:00,"['Tao Hu', 'Wenhang Ge', 'Yuyang Zhao', 'Gim Hee Lee']",http://arxiv.org/abs/2404.14329v2
Multitask Learning in Minimally Invasive Surgical Vision: A Review,"Minimally invasive surgery (MIS) has revolutionized many procedures and led
to reduced recovery time and risk of patient injury. However, MIS poses
additional complexity and burden on surgical teams. Data-driven surgical vision
algorithms are thought to be key building blocks in the development of future
MIS systems with improved autonomy. Recent advancements in machine learning and
computer vision have led to successful applications in analyzing videos
obtained from MIS with the promise of alleviating challenges in MIS videos.
Surgical scene and action understanding encompasses multiple related tasks
that, when solved individually, can be memory-intensive, inefficient, and fail
to capture task relationships. Multitask learning (MTL), a learning paradigm
that leverages information from multiple related tasks to improve performance
and aid generalization, is well suited for fine-grained and high-level
understanding of MIS data. This review provides a narrative overview of the
current state-of-the-art MTL systems that leverage videos obtained from MIS.
Beyond listing published approaches, we discuss the benefits and limitations of
these MTL systems. Moreover, this manuscript presents an analysis of the
literature for various application fields of MTL in MIS, including those with
large models, highlighting notable trends, new directions of research, and
developments.",2024-01-16 10:18:57+00:00,"['Oluwatosin Alabi', 'Tom Vercauteren', 'Miaojing Shi']",http://arxiv.org/abs/2401.08256v2
Bounding Boxes and Probabilistic Graphical Models: Video Anomaly Detection Simplified,"In this study, we formulate the task of Video Anomaly Detection as a
probabilistic analysis of object bounding boxes. We hypothesize that the
representation of objects via their bounding boxes only, can be sufficient to
successfully identify anomalous events in a scene. The implied value of this
approach is increased object anonymization, faster model training and fewer
computational resources. This can particularly benefit applications within
video surveillance running on edge devices such as cameras. We design our model
based on human reasoning which lends itself to explaining model output in
human-understandable terms. Meanwhile, the slowest model trains within less
than 7 seconds on a 11th Generation Intel Core i9 Processor. While our approach
constitutes a drastic reduction of problem feature space in comparison with
prior art, we show that this does not result in a reduction in performance: the
results we report are highly competitive on the benchmark datasets CUHK Avenue
and ShanghaiTech, and significantly exceed on the latest State-of-the-Art
results on StreetScene, which has so far proven to be the most challenging VAD
dataset.",2024-07-08 14:52:03+00:00,"['Mia Siemon', 'Thomas B. Moeslund', 'Barry Norton', 'Kamal Nasrollahi']",http://arxiv.org/abs/2407.06000v2
SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners,"We introduce SAM2Point, a preliminary exploration adapting Segment Anything
Model 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point
interprets any 3D data as a series of multi-directional videos, and leverages
SAM 2 for 3D-space segmentation, without further training or 2D-3D projection.
Our framework supports various prompt types, including 3D points, boxes, and
masks, and can generalize across diverse scenarios, such as 3D objects, indoor
scenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple
3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight
the robust generalization capabilities of SAM2Point. To our best knowledge, we
present the most faithful implementation of SAM in 3D, which may serve as a
starting point for future research in promptable 3D segmentation. Online Demo:
https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:
https://github.com/ZiyuGuo99/SAM2Point .",2024-08-29 17:59:45+00:00,"['Ziyu Guo', 'Renrui Zhang', 'Xiangyang Zhu', 'Chengzhuo Tong', 'Peng Gao', 'Chunyuan Li', 'Pheng-Ann Heng']",http://arxiv.org/abs/2408.16768v1
ACT-Bench: Towards Action Controllable World Models for Autonomous Driving,"World models have emerged as promising neural simulators for autonomous
driving, with the potential to supplement scarce real-world data and enable
closed-loop evaluations. However, current research primarily evaluates these
models based on visual realism or downstream task performance, with limited
focus on fidelity to specific action instructions - a crucial property for
generating targeted simulation scenes. Although some studies address action
fidelity, their evaluations rely on closed-source mechanisms, limiting
reproducibility. To address this gap, we develop an open-access evaluation
framework, ACT-Bench, for quantifying action fidelity, along with a baseline
world model, Terra. Our benchmarking framework includes a large-scale dataset
pairing short context videos from nuScenes with corresponding future trajectory
data, which provides conditional input for generating future video frames and
enables evaluation of action fidelity for executed motions. Furthermore, Terra
is trained on multiple large-scale trajectory-annotated datasets to enhance
action fidelity. Leveraging this framework, we demonstrate that the
state-of-the-art model does not fully adhere to given instructions, while Terra
achieves improved action fidelity. All components of our benchmark framework
will be made publicly available to support future research.",2024-12-06 01:06:28+00:00,"['Hidehisa Arai', 'Keishi Ishihara', 'Tsubasa Takahashi', 'Yu Yamaguchi']",http://arxiv.org/abs/2412.05337v1
FreeStyle: Free Lunch for Text-guided Style Transfer using Diffusion Models,"The rapid development of generative diffusion models has significantly
advanced the field of style transfer. However, most current style transfer
methods based on diffusion models typically involve a slow iterative
optimization process, e.g., model fine-tuning and textual inversion of style
concept. In this paper, we introduce FreeStyle, an innovative style transfer
method built upon a pre-trained large diffusion model, requiring no further
optimization. Besides, our method enables style transfer only through a text
description of the desired style, eliminating the necessity of style images.
Specifically, we propose a dual-stream encoder and single-stream decoder
architecture, replacing the conventional U-Net in diffusion models. In the
dual-stream encoder, two distinct branches take the content image and style
text prompt as inputs, achieving content and style decoupling. In the decoder,
we further modulate features from the dual streams based on a given content
image and the corresponding style text prompt for precise style transfer. Our
experimental results demonstrate high-quality synthesis and fidelity of our
method across various content images and style text prompts. Compared with
state-of-the-art methods that require training, our FreeStyle approach notably
reduces the computational burden by thousands of iterations, while achieving
comparable or superior performance across multiple evaluation metrics including
CLIP Aesthetic Score, CLIP Score, and Preference. We have released the code at:
https://github.com/FreeStyleFreeLunch/FreeStyle.",2024-01-28 12:00:31+00:00,"['Feihong He', 'Gang Li', 'Fuhui Sun', 'Mengyuan Zhang', 'Lingyu Si', 'Xiaoyan Wang', 'Li Shen']",http://arxiv.org/abs/2401.15636v3
Re-DiffiNet: Modeling discrepancies in tumor segmentation using diffusion models,"Identification of tumor margins is essential for surgical decision-making for
glioblastoma patients and provides reliable assistance for neurosurgeons.
Despite improvements in deep learning architectures for tumor segmentation over
the years, creating a fully autonomous system suitable for clinical floors
remains a formidable challenge because the model predictions have not yet
reached the desired level of accuracy and generalizability for clinical
applications. Generative modeling techniques have seen significant improvements
in recent times. Specifically, Generative Adversarial Networks (GANs) and
Denoising-diffusion-based models (DDPMs) have been used to generate
higher-quality images with fewer artifacts and finer attributes. In this work,
we introduce a framework called Re-Diffinet for modeling the discrepancy
between the outputs of a segmentation model like U-Net and the ground truth,
using DDPMs. By explicitly modeling the discrepancy, the results show an
average improvement of 0.55\% in the Dice score and 16.28\% in HD95 from
cross-validation over 5-folds, compared to the state-of-the-art U-Net
segmentation model.",2024-02-12 01:03:39+00:00,"['Tianyi Ren', 'Abhishek Sharma', 'Juampablo Heras Rivera', 'Harshitha Rebala', 'Ethan Honey', 'Agamdeep Chopra', 'Jacob Ruzevick', 'Mehmet Kurt']",http://arxiv.org/abs/2402.07354v4
TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for Dynamic UAV-based Scenes,"In this paper, we present a new approach to bridge the domain gap between
synthetic and real-world data for unmanned aerial vehicle (UAV)-based
perception. Our formulation is designed for dynamic scenes, consisting of small
moving objects or human actions. We propose an extension of K-Planes Neural
Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature
vectors. The tiered feature vectors are generated to effectively model
conceptual information about a scene as well as an image decoder that
transforms output feature maps into RGB images. Our technique leverages the
information amongst both static and dynamic objects within a scene and is able
to capture salient scene attributes of high altitude videos. We evaluate its
performance on challenging datasets, including Okutama Action and UG2, and
observe considerable improvement in accuracy over state of the art neural
rendering methods.",2024-05-04 21:55:33+00:00,"['Christopher Maxey', 'Jaehoon Choi', 'Yonghan Lee', 'Hyungtae Lee', 'Dinesh Manocha', 'Heesung Kwon']",http://arxiv.org/abs/2405.02762v2
Task-Oriented Hierarchical Object Decomposition for Visuomotor Control,"Good pre-trained visual representations could enable robots to learn
visuomotor policy efficiently. Still, existing representations take a
one-size-fits-all-tasks approach that comes with two important drawbacks: (1)
Being completely task-agnostic, these representations cannot effectively ignore
any task-irrelevant information in the scene, and (2) They often lack the
representational capacity to handle unconstrained/complex real-world scenes.
Instead, we propose to train a large combinatorial family of representations
organized by scene entities: objects and object parts. This hierarchical object
decomposition for task-oriented representations (HODOR) permits selectively
assembling different representations specific to each task while scaling in
representational capacity with the complexity of the scene and the task. In our
experiments, we find that HODOR outperforms prior pre-trained representations,
both scene vector representations and object-centric representations, for
sample-efficient imitation learning across 5 simulated and 5 real-world
manipulation tasks. We further find that the invariances captured in HODOR are
inherited into downstream policies, which can robustly generalize to
out-of-distribution test conditions, permitting zero-shot skill chaining.
Appendix, code, and videos: https://sites.google.com/view/hodor-corl24.",2024-11-02 15:28:06+00:00,"['Jianing Qian', 'Yunshuang Li', 'Bernadette Bucher', 'Dinesh Jayaraman']",http://arxiv.org/abs/2411.01284v1
Leveraging Scene Geometry and Depth Information for Robust Image Deraining,"Image deraining holds great potential for enhancing the vision of autonomous
vehicles in rainy conditions, contributing to safer driving. Previous works
have primarily focused on employing a single network architecture to generate
derained images. However, they often fail to fully exploit the rich prior
knowledge embedded in the scenes. Particularly, most methods overlook the depth
information that can provide valuable context about scene geometry and guide
more robust deraining. In this work, we introduce a novel learning framework
that integrates multiple networks: an AutoEncoder for deraining, an auxiliary
network to incorporate depth information, and two supervision networks to
enforce feature consistency between rainy and clear scenes. This multi-network
design enables our model to effectively capture the underlying scene structure,
producing clearer and more accurately derained images, leading to improved
object detection for autonomous vehicles. Extensive experiments on three
widely-used datasets demonstrated the effectiveness of our proposed method.",2024-12-27 20:18:46+00:00,"['Ningning Xu', 'Jidong J. Yang']",http://arxiv.org/abs/2412.19913v1
XS-VID: An Extremely Small Video Object Detection Dataset,"Small Video Object Detection (SVOD) is a crucial subfield in modern computer
vision, essential for early object discovery and detection. However, existing
SVOD datasets are scarce and suffer from issues such as insufficiently small
objects, limited object categories, and lack of scene diversity, leading to
unitary application scenarios for corresponding methods. To address this gap,
we develop the XS-VID dataset, which comprises aerial data from various periods
and scenes, and annotates eight major object categories. To further evaluate
existing methods for detecting extremely small objects, XS-VID extensively
collects three types of objects with smaller pixel areas: extremely small
(\textit{es}, $0\sim12^2$), relatively small (\textit{rs}, $12^2\sim20^2$), and
generally small (\textit{gs}, $20^2\sim32^2$). XS-VID offers unprecedented
breadth and depth in covering and quantifying minuscule objects, significantly
enriching the scene and object diversity in the dataset. Extensive validations
on XS-VID and the publicly available VisDrone2019VID dataset show that existing
methods struggle with small object detection and significantly underperform
compared to general object detectors. Leveraging the strengths of previous
methods and addressing their weaknesses, we propose YOLOFT, which enhances
local feature associations and integrates temporal motion features,
significantly improving the accuracy and stability of SVOD. Our datasets and
benchmarks are available at \url{https://gjhhust.github.io/XS-VID/}.",2024-07-25 15:42:46+00:00,"['Jiahao Guo', 'Ziyang Xu', 'Lianjun Wu', 'Fei Gao', 'Wenyu Liu', 'Xinggang Wang']",http://arxiv.org/abs/2407.18137v1
Evaluating Text-to-Visual Generation with Image-to-Text Generation,"Despite significant progress in generative AI, comprehensive evaluation
remains challenging because of the lack of effective metrics and standardized
benchmarks. For instance, the widely-used CLIPScore measures the alignment
between a (generated) image and text prompt, but it fails to produce reliable
scores for complex prompts involving compositions of objects, attributes, and
relations. One reason is that text encoders of CLIP can notoriously act as a
""bag of words"", conflating prompts such as ""the horse is eating the grass"" with
""the grass is eating the horse"". To address this, we introduce the VQAScore,
which uses a visual-question-answering (VQA) model to produce an alignment
score by computing the probability of a ""Yes"" answer to a simple ""Does this
figure show '{text}'?"" question. Though simpler than prior art, VQAScore
computed with off-the-shelf models produces state-of-the-art results across
many (8) image-text alignment benchmarks. We also compute VQAScore with an
in-house model that follows best practices in the literature. For example, we
use a bidirectional image-question encoder that allows image embeddings to
depend on the question being asked (and vice versa). Our in-house model,
CLIP-FlanT5, outperforms even the strongest baselines that make use of the
proprietary GPT-4V. Interestingly, although we train with only images, VQAScore
can also align text with video and 3D models. VQAScore allows researchers to
benchmark text-to-visual generation using complex texts that capture the
compositional structure of real-world prompts. We introduce GenAI-Bench, a more
challenging benchmark with 1,600 compositional text prompts that require
parsing scenes, objects, attributes, relationships, and high-order reasoning
like comparison and logic. GenAI-Bench also offers over 15,000 human ratings
for leading image and video generation models such as Stable Diffusion, DALL-E
3, and Gen2.",2024-04-01 17:58:06+00:00,"['Zhiqiu Lin', 'Deepak Pathak', 'Baiqi Li', 'Jiayao Li', 'Xide Xia', 'Graham Neubig', 'Pengchuan Zhang', 'Deva Ramanan']",http://arxiv.org/abs/2404.01291v2
Multiscale Latent Diffusion Model for Enhanced Feature Extraction from Medical Images,"Various imaging modalities are used in patient diagnosis, each offering
unique advantages and valuable insights into anatomy and pathology. Computed
Tomography (CT) is crucial in diagnostics, providing high-resolution images for
precise internal organ visualization. CT's ability to detect subtle tissue
variations is vital for diagnosing diseases like lung cancer, enabling early
detection and accurate tumor assessment. However, variations in CT scanner
models and acquisition protocols introduce significant variability in the
extracted radiomic features, even when imaging the same patient. This
variability poses considerable challenges for downstream research and clinical
analysis, which depend on consistent and reliable feature extraction. Current
methods for medical image feature extraction, often based on supervised
learning approaches, including GAN-based models, face limitations in
generalizing across different imaging environments. In response to these
challenges, we propose LTDiff++, a multiscale latent diffusion model designed
to enhance feature extraction in medical imaging. The model addresses
variability by standardizing non-uniform distributions in the latent space,
improving feature consistency. LTDiff++ utilizes a UNet++ encoder-decoder
architecture coupled with a conditional Denoising Diffusion Probabilistic Model
(DDPM) at the latent bottleneck to achieve robust feature extraction and
standardization. Extensive empirical evaluations on both patient and phantom CT
datasets demonstrate significant improvements in image standardization, with
higher Concordance Correlation Coefficients (CCC) across multiple radiomic
feature categories. Through these advancements, LTDiff++ represents a promising
solution for overcoming the inherent variability in medical imaging data,
offering improved reliability and accuracy in feature extraction processes.",2024-10-05 02:13:57+00:00,"['Rabeya Tus Sadia', 'Jie Zhang', 'Jin Chen']",http://arxiv.org/abs/2410.04000v3
CamI2V: Camera-Controlled Image-to-Video Diffusion Model,"Recent advancements have integrated camera pose as a user-friendly and
physics-informed condition in video diffusion models, enabling precise camera
control. In this paper, we identify one of the key challenges as effectively
modeling noisy cross-frame interactions to enhance geometry consistency and
camera controllability. We innovatively associate the quality of a condition
with its ability to reduce uncertainty and interpret noisy cross-frame features
as a form of noisy condition. Recognizing that noisy conditions provide
deterministic information while also introducing randomness and potential
misguidance due to added noise, we propose applying epipolar attention to only
aggregate features along corresponding epipolar lines, thereby accessing an
optimal amount of noisy conditions. Additionally, we address scenarios where
epipolar lines disappear, commonly caused by rapid camera movements, dynamic
objects, or occlusions, ensuring robust performance in diverse environments.
Furthermore, we develop a more robust and reproducible evaluation pipeline to
address the inaccuracies and instabilities of existing camera control metrics.
Our method achieves a 25.64% improvement in camera controllability on the
RealEstate10K dataset without compromising dynamics or generation quality and
demonstrates strong generalization to out-of-domain images. Training and
inference require only 24GB and 12GB of memory, respectively, for 16-frame
sequences at 256x256 resolution. We will release all checkpoints, along with
training and evaluation code. Dynamic videos are best viewed at
https://zgctroy.github.io/CamI2V.",2024-10-21 12:36:27+00:00,"['Guangcong Zheng', 'Teng Li', 'Rui Jiang', 'Yehao Lu', 'Tao Wu', 'Xi Li']",http://arxiv.org/abs/2410.15957v3
Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data,"We study the problem of estimating the body movements of a camera wearer from
egocentric videos. Current methods for ego-body pose estimation rely on
temporally dense sensor data, such as IMU measurements from spatially sparse
body parts like the head and hands. However, we propose that even temporally
sparse observations, such as hand poses captured intermittently from egocentric
videos during natural or periodic hand movements, can effectively constrain
overall body motion. Naively applying diffusion models to generate full-body
pose from head pose and sparse hand pose leads to suboptimal results. To
overcome this, we develop a two-stage approach that decomposes the problem into
temporal completion and spatial completion. First, our method employs masked
autoencoders to impute hand trajectories by leveraging the spatiotemporal
correlations between the head pose sequence and intermittent hand poses,
providing uncertainty estimates. Subsequently, we employ conditional diffusion
models to generate plausible full-body motions based on these temporally dense
trajectories of the head and hands, guided by the uncertainty estimates from
the imputation. The effectiveness of our method was rigorously tested and
validated through comprehensive experiments conducted on various HMD setup with
AMASS and Ego-Exo4D datasets.",2024-11-05 23:53:19+00:00,"['Seunggeun Chi', 'Pin-Hao Huang', 'Enna Sachdeva', 'Hengbo Ma', 'Karthik Ramani', 'Kwonjoon Lee']",http://arxiv.org/abs/2411.03561v1
Frequency-Aware Guidance for Blind Image Restoration via Diffusion Models,"Blind image restoration remains a significant challenge in low-level vision
tasks. Recently, denoising diffusion models have shown remarkable performance
in image synthesis. Guided diffusion models, leveraging the potent generative
priors of pre-trained models along with a differential guidance loss, have
achieved promising results in blind image restoration. However, these models
typically consider data consistency solely in the spatial domain, often
resulting in distorted image content. In this paper, we propose a novel
frequency-aware guidance loss that can be integrated into various diffusion
models in a plug-and-play manner. Our proposed guidance loss, based on 2D
discrete wavelet transform, simultaneously enforces content consistency in both
the spatial and frequency domains. Experimental results demonstrate the
effectiveness of our method in three blind restoration tasks: blind image
deblurring, imaging through turbulence, and blind restoration for multiple
degradations. Notably, our method achieves a significant improvement in PSNR
score, with a remarkable enhancement of 3.72\,dB in image deblurring. Moreover,
our method exhibits superior capability in generating images with rich details
and reduced distortion, leading to the best visual quality.",2024-11-19 12:18:16+00:00,"['Jun Xiao', 'Zihang Lyu', 'Hao Xie', 'Cong Zhang', 'Yakun Ju', 'Changjian Shui', 'Kin-Man Lam']",http://arxiv.org/abs/2411.12450v1
Do You Guys Want to Dance: Zero-Shot Compositional Human Dance Generation with Multiple Persons,"Human dance generation (HDG) aims to synthesize realistic videos from images
and sequences of driving poses. Despite great success, existing methods are
limited to generating videos of a single person with specific backgrounds,
while the generalizability for real-world scenarios with multiple persons and
complex backgrounds remains unclear. To systematically measure the
generalizability of HDG models, we introduce a new task, dataset, and
evaluation protocol of compositional human dance generation (cHDG). Evaluating
the state-of-the-art methods on cHDG, we empirically find that they fail to
generalize to real-world scenarios. To tackle the issue, we propose a novel
zero-shot framework, dubbed MultiDance-Zero, that can synthesize videos
consistent with arbitrary multiple persons and background while precisely
following the driving poses. Specifically, in contrast to straightforward DDIM
or null-text inversion, we first present a pose-aware inversion method to
obtain the noisy latent code and initialization text embeddings, which can
accurately reconstruct the composed reference image. Since directly generating
videos from them will lead to severe appearance inconsistency, we propose a
compositional augmentation strategy to generate augmented images and utilize
them to optimize a set of generalizable text embeddings. In addition,
consistency-guided sampling is elaborated to encourage the background and
keypoints of the estimated clean image at each reverse step to be close to
those of the reference image, further improving the temporal consistency of
generated videos. Extensive qualitative and quantitative results demonstrate
the effectiveness and superiority of our approach.",2024-01-24 10:44:16+00:00,"['Zhe Xu', 'Kun Wei', 'Xu Yang', 'Cheng Deng']",http://arxiv.org/abs/2401.13363v1
V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation,"In the field of portrait video generation, the use of single images to
generate portrait videos has become increasingly prevalent. A common approach
involves leveraging generative models to enhance adapters for controlled
generation. However, control signals (e.g., text, audio, reference image, pose,
depth map, etc.) can vary in strength. Among these, weaker conditions often
struggle to be effective due to interference from stronger conditions, posing a
challenge in balancing these conditions. In our work on portrait video
generation, we identified audio signals as particularly weak, often
overshadowed by stronger signals such as facial pose and reference image.
However, direct training with weak signals often leads to difficulties in
convergence. To address this, we propose V-Express, a simple method that
balances different control signals through the progressive training and the
conditional dropout operation. Our method gradually enables effective control
by weak conditions, thereby achieving generation capabilities that
simultaneously take into account the facial pose, reference image, and audio.
The experimental results demonstrate that our method can effectively generate
portrait videos controlled by audio. Furthermore, a potential solution is
provided for the simultaneous and effective use of conditions of varying
strengths.",2024-06-04 17:32:52+00:00,"['Cong Wang', 'Kuan Tian', 'Jun Zhang', 'Yonghang Guan', 'Feng Luo', 'Fei Shen', 'Zhiwei Jiang', 'Qing Gu', 'Xiao Han', 'Wei Yang']",http://arxiv.org/abs/2406.02511v1
Image inpainting for corrupted images by using the semi-super resolution GAN,"Image inpainting is a valuable technique for enhancing images that have been
corrupted. The primary challenge in this research revolves around the extent of
corruption in the input image that the deep learning model must restore. To
address this challenge, we introduce a Generative Adversarial Network (GAN) for
learning and replicating the missing pixels. Additionally, we have developed a
distinct variant of the Super-Resolution GAN (SRGAN), which we refer to as the
Semi-SRGAN (SSRGAN). Furthermore, we leveraged three diverse datasets to assess
the robustness and accuracy of our proposed model. Our training process
involves varying levels of pixel corruption to attain optimal accuracy and
generate high-quality images.",2024-09-19 10:21:16+00:00,"['Mehrshad Momen-Tayefeh', 'Mehrdad Momen-Tayefeh', 'Amir Ali Ghafourian Ghahramani']",http://arxiv.org/abs/2409.12636v1
Multi-resolution Guided 3D GANs for Medical Image Translation,"Medical image translation is the process of converting from one imaging
modality to another, in order to reduce the need for multiple image
acquisitions from the same patient. This can enhance the efficiency of
treatment by reducing the time, equipment, and labor needed. In this paper, we
introduce a multi-resolution guided Generative Adversarial Network (GAN)-based
framework for 3D medical image translation. Our framework uses a 3D
multi-resolution Dense-Attention UNet (3D-mDAUNet) as the generator and a 3D
multi-resolution UNet as the discriminator, optimized with a unique combination
of loss functions including voxel-wise GAN loss and 2.5D perception loss. Our
approach yields promising results in volumetric image quality assessment (IQA)
across a variety of imaging modalities, body regions, and age groups,
demonstrating its robustness. Furthermore, we propose a synthetic-to-real
applicability assessment as an additional evaluation to assess the
effectiveness of synthetic data in downstream applications such as
segmentation. This comprehensive evaluation shows that our method produces
synthetic medical images not only of high-quality but also potentially useful
in clinical applications. Our code is available at github.com/juhha/3D-mADUNet.",2024-11-30 20:11:55+00:00,"['Juhyung Ha', 'Jong Sung Park', 'David Crandall', 'Eleftherios Garyfallidis', 'Xuhong Zhang']",http://arxiv.org/abs/2412.00575v1
"""Sora is Incredible and Scary"": Emerging Governance Challenges of Text-to-Video Generative AI Models","Text-to-video generative AI models such as Sora OpenAI have the potential to
disrupt multiple industries. In this paper, we report a qualitative social
media analysis aiming to uncover people's perceived impact of and concerns
about Sora's integration. We collected and analyzed comments (N=292) under
popular posts about Sora-generated videos, comparison between Sora videos and
Midjourney images, and artists' complaints about copyright infringement by
Generative AI. We found that people were most concerned about Sora's impact on
content creation-related industries. Emerging governance challenges included
the for-profit nature of OpenAI, the blurred boundaries between real and fake
content, human autonomy, data privacy, copyright issues, and environmental
impact. Potential regulatory solutions proposed by people included law-enforced
labeling of AI content and AI literacy education for the public. Based on the
findings, we discuss the importance of gauging people's tech perceptions early
and propose policy recommendations to regulate Sora before its public release.",2024-04-10 02:03:59+00:00,"['Kyrie Zhixuan Zhou', 'Abhinav Choudhry', 'Ece Gumusel', 'Madelyn Rose Sanfilippo']",http://arxiv.org/abs/2406.11859v1
Improving Unsupervised Video Object Segmentation via Fake Flow Generation,"Unsupervised video object segmentation (VOS), also known as video salient
object detection, aims to detect the most prominent object in a video at the
pixel level. Recently, two-stream approaches that leverage both RGB images and
optical flow maps have gained significant attention. However, the limited
amount of training data remains a substantial challenge. In this study, we
propose a novel data generation method that simulates fake optical flows from
single images, thereby creating large-scale training data for stable network
learning. Inspired by the observation that optical flow maps are highly
dependent on depth maps, we generate fake optical flows by refining and
augmenting the estimated depth maps of each image. By incorporating our
simulated image-flow pairs, we achieve new state-of-the-art performance on all
public benchmark datasets without relying on complex modules. We believe that
our data generation method represents a potential breakthrough for future VOS
research.",2024-07-16 13:32:50+00:00,"['Suhwan Cho', 'Minhyeok Lee', 'Jungho Lee', 'Donghyeong Kim', 'Seunghoon Lee', 'Sungmin Woo', 'Sangyoun Lee']",http://arxiv.org/abs/2407.11714v1
Alignment is All You Need: A Training-free Augmentation Strategy for Pose-guided Video Generation,"Character animation is a transformative field in computer graphics and
vision, enabling dynamic and realistic video animations from static images.
Despite advancements, maintaining appearance consistency in animations remains
a challenge. Our approach addresses this by introducing a training-free
framework that ensures the generated video sequence preserves the reference
image's subtleties, such as physique and proportions, through a dual alignment
strategy. We decouple skeletal and motion priors from pose information,
enabling precise control over animation generation. Our method also improves
pixel-level alignment for conditional control from the reference character,
enhancing the temporal consistency and visual cohesion of animations. Our
method significantly enhances the quality of video generation without the need
for large datasets or expensive computational resources.",2024-08-29 13:08:12+00:00,"['Xiaoyu Jin', 'Zunnan Xu', 'Mingwen Ou', 'Wenming Yang']",http://arxiv.org/abs/2408.16506v1
GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation,"We present GR-2, a state-of-the-art generalist robot agent for versatile and
generalizable robot manipulation. GR-2 is first pre-trained on a vast number of
Internet videos to capture the dynamics of the world. This large-scale
pre-training, involving 38 million video clips and over 50 billion tokens,
equips GR-2 with the ability to generalize across a wide range of robotic tasks
and environments during subsequent policy learning. Following this, GR-2 is
fine-tuned for both video generation and action prediction using robot
trajectories. It exhibits impressive multi-task learning capabilities,
achieving an average success rate of 97.7% across more than 100 tasks.
Moreover, GR-2 demonstrates exceptional generalization to new, previously
unseen scenarios, including novel backgrounds, environments, objects, and
tasks. Notably, GR-2 scales effectively with model size, underscoring its
potential for continued growth and application. Project page:
\url{https://gr2-manipulation.github.io}.",2024-10-08 16:00:47+00:00,"['Chi-Lam Cheang', 'Guangzeng Chen', 'Ya Jing', 'Tao Kong', 'Hang Li', 'Yifeng Li', 'Yuxiao Liu', 'Hongtao Wu', 'Jiafeng Xu', 'Yichu Yang', 'Hanbo Zhang', 'Minzhao Zhu']",http://arxiv.org/abs/2410.06158v1
Allegro: Open the Black Box of Commercial-Level Video Generation Model,"Significant advancements have been made in the field of video generation,
with the open-source community contributing a wealth of research papers and
tools for training high-quality models. However, despite these efforts, the
available information and resources remain insufficient for achieving
commercial-level performance. In this report, we open the black box and
introduce $\textbf{Allegro}$, an advanced video generation model that excels in
both quality and temporal consistency. We also highlight the current
limitations in the field and present a comprehensive methodology for training
high-performance, commercial-level video generation models, addressing key
aspects such as data, model architecture, training pipeline, and evaluation.
Our user study shows that Allegro surpasses existing open-source models and
most commercial models, ranking just behind Hailuo and Kling. Code:
https://github.com/rhymes-ai/Allegro , Model:
https://huggingface.co/rhymes-ai/Allegro , Gallery:
https://rhymes.ai/allegro_gallery .",2024-10-20 17:51:35+00:00,"['Yuan Zhou', 'Qiuyue Wang', 'Yuxuan Cai', 'Huan Yang']",http://arxiv.org/abs/2410.15458v1
NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis,"Talking face synthesis driven by audio is one of the current research
hotspots in the fields of multidimensional signal processing and multimedia.
Neural Radiance Field (NeRF) has recently been brought to this research field
in order to enhance the realism and 3D effect of the generated faces. However,
most existing NeRF-based methods either burden NeRF with complex learning tasks
while lacking methods for supervised multimodal feature fusion, or cannot
precisely map audio to the facial region related to speech movements. These
reasons ultimately result in existing methods generating inaccurate lip shapes.
This paper moves a portion of NeRF learning tasks ahead and proposes a talking
face synthesis method via NeRF with attention-based disentanglement (NeRF-AD).
In particular, an Attention-based Disentanglement module is introduced to
disentangle the face into Audio-face and Identity-face using speech-related
facial action unit (AU) information. To precisely regulate how audio affects
the talking face, we only fuse the Audio-face with audio feature. In addition,
AU information is also utilized to supervise the fusion of these two
modalities. Extensive qualitative and quantitative experiments demonstrate that
our NeRF-AD outperforms state-of-the-art methods in generating realistic
talking face videos, including image quality and lip synchronization. To view
video results, please refer to https://xiaoxingliu02.github.io/NeRF-AD.",2024-01-23 08:54:10+00:00,"['Chongke Bi', 'Xiaoxing Liu', 'Zhilei Liu']",http://arxiv.org/abs/2401.12568v1
Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style,"Although automatically animating audio-driven talking heads has recently
received growing interest, previous efforts have mainly concentrated on
achieving lip synchronization with the audio, neglecting two crucial elements
for generating expressive videos: emotion style and art style. In this paper,
we present an innovative audio-driven talking face generation method called
Style2Talker. It involves two stylized stages, namely Style-E and Style-A,
which integrate text-controlled emotion style and picture-controlled art style
into the final output. In order to prepare the scarce emotional text
descriptions corresponding to the videos, we propose a labor-free paradigm that
employs large-scale pretrained models to automatically annotate emotional text
labels for existing audiovisual datasets. Incorporating the synthetic emotion
texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion
representations, which are combined with the audio, serving as the condition
for an efficient latent diffusion model designed to produce emotional motion
coefficients of a 3DMM model. Moving on to the Style-A stage, we develop a
coefficient-driven motion generator and an art-specific style path embedded in
the well-known StyleGAN. This allows us to synthesize high-resolution
artistically stylized talking head videos using the generated emotional motion
coefficients and an art style source picture. Moreover, to better preserve
image details and avoid artifacts, we provide StyleGAN with the multi-scale
content features extracted from the identity image and refine its intermediate
feature maps by the designed content encoder and refinement network,
respectively. Extensive experimental results demonstrate our method outperforms
existing state-of-the-art methods in terms of audio-lip synchronization and
performance of both emotion style and art style.",2024-03-11 01:32:29+00:00,"['Shuai Tan', 'Bin Ji', 'Ye Pan']",http://arxiv.org/abs/2403.06365v2
MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation,"Video foreground segmentation (VFS) is an important computer vision task
wherein one aims to segment the objects under motion from the background. Most
of the current methods are image-based, i.e., rely only on spatial cues while
ignoring motion cues. Therefore, they tend to overfit the training data and
don't generalize well to out-of-domain (OOD) distribution. To solve the above
problem, prior works exploited several cues such as optical flow, background
subtraction mask, etc. However, having a video data with annotations like
optical flow is a challenging task. In this paper, we utilize the temporal
information and the spatial cues from the video data to improve OOD
performance. However, the challenge lies in how we model the temporal
information given the video data in an interpretable way creates a very
noticeable difference. We therefore devise a strategy that integrates the
temporal context of the video in the development of VFS. Our approach give rise
to deep learning architectures, namely MUSTAN1 and MUSTAN2 and they are based
on the idea of multi-scale temporal context as an attention, i.e., aids our
models to learn better representations that are beneficial for VFS. Further, we
introduce a new video dataset, namely Indoor Surveillance Dataset (ISD) for
VFS. It has multiple annotations on a frame level such as foreground binary
mask, depth map, and instance semantic annotations. Therefore, ISD can benefit
other computer vision tasks. We validate the efficacy of our architectures and
compare the performance with baselines. We demonstrate that proposed methods
significantly outperform the benchmark methods on OOD. In addition, the
performance of MUSTAN2 is significantly improved on certain video categories on
OOD data due to ISD.",2024-02-01 13:47:23+00:00,"['Praveen Kumar Pokala', 'Jaya Sai Kiran Patibandla', 'Naveen Kumar Pandey', 'Balakrishna Reddy Pailla']",http://arxiv.org/abs/2402.00918v1
MVAD: A Multiple Visual Artifact Detector for Video Streaming,"Visual artifacts are often introduced into streamed video content, due to
prevailing conditions during content production and delivery. Since these can
degrade the quality of the user's experience, it is important to automatically
and accurately detect them in order to enable effective quality measurement and
enhancement. Existing detection methods often focus on a single type of
artifact and/or determine the presence of an artifact through thresholding
objective quality indices. Such approaches have been reported to offer
inconsistent prediction performance and are also impractical for real-world
applications where multiple artifacts co-exist and interact. In this paper, we
propose a Multiple Visual Artifact Detector, MVAD, for video streaming which,
for the first time, is able to detect multiple artifacts using a single
framework that is not reliant on video quality assessment models. Our approach
employs a new Artifact-aware Dynamic Feature Extractor (ADFE) to obtain
artifact-relevant spatial features within each frame for multiple artifact
types. The extracted features are further processed by a Recurrent Memory
Vision Transformer (RMViT) module, which captures both short-term and long-term
temporal information within the input video. The proposed network architecture
is optimized in an end-to-end manner based on a new, large and diverse training
database that is generated by simulating the video streaming pipeline and based
on Adversarial Data Augmentation. This model has been evaluated on two video
artifact databases, Maxwell and BVI-Artifact, and achieves consistent and
improved prediction results for ten target visual artifacts when compared to
seven existing single and multiple artifact detectors. The source code and
training database will be available at
https://chenfeng-bristol.github.io/MVAD/.",2024-05-31 21:56:04+00:00,"['Chen Feng', 'Duolikun Danier', 'Fan Zhang', 'Alex Mackin', 'Andrew Collins', 'David Bull']",http://arxiv.org/abs/2406.00212v2
SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code,"This paper introduces SceneCraft, a Large Language Model (LLM) Agent
converting text descriptions into Blender-executable Python scripts which
render complex scenes with up to a hundred 3D assets. This process requires
complex spatial planning and arrangement. We tackle these challenges through a
combination of advanced abstraction, strategic planning, and library learning.
SceneCraft first models a scene graph as a blueprint, detailing the spatial
relationships among assets in the scene. SceneCraft then writes Python scripts
based on this graph, translating relationships into numerical constraints for
asset layout. Next, SceneCraft leverages the perceptual strengths of
vision-language foundation models like GPT-V to analyze rendered images and
iteratively refine the scene. On top of this process, SceneCraft features a
library learning mechanism that compiles common script functions into a
reusable library, facilitating continuous self-improvement without expensive
LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses
existing LLM-based agents in rendering complex scenes, as shown by its
adherence to constraints and favorable human assessments. We also showcase the
broader application potential of SceneCraft by reconstructing detailed 3D
scenes from the Sintel movie and guiding a video generative model with
generated scenes as intermediary control signal.",2024-03-02 16:16:26+00:00,"['Ziniu Hu', 'Ahmet Iscen', 'Aashi Jain', 'Thomas Kipf', 'Yisong Yue', 'David A. Ross', 'Cordelia Schmid', 'Alireza Fathi']",http://arxiv.org/abs/2403.01248v1
Spatial-frequency Dual-Domain Feature Fusion Network for Low-Light Remote Sensing Image Enhancement,"Low-light remote sensing images generally feature high resolution and high
spatial complexity, with continuously distributed surface features in space.
This continuity in scenes leads to extensive long-range correlations in spatial
domains within remote sensing images. Convolutional Neural Networks, which rely
on local correlations for long-distance modeling, struggle to establish
long-range correlations in such images. On the other hand, transformer-based
methods that focus on global information face high computational complexities
when processing high-resolution remote sensing images. From another
perspective, Fourier transform can compute global information without
introducing a large number of parameters, enabling the network to more
efficiently capture the overall image structure and establish long-range
correlations. Therefore, we propose a Dual-Domain Feature Fusion Network (DFFN)
for low-light remote sensing image enhancement. Specifically, this challenging
task of low-light enhancement is divided into two more manageable sub-tasks:
the first phase learns amplitude information to restore image brightness, and
the second phase learns phase information to refine details. To facilitate
information exchange between the two phases, we designed an information fusion
affine block that combines data from different phases and scales. Additionally,
we have constructed two dark light remote sensing datasets to address the
current lack of datasets in dark light remote sensing image enhancement.
Extensive evaluations show that our method outperforms existing
state-of-the-art methods. The code is available at
https://github.com/iijjlk/DFFN.",2024-04-26 13:21:31+00:00,"['Zishu Yao', 'Guodong Fan', 'Jinfu Fan', 'Min Gan', 'C. L. Philip Chen']",http://arxiv.org/abs/2404.17400v2
Phased Consistency Models,"Consistency Models (CMs) have made significant progress in accelerating the
generation of diffusion models. However, their application to high-resolution,
text-conditioned image generation in the latent space remains unsatisfactory.
In this paper, we identify three key flaws in the current design of Latent
Consistency Models (LCMs). We investigate the reasons behind these limitations
and propose Phased Consistency Models (PCMs), which generalize the design space
and address the identified limitations. Our evaluations demonstrate that PCMs
outperform LCMs across 1--16 step generation settings. While PCMs are
specifically designed for multi-step refinement, they achieve comparable 1-step
generation results to previously state-of-the-art specifically designed 1-step
methods. Furthermore, we show the methodology of PCMs is versatile and
applicable to video generation, enabling us to train the state-of-the-art
few-step text-to-video generator. Our code is available at
https://github.com/G-U-N/Phased-Consistency-Model.",2024-05-28 17:47:19+00:00,"['Fu-Yun Wang', 'Zhaoyang Huang', 'Alexander William Bergman', 'Dazhong Shen', 'Peng Gao', 'Michael Lingelbach', 'Keqiang Sun', 'Weikang Bian', 'Guanglu Song', 'Yu Liu', 'Xiaogang Wang', 'Hongsheng Li']",http://arxiv.org/abs/2405.18407v2
Harmonizing Pixels and Melodies: Maestro-Guided Film Score Generation and Composition Style Transfer,"We introduce a film score generation framework to harmonize visual pixels and
music melodies utilizing a latent diffusion model. Our framework processes film
clips as input and generates music that aligns with a general theme while
offering the capability to tailor outputs to a specific composition style. Our
model directly produces music from video, utilizing a streamlined and efficient
tuning mechanism on ControlNet. It also integrates a film encoder adept at
understanding the film's semantic depth, emotional impact, and aesthetic
appeal. Additionally, we introduce a novel, effective yet straightforward
evaluation metric to evaluate the originality and recognizability of music
within film scores. To fill this gap for film scores, we curate a comprehensive
dataset of film videos and legendary original scores, injecting domain-specific
knowledge into our data-driven generation model. Our model outperforms existing
methodologies in creating film scores, capable of generating music that
reflects the guidance of a maestro's style, thereby redefining the benchmark
for automated film scores and laying a robust groundwork for future research in
this domain. The code and generated samples are available at
https://anonymous.4open.science/r/HPM.",2024-11-12 04:34:09+00:00,"['F. Qi', 'L. Ni', 'C. Xu']",http://arxiv.org/abs/2411.07539v1
Gait Sequence Upsampling using Diffusion Models for Single LiDAR Sensors,"Recently, 3D LiDAR has emerged as a promising technique in the field of
gait-based person identification, serving as an alternative to traditional RGB
cameras, due to its robustness under varying lighting conditions and its
ability to capture 3D geometric information. However, long capture distances or
the use of low-cost LiDAR sensors often result in sparse human point clouds,
leading to a decline in identification performance. To address these
challenges, we propose a sparse-to-dense upsampling model for pedestrian point
clouds in LiDAR-based gait recognition, named LidarGSU, which is designed to
improve the generalization capability of existing identification models. Our
method utilizes diffusion probabilistic models (DPMs), which have shown high
fidelity in generative tasks such as image completion. In this work, we
leverage DPMs on sparse sequential pedestrian point clouds as conditional masks
in a video-to-video translation approach, applied in an inpainting manner. We
conducted extensive experiments on the SUSTeck1K dataset to evaluate the
generative quality and recognition performance of the proposed method.
Furthermore, we demonstrate the applicability of our upsampling model using a
real-world dataset, captured with a low-resolution sensor across varying
measurement distances.",2024-10-11 10:11:21+00:00,"['Jeongho Ahn', 'Kazuto Nakashima', 'Koki Yoshino', 'Yumi Iwashita', 'Ryo Kurazume']",http://arxiv.org/abs/2410.08680v2
SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild,"Self-occlusion is common when capturing people in the wild, where the
performer do not follow predefined motion scripts. This challenges existing
monocular human reconstruction systems that assume full body visibility. We
introduce Self-Occluded Avatar Recovery (SOAR), a method for complete human
reconstruction from partial observations where parts of the body are entirely
unobserved. SOAR leverages structural normal prior and generative diffusion
prior to address such an ill-posed reconstruction problem. For structural
normal prior, we model human with an reposable surfel model with well-defined
and easily readable shapes. For generative diffusion prior, we perform an
initial reconstruction and refine it using score distillation. On various
benchmarks, we show that SOAR performs favorably than state-of-the-art
reconstruction and generation methods, and on-par comparing to concurrent
works. Additional video results and code are available at
https://soar-avatar.github.io/.",2024-10-31 10:35:59+00:00,"['Zhuoyang Pan', 'Angjoo Kanazawa', 'Hang Gao']",http://arxiv.org/abs/2410.23800v1
Coherent Temporal Synthesis for Incremental Action Segmentation,"Data replay is a successful incremental learning technique for images. It
prevents catastrophic forgetting by keeping a reservoir of previous data,
original or synthesized, to ensure the model retains past knowledge while
adapting to novel concepts. However, its application in the video domain is
rudimentary, as it simply stores frame exemplars for action recognition. This
paper presents the first exploration of video data replay techniques for
incremental action segmentation, focusing on action temporal modeling. We
propose a Temporally Coherent Action (TCA) model, which represents actions
using a generative model instead of storing individual frames. The integration
of a conditioning variable that captures temporal coherence allows our model to
understand the evolution of action features over time. Therefore, action
segments generated by TCA for replay are diverse and temporally coherent. In a
10-task incremental setup on the Breakfast dataset, our approach achieves
significant increases in accuracy for up to 22% compared to the baselines.",2024-03-10 06:07:06+00:00,"['Guodong Ding', 'Hans Golong', 'Angela Yao']",http://arxiv.org/abs/2403.06102v1
Inclusion 2024 Global Multimedia Deepfake Detection: Towards Multi-dimensional Facial Forgery Detection,"In this paper, we present the Global Multimedia Deepfake Detection held
concurrently with the Inclusion 2024. Our Multimedia Deepfake Detection aims to
detect automatic image and audio-video manipulations including but not limited
to editing, synthesis, generation, Photoshop,etc. Our challenge has attracted
1500 teams from all over the world, with about 5000 valid result submission
counts. We invite the top 20 teams to present their solutions to the challenge,
from which the top 3 teams are awarded prizes in the grand finale. In this
paper, we present the solutions from the top 3 teams of the two tracks, to
boost the research work in the field of image and audio-video forgery
detection. The methodologies developed through the challenge will contribute to
the development of next-generation deepfake detection systems and we encourage
participants to open source their methods.",2024-12-30 09:58:27+00:00,"['Yi Zhang', 'Weize Gao', 'Changtao Miao', 'Man Luo', 'Jianshu Li', 'Wenzhong Deng', 'Zhe Li', 'Bingyu Hu', 'Weibin Yao', 'Wenbo Zhou', 'Tao Gong', 'Qi Chu']",http://arxiv.org/abs/2412.20833v1
From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation,"Video encompasses both visual and auditory data, creating a perceptually rich
experience where these two modalities complement each other. As such, videos
are a valuable type of media for the investigation of the interplay between
audio and visual elements. Previous studies of audio-visual modalities
primarily focused on either audio-visual representation learning or generative
modeling of a modality conditioned on the other, creating a disconnect between
these two branches. A unified framework that learns representation and
generates modalities has not been developed yet. In this work, we introduce a
novel framework called Vision to Audio and Beyond (VAB) to bridge the gap
between audio-visual representation learning and vision-to-audio generation.
The key approach of VAB is that rather than working with raw video frames and
audio data, VAB performs representation learning and generative modeling within
latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an
image encoder to obtain audio tokens and visual features, respectively. It then
performs the pre-training task of visual-conditioned masked audio token
prediction. This training strategy enables the model to engage in contextual
learning and simultaneous video-to-audio generation. After the pre-training
phase, VAB employs the iterative-decoding approach to rapidly generate audio
tokens conditioned on visual features. Since VAB is a unified model, its
backbone can be fine-tuned for various audio-visual downstream tasks. Our
experiments showcase the efficiency of VAB in producing high-quality audio from
video, and its capability to acquire semantic audio-visual features, leading to
competitive results in audio-visual retrieval and classification.",2024-09-27 20:26:34+00:00,"['Kun Su', 'Xiulong Liu', 'Eli Shlizerman']",http://arxiv.org/abs/2409.19132v1
Faster Diffusion Action Segmentation,"Temporal Action Segmentation (TAS) is an essential task in video analysis,
aiming to segment and classify continuous frames into distinct action segments.
However, the ambiguous boundaries between actions pose a significant challenge
for high-precision segmentation. Recent advances in diffusion models have
demonstrated substantial success in TAS tasks due to their stable training
process and high-quality generation capabilities. However, the heavy sampling
steps required by diffusion models pose a substantial computational burden,
limiting their practicality in real-time applications. Additionally, most
related works utilize Transformer-based encoder architectures. Although these
architectures excel at capturing long-range dependencies, they incur high
computational costs and face feature-smoothing issues when processing long
video sequences. To address these challenges, we propose EffiDiffAct, an
efficient and high-performance TAS algorithm. Specifically, we develop a
lightweight temporal feature encoder that reduces computational overhead and
mitigates the rank collapse phenomenon associated with traditional
self-attention mechanisms. Furthermore, we introduce an adaptive skip strategy
that allows for dynamic adjustment of timestep lengths based on computed
similarity metrics during inference, thereby further enhancing computational
efficiency. Comprehensive experiments on the 50Salads, Breakfast, and GTEA
datasets demonstrated the effectiveness of the proposed algorithm.",2024-08-04 13:23:18+00:00,"['Shuaibing Wang', 'Shunli Wang', 'Mingcheng Li', 'Dingkang Yang', 'Haopeng Kuang', 'Ziyun Qian', 'Lihua Zhang']",http://arxiv.org/abs/2408.02024v1
TaQ-DiT: Time-aware Quantization for Diffusion Transformers,"Transformer-based diffusion models, dubbed Diffusion Transformers (DiTs),
have achieved state-of-the-art performance in image and video generation tasks.
However, their large model size and slow inference speed limit their practical
applications, calling for model compression methods such as quantization.
Unfortunately, existing DiT quantization methods overlook (1) the impact of
reconstruction and (2) the varying quantization sensitivities across different
layers, which hinder their achievable performance. To tackle these issues, we
propose innovative time-aware quantization for DiTs (TaQ-DiT). Specifically,
(1) we observe a non-convergence issue when reconstructing weights and
activations separately during quantization and introduce a joint reconstruction
method to resolve this problem. (2) We discover that Post-GELU activations are
particularly sensitive to quantization due to their significant variability
across different denoising steps as well as extreme asymmetries and variations
within each step. To address this, we propose time-variance-aware
transformations to facilitate more effective quantization. Experimental results
show that when quantizing DiTs' weights to 4-bit and activations to 8-bit
(W4A8), our method significantly surpasses previous quantization methods.",2024-11-21 14:34:46+00:00,"['Xinyan Liu', 'Huihong Shi', 'Yang Xu', 'Zhongfeng Wang']",http://arxiv.org/abs/2411.14172v1
Shape-Preserving Generation of Food Images for Automatic Dietary Assessment,"Traditional dietary assessment methods heavily rely on self-reporting, which
is time-consuming and prone to bias. Recent advancements in Artificial
Intelligence (AI) have revealed new possibilities for dietary assessment,
particularly through analysis of food images. Recognizing foods and estimating
food volumes from images are known as the key procedures for automatic dietary
assessment. However, both procedures required large amounts of training images
labeled with food names and volumes, which are currently unavailable.
Alternatively, recent studies have indicated that training images can be
artificially generated using Generative Adversarial Networks (GANs).
Nonetheless, convenient generation of large amounts of food images with known
volumes remain a challenge with the existing techniques. In this work, we
present a simple GAN-based neural network architecture for conditional food
image generation. The shapes of the food and container in the generated images
closely resemble those in the reference input image. Our experiments
demonstrate the realism of the generated images and shape-preserving
capabilities of the proposed framework.",2024-08-23 20:18:51+00:00,"['Guangzong Chen', 'Zhi-Hong Mao', 'Mingui Sun', 'Kangni Liu', 'Wenyan Jia']",http://arxiv.org/abs/2408.13358v1
Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance,"The synthesis of 3D facial animations from speech has garnered considerable
attention. Due to the scarcity of high-quality 4D facial data and
well-annotated abundant multi-modality labels, previous methods often suffer
from limited realism and a lack of lexible conditioning. We address this
challenge through a trilogy. We first introduce Generalized Neural Parametric
Facial Asset (GNPFA), an efficient variational auto-encoder mapping facial
geometry and images to a highly generalized expression latent space, decoupling
expressions and identities. Then, we utilize GNPFA to extract high-quality
expressions and accurate head poses from a large array of videos. This presents
the M2F-D dataset, a large, diverse, and scan-level co-speech 3D facial
animation dataset with well-annotated emotional and style labels. Finally, we
propose Media2Face, a diffusion model in GNPFA latent space for co-speech
facial animation generation, accepting rich multi-modality guidances from
audio, text, and image. Extensive experiments demonstrate that our model not
only achieves high fidelity in facial animation synthesis but also broadens the
scope of expressiveness and style adaptability in 3D facial animation.",2024-01-28 16:17:59+00:00,"['Qingcheng Zhao', 'Pengyu Long', 'Qixuan Zhang', 'Dafei Qin', 'Han Liang', 'Longwen Zhang', 'Yingliang Zhang', 'Jingyi Yu', 'Lan Xu']",http://arxiv.org/abs/2401.15687v2
VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model,"Generating multi-view images based on text or single-image prompts is a
critical capability for the creation of 3D content. Two fundamental questions
on this topic are what data we use for training and how to ensure multi-view
consistency. This paper introduces a novel framework that makes fundamental
contributions to both questions. Unlike leveraging images from 2D diffusion
models for training, we propose a dense consistent multi-view generation model
that is fine-tuned from off-the-shelf video generative models. Images from
video generative models are more suitable for multi-view generation because the
underlying network architecture that generates them employs a temporal module
to enforce frame consistency. Moreover, the video data sets used to train these
models are abundant and diverse, leading to a reduced train-finetuning domain
gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising
Sampling, which first employs a feed-forward reconstruction module to get an
explicit global 3D model, and then adopts a sampling strategy that effectively
involves images rendered from the global 3D model into the denoising sampling
loop to improve the multi-view consistency of the final images. As a
by-product, this module also provides a fast way to create 3D assets
represented by 3D Gaussians within a few seconds. Our approach can generate 24
dense views and converges much faster in training than state-of-the-art
approaches (4 GPU hours versus many thousand GPU hours) with comparable visual
quality and consistency. By further fine-tuning, our approach outperforms
existing state-of-the-art methods in both quantitative metrics and visual
effects. Our project page is aigc3d.github.io/VideoMV.",2024-03-18 17:48:15+00:00,"['Qi Zuo', 'Xiaodong Gu', 'Lingteng Qiu', 'Yuan Dong', 'Zhengyi Zhao', 'Weihao Yuan', 'Rui Peng', 'Siyu Zhu', 'Zilong Dong', 'Liefeng Bo', 'Qixing Huang']",http://arxiv.org/abs/2403.12010v1
Human-Activity AGV Quality Assessment: A Benchmark Dataset and an Objective Evaluation Metric,"AI-driven video generation techniques have made significant progress in
recent years. However, AI-generated videos (AGVs) involving human activities
often exhibit substantial visual and semantic distortions, hindering the
practical application of video generation technologies in real-world scenarios.
To address this challenge, we conduct a pioneering study on human activity AGV
quality assessment, focusing on visual quality evaluation and the
identification of semantic distortions. First, we construct the AI-Generated
Human activity Video Quality Assessment (Human-AGVQA) dataset, consisting of
3,200 AGVs derived from 8 popular text-to-video (T2V) models using 400 text
prompts that describe diverse human activities. We conduct a subjective study
to evaluate the human appearance quality, action continuity quality, and
overall video quality of AGVs, and identify semantic issues of human body
parts. Based on Human-AGVQA, we benchmark the performance of T2V models and
analyze their strengths and weaknesses in generating different categories of
human activities. Second, we develop an objective evaluation metric, named
AI-Generated Human activity Video Quality metric (GHVQ), to automatically
analyze the quality of human activity AGVs. GHVQ systematically extracts
human-focused quality features, AI-generated content-aware quality features,
and temporal continuity features, making it a comprehensive and explainable
quality metric for human activity AGVs. The extensive experimental results show
that GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a
large margin, demonstrating its efficacy in assessing the quality of human
activity AGVs. The Human-AGVQA dataset and GHVQ metric will be released in
public at https://github.com/zczhang-sjtu/GHVQ.git",2024-11-25 17:58:43+00:00,"['Zhichao Zhang', 'Wei Sun', 'Xinyue Li', 'Yunhao Li', 'Qihang Ge', 'Jun Jia', 'Zicheng Zhang', 'Zhongpeng Ji', 'Fengyu Sun', 'Shangling Jui', 'Xiongkuo Min', 'Guangtao Zhai']",http://arxiv.org/abs/2411.16619v1
DDPM based X-ray Image Synthesizer,"Access to high-quality datasets in the medical industry limits machine
learning model performance. To address this issue, we propose a Denoising
Diffusion Probabilistic Model (DDPM) combined with a UNet architecture for
X-ray image synthesis. Focused on pneumonia medical condition, our methodology
employs over 3000 pneumonia X-ray images obtained from Kaggle for training.
Results demonstrate the effectiveness of our approach, as the model
successfully generated realistic images with low Mean Squared Error (MSE). The
synthesized images showed distinct differences from non-pneumonia images,
highlighting the model's ability to capture key features of positive cases.
Beyond pneumonia, the applications of this synthesizer extend to various
medical conditions, provided an ample dataset is available. The capability to
produce high-quality images can potentially enhance machine learning models'
performance, aiding in more accurate and efficient medical diagnoses. This
innovative DDPM-based X-ray photo synthesizer presents a promising avenue for
addressing the scarcity of positive medical image datasets, paving the way for
improved medical image analysis and diagnosis in the healthcare industry.",2024-01-03 04:35:58+00:00,"['Praveen Mahaulpatha', 'Thulana Abeywardane', 'Tomson George']",http://arxiv.org/abs/2401.01539v1
AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation,"In this study, we propose AniPortrait, a novel framework for generating
high-quality animation driven by audio and a reference portrait image. Our
methodology is divided into two stages. Initially, we extract 3D intermediate
representations from audio and project them into a sequence of 2D facial
landmarks. Subsequently, we employ a robust diffusion model, coupled with a
motion module, to convert the landmark sequence into photorealistic and
temporally consistent portrait animation. Experimental results demonstrate the
superiority of AniPortrait in terms of facial naturalness, pose diversity, and
visual quality, thereby offering an enhanced perceptual experience. Moreover,
our methodology exhibits considerable potential in terms of flexibility and
controllability, which can be effectively applied in areas such as facial
motion editing or face reenactment. We release code and model weights at
https://github.com/scutzzj/AniPortrait",2024-03-26 13:35:02+00:00,"['Huawei Wei', 'Zejun Yang', 'Zhisheng Wang']",http://arxiv.org/abs/2403.17694v1
High Quality Human Image Animation using Regional Supervision and Motion Blur Condition,"Recent advances in video diffusion models have enabled realistic and
controllable human image animation with temporal coherence. Although generating
reasonable results, existing methods often overlook the need for regional
supervision in crucial areas such as the face and hands, and neglect the
explicit modeling for motion blur, leading to unrealistic low-quality
synthesis. To address these limitations, we first leverage regional supervision
for detailed regions to enhance face and hand faithfulness. Second, we model
the motion blur explicitly to further improve the appearance quality. Third, we
explore novel training strategies for high-resolution human animation to
improve the overall fidelity. Experimental results demonstrate that our
proposed method outperforms state-of-the-art approaches, achieving significant
improvements upon the strongest baseline by more than 21.0% and 57.4% in terms
of reconstruction precision (L1) and perceptual quality (FVD) on HumanDance
dataset. Code and model will be made available.",2024-09-29 06:46:31+00:00,"['Zhongcong Xu', 'Chaoyue Song', 'Guoxian Song', 'Jianfeng Zhang', 'Jun Hao Liew', 'Hongyi Xu', 'You Xie', 'Linjie Luo', 'Guosheng Lin', 'Jiashi Feng', 'Mike Zheng Shou']",http://arxiv.org/abs/2409.19580v1
S$^{2}$-DMs:Skip-Step Diffusion Models,"Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.",2024-01-03 03:08:32+00:00,"['Yixuan Wang', 'Shuangyin Li']",http://arxiv.org/abs/2401.01520v2
Mobius: A High Efficient Spatial-Temporal Parallel Training Paradigm for Text-to-Video Generation Task,"Inspired by the success of the text-to-image (T2I) generation task, many
researchers are devoting themselves to the text-to-video (T2V) generation task.
Most of the T2V frameworks usually inherit from the T2I model and add
extra-temporal layers of training to generate dynamic videos, which can be
viewed as a fine-tuning task. However, the traditional 3D-Unet is a serial mode
and the temporal layers follow the spatial layers, which will result in high
GPU memory and training time consumption according to its serial feature flow.
We believe that this serial mode will bring more training costs with the large
diffusion model and massive datasets, which are not environmentally friendly
and not suitable for the development of the T2V. Therefore, we propose a highly
efficient spatial-temporal parallel training paradigm for T2V tasks, named
Mobius. In our 3D-Unet, the temporal layers and spatial layers are parallel,
which optimizes the feature flow and backpropagation. The Mobius will save 24%
GPU memory and 12% training time, which can greatly improve the T2V fine-tuning
task and provide a novel insight for the AIGC community. We will release our
codes in the future.",2024-07-09 07:47:16+00:00,"['Yiran Yang', 'Jinchao Zhang', 'Ying Deng', 'Jie Zhou']",http://arxiv.org/abs/2407.06617v4
UWAFA-GAN: Ultra-Wide-Angle Fluorescein Angiography Transformation via Multi-scale Generation and Registration Enhancement,"Fundus photography, in combination with the ultra-wide-angle fundus (UWF)
techniques, becomes an indispensable diagnostic tool in clinical settings by
offering a more comprehensive view of the retina. Nonetheless, UWF fluorescein
angiography (UWF-FA) necessitates the administration of a fluorescent dye via
injection into the patient's hand or elbow unlike UWF scanning laser
ophthalmoscopy (UWF-SLO). To mitigate potential adverse effects associated with
injections, researchers have proposed the development of cross-modality medical
image generation algorithms capable of converting UWF-SLO images into their
UWF-FA counterparts. Current image generation techniques applied to fundus
photography encounter difficulties in producing high-resolution retinal images,
particularly in capturing minute vascular lesions. To address these issues, we
introduce a novel conditional generative adversarial network (UWAFA-GAN) to
synthesize UWF-FA from UWF-SLO. This approach employs multi-scale generators
and an attention transmit module to efficiently extract both global structures
and local lesions. Additionally, to counteract the image blurriness issue that
arises from training with misaligned data, a registration module is integrated
within this framework. Our method performs non-trivially on inception scores
and details generation. Clinical user studies further indicate that the UWF-FA
images generated by UWAFA-GAN are clinically comparable to authentic images in
terms of diagnostic reliability. Empirical evaluations on our proprietary UWF
image datasets elucidate that UWAFA-GAN outperforms extant methodologies. The
code is accessible at https://github.com/Tinysqua/UWAFA-GAN.",2024-05-01 14:27:43+00:00,"['Ruiquan Ge', 'Zhaojie Fang', 'Pengxue Wei', 'Zhanghao Chen', 'Hongyang Jiang', 'Ahmed Elazab', 'Wangting Li', 'Xiang Wan', 'Shaochong Zhang', 'Changmiao Wang']",http://arxiv.org/abs/2405.00542v1
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning,"It is challenging for models to understand complex, multimodal content such
as television clips, and this is in part because video-language models often
rely on single-modality reasoning and lack interpretability. To combat these
issues we propose TV-TREES, the first multimodal entailment tree generator.
TV-TREES serves as an approach to video understanding that promotes
interpretable joint-modality reasoning by searching for trees of entailment
relationships between simple text-video evidence and higher-level conclusions
that prove question-answer pairs. We also introduce the task of multimodal
entailment tree generation to evaluate reasoning quality. Our method's
performance on the challenging TVQA benchmark demonstrates interpretable,
state-of-the-art zero-shot performance on full clips, illustrating that
multimodal entailment tree generation can be a best-of-both-worlds alternative
to black-box systems.",2024-02-29 18:57:01+00:00,"['Kate Sanders', 'Nathaniel Weir', 'Benjamin Van Durme']",http://arxiv.org/abs/2402.19467v4
Adversarially Masked Video Consistency for Unsupervised Domain Adaptation,"We study the problem of unsupervised domain adaptation for egocentric videos.
We propose a transformer-based model to learn class-discriminative and
domain-invariant feature representations. It consists of two novel designs. The
first module is called Generative Adversarial Domain Alignment Network with the
aim of learning domain-invariant representations. It simultaneously learns a
mask generator and a domain-invariant encoder in an adversarial way. The
domain-invariant encoder is trained to minimize the distance between the source
and target domain. The masking generator, conversely, aims at producing
challenging masks by maximizing the domain distance. The second is a Masked
Consistency Learning module to learn class-discriminative representations. It
enforces the prediction consistency between the masked target videos and their
full forms. To better evaluate the effectiveness of domain adaptation methods,
we construct a more challenging benchmark for egocentric videos, U-Ego4D. Our
method achieves state-of-the-art performance on the Epic-Kitchen and the
proposed U-Ego4D benchmark.",2024-03-24 17:13:46+00:00,"['Xiaoyu Zhu', 'Junwei Liang', 'Po-Yao Huang', 'Alex Hauptmann']",http://arxiv.org/abs/2403.16242v1
JoyHallo: Digital human model for Mandarin,"In audio-driven video generation, creating Mandarin videos presents
significant challenges. Collecting comprehensive Mandarin datasets is
difficult, and the complex lip movements in Mandarin further complicate model
training compared to English. In this study, we collected 29 hours of Mandarin
speech video from JD Health International Inc. employees, resulting in the
jdh-Hallo dataset. This dataset includes a diverse range of ages and speaking
styles, encompassing both conversational and specialized medical topics. To
adapt the JoyHallo model for Mandarin, we employed the Chinese wav2vec2 model
for audio feature embedding. A semi-decoupled structure is proposed to capture
inter-feature relationships among lip, expression, and pose features. This
integration not only improves information utilization efficiency but also
accelerates inference speed by 14.3%. Notably, JoyHallo maintains its strong
ability to generate English videos, demonstrating excellent cross-language
generation capabilities. The code and models are available at
https://jdh-algo.github.io/JoyHallo.",2024-09-20 06:57:42+00:00,"['Sheng Shi', 'Xuyang Cao', 'Jun Zhao', 'Guoxin Wang']",http://arxiv.org/abs/2409.13268v1
Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality,"The Fr\'echet Video Distance (FVD) is a widely adopted metric for evaluating
video generation distribution quality. However, its effectiveness relies on
critical assumptions. Our analysis reveals three significant limitations: (1)
the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the
insensitivity of I3D features to temporal distortions; (3) the impractical
sample sizes required for reliable estimation. These findings undermine FVD's
reliability and show that FVD falls short as a standalone metric for video
generation evaluation. After extensive analysis of a wide range of metrics and
backbone architectures, we propose JEDi, the JEPA Embedding Distance, based on
features derived from a Joint Embedding Predictive Architecture, measured using
Maximum Mean Discrepancy with polynomial kernel. Our experiments on multiple
open-source datasets show clear evidence that it is a superior alternative to
the widely used FVD metric, requiring only 16% of the samples to reach its
steady value, while increasing alignment with human evaluation by 34%, on
average.",2024-10-07 17:07:21+00:00,"['Ge Ya Luo', 'Gian Mario Favero', 'Zhi Hao Luo', 'Alexia Jolicoeur-Martineau', 'Christopher Pal']",http://arxiv.org/abs/2410.05203v2
"VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model","Vision Language Models (VLMs) have recently been adopted in robotics for
their capability in common sense reasoning and generalizability. Existing work
has applied VLMs to generate task and motion planning from natural language
instructions and simulate training data for robot learning. In this work, we
explore using VLM to interpret human demonstration videos and generate robot
task planning. Our method integrates keyframe selection, visual perception, and
VLM reasoning into a pipeline. We named it SeeDo because it enables the VLM to
''see'' human demonstrations and explain the corresponding plans to the robot
for it to ''do''. To validate our approach, we collected a set of long-horizon
human videos demonstrating pick-and-place tasks in three diverse categories and
designed a set of metrics to comprehensively benchmark SeeDo against several
baselines, including state-of-the-art video-input VLMs. The experiments
demonstrate SeeDo's superior performance. We further deployed the generated
task plans in both a simulation environment and on a real robot arm.",2024-10-11 13:17:52+00:00,"['Beichen Wang', 'Juexiao Zhang', 'Shuwen Dong', 'Irving Fang', 'Chen Feng']",http://arxiv.org/abs/2410.08792v1
It's Just Another Day: Unique Video Captioning by Discriminative Prompting,"Long videos contain many repeating actions, events and shots. These
repetitions are frequently given identical captions, which makes it difficult
to retrieve the exact desired clip using a text search. In this paper, we
formulate the problem of unique captioning: Given multiple clips with the same
caption, we generate a new caption for each clip that uniquely identifies it.
We propose Captioning by Discriminative Prompting (CDP), which predicts a
property that can separate identically captioned clips, and use it to generate
unique captions. We introduce two benchmarks for unique captioning, based on
egocentric footage and timeloop movies - where repeating actions are common. We
demonstrate that captions generated by CDP improve text-to-video R@1 by 15% for
egocentric videos and 10% in timeloop movies.",2024-10-15 15:41:49+00:00,"['Toby Perrett', 'Tengda Han', 'Dima Damen', 'Andrew Zisserman']",http://arxiv.org/abs/2410.11702v1
Empowering the Deaf and Hard of Hearing Community: Enhancing Video Captions Using Large Language Models,"In today's digital age, video content is prevalent, serving as a primary
source of information, education, and entertainment. However, the Deaf and Hard
of Hearing (DHH) community often faces significant challenges in accessing
video content due to the inadequacy of automatic speech recognition (ASR)
systems in providing accurate and reliable captions. This paper addresses the
urgent need to improve video caption quality by leveraging Large Language
Models (LLMs). We present a comprehensive study that explores the integration
of LLMs to enhance the accuracy and context-awareness of captions generated by
ASR systems. Our methodology involves a novel pipeline that corrects
ASR-generated captions using advanced LLMs. It explicitly focuses on models
like GPT-3.5 and Llama2-13B due to their robust performance in language
comprehension and generation tasks. We introduce a dataset representative of
real-world challenges the DHH community faces to evaluate our proposed
pipeline. Our results indicate that LLM-enhanced captions significantly improve
accuracy, as evidenced by a notably lower Word Error Rate (WER) achieved by
ChatGPT-3.5 (WER: 9.75%) compared to the original ASR captions (WER: 23.07%),
ChatGPT-3.5 shows an approximate 57.72% improvement in WER compared to the
original ASR captions.",2024-11-30 03:52:08+00:00,"['Nadeen Fathallah', 'Monika Bhole', 'Steffen Staab']",http://arxiv.org/abs/2412.00342v1
Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models,"Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow
to localize tumors and observe their contrast kinetics, which is essential for
cancer characterization and respective treatment decision-making. However,
contrast agent administration is not only associated with adverse health risks,
but also restricted for patients during pregnancy, and for those with kidney
malfunction, or other adverse reactions. With contrast uptake as key biomarker
for lesion malignancy, cancer recurrence risk, and treatment response, it
becomes pivotal to reduce the dependency on intravenous contrast agent
administration. To this end, we propose a multi-conditional latent diffusion
model capable of acquisition time-conditioned image synthesis of DCE-MRI
temporal sequences. To evaluate medical image synthesis, we additionally
propose and validate the Fr\'echet radiomics distance as an image quality
measure based on biomarker variability between synthetic and real imaging data.
Our results demonstrate our method's ability to generate realistic
multi-sequence fat-saturated breast DCE-MRI and uncover the emerging potential
of deep learning based contrast kinetics simulation. We publicly share our
accessible codebase at https://github.com/RichardObi/ccnet and provide a
user-friendly library for Fr\'echet radiomics distance calculation at
https://pypi.org/project/frd-score.",2024-03-20 18:01:57+00:00,"['Richard Osuala', 'Daniel M. Lang', 'Preeti Verma', 'Smriti Joshi', 'Apostolia Tsirikoglou', 'Grzegorz Skorupko', 'Kaisar Kushibar', 'Lidia Garrucho', 'Walter H. L. Pinaya', 'Oliver Diaz', 'Julia A. Schnabel', 'Karim Lekadir']",http://arxiv.org/abs/2403.13890v3
Prediction with Action: Visual Policy Learning via Joint Denoising Process,"Diffusion models have demonstrated remarkable capabilities in image
generation tasks, including image editing and video creation, representing a
good understanding of the physical world. On the other line, diffusion models
have also shown promise in robotic control tasks by denoising actions, known as
diffusion policy. Although the diffusion generative model and diffusion policy
exhibit distinct capabilities--image prediction and robotic action,
respectively--they technically follow a similar denoising process. In robotic
tasks, the ability to predict future images and generate actions is highly
correlated since they share the same underlying dynamics of the physical world.
Building on this insight, we introduce PAD, a novel visual policy learning
framework that unifies image Prediction and robot Action within a joint
Denoising process. Specifically, PAD utilizes Diffusion Transformers (DiT) to
seamlessly integrate images and robot states, enabling the simultaneous
prediction of future images and robot actions. Additionally, PAD supports
co-training on both robotic demonstrations and large-scale video datasets and
can be easily extended to other robotic modalities, such as depth images. PAD
outperforms previous methods, achieving a significant 26.3% relative
improvement on the full Metaworld benchmark, by utilizing a single
text-conditioned visual policy within a data-efficient imitation learning
setting. Furthermore, PAD demonstrates superior generalization to unseen tasks
in real-world robot manipulation settings with 28.0% success rate increase
compared to the strongest baseline. Project page at
https://sites.google.com/view/pad-paper",2024-11-27 09:54:58+00:00,"['Yanjiang Guo', 'Yucheng Hu', 'Jianke Zhang', 'Yen-Jen Wang', 'Xiaoyu Chen', 'Chaochao Lu', 'Jianyu Chen']",http://arxiv.org/abs/2411.18179v1
NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation,"Vision-and-language navigation (VLN) stands as a key research problem of
Embodied AI, aiming at enabling agents to navigate in unseen environments
following linguistic instructions. In this field, generalization is a
long-standing challenge, either to out-of-distribution scenes or from Sim to
Real. In this paper, we propose NaVid, a video-based large vision language
model (VLM), to mitigate such a generalization gap. NaVid makes the first
endeavor to showcase the capability of VLMs to achieve state-of-the-art level
navigation performance without any maps, odometers, or depth inputs. Following
human instruction, NaVid only requires an on-the-fly video stream from a
monocular RGB camera equipped on the robot to output the next-step action. Our
formulation mimics how humans navigate and naturally gets rid of the problems
introduced by odometer noises, and the Sim2Real gaps from map or depth inputs.
Moreover, our video-based approach can effectively encode the historical
observations of robots as spatio-temporal contexts for decision making and
instruction following. We train NaVid with 510k navigation samples collected
from continuous environments, including action-planning and
instruction-reasoning samples, along with 763k large-scale web data. Extensive
experiments show that NaVid achieves state-of-the-art performance in simulation
environments and the real world, demonstrating superior cross-dataset and
Sim2Real transfer. We thus believe our proposed VLM approach plans the next
step for not only the navigation agents but also this research field.",2024-02-24 16:39:16+00:00,"['Jiazhao Zhang', 'Kunyu Wang', 'Rongtao Xu', 'Gengze Zhou', 'Yicong Hong', 'Xiaomeng Fang', 'Qi Wu', 'Zhizheng Zhang', 'He Wang']",http://arxiv.org/abs/2402.15852v7
VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation,"Visual generative models have achieved remarkable progress in synthesizing
photorealistic images and videos, yet aligning their outputs with human
preferences across critical dimensions remains a persistent challenge. Though
reinforcement learning from human feedback offers promise for preference
alignment, existing reward models for visual generation face limitations,
including black-box scoring without interpretability and potentially resultant
unexpected biases. We present VisionReward, a general framework for learning
human visual preferences in both image and video generation. Specifically, we
employ a hierarchical visual assessment framework to capture fine-grained human
preferences, and leverages linear weighting to enable interpretable preference
learning. Furthermore, we propose a multi-dimensional consistent strategy when
using VisionReward as a reward model during preference optimization for visual
generation. Experiments show that VisionReward can significantly outperform
existing image and video reward models on both machine metrics and human
evaluation. Notably, VisionReward surpasses VideoScore by 17.2% in preference
prediction accuracy, and text-to-video models with VisionReward achieve a 31.6%
higher pairwise win rate compared to the same models using VideoScore. All code
and datasets are provided at https://github.com/THUDM/VisionReward.",2024-12-30 16:24:09+00:00,"['Jiazheng Xu', 'Yu Huang', 'Jiale Cheng', 'Yuanming Yang', 'Jiajun Xu', 'Yuan Wang', 'Wenbo Duan', 'Shen Yang', 'Qunlin Jin', 'Shurun Li', 'Jiayan Teng', 'Zhuoyi Yang', 'Wendi Zheng', 'Xiao Liu', 'Ming Ding', 'Xiaohan Zhang', 'Xiaotao Gu', 'Shiyu Huang', 'Minlie Huang', 'Jie Tang', 'Yuxiao Dong']",http://arxiv.org/abs/2412.21059v2
Articulated Object Manipulation using Online Axis Estimation with SAM2-Based Tracking,"Articulated object manipulation requires precise object interaction, where
the object's axis must be carefully considered. Previous research employed
interactive perception for manipulating articulated objects, but typically,
open-loop approaches often suffer from overlooking the interaction dynamics. To
address this limitation, we present a closed-loop pipeline integrating
interactive perception with online axis estimation from segmented 3D point
clouds. Our method leverages any interactive perception technique as a
foundation for interactive perception, inducing slight object movement to
generate point cloud frames of the evolving dynamic scene. These point clouds
are then segmented using Segment Anything Model 2 (SAM2), after which the
moving part of the object is masked for accurate motion online axis estimation,
guiding subsequent robotic actions. Our approach significantly enhances the
precision and efficiency of manipulation tasks involving articulated objects.
Experiments in simulated environments demonstrate that our method outperforms
baseline approaches, especially in tasks that demand precise axis-based
control. Project Page:
https://hytidel.github.io/video-tracking-for-axis-estimation/.",2024-09-24 17:59:56+00:00,"['Xi Wang', 'Tianxing Chen', 'Qiaojun Yu', 'Tianling Xu', 'Zanxin Chen', 'Yiting Fu', 'Ziqi He', 'Cewu Lu', 'Yao Mu', 'Ping Luo']",http://arxiv.org/abs/2409.16287v2
Learning Correlation Structures for Vision Transformers,"We introduce a new attention mechanism, dubbed structural self-attention
(StructSA), that leverages rich correlation patterns naturally emerging in
key-query interactions of attention. StructSA generates attention maps by
recognizing space-time structures of key-query correlations via convolution and
uses them to dynamically aggregate local contexts of value features. This
effectively leverages rich structural patterns in images and videos such as
scene layouts, object motion, and inter-object relations. Using StructSA as a
main building block, we develop the structural vision transformer (StructViT)
and evaluate its effectiveness on both image and video classification tasks,
achieving state-of-the-art results on ImageNet-1K, Kinetics-400,
Something-Something V1 & V2, Diving-48, and FineGym.",2024-04-05 07:13:28+00:00,"['Manjin Kim', 'Paul Hongsuck Seo', 'Cordelia Schmid', 'Minsu Cho']",http://arxiv.org/abs/2404.03924v1
MLS-Track: Multilevel Semantic Interaction in RMOT,"The new trend in multi-object tracking task is to track objects of interest
using natural language. However, the scarcity of paired prompt-instance data
hinders its progress. To address this challenge, we propose a high-quality yet
low-cost data generation method base on Unreal Engine 5 and construct a
brand-new benchmark dataset, named Refer-UE-City, which primarily includes
scenes from intersection surveillance videos, detailing the appearance and
actions of people and vehicles. Specifically, it provides 14 videos with a
total of 714 expressions, and is comparable in scale to the Refer-KITTI
dataset. Additionally, we propose a multi-level semantic-guided multi-object
framework called MLS-Track, where the interaction between the model and text is
enhanced layer by layer through the introduction of Semantic Guidance Module
(SGM) and Semantic Correlation Branch (SCB). Extensive experiments on
Refer-UE-City and Refer-KITTI datasets demonstrate the effectiveness of our
proposed framework and it achieves state-of-the-art performance. Code and
datatsets will be available.",2024-04-18 09:31:03+00:00,"['Zeliang Ma', 'Song Yang', 'Zhe Cui', 'Zhicheng Zhao', 'Fei Su', 'Delong Liu', 'Jingyu Wang']",http://arxiv.org/abs/2404.12031v1
Grouped Discrete Representation Guides Object-Centric Learning,"Similar to humans perceiving visual scenes as objects, Object-Centric
Learning (OCL) can abstract dense images or videos into sparse object-level
features. Transformer-based OCL handles complex textures well due to the
decoding guidance of discrete representation, obtained by discretizing noisy
features in image or video feature maps using template features from a
codebook. However, treating features as minimal units overlooks their composing
attributes, thus impeding model generalization; indexing features with natural
numbers loses attribute-level commonalities and characteristics, thus
diminishing heuristics for model convergence. We propose \textit{Grouped
Discrete Representation} (GDR) to address these issues by grouping features
into attributes and indexing them with tuple numbers. In extensive experiments
across different query initializations, dataset modalities, and model
architectures, GDR consistently improves convergence and generalizability.
Visualizations show that our method effectively captures attribute-level
information in features. The source code will be available upon acceptance.",2024-07-01 19:00:40+00:00,"['Rongzhen Zhao', 'Vivienne Wang', 'Juho Kannala', 'Joni Pajarinen']",http://arxiv.org/abs/2407.01726v3
Hybrid Local-Global Context Learning for Neural Video Compression,"In neural video codecs, current state-of-the-art methods typically adopt
multi-scale motion compensation to handle diverse motions. These methods
estimate and compress either optical flow or deformable offsets to reduce
inter-frame redundancy. However, flow-based methods often suffer from
inaccurate motion estimation in complicated scenes. Deformable
convolution-based methods are more robust but have a higher bit cost for motion
coding. In this paper, we propose a hybrid context generation module, which
combines the advantages of the above methods in an optimal way and achieves
accurate compensation at a low bit cost. Specifically, considering the
characteristics of features at different scales, we adopt flow-guided
deformable compensation at largest-scale to produce accurate alignment in
detailed regions. For smaller-scale features, we perform flow-based warping to
save the bit cost for motion coding. Furthermore, we design a local-global
context enhancement module to fully explore the local-global information of
previous reconstructed signals. Experimental results demonstrate that our
proposed Hybrid Local-Global Context learning (HLGC) method can significantly
enhance the state-of-the-art methods on standard test datasets.",2024-11-30 11:40:31+00:00,"['Yongqi Zhai', 'Jiayu Yang', 'Wei Jiang', 'Chunhui Yang', 'Luyang Tang', 'Ronggang Wang']",http://arxiv.org/abs/2412.00446v1
EgoPoints: Advancing Point Tracking for Egocentric Videos,"We introduce EgoPoints, a benchmark for point tracking in egocentric videos.
We annotate 4.7K challenging tracks in egocentric sequences. Compared to the
popular TAP-Vid-DAVIS evaluation benchmark, we include 9x more points that go
out-of-view and 59x more points that require re-identification (ReID) after
returning to view. To measure the performance of models on these challenging
points, we introduce evaluation metrics that specifically monitor tracking
performance on points in-view, out-of-view, and points that require
re-identification. We then propose a pipeline to create semi-real sequences,
with automatic ground truth. We generate 11K such sequences by combining
dynamic Kubric objects with scene points from EPIC Fields. When fine-tuning
point tracking methods on these sequences and evaluating on our annotated
EgoPoints sequences, we improve CoTracker across all metrics, including the
tracking accuracy $\delta^\star_{\text{avg}}$ by 2.7 percentage points and
accuracy on ReID sequences (ReID$\delta_{\text{avg}}$) by 2.4 points. We also
improve $\delta^\star_{\text{avg}}$ and ReID$\delta_{\text{avg}}$ of PIPs++ by
0.3 and 2.8 respectively.",2024-12-05 20:12:29+00:00,"['Ahmad Darkhalil', 'Rhodri Guerrier', 'Adam W. Harley', 'Dima Damen']",http://arxiv.org/abs/2412.04592v1
Prism: Semi-Supervised Multi-View Stereo with Monocular Structure Priors,"The promise of unsupervised multi-view-stereo (MVS) is to leverage large
unlabeled datasets, yet current methods underperform when training on difficult
data, such as handheld smartphone videos of indoor scenes. Meanwhile,
high-quality synthetic datasets are available but MVS networks trained on these
datasets fail to generalize to real-world examples. To bridge this gap, we
propose a semi-supervised learning framework that allows us to train on real
and rendered images jointly, capturing structural priors from synthetic data
while ensuring parity with the real-world domain. Central to our framework is a
novel set of losses that leverages powerful existing monocular relative-depth
estimators trained on the synthetic dataset, transferring the rich structure of
this relative depth to the MVS predictions on unlabeled data. Inspired by
perceptual image metrics, we compare the MVS and monocular predictions via a
deep feature loss and a multi-scale statistical loss. Our full framework, which
we call Prism, achieves large quantitative and qualitative improvements over
current unsupervised and synthetic-supervised MVS networks. This is a
best-case-scenario result, opening the door to using both unlabeled smartphone
videos and photorealistic synthetic datasets for training MVS networks.",2024-12-08 01:05:41+00:00,"['Alex Rich', 'Noah Stier', 'Pradeep Sen', 'Tobias Hllerer']",http://arxiv.org/abs/2412.05771v1
Text-to-Image GAN with Pretrained Representations,"Generating desired images conditioned on given text descriptions has received
lots of attention. Recently, diffusion models and autoregressive models have
demonstrated their outstanding expressivity and gradually replaced GAN as the
favored architectures for text-to-image synthesis. However, they still face
some obstacles: slow inference speed and expensive training costs. To achieve
more powerful and faster text-to-image synthesis under complex scenes, we
propose TIGER, a text-to-image GAN with pretrained representations. To be
specific, we propose a vision-empowered discriminator and a high-capacity
generator. (i) The vision-empowered discriminator absorbs the complex scene
understanding ability and the domain generalization ability from pretrained
vision models to enhance model performance. Unlike previous works, we explore
stacking multiple pretrained models in our discriminator to collect multiple
different representations. (ii) The high-capacity generator aims to achieve
effective text-image fusion while increasing the model capacity. The
high-capacity generator consists of multiple novel high-capacity fusion blocks
(HFBlock). And the HFBlock contains several deep fusion modules and a global
fusion module, which play different roles to benefit our model. Extensive
experiments demonstrate the outstanding performance of our proposed TIGER both
on standard and zero-shot text-to-image synthesis tasks. On the standard
text-to-image synthesis task, TIGER achieves state-of-the-art performance on
two challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On
the zero-shot text-to-image synthesis task, we achieve comparable performance
with fewer model parameters, smaller training data size and faster inference
speed. Additionally, more experiments and analyses are conducted in the
Supplementary Material.",2024-12-30 19:30:40+00:00,"['Xiaozhou You', 'Jian Zhang']",http://arxiv.org/abs/2501.00116v1
Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning,"Human-centered dynamic scene understanding plays a pivotal role in enhancing
the capability of robotic and autonomous systems, in which Video-based
Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene
understanding, aimed at comprehensively understanding HOI relationships within
a video to benefit the behavioral decisions of mobile robots and autonomous
driving systems. Although previous V-HOI detection models have made significant
strides in accurate detection on specific datasets, they still lack the general
reasoning ability like human beings to effectively induce HOI relationships. In
this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a
novel framework consisting of a series of plug-and-play modules that could
facilitate the performance of current V-HOI detection models by leveraging the
strong reasoning ability of different off-the-shelf pre-trained large language
models (LLMs). We design a two-stage collaboration system of different LLMs for
the V-HOI task. Specifically, in the first stage, we design a Cross-Agents
Reasoning scheme to leverage the LLM conduct reasoning from different aspects.
In the second stage, we perform Multi-LLMs Debate to get the final reasoning
answer based on the different knowledge in different LLMs. Additionally, we
devise an auxiliary training strategy that utilizes CLIP, a large
vision-language model to enhance the base V-HOI models' discriminative ability
to better cooperate with LLMs. We validate the superiority of our design by
demonstrating its effectiveness in improving the prediction accuracy of the
base V-HOI model via reasoning from multiple perspectives.",2024-03-15 08:51:15+00:00,"['Hang Zhang', 'Wenxiao Zhang', 'Haoxuan Qu', 'Jun Liu']",http://arxiv.org/abs/2403.10107v2
Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation,"Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.",2024-03-26 16:36:43+00:00,"['Abdelrhman Werby', 'Chenguang Huang', 'Martin Bchner', 'Abhinav Valada', 'Wolfram Burgard']",http://arxiv.org/abs/2403.17846v2
Elite-EvGS: Learning Event-based 3D Gaussian Splatting by Distilling Event-to-Video Priors,"Event cameras are bio-inspired sensors that output asynchronous and sparse
event streams, instead of fixed frames. Benefiting from their distinct
advantages, such as high dynamic range and high temporal resolution, event
cameras have been applied to address 3D reconstruction, important for robotic
mapping. Recently, neural rendering techniques, such as 3D Gaussian splatting
(3DGS), have been shown successful in 3D reconstruction. However, it still
remains under-explored how to develop an effective event-based 3DGS pipeline.
In particular, as 3DGS typically depends on high-quality initialization and
dense multiview constraints, a potential problem appears for the 3DGS
optimization with events given its inherent sparse property. To this end, we
propose a novel event-based 3DGS framework, named Elite-EvGS. Our key idea is
to distill the prior knowledge from the off-the-shelf event-to-video (E2V)
models to effectively reconstruct 3D scenes from events in a coarse-to-fine
optimization manner. Specifically, to address the complexity of 3DGS
initialization from events, we introduce a novel warm-up initialization
strategy that optimizes a coarse 3DGS from the frames generated by E2V models
and then incorporates events to refine the details. Then, we propose a
progressive event supervision strategy that employs the window-slicing
operation to progressively reduce the number of events used for supervision.
This subtly relives the temporal randomness of the event frames, benefiting the
optimization of local textural and global structural details. Experiments on
the benchmark datasets demonstrate that Elite-EvGS can reconstruct 3D scenes
with better textural and structural details. Meanwhile, our method yields
plausible performance on the captured real-world data, including diverse
challenging conditions, such as fast motion and low light scenes.",2024-09-20 10:47:52+00:00,"['Zixin Zhang', 'Kanghao Chen', 'Lin Wang']",http://arxiv.org/abs/2409.13392v1
PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation,"While previous audio-driven talking head generation (THG) methods generate
head poses from driving audio, the generated poses or lips cannot match the
audio well or are not editable. In this study, we propose \textbf{PoseTalk}, a
THG system that can freely generate lip-synchronized talking head videos with
free head poses conditioned on text prompts and audio. The core insight of our
method is using head pose to connect visual, linguistic, and audio signals.
First, we propose to generate poses from both audio and text prompts, where the
audio offers short-term variations and rhythm correspondence of the head
movements and the text prompts describe the long-term semantics of head
motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to
generate motion latent from text prompts and audio cues in a pose latent space.
Second, we observe a loss-imbalance problem: the loss for the lip region
contributes less than 4\% of the total reconstruction loss caused by both pose
and lip, making optimization lean towards head movements rather than lip
shapes. To address this issue, we propose a refinement-based learning strategy
to synthesize natural talking videos using two cascaded networks, i.e.,
CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce
animated images in novel poses and the RefineNet focuses on learning finer lip
motions by progressively estimating lip motions from low-to-high resolutions,
yielding improved lip-synchronization performance. Experiments demonstrate our
pose prediction strategy achieves better pose diversity and realness compared
to text-only or audio-only, and our video generator model outperforms
state-of-the-art methods in synthesizing talking videos with natural head
motions. Project: https://junleen.github.io/projects/posetalk.",2024-09-04 12:30:25+00:00,"['Jun Ling', 'Yiwen Wang', 'Han Xue', 'Rong Xie', 'Li Song']",http://arxiv.org/abs/2409.02657v1
Mora: Enabling Generalist Video Generation via A Multi-Agent Framework,"Text-to-video generation has made significant strides, but replicating the
capabilities of advanced systems like OpenAI Sora remains challenging due to
their closed-source nature. Existing open-source methods struggle to achieve
comparable performance, often hindered by ineffective agent collaboration and
inadequate training data quality. In this paper, we introduce Mora, a novel
multi-agent framework that leverages existing open-source modules to replicate
Sora functionalities. We address these fundamental limitations by proposing
three key techniques: (1) multi-agent fine-tuning with a self-modulation factor
to enhance inter-agent coordination, (2) a data-free training strategy that
uses large models to synthesize training data, and (3) a human-in-the-loop
mechanism combined with multimodal large language models for data filtering to
ensure high-quality training datasets. Our comprehensive experiments on six
video generation tasks demonstrate that Mora achieves performance comparable to
Sora on VBench, outperforming existing open-source methods across various
tasks. Specifically, in the text-to-video generation task, Mora achieved a
Video Quality score of 0.800, surpassing Sora 0.797 and outperforming all other
baseline models across six key metrics. Additionally, in the image-to-video
generation task, Mora achieved a perfect Dynamic Degree score of 1.00,
demonstrating exceptional capability in enhancing motion realism and achieving
higher Imaging Quality than Sora. These results highlight the potential of
collaborative multi-agent systems and human-in-the-loop mechanisms in advancing
text-to-video generation. Our code is available at
\url{https://github.com/lichao-sun/Mora}.",2024-03-20 02:19:21+00:00,"['Zhengqing Yuan', 'Yixin Liu', 'Yihan Cao', 'Weixiang Sun', 'Haolong Jia', 'Ruoxi Chen', 'Zhaoxu Li', 'Bin Lin', 'Li Yuan', 'Lifang He', 'Chi Wang', 'Yanfang Ye', 'Lichao Sun']",http://arxiv.org/abs/2403.13248v3
GANmut: Generating and Modifying Facial Expressions,"In the realm of emotion synthesis, the ability to create authentic and
nuanced facial expressions continues to gain importance. The GANmut study
discusses a recently introduced advanced GAN framework that, instead of relying
on predefined labels, learns a dynamic and interpretable emotion space. This
methodology maps each discrete emotion as vectors starting from a neutral
state, their magnitude reflecting the emotion's intensity. The current project
aims to extend the study of this framework by benchmarking across various
datasets, image resolutions, and facial detection methodologies. This will
involve conducting a series of experiments using two emotional datasets:
Aff-Wild2 and AffNet. Aff-Wild2 contains videos captured in uncontrolled
environments, which include diverse camera angles, head positions, and lighting
conditions, providing a real-world challenge. AffNet offers images with
labelled emotions, improving the diversity of emotional expressions available
for training. The first two experiments will focus on training GANmut using the
Aff-Wild2 dataset, processed with either RetinaFace or MTCNN, both of which are
high-performance deep learning face detectors. This setup will help determine
how well GANmut can learn to synthesise emotions under challenging conditions
and assess the comparative effectiveness of these face detection technologies.
The subsequent two experiments will merge the Aff-Wild2 and AffNet datasets,
combining the real world variability of Aff-Wild2 with the diverse emotional
labels of AffNet. The same face detectors, RetinaFace and MTCNN, will be
employed to evaluate whether the enhanced diversity of the combined datasets
improves GANmut's performance and to compare the impact of each face detection
method in this hybrid setup.",2024-06-16 21:28:28+00:00,['Maria Surani'],http://arxiv.org/abs/2406.11079v1
Microplastic Identification Using AI-Driven Image Segmentation and GAN-Generated Ecological Context,"Current methods for microplastic identification in water samples are costly
and require expert analysis. Here, we propose a deep learning segmentation
model to automatically identify microplastics in microscopic images. We labeled
images of microplastic from the Moore Institute for Plastic Pollution Research
and employ a Generative Adversarial Network (GAN) to supplement and generate
diverse training data. To verify the validity of the generated data, we
conducted a reader study where an expert was able to discern the generated
microplastic from real microplastic at a rate of 68 percent. Our segmentation
model trained on the combined data achieved an F1-Score of 0.91 on a diverse
dataset, compared to the model without generated data's 0.82. With our findings
we aim to enhance the ability of both experts and citizens to detect
microplastic across diverse ecological contexts, thereby improving the cost and
accessibility of microplastic analysis.",2024-10-25 14:57:09+00:00,"['Alex Dils', 'David Raymond', 'Jack Spottiswood', 'Samay Kodige', 'Dylan Karmin', 'Rikhil Kokal', 'Win Cowger', 'Chris Sade']",http://arxiv.org/abs/2410.19604v1
Cross-modality image synthesis from TOF-MRA to CTA using diffusion-based models,"Cerebrovascular disease often requires multiple imaging modalities for
accurate diagnosis, treatment, and monitoring. Computed Tomography Angiography
(CTA) and Time-of-Flight Magnetic Resonance Angiography (TOF-MRA) are two
common non-invasive angiography techniques, each with distinct strengths in
accessibility, safety, and diagnostic accuracy. While CTA is more widely used
in acute stroke due to its faster acquisition times and higher diagnostic
accuracy, TOF-MRA is preferred for its safety, as it avoids radiation exposure
and contrast agent-related health risks. Despite the predominant role of CTA in
clinical workflows, there is a scarcity of open-source CTA data, limiting the
research and development of AI models for tasks such as large vessel occlusion
detection and aneurysm segmentation. This study explores diffusion-based
image-to-image translation models to generate synthetic CTA images from TOF-MRA
input. We demonstrate the modality conversion from TOF-MRA to CTA and show that
diffusion models outperform a traditional U-Net-based approach. Our work
compares different state-of-the-art diffusion architectures and samplers,
offering recommendations for optimal model performance in this cross-modality
translation task.",2024-09-16 08:43:37+00:00,"['Alexander Koch', 'Orhun Utku Aydin', 'Adam Hilbert', 'Jana Rieger', 'Satoru Tanioka', 'Fujimaro Ishida', 'Dietmar Frey']",http://arxiv.org/abs/2409.10089v1
PoCo: Policy Composition from and for Heterogeneous Robot Learning,"Training general robotic policies from heterogeneous data for different tasks
is a significant challenge. Existing robotic datasets vary in different
modalities such as color, depth, tactile, and proprioceptive information, and
collected in different domains such as simulation, real robots, and human
videos. Current methods usually collect and pool all data from one domain to
train a single policy to handle such heterogeneity in tasks and domains, which
is prohibitively expensive and difficult. In this work, we present a flexible
approach, dubbed Policy Composition, to combine information across such diverse
modalities and domains for learning scene-level and task-level generalized
manipulation skills, by composing different data distributions represented with
diffusion models. Our method can use task-level composition for multi-task
manipulation and be composed with analytic cost functions to adapt policy
behaviors at inference time. We train our method on simulation, human, and real
robot data and evaluate in tool-use tasks. The composed policy achieves robust
and dexterous performance under varying scenes and tasks and outperforms
baselines from a single data source in both simulation and real-world
experiments. See https://liruiw.github.io/policycomp for more details .",2024-02-04 14:51:49+00:00,"['Lirui Wang', 'Jialiang Zhao', 'Yilun Du', 'Edward H. Adelson', 'Russ Tedrake']",http://arxiv.org/abs/2402.02511v3
One Shot GANs for Long Tail Problem in Skin Lesion Dataset using novel content space assessment metric,"Long tail problems frequently arise in the medical field, particularly due to
the scarcity of medical data for rare conditions. This scarcity often leads to
models overfitting on such limited samples. Consequently, when training models
on datasets with heavily skewed classes, where the number of samples varies
significantly, a problem emerges. Training on such imbalanced datasets can
result in selective detection, where a model accurately identifies images
belonging to the majority classes but disregards those from minority classes.
This causes the model to lack generalizability, preventing its use on newer
data. This poses a significant challenge in developing image detection and
diagnosis models for medical image datasets. To address this challenge, the One
Shot GANs model was employed to augment the tail class of HAM10000 dataset by
generating additional samples. Furthermore, to enhance accuracy, a novel metric
tailored to suit One Shot GANs was utilized.",2024-09-30 04:51:54+00:00,"['Kunal Deo', 'Deval Mehta', 'Kshitij Jadhav']",http://arxiv.org/abs/2409.19945v1
Interactive Scene Authoring with Specialized Generative Primitives,"Generating high-quality 3D digital assets often requires expert knowledge of
complex design tools. We introduce Specialized Generative Primitives, a
generative framework that allows non-expert users to author high-quality 3D
scenes in a seamless, lightweight, and controllable manner. Each primitive is
an efficient generative model that captures the distribution of a single
exemplar from the real world. With our framework, users capture a video of an
environment, which we turn into a high-quality and explicit appearance model
thanks to 3D Gaussian Splatting. Users then select regions of interest guided
by semantically-aware features. To create a generative primitive, we adapt
Generative Cellular Automata to single-exemplar training and controllable
generation. We decouple the generative task from the appearance model by
operating on sparse voxels and we recover a high-quality output with a
subsequent sparse patch consistency step. Each primitive can be trained within
10 minutes and used to author new scenes interactively in a fully compositional
manner. We showcase interactive sessions where various primitives are extracted
from real-world scenes and controlled to create 3D assets and scenes in a few
minutes. We also demonstrate additional capabilities of our primitives:
handling various 3D representations to control generation, transferring
appearances, and editing geometries.",2024-12-20 04:39:50+00:00,"['Clment Jambon', 'Changwoon Choi', 'Dongsu Zhang', 'Olga Sorkine-Hornung', 'Young Min Kim']",http://arxiv.org/abs/2412.16253v1
MGAN-CRCM: A Novel Multiple Generative Adversarial Network and Coarse-Refinement Based Cognizant Method for Image Inpainting,"Image inpainting is a widely used technique in computer vision for
reconstructing missing or damaged pixels in images. Recent advancements with
Generative Adversarial Networks (GANs) have demonstrated superior performance
over traditional methods due to their deep learning capabilities and
adaptability across diverse image domains. Residual Networks (ResNet) have also
gained prominence for their ability to enhance feature representation and
compatibility with other architectures. This paper introduces a novel
architecture combining GAN and ResNet models to improve image inpainting
outcomes. Our framework integrates three components: Transpose
Convolution-based GAN for guided and blind inpainting, Fast
ResNet-Convolutional Neural Network (FR-CNN) for object removal, and
Co-Modulation GAN (Co-Mod GAN) for refinement. The model's performance was
evaluated on benchmark datasets, achieving accuracies of 96.59% on Image-Net,
96.70% on Places2, and 96.16% on CelebA. Comparative analyses demonstrate that
the proposed architecture outperforms existing methods, highlighting its
effectiveness in both qualitative and quantitative evaluations.",2024-12-25 22:54:28+00:00,"['Nafiz Al Asad', 'Md. Appel Mahmud Pranto', 'Shbiruzzaman Shiam', 'Musaddeq Mahmud Akand', 'Mohammad Abu Yousuf', 'Khondokar Fida Hasan', 'Mohammad Ali Moni']",http://arxiv.org/abs/2412.19000v1
Instant Policy: In-Context Imitation Learning via Graph Diffusion,"Following the impressive capabilities of in-context learning with large
transformers, In-Context Imitation Learning (ICIL) is a promising opportunity
for robotics. We introduce Instant Policy, which learns new tasks instantly
(without further training) from just one or two demonstrations, achieving ICIL
through two key components. First, we introduce inductive biases through a
graph representation and model ICIL as a graph generation problem with a
learned diffusion process, enabling structured reasoning over demonstrations,
observations, and actions. Second, we show that such a model can be trained
using pseudo-demonstrations - arbitrary trajectories generated in simulation -
as a virtually infinite pool of training data. Simulated and real experiments
show that Instant Policy enables rapid learning of various everyday robot
tasks. We also show how it can serve as a foundation for cross-embodiment and
zero-shot transfer to language-defined tasks. Code and videos are available at
https://www.robot-learning.uk/instant-policy.",2024-11-19 16:45:52+00:00,"['Vitalis Vosylius', 'Edward Johns']",http://arxiv.org/abs/2411.12633v1
FaceLift: Single Image to 3D Head with View Generation and GS-LRM,"We present FaceLift, a feed-forward approach for rapid, high-quality,
360-degree head reconstruction from a single image. Our pipeline begins by
employing a multi-view latent diffusion model that generates consistent side
and back views of the head from a single facial input. These generated views
then serve as input to a GS-LRM reconstructor, which produces a comprehensive
3D representation using Gaussian splats. To train our system, we develop a
dataset of multi-view renderings using synthetic 3D human head as-sets. The
diffusion-based multi-view generator is trained exclusively on synthetic head
images, while the GS-LRM reconstructor undergoes initial training on Objaverse
followed by fine-tuning on synthetic head data. FaceLift excels at preserving
identity and maintaining view consistency across views. Despite being trained
solely on synthetic data, FaceLift demonstrates remarkable generalization to
real-world images. Through extensive qualitative and quantitative evaluations,
we show that FaceLift outperforms state-of-the-art methods in 3D head
reconstruction, highlighting its practical applicability and robust performance
on real-world images. In addition to single image reconstruction, FaceLift
supports video inputs for 4D novel view synthesis and seamlessly integrates
with 2D reanimation techniques to enable 3D facial animation. Project page:
https://weijielyu.github.io/FaceLift.",2024-12-23 18:59:49+00:00,"['Weijie Lyu', 'Yi Zhou', 'Ming-Hsuan Yang', 'Zhixin Shu']",http://arxiv.org/abs/2412.17812v1
SMART: Scene-motion-aware human action recognition framework for mental disorder group,"Patients with mental disorders often exhibit risky abnormal actions, such as
climbing walls or hitting windows, necessitating intelligent video behavior
monitoring for smart healthcare with the rising Internet of Things (IoT)
technology. However, the development of vision-based Human Action Recognition
(HAR) for these actions is hindered by the lack of specialized algorithms and
datasets. In this paper, we innovatively propose to build a vision-based HAR
dataset including abnormal actions often occurring in the mental disorder group
and then introduce a novel Scene-Motion-aware Action Recognition Technology
framework, named SMART, consisting of two technical modules. First, we propose
a scene perception module to extract human motion trajectory and human-scene
interaction features, which introduces additional scene information for a
supplementary semantic representation of the above actions. Second, the
multi-stage fusion module fuses the skeleton motion, motion trajectory, and
human-scene interaction features, enhancing the semantic association between
the skeleton motion and the above supplementary representation, thus generating
a comprehensive representation with both human motion and scene information.
The effectiveness of our proposed method has been validated on our
self-collected HAR dataset (MentalHAD), achieving 94.9% and 93.1% accuracy in
un-seen subjects and scenes and outperforming state-of-the-art approaches by
6.5% and 13.2%, respectively. The demonstrated subject- and scene-
generalizability makes it possible for SMART's migration to practical
deployment in smart healthcare systems for mental disorder patients in medical
settings. The code and dataset will be released publicly for further research:
https://github.com/Inowlzy/SMART.git.",2024-06-07 05:29:42+00:00,"['Zengyuan Lai', 'Jiarui Yang', 'Songpengcheng Xia', 'Qi Wu', 'Zhen Sun', 'Wenxian Yu', 'Ling Pei']",http://arxiv.org/abs/2406.04649v1
SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space,"Combining face swapping with lip synchronization technology offers a
cost-effective solution for customized talking face generation. However,
directly cascading existing models together tends to introduce significant
interference between tasks and reduce video clarity because the interaction
space is limited to the low-level semantic RGB space. To address this issue, we
propose an innovative unified framework, SwapTalk, which accomplishes both face
swapping and lip synchronization tasks in the same latent space. Referring to
recent work on face generation, we choose the VQ-embedding space due to its
excellent editability and fidelity performance. To enhance the framework's
generalization capabilities for unseen identities, we incorporate identity loss
during the training of the face swapping module. Additionally, we introduce
expert discriminator supervision within the latent space during the training of
the lip synchronization module to elevate synchronization quality. In the
evaluation phase, previous studies primarily focused on the self-reconstruction
of lip movements in synchronous audio-visual videos. To better approximate
real-world applications, we expand the evaluation scope to asynchronous
audio-video scenarios. Furthermore, we introduce a novel identity consistency
metric to more comprehensively assess the identity consistency over time series
in generated facial videos. Experimental results on the HDTF demonstrate that
our method significantly surpasses existing techniques in video quality, lip
synchronization accuracy, face swapping fidelity, and identity consistency. Our
demo is available at http://swaptalk.cc.",2024-05-09 09:22:09+00:00,"['Zeren Zhang', 'Haibo Qin', 'Jiayu Huang', 'Yixin Li', 'Hui Lin', 'Yitao Duan', 'Jinwen Ma']",http://arxiv.org/abs/2405.05636v1
ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction,"Autoregressive models (ARMs) and diffusion models (DMs) represent two leading
paradigms in generative modeling, each excelling in distinct areas: ARMs in
global context modeling and long-sequence generation, and DMs in generating
high-quality local contexts, especially for continuous data such as images and
short videos. However, ARMs often suffer from exponential error accumulation
over long sequences, leading to physically implausible results, while DMs are
limited by their local context generation capabilities. In this work, we
introduce Autoregressive Coherent multimodal generation with Diffusion
Correction (ACDC), a zero-shot approach that combines the strengths of both
ARMs and DMs at the inference stage without the need for additional
fine-tuning. ACDC leverages ARMs for global context generation and
memory-conditioned DMs for local correction, ensuring high-quality outputs by
correcting artifacts in generated multimodal tokens. In particular, we propose
a memory module based on large language models (LLMs) that dynamically adjusts
the conditioning texts for the DMs, preserving crucial global context
information. Our experiments on multimodal tasks, including coherent
multi-frame story generation and autoregressive video generation, demonstrate
that ACDC effectively mitigates the accumulation of errors and significantly
enhances the quality of generated outputs, achieving superior performance while
remaining agnostic to specific ARM and DM architectures. Project page:
https://acdc2025.github.io/",2024-10-07 03:22:51+00:00,"['Hyungjin Chung', 'Dohun Lee', 'Jong Chul Ye']",http://arxiv.org/abs/2410.04721v1
Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face Deepfake Detection,"The deepfake threats to society and cybersecurity have provoked significant
public apprehension, driving intensified efforts within the realm of deepfake
video detection. Current video-level methods are mostly based on {3D CNNs}
resulting in high computational demands, although have achieved good
performance. This paper introduces an elegantly simple yet effective strategy
named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined
layout to realize the preservation of spatial and temporal dependencies. This
transformation process involves sequentially masking frames at the same
positions within each frame. These frames are then resized into sub-frames and
reorganized into the predetermined layout, forming thumbnails. TALL is
model-agnostic and has remarkable simplicity, necessitating only minimal code
modifications. Furthermore, we introduce a graph reasoning block (GRB) and
semantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB
enhances interactions between different semantic regions to capture
semantic-level inconsistency clues. The semantic consistency loss imposes
consistency constraints on semantic features to improve model generalization
ability. Extensive experiments on intra-dataset, cross-dataset,
diffusion-generated image detection, and deepfake generation method recognition
show that TALL++ achieves results surpassing or comparable to the
state-of-the-art methods, demonstrating the effectiveness of our approaches for
various deepfake detection problems. The code is available at
https://github.com/rainy-xu/TALL4Deepfake.",2024-03-15 12:48:44+00:00,"['Yuting Xu', 'Jian Liang', 'Lijun Sheng', 'Xiao-Yu Zhang']",http://arxiv.org/abs/2403.10261v2
PGCS: Physical Law embedded Generative Cloud Synthesis in Remote Sensing Images,"Data quantity and quality are both critical for information extraction and
analyzation in remote sensing. However, the current remote sensing datasets
often fail to meet these two requirements, for which cloud is a primary factor
degrading the data quantity and quality. This limitation affects the precision
of results in remote sensing application, particularly those derived from
data-driven techniques. In this paper, a physical law embedded generative cloud
synthesis method (PGCS) is proposed to generate diverse realistic cloud images
to enhance real data and promote the development of algorithms for subsequent
tasks, such as cloud correction, cloud detection, and data augmentation for
classification, recognition, and segmentation. The PGCS method involves two key
phases: spatial synthesis and spectral synthesis. In the spatial synthesis
phase, a style-based generative adversarial network is utilized to simulate the
spatial characteristics, generating an infinite number of single-channel
clouds. In the spectral synthesis phase, the atmospheric scattering law is
embedded through a local statistics and global fitting method, converting the
single-channel clouds into multi-spectral clouds. The experimental results
demonstrate that PGCS achieves a high accuracy in both phases and performs
better than three other existing cloud synthesis methods. Two cloud correction
methods are developed from PGCS and exhibits a superior performance compared to
state-of-the-art methods in the cloud correction task. Furthermore, the
application of PGCS with data from various sensors was investigated and
successfully extended. Code will be provided at
https://github.com/Liying-Xu/PGCS.",2024-10-22 12:36:03+00:00,"['Liying Xu', 'Huifang Li', 'Huanfeng Shen', 'Mingyang Lei', 'Tao Jiang']",http://arxiv.org/abs/2410.16955v1
Non-asymptotic bounds for forward processes in denoising diffusions: Ornstein-Uhlenbeck is hard to beat,"Denoising diffusion probabilistic models (DDPMs) represent a recent advance
in generative modelling that has delivered state-of-the-art results across many
domains of applications. Despite their success, a rigorous theoretical
understanding of the error within DDPMs, particularly the non-asymptotic bounds
required for the comparison of their efficiency, remain scarce. Making minimal
assumptions on the initial data distribution, allowing for example the manifold
hypothesis, this paper presents explicit non-asymptotic bounds on the forward
diffusion error in total variation (TV), expressed as a function of the
terminal time $T$.
  We parametrise multi-modal data distributions in terms of the distance $R$ to
their furthest modes and consider forward diffusions with additive and
multiplicative noise. Our analysis rigorously proves that, under mild
assumptions, the canonical choice of the Ornstein-Uhlenbeck (OU) process cannot
be significantly improved in terms of reducing the terminal time $T$ as a
function of $R$ and error tolerance $\varepsilon>0$. Motivated by data
distributions arising in generative modelling, we also establish a cut-off like
phenomenon (as $R\to\infty$) for the convergence to its invariant measure in TV
of an OU process, initialized at a multi-modal distribution with maximal mode
distance $R$.",2024-08-25 10:28:31+00:00,"['Miha Brear', 'Aleksandar Mijatovi']",http://arxiv.org/abs/2408.13799v1
MaGGIe: Masked Guided Gradual Human Instance Matting,"Human matting is a foundation task in image and video processing, where human
foreground pixels are extracted from the input. Prior works either improve the
accuracy by additional guidance or improve the temporal consistency of a single
instance across frames. We propose a new framework MaGGIe, Masked Guided
Gradual Human Instance Matting, which predicts alpha mattes progressively for
each human instances while maintaining the computational cost, precision, and
consistency. Our method leverages modern architectures, including transformer
attention and sparse convolution, to output all instance mattes simultaneously
without exploding memory and latency. Although keeping constant inference costs
in the multiple-instance scenario, our framework achieves robust and versatile
performance on our proposed synthesized benchmarks. With the higher quality
image and video matting benchmarks, the novel multi-instance synthesis approach
from publicly available sources is introduced to increase the generalization of
models in real-world scenarios.",2024-04-24 17:59:53+00:00,"['Chuong Huynh', 'Seoung Wug Oh', 'Abhinav Shrivastava', 'Joon-Young Lee']",http://arxiv.org/abs/2404.16035v1
Video2MR: Automatically Generating Mixed Reality 3D Instructions by Augmenting Extracted Motion from 2D Videos,"This paper introduces Video2MR, a mixed reality system that automatically
generates 3D sports and exercise instructions from 2D videos. Mixed reality
instructions have great potential for physical training, but existing works
require substantial time and cost to create these 3D experiences. Video2MR
overcomes this limitation by transforming arbitrary instructional videos
available online into MR 3D avatars with AI-enabled motion capture
(DeepMotion). Then, it automatically enhances the avatar motion through the
following augmentation techniques: 1) contrasting and highlighting differences
between the user and avatar postures, 2) visualizing key trajectories and
movements of specific body parts, 3) manipulation of time and speed using body
motion, and 4) spatially repositioning avatars for different perspectives.
Developed on Hololens 2 and Azure Kinect, we showcase various use cases,
including yoga, dancing, soccer, tennis, and other physical exercises. The
study results confirm that Video2MR provides more engaging and playful learning
experiences, compared to existing 2D video instructions.",2024-05-28 20:19:38+00:00,"['Keiichi Ihara', 'Kyzyl Monteiro', 'Mehrad Faridan', 'Rubaiat Habib Kazi', 'Ryo Suzuki']",http://arxiv.org/abs/2405.18565v1
Exploring AI-based Anonymization of Industrial Image and Video Data in the Context of Feature Preservation,"With rising technologies, the protection of privacy-sensitive information is
becoming increasingly important. In industry and production facilities, image
or video recordings are beneficial for documentation, tracing production errors
or coordinating workflows. Individuals in images or videos need to be
anonymized. However, the anonymized data should be reusable for further
applications. In this work, we apply the Deep Learning-based full-body
anonymization framework DeepPrivacy2, which generates artificial identities, to
industrial image and video data. We compare its performance with conventional
anonymization techniques. Therefore, we consider the quality of identity
generation, temporal consistency, and the applicability of pose estimation and
action recognition.",2024-05-29 15:15:52+00:00,"['Sabrina Cynthia Triess', 'Timo Leitritz', 'Christian Jauch']",http://arxiv.org/abs/2405.19173v1
MotionBooth: Motion-Aware Customized Text-to-Video Generation,"In this work, we present MotionBooth, an innovative framework designed for
animating customized subjects with precise control over both object and camera
movements. By leveraging a few images of a specific object, we efficiently
fine-tune a text-to-video model to capture the object's shape and attributes
accurately. Our approach presents subject region loss and video preservation
loss to enhance the subject's learning performance, along with a subject token
cross-attention loss to integrate the customized subject with motion control
signals. Additionally, we propose training-free techniques for managing subject
and camera motions during inference. In particular, we utilize cross-attention
map manipulation to govern subject motion and introduce a novel latent shift
module for camera movement control as well. MotionBooth excels in preserving
the appearance of subjects while simultaneously controlling the motions in
generated videos. Extensive quantitative and qualitative evaluations
demonstrate the superiority and effectiveness of our method. Our project page
is at https://jianzongwu.github.io/projects/motionbooth",2024-06-25 17:42:25+00:00,"['Jianzong Wu', 'Xiangtai Li', 'Yanhong Zeng', 'Jiangning Zhang', 'Qianyu Zhou', 'Yining Li', 'Yunhai Tong', 'Kai Chen']",http://arxiv.org/abs/2406.17758v3
ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos,"We introduce ReXTime, a benchmark designed to rigorously test AI models'
ability to perform temporal reasoning within video events. Specifically,
ReXTime focuses on reasoning across time, i.e. human-like understanding when
the question and its corresponding answer occur in different video segments.
This form of reasoning, requiring advanced understanding of cause-and-effect
relationships across video segments, poses significant challenges to even the
frontier multimodal large language models. To facilitate this evaluation, we
develop an automated pipeline for generating temporal reasoning question-answer
pairs, significantly reducing the need for labor-intensive manual annotations.
Our benchmark includes 921 carefully vetted validation samples and 2,143 test
samples, each manually curated for accuracy and relevance. Evaluation results
show that while frontier large language models outperform academic models, they
still lag behind human performance by a significant 14.3% accuracy gap.
Additionally, our pipeline creates a training dataset of 9,695 machine
generated samples without manual effort, which empirical studies suggest can
enhance the across-time reasoning via fine-tuning.",2024-06-27 17:59:45+00:00,"['Jr-Jen Chen', 'Yu-Chien Liao', 'Hsi-Che Lin', 'Yu-Chu Yu', 'Yen-Chun Chen', 'Yu-Chiang Frank Wang']",http://arxiv.org/abs/2406.19392v2
Unified Embedding Alignment for Open-Vocabulary Video Instance Segmentation,"Open-Vocabulary Video Instance Segmentation (VIS) is attracting increasing
attention due to its ability to segment and track arbitrary objects. However,
the recent Open-Vocabulary VIS attempts obtained unsatisfactory results,
especially in terms of generalization ability of novel categories. We discover
that the domain gap between the VLM features (e.g., CLIP) and the instance
queries and the underutilization of temporal consistency are two central
causes. To mitigate these issues, we design and train a novel Open-Vocabulary
VIS baseline called OVFormer. OVFormer utilizes a lightweight module for
unified embedding alignment between query embeddings and CLIP image embeddings
to remedy the domain gap. Unlike previous image-based training methods, we
conduct video-based model training and deploy a semi-online inference scheme to
fully mine the temporal consistency in the video. Without bells and whistles,
OVFormer achieves 21.9 mAP with a ResNet-50 backbone on LV-VIS, exceeding the
previous state-of-the-art performance by 7.7. Extensive experiments on some
Close-Vocabulary VIS datasets also demonstrate the strong zero-shot
generalization ability of OVFormer (+ 7.6 mAP on YouTube-VIS 2019, + 3.9 mAP on
OVIS). Code is available at https://github.com/fanghaook/OVFormer.",2024-07-10 07:30:51+00:00,"['Hao Fang', 'Peng Wu', 'Yawei Li', 'Xinxin Zhang', 'Xiankai Lu']",http://arxiv.org/abs/2407.07427v2
ecVoice: Audio Text Extraction and Optimization of Video Based on Idioms Similarity Replacement,"The Text Extraction of the Audio from the Video plays an important role in
multimedia editing and processing. As a popular open source toolkit, Whisper
performs fast in human voice recognition. However, the recognition performance
is dependent on the computing resource, which makes the low computing memory
running Whisper become difficult. Our paper presents an available solution to
extract the human voice from the video and gain the high quality text
generation from the voice. The generated voice can be used in video language
translation and translated voice simulation. To improve the extraction and
transform quality of human voice, we present ecVoice, a method using the idioms
similarity computation and analysis to improve the quality of audio text
extraction. Relative experiments are held to verify that the ecVoice can
improve the idiom grammar correction rate to 90\% on average. The method is
simple but fast which means this method will cause less bad influence of
consuming computing resources when improving the voice recognition rate. Our
method and solution can significantly enhance the Whisper recognition with low
computing memory.",2024-05-20 20:27:17+00:00,['Jinwei Lin'],http://arxiv.org/abs/2407.09489v1
Real Face Video Animation Platform,"In recent years, facial video generation models have gained popularity.
However, these models often lack expressive power when dealing with exaggerated
anime-style faces due to the absence of high-quality anime-style face training
sets. We propose a facial animation platform that enables real-time conversion
from real human faces to cartoon-style faces, supporting multiple models. Built
on the Gradio framework, our platform ensures excellent interactivity and
user-friendliness. Users can input a real face video or image and select their
desired cartoon style. The system will then automatically analyze facial
features, execute necessary preprocessing, and invoke appropriate models to
generate expressive anime-style faces. We employ a variety of models within our
system to process the HDTF dataset, thereby creating an animated facial video
dataset.",2024-07-12 14:17:41+00:00,"['Xiaokai Chen', 'Xuan Liu', 'Donglin Di', 'Yongjia Ma', 'Wei Chen', 'Tonghua Su']",http://arxiv.org/abs/2407.18955v1
One-shot Video Imitation via Parameterized Symbolic Abstraction Graphs,"Learning to manipulate dynamic and deformable objects from a single
demonstration video holds great promise in terms of scalability. Previous
approaches have predominantly focused on either replaying object relationships
or actor trajectories. The former often struggles to generalize across diverse
tasks, while the latter suffers from data inefficiency. Moreover, both
methodologies encounter challenges in capturing invisible physical attributes,
such as forces. In this paper, we propose to interpret video demonstrations
through Parameterized Symbolic Abstraction Graphs (PSAG), where nodes represent
objects and edges denote relationships between objects. We further ground
geometric constraints through simulation to estimate non-geometric, visually
imperceptible attributes. The augmented PSAG is then applied in real robot
experiments. Our approach has been validated across a range of tasks, such as
Cutting Avocado, Cutting Vegetable, Pouring Liquid, Rolling Dough, and Slicing
Pizza. We demonstrate successful generalization to novel objects with distinct
visual and physical properties.",2024-08-22 18:26:47+00:00,"['Jianren Wang', 'Kangni Liu', 'Dingkun Guo', 'Xian Zhou', 'Christopher G Atkeson']",http://arxiv.org/abs/2408.12674v2
Less is more: concatenating videos for Sign Language Translation from a small set of signs,"The limited amount of labeled data for training the Brazilian Sign Language
(Libras) to Portuguese Translation models is a challenging problem due to video
collection and annotation costs. This paper proposes generating sign language
content by concatenating short clips containing isolated signals for training
Sign Language Translation models. We employ the V-LIBRASIL dataset, composed of
4,089 sign videos for 1,364 signs, interpreted by at least three persons, to
create hundreds of thousands of sentences with their respective Libras
translation, and then, to feed the model. More specifically, we propose several
experiments varying the vocabulary size and sentence structure, generating
datasets with approximately 170K, 300K, and 500K videos. Our results achieve
meaningful scores of 9.2% and 26.2% for BLEU-4 and METEOR, respectively. Our
technique enables the creation or extension of existing datasets at a much
lower cost than the collection and annotation of thousands of sentences
providing clear directions for future works.",2024-09-03 00:14:15+00:00,"['David Vinicius da Silva', 'Valter Estevam', 'David Menotti']",http://arxiv.org/abs/2409.01506v1
MultiCounter: Multiple Action Agnostic Repetition Counting in Untrimmed Videos,"Multi-instance Repetitive Action Counting (MRAC) aims to estimate the number
of repetitive actions performed by multiple instances in untrimmed videos,
commonly found in human-centric domains like sports and exercise. In this
paper, we propose MultiCounter, a fully end-to-end deep learning framework that
enables simultaneous detection, tracking, and counting of repetitive actions of
multiple human instances. Specifically, MultiCounter incorporates two novel
modules: 1) mixed spatiotemporal interaction for efficient context correlation
across consecutive frames, and 2) task-specific heads for accurate perception
of periodic boundaries and generalization for action-agnostic human instances.
We train MultiCounter on a synthetic dataset called MultiRep generated from
annotated real-world videos. Experiments on the MultiRep dataset validate the
fundamental challenge of MRAC tasks and showcase the superiority of our
proposed model. Compared to ByteTrack+RepNet, a solution that combines an
advanced tracker with a single repetition counter, MultiCounter substantially
improves Period-mAP by 41.0%, reduces AvgMAE by 58.6%, and increases AvgOBO
1.48 times. This sets a new benchmark in the field of MRAC. Moreover,
MultiCounter runs in real-time on a commodity GPU server and is insensitive to
the number of human instances in a video.",2024-09-06 05:57:49+00:00,"['Yin Tang', 'Wei Luo', 'Jinrui Zhang', 'Wei Huang', 'Ruihai Jing', 'Deyu Zhang']",http://arxiv.org/abs/2409.04035v1
L-C4: Language-Based Video Colorization for Creative and Consistent Color,"Automatic video colorization is inherently an ill-posed problem because each
monochrome frame has multiple optional color candidates. Previous
exemplar-based video colorization methods restrict the user's imagination due
to the elaborate retrieval process. Alternatively, conditional image
colorization methods combined with post-processing algorithms still struggle to
maintain temporal consistency. To address these issues, we present
Language-based video Colorization for Creative and Consistent Colors (L-C4) to
guide the colorization process using user-provided language descriptions. Our
model is built upon a pre-trained cross-modality generative model, leveraging
its comprehensive language understanding and robust color representation
abilities. We introduce the cross-modality pre-fusion module to generate
instance-aware text embeddings, enabling the application of creative colors.
Additionally, we propose temporally deformable attention to prevent flickering
or color shifts, and cross-clip fusion to maintain long-term color consistency.
Extensive experimental results demonstrate that L-C4 outperforms relevant
methods, achieving semantically accurate colors, unrestricted creative
correspondence, and temporally robust consistency.",2024-10-07 12:16:21+00:00,"['Zheng Chang', 'Shuchen Weng', 'Huan Ouyang', 'Yu Li', 'Si Li', 'Boxin Shi']",http://arxiv.org/abs/2410.04972v2
DesignMinds: Enhancing Video-Based Design Ideation with Vision-Language Model and Context-Injected Large Language Model,"Ideation is a critical component of video-based design (VBD), where videos
serve as the primary medium for design exploration and inspiration. The
emergence of generative AI offers considerable potential to enhance this
process by streamlining video analysis and facilitating idea generation. In
this paper, we present DesignMinds, a prototype that integrates a
state-of-the-art Vision-Language Model (VLM) with a context-enhanced Large
Language Model (LLM) to support ideation in VBD. To evaluate DesignMinds, we
conducted a between-subject study with 35 design practitioners, comparing its
performance to a baseline condition. Our results demonstrate that DesignMinds
significantly enhances the flexibility and originality of ideation, while also
increasing task engagement. Importantly, the introduction of this technology
did not negatively impact user experience, technology acceptance, or usability.",2024-11-06 11:00:44+00:00,"['Tianhao He', 'Andrija Stankovic', 'Evangelos Niforatos', 'Gerd Kortuem']",http://arxiv.org/abs/2411.03827v1
Identity-Preserving Pose-Guided Character Animation via Facial Landmarks Transformation,"Creating realistic pose-guided image-to-video character animations while
preserving facial identity remains challenging, especially in complex and
dynamic scenarios such as dancing, where precise identity consistency is
crucial. Existing methods frequently encounter difficulties maintaining facial
coherence due to misalignments between facial landmarks extracted from driving
videos that provide head pose and expression cues and the facial geometry of
the reference images. To address this limitation, we introduce the Facial
Landmarks Transformation (FLT) method, which leverages a 3D Morphable Model to
address this limitation. FLT converts 2D landmarks into a 3D face model,
adjusts the 3D face model to align with the reference identity, and then
transforms them back into 2D landmarks to guide the image-to-video generation
process. This approach ensures accurate alignment with the reference facial
geometry, enhancing the consistency between generated videos and reference
images. Experimental results demonstrate that FLT effectively preserves facial
identity, significantly improving pose-guided character animation models.",2024-12-12 06:13:32+00:00,"['Lianrui Mu', 'Xingze Zhou', 'Wenjie Zheng', 'Jiangnan Ye', 'Haoji Hu']",http://arxiv.org/abs/2412.08976v2
PolySmart @ TRECVid 2024 Video Captioning (VTT),"In this paper, we present our methods and results for the Video-To-Text (VTT)
task at TRECVid 2024, exploring the capabilities of Vision-Language Models
(VLMs) like LLaVA and LLaVA-NeXT-Video in generating natural language
descriptions for video content. We investigate the impact of fine-tuning VLMs
on VTT datasets to enhance description accuracy, contextual relevance, and
linguistic consistency. Our analysis reveals that fine-tuning substantially
improves the model's ability to produce more detailed and domain-aligned text,
bridging the gap between generic VLM tasks and the specialized needs of VTT.
Experimental results demonstrate that our fine-tuned model outperforms baseline
VLMs across various evaluation metrics, underscoring the importance of
domain-specific tuning for complex VTT tasks.",2024-12-20 02:45:37+00:00,"['Jiaxin Wu', 'Wengyu Zhang', 'Xiao-Yong Wei', 'Qing Li']",http://arxiv.org/abs/2412.15509v3
"Multi-Modal Generative AI: Multi-modal LLM, Diffusion and Beyond","Multi-modal generative AI has received increasing attention in both academia
and industry. Particularly, two dominant families of techniques are: i) The
multi-modal large language model (MLLM) such as GPT-4V, which shows impressive
ability for multi-modal understanding; ii) The diffusion model such as Sora,
which exhibits remarkable multi-modal powers, especially with respect to visual
generation. As such, one natural question arises: Is it possible to have a
unified model for both understanding and generation? To answer this question,
in this paper, we first provide a detailed review of both MLLM and diffusion
models, including their probabilistic modeling procedure, multi-modal
architecture design, and advanced applications to image/video large language
models as well as text-to-image/video generation. Then, we discuss the two
important questions on the unified model: i) whether the unified model should
adopt the auto-regressive or diffusion probabilistic modeling, and ii) whether
the model should utilize a dense architecture or the Mixture of Experts(MoE)
architectures to better support generation and understanding, two objectives.
We further provide several possible strategies for building a unified model and
analyze their potential advantages and disadvantages. We also summarize
existing large-scale multi-modal datasets for better model pretraining in the
future. To conclude the paper, we present several challenging future
directions, which we believe can contribute to the ongoing advancement of
multi-modal generative AI.",2024-09-23 13:16:09+00:00,"['Hong Chen', 'Xin Wang', 'Yuwei Zhou', 'Bin Huang', 'Yipeng Zhang', 'Wei Feng', 'Houlun Chen', 'Zeyang Zhang', 'Siao Tang', 'Wenwu Zhu']",http://arxiv.org/abs/2409.14993v1
HC$^3$L-Diff: Hybrid conditional latent diffusion with high frequency enhancement for CBCT-to-CT synthesis,"Background: Cone-beam computed tomography (CBCT) plays a crucial role in
image-guided radiotherapy, but artifacts and noise make them unsuitable for
accurate dose calculation. Artificial intelligence methods have shown promise
in enhancing CBCT quality to produce synthetic CT (sCT) images. However,
existing methods either produce images of suboptimal quality or incur excessive
time costs, failing to satisfy clinical practice standards. Methods and
materials: We propose a novel hybrid conditional latent diffusion model for
efficient and accurate CBCT-to-CT synthesis, named HC$^3$L-Diff. We employ the
Unified Feature Encoder (UFE) to compress images into a low-dimensional latent
space, thereby optimizing computational efficiency. Beyond the use of CBCT
images, we propose integrating its high-frequency knowledge as a hybrid
condition to guide the diffusion model in generating sCT images with preserved
structural details. This high-frequency information is captured using our
designed High-Frequency Extractor (HFE). During inference, we utilize denoising
diffusion implicit model to facilitate rapid sampling. We construct a new
in-house prostate dataset with paired CBCT and CT to validate the effectiveness
of our method. Result: Extensive experimental results demonstrate that our
approach outperforms state-of-the-art methods in terms of sCT quality and
generation efficiency. Moreover, our medical physicist conducts the dosimetric
evaluations to validate the benefit of our method in practical dose
calculation, achieving a remarkable 93.8% gamma passing rate with a 2%/2mm
criterion, superior to other methods. Conclusion: The proposed HC$^3$L-Diff can
efficiently achieve high-quality CBCT-to-CT synthesis in only over 2 mins per
patient. Its promising performance in dose calculation shows great potential
for enhancing real-world adaptive radiotherapy.",2024-11-03 14:00:12+00:00,"['Shi Yin', 'Hongqi Tan', 'Li Ming Chong', 'Haofeng Liu', 'Hui Liu', 'Kang Hao Lee', 'Jeffrey Kit Loong Tuan', 'Dean Ho', 'Yueming Jin']",http://arxiv.org/abs/2411.01575v1
Diffusion Models to Enhance the Resolution of Microscopy Images: A Tutorial,"Diffusion models have emerged as a prominent technique in generative modeling
with neural networks, making their mark in tasks like text-to-image translation
and super-resolution. In this tutorial, we provide a comprehensive guide to
build denoising diffusion probabilistic models (DDPMs) from scratch, with a
specific focus on transforming low-resolution microscopy images into their
corresponding high-resolution versions. We provide the theoretical background,
mathematical derivations, and a detailed Python code implementation using
PyTorch, along with techniques to enhance model performance.",2024-09-24 22:29:22+00:00,"['Harshith Bachimanchi', 'Giovanni Volpe']",http://arxiv.org/abs/2409.16488v1
RadRotator: 3D Rotation of Radiographs with Diffusion Models,"Transforming two-dimensional (2D) images into three-dimensional (3D) volumes
is a well-known yet challenging problem for the computer vision community. In
the medical domain, a few previous studies attempted to convert two or more
input radiographs into computed tomography (CT) volumes. Following their
effort, we introduce a diffusion model-based technology that can rotate the
anatomical content of any input radiograph in 3D space, potentially enabling
the visualization of the entire anatomical content of the radiograph from any
viewpoint in 3D. Similar to previous studies, we used CT volumes to create
Digitally Reconstructed Radiographs (DRRs) as the training data for our model.
However, we addressed two significant limitations encountered in previous
studies: 1. We utilized conditional diffusion models with classifier-free
guidance instead of Generative Adversarial Networks (GANs) to achieve higher
mode coverage and improved output image quality, with the only trade-off being
slower inference time, which is often less critical in medical applications;
and 2. We demonstrated that the unreliable output of style transfer deep
learning (DL) models, such as Cycle-GAN, to transfer the style of actual
radiographs to DRRs could be replaced with a simple yet effective training
transformation that randomly changes the pixel intensity histograms of the
input and ground-truth imaging data during training. This transformation makes
the diffusion model agnostic to any distribution variations of the input data
pixel intensity, enabling the reliable training of a DL model on input DRRs and
applying the exact same model to conventional radiographs (or DRRs) during
inference.",2024-04-19 16:55:12+00:00,"['Pouria Rouzrokh', 'Bardia Khosravi', 'Shahriar Faghani', 'Kellen L. Mulford', 'Michael J. Taunton', 'Bradley J. Erickson', 'Cody C. Wyles']",http://arxiv.org/abs/2404.13000v1
A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images using a GAN,"Due to privacy issues and limited amount of publicly available labeled
datasets in the domain of medical imaging, we propose an image generation
pipeline to synthesize 3D echocardiographic images with corresponding ground
truth labels, to alleviate the need for data collection and for laborious and
error-prone human labeling of images for subsequent Deep Learning (DL) tasks.
The proposed method utilizes detailed anatomical segmentations of the heart as
ground truth label sources. This initial dataset is combined with a second
dataset made up of real 3D echocardiographic images to train a Generative
Adversarial Network (GAN) to synthesize realistic 3D cardiovascular Ultrasound
images paired with ground truth labels. To generate the synthetic 3D dataset,
the trained GAN uses high resolution anatomical models from Computed Tomography
(CT) as input. A qualitative analysis of the synthesized images showed that the
main structures of the heart are well delineated and closely follow the labels
obtained from the anatomical models. To assess the usability of these synthetic
images for DL tasks, segmentation algorithms were trained to delineate the left
ventricle, left atrium, and myocardium. A quantitative analysis of the 3D
segmentations given by the models trained with the synthetic images indicated
the potential use of this GAN approach to generate 3D synthetic data, use the
data to train DL models for different clinical tasks, and therefore tackle the
problem of scarcity of 3D labeled echocardiography datasets.",2024-03-08 15:26:27+00:00,"['Cristiana Tiago', 'Andrew Gilbert', 'Ahmed S. Beela', 'Svein Arne Aase', 'Sten Roar Snare', 'Jurica Sprem']",http://arxiv.org/abs/2403.05384v1
Early Stopping Criteria for Training Generative Adversarial Networks in Biomedical Imaging,"Generative Adversarial Networks (GANs) have high computational costs to train
their complex architectures. Throughout the training process, GANs' output is
analyzed qualitatively based on the loss and synthetic images' diversity and
quality. Based on this qualitative analysis, training is manually halted once
the desired synthetic images are generated. By utilizing an early stopping
criterion, the computational cost and dependence on manual oversight can be
reduced yet impacted by training problems such as mode collapse,
non-convergence, and instability. This is particularly prevalent in biomedical
imagery, where training problems degrade the diversity and quality of synthetic
images, and the high computational cost associated with training makes complex
architectures increasingly inaccessible. This work proposes a novel early
stopping criteria to quantitatively detect training problems, halt training,
and reduce the computational costs associated with synthesizing biomedical
images. Firstly, the range of generator and discriminator loss values is
investigated to assess whether mode collapse, non-convergence, and instability
occur sequentially, concurrently, or interchangeably throughout the training of
GANs. Secondly, utilizing these occurrences in conjunction with the Mean
Structural Similarity Index (MS-SSIM) and Fr\'echet Inception Distance (FID)
scores of synthetic images forms the basis of the proposed early stopping
criteria. This work helps identify the occurrence of training problems in GANs
using low-resource computational cost and reduces training time to generate
diversified and high-quality synthetic images.",2024-05-31 16:33:20+00:00,"['Muhammad Muneeb Saad', 'Mubashir Husain Rehmani', ""Ruairi O'Reilly""]",http://arxiv.org/abs/2405.20987v1
Glance and Focus: Memory Prompting for Multi-Event Video Question Answering,"Video Question Answering (VideoQA) has emerged as a vital tool to evaluate
agents' ability to understand human daily behaviors. Despite the recent success
of large vision language models in many multi-modal tasks, complex situation
reasoning over videos involving multiple human-object interaction events still
remains challenging. In contrast, humans can easily tackle it by using a series
of episode memories as anchors to quickly locate question-related key moments
for reasoning. To mimic this effective reasoning strategy, we propose the
Glance-Focus model. One simple way is to apply an action detection model to
predict a set of actions as key memories. However, these actions within a
closed set vocabulary are hard to generalize to various video domains. Instead
of that, we train an Encoder-Decoder to generate a set of dynamic event
memories at the glancing stage. Apart from using supervised bipartite matching
to obtain the event memories, we further design an unsupervised memory
generation method to get rid of dependence on event annotations. Next, at the
focusing stage, these event memories act as a bridge to establish the
correlation between the questions with high-level event concepts and low-level
lengthy video content. Given the question, the model first focuses on the
generated key event memory, then focuses on the most relevant moment for
reasoning through our designed multi-level cross-attention mechanism. We
conduct extensive experiments on four Multi-Event VideoQA benchmarks including
STAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves
state-of-the-art results, surpassing current large models in various
challenging reasoning tasks. The code and models are available at
https://github.com/ByZ0e/Glance-Focus.",2024-01-03 03:51:16+00:00,"['Ziyi Bai', 'Ruiping Wang', 'Xilin Chen']",http://arxiv.org/abs/2401.01529v1
Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring at Intersections,"Computer vision has advanced research methodologies, enhancing system
services across various fields. It is a core component in traffic monitoring
systems for improving road safety; however, these monitoring systems don't
preserve the privacy of pedestrians who appear in the videos, potentially
revealing their identities. Addressing this issue, our paper introduces
Video-to-Text Pedestrian Monitoring (VTPM), which monitors pedestrian movements
at intersections and generates real-time textual reports, including traffic
signal and weather information. VTPM uses computer vision models for pedestrian
detection and tracking, achieving a latency of 0.05 seconds per video frame.
Additionally, it detects crossing violations with 90.2% accuracy by
incorporating traffic signal data. The proposed framework is equipped with
Phi-3 mini-4k to generate real-time textual reports of pedestrian activity
while stating safety concerns like crossing violations, conflicts, and the
impact of weather on their behavior with latency of 0.33 seconds. To enhance
comprehensive analysis of the generated textual reports, Phi-3 medium is
fine-tuned for historical analysis of these generated textual reports. This
fine-tuning enables more reliable analysis about the pedestrian safety at
intersections, effectively detecting patterns and safety critical events. The
proposed VTPM offers a more efficient alternative to video footage by using
textual reports reducing memory usage, saving up to 253 million percent,
eliminating privacy issues, and enabling comprehensive interactive historical
analysis.",2024-08-21 14:21:53+00:00,"['Ahmed S. Abdelrahman', 'Mohamed Abdel-Aty', 'Dongdong Wang']",http://arxiv.org/abs/2408.11649v1
Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs,"Advancements in generative models have sparked significant interest in
generating images while adhering to specific structural guidelines. Scene graph
to image generation is one such task of generating images which are consistent
with the given scene graph. However, the complexity of visual scenes poses a
challenge in accurately aligning objects based on specified relations within
the scene graph. Existing methods approach this task by first predicting a
scene layout and generating images from these layouts using adversarial
training. In this work, we introduce a novel approach to generate images from
scene graphs which eliminates the need of predicting intermediate layouts. We
leverage pre-trained text-to-image diffusion models and CLIP guidance to
translate graph knowledge into images. Towards this, we first pre-train our
graph encoder to align graph features with CLIP features of corresponding
images using a GAN based training. Further, we fuse the graph features with
CLIP embedding of object labels present in the given scene graph to create a
graph consistent CLIP guided conditioning signal. In the conditioning input,
object embeddings provide coarse structure of the image and graph features
provide structural alignment based on relationships among objects. Finally, we
fine tune a pre-trained diffusion model with the graph consistent conditioning
signal with reconstruction and CLIP alignment loss. Elaborate experiments
reveal that our method outperforms existing methods on standard benchmarks of
COCO-stuff and Visual Genome dataset.",2024-01-25 11:46:31+00:00,"['Rameshwar Mishra', 'A V Subramanyam']",http://arxiv.org/abs/2401.14111v3
FIND: Fine-tuning Initial Noise Distribution with Policy Optimization for Diffusion Models,"In recent years, large-scale pre-trained diffusion models have demonstrated
their outstanding capabilities in image and video generation tasks. However,
existing models tend to produce visual objects commonly found in the training
dataset, which diverges from user input prompts. The underlying reason behind
the inaccurate generated results lies in the model's difficulty in sampling
from specific intervals of the initial noise distribution corresponding to the
prompt. Moreover, it is challenging to directly optimize the initial
distribution, given that the diffusion process involves multiple denoising
steps. In this paper, we introduce a Fine-tuning Initial Noise Distribution
(FIND) framework with policy optimization, which unleashes the powerful
potential of pre-trained diffusion networks by directly optimizing the initial
distribution to align the generated contents with user-input prompts. To this
end, we first reformulate the diffusion denoising procedure as a one-step
Markov decision process and employ policy optimization to directly optimize the
initial distribution. In addition, a dynamic reward calibration module is
proposed to ensure training stability during optimization. Furthermore, we
introduce a ratio clipping algorithm to utilize historical data for network
training and prevent the optimized distribution from deviating too far from the
original policy to restrain excessive optimization magnitudes. Extensive
experiments demonstrate the effectiveness of our method in both text-to-image
and text-to-video tasks, surpassing SOTA methods in achieving consistency
between prompts and the generated content. Our method achieves 10 times faster
than the SOTA approach. Our homepage is available at
\url{https://github.com/vpx-ecnu/FIND-website}.",2024-07-28 10:07:55+00:00,"['Changgu Chen', 'Libing Yang', 'Xiaoyan Yang', 'Lianggangxu Chen', 'Gaoqi He', 'CHangbo Wang', 'Yang Li']",http://arxiv.org/abs/2407.19453v1
Additional Look into GAN-based Augmentation for Deep Learning COVID-19 Image Classification,"The availability of training data is one of the main limitations in deep
learning applications for medical imaging. Data augmentation is a popular
approach to overcome this problem. A new approach is a Machine Learning based
augmentation, in particular usage of Generative Adversarial Networks (GAN). In
this case, GANs generate images similar to the original dataset so that the
overall training data amount is bigger, which leads to better performance of
trained networks. A GAN model consists of two networks, a generator and a
discriminator interconnected in a feedback loop which creates a competitive
environment. This work is a continuation of the previous research where we
trained StyleGAN2-ADA by Nvidia on the limited COVID-19 chest X-ray image
dataset. In this paper, we study the dependence of the GAN-based augmentation
performance on dataset size with a focus on small samples. Two datasets are
considered, one with 1000 images per class (4000 images in total) and the
second with 500 images per class (2000 images in total). We train StyleGAN2-ADA
with both sets and then, after validating the quality of generated images, we
use trained GANs as one of the augmentations approaches in multi-class
classification problems. We compare the quality of the GAN-based augmentation
approach to two different approaches (classical augmentation and no
augmentation at all) by employing transfer learning-based classification of
COVID-19 chest X-ray images. The results are quantified using different
classification quality metrics and compared to the results from the literature.
The GAN-based augmentation approach is found to be comparable with classical
augmentation in the case of medium and large datasets but underperforms in the
case of smaller datasets. The correlation between the size of the original
dataset and the quality of classification is visible independently from the
augmentation approach.",2024-01-26 08:28:13+00:00,"['Oleksandr Fedoruk', 'Konrad Klimaszewski', 'Aleksander Ogonowski', 'Micha Kruk']",http://arxiv.org/abs/2401.14705v2
Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem,"Typical LiDAR-based 3D object detection models are trained in a supervised
manner with real-world data collection, which is often imbalanced over classes
(or long-tailed). To deal with it, augmenting minority-class examples by
sampling ground truth (GT) LiDAR points from a database and pasting them into a
scene of interest is often used, but challenges still remain: inflexibility in
locating GT samples and limited sample diversity. In this work, we propose to
leverage pseudo-LiDAR point clouds generated (at a low cost) from videos
capturing a surround view of miniatures or real-world objects of minor classes.
Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of
three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D
view synthesis model, (ii) object-level domain alignment with LiDAR intensity
estimation and (iii) a hybrid context-aware placement method from ground and
map information. We demonstrate the superiority and generality of our method
through performance improvements in extensive experiments conducted on three
popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the
datasets with large domain gaps captured by different LiDAR configurations. Our
code and data will be publicly available upon publication.",2024-03-18 08:50:04+00:00,"['Mincheol Chang', 'Siyeong Lee', 'Jinkyu Kim', 'Namil Kim']",http://arxiv.org/abs/2403.11573v2
PCQA: A Strong Baseline for AIGC Quality Assessment Based on Prompt Condition,"The development of Large Language Models (LLM) and Diffusion Models brings
the boom of Artificial Intelligence Generated Content (AIGC). It is essential
to build an effective quality assessment framework to provide a quantifiable
evaluation of different images or videos based on the AIGC technologies. The
content generated by AIGC methods is driven by the crafted prompts. Therefore,
it is intuitive that the prompts can also serve as the foundation of the AIGC
quality assessment. This study proposes an effective AIGC quality assessment
(QA) framework. First, we propose a hybrid prompt encoding method based on a
dual-source CLIP (Contrastive Language-Image Pre-Training) text encoder to
understand and respond to the prompt conditions. Second, we propose an
ensemble-based feature mixer module to effectively blend the adapted prompt and
vision features. The empirical study practices in two datasets: AIGIQA-20K
(AI-Generated Image Quality Assessment database) and T2VQA-DB (Text-to-Video
Quality Assessment DataBase), which validates the effectiveness of our proposed
method: Prompt Condition Quality Assessment (PCQA). Our proposed simple and
feasible framework may promote research development in the multimodal
generation field.",2024-04-20 07:05:45+00:00,"['Xi Fang', 'Weigang Wang', 'Xiaoxin Lv', 'Jun Yan']",http://arxiv.org/abs/2404.13299v1
PainDiffusion: Learning to Express Pain,"Accurate pain expression synthesis is essential for improving clinical
training and human-robot interaction. Current Robotic Patient Simulators (RPSs)
lack realistic pain facial expressions, limiting their effectiveness in medical
training. In this work, we introduce PainDiffusion, a generative model that
synthesizes naturalistic facial pain expressions. Unlike traditional heuristic
or autoregressive methods, PainDiffusion operates in a continuous latent space,
ensuring smoother and more natural facial motion while supporting
indefinite-length generation via diffusion forcing. Our approach incorporates
intrinsic characteristics such as pain expressiveness and emotion, allowing for
personalized and controllable pain expression synthesis. We train and evaluate
our model using the BioVid HeatPain Database. Additionally, we integrate
PainDiffusion into a robotic system to assess its applicability in real-time
rehabilitation exercises. Qualitative studies with clinicians reveal that
PainDiffusion produces realistic pain expressions, with a 31.2% (std 4.8%)
preference rate against ground-truth recordings. Our results suggest that
PainDiffusion can serve as a viable alternative to real patients in clinical
training and simulation, bridging the gap between synthetic and naturalistic
pain expression. Code and videos are available at:
https://damtien444.github.io/paindf/",2024-09-18 01:55:00+00:00,"['Quang Tien Dam', 'Tri Tung Nguyen Nguyen', 'Yuki Endo', 'Dinh Tuan Tran', 'Joo-Ho Lee']",http://arxiv.org/abs/2409.11635v2
F2FLDM: Latent Diffusion Models with Histopathology Pre-Trained Embeddings for Unpaired Frozen Section to FFPE Translation,"The Frozen Section (FS) technique is a rapid and efficient method, taking
only 15-30 minutes to prepare slides for pathologists' evaluation during
surgery, enabling immediate decisions on further surgical interventions.
However, FS process often introduces artifacts and distortions like folds and
ice-crystal effects. In contrast, these artifacts and distortions are absent in
the higher-quality formalin-fixed paraffin-embedded (FFPE) slides, which
require 2-3 days to prepare. While Generative Adversarial Network (GAN)-based
methods have been used to translate FS to FFPE images (F2F), they may leave
morphological inaccuracies with remaining FS artifacts or introduce new
artifacts, reducing the quality of these translations for clinical assessments.
In this study, we benchmark recent generative models, focusing on GANs and
Latent Diffusion Models (LDMs), to overcome these limitations. We introduce a
novel approach that combines LDMs with Histopathology Pre-Trained Embeddings to
enhance restoration of FS images. Our framework leverages LDMs conditioned by
both text and pre-trained embeddings to learn meaningful features of FS and
FFPE histopathology images. Through diffusion and denoising techniques, our
approach not only preserves essential diagnostic attributes like color staining
and tissue morphology but also proposes an embedding translation mechanism to
better predict the targeted FFPE representation of input FS images. As a
result, this work achieves a significant improvement in classification
performance, with the Area Under the Curve rising from 81.99% to 94.64%,
accompanied by an advantageous CaseFD. This work establishes a new benchmark
for FS to FFPE image translation quality, promising enhanced reliability and
accuracy in histopathology FS image analysis. Our work is available at
https://minhmanho.github.io/f2f_ldm/.",2024-04-19 06:32:21+00:00,"['Man M. Ho', 'Shikha Dubey', 'Yosep Chong', 'Beatrice Knudsen', 'Tolga Tasdizen']",http://arxiv.org/abs/2404.12650v1
Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos,"We propose a generative model that, given a coarsely edited image,
synthesizes a photorealistic output that follows the prescribed layout. Our
method transfers fine details from the original image and preserves the
identity of its parts. Yet, it adapts it to the lighting and context defined by
the new layout. Our key insight is that videos are a powerful source of
supervision for this task: objects and camera motions provide many observations
of how the world changes with viewpoint, lighting, and physical interactions.
We construct an image dataset in which each sample is a pair of source and
target frames extracted from the same video at randomly chosen time intervals.
We warp the source frame toward the target using two motion models that mimic
the expected test-time user edits. We supervise our model to translate the
warped image into the ground truth, starting from a pretrained diffusion model.
Our model design explicitly enables fine detail transfer from the source frame
to the generated image, while closely following the user-specified layout. We
show that by using simple segmentations and coarse 2D manipulations, we can
synthesize a photorealistic edit faithful to the user's input while addressing
second-order effects like harmonizing the lighting and physical interactions
between edited objects.",2024-03-19 17:59:58+00:00,"['Hadi Alzayer', 'Zhihao Xia', 'Xuaner Zhang', 'Eli Shechtman', 'Jia-Bin Huang', 'Michael Gharbi']",http://arxiv.org/abs/2403.13044v1
HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation,"Human image animation involves generating videos from a character photo,
allowing user control and unlocking the potential for video and movie
production. While recent approaches yield impressive results using high-quality
training data, the inaccessibility of these datasets hampers fair and
transparent benchmarking. Moreover, these approaches prioritize 2D human motion
and overlook the significance of camera motions in videos, leading to limited
control and unstable video generation. To demystify the training data, we
present HumanVid, the first large-scale high-quality dataset tailored for human
image animation, which combines crafted real-world and synthetic data. For the
real-world data, we compile a vast collection of real-world videos from the
internet. We developed and applied careful filtering rules to ensure video
quality, resulting in a curated collection of 20K high-resolution (1080P)
human-centric videos. Human and camera motion annotation is accomplished using
a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset,
we collected 10K 3D avatar assets and leveraged existing assets of body shapes,
skin textures and clothings. Notably, we introduce a rule-based camera
trajectory generation method, enabling the synthetic pipeline to incorporate
diverse and precise camera motion annotation, which can rarely be found in
real-world data. To verify the effectiveness of HumanVid, we establish a
baseline model named CamAnimate, short for Camera-controllable Human Animation,
that considers both human and camera motions as conditions. Through extensive
experimentation, we demonstrate that such simple baseline training on our
HumanVid achieves state-of-the-art performance in controlling both human pose
and camera motions, setting a new benchmark. Demo, data and code could be found
in the project website: https://humanvid.github.io/.",2024-07-24 17:15:58+00:00,"['Zhenzhi Wang', 'Yixuan Li', 'Yanhong Zeng', 'Youqing Fang', 'Yuwei Guo', 'Wenran Liu', 'Jing Tan', 'Kai Chen', 'Tianfan Xue', 'Bo Dai', 'Dahua Lin']",http://arxiv.org/abs/2407.17438v3
Depth Anything in Medical Images: A Comparative Study,"Monocular depth estimation (MDE) is a critical component of many medical
tracking and mapping algorithms, particularly from endoscopic or laparoscopic
video. However, because ground truth depth maps cannot be acquired from real
patient data, supervised learning is not a viable approach to predict depth
maps for medical scenes. Although self-supervised learning for MDE has recently
gained attention, the outputs are difficult to evaluate reliably and each MDE's
generalizability to other patients and anatomies is limited. This work
evaluates the zero-shot performance of the newly released Depth Anything Model
on medical endoscopic and laparoscopic scenes. We compare the accuracy and
inference speeds of Depth Anything with other MDE models trained on general
scenes as well as in-domain models trained on endoscopic data. Our findings
show that although the zero-shot capability of Depth Anything is quite
impressive, it is not necessarily better than other models in both speed and
performance. We hope that this study can spark further research in employing
foundation models for MDE in medical scenes.",2024-01-29 22:03:49+00:00,"['John J. Han', 'Ayberk Acar', 'Callahan Henry', 'Jie Ying Wu']",http://arxiv.org/abs/2401.16600v1
DGD: Dynamic 3D Gaussians Distillation,"We tackle the task of learning dynamic 3D semantic radiance fields given a
single monocular video as input. Our learned semantic radiance field captures
per-point semantics as well as color and geometric properties for a dynamic 3D
scene, enabling the generation of novel views and their corresponding
semantics. This enables the segmentation and tracking of a diverse set of 3D
semantic entities, specified using a simple and intuitive interface that
includes a user click or a text prompt. To this end, we present DGD, a unified
3D representation for both the appearance and semantics of a dynamic 3D scene,
building upon the recently proposed dynamic 3D Gaussians representation. Our
representation is optimized over time with both color and semantic information.
Key to our method is the joint optimization of the appearance and semantic
attributes, which jointly affect the geometric properties of the scene. We
evaluate our approach in its ability to enable dense semantic 3D object
tracking and demonstrate high-quality results that are fast to render, for a
diverse set of scenes. Our project webpage is available on
https://isaaclabe.github.io/DGD-Website/",2024-05-29 17:52:22+00:00,"['Isaac Labe', 'Noam Issachar', 'Itai Lang', 'Sagie Benaim']",http://arxiv.org/abs/2405.19321v1
MLLM-SUL: Multimodal Large Language Model for Semantic Scene Understanding and Localization in Traffic Scenarios,"Multimodal large language models (MLLMs) have shown satisfactory effects in
many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint
semantic scene understanding and risk localization tasks, while only relying on
front-view images. In the proposed MLLM-SUL framework, a dual-branch visual
encoder is first designed to extract features from two resolutions, and rich
visual information is conducive to the language model describing risk objects
of different sizes accurately. Then for the language generation, LLaMA model is
fine-tuned to predict scene descriptions, containing the type of driving
scenario, actions of risk objects, and driving intentions and suggestions of
ego-vehicle. Ultimately, a transformer-based network incorporating a regression
token is trained to locate the risk objects. Extensive experiments on the
existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate
that our method is efficient, surpassing many state-of-the-art image-based and
video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and
298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the
localization task. Codes and datasets are available at
https://github.com/fjq-tongji/MLLM-SUL.",2024-12-27 02:05:38+00:00,"['Jiaqi Fan', 'Jianhua Wu', 'Jincheng Gao', 'Jianhao Yu', 'Yafei Wang', 'Hongqing Chu', 'Bingzhao Gao']",http://arxiv.org/abs/2412.19406v1
Evolving from Single-modal to Multi-modal Facial Deepfake Detection: A Survey,"This survey addresses the critical challenge of deepfake detection amidst the
rapid advancements in artificial intelligence. As AI-generated media, including
video, audio and text, become more realistic, the risk of misuse to spread
misinformation and commit identity fraud increases. Focused on face-centric
deepfakes, this work traces the evolution from traditional single-modality
methods to sophisticated multi-modal approaches that handle audio-visual and
text-visual scenarios. We provide comprehensive taxonomies of detection
techniques, discuss the evolution of generative methods from auto-encoders and
GANs to diffusion models, and categorize these technologies by their unique
attributes. To our knowledge, this is the first survey of its kind. We also
explore the challenges of adapting detection methods to new generative models
and enhancing the reliability and robustness of deepfake detectors, proposing
directions for future research. This survey offers a detailed roadmap for
researchers, supporting the development of technologies to counter the
deceptive use of AI in media creation, particularly facial forgery. A curated
list of all related papers can be found at
\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}.",2024-06-11 05:48:04+00:00,"['Ping Liu', 'Qiqi Tao', 'Joey Tianyi Zhou']",http://arxiv.org/abs/2406.06965v3
VastTrack: Vast Category Visual Object Tracking,"In this paper, we introduce a novel benchmark, dubbed VastTrack, towards
facilitating the development of more general visual tracking via encompassing
abundant classes and videos. VastTrack possesses several attractive properties:
(1) Vast Object Category. In particular, it covers target objects from 2,115
classes, largely surpassing object categories of existing popular benchmarks
(e.g., GOT-10k with 563 classes and LaSOT with 70 categories). With such vast
object classes, we expect to learn more general object tracking. (2) Larger
scale. Compared with current benchmarks, VastTrack offers 50,610 sequences with
4.2 million frames, which makes it to date the largest benchmark regarding the
number of videos, and thus could benefit training even more powerful visual
trackers in the deep learning era. (3) Rich Annotation. Besides conventional
bounding box annotations, VastTrack also provides linguistic descriptions for
the videos. The rich annotations of VastTrack enables development of both the
vision-only and the vision-language tracking. To ensure precise annotation, all
videos are manually labeled with multiple rounds of careful inspection and
refinement. To understand performance of existing trackers and to provide
baselines for future comparison, we extensively assess 25 representative
trackers. The results, not surprisingly, show significant drops compared to
those on current datasets due to lack of abundant categories and videos from
diverse scenarios for training, and more efforts are required to improve
general tracking. Our VastTrack and all the evaluation results will be made
publicly available https://github.com/HengLan/VastTrack.",2024-03-06 06:39:43+00:00,"['Liang Peng', 'Junyuan Gao', 'Xinran Liu', 'Weihong Li', 'Shaohua Dong', 'Zhipeng Zhang', 'Heng Fan', 'Libo Zhang']",http://arxiv.org/abs/2403.03493v1
CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation,"Open-vocabulary video instance segmentation strives to segment and track
instances belonging to an open set of categories in a videos. The
vision-language model Contrastive Language-Image Pre-training (CLIP) has shown
robust zero-shot classification ability in image-level open-vocabulary tasks.
In this paper, we propose a simple encoder-decoder network, called CLIP-VIS, to
adapt CLIP for open-vocabulary video instance segmentation. Our CLIP-VIS adopts
frozen CLIP and introduces three modules, including class-agnostic mask
generation, temporal topK-enhanced matching, and weighted open-vocabulary
classification. Given a set of initial queries, class-agnostic mask generation
introduces a pixel decoder and a transformer decoder on CLIP pre-trained image
encoder to predict query masks and corresponding object scores and mask IoU
scores. Then, temporal topK-enhanced matching performs query matching across
frames using the K mostly matched frames. Finally, weighted open-vocabulary
classification first employs mask pooling to generate query visual features
from CLIP pre-trained image encoder, and second performs weighted
classification using object scores and mask IoU scores. Our CLIP-VIS does not
require the annotations of instance categories and identities. The experiments
are performed on various video instance segmentation datasets, which
demonstrate the effectiveness of our proposed method, especially for novel
categories. When using ConvNeXt-B as backbone, our CLIP-VIS achieves the AP and
APn scores of 32.2% and 40.2% on the validation set of LV-VIS dataset, which
outperforms OV2Seg by 11.1% and 23.9% respectively. We will release the source
code and models at https://github.com/zwq456/CLIP-VIS.git.",2024-03-19 05:27:04+00:00,"['Wenqi Zhu', 'Jiale Cao', 'Jin Xie', 'Shuangming Yang', 'Yanwei Pang']",http://arxiv.org/abs/2403.12455v3
Image and Video Compression using Generative Sparse Representation with Fidelity Controls,"We propose a framework for learned image and video compression using the
generative sparse visual representation (SVR) guided by fidelity-preserving
controls. By embedding inputs into a discrete latent space spanned by learned
visual codebooks, SVR-based compression transmits integer codeword indices,
which is efficient and cross-platform robust. However, high-quality (HQ)
reconstruction in the decoder relies on intermediate feature inputs from the
encoder via direct connections. Due to the prohibitively high transmission
costs, previous SVR-based compression methods remove such feature links,
resulting in largely degraded reconstruction quality. In this work, we treat
the intermediate features as fidelity-preserving control signals that guide the
conditioned generative reconstruction in the decoder. Instead of discarding or
directly transferring such signals, we draw them from a low-quality (LQ)
fidelity-preserving alternative input that is sent to the decoder with very low
bitrate. These control signals provide complementary fidelity cues to improve
reconstruction, and their quality is determined by the compression rate of the
LQ alternative, which can be tuned to trade off bitrate, fidelity and
perceptual quality. Our framework can be conveniently used for both learned
image compression (LIC) and learned video compression (LVC). Since SVR is
robust against input perturbations, a large portion of codeword indices between
adjacent frames can be the same. By only transferring different indices,
SVR-based LIC and LVC can share a similar processing pipeline. Experiments over
standard image and video compression benchmarks demonstrate the effectiveness
of our approach.",2024-04-09 07:27:58+00:00,"['Wei Jiang', 'Wei Wang']",http://arxiv.org/abs/2404.06076v1
Improving Interpretable Embeddings for Ad-hoc Video Search with Generative Captions and Multi-word Concept Bank,"Aligning a user query and video clips in cross-modal latent space and that
with semantic concepts are two mainstream approaches for ad-hoc video search
(AVS). However, the effectiveness of existing approaches is bottlenecked by the
small sizes of available video-text datasets and the low quality of concept
banks, which results in the failures of unseen queries and the
out-of-vocabulary problem. This paper addresses these two problems by
constructing a new dataset and developing a multi-word concept bank.
Specifically, capitalizing on a generative model, we construct a new dataset
consisting of 7 million generated text and video pairs for pre-training. To
tackle the out-of-vocabulary problem, we develop a multi-word concept bank
based on syntax analysis to enhance the capability of a state-of-the-art
interpretable AVS method in modeling relationships between query words. We also
study the impact of current advanced features on the method. Experimental
results show that the integration of the above-proposed elements doubles the
R@1 performance of the AVS method on the MSRVTT dataset and improves the xinfAP
on the TRECVid AVS query sets for 2016-2023 (eight years) by a margin from 2%
to 77%, with an average about 20%.",2024-04-09 09:54:21+00:00,"['Jiaxin Wu', 'Chong-Wah Ngo', 'Wing-Kwong Chan']",http://arxiv.org/abs/2404.06173v1
Sakuga-42M Dataset: Scaling Up Cartoon Research,"Hand-drawn cartoon animation employs sketches and flat-color segments to
create the illusion of motion. While recent advancements like CLIP, SVD, and
Sora show impressive results in understanding and generating natural video by
scaling large models with extensive datasets, they are not as effective for
cartoons. Through our empirical experiments, we argue that this ineffectiveness
stems from a notable bias in hand-drawn cartoons that diverges from the
distribution of natural videos. Can we harness the success of the scaling
paradigm to benefit cartoon research? Unfortunately, until now, there has not
been a sizable cartoon dataset available for exploration. In this research, we
propose the Sakuga-42M Dataset, the first large-scale cartoon animation
dataset. Sakuga-42M comprises 42 million keyframes covering various artistic
styles, regions, and years, with comprehensive semantic annotations including
video-text description pairs, anime tags, content taxonomies, etc. We pioneer
the benefits of such a large-scale cartoon dataset on comprehension and
generation tasks by finetuning contemporary foundation models like Video CLIP,
Video Mamba, and SVD, achieving outstanding performance on cartoon-related
tasks. Our motivation is to introduce large-scaling to cartoon research and
foster generalization and robustness in future cartoon applications. Dataset,
Code, and Pretrained Models will be publicly available.",2024-05-13 01:50:05+00:00,['Zhenglin Pan'],http://arxiv.org/abs/2405.07425v1
StyleTalk++: A Unified Framework for Controlling the Speaking Styles of Talking Heads,"Individuals have unique facial expression and head pose styles that reflect
their personalized speaking styles. Existing one-shot talking head methods
cannot capture such personalized characteristics and therefore fail to produce
diverse speaking styles in the final videos. To address this challenge, we
propose a one-shot style-controllable talking face generation method that can
obtain speaking styles from reference speaking videos and drive the one-shot
portrait to speak with the reference speaking styles and another piece of
audio. Our method aims to synthesize the style-controllable coefficients of a
3D Morphable Model (3DMM), including facial expressions and head movements, in
a unified framework. Specifically, the proposed framework first leverages a
style encoder to extract the desired speaking styles from the reference videos
and transform them into style codes. Then, the framework uses a style-aware
decoder to synthesize the coefficients of 3DMM from the audio input and style
codes. During decoding, our framework adopts a two-branch architecture, which
generates the stylized facial expression coefficients and stylized head
movement coefficients, respectively. After obtaining the coefficients of 3DMM,
an image renderer renders the expression coefficients into a specific person's
talking-head video. Extensive experiments demonstrate that our method generates
visually authentic talking head videos with diverse speaking styles from only
one portrait image and an audio clip.",2024-09-14 03:49:38+00:00,"['Suzhen Wang', 'Yifeng Ma', 'Yu Ding', 'Zhipeng Hu', 'Changjie Fan', 'Tangjie Lv', 'Zhidong Deng', 'Xin Yu']",http://arxiv.org/abs/2409.09292v1
SUSTechGAN: Image Generation for Object Detection in Adverse Conditions of Autonomous Driving,"Autonomous driving significantly benefits from data-driven deep neural
networks. However, the data in autonomous driving typically fits the
long-tailed distribution, in which the critical driving data in adverse
conditions is hard to collect. Although generative adversarial networks (GANs)
have been applied to augment data for autonomous driving, generating driving
images in adverse conditions is still challenging. In this work, we propose a
novel framework, SUSTechGAN, with customized dual attention modules,
multi-scale generators, and a novel loss function to generate driving images
for improving object detection of autonomous driving in adverse conditions. We
test the SUSTechGAN and the well-known GANs to generate driving images in
adverse conditions of rain and night and apply the generated images to retrain
object detection networks. Specifically, we add generated images into the
training datasets to retrain the well-known YOLOv5 and evaluate the improvement
of the retrained YOLOv5 for object detection in adverse conditions. The
experimental results show that the generated driving images by our SUSTechGAN
significantly improved the performance of retrained YOLOv5 in rain and night
conditions, which outperforms the well-known GANs. The open-source code, video
description and datasets are available on the page 1 to facilitate image
generation development in autonomous driving under adverse conditions.",2024-07-18 15:32:25+00:00,"['Gongjin Lan', 'Yang Peng', 'Qi Hao', 'Chengzhong Xu']",http://arxiv.org/abs/2408.01430v2
1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation,"The recent transformer-based models have dominated the Referring Video Object
Segmentation (RVOS) task due to the superior performance. Most prior works
adopt unified DETR framework to generate segmentation masks in
query-to-instance manner. In this work, we integrate strengths of that leading
RVOS models to build up an effective paradigm. We first obtain binary mask
sequences from the RVOS models. To improve the consistency and quality of
masks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally
ensembles RVOS models based on framework design as well as training strategy,
and leverages different video object segmentation (VOS) models to enhance mask
coherence by object propagation mechanism. Our method achieves 75.7% J&F on
Ref-Youtube-VOS validation set and 70% J&F on test set, which ranks 1st place
on 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.
Code is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.",2024-01-01 04:24:48+00:00,"['Zhuoyan Luo', 'Yicheng Xiao', 'Yong Liu', 'Yitong Wang', 'Yansong Tang', 'Xiu Li', 'Yujiu Yang']",http://arxiv.org/abs/2401.00663v1
Large Model based Sequential Keyframe Extraction for Video Summarization,"Keyframe extraction aims to sum up a video's semantics with the minimum
number of its frames. This paper puts forward a Large Model based Sequential
Keyframe Extraction for video summarization, dubbed LMSKE, which contains three
stages as below. First, we use the large model ""TransNetV21"" to cut the video
into consecutive shots, and employ the large model ""CLIP2"" to generate each
frame's visual feature within each shot; Second, we develop an adaptive
clustering algorithm to yield candidate keyframes for each shot, with each
candidate keyframe locating nearest to a cluster center; Third, we further
reduce the above candidate keyframes via redundancy elimination within each
shot, and finally concatenate them in accordance with the sequence of shots as
the final sequential keyframes. To evaluate LMSKE, we curate a benchmark
dataset and conduct rich experiments, whose results exhibit that LMSKE performs
much better than quite a few SOTA competitors with average F1 of 0.5311,
average fidelity of 0.8141, and average compression ratio of 0.9922.",2024-01-10 07:09:01+00:00,"['Kailong Tan', 'Yuxiang Zhou', 'Qianchen Xia', 'Rui Liu', 'Yong Chen']",http://arxiv.org/abs/2401.04962v1
Enhancing Surveillance Camera FOV Quality via Semantic Line Detection and Classification with Deep Hough Transform,"The quality of recorded videos and images is significantly influenced by the
camera's field of view (FOV). In critical applications like surveillance
systems and self-driving cars, an inadequate FOV can give rise to severe safety
and security concerns, including car accidents and thefts due to the failure to
detect individuals and objects. The conventional methods for establishing the
correct FOV heavily rely on human judgment and lack automated mechanisms to
assess video and image quality based on FOV. In this paper, we introduce an
innovative approach that harnesses semantic line detection and classification
alongside deep Hough transform to identify semantic lines, thus ensuring a
suitable FOV by understanding 3D view through parallel lines. Our approach
yields an effective F1 score of 0.729 on the public EgoCart dataset, coupled
with a notably high median score in the line placement metric. We illustrate
that our method offers a straightforward means of assessing the quality of the
camera's field of view, achieving a classification accuracy of 83.8\%. This
metric can serve as a proxy for evaluating the potential performance of video
and image quality applications.",2024-01-17 15:30:17+00:00,"['Andrew C. Freeman', 'Wenjing Shi', 'Bin Hwang']",http://arxiv.org/abs/2401.09515v1
On the Audio Hallucinations in Large Audio-Video Language Models,"Large audio-video language models can generate descriptions for both video
and audio. However, they sometimes ignore audio content, producing audio
descriptions solely reliant on visual information. This paper refers to this as
audio hallucinations and analyzes them in large audio-video language models. We
gather 1,000 sentences by inquiring about audio information and annotate them
whether they contain hallucinations. If a sentence is hallucinated, we also
categorize the type of hallucination. The results reveal that 332 sentences are
hallucinated with distinct trends observed in nouns and verbs for each
hallucination type. Based on this, we tackle a task of audio hallucination
classification using pre-trained audio-text models in the zero-shot and
fine-tuning settings. Our experimental results reveal that the zero-shot models
achieve higher performance (52.2% in F1) than the random (40.3%) and the
fine-tuning models achieve 87.9%, outperforming the zero-shot models.",2024-01-18 07:50:07+00:00,"['Taichi Nishimura', 'Shota Nakada', 'Masayoshi Kondo']",http://arxiv.org/abs/2401.09774v1
Training-Free Action Recognition and Goal Inference with Dynamic Frame Selection,"We introduce VidTFS, a Training-free, open-vocabulary video goal and action
inference framework that combines the frozen vision foundational model (VFM)
and large language model (LLM) with a novel dynamic Frame Selection module. Our
experiments demonstrate that the proposed frame selection module improves the
performance of the framework significantly. We validate the performance of the
proposed VidTFS on four widely used video datasets, including CrossTask, COIN,
UCF101, and ActivityNet, covering goal inference and action recognition tasks
under open-vocabulary settings without requiring any training or fine-tuning.
The results show that VidTFS outperforms pretrained and instruction-tuned
multimodal language models that directly stack LLM and VFM for downstream video
inference tasks. Our VidTFS with its adaptability shows the future potential
for generalizing to new training-free video inference tasks.",2024-01-23 03:45:05+00:00,"['Ee Yeo Keat', 'Zhang Hao', 'Alexander Matyasko', 'Basura Fernando']",http://arxiv.org/abs/2401.12471v2
World Model on Million-Length Video And Language With Blockwise RingAttention,"Enabling long-context understanding remains a key challenge in scaling
existing sequence models -- a crucial component in developing generally
intelligent models that can process and operate over long temporal horizons
that potentially consist of millions of tokens. In this paper, we aim to
address these challenges by providing a comprehensive exploration of the full
development process for producing 1M context language models and video-language
models, setting new benchmarks in language retrieval and new capabilities in
long video understanding. We detail our long context data curation process,
progressive context extension from 4K to 1M tokens, and present an efficient
open-source implementation for scalable training on long sequences.
Additionally, we open-source a family of 7B parameter models capable of
processing long text documents and videos exceeding 1M tokens.",2024-02-13 07:47:36+00:00,"['Hao Liu', 'Wilson Yan', 'Matei Zaharia', 'Pieter Abbeel']",http://arxiv.org/abs/2402.08268v4
Multi-modality transrectal ultrasound video classification for identification of clinically significant prostate cancer,"Prostate cancer is the most common noncutaneous cancer in the world.
Recently, multi-modality transrectal ultrasound (TRUS) has increasingly become
an effective tool for the guidance of prostate biopsies. With the aim of
effectively identifying prostate cancer, we propose a framework for the
classification of clinically significant prostate cancer (csPCa) from
multi-modality TRUS videos. The framework utilizes two 3D ResNet-50 models to
extract features from B-mode images and shear wave elastography images,
respectively. An adaptive spatial fusion module is introduced to aggregate two
modalities' features. An orthogonal regularized loss is further used to
mitigate feature redundancy. The proposed framework is evaluated on an in-house
dataset containing 512 TRUS videos, and achieves favorable performance in
identifying csPCa with an area under curve (AUC) of 0.84. Furthermore, the
visualized class activation mapping (CAM) images generated from the proposed
framework may provide valuable guidance for the localization of csPCa, thus
facilitating the TRUS-guided targeted biopsy. Our code is publicly available at
https://github.com/2313595986/ProstateTRUS.",2024-02-14 07:06:30+00:00,"['Hong Wu', 'Juan Fu', 'Hongsheng Ye', 'Yuming Zhong', 'Xuebin Zhou', 'Jianhua Zhou', 'Yi Wang']",http://arxiv.org/abs/2402.08987v2
Digital Video Manipulation Detection Technique Based on Compression Algorithms,"Digital images and videos play a very important role in everyday life.
Nowadays, people have access the affordable mobile devices equipped with
advanced integrated cameras and powerful image processing applications.
Technological development facilitates not only the generation of multimedia
content, but also the intentional modification of it, either with recreational
or malicious purposes. This is where forensic techniques to detect manipulation
of images and videos become essential. This paper proposes a forensic technique
by analysing compression algorithms used by the H.264 coding. The presence of
recompression uses information of macroblocks, a characteristic of the
H.264-MPEG4 standard, and motion vectors. A Vector Support Machine is used to
create the model that allows to accurately detect if a video has been
recompressed.",2024-02-03 16:05:27+00:00,"['Edgar Gonzalez Fernandez', 'Ana Lucila Sandoval Orozco', 'Luis Javier Garcia Villalba']",http://arxiv.org/abs/2403.07891v1
Vid2Real HRI: Align video-based HRI study designs with real-world settings,"HRI research using autonomous robots in real-world settings can produce
results with the highest ecological validity of any study modality, but many
difficulties limit such studies' feasibility and effectiveness. We propose
Vid2Real HRI, a research framework to maximize real-world insights offered by
video-based studies. The Vid2Real HRI framework was used to design an online
study using first-person videos of robots as real-world encounter surrogates.
The online study ($n = 385$) distinguished the within-subjects effects of four
robot behavioral conditions on perceived social intelligence and human
willingness to help the robot enter an exterior door. A real-world,
between-subjects replication ($n = 26$) using two conditions confirmed the
validity of the online study's findings and the sufficiency of the participant
recruitment target ($22$) based on a power analysis of online study results.
The Vid2Real HRI framework offers HRI researchers a principled way to take
advantage of the efficiency of video-based study modalities while generating
directly transferable knowledge of real-world HRI. Code and data from the study
are provided at https://vid2real.github.io/vid2realHRI",2024-03-23 11:09:41+00:00,"['Elliott Hauser', 'Yao-Cheng Chan', 'Sadanand Modak', 'Joydeep Biswas', 'Justin Hart']",http://arxiv.org/abs/2403.15798v1
Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment,"Weakly-supervised action segmentation is a task of learning to partition a
long video into several action segments, where training videos are only
accompanied by transcripts (ordered list of actions). Most of existing methods
need to infer pseudo segmentation for training by serial alignment between all
frames and the transcript, which is time-consuming and hard to be parallelized
while training. In this work, we aim to escape from this inefficient alignment
with massive but redundant frames, and instead to directly localize a few
action transitions for pseudo segmentation generation, where a transition
refers to the change from an action segment to its next adjacent one in the
transcript. As the true transitions are submerged in noisy boundaries due to
intra-segment visual variation, we propose a novel Action-Transition-Aware
Boundary Alignment (ATBA) framework to efficiently and effectively filter out
noisy boundaries and detect transitions. In addition, to boost the semantic
learning in the case that noise is inevitably present in the pseudo
segmentation, we also introduce video-level losses to utilize the trusted
video-level supervision. Extensive experiments show the effectiveness of our
approach on both performance and training speed.",2024-03-28 08:39:44+00:00,"['Angchi Xu', 'Wei-Shi Zheng']",http://arxiv.org/abs/2403.19225v1
What is Point Supervision Worth in Video Instance Segmentation?,"Video instance segmentation (VIS) is a challenging vision task that aims to
detect, segment, and track objects in videos. Conventional VIS methods rely on
densely-annotated object masks which are expensive. We reduce the human
annotations to only one point for each object in a video frame during training,
and obtain high-quality mask predictions close to fully supervised models. Our
proposed training method consists of a class-agnostic proposal generation
module to provide rich negative samples and a spatio-temporal point-based
matcher to match the object queries with the provided point annotations.
Comprehensive experiments on three VIS benchmarks demonstrate competitive
performance of the proposed framework, nearly matching fully supervised
methods.",2024-04-01 17:38:25+00:00,"['Shuaiyi Huang', 'De-An Huang', 'Zhiding Yu', 'Shiyi Lan', 'Subhashree Radhakrishnan', 'Jose M. Alvarez', 'Abhinav Shrivastava', 'Anima Anandkumar']",http://arxiv.org/abs/2404.01990v1
ProTA: Probabilistic Token Aggregation for Text-Video Retrieval,"Text-video retrieval aims to find the most relevant cross-modal samples for a
given query. Recent methods focus on modeling the whole spatial-temporal
relations. However, since video clips contain more diverse content than
captions, the model aligning these asymmetric video-text pairs has a high risk
of retrieving many false positive results. In this paper, we propose
Probabilistic Token Aggregation (ProTA) to handle cross-modal interaction with
content asymmetry. Specifically, we propose dual partial-related aggregation to
disentangle and re-aggregate token representations in both low-dimension and
high-dimension spaces. We propose token-based probabilistic alignment to
generate token-level probabilistic representation and maintain the feature
representation diversity. In addition, an adaptive contrastive loss is proposed
to learn compact cross-modal distribution space. Based on extensive
experiments, ProTA achieves significant improvements on MSR-VTT (50.9%), LSMDC
(25.8%), and DiDeMo (47.2%).",2024-04-18 14:20:30+00:00,"['Han Fang', 'Xianghao Zang', 'Chao Ban', 'Zerun Feng', 'Lanxiang Zhou', 'Zhongjiang He', 'Yongxiang Li', 'Hao Sun']",http://arxiv.org/abs/2404.12216v2
ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection,"Bimanual manipulation is a longstanding challenge in robotics due to the
large number of degrees of freedom and the strict spatial and temporal
synchronization required to generate meaningful behavior. Humans learn bimanual
manipulation skills by watching other humans and by refining their abilities
through play. In this work, we aim to enable robots to learn bimanual
manipulation behaviors from human video demonstrations and fine-tune them
through interaction. Inspired by seminal work in psychology and biomechanics,
we propose modeling the interaction between two hands as a serial kinematic
linkage -- as a screw motion, in particular, that we use to define a new action
space for bimanual manipulation: screw actions. We introduce ScrewMimic, a
framework that leverages this novel action representation to facilitate
learning from human demonstration and self-supervised policy fine-tuning. Our
experiments demonstrate that ScrewMimic is able to learn several complex
bimanual behaviors from a single human video demonstration, and that it
outperforms baselines that interpret demonstrations and fine-tune directly in
the original space of motion of both arms. For more information and video
results, https://robin-lab.cs.utexas.edu/ScrewMimic/",2024-05-06 17:43:34+00:00,"['Arpit Bahety', 'Priyanka Mandikal', 'Ben Abbatematteo', 'Roberto Martn-Martn']",http://arxiv.org/abs/2405.03666v1
Space-time Reinforcement Network for Video Object Segmentation,"Recently, video object segmentation (VOS) networks typically use memory-based
methods: for each query frame, the mask is predicted by space-time matching to
memory frames. Despite these methods having superior performance, they suffer
from two issues: 1) Challenging data can destroy the space-time coherence
between adjacent video frames. 2) Pixel-level matching will lead to undesired
mismatching caused by the noises or distractors. To address the aforementioned
issues, we first propose to generate an auxiliary frame between adjacent
frames, serving as an implicit short-temporal reference for the query one.
Next, we learn a prototype for each video object and prototype-level matching
can be implemented between the query and memory. The experiment demonstrated
that our network outperforms the state-of-the-art method on the DAVIS 2017,
achieving a J&F score of 86.4%, and attains a competitive result 85.0% on
YouTube VOS 2018. In addition, our network exhibits a high inference speed of
32+ FPS.",2024-05-07 06:26:30+00:00,"['Yadang Chen', 'Wentao Zhu', 'Zhi-Xin Yang', 'Enhua Wu']",http://arxiv.org/abs/2405.04042v1
"""Previously on ..."" From Recaps to Story Summarization","We introduce multimodal story summarization by leveraging TV episode recaps -
short video sequences interweaving key story moments from previous episodes to
bring viewers up to speed. We propose PlotSnap, a dataset featuring two crime
thriller TV shows with rich recaps and long episodes of 40 minutes. Story
summarization labels are unlocked by matching recap shots to corresponding
sub-stories in the episode. We propose a hierarchical model TaleSumm that
processes entire episodes by creating compact shot and dialog representations,
and predicts importance scores for each video shot and dialog utterance by
enabling interactions between local story groups. Unlike traditional
summarization, our method extracts multiple plot points from long videos. We
present a thorough evaluation on story summarization, including promising
cross-series generalization. TaleSumm also shows good results on classic video
summarization benchmarks.",2024-05-19 09:09:54+00:00,"['Aditya Kumar Singh', 'Dhruv Srivastava', 'Makarand Tapaswi']",http://arxiv.org/abs/2405.11487v1
2nd Place Solution for MOSE Track in CVPR 2024 PVUW workshop: Complex Video Object Segmentation,"Complex video object segmentation serves as a fundamental task for a wide
range of downstream applications such as video editing and automatic data
annotation. Here we present the 2nd place solution in the MOSE track of PVUW
2024. To mitigate problems caused by tiny objects, similar objects and fast
movements in MOSE. We use instance segmentation to generate extra pretraining
data from the valid and test set of MOSE. The segmented instances are combined
with objects extracted from COCO to augment the training data and enhance
semantic representation of the baseline model. Besides, motion blur is added
during training to increase robustness against image blur induced by motion.
Finally, we apply test time augmentation (TTA) and memory strategy to the
inference stage. Our method ranked 2nd in the MOSE track of PVUW 2024, with a
$\mathcal{J}$ of 0.8007, a $\mathcal{F}$ of 0.8683 and a
$\mathcal{J}$\&$\mathcal{F}$ of 0.8345.",2024-06-12 13:21:33+00:00,"['Zhensong Xu', 'Jiangtao Yao', 'Chengjing Wu', 'Ting Liu', 'Luoqi Liu']",http://arxiv.org/abs/2406.08192v1
Multi-Scale Temporal Difference Transformer for Video-Text Retrieval,"Currently, in the field of video-text retrieval, there are many
transformer-based methods. Most of them usually stack frame features and
regrade frames as tokens, then use transformers for video temporal modeling.
However, they commonly neglect the inferior ability of the transformer modeling
local temporal information. To tackle this problem, we propose a transformer
variant named Multi-Scale Temporal Difference Transformer (MSTDT). MSTDT mainly
addresses the defects of the traditional transformer which has limited ability
to capture local temporal information. Besides, in order to better model the
detailed dynamic information, we make use of the difference feature between
frames, which practically reflects the dynamic movement of a video. We extract
the inter-frame difference feature and integrate the difference and frame
feature by the multi-scale temporal transformer. In general, our proposed MSTDT
consists of a short-term multi-scale temporal difference transformer and a
long-term temporal transformer. The former focuses on modeling local temporal
information, the latter aims at modeling global temporal information. At last,
we propose a new loss to narrow the distance of similar samples. Extensive
experiments show that backbone, such as CLIP, with MSTDT has attained a new
state-of-the-art result.",2024-06-23 13:59:31+00:00,"['Ni Wang', 'Dongliang Liao', 'Xing Xu']",http://arxiv.org/abs/2406.16111v1
VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models,"Recent advancements in Multimodal Large Language Models (MLLMs) have extended
their capabilities to video understanding. Yet, these models are often plagued
by ""hallucinations"", where irrelevant or nonsensical content is generated,
deviating from the actual video context. This work introduces VideoHallucer,
the first comprehensive benchmark for hallucination detection in large
video-language models (LVLMs). VideoHallucer categorizes hallucinations into
two main types: intrinsic and extrinsic, offering further subcategories for
detailed analysis, including object-relation, temporal, semantic detail,
extrinsic factual, and extrinsic non-factual hallucinations. We adopt an
adversarial binary VideoQA method for comprehensive evaluation, where pairs of
basic and hallucinated questions are crafted strategically. By evaluating
eleven LVLMs on VideoHallucer, we reveal that i) the majority of current models
exhibit significant issues with hallucinations; ii) while scaling datasets and
parameters improves models' ability to detect basic visual cues and
counterfactuals, it provides limited benefit for detecting extrinsic factual
hallucinations; iii) existing models are more adept at detecting facts than
identifying hallucinations. As a byproduct, these analyses further instruct the
development of our self-PEP framework, achieving an average of 5.38%
improvement in hallucination resistance across all model architectures.",2024-06-24 06:21:59+00:00,"['Yuxuan Wang', 'Yueqian Wang', 'Dongyan Zhao', 'Cihang Xie', 'Zilong Zheng']",http://arxiv.org/abs/2406.16338v1
Exposure Completing for Temporally Consistent Neural High Dynamic Range Video Rendering,"High dynamic range (HDR) video rendering from low dynamic range (LDR) videos
where frames are of alternate exposure encounters significant challenges, due
to the exposure change and absence at each time stamp. The exposure change and
absence make existing methods generate flickering HDR results. In this paper,
we propose a novel paradigm to render HDR frames via completing the absent
exposure information, hence the exposure information is complete and
consistent. Our approach involves interpolating neighbor LDR frames in the time
dimension to reconstruct LDR frames for the absent exposures. Combining the
interpolated and given LDR frames, the complete set of exposure information is
available at each time stamp. This benefits the fusing process for HDR results,
reducing noise and ghosting artifacts therefore improving temporal consistency.
Extensive experimental evaluations on standard benchmarks demonstrate that our
method achieves state-of-the-art performance, highlighting the importance of
absent exposure completing in HDR video rendering. The code is available at
https://github.com/cuijiahao666/NECHDR.",2024-07-18 09:13:08+00:00,"['Jiahao Cui', 'Wei Jiang', 'Zhan Peng', 'Zhiyu Pan', 'Zhiguo Cao']",http://arxiv.org/abs/2407.13309v2
End-to-End Video Question Answering with Frame Scoring Mechanisms and Adaptive Sampling,"Video Question Answering (VideoQA) has emerged as a challenging frontier in
the field of multimedia processing, requiring intricate interactions between
visual and textual modalities. Simply uniformly sampling frames or
indiscriminately aggregating frame-level visual features often falls short in
capturing the nuanced and relevant contexts of videos to well perform VideoQA.
To mitigate these issues, we propose VidF4, a novel VideoQA framework equipped
with tailored frame selection strategy for effective and efficient VideoQA. We
propose three frame-scoring mechanisms that consider both question relevance
and inter-frame similarity to evaluate the importance of each frame for a given
question on the video. Furthermore, we design a differentiable adaptive frame
sampling mechanism to facilitate end-to-end training for the frame selector and
answer generator. The experimental results across three widely adopted
benchmarks demonstrate that our model consistently outperforms existing VideoQA
methods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA
(+1.0%). Furthermore, through both quantitative and qualitative analyses, we
validate the effectiveness of each design choice.",2024-07-21 04:09:37+00:00,"['Jianxin Liang', 'Xiaojun Meng', 'Yueqian Wang', 'Chang Liu', 'Qun Liu', 'Dongyan Zhao']",http://arxiv.org/abs/2407.15047v2
From Data to Story: Towards Automatic Animated Data Video Creation with LLM-based Multi-Agent Systems,"Creating data stories from raw data is challenging due to humans' limited
attention spans and the need for specialized skills. Recent advancements in
large language models (LLMs) offer great opportunities to develop systems with
autonomous agents to streamline the data storytelling workflow. Though
multi-agent systems have benefits such as fully realizing LLM potentials with
decomposed tasks for individual agents, designing such systems also faces
challenges in task decomposition, performance optimization for sub-tasks, and
workflow design. To better understand these issues, we develop Data Director,
an LLM-based multi-agent system designed to automate the creation of animated
data videos, a representative genre of data stories. Data Director interprets
raw data, breaks down tasks, designs agent roles to make informed decisions
automatically, and seamlessly integrates diverse components of data videos. A
case study demonstrates Data Director's effectiveness in generating data
videos. Throughout development, we have derived lessons learned from addressing
challenges, guiding further advancements in autonomous agents for data
storytelling. We also shed light on future directions for global optimization,
human-in-the-loop design, and the application of advanced multi-modal LLMs.",2024-08-07 16:25:39+00:00,"['Leixian Shen', 'Haotian Li', 'Yun Wang', 'Huamin Qu']",http://arxiv.org/abs/2408.03876v1
Continuous Perception Benchmark,"Humans continuously perceive and process visual signals. However, current
video models typically either sample key frames sparsely or divide videos into
chunks and densely sample within each chunk. This approach stems from the fact
that most existing video benchmarks can be addressed by analyzing key frames or
aggregating information from separate chunks. We anticipate that the next
generation of vision models will emulate human perception by processing visual
input continuously and holistically. To facilitate the development of such
models, we propose the Continuous Perception Benchmark, a video question
answering task that cannot be solved by focusing solely on a few frames or by
captioning small chunks and then summarizing using language models. Extensive
experiments demonstrate that existing models, whether commercial or
open-source, struggle with these tasks, indicating the need for new technical
advancements in this direction.",2024-08-15 00:45:21+00:00,"['Zeyu Wang', 'Zhenzhen Weng', 'Serena Yeung-Levy']",http://arxiv.org/abs/2408.07867v1
VDPI: Video Deblurring with Pseudo-inverse Modeling,"Video deblurring is a challenging task that aims to recover sharp sequences
from blur and noisy observations. The image-formation model plays a crucial
role in traditional model-based methods, constraining the possible solutions.
However, this is only the case for some deep learning-based methods. Despite
deep-learning models achieving better results, traditional model-based methods
remain widely popular due to their flexibility. An increasing number of
scholars combine the two to achieve better deblurring performance. This paper
proposes introducing knowledge of the image-formation model into a deep
learning network by using the pseudo-inverse of the blur. We use a deep network
to fit the blurring and estimate pseudo-inverse. Then, we use this estimation,
combined with a variational deep-learning network, to deblur the video
sequence. Notably, our experimental results demonstrate that such modifications
can significantly improve the performance of deep learning models for video
deblurring. Furthermore, our experiments on different datasets achieved notable
performance improvements, proving that our proposed method can generalize to
different scenarios and cameras.",2024-09-01 16:44:21+00:00,"['Zhihao Huang', 'Santiago Lopez-Tapia', 'Aggelos K. Katsaggelos']",http://arxiv.org/abs/2409.00777v1
Learning Keypoints for Multi-Agent Behavior Analysis using Self-Supervision,"The study of social interactions and collective behaviors through multi-agent
video analysis is crucial in biology. While self-supervised keypoint discovery
has emerged as a promising solution to reduce the need for manual keypoint
annotations, existing methods often struggle with videos containing multiple
interacting agents, especially those of the same species and color. To address
this, we introduce B-KinD-multi, a novel approach that leverages pre-trained
video segmentation models to guide keypoint discovery in multi-agent scenarios.
This eliminates the need for time-consuming manual annotations on new
experimental settings and organisms. Extensive evaluations demonstrate improved
keypoint regression and downstream behavioral classification in videos of
flies, mice, and rats. Furthermore, our method generalizes well to other
species, including ants, bees, and humans, highlighting its potential for broad
applications in automated keypoint annotation for multi-agent behavior
analysis. Code available under: https://danielpkhalil.github.io/B-KinD-Multi",2024-09-14 14:46:44+00:00,"['Daniel Khalil', 'Christina Liu', 'Pietro Perona', 'Jennifer J. Sun', 'Markus Marks']",http://arxiv.org/abs/2409.09455v1
Self-Prompting Polyp Segmentation in Colonoscopy using Hybrid Yolo-SAM 2 Model,"Early diagnosis and treatment of polyps during colonoscopy are essential for
reducing the incidence and mortality of Colorectal Cancer (CRC). However, the
variability in polyp characteristics and the presence of artifacts in
colonoscopy images and videos pose significant challenges for accurate and
efficient polyp detection and segmentation. This paper presents a novel
approach to polyp segmentation by integrating the Segment Anything Model (SAM
2) with the YOLOv8 model. Our method leverages YOLOv8's bounding box
predictions to autonomously generate input prompts for SAM 2, thereby reducing
the need for manual annotations. We conducted exhaustive tests on five
benchmark colonoscopy image datasets and two colonoscopy video datasets,
demonstrating that our method exceeds state-of-the-art models in both image and
video segmentation tasks. Notably, our approach achieves high segmentation
accuracy using only bounding box annotations, significantly reducing annotation
time and effort. This advancement holds promise for enhancing the efficiency
and scalability of polyp detection in clinical settings
https://github.com/sajjad-sh33/YOLO_SAM2.",2024-09-14 17:11:37+00:00,"['Mobina Mansoori', 'Sajjad Shahabodini', 'Jamshid Abouei', 'Konstantinos N. Plataniotis', 'Arash Mohammadi']",http://arxiv.org/abs/2409.09484v1
Shaking the Fake: Detecting Deepfake Videos in Real Time via Active Probes,"Real-time deepfake, a type of generative AI, is capable of ""creating""
non-existing contents (e.g., swapping one's face with another) in a video. It
has been, very unfortunately, misused to produce deepfake videos (during web
conferences, video calls, and identity authentication) for malicious purposes,
including financial scams and political misinformation. Deepfake detection, as
the countermeasure against deepfake, has attracted considerable attention from
the academic community, yet existing works typically rely on learning passive
features that may perform poorly beyond seen datasets. In this paper, we
propose SFake, a new real-time deepfake detection method that innovatively
exploits deepfake models' inability to adapt to physical interference.
Specifically, SFake actively sends probes to trigger mechanical vibrations on
the smartphone, resulting in the controllable feature on the footage.
Consequently, SFake determines whether the face is swapped by deepfake based on
the consistency of the facial area with the probe pattern. We implement SFake,
evaluate its effectiveness on a self-built dataset, and compare it with six
other detection methods. The results show that SFake outperforms other
detection methods with higher detection accuracy, faster process speed, and
lower memory consumption.",2024-09-17 04:58:30+00:00,"['Zhixin Xie', 'Jun Luo']",http://arxiv.org/abs/2409.10889v1
DressRecon: Freeform 4D Human Reconstruction from Monocular Video,"We present a method to reconstruct time-consistent human body models from
monocular videos, focusing on extremely loose clothing or handheld object
interactions. Prior work in human reconstruction is either limited to tight
clothing with no object interactions, or requires calibrated multi-view
captures or personalized template scans which are costly to collect at scale.
Our key insight for high-quality yet flexible reconstruction is the careful
combination of generic human priors about articulated body shape (learned from
large-scale training data) with video-specific articulated ""bag-of-bones""
deformation (fit to a single video via test-time optimization). We accomplish
this by learning a neural implicit model that disentangles body versus clothing
deformations as separate motion model layers. To capture subtle geometry of
clothing, we leverage image-based priors such as human body pose, surface
normals, and optical flow during optimization. The resulting neural fields can
be extracted into time-consistent meshes, or further optimized as explicit 3D
Gaussians for high-fidelity interactive rendering. On datasets with highly
challenging clothing deformations and object interactions, DressRecon yields
higher-fidelity 3D reconstructions than prior art. Project page:
https://jefftan969.github.io/dressrecon/",2024-09-30 17:59:15+00:00,"['Jeff Tan', 'Donglai Xiang', 'Shubham Tulsiani', 'Deva Ramanan', 'Gengshan Yang']",http://arxiv.org/abs/2409.20563v2
TikGuard: A Deep Learning Transformer-Based Solution for Detecting Unsuitable TikTok Content for Kids,"The rise of short-form videos on platforms like TikTok has brought new
challenges in safeguarding young viewers from inappropriate content.
Traditional moderation methods often fall short in handling the vast and
rapidly changing landscape of user-generated videos, increasing the risk of
children encountering harmful material. This paper introduces TikGuard, a
transformer-based deep learning approach aimed at detecting and flagging
content unsuitable for children on TikTok. By using a specially curated
dataset, TikHarm, and leveraging advanced video classification techniques,
TikGuard achieves an accuracy of 86.7%, showing a notable improvement over
existing methods in similar contexts. While direct comparisons are limited by
the uniqueness of the TikHarm dataset, TikGuard's performance highlights its
potential in enhancing content moderation, contributing to a safer online
experience for minors. This study underscores the effectiveness of transformer
models in video classification and sets a foundation for future research in
this area.",2024-10-01 05:00:05+00:00,"['Mazen Balat', 'Mahmoud Essam Gabr', 'Hend Bakr', 'Ahmed B. Zaky']",http://arxiv.org/abs/2410.00403v1
The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024,"This report presents our method for Temporal Action Localisation (TAL), which
focuses on identifying and classifying actions within specific time intervals
throughout a video sequence. We employ a data augmentation technique by
expanding the training dataset using overlapping labels from the
Something-SomethingV2 dataset, enhancing the model's ability to generalize
across various action classes. For feature extraction, we utilize
state-of-the-art models, including UMT, VideoMAEv2 for video features, and
BEATs and CAV-MAE for audio features. Our approach involves training both
multimodal (video and audio) and unimodal (video only) models, followed by
combining their predictions using the Weighted Box Fusion (WBF) method. This
fusion strategy ensures robust action localisation. our overall approach
achieves a score of 0.5498, securing first place in the competition.",2024-10-08 01:07:21+00:00,"['Yinan Han', 'Qingyuan Jiang', 'Hongming Mei', 'Yang Yang', 'Jinhui Tang']",http://arxiv.org/abs/2410.09088v1
CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos,"Most state-of-the-art point trackers are trained on synthetic data due to the
difficulty of annotating real videos for this task. However, this can result in
suboptimal performance due to the statistical gap between synthetic and real
videos. In order to understand these issues better, we introduce CoTracker3,
comprising a new tracking model and a new semi-supervised training recipe. This
allows real videos without annotations to be used during training by generating
pseudo-labels using off-the-shelf teachers. The new model eliminates or
simplifies components from previous trackers, resulting in a simpler and often
smaller architecture. This training scheme is much simpler than prior work and
achieves better results using 1,000 times less data. We further study the
scaling behaviour to understand the impact of using more real unsupervised data
in point tracking. The model is available in online and offline variants and
reliably tracks visible and occluded points.",2024-10-15 17:56:32+00:00,"['Nikita Karaev', 'Iurii Makarov', 'Jianyuan Wang', 'Natalia Neverova', 'Andrea Vedaldi', 'Christian Rupprecht']",http://arxiv.org/abs/2410.11831v1
Pseudo Dataset Generation for Out-of-Domain Multi-Camera View Recommendation,"Multi-camera systems are indispensable in movies, TV shows, and other media.
Selecting the appropriate camera at every timestamp has a decisive impact on
production quality and audience preferences. Learning-based view recommendation
frameworks can assist professionals in decision-making. However, they often
struggle outside of their training domains. The scarcity of labeled
multi-camera view recommendation datasets exacerbates the issue. Based on the
insight that many videos are edited from the original multi-camera videos, we
propose transforming regular videos into pseudo-labeled multi-camera view
recommendation datasets. Promisingly, by training the model on pseudo-labeled
datasets stemming from videos in the target domain, we achieve a 68% relative
improvement in the model's accuracy in the target domain and bridge the
accuracy gap between in-domain and never-before-seen domains.",2024-10-17 14:21:22+00:00,"['Kuan-Ying Lee', 'Qian Zhou', 'Klara Nahrstedt']",http://arxiv.org/abs/2410.13585v1
Zero-Shot Action Recognition in Surveillance Videos,"The growing demand for surveillance in public spaces presents significant
challenges due to the shortage of human resources. Current AI-based video
surveillance systems heavily rely on core computer vision models that require
extensive finetuning, which is particularly difficult in surveillance settings
due to limited datasets and difficult setting (viewpoint, low quality, etc.).
In this work, we propose leveraging Large Vision-Language Models (LVLMs), known
for their strong zero and few-shot generalization, to tackle video
understanding tasks in surveillance. Specifically, we explore VideoLLaMA2, a
state-of-the-art LVLM, and an improved token-level sampling method,
Self-Reflective Sampling (Self-ReS). Our experiments on the UCF-Crime dataset
show that VideoLLaMA2 represents a significant leap in zero-shot performance,
with 20% boost over the baseline. Self-ReS additionally increases zero-shot
action recognition performance to 44.6%. These results highlight the potential
of LVLMs, paired with improved sampling techniques, for advancing surveillance
video analysis in diverse scenarios.",2024-10-28 15:13:53+00:00,"['Joao Pereira', 'Vasco Lopes', 'David Semedo', 'Joao Neves']",http://arxiv.org/abs/2410.21113v2
DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models,"Video large language models (VLLMs) have significantly advanced recently in
processing complex video content, yet their inference efficiency remains
constrained because of the high computational cost stemming from the thousands
of visual tokens generated from the video inputs. We empirically observe that,
unlike single image inputs, VLLMs typically attend visual tokens from different
frames at different decoding iterations, making a one-shot pruning strategy
prone to removing important tokens by mistake. Motivated by this, we present
DyCoke, a training-free token compression method to optimize token
representation and accelerate VLLMs. DyCoke incorporates a plug-and-play
temporal compression module to minimize temporal redundancy by merging
redundant tokens across frames, and applies dynamic KV cache reduction to prune
spatially redundant tokens selectively. It ensures high-quality inference by
dynamically retaining the critical tokens at each decoding step. Extensive
experimental results demonstrate that DyCoke can outperform the prior SoTA
counterparts, achieving 1.5X inference speedup, 1.4X memory reduction against
the baseline VLLM, while still improving the performance, with no training.",2024-11-22 15:55:19+00:00,"['Keda Tao', 'Can Qin', 'Haoxuan You', 'Yang Sui', 'Huan Wang']",http://arxiv.org/abs/2411.15024v2
MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity Prediction,"The surge in micro-videos is transforming the concept of popularity. As
researchers delve into vast multi-modal datasets, there is a growing interest
in understanding the origins of this popularity and the forces driving its
rapid expansion. Recent studies suggest that the virality of short videos is
not only tied to their inherent multi-modal content but is also heavily
influenced by the strength of platform recommendations driven by audience
feedback. In this paper, we introduce a framework for capturing long-term
dependencies in user feedback and dynamic event interactions, based on the
Mamba Hawkes process. Our experiments on the large-scale open-source
multi-modal dataset show that our model significantly outperforms
state-of-the-art approaches across various metrics by 23.2%. We believe our
model's capability to map the relationships within user feedback behavior
sequences will not only contribute to the evolution of next-generation
recommendation algorithms and platform applications but also enhance our
understanding of micro video dissemination and its broader societal impact.",2024-11-23 05:13:27+00:00,"['Jiacheng Lu', 'Mingyuan Xiao', 'Weijian Wang', 'Yuxin Du', 'Yi Cui', 'Jingnan Zhao', 'Cheng Hua']",http://arxiv.org/abs/2411.15455v1
Advanced Learning-Based Inter Prediction for Future Video Coding,"In the fourth generation Audio Video coding Standard (AVS4), the Inter
Prediction Filter (INTERPF) reduces discontinuities between prediction and
adjacent reconstructed pixels in inter prediction. The paper proposes a low
complexity learning-based inter prediction (LLIP) method to replace the
traditional INTERPF. LLIP enhances the filtering process by leveraging a
lightweight neural network model, where parameters can be exported for
efficient inference. Specifically, we extract pixels and coordinates utilized
by the traditional INTERPF to form the training dataset. Subsequently, we
export the weights and biases of the trained neural network model and implement
the inference process without any third-party dependency, enabling seamless
integration into video codec without relying on Libtorch, thus achieving faster
inference speed. Ultimately, we replace the traditional handcraft filtering
parameters in INTERPF with the learned optimal filtering parameters. This
practical solution makes the combination of deep learning encoding tools with
traditional video encoding schemes more efficient. Experimental results show
that our approach achieves 0.01%, 0.31%, and 0.25% coding gain for the Y, U,
and V components under the random access (RA) configuration on average.",2024-11-24 08:47:00+00:00,"['Yanchen Zhao', 'Wenhong Duan', 'Chuanmin Jia', 'Shanshe Wang', 'Siwei Ma']",http://arxiv.org/abs/2411.15759v1
HyperSeg: Towards Universal Visual Segmentation with Large Language Model,"This paper aims to address universal segmentation for image and video
perception with the strong reasoning ability empowered by Visual Large Language
Models (VLLMs). Despite significant progress in current unified segmentation
methods, limitations in adaptation to both image and video scenarios, as well
as the complex reasoning segmentation, make it difficult for them to handle
various challenging instructions and achieve an accurate understanding of
fine-grained vision-language correlations. We propose HyperSeg, the first
VLLM-based universal segmentation model for pixel-level image and video
perception, encompassing generic segmentation tasks and more complex reasoning
perception tasks requiring powerful reasoning abilities and world knowledge.
Besides, to fully leverage the recognition capabilities of VLLMs and the
fine-grained visual information, HyperSeg incorporates hybrid entity
recognition and fine-grained visual perceiver modules for various segmentation
tasks. Combined with the temporal adapter, HyperSeg achieves a comprehensive
understanding of temporal information. Experimental results validate the
effectiveness of our insights in resolving universal image and video
segmentation tasks, including the more complex reasoning perception tasks. Our
code is available.",2024-11-26 17:18:20+00:00,"['Cong Wei', 'Yujie Zhong', 'Haoxian Tan', 'Yong Liu', 'Zheng Zhao', 'Jie Hu', 'Yujiu Yang']",http://arxiv.org/abs/2411.17606v2
Towards Universal Soccer Video Understanding,"As a globally celebrated sport, soccer has attracted widespread interest from
fans all over the world. This paper aims to develop a comprehensive multi-modal
framework for soccer video understanding. Specifically, we make the following
contributions in this paper: (i) we introduce SoccerReplay-1988, the largest
multi-modal soccer dataset to date, featuring videos and detailed annotations
from 1,988 complete matches, with an automated annotation pipeline; (ii) we
present an advanced soccer-specific visual encoder, MatchVision, which
leverages spatiotemporal information across soccer videos and excels in various
downstream tasks; (iii) we conduct extensive experiments and ablation studies
on event classification, commentary generation, and multi-view foul
recognition. MatchVision demonstrates state-of-the-art performance on all of
them, substantially outperforming existing models, which highlights the
superiority of our proposed data and model. We believe that this work will
offer a standard paradigm for sports understanding research.",2024-12-02 18:58:04+00:00,"['Jiayuan Rao', 'Haoning Wu', 'Hao Jiang', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie']",http://arxiv.org/abs/2412.01820v3
Lightweight Stochastic Video Prediction via Hybrid Warping,"Accurate video prediction by deep neural networks, especially for dynamic
regions, is a challenging task in computer vision for critical applications
such as autonomous driving, remote working, and telemedicine. Due to inherent
uncertainties, existing prediction models often struggle with the complexity of
motion dynamics and occlusions. In this paper, we propose a novel stochastic
long-term video prediction model that focuses on dynamic regions by employing a
hybrid warping strategy. By integrating frames generated through forward and
backward warpings, our approach effectively compensates for the weaknesses of
each technique, improving the prediction accuracy and realism of moving regions
in videos while also addressing uncertainty by making stochastic predictions
that account for various motions. Furthermore, considering real-time
predictions, we introduce a MobileNet-based lightweight architecture into our
model. Our model, called SVPHW, achieves state-of-the-art performance on two
benchmark datasets.",2024-12-04 06:33:27+00:00,"['Kazuki Kotoyori', 'Shota Hirose', 'Heming Sun', 'Jiro Katto']",http://arxiv.org/abs/2412.03061v1
Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM,"The application of Large Vision-Language Models (LVLMs) for analyzing images
and videos is an exciting and rapidly evolving field. In recent years, we've
seen significant growth in high-quality image-text datasets for fine-tuning
image understanding, but there is still a lack of comparable datasets for
videos. Additionally, many VideoLLMs are extensions of single-image VLMs, which
may not efficiently handle the complexities of longer videos. In this study, we
introduce a large-scale synthetic dataset created from proprietary models,
using carefully designed prompts to tackle a wide range of questions. We also
explore a dynamic visual token compression architecture that strikes a balance
between computational efficiency and performance. Our proposed \model{}
achieves state-of-the-art results across various video tasks and shows
impressive generalization, setting new baselines in multi-image understanding.
Notably, \model{} delivers an absolute improvement of 2.7\% over
LLaVA-OneVision on VideoMME and 10.7\% on MuirBench. Codes are available at
https://github.com/Hon-Wong/ByteVideoLLM",2024-12-12 18:20:41+00:00,"['Han Wang', 'Yuxiang Nie', 'Yongjie Ye', 'Deng GuanYu', 'Yanjie Wang', 'Shuai Li', 'Haiyang Yu', 'Jinghui Lu', 'Can Huang']",http://arxiv.org/abs/2412.09530v1
Color Enhancement for V-PCC Compressed Point Cloud via 2D Attribute Map Optimization,"Video-based point cloud compression (V-PCC) converts the dynamic point cloud
data into video sequences using traditional video codecs for efficient
encoding. However, this lossy compression scheme introduces artifacts that
degrade the color attributes of the data. This paper introduces a framework
designed to enhance the color quality in the V-PCC compressed point clouds. We
propose the lightweight de-compression Unet (LDC-Unet), a 2D neural network, to
optimize the projection maps generated during V-PCC encoding. The optimized 2D
maps will then be back-projected to the 3D space to enhance the corresponding
point cloud attributes. Additionally, we introduce a transfer learning strategy
and develop a customized natural image dataset for the initial training. The
model was then fine-tuned using the projection maps of the compressed point
clouds. The whole strategy effectively addresses the scarcity of point cloud
training data. Our experiments, conducted on the public 8i voxelized full
bodies long sequences (8iVSLF) dataset, demonstrate the effectiveness of our
proposed method in improving the color quality.",2024-12-19 01:58:00+00:00,"['Jingwei Bao', 'Yu Liu', 'Zeliang Li', 'Shuyuan Zhu', 'Siu-Kei Au Yeung']",http://arxiv.org/abs/2412.14449v1
Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation,"This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical
agent for multi-task robotic manipulation. HDP factorises a manipulation policy
into a hierarchical structure: a high-level task-planning agent which predicts
a distant next-best end-effector pose (NBP), and a low-level goal-conditioned
diffusion policy which generates optimal motion trajectories. The factorised
policy representation allows HDP to tackle both long-horizon task planning
while generating fine-grained low-level actions. To generate context-aware
motion trajectories while satisfying robot kinematics constraints, we present a
novel kinematics-aware goal-conditioned control agent, Robot Kinematics
Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the
end-effector pose and joint position trajectories, and distill the accurate but
kinematics-unaware end-effector pose diffuser to the kinematics-aware but less
accurate joint position diffuser via differentiable kinematics. Empirically, we
show that HDP achieves a significantly higher success rate than the
state-of-the-art methods in both simulation and real-world.",2024-03-06 17:50:26+00:00,"['Xiao Ma', 'Sumit Patidar', 'Iain Haughton', 'Stephen James']",http://arxiv.org/abs/2403.03890v1
Denoising Diffusion Probabilistic Models in Six Simple Steps,"Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of
deep generative model that have been successfully applied to a diverse range of
problems including image and video generation, protein and material synthesis,
weather forecasting, and neural surrogates of partial differential equations.
Despite their ubiquity it is hard to find an introduction to DDPMs which is
simple, comprehensive, clean and clear. The compact explanations necessary in
research papers are not able to elucidate all of the different design steps
taken to formulate the DDPM and the rationale of the steps that are presented
is often omitted to save space. Moreover, the expositions are typically
presented from the variational lower bound perspective which is unnecessary and
arguably harmful as it obfuscates why the method is working and suggests
generalisations that do not perform well in practice. On the other hand,
perspectives that take the continuous time-limit are beautiful and general, but
they have a high barrier-to-entry as they require background knowledge of
stochastic differential equations and probability flow. In this note, we
distill down the formulation of the DDPM into six simple steps each of which
comes with a clear rationale. We assume that the reader is familiar with
fundamental topics in machine learning including basic probabilistic modelling,
Gaussian distributions, maximum likelihood estimation, and deep learning.",2024-02-06 20:43:04+00:00,"['Richard E. Turner', 'Cristiana-Diana Diaconu', 'Stratis Markou', 'Aliaksandra Shysheya', 'Andrew Y. K. Foong', 'Bruno Mlodozeniec']",http://arxiv.org/abs/2402.04384v2
On Differentially Private 3D Medical Image Synthesis with Controllable Latent Diffusion Models,"Generally, the small size of public medical imaging datasets coupled with
stringent privacy concerns, hampers the advancement of data-hungry deep
learning models in medical imaging. This study addresses these challenges for
3D cardiac MRI images in the short-axis view. We propose Latent Diffusion
Models that generate synthetic images conditioned on medical attributes, while
ensuring patient privacy through differentially private model training. To our
knowledge, this is the first work to apply and quantify differential privacy in
3D medical image generation. We pre-train our models on public data and
finetune them with differential privacy on the UK Biobank dataset. Our
experiments reveal that pre-training significantly improves model performance,
achieving a Fr\'echet Inception Distance (FID) of 26.77 at $\epsilon=10$,
compared to 92.52 for models without pre-training. Additionally, we explore the
trade-off between privacy constraints and image quality, investigating how
tighter privacy budgets affect output controllability and may lead to degraded
performance. Our results demonstrate that proper consideration during training
with differential privacy can substantially improve the quality of synthetic
cardiac MRI images, but there are still notable challenges in achieving
consistent medical realism.",2024-07-23 11:49:58+00:00,"['Deniz Daum', 'Richard Osuala', 'Anneliese Riess', 'Georgios Kaissis', 'Julia A. Schnabel', 'Maxime Di Folco']",http://arxiv.org/abs/2407.16405v1
Unconditional Latent Diffusion Models Memorize Patient Imaging Data: Implications for Openly Sharing Synthetic Data,"AI models present a wide range of applications in the field of medicine.
However, achieving optimal performance requires access to extensive healthcare
data, which is often not readily available. Furthermore, the imperative to
preserve patient privacy restricts patient data sharing with third parties and
even within institutes. Recently, generative AI models have been gaining
traction for facilitating open-data sharing by proposing synthetic data as
surrogates of real patient data. Despite the promise, some of these models are
susceptible to patient data memorization, where models generate patient data
copies instead of novel synthetic samples. Considering the importance of the
problem, surprisingly it has received relatively little attention in the
medical imaging community. To this end, we assess memorization in unconditional
latent diffusion models. We train latent diffusion models on CT, MR, and X-ray
datasets for synthetic data generation. We then detect the amount of training
data memorized utilizing our novel self-supervised copy detection approach and
further investigate various factors that can influence memorization. Our
findings show a surprisingly high degree of patient data memorization across
all datasets. Comparison with non-diffusion generative models, such as
autoencoders and generative adversarial networks, indicates that while latent
diffusion models are more susceptible to memorization, overall they outperform
non-diffusion models in synthesis quality. Further analyses reveal that using
augmentation strategies, small architecture, and increasing dataset can reduce
memorization while over-training the models can enhance it. Collectively, our
results emphasize the importance of carefully training generative models on
private medical imaging datasets, and examining the synthetic data to ensure
patient privacy before sharing it for medical research and applications.",2024-02-01 22:58:21+00:00,"['Salman Ul Hassan Dar', 'Marvin Seyfarth', 'Isabelle Ayx', 'Theano Papavassiliu', 'Stefan O. Schoenberg', 'Robert Malte Siepmann', 'Fabian Christopher Laqua', 'Jannik Kahmann', 'Norbert Frey', 'Bettina Baeler', 'Sebastian Foersch', 'Daniel Truhn', 'Jakob Nikolas Kather', 'Sandy Engelhardt']",http://arxiv.org/abs/2402.01054v3
Tracking Virtual Meetings in the Wild: Re-identification in Multi-Participant Virtual Meetings,"In recent years, workplaces and educational institutes have widely adopted
virtual meeting platforms. This has led to a growing interest in analyzing and
extracting insights from these meetings, which requires effective detection and
tracking of unique individuals. In practice, there is no standardization in
video meetings recording layout, and how they are captured across the different
platforms and services. This, in turn, creates a challenge in acquiring this
data stream and analyzing it in a uniform fashion. Our approach provides a
solution to the most general form of video recording, usually consisting of a
grid of participants (\cref{fig:videomeeting}) from a single video source with
no metadata on participant locations, while using the least amount of
constraints and assumptions as to how the data was acquired. Conventional
approaches often use YOLO models coupled with tracking algorithms, assuming
linear motion trajectories akin to that observed in CCTV footage. However, such
assumptions fall short in virtual meetings, where participant video feed window
can abruptly change location across the grid. In an organic video meeting
setting, participants frequently join and leave, leading to sudden, non-linear
movements on the video grid. This disrupts optical flow-based tracking methods
that depend on linear motion. Consequently, standard object detection and
tracking methods might mistakenly assign multiple participants to the same
tracker. In this paper, we introduce a novel approach to track and re-identify
participants in remote video meetings, by utilizing the spatio-temporal priors
arising from the data in our domain. This, in turn, increases tracking
capabilities compared to the use of general object tracking. Our approach
reduces the error rate by 95% on average compared to YOLO-based tracking
methods as a baseline.",2024-09-15 19:37:37+00:00,"['Oriel Perl', 'Ido Leshem', 'Uria Franko', 'Yuval Goldman']",http://arxiv.org/abs/2409.09841v1
PhysMotion: Physics-Grounded Dynamics From a Single Image,"We introduce PhysMotion, a novel framework that leverages principled
physics-based simulations to guide intermediate 3D representations generated
from a single image and input conditions (e.g., applied force and torque),
producing high-quality, physically plausible video generation. By utilizing
continuum mechanics-based simulations as a prior knowledge, our approach
addresses the limitations of traditional data-driven generative models and
result in more consistent physically plausible motions. Our framework begins by
reconstructing a feed-forward 3D Gaussian from a single image through geometry
optimization. This representation is then time-stepped using a differentiable
Material Point Method (MPM) with continuum mechanics-based elastoplasticity
models, which provides a strong foundation for realistic dynamics, albeit at a
coarse level of detail. To enhance the geometry, appearance and ensure
spatiotemporal consistency, we refine the initial simulation using a
text-to-image (T2I) diffusion model with cross-frame attention, resulting in a
physically plausible video that retains intricate details comparable to the
input image. We conduct comprehensive qualitative and quantitative evaluations
to validate the efficacy of our method. Our project page is available at:
https://supertan0204.github.io/physmotion_website/.",2024-11-26 07:59:11+00:00,"['Xiyang Tan', 'Ying Jiang', 'Xuan Li', 'Zeshun Zong', 'Tianyi Xie', 'Yin Yang', 'Chenfanfu Jiang']",http://arxiv.org/abs/2411.17189v2
Federated Voxel Scene Graph for Intracranial Hemorrhage,"Intracranial Hemorrhage is a potentially lethal condition whose manifestation
is vastly diverse and shifts across clinical centers worldwide.
Deep-learning-based solutions are starting to model complex relations between
brain structures, but still struggle to generalize. While gathering more
diverse data is the most natural approach, privacy regulations often limit the
sharing of medical data. We propose the first application of Federated Scene
Graph Generation. We show that our models can leverage the increased training
data diversity. For Scene Graph Generation, they can recall up to 20% more
clinically relevant relations across datasets compared to models trained on a
single centralized dataset. Learning structured data representation in a
federated setting can open the way to the development of new methods that can
leverage this finer information to regularize across clients more effectively.",2024-11-01 13:37:47+00:00,"['Antoine P. Sanner', 'Jonathan Stieber', 'Nils F. Grauhan', 'Suam Kim', 'Marc A. Brockmann', 'Ahmed E. Othman', 'Anirban Mukhopadhyay']",http://arxiv.org/abs/2411.00578v1
3DArticCyclists: Generating Synthetic Articulated 8D Pose-Controllable Cyclist Data for Computer Vision Applications,"In Autonomous Driving (AD) Perception, cyclists are considered
safety-critical scene objects. Commonly used publicly-available AD datasets
typically contain large amounts of car and vehicle object instances but a low
number of cyclist instances, usually with limited appearance and pose
diversity. This cyclist training data scarcity problem not only limits the
generalization of deep-learning perception models for cyclist semantic
segmentation, pose estimation, and cyclist crossing intention prediction, but
also limits research on new cyclist-related tasks such as fine-grained cyclist
pose estimation and spatio-temporal analysis under complex interactions between
humans and articulated objects. To address this data scarcity problem, in this
paper we propose a framework to generate synthetic dynamic 3D cyclist data
assets that can be used to generate training data for different tasks. In our
framework, we designed a methodology for creating a new part-based multi-view
articulated synthetic 3D bicycle dataset that we call 3DArticBikes that we use
to train a 3D Gaussian Splatting (3DGS)-based reconstruction and image
rendering method. We then propose a parametric bicycle 3DGS composition model
to assemble 8-DoF pose-controllable 3D bicycles. Finally, using dynamic
information from cyclist videos, we build a complete synthetic dynamic 3D
cyclist (rider pedaling a bicycle) by re-posing a selectable synthetic 3D
person, while automatically placing the rider onto one of our new articulated
3D bicycles using a proposed 3D Keypoint optimization-based Inverse Kinematics
pose refinement. We present both, qualitative and quantitative results where we
compare our generated cyclists against those from a recent stable
diffusion-based method.",2024-10-14 17:50:47+00:00,"['Eduardo R. Corral-Soto', 'Yang Liu', 'Tongtong Cao', 'Yuan Ren', 'Liu Bingbing']",http://arxiv.org/abs/2410.10782v2
An Empirical Study of Excitation and Aggregation Design Adaptions in CLIP4Clip for Video-Text Retrieval,"CLIP4Clip model transferred from the CLIP has been the de-factor standard to
solve the video clip retrieval task from frame-level input, triggering the
surge of CLIP4Clip-based models in the video-text retrieval domain. In this
work, we rethink the inherent limitation of widely-used mean pooling operation
in the frame features aggregation and investigate the adaptions of excitation
and aggregation design for discriminative video representation generation. We
present a novel excitationand-aggregation design, including (1) The excitation
module is available for capturing non-mutuallyexclusive relationships among
frame features and achieving frame-wise features recalibration, and (2) The
aggregation module is applied to learn exclusiveness used for frame
representations aggregation. Similarly, we employ the cascade of sequential
module and aggregation design to generate discriminative video representation
in the sequential type. Besides, we adopt the excitation design in the tight
type to obtain representative frame features for multi-modal interaction. The
proposed modules are evaluated on three benchmark datasets of MSR-VTT,
ActivityNet and DiDeMo, achieving MSR-VTT (43.9 R@1), ActivityNet (44.1 R@1)
and DiDeMo (31.0 R@1). They outperform the CLIP4Clip results by +1.2% (+0.5%),
+4.5% (+1.9%) and +9.5% (+2.7%) relative (absolute) improvements, demonstrating
the superiority of our proposed excitation and aggregation designs. We hope our
work will serve as an alternative for frame representations aggregation and
facilitate future research.",2024-05-25 07:45:10+00:00,"['Xiaolun Jing', 'Genke Yang', 'Jian Chu']",http://arxiv.org/abs/2406.01604v2
Video Coding with Cross-Component Sample Offset,"Beyond the exploration of traditional spatial, temporal and subjective visual
signal redundancy in image and video compression, recent research has focused
on leveraging cross-color component redundancy to enhance coding efficiency.
Cross-component coding approaches are motivated by the statistical correlations
among different color components, such as those in the Y'CbCr color space,
where luma (Y) color component typically exhibits finer details than chroma
(Cb/Cr) color components. Inspired by previous cross-component coding
algorithms, this paper introduces a novel in-loop filtering approach named
Cross-Component Sample Offset (CCSO). CCSO utilizes co-located and neighboring
luma samples to generate correction signals for both luma and chroma
reconstructed samples. It is a multiplication-free, non-linear mapping process
implemented using a look-up-table. The input to the mapping is a group of
reconstructed luma samples, and the output is an offset value applied on the
center luma or co-located chroma sample. Experimental results demonstrate that
the proposed CCSO can be applied to both image and video coding, resulting in
improved coding efficiency and visual quality. The method has been adopted into
an experimental next-generation video codec beyond AV1 developed by the
Alliance for Open Media (AOMedia), achieving significant objective coding gains
up to 3.5\,\% and 1.8\,\% for PSNR and VMAF quality metrics, respectively,
under random access configuration. Additionally, CCSO notably improves the
subjective visual quality.",2024-06-03 21:27:56+00:00,"['Han Gao', 'Xin Zhao', 'Tianqi Liu', 'Shan Liu']",http://arxiv.org/abs/2406.01795v1
Hallucination Mitigation Prompts Long-term Video Understanding,"Recently, multimodal large language models have made significant advancements
in video understanding tasks. However, their ability to understand unprocessed
long videos is very limited, primarily due to the difficulty in supporting the
enormous memory overhead. Although existing methods achieve a balance between
memory and information by aggregating frames, they inevitably introduce the
severe hallucination issue. To address this issue, this paper constructs a
comprehensive hallucination mitigation pipeline based on existing MLLMs.
Specifically, we use the CLIP Score to guide the frame sampling process with
questions, selecting key frames relevant to the question. Then, We inject
question information into the queries of the image Q-former to obtain more
important visual features. Finally, during the answer generation stage, we
utilize chain-of-thought and in-context learning techniques to explicitly
control the generation of answers. It is worth mentioning that for the
breakpoint mode, we found that image understanding models achieved better
results than video understanding models. Therefore, we aggregated the answers
from both types of models using a comparison mechanism. Ultimately, We achieved
84.2\% and 62.9\% for the global and breakpoint modes respectively on the
MovieChat dataset, surpassing the official baseline model by 29.1\% and 24.1\%.
Moreover the proposed method won the third place in the CVPR LOVEU 2024
Long-Term Video Question Answering Challenge. The code is avaiable at
https://github.com/lntzm/CVPR24Track-LongVideo",2024-06-17 08:44:03+00:00,"['Yiwei Sun', 'Zhihang Liu', 'Chuanbin Liu', 'Bowei Pu', 'Zhihan Zhang', 'Hongtao Xie']",http://arxiv.org/abs/2406.11333v1
Graph Unfolding and Sampling for Transitory Video Summarization via Gershgorin Disc Alignment,"User-generated videos (UGVs) uploaded from mobile phones to social media
sites like YouTube and TikTok are short and non-repetitive. We summarize a
transitory UGV into several keyframes in linear time via fast graph sampling
based on Gershgorin disc alignment (GDA). Specifically, we first model a
sequence of $N$ frames in a UGV as an $M$-hop path graph $\mathcal{G}^o$ for $M
\ll N$, where the similarity between two frames within $M$ time instants is
encoded as a positive edge based on feature similarity. Towards efficient
sampling, we then ""unfold"" $\mathcal{G}^o$ to a $1$-hop path graph
$\mathcal{G}$, specified by a generalized graph Laplacian matrix $\mathcal{L}$,
via one of two graph unfolding procedures with provable performance bounds. We
show that maximizing the smallest eigenvalue $\lambda_{\min}(\mathbf{B})$ of a
coefficient matrix $\mathbf{B} = \textit{diag}\left(\mathbf{h}\right) + \mu
\mathcal{L}$, where $\mathbf{h}$ is the binary keyframe selection vector, is
equivalent to minimizing a worst-case signal reconstruction error. We maximize
instead the Gershgorin circle theorem (GCT) lower bound
$\lambda^-_{\min}(\mathbf{B})$ by choosing $\mathbf{h}$ via a new fast graph
sampling algorithm that iteratively aligns left-ends of Gershgorin discs for
all graph nodes (frames). Extensive experiments on multiple short video
datasets show that our algorithm achieves comparable or better video
summarization performance compared to state-of-the-art methods, at a
substantially reduced complexity.",2024-08-03 20:08:02+00:00,"['Sadid Sahami', 'Gene Cheung', 'Chia-Wen Lin']",http://arxiv.org/abs/2408.01859v1
VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI,"Recent advancements in Multi-modal Large Language Models (MLLMs) have opened
new avenues for applications in Embodied AI. Building on previous work,
EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating
egocentric video understanding capabilities. To bridge the gap between MLLMs
and low-level control in Embodied AI, we design four key interrelated tasks:
video question-answering, hierarchy planning, visual grounding and reward
modeling. To minimize manual annotation costs, we develop an automatic data
generation pipeline based on the Ego4D dataset, leveraging the prior knowledge
and multimodal capabilities of GPT-4o. Three human annotators then filter the
generated data to ensure diversity and quality, resulting in the VidEgoThink
benchmark. We conduct extensive experiments with three types of models:
API-based MLLMs, open-source image-based MLLMs, and open-source video-based
MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform
poorly across all tasks related to egocentric video understanding. These
findings suggest that foundation models still require significant advancements
to be effectively applied to first-person scenarios in Embodied AI. In
conclusion, VidEgoThink reflects a research trend towards employing MLLMs for
egocentric vision, akin to human capabilities, enabling active observation and
interaction in the complex real-world environments.",2024-10-15 14:08:53+00:00,"['Sijie Cheng', 'Kechen Fang', 'Yangyang Yu', 'Sicheng Zhou', 'Bohao Li', 'Ye Tian', 'Tingguang Li', 'Lei Han', 'Yang Liu']",http://arxiv.org/abs/2410.11623v1
Interpretable Representation Learning from Videos using Nonlinear Priors,"Learning interpretable representations of visual data is an important
challenge, to make machines' decisions understandable to humans and to improve
generalisation outside of the training distribution. To this end, we propose a
deep learning framework where one can specify nonlinear priors for videos (e.g.
of Newtonian physics) that allow the model to learn interpretable latent
variables and use these to generate videos of hypothetical scenarios not
observed at training time. We do this by extending the Variational Auto-Encoder
(VAE) prior from a simple isotropic Gaussian to an arbitrary nonlinear temporal
Additive Noise Model (ANM), which can describe a large number of processes
(e.g. Newtonian physics). We propose a novel linearization method that
constructs a Gaussian Mixture Model (GMM) approximating the prior, and derive a
numerically stable Monte Carlo estimate of the KL divergence between the
posterior and prior GMMs. We validate the method on different real-world
physics videos including a pendulum, a mass on a spring, a falling object and a
pulsar (rotating neutron star). We specify a physical prior for each experiment
and show that the correct variables are learned. Once a model is trained, we
intervene on it to change different physical variables (such as oscillation
amplitude or adding air drag) to generate physically correct videos of
hypothetical scenarios that were not observed previously.",2024-10-24 08:39:24+00:00,"['Marian Longa', 'Joo F. Henriques']",http://arxiv.org/abs/2410.18539v1
Towards a Unified Method for Network Dynamic via Adversarial Weighted Link Prediction,"Network dynamic (e.g., traffic burst in data center networks and channel
fading in cellular WiFi networks) has a great impact on the performance of
communication networks (e.g., throughput, capacity, delay, and jitter). This
article proposes a unified prediction-based method to handle the dynamic of
various network systems. From the view of graph deep learning, I generally
formulate the dynamic prediction of networks as a temporal link prediction task
and analyze the possible challenges of the prediction of weighted networks,
where link weights have the wide-value-range and sparsity issues. Inspired by
the high-resolution video frame prediction with generative adversarial network
(GAN), I try to adopt adversarial learning to generate high-quality predicted
snapshots for network dynamic, which is expected to support the precise and
fine-grained network control. A novel high-quality temporal link prediction
(HQ-TLP) model with GAN is then developed to illustrate the potential of my
basic idea. Extensive experiments for various application scenarios further
demonstrate the powerful capability of HQ-TLP.",2024-01-07 10:30:51+00:00,['Meng Qin'],http://arxiv.org/abs/2401.03444v1
StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image Translation,"Most image-to-image translation models postulate that a unique correspondence
exists between the semantic classes of the source and target domains. However,
this assumption does not always hold in real-world scenarios due to divergent
distributions, different class sets, and asymmetrical information
representation. As conventional GANs attempt to generate images that match the
distribution of the target domain, they may hallucinate spurious instances of
classes absent from the source domain, thereby diminishing the usefulness and
reliability of translated images. CycleGAN-based methods are also known to hide
the mismatched information in the generated images to bypass cycle consistency
objectives, a process known as steganography. In response to the challenge of
non-bijective image translation, we introduce StegoGAN, a novel model that
leverages steganography to prevent spurious features in generated images. Our
approach enhances the semantic consistency of the translated images without
requiring additional postprocessing or supervision. Our experimental
evaluations demonstrate that StegoGAN outperforms existing GAN-based models
across various non-bijective image-to-image translation tasks, both
qualitatively and quantitatively. Our code and pretrained models are accessible
at https://github.com/sian-wusidi/StegoGAN.",2024-03-29 12:23:58+00:00,"['Sidi Wu', 'Yizi Chen', 'Samuel Mermet', 'Lorenz Hurni', 'Konrad Schindler', 'Nicolas Gonthier', 'Loic Landrieu']",http://arxiv.org/abs/2403.20142v1
End-to-end Inception-Unet based Generative Adversarial Networks for Snow and Rain Removals,"The superior performance introduced by deep learning approaches in removing
atmospheric particles such as snow and rain from a single image; favors their
usage over classical ones. However, deep learning-based approaches still suffer
from challenges related to the particle appearance characteristics such as
size, type, and transparency. Furthermore, due to the unique characteristics of
rain and snow particles, single network based deep learning approaches struggle
in handling both degradation scenarios simultaneously. In this paper, a global
framework that consists of two Generative Adversarial Networks (GANs) is
proposed where each handles the removal of each particle individually. The
architectures of both desnowing and deraining GANs introduce the integration of
a feature extraction phase with the classical U-net generator network which in
turn enhances the removal performance in the presence of severe variations in
size and appearance. Furthermore, a realistic dataset that contains pairs of
snowy images next to their groundtruth images estimated using a low-rank
approximation approach; is presented. The experiments show that the proposed
desnowing and deraining approaches achieve significant improvements in
comparison to the state-of-the-art approaches when tested on both synthetic and
realistic datasets.",2024-11-07 15:58:17+00:00,"['Ibrahim Kajo', 'Mohamed Kas', 'Yassine Ruichek']",http://arxiv.org/abs/2411.04821v1
DrivingSphere: Building a High-fidelity 4D World for Closed-loop Simulation,"Autonomous driving evaluation requires simulation environments that closely
replicate actual road conditions, including real-world sensory data and
responsive feedback loops. However, many existing simulations need to predict
waypoints along fixed routes on public datasets or synthetic photorealistic
data, \ie, open-loop simulation usually lacks the ability to assess dynamic
decision-making. While the recent efforts of closed-loop simulation offer
feedback-driven environments, they cannot process visual sensor inputs or
produce outputs that differ from real-world data. To address these challenges,
we propose DrivingSphere, a realistic and closed-loop simulation framework. Its
core idea is to build 4D world representation and generate real-life and
controllable driving scenarios. In specific, our framework includes a Dynamic
Environment Composition module that constructs a detailed 4D driving world with
a format of occupancy equipping with static backgrounds and dynamic objects,
and a Visual Scene Synthesis module that transforms this data into
high-fidelity, multi-view video outputs, ensuring spatial and temporal
consistency. By providing a dynamic and realistic simulation environment,
DrivingSphere enables comprehensive testing and validation of autonomous
driving algorithms, ultimately advancing the development of more reliable
autonomous cars. The benchmark will be publicly released.",2024-11-18 03:00:33+00:00,"['Tianyi Yan', 'Dongming Wu', 'Wencheng Han', 'Junpeng Jiang', 'Xia Zhou', 'Kun Zhan', 'Cheng-zhong Xu', 'Jianbing Shen']",http://arxiv.org/abs/2411.11252v1
Deep Generative Data Assimilation in Multimodal Setting,"Robust integration of physical knowledge and data is key to improve
computational simulations, such as Earth system models. Data assimilation is
crucial for achieving this goal because it provides a systematic framework to
calibrate model outputs with observations, which can include remote sensing
imagery and ground station measurements, with uncertainty quantification.
Conventional methods, including Kalman filters and variational approaches,
inherently rely on simplifying linear and Gaussian assumptions, and can be
computationally expensive. Nevertheless, with the rapid adoption of data-driven
methods in many areas of computational sciences, we see the potential of
emulating traditional data assimilation with deep learning, especially
generative models. In particular, the diffusion-based probabilistic framework
has large overlaps with data assimilation principles: both allows for
conditional generation of samples with a Bayesian inverse framework. These
models have shown remarkable success in text-conditioned image generation or
image-controlled video synthesis. Likewise, one can frame data assimilation as
observation-conditioned state calibration. In this work, we propose SLAMS:
Score-based Latent Assimilation in Multimodal Setting. Specifically, we
assimilate in-situ weather station data and ex-situ satellite imagery to
calibrate the vertical temperature profiles, globally. Through extensive
ablation, we demonstrate that SLAMS is robust even in low-resolution, noisy,
and sparse data settings. To our knowledge, our work is the first to apply deep
generative framework for multimodal data assimilation using real-world
datasets; an important step for building robust computational simulators,
including the next-generation Earth system models. Our code is available at:
https://github.com/yongquan-qu/SLAMS",2024-04-10 00:25:09+00:00,"['Yongquan Qu', 'Juan Nathaniel', 'Shuolin Li', 'Pierre Gentine']",http://arxiv.org/abs/2404.06665v3
CSG: A Context-Semantic Guided Diffusion Approach in De Novo Musculoskeletal Ultrasound Image Generation,"The use of synthetic images in medical imaging Artificial Intelligence (AI)
solutions has been shown to be beneficial in addressing the limited
availability of diverse, unbiased, and representative data. Despite the
extensive use of synthetic image generation methods, controlling the semantics
variability and context details remains challenging, limiting their
effectiveness in producing diverse and representative medical image datasets.
In this work, we introduce a scalable semantic and context-conditioned
generative model, coined CSG (Context-Semantic Guidance). This dual
conditioning approach allows for comprehensive control over both structure and
appearance, advancing the synthesis of realistic and diverse ultrasound images.
We demonstrate the ability of CSG to generate findings (pathological anomalies)
in musculoskeletal (MSK) ultrasound images. Moreover, we test the quality of
the synthetic images using a three-fold validation protocol. The results show
that the synthetic images generated by CSG improve the performance of semantic
segmentation models, exhibit enhanced similarity to real images compared to the
baseline methods, and are undistinguishable from real images according to a
Turing test. Furthermore, we demonstrate an extension of the CSG that allows
enhancing the variability space of images by synthetically generating
augmentations of anatomical geometries and textures.",2024-12-08 06:48:09+00:00,"['Elay Dahan', 'Hedda Cohen Indelman', 'Angeles M. Perez-Agosto', 'Carmit Shiran', 'Gopal Avinash', 'Doron Shaked', 'Nati Daniel']",http://arxiv.org/abs/2412.05833v1
VideoGUI: A Benchmark for GUI Automation from Instructional Videos,"Graphical User Interface (GUI) automation holds significant promise for
enhancing human productivity by assisting with computer tasks. Existing task
formulations primarily focus on simple tasks that can be specified by a single,
language-only instruction, such as ""Insert a new slide."" In this work, we
introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI
assistants on visual-centric GUI tasks. Sourced from high-quality web
instructional videos, our benchmark focuses on tasks involving professional and
novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex
activities (e.g., video editing). VideoGUI evaluates GUI assistants through a
hierarchical process, allowing for identification of the specific levels at
which they may fail: (i) high-level planning: reconstruct procedural subtasks
from visual conditions without language descriptions; (ii) middle-level
planning: generate sequences of precise action narrations based on visual state
(i.e., screenshot) and goals; (iii) atomic action execution: perform specific
actions such as accurately clicking designated elements. For each level, we
design evaluation metrics across individual dimensions to provide clear
signals, such as individual performance in clicking, dragging, typing, and
scrolling for atomic action execution. Our evaluation on VideoGUI reveals that
even the SoTA large multimodal model GPT4o performs poorly on visual-centric
GUI tasks, especially for high-level planning.",2024-06-14 17:59:08+00:00,"['Kevin Qinghong Lin', 'Linjie Li', 'Difei Gao', 'Qinchen WU', 'Mingyi Yan', 'Zhengyuan Yang', 'Lijuan Wang', 'Mike Zheng Shou']",http://arxiv.org/abs/2406.10227v1
Exploration and Improvement of Nerf-based 3D Scene Editing Techniques,"NeRF's high-quality scene synthesis capability was quickly accepted by
scholars in the years after it was proposed, and significant progress has been
made in 3D scene representation and synthesis. However, the high computational
cost limits intuitive and efficient editing of scenes, making NeRF's
development in the scene editing field facing many challenges. This paper
reviews the preliminary explorations of scholars on NeRF in the scene or object
editing field in recent years, mainly changing the shape and texture of scenes
or objects in new synthesized scenes; through the combination of residual
models such as GaN and Transformer with NeRF, the generalization ability of
NeRF scene editing has been further expanded, including realizing real-time new
perspective editing feedback, multimodal editing of text synthesized 3D scenes,
4D synthesis performance, and in-depth exploration in light and shadow editing,
initially achieving optimization of indirect touch editing and detail
representation in complex scenes. Currently, most NeRF editing methods focus on
the touch points and materials of indirect points, but when dealing with more
complex or larger 3D scenes, it is difficult to balance accuracy, breadth,
efficiency, and quality. Overcoming these challenges may become the direction
of future NeRF 3D scene editing technology.",2024-01-23 02:53:06+00:00,"['Shun Fang', 'Ming Cui', 'Xing Feng', 'Yanan Zhang']",http://arxiv.org/abs/2401.12456v1
GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for One-shot Generalizable Neural Radiance Fields,"In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task
which targets synthesizing photo-realistic novel views given only one reference
image per scene. Previous One-shot Generalizable Neural Radiance Fields
(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,
yet suffer the blurry issue due to the encoder-only architecture that highly
relies on the limited reference image. On the other hand, recent
diffusion-based image-to-3d methods show vivid plausible results via distilling
pre-trained 2D diffusion models into a 3D representation, yet require tedious
per-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a
Generative Detail compensation framework via GAN and Diffusion that is both
inference-time finetuning-free and with vivid plausible details. In detail,
following a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a
One-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer
(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model
into the existing OG-NeRF pipeline for primarily relieving the blurry issue
with in-distribution priors captured from the training dataset, achieving a
good balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at
the fine stage, Diff3DE further leverages the pre-trained image diffusion
models to complement rich out-distribution details while maintaining decent 3D
consistency. Extensive experiments on both the synthetic and real-world
datasets show that GD$^2$-NeRF noticeably improves the details while without
per-scene finetuning.",2024-01-01 00:08:39+00:00,"['Xiao Pan', 'Zongxin Yang', 'Shuai Bai', 'Yi Yang']",http://arxiv.org/abs/2401.00616v3
3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations,"Imitation learning provides an efficient way to teach robots dexterous
skills; however, learning complex skills robustly and generalizablely usually
consumes large amounts of human demonstrations. To tackle this challenging
problem, we present 3D Diffusion Policy (DP3), a novel visual imitation
learning approach that incorporates the power of 3D visual representations into
diffusion policies, a class of conditional action generative models. The core
design of DP3 is the utilization of a compact 3D visual representation,
extracted from sparse point clouds with an efficient point encoder. In our
experiments involving 72 simulation tasks, DP3 successfully handles most tasks
with just 10 demonstrations and surpasses baselines with a 24.2% relative
improvement. In 4 real robot tasks, DP3 demonstrates precise control with a
high success rate of 85%, given only 40 demonstrations of each task, and shows
excellent generalization abilities in diverse aspects, including space,
viewpoint, appearance, and instance. Interestingly, in real robot experiments,
DP3 rarely violates safety requirements, in contrast to baseline methods which
frequently do, necessitating human intervention. Our extensive evaluation
highlights the critical importance of 3D representations in real-world robot
learning. Videos, code, and data are available on
https://3d-diffusion-policy.github.io .",2024-03-06 18:58:49+00:00,"['Yanjie Ze', 'Gu Zhang', 'Kangning Zhang', 'Chenyuan Hu', 'Muhan Wang', 'Huazhe Xu']",http://arxiv.org/abs/2403.03954v7
Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis,"While replacing Gaussian decoders with a conditional diffusion model enhances
the perceptual quality of reconstructions in neural image compression, their
lack of inductive bias for image data restricts their ability to achieve
state-of-the-art perceptual levels. To address this limitation, we adopt a
non-isotropic diffusion model at the decoder side. This model imposes an
inductive bias aimed at distinguishing between frequency contents, thereby
facilitating the generation of high-quality images. Moreover, our framework is
equipped with a novel entropy model that accurately models the probability
distribution of latent representation by exploiting spatio-channel correlations
in latent space, while accelerating the entropy decoding step. This
channel-wise entropy model leverages both local and global spatial contexts
within each channel chunk. The global spatial context is built upon the
Transformer, which is specifically designed for image compression tasks. The
designed Transformer employs a Laplacian-shaped positional encoding, the
learnable parameters of which are adaptively adjusted for each channel cluster.
Our experiments demonstrate that our proposed framework yields better
perceptual quality compared to cutting-edge generative-based codecs, and the
proposed entropy model contributes to notable bitrate savings.",2024-03-24 18:33:16+00:00,"['Atefeh Khoshkhahtinat', 'Ali Zafari', 'Piyush M. Mehta', 'Nasser M. Nasrabadi']",http://arxiv.org/abs/2403.16258v1
FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with Conditional Diffusion Model,"The quality of fetal MRI is significantly affected by unpredictable and
substantial fetal motion, leading to the introduction of artifacts even when
fast acquisition sequences are employed. The development of 3D real-time fetal
pose estimation approaches on volumetric EPI fetal MRI opens up a promising
avenue for fetal motion monitoring and prediction. Challenges arise in fetal
pose estimation due to limited number of real scanned fetal MR training images,
hindering model generalization when the acquired fetal MRI lacks adequate pose.
  In this study, we introduce FetalDiffusion, a novel approach utilizing a
conditional diffusion model to generate 3D synthetic fetal MRI with
controllable pose. Additionally, an auxiliary pose-level loss is adopted to
enhance model performance. Our work demonstrates the success of this proposed
model by producing high-quality synthetic fetal MRI images with accurate and
recognizable fetal poses, comparing favorably with in-vivo real fetal MRI.
Furthermore, we show that the integration of synthetic fetal MR images enhances
the fetal pose estimation model's performance, particularly when the number of
available real scanned data is limited resulting in 15.4% increase in PCK and
50.2% reduced in mean error. All experiments are done on a single 32GB V100
GPU. Our method holds promise for improving real-time tracking models, thereby
addressing fetal motion issues more effectively.",2024-03-29 19:58:13+00:00,"['Molin Zhang', 'Polina Golland', 'Patricia Ellen Grant', 'Elfar Adalsteinsson']",http://arxiv.org/abs/2404.00132v1
Enhanced segmentation of femoral bone metastasis in CT scans of patients using synthetic data generation with 3D diffusion models,"Purpose: Bone metastasis have a major impact on the quality of life of
patients and they are diverse in terms of size and location, making their
segmentation complex. Manual segmentation is time-consuming, and expert
segmentations are subject to operator variability, which makes obtaining
accurate and reproducible segmentations of bone metastasis on CT-scans a
challenging yet important task to achieve. Materials and Methods: Deep learning
methods tackle segmentation tasks efficiently but require large datasets along
with expert manual segmentations to generalize on new images. We propose an
automated data synthesis pipeline using 3D Denoising Diffusion Probabilistic
Models (DDPM) to enchance the segmentation of femoral metastasis from CT-scan
volumes of patients. We used 29 existing lesions along with 26 healthy femurs
to create new realistic synthetic metastatic images, and trained a DDPM to
improve the diversity and realism of the simulated volumes. We also
investigated the operator variability on manual segmentation. Results: We
created 5675 new volumes, then trained 3D U-Net segmentation models on real and
synthetic data to compare segmentation performance, and we evaluated the
performance of the models depending on the amount of synthetic data used in
training. Conclusion: Our results showed that segmentation models trained with
synthetic data outperformed those trained on real volumes only, and that those
models perform especially well when considering operator variability.",2024-09-17 09:21:19+00:00,"['Emile Saillard', 'Aurlie Levillain', 'David Mitton', 'Jean-Baptiste Pialat', 'Cyrille Confavreux', 'Hlne Follet', 'Thomas Grenier']",http://arxiv.org/abs/2409.11011v1
Structure-Aware Stylized Image Synthesis for Robust Medical Image Segmentation,"Accurate medical image segmentation is essential for effective diagnosis and
treatment planning but is often challenged by domain shifts caused by
variations in imaging devices, acquisition conditions, and patient-specific
attributes. Traditional domain generalization methods typically require
inclusion of parts of the test domain within the training set, which is not
always feasible in clinical settings with limited diverse data. Additionally,
although diffusion models have demonstrated strong capabilities in image
generation and style transfer, they often fail to preserve the critical
structural information necessary for precise medical analysis. To address these
issues, we propose a novel medical image segmentation method that combines
diffusion models and Structure-Preserving Network for structure-aware one-shot
image stylization. Our approach effectively mitigates domain shifts by
transforming images from various sources into a consistent style while
maintaining the location, size, and shape of lesions. This ensures robust and
accurate segmentation even when the target domain is absent from the training
data. Experimental evaluations on colonoscopy polyp segmentation and skin
lesion segmentation datasets show that our method enhances the robustness and
accuracy of segmentation models, achieving superior performance metrics
compared to baseline models without style transfer. This structure-aware
stylization framework offers a practical solution for improving medical image
segmentation across diverse domains, facilitating more reliable clinical
diagnoses.",2024-12-05 16:15:32+00:00,"['Jie Bao', 'Zhixin Zhou', 'Wen Jung Li', 'Rui Luo']",http://arxiv.org/abs/2412.04296v1
Adversarial Augmentation Training Makes Action Recognition Models More Robust to Realistic Video Distribution Shifts,"Despite recent advances in video action recognition achieving strong
performance on existing benchmarks, these models often lack robustness when
faced with natural distribution shifts between training and test data. We
propose two novel evaluation methods to assess model resilience to such
distribution disparity. One method uses two different datasets collected from
different sources and uses one for training and validation, and the other for
testing. More precisely, we created dataset splits of HMDB-51 or UCF-101 for
training, and Kinetics-400 for testing, using the subset of the classes that
are overlapping in both train and test datasets. The other proposed method
extracts the feature mean of each class from the target evaluation dataset's
training data (i.e. class prototype) and estimates test video prediction as a
cosine similarity score between each sample to the class prototypes of each
target class. This procedure does not alter model weights using the target
dataset and it does not require aligning overlapping classes of two different
datasets, thus is a very efficient method to test the model robustness to
distribution shifts without prior knowledge of the target distribution. We
address the robustness problem by adversarial augmentation training -
generating augmented views of videos that are ""hard"" for the classification
model by applying gradient ascent on the augmentation parameters - as well as
""curriculum"" scheduling the strength of the video augmentations. We
experimentally demonstrate the superior performance of the proposed adversarial
augmentation approach over baselines across three state-of-the-art action
recognition models - TSM, Video Swin Transformer, and Uniformer. The presented
work provides critical insight into model robustness to distribution shifts and
presents effective techniques to enhance video action recognition performance
in a real-world deployment.",2024-01-21 05:50:39+00:00,"['Kiyoon Kim', 'Shreyank N Gowda', 'Panagiotis Eustratiadis', 'Antreas Antoniou', 'Robert B Fisher']",http://arxiv.org/abs/2401.11406v1
Motion meets Attention: Video Motion Prompts,"Videos contain rich spatio-temporal information. Traditional methods for
extracting motion, used in tasks such as action recognition, often rely on
visual contents rather than precise motion features. This phenomenon is
referred to as 'blind motion extraction' behavior, which proves inefficient in
capturing motions of interest due to a lack of motion-guided cues. Recently,
attention mechanisms have enhanced many computer vision tasks by effectively
highlighting salient visual areas. Inspired by this, we propose a modified
Sigmoid function with learnable slope and shift parameters as an attention
mechanism to modulate motion signals from frame differencing maps. This
approach generates a sequence of attention maps that enhance the processing of
motion-related video content. To ensure temporal continuity and smoothness of
the attention maps, we apply pair-wise temporal attention variation
regularization to remove unwanted motions (e.g., noise) while preserving
important ones. We then perform Hadamard product between each pair of attention
maps and the original video frames to highlight the evolving motions of
interest over time. These highlighted motions, termed video motion prompts, are
subsequently used as inputs to the model instead of the original video frames.
We formalize this process as a motion prompt layer and incorporate the
regularization term into the loss function to learn better motion prompts. This
layer serves as an adapter between the model and the video data, bridging the
gap between traditional 'blind motion extraction' and the extraction of
relevant motions of interest. We show that our lightweight, plug-and-play
motion prompt layer seamlessly integrates into models like SlowFast, X3D, and
TimeSformer, enhancing performance on benchmarks such as FineGym and MPII
Cooking 2.",2024-07-03 14:59:46+00:00,"['Qixiang Chen', 'Lei Wang', 'Piotr Koniusz', 'Tom Gedeon']",http://arxiv.org/abs/2407.03179v2
VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation,"Large multimodal models (LMMs) with advanced video analysis capabilities have
recently garnered significant attention. However, most evaluations rely on
traditional methods like multiple-choice questions in benchmarks such as
VideoMME and LongVideoBench, which are prone to lack the depth needed to
capture the complex demands of real-world users. To address this limitation-and
due to the prohibitive cost and slow pace of human annotation for video
tasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS
Chatbot Arena's framework, designed to automatically assess LMMs' video
analysis abilities. VideoAutoArena utilizes user simulation to generate
open-ended, adaptive questions that rigorously assess model performance in
video understanding. The benchmark features an automated, scalable evaluation
framework, incorporating a modified ELO Rating System for fair and continuous
comparisons across multiple LMMs. To validate our automated judging system, we
construct a 'gold standard' using a carefully curated subset of human
annotations, demonstrating that our arena strongly aligns with human judgment
while maintaining scalability. Additionally, we introduce a fault-driven
evolution strategy, progressively increasing question complexity to push models
toward handling more challenging video analysis scenarios. Experimental results
demonstrate that VideoAutoArena effectively differentiates among
state-of-the-art LMMs, providing insights into model strengths and areas for
improvement. To further streamline our evaluation, we introduce VideoAutoBench
as an auxiliary benchmark, where human annotators label winners in a subset of
VideoAutoArena battles. We use GPT-4o as a judge to compare responses against
these human-validated answers. Together, VideoAutoArena and VideoAutoBench
offer a cost-effective, and scalable framework for evaluating LMMs in
user-centric video analysis.",2024-11-20 12:48:34+00:00,"['Ziyang Luo', 'Haoning Wu', 'Dongxu Li', 'Jing Ma', 'Mohan Kankanhalli', 'Junnan Li']",http://arxiv.org/abs/2411.13281v2
Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video Deepfake Dataset,"Deepfakes offer great potential for innovation and creativity, but they also
pose significant risks to privacy, trust, and security. With a vast
Hindi-speaking population, India is particularly vulnerable to deepfake-driven
misinformation campaigns. Fake videos or speeches in Hindi can have an enormous
impact on rural and semi-urban communities, where digital literacy tends to be
lower and people are more inclined to trust video content. The development of
effective frameworks and detection tools to combat deepfake misuse requires
high-quality, diverse, and extensive datasets. The existing popular datasets
like FF-DF (FaceForensics++), and DFDC (DeepFake Detection Challenge) are based
on English language.. Hence, this paper aims to create a first novel Hindi deep
fake dataset, named ``Hindi audio-video-Deepfake'' (HAV-DF). The dataset has
been generated using the faceswap, lipsyn and voice cloning methods. This
multi-step process allows us to create a rich, varied dataset that captures the
nuances of Hindi speech and facial expressions, providing a robust foundation
for training and evaluating deepfake detection models in a Hindi language
context. It is unique of its kind as all of the previous datasets contain
either deepfake videos or synthesized audio. This type of deepfake dataset can
be used for training a detector for both deepfake video and audio datasets.
Notably, the newly introduced HAV-DF dataset demonstrates lower detection
accuracy's across existing detection methods like Headpose, Xception-c40, etc.
Compared to other well-known datasets FF-DF, and DFDC. This trend suggests that
the HAV-DF dataset presents deeper challenges to detect, possibly due to its
focus on Hindi language content and diverse manipulation techniques. The HAV-DF
dataset fills the gap in Hindi-specific deepfake datasets, aiding multilingual
deepfake detection development.",2024-11-23 05:18:43+00:00,"['Sukhandeep Kaur', 'Mubashir Buhari', 'Naman Khandelwal', 'Priyansh Tyagi', 'Kiran Sharma']",http://arxiv.org/abs/2411.15457v1
InstruGen: Automatic Instruction Generation for Vision-and-Language Navigation Via Large Multimodal Models,"Recent research on Vision-and-Language Navigation (VLN) indicates that agents
suffer from poor generalization in unseen environments due to the lack of
realistic training environments and high-quality path-instruction pairs. Most
existing methods for constructing realistic navigation scenes have high costs,
and the extension of instructions mainly relies on predefined templates or
rules, lacking adaptability. To alleviate the issue, we propose InstruGen, a
VLN path-instruction pairs generation paradigm. Specifically, we use YouTube
house tour videos as realistic navigation scenes and leverage the powerful
visual understanding and generation abilities of large multimodal models (LMMs)
to automatically generate diverse and high-quality VLN path-instruction pairs.
Our method generates navigation instructions with different granularities and
achieves fine-grained alignment between instructions and visual observations,
which was difficult to achieve with previous methods. Additionally, we design a
multi-stage verification mechanism to reduce hallucinations and inconsistency
of LMMs. Experimental results demonstrate that agents trained with
path-instruction pairs generated by InstruGen achieves state-of-the-art
performance on the R2R and RxR benchmarks, particularly in unseen environments.
Code is available at https://github.com/yanyu0526/InstruGen.",2024-11-18 09:11:48+00:00,"['Yu Yan', 'Rongtao Xu', 'Jiazhao Zhang', 'Peiyang Li', 'Xiaodan Liang', 'Jianqin Yin']",http://arxiv.org/abs/2411.11394v1
Cross Group Attention and Group-wise Rolling for Multimodal Medical Image Synthesis,"Multimodal MR image synthesis aims to generate missing modality image by
fusing and mapping a few available MRI data. Most existing approaches typically
adopt an image-to-image translation scheme. However, these methods often suffer
from sub-optimal performance due to the spatial misalignment between different
modalities while they are typically treated as input channels. Therefore, in
this paper, we propose an Adaptive Group-wise Interaction Network (AGI-Net)
that explores both inter-modality and intra-modality relationships for
multimodal MR image synthesis. Specifically, groups are first pre-defined along
the channel dimension and then we perform an adaptive rolling for the standard
convolutional kernel to capture inter-modality spatial correspondences. At the
same time, a cross-group attention module is introduced to fuse information
across different channel groups, leading to better feature representation. We
evaluated the effectiveness of our model on the publicly available IXI and
BraTS2023 datasets, where the AGI-Net achieved state-of-the-art performance for
multimodal MR image synthesis. Code will be released.",2024-11-22 02:29:37+00:00,"['Tao Song', 'Yicheng Wu', 'Minhao Hu', 'Xiangde Luo', 'Linda Wei', 'Guotai Wang', 'Yi Guo', 'Feng Xu', 'Shaoting Zhang']",http://arxiv.org/abs/2411.14684v1
Modeling Drivers' Risk Perception via Attention to Improve Driving Assistance,"Advanced Driver Assistance Systems (ADAS) alert drivers during
safety-critical scenarios but often provide superfluous alerts due to a lack of
consideration for drivers' knowledge or scene awareness. Modeling these aspects
together in a data-driven way is challenging due to the scarcity of critical
scenario data with in-cabin driver state and world state recorded together. We
explore the benefits of driver modeling in the context of Forward Collision
Warning (FCW) systems. Working with real-world video dataset of on-road FCW
deployments, we collect observers' subjective validity rating of the deployed
alerts. We also annotate participants' gaze-to-objects and extract 3D
trajectories of the ego vehicle and other vehicles semi-automatically. We
generate a risk estimate of the scene and the drivers' perception in a two step
process: First, we model the movement of vehicles in a given scenario as a
joint trajectory forecasting problem. Then, we reason about the drivers' risk
perception of the scene by counterfactually modifying the input to the
forecasting model to represent the drivers' actual observations of vehicles in
the scene. The difference in these behaviours gives us an estimate of driver
behaviour that accounts for their actual (inattentive) observations and their
downstream effect on overall scene risk. We compare both a learned scene
representation as well as a more traditional ``worse-case'' deceleration model
to achieve the future trajectory forecast. Our experiments show that using this
risk formulation to generate FCW alerts may lead to improved false positive
rate of FCWs and improved FCW timing.",2024-09-07 07:02:06+00:00,"['Abhijat Biswas', 'John Gideon', 'Kimimasa Tamura', 'Guy Rosman']",http://arxiv.org/abs/2409.04738v1
Paired Conditional Generative Adversarial Network for Highly Accelerated Liver 4D MRI,"Purpose: 4D MRI with high spatiotemporal resolution is desired for
image-guided liver radiotherapy. Acquiring densely sampling k-space data is
time-consuming. Accelerated acquisition with sparse samples is desirable but
often causes degraded image quality or long reconstruction time. We propose the
Reconstruct Paired Conditional Generative Adversarial Network (Re-Con-GAN) to
shorten the 4D MRI reconstruction time while maintaining the reconstruction
quality.
  Methods: Patients who underwent free-breathing liver 4D MRI were included in
the study. Fully- and retrospectively under-sampled data at 3, 6 and 10 times
(3x, 6x and 10x) were first reconstructed using the nuFFT algorithm. Re-Con-GAN
then trained input and output in pairs. Three types of networks, ResNet9, UNet
and reconstruction swin transformer, were explored as generators. PatchGAN was
selected as the discriminator. Re-Con-GAN processed the data (3D+t) as temporal
slices (2D+t). A total of 48 patients with 12332 temporal slices were split
into training (37 patients with 10721 slices) and test (11 patients with 1611
slices).
  Results: Re-Con-GAN consistently achieved comparable/better PSNR, SSIM, and
RMSE scores compared to CS/UNet models. The inference time of Re-Con-GAN, UNet
and CS are 0.15s, 0.16s, and 120s. The GTV detection task showed that
Re-Con-GAN and CS, compared to UNet, better improved the dice score (3x
Re-Con-GAN 80.98%; 3x CS 80.74%; 3x UNet 79.88%) of unprocessed under-sampled
images (3x 69.61%).
  Conclusion: A generative network with adversarial training is proposed with
promising and efficient reconstruction results demonstrated on an in-house
dataset. The rapid and qualitative reconstruction of 4D liver MR has the
potential to facilitate online adaptive MR-guided radiotherapy for liver
cancer.",2024-05-20 20:14:23+00:00,"['Di Xu', 'Xin Miao', 'Hengjie Liu', 'Jessica E. Scholey', 'Wensha Yang', 'Mary Feng', 'Michael Ohliger', 'Hui Lin', 'Yi Lao', 'Yang Yang', 'Ke Sheng']",http://arxiv.org/abs/2405.12357v1
GGHead: Fast and Generalizable 3D Gaussian Heads,"Learning 3D head priors from large 2D image collections is an important step
towards high-quality 3D-aware human modeling. A core requirement is an
efficient architecture that scales well to large-scale datasets and large image
resolutions. Unfortunately, existing 3D GANs struggle to scale to generate
samples at high resolutions due to their relatively slow train and render
speeds, and typically have to rely on 2D superresolution networks at the
expense of global 3D consistency. To address these challenges, we propose
Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian
Splatting representation within a 3D GAN framework. To generate a 3D
representation, we employ a powerful 2D CNN generator to predict Gaussian
attributes in the UV space of a template head mesh. This way, GGHead exploits
the regularity of the template's UV layout, substantially facilitating the
challenging task of predicting an unstructured set of 3D Gaussians. We further
improve the geometric fidelity of the generated 3D representations with a novel
total variation loss on rendered UV coordinates. Intuitively, this
regularization encourages that neighboring rendered pixels should stem from
neighboring Gaussians in the template's UV space. Taken together, our pipeline
can efficiently generate 3D heads trained only from single-view 2D image
observations. Our proposed framework matches the quality of existing 3D head
GANs on FFHQ while being both substantially faster and fully 3D consistent. As
a result, we demonstrate real-time generation and rendering of high-quality
3D-consistent heads at $1024^2$ resolution for the first time. Project Website:
https://tobias-kirschstein.github.io/gghead",2024-06-13 17:54:38+00:00,"['Tobias Kirschstein', 'Simon Giebenhain', 'Jiapeng Tang', 'Markos Georgopoulos', 'Matthias Niener']",http://arxiv.org/abs/2406.09377v2
Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation,"Enabling robotic manipulation that generalizes to out-of-distribution scenes
is a crucial step toward open-world embodied intelligence. For human beings,
this ability is rooted in the understanding of semantic correspondence among
objects, which naturally transfers the interaction experience of familiar
objects to novel ones. Although robots lack such a reservoir of interaction
experience, the vast availability of human videos on the Internet may serve as
a valuable resource, from which we extract an affordance memory including the
contact points. Inspired by the natural way humans think, we propose Robo-ABC:
when confronted with unfamiliar objects that require generalization, the robot
can acquire affordance by retrieving objects that share visual or semantic
similarities from the affordance memory. The next step is to map the contact
points of the retrieved objects to the new object. While establishing this
correspondence may present formidable challenges at first glance, recent
research finds it naturally arises from pre-trained diffusion models, enabling
affordance mapping even across disparate object categories. Through the
Robo-ABC framework, robots may generalize to manipulate out-of-category objects
in a zero-shot manner without any manual annotation, additional training, part
segmentation, pre-coded knowledge, or viewpoint restrictions. Quantitatively,
Robo-ABC significantly enhances the accuracy of visual affordance retrieval by
a large margin of 31.6% compared to state-of-the-art (SOTA) end-to-end
affordance models. We also conduct real-world experiments of cross-category
object-grasping tasks. Robo-ABC achieved a success rate of 85.7%, proving its
capacity for real-world tasks.",2024-01-15 06:02:30+00:00,"['Yuanchen Ju', 'Kaizhe Hu', 'Guowei Zhang', 'Gu Zhang', 'Mingrun Jiang', 'Huazhe Xu']",http://arxiv.org/abs/2401.07487v1
Neural Rendering and Its Hardware Acceleration: A Review,"Neural rendering is a new image and video generation method based on deep
learning. It combines the deep learning model with the physical knowledge of
computer graphics, to obtain a controllable and realistic scene model, and
realize the control of scene attributes such as lighting, camera parameters,
posture and so on. On the one hand, neural rendering can not only make full use
of the advantages of deep learning to accelerate the traditional forward
rendering process, but also provide new solutions for specific tasks such as
inverse rendering and 3D reconstruction. On the other hand, the design of
innovative hardware structures that adapt to the neural rendering pipeline
breaks through the parallel computing and power consumption bottleneck of
existing graphics processors, which is expected to provide important support
for future key areas such as virtual and augmented reality, film and television
creation and digital entertainment, artificial intelligence and the metaverse.
In this paper, we review the technical connotation, main challenges, and
research progress of neural rendering. On this basis, we analyze the common
requirements of neural rendering pipeline for hardware acceleration and the
characteristics of the current hardware acceleration architecture, and then
discuss the design challenges of neural rendering processor architecture.
Finally, the future development trend of neural rendering processor
architecture is prospected.",2024-01-06 07:57:11+00:00,"['Xinkai Yan', 'Jieting Xu', 'Yuchi Huo', 'Hujun Bao']",http://arxiv.org/abs/2402.00028v1
SelfDRSC++: Self-Supervised Learning for Dual Reversed Rolling Shutter Correction,"Modern consumer cameras commonly employ the rolling shutter (RS) imaging
mechanism, via which images are captured by scanning scenes row-by-row,
resulting in RS distortion for dynamic scenes. To correct RS distortion,
existing methods adopt a fully supervised learning manner that requires high
framerate global shutter (GS) images as ground-truth for supervision. In this
paper, we propose an enhanced Self-supervised learning framework for Dual
reversed RS distortion Correction (SelfDRSC++). Firstly, we introduce a
lightweight DRSC network that incorporates a bidirectional correlation matching
block to refine the joint optimization of optical flows and corrected RS
features, thereby improving correction performance while reducing network
parameters. Subsequently, to effectively train the DRSC network, we propose a
self-supervised learning strategy that ensures cycle consistency between input
and reconstructed dual reversed RS images. The RS reconstruction in SelfDRSC++
can be interestingly formulated as a specialized instance of video frame
interpolation, where each row in reconstructed RS images is interpolated from
predicted GS images by utilizing RS distortion time maps. By achieving superior
performance while simplifying the training process, SelfDRSC++ enables feasible
one-stage self-supervised training. Additionally, besides start and end RS
scanning time, SelfDRSC++ allows supervision of GS images at arbitrary
intermediate scanning times, thus enabling the learned DRSC network to generate
high framerate GS videos. The code and trained models are available at
\url{https://github.com/shangwei5/SelfDRSC_plusplus}.",2024-08-21 08:17:22+00:00,"['Wei Shang', 'Dongwei Ren', 'Wanying Zhang', 'Qilong Wang', 'Pengfei Zhu', 'Wangmeng Zuo']",http://arxiv.org/abs/2408.11411v1
Navigation World Models,"Navigation is a fundamental skill of agents with visual-motor capabilities.
We introduce a Navigation World Model (NWM), a controllable video generation
model that predicts future visual observations based on past observations and
navigation actions. To capture complex environment dynamics, NWM employs a
Conditional Diffusion Transformer (CDiT), trained on a diverse collection of
egocentric videos of both human and robotic agents, and scaled up to 1 billion
parameters. In familiar environments, NWM can plan navigation trajectories by
simulating them and evaluating whether they achieve the desired goal. Unlike
supervised navigation policies with fixed behavior, NWM can dynamically
incorporate constraints during planning. Experiments demonstrate its
effectiveness in planning trajectories from scratch or by ranking trajectories
sampled from an external policy. Furthermore, NWM leverages its learned visual
priors to imagine trajectories in unfamiliar environments from a single input
image, making it a flexible and powerful tool for next-generation navigation
systems.",2024-12-04 18:59:45+00:00,"['Amir Bar', 'Gaoyue Zhou', 'Danny Tran', 'Trevor Darrell', 'Yann LeCun']",http://arxiv.org/abs/2412.03572v1
DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers,"World model-based searching and planning are widely recognized as a promising
path toward human-level physical intelligence. However, current driving world
models primarily rely on video diffusion models, which specialize in visual
generation but lack the flexibility to incorporate other modalities like
action. In contrast, autoregressive transformers have demonstrated exceptional
capability in modeling multimodal data. Our work aims to unify both driving
model simulation and trajectory planning into a single sequence modeling
problem. We introduce a multimodal driving language based on interleaved image
and action tokens, and develop DrivingGPT to learn joint world modeling and
planning through standard next-token prediction. Our DrivingGPT demonstrates
strong performance in both action-conditioned video generation and end-to-end
planning, outperforming strong baselines on large-scale nuPlan and NAVSIM
benchmarks.",2024-12-24 18:59:37+00:00,"['Yuntao Chen', 'Yuqi Wang', 'Zhaoxiang Zhang']",http://arxiv.org/abs/2412.18607v1
Generating Print-Ready Personalized AI Art Products from Minimal User Inputs,"We present a novel framework to advance generative artificial intelligence
(AI) applications in the realm of printed art products, specifically addressing
large-format products that require high-resolution artworks. The framework
consists of a pipeline that addresses two major challenges in the domain: the
high complexity of generating effective prompts, and the low native resolution
of images produced by diffusion models. By integrating AI-enhanced prompt
generations with AI-powered upscaling techniques, our framework can efficiently
produce high-quality, diverse artistic images suitable for many new commercial
use cases. Our work represents a significant step towards democratizing
high-quality AI art, opening new avenues for consumers, artists, designers, and
businesses.",2024-03-28 18:48:19+00:00,"['Noah Pursell', 'Anindya Maiti']",http://arxiv.org/abs/2405.18247v1
Multi-Scale Texture Loss for CT denoising with GANs,"Generative Adversarial Networks (GANs) have proved as a powerful framework
for denoising applications in medical imaging. However, GAN-based denoising
algorithms still suffer from limitations in capturing complex relationships
within the images. In this regard, the loss function plays a crucial role in
guiding the image generation process, encompassing how much a synthetic image
differs from a real image. To grasp highly complex and non-linear textural
relationships in the training process, this work presents a novel approach to
capture and embed multi-scale texture information into the loss function. Our
method introduces a differentiable multi-scale texture representation of the
images dynamically aggregated by a self-attention layer, thus exploiting
end-to-end gradient-based optimization. We validate our approach by carrying
out extensive experiments in the context of low-dose CT denoising, a
challenging application that aims to enhance the quality of noisy CT scans. We
utilize three publicly available datasets, including one simulated and two real
datasets. The results are promising as compared to other well-established loss
functions, being also consistent across three different GAN architectures. The
code is available at:
https://github.com/TrainLaboratory/MultiScaleTextureLoss-MSTLF",2024-03-25 11:28:52+00:00,"['Francesco Di Feola', 'Lorenzo Tronchin', 'Valerio Guarrasi', 'Paolo Soda']",http://arxiv.org/abs/2403.16640v2
Face to Cartoon Incremental Super-Resolution using Knowledge Distillation,"Facial super-resolution/hallucination is an important area of research that
seeks to enhance low-resolution facial images for a variety of applications.
While Generative Adversarial Networks (GANs) have shown promise in this area,
their ability to adapt to new, unseen data remains a challenge. This paper
addresses this problem by proposing an incremental super-resolution using GANs
with knowledge distillation (ISR-KD) for face to cartoon. Previous research in
this area has not investigated incremental learning, which is critical for
real-world applications where new data is continually being generated. The
proposed ISR-KD aims to develop a novel unified framework for facial
super-resolution that can handle different settings, including different types
of faces such as cartoon face and various levels of detail. To achieve this, a
GAN-based super-resolution network was pre-trained on the CelebA dataset and
then incrementally trained on the iCartoonFace dataset, using knowledge
distillation to retain performance on the CelebA test set while improving the
performance on iCartoonFace test set. Our experiments demonstrate the
effectiveness of knowledge distillation in incrementally adding capability to
the model for cartoon face super-resolution while retaining the learned
knowledge for facial hallucination tasks in GANs.",2024-01-27 10:06:52+00:00,"['Trinetra Devkatte', 'Shiv Ram Dubey', 'Satish Kumar Singh', 'Abdenour Hadid']",http://arxiv.org/abs/2401.15366v1
OED: Towards One-stage End-to-End Dynamic Scene Graph Generation,"Dynamic Scene Graph Generation (DSGG) focuses on identifying visual
relationships within the spatial-temporal domain of videos. Conventional
approaches often employ multi-stage pipelines, which typically consist of
object detection, temporal association, and multi-relation classification.
However, these methods exhibit inherent limitations due to the separation of
multiple stages, and independent optimization of these sub-problems may yield
sub-optimal solutions. To remedy these limitations, we propose a one-stage
end-to-end framework, termed OED, which streamlines the DSGG pipeline. This
framework reformulates the task as a set prediction problem and leverages
pair-wise features to represent each subject-object pair within the scene
graph. Moreover, another challenge of DSGG is capturing temporal dependencies,
we introduce a Progressively Refined Module (PRM) for aggregating temporal
context without the constraints of additional trackers or handcrafted
trajectories, enabling end-to-end optimization of the network. Extensive
experiments conducted on the Action Genome benchmark demonstrate the
effectiveness of our design. The code and models are available at
\url{https://github.com/guanw-pku/OED}.",2024-05-27 08:18:41+00:00,"['Guan Wang', 'Zhimin Li', 'Qingchao Chen', 'Yang Liu']",http://arxiv.org/abs/2405.16925v1
When Diffusion MRI Meets Diffusion Model: A Novel Deep Generative Model for Diffusion MRI Generation,"Diffusion MRI (dMRI) is an advanced imaging technique characterizing tissue
microstructure and white matter structural connectivity of the human brain. The
demand for high-quality dMRI data is growing, driven by the need for better
resolution and improved tissue contrast. However, acquiring high-quality dMRI
data is expensive and time-consuming. In this context, deep generative modeling
emerges as a promising solution to enhance image quality while minimizing
acquisition costs and scanning time. In this study, we propose a novel
generative approach to perform dMRI generation using deep diffusion models. It
can generate high dimension (4D) and high resolution data preserving the
gradients information and brain structure. We demonstrated our method through
an image mapping task aimed at enhancing the quality of dMRI images from 3T to
7T. Our approach demonstrates highly enhanced performance in generating dMRI
images when compared to the current state-of-the-art (SOTA) methods. This
achievement underscores a substantial progression in enhancing dMRI quality,
highlighting the potential of our novel generative approach to revolutionize
dMRI imaging standards.",2024-08-23 08:03:15+00:00,"['Xi Zhu', 'Wei Zhang', 'Yijie Li', ""Lauren J. O'Donnell"", 'Fan Zhang']",http://arxiv.org/abs/2408.12897v1
Open-Vocabulary Spatio-Temporal Action Detection,"Spatio-temporal action detection (STAD) is an important fine-grained video
understanding task. Current methods require box and label supervision for all
action classes in advance. However, in real-world applications, it is very
likely to come across new action classes not seen in training because the
action category space is large and hard to enumerate. Also, the cost of data
annotation and model training for new classes is extremely high for traditional
methods, as we need to perform detailed box annotations and re-train the whole
network from scratch. In this paper, we propose a new challenging setting by
performing open-vocabulary STAD to better mimic the situation of action
detection in an open world. Open-vocabulary spatio-temporal action detection
(OV-STAD) requires training a model on a limited set of base classes with box
and label supervision, which is expected to yield good generalization
performance on novel action classes. For OV-STAD, we build two benchmarks based
on the existing STAD datasets and propose a simple but effective method based
on pretrained video-language models (VLM). To better adapt the holistic VLM for
the fine-grained action detection task, we carefully fine-tune it on the
localized video region-text pairs. This customized fine-tuning endows the VLM
with better motion understanding, thus contributing to a more accurate
alignment between video regions and texts. Local region feature and global
video feature fusion before alignment is adopted to further improve the action
detection performance by providing global context. Our method achieves a
promising performance on novel classes.",2024-05-17 14:52:47+00:00,"['Tao Wu', 'Shuqiu Ge', 'Jie Qin', 'Gangshan Wu', 'Limin Wang']",http://arxiv.org/abs/2405.10832v1
LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living,"Current Large Language Vision Models (LLVMs) trained on web videos perform
well in general video understanding but struggle with fine-grained details,
complex human-object interactions (HOI), and view-invariant representation
learning essential for Activities of Daily Living (ADL). This limitation stems
from a lack of specialized ADL video instruction-tuning datasets and
insufficient modality integration to capture discriminative action
representations. To address this, we propose a semi-automated framework for
curating ADL datasets, creating ADL-X, a multiview, multimodal RGBS
instruction-tuning dataset. Additionally, we introduce LLAVIDAL, an LLVM
integrating videos, 3D skeletons, and HOIs to model ADL's complex
spatiotemporal relationships. For training LLAVIDAL a simple joint alignment of
all modalities yields suboptimal results; thus, we propose a Multimodal
Progressive (MMPro) training strategy, incorporating modalities in stages
following a curriculum. We also establish ADL MCQ and video description
benchmarks to assess LLVM performance in ADL tasks. Trained on ADL-X, LLAVIDAL
achieves state-of-the-art performance across ADL benchmarks. Code and data will
be made publicly available at: https://adl-x.github.io/.",2024-06-13 17:59:05+00:00,"['Dominick Reilly', 'Rajatsubhra Chakraborty', 'Arkaprava Sinha', 'Manish Kumar Govind', 'Pu Wang', 'Francois Bremond', 'Le Xue', 'Srijan Das']",http://arxiv.org/abs/2406.09390v3
HPC: Hierarchical Progressive Coding Framework for Volumetric Video,"Volumetric video based on Neural Radiance Field (NeRF) holds vast potential
for various 3D applications, but its substantial data volume poses significant
challenges for compression and transmission. Current NeRF compression lacks the
flexibility to adjust video quality and bitrate within a single model for
various network and device capacities. To address these issues, we propose HPC,
a novel hierarchical progressive volumetric video coding framework achieving
variable bitrate using a single model. Specifically, HPC introduces a
hierarchical representation with a multi-resolution residual radiance field to
reduce temporal redundancy in long-duration sequences while simultaneously
generating various levels of detail. Then, we propose an end-to-end progressive
learning approach with a multi-rate-distortion loss function to jointly
optimize both hierarchical representation and compression. Our HPC trained only
once can realize multiple compression levels, while the current methods need to
train multiple fixed-bitrate models for different rate-distortion (RD)
tradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality
levels with variable bitrate by a single model and exhibits competitive RD
performance, even outperforming fixed-bitrate models across various datasets.",2024-07-12 06:34:24+00:00,"['Zihan Zheng', 'Houqiang Zhong', 'Qiang Hu', 'Xiaoyun Zhang', 'Li Song', 'Ya Zhang', 'Yanfeng Wang']",http://arxiv.org/abs/2407.09026v2
Zero-shot Action Localization via the Confidence of Large Vision-Language Models,"Precise action localization in untrimmed video is vital for fields such as
professional sports and minimally invasive surgery, where the delineation of
particular motions in recordings can dramatically enhance analysis. But in many
cases, large scale datasets with video-label pairs for localization are
unavailable, limiting the opportunity to fine-tune video-understanding models.
Recent developments in large vision-language models (LVLM) address this need
with impressive zero-shot capabilities in a variety of video understanding
tasks. However, the adaptation of LVLMs, with their powerful visual question
answering capabilities, to zero-shot localization in long-form video is still
relatively unexplored. To this end, we introduce a true Zero-shot Action
Localization method (ZEAL). Specifically, we leverage the built-in action
knowledge of a large language model (LLM) to inflate actions into detailed
descriptions of the archetypal start and end of the action. These descriptions
serve as queries to LVLM for generating frame-level confidence scores which can
be aggregated to produce localization outputs. The simplicity and flexibility
of our method lends it amenable to more capable LVLMs as they are developed,
and we demonstrate remarkable results in zero-shot action localization on a
challenging benchmark, without any training. Our code is publicly available at
$\href{https://github.com/josaklil-ai/zeal}{github.com/josaklil-ai/zeal}$.",2024-10-18 09:51:14+00:00,"['Josiah Aklilu', 'Xiaohan Wang', 'Serena Yeung-Levy']",http://arxiv.org/abs/2410.14340v2
Identity-Driven Multimedia Forgery Detection via Reference Assistance,"Recent advancements in ""deepfake"" techniques have paved the way for
generating various media forgeries. In response to the potential hazards of
these media forgeries, many researchers engage in exploring detection methods,
increasing the demand for high-quality media forgery datasets. Despite this,
existing datasets have certain limitations. Firstly, most datasets focus on
manipulating visual modality and usually lack diversity, as only a few forgery
approaches are considered. Secondly, the quality of media is often inadequate
in clarity and naturalness. Meanwhile, the size of the dataset is also limited.
Thirdly, it is commonly observed that real-world forgeries are motivated by
identity, yet the identity information of the individuals portrayed in these
forgeries within existing datasets remains under-explored. For detection,
identity information could be an essential clue to boost performance. Moreover,
official media concerning relevant identities on the Internet can serve as
prior knowledge, aiding both the audience and forgery detectors in determining
the true identity. Therefore, we propose an identity-driven multimedia forgery
dataset, IDForge, which contains 249,138 video shots sourced from 324 wild
videos of 54 celebrities collected from the Internet. The fake video shots
involve 9 types of manipulation across visual, audio, and textual modalities.
Additionally, IDForge provides extra 214,438 real video shots as a reference
set for the 54 celebrities. Correspondingly, we propose the Reference-assisted
Multimodal Forgery Detection Network (R-MFDN), aiming at the detection of
deepfake videos. Through extensive experiments on the proposed dataset, we
demonstrate the effectiveness of R-MFDN on the multimedia detection task.",2024-01-22 08:59:09+00:00,"['Junhao Xu', 'Jingjing Chen', 'Xue Song', 'Feng Han', 'Haijun Shan', 'Yugang Jiang']",http://arxiv.org/abs/2401.11764v2
Digital Twin-Based Network Management for Better QoE in Multicast Short Video Streaming,"Multicast short video streaming can enhance bandwidth utilization by enabling
simultaneous video transmission to multiple users over shared wireless
channels. The existing network management schemes mainly rely on the sequential
buffering principle and general quality of experience (QoE) model, which may
deteriorate QoE when users' swipe behaviors exhibit distinct spatiotemporal
variation. In this paper, we propose a digital twin (DT)-based network
management scheme to enhance QoE. Firstly, user status emulated by the DT is
utilized to estimate the transmission capabilities and watching probability
distributions of sub-multicast groups (SMGs) for an adaptive segment buffering.
The SMGs' buffers are aligned to the unique virtual buffers managed by the DT
for a fine-grained buffer update. Then, a multicast QoE model consisting of
rebuffering time, video quality, and quality variation is developed, by
considering the mutual influence of segment buffering among SMGs. Finally, a
joint optimization problem of segment version selection and slot division is
formulated to maximize QoE. To efficiently solve the problem, a
data-model-driven algorithm is proposed by integrating a convex optimization
method and a deep reinforcement learning algorithm. Simulation results based on
the real-world dataset demonstrate that the proposed DT-based network
management scheme outperforms benchmark schemes in terms of QoE improvement.",2024-01-23 15:01:49+00:00,"['Xinyu Huang', 'Shisheng Hu', 'Haojun Yang', 'Xinghan Wang', 'Yingying Pei', 'Xuemin Shen']",http://arxiv.org/abs/2401.12826v1
CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion,"Despite impressive advancements in recent multimodal reasoning approaches,
they are still limited in flexibility and efficiency, as these models typically
process only a few fixed modality inputs and require updates to numerous
parameters. This paper tackles these critical challenges and proposes CREMA, a
generalizable, highly efficient, and modular modality-fusion framework that can
incorporate any new modality to enhance video reasoning. We first augment
multiple informative modalities (such as optical flow, 3D point cloud, audio,
thermal heatmap, and touch map) from given videos without extra human
annotation by leveraging sensors or existing pre-trained models. Next, we
introduce a query transformer with multiple parameter-efficient modules
associated with each accessible modality. It projects diverse modality features
to the LLM token embedding space, allowing the model to integrate different
data types for response generation. Furthermore, we propose a novel progressive
multimodal fusion design supported by a lightweight fusion module and
modality-sequential training strategy. It helps compress information across
various assisting modalities, maintaining computational efficiency in the LLM
while improving performance. We validate our method on 7 video-language
reasoning tasks assisted by diverse modalities, including conventional VideoQA
and Video-Audio/3D/Touch/Thermal QA, and achieve better/equivalent performance
against strong multimodal LLMs, including OneLLM, BLIP-2, and SeViLA while
reducing over 90% trainable parameters. We provide extensive analyses of CREMA,
including the impact of each modality on reasoning domains, the design of the
fusion module, and example visualizations.",2024-02-08 18:27:22+00:00,"['Shoubin Yu', 'Jaehong Yoon', 'Mohit Bansal']",http://arxiv.org/abs/2402.05889v4
LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing,"Video creation has become increasingly popular, yet the expertise and effort
required for editing often pose barriers to beginners. In this paper, we
explore the integration of large language models (LLMs) into the video editing
workflow to reduce these barriers. Our design vision is embodied in LAVE, a
novel system that provides LLM-powered agent assistance and language-augmented
editing features. LAVE automatically generates language descriptions for the
user's footage, serving as the foundation for enabling the LLM to process
videos and assist in editing tasks. When the user provides editing objectives,
the agent plans and executes relevant actions to fulfill them. Moreover, LAVE
allows users to edit videos through either the agent or direct UI manipulation,
providing flexibility and enabling manual refinement of agent actions. Our user
study, which included eight participants ranging from novices to proficient
editors, demonstrated LAVE's effectiveness. The results also shed light on user
perceptions of the proposed LLM-assisted editing paradigm and its impact on
users' creativity and sense of co-creation. Based on these findings, we propose
design implications to inform the future development of agent-assisted content
editing.",2024-02-15 19:53:11+00:00,"['Bryan Wang', 'Yuliang Li', 'Zhaoyang Lv', 'Haijun Xia', 'Yan Xu', 'Raj Sodhi']",http://arxiv.org/abs/2402.10294v1
Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection,"Most models for weakly supervised video anomaly detection (WS-VAD) rely on
multiple instance learning, aiming to distinguish normal and abnormal snippets
without specifying the type of anomaly. However, the ambiguous nature of
anomaly definitions across contexts may introduce inaccuracy in discriminating
abnormal and normal events. To show the model what is anomalous, a novel
framework is proposed to guide the learning of suspected anomalies from event
prompts. Given a textual prompt dictionary of potential anomaly events and the
captions generated from anomaly videos, the semantic anomaly similarity between
them could be calculated to identify the suspected events for each video
snippet. It enables a new multi-prompt learning process to constrain the
visual-semantic features across all videos, as well as provides a new way to
label pseudo anomalies for self-training. To demonstrate its effectiveness,
comprehensive experiments and detailed ablation studies are conducted on four
datasets, namely XD-Violence, UCF-Crime, TAD, and ShanghaiTech. Our proposed
model outperforms most state-of-the-art methods in terms of AP or AUC (86.5\%,
\hl{90.4}\%, 94.4\%, and 97.4\%). Furthermore, it shows promising performance
in open-set and cross-dataset cases. The data, code, and models can be found
at: \url{https://github.com/shiwoaz/lap}.",2024-03-02 10:42:47+00:00,"['Chenchen Tao', 'Xiaohao Peng', 'Chong Wang', 'Jiafei Wu', 'Puning Zhao', 'Jun Wang', 'Jiangbo Qian']",http://arxiv.org/abs/2403.01169v2
HDRFlow: Real-Time HDR Video Reconstruction with Large Motions,"Reconstructing High Dynamic Range (HDR) video from image sequences captured
with alternating exposures is challenging, especially in the presence of large
camera or object motion. Existing methods typically align low dynamic range
sequences using optical flow or attention mechanism for deghosting. However,
they often struggle to handle large complex motions and are computationally
expensive. To address these challenges, we propose a robust and efficient flow
estimator tailored for real-time HDR video reconstruction, named HDRFlow.
HDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an
efficient flow network with a multi-size large kernel (MLK), and a new HDR flow
training scheme. The HALoss supervises our flow network to learn an
HDR-oriented flow for accurate alignment in saturated and dark regions. The MLK
can effectively model large motions at a negligible cost. In addition, we
incorporate synthetic data, Sintel, into our training dataset, utilizing both
its provided forward flow and backward flow generated by us to supervise our
flow network, enhancing our performance in large motion regions. Extensive
experiments demonstrate that our HDRFlow outperforms previous methods on
standard benchmarks. To the best of our knowledge, HDRFlow is the first
real-time HDR video reconstruction method for video sequences captured with
alternating exposures, capable of processing 720p resolution inputs at 25ms.",2024-03-06 04:13:29+00:00,"['Gangwei Xu', 'Yujin Wang', 'Jinwei Gu', 'Tianfan Xue', 'Xin Yang']",http://arxiv.org/abs/2403.03447v1
Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning,"Identifying highlight moments of raw video materials is crucial for improving
the efficiency of editing videos that are pervasive on internet platforms.
However, the extensive work of manually labeling footage has created obstacles
to applying supervised methods to videos of unseen categories. The absence of
an audio modality that contains valuable cues for highlight detection in many
videos also makes it difficult to use multimodal strategies. In this paper, we
propose a novel model with cross-modal perception for unsupervised highlight
detection. The proposed model learns representations with visual-audio level
semantics from image-audio pair data via a self-reconstruction task. To achieve
unsupervised highlight detection, we investigate the latent representations of
the network and propose the representation activation sequence learning (RASL)
module with k-point contrastive learning to learn significant representation
activations. To connect the visual modality with the audio modality, we use the
symmetric contrastive learning (SCL) module to learn the paired visual and
audio representations. Furthermore, an auxiliary task of masked feature vector
sequence (FVS) reconstruction is simultaneously conducted during pretraining
for representation enhancement. During inference, the cross-modal pretrained
model can generate representations with paired visual-audio semantics given
only the visual modality. The RASL module is used to output the highlight
scores. The experimental results show that the proposed framework achieves
superior performance compared to other state-of-the-art approaches.",2024-03-14 13:52:03+00:00,"['Tingtian Li', 'Zixun Sun', 'Xinyu Xiao']",http://arxiv.org/abs/2403.09401v3
Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality,"Video paragraph captioning (VPC) involves generating detailed narratives for
long videos, utilizing supportive modalities such as speech and event
boundaries. However, the existing models are constrained by the assumption of
constant availability of a single auxiliary modality, which is impractical
given the diversity and unpredictable nature of real-world scenarios. To this
end, we propose a Missing-Resistant framework MR-VPC that effectively harnesses
all available auxiliary inputs and maintains resilience even in the absence of
certain modalities. Under this framework, we propose the Multimodal VPC (MVPC)
architecture integrating video, speech, and event boundary inputs in a unified
manner to process various auxiliary inputs. Moreover, to fortify the model
against incomplete data, we introduce DropAM, a data augmentation strategy that
randomly omits auxiliary inputs, paired with DistillAM, a regularization target
that distills knowledge from teacher models trained on modality-complete data,
enabling efficient learning in modality-deficient environments. Through
exhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has
proven to deliver superior performance on modality-complete and
modality-missing test data. This work highlights the significance of developing
resilient VPC models and paves the way for more adaptive, robust multimodal
video understanding.",2024-03-28 08:35:46+00:00,"['Sishuo Chen', 'Lei Li', 'Shuhuai Ren', 'Rundong Gao', 'Yuanxin Liu', 'Xiaohan Bi', 'Xu Sun', 'Lu Hou']",http://arxiv.org/abs/2403.19221v1
Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition,"Purpose: In order to produce a surgical gesture recognition system that can
support a wide variety of procedures, either a very large annotated dataset
must be acquired, or fitted models must generalize to new labels (so called
""zero-shot"" capability). In this paper we investigate the feasibility of latter
option. Methods: Leveraging the Bridge-Prompt framework, we prompt-tune a
pre-trained vision-text model (CLIP) for gesture recognition in surgical
videos. This can utilize extensive outside video data such as text, but also
make use of label meta-data and weakly supervised contrastive losses. Results:
Our experiments show that prompt-based video encoder outperforms standard
encoders in surgical gesture recognition tasks. Notably, it displays strong
performance in zero-shot scenarios, where gestures/tasks that were not provided
during the encoder training phase are included in the prediction phase.
Additionally, we measure the benefit of inclusion text descriptions in the
feature extractor training schema. Conclusion Bridge-Prompt and similar
pre-trained+prompt-tuned video encoder models present significant visual
representation for surgical robotics, especially in gesture recognition tasks.
Given the diverse range of surgical tasks (gestures), the ability of these
models to zero-shot transfer without the need for any task (gesture) specific
retraining makes them invaluable.",2024-03-28 19:10:54+00:00,"['Mingxing Rao', 'Yinhong Qin', 'Soheil Kolouri', 'Jie Ying Wu', 'Daniel Moyer']",http://arxiv.org/abs/2403.19786v2
Simultaneous Detection and Interaction Reasoning for Object-Centric Action Recognition,"The interactions between human and objects are important for recognizing
object-centric actions. Existing methods usually adopt a two-stage pipeline,
where object proposals are first detected using a pretrained detector, and then
are fed to an action recognition model for extracting video features and
learning the object relations for action recognition. However, since the action
prior is unknown in the object detection stage, important objects could be
easily overlooked, leading to inferior action recognition performance. In this
paper, we propose an end-to-end object-centric action recognition framework
that simultaneously performs Detection And Interaction Reasoning in one stage.
Particularly, after extracting video features with a base network, we create
three modules for concurrent object detection and interaction reasoning. First,
a Patch-based Object Decoder generates proposals from video patch tokens. Then,
an Interactive Object Refining and Aggregation identifies important objects for
action recognition, adjusts proposal scores based on position and appearance,
and aggregates object-level info into a global video representation. Lastly, an
Object Relation Modeling module encodes object relations. These three modules
together with the video feature extractor can be trained jointly in an
end-to-end fashion, thus avoiding the heavy reliance on an off-the-shelf object
detector, and reducing the multi-stage training burden. We conduct experiments
on two datasets, Something-Else and Ikea-Assembly, to evaluate the performance
of our proposed approach on conventional, compositional, and few-shot action
recognition tasks. Through in-depth experimental analysis, we show the crucial
role of interactive objects in learning for action recognition, and we can
outperform state-of-the-art methods on both datasets.",2024-04-18 05:06:12+00:00,"['Xunsong Li', 'Pengzhan Sun', 'Yangcen Liu', 'Lixin Duan', 'Wen Li']",http://arxiv.org/abs/2404.11903v1
SiNC+: Adaptive Camera-Based Vitals with Unsupervised Learning of Periodic Signals,"Subtle periodic signals, such as blood volume pulse and respiration, can be
extracted from RGB video, enabling noncontact health monitoring at low cost.
Advancements in remote pulse estimation -- or remote photoplethysmography
(rPPG) -- are currently driven by deep learning solutions. However, modern
approaches are trained and evaluated on benchmark datasets with ground truth
from contact-PPG sensors. We present the first non-contrastive unsupervised
learning framework for signal regression to mitigate the need for labelled
video data. With minimal assumptions of periodicity and finite bandwidth, our
approach discovers the blood volume pulse directly from unlabelled videos. We
find that encouraging sparse power spectra within normal physiological
bandlimits and variance over batches of power spectra is sufficient for
learning visual features of periodic signals. We perform the first experiments
utilizing unlabelled video data not specifically created for rPPG to train
robust pulse rate estimators. Given the limited inductive biases, we
successfully applied the same approach to camera-based respiration by changing
the bandlimits of the target signal. This shows that the approach is general
enough for unsupervised learning of bandlimited quasi-periodic signals from
different domains. Furthermore, we show that the framework is effective for
finetuning models on unlabelled video from a single subject, allowing for
personalized and adaptive signal regressors.",2024-04-20 19:17:40+00:00,"['Jeremy Speth', 'Nathan Vance', 'Patrick Flynn', 'Adam Czajka']",http://arxiv.org/abs/2404.13449v1
One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features,"Open-vocabulary Temporal Action Detection (Open-vocab TAD) is an advanced
video analysis approach that expands Closed-vocabulary Temporal Action
Detection (Closed-vocab TAD) capabilities. Closed-vocab TAD is typically
confined to localizing and classifying actions based on a predefined set of
categories. In contrast, Open-vocab TAD goes further and is not limited to
these predefined categories. This is particularly useful in real-world
scenarios where the variety of actions in videos can be vast and not always
predictable. The prevalent methods in Open-vocab TAD typically employ a 2-stage
approach, which involves generating action proposals and then identifying those
actions. However, errors made during the first stage can adversely affect the
subsequent action identification accuracy. Additionally, existing studies face
challenges in handling actions of different durations owing to the use of fixed
temporal processing methods. Therefore, we propose a 1-stage approach
consisting of two primary modules: Multi-scale Video Analysis (MVA) and
Video-Text Alignment (VTA). The MVA module captures actions at varying temporal
resolutions, overcoming the challenge of detecting actions with diverse
durations. The VTA module leverages the synergy between visual and textual
modalities to precisely align video segments with corresponding action labels,
a critical step for accurate action identification in Open-vocab scenarios.
Evaluations on widely recognized datasets THUMOS14 and ActivityNet-1.3, showed
that the proposed method achieved superior results compared to the other
methods in both Open-vocab and Closed-vocab settings. This serves as a strong
demonstration of the effectiveness of the proposed method in the TAD task.",2024-04-30 13:14:28+00:00,"['Trung Thanh Nguyen', 'Yasutomo Kawanishi', 'Takahiro Komamizu', 'Ichiro Ide']",http://arxiv.org/abs/2404.19542v1
In Anticipation of Perfect Deepfake: Identity-anchored Artifact-agnostic Detection under Rebalanced Deepfake Detection Protocol,"As deep generative models advance, we anticipate deepfakes achieving
""perfection""-generating no discernible artifacts or noise. However, current
deepfake detectors, intentionally or inadvertently, rely on such artifacts for
detection, as they are exclusive to deepfakes and absent in genuine examples.
To bridge this gap, we introduce the Rebalanced Deepfake Detection Protocol
(RDDP) to stress-test detectors under balanced scenarios where genuine and
forged examples bear similar artifacts. We offer two RDDP variants:
RDDP-WHITEHAT uses white-hat deepfake algorithms to create 'self-deepfakes,'
genuine portrait videos with the resemblance of the underlying identity, yet
carry similar artifacts to deepfake videos; RDDP-SURROGATE employs surrogate
functions (e.g., Gaussian noise) to process both genuine and forged examples,
introducing equivalent noise, thereby sidestepping the need of deepfake
algorithms.
  Towards detecting perfect deepfake videos that aligns with genuine ones, we
present ID-Miner, a detector that identifies the puppeteer behind the disguise
by focusing on motion over artifacts or appearances. As an identity-based
detector, it authenticates videos by comparing them with reference footage.
Equipped with the artifact-agnostic loss at frame-level and the
identity-anchored loss at video-level, ID-Miner effectively singles out
identity signals amidst distracting variations. Extensive experiments comparing
ID-Miner with 12 baseline detectors under both conventional and RDDP
evaluations with two deepfake datasets, along with additional qualitative
studies, affirm the superiority of our method and the necessity for detectors
designed to counter perfect deepfakes.",2024-05-01 12:48:13+00:00,"['Wei-Han Wang', 'Chin-Yuan Yeh', 'Hsi-Wen Chen', 'De-Nian Yang', 'Ming-Syan Chen']",http://arxiv.org/abs/2405.00483v1
Do Egocentric Video-Language Models Truly Understand Hand-Object Interactions?,"Egocentric video-language pretraining is a crucial step in advancing the
understanding of hand-object interactions in first-person scenarios. Despite
successes on existing testbeds, we find that current EgoVLMs can be easily
misled by simple modifications, such as changing the verbs or nouns in
interaction descriptions, with models struggling to distinguish between these
changes. This raises the question: Do EgoVLMs truly understand hand-object
interactions? To address this question, we introduce a benchmark called
EgoHOIBench, revealing the performance limitation of current egocentric models
when confronted with such challenges. We attribute this performance gap to
insufficient fine-grained supervision and the greater difficulty EgoVLMs
experience in recognizing verbs compared to nouns. To tackle these issues, we
propose a novel asymmetric contrastive objective named EgoNCE++. For the
video-to-text objective, we enhance text supervision by generating negative
captions using large language models or leveraging pretrained vocabulary for
HOI-related word substitutions. For the text-to-video objective, we focus on
preserving an object-centric feature space that clusters video representations
based on shared nouns. Extensive experiments demonstrate that EgoNCE++
significantly enhances EgoHOI understanding, leading to improved performance
across various EgoVLMs in tasks such as multi-instance retrieval, action
recognition, and temporal understanding. Our code is available at
https://github.com/xuboshen/EgoNCEpp.",2024-05-28 00:27:29+00:00,"['Boshen Xu', 'Ziheng Wang', 'Yang Du', 'Zhinan Song', 'Sipeng Zheng', 'Qin Jin']",http://arxiv.org/abs/2405.17719v3
PTM-VQA: Efficient Video Quality Assessment Leveraging Diverse PreTrained Models from the Wild,"Video quality assessment (VQA) is a challenging problem due to the numerous
factors that can affect the perceptual quality of a video, \eg, content
attractiveness, distortion type, motion pattern, and level. However, annotating
the Mean opinion score (MOS) for videos is expensive and time-consuming, which
limits the scale of VQA datasets, and poses a significant obstacle for deep
learning-based methods. In this paper, we propose a VQA method named PTM-VQA,
which leverages PreTrained Models to transfer knowledge from models pretrained
on various pre-tasks, enabling benefits for VQA from different aspects.
  Specifically, we extract features of videos from different pretrained models
with frozen weights and integrate them to generate representation. Since these
models possess various fields of knowledge and are often trained with labels
irrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility
(ICID) loss to impose constraints on features extracted by multiple pretrained
models. The intra-consistency constraint ensures that features extracted by
different pretrained models are in the same unified quality-aware latent space,
while the inter-divisibility introduces pseudo clusters based on the annotation
of samples and tries to separate features of samples from different clusters.
Furthermore, with a constantly growing number of pretrained models, it is
crucial to determine which models to use and how to use them. To address this
problem, we propose an efficient scheme to select suitable candidates. Models
with better clustering performance on VQA datasets are chosen to be our
candidates. Extensive experiments demonstrate the effectiveness of the proposed
method.",2024-05-28 02:37:29+00:00,"['Kun Yuan', 'Hongbo Liu', 'Mading Li', 'Muyi Sun', 'Ming Sun', 'Jiachao Gong', 'Jinhua Hao', 'Chao Zhou', 'Yansong Tang']",http://arxiv.org/abs/2405.17765v1
Differentiable Task Graph Learning: Procedural Activity Representation and Online Mistake Detection from Egocentric Videos,"Procedural activities are sequences of key-steps aimed at achieving specific
goals. They are crucial to build intelligent agents able to assist users
effectively. In this context, task graphs have emerged as a
human-understandable representation of procedural activities, encoding a
partial ordering over the key-steps. While previous works generally relied on
hand-crafted procedures to extract task graphs from videos, in this paper, we
propose an approach based on direct maximum likelihood optimization of edges'
weights, which allows gradient-based learning of task graphs and can be
naturally plugged into neural network architectures. Experiments on the
CaptainCook4D dataset demonstrate the ability of our approach to predict
accurate task graphs from the observation of action sequences, with an
improvement of +16.7% over previous approaches. Owing to the differentiability
of the proposed framework, we also introduce a feature-based approach, aiming
to predict task graphs from key-step textual or video embeddings, for which we
observe emerging video understanding abilities. Task graphs learned with our
approach are also shown to significantly enhance online mistake detection in
procedural egocentric videos, achieving notable gains of +19.8% and +7.5% on
the Assembly101-O and EPIC-Tent-O datasets. Code for replicating experiments is
available at https://github.com/fpv-iplab/Differentiable-Task-Graph-Learning.",2024-06-03 16:11:39+00:00,"['Luigi Seminara', 'Giovanni Maria Farinella', 'Antonino Furnari']",http://arxiv.org/abs/2406.01486v3
L4GM: Large 4D Gaussian Reconstruction Model,"We present L4GM, the first 4D Large Reconstruction Model that produces
animated objects from a single-view video input -- in a single feed-forward
pass that takes only a second. Key to our success is a novel dataset of
multiview videos containing curated, rendered animated objects from Objaverse.
This dataset depicts 44K diverse objects with 110K animations rendered in 48
viewpoints, resulting in 12M videos with a total of 300M frames. We keep our
L4GM simple for scalability and build directly on top of LGM, a pretrained 3D
Large Reconstruction Model that outputs 3D Gaussian ellipsoids from multiview
image input. L4GM outputs a per-frame 3D Gaussian Splatting representation from
video frames sampled at a low fps and then upsamples the representation to a
higher fps to achieve temporal smoothness. We add temporal self-attention
layers to the base LGM to help it learn consistency across time, and utilize a
per-timestep multiview rendering loss to train the model. The representation is
upsampled to a higher framerate by training an interpolation model which
produces intermediate 3D Gaussian representations. We showcase that L4GM that
is only trained on synthetic data generalizes extremely well on in-the-wild
videos, producing high quality animated 3D assets.",2024-06-14 17:51:18+00:00,"['Jiawei Ren', 'Kevin Xie', 'Ashkan Mirzaei', 'Hanxue Liang', 'Xiaohui Zeng', 'Karsten Kreis', 'Ziwei Liu', 'Antonio Torralba', 'Sanja Fidler', 'Seung Wook Kim', 'Huan Ling']",http://arxiv.org/abs/2406.10324v1
Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies,"Large Language Models (LLMs) have demonstrated effectiveness not only in
language tasks but also in video reasoning. This paper introduces a novel
dataset, Tropes in Movies (TiM), designed as a testbed for exploring two
critical yet previously overlooked video reasoning skills: (1) Abstract
Perception: understanding and tokenizing abstract concepts in videos, and (2)
Long-range Compositional Reasoning: planning and integrating intermediate
reasoning steps for understanding long-range videos with numerous frames.
Utilizing tropes from movie storytelling, TiM evaluates the reasoning
capabilities of state-of-the-art LLM-based approaches. Our experiments show
that current methods, including Captioner-Reasoner, Large Multimodal Model
Instruction Fine-tuning, and Visual Programming, only marginally outperform a
random baseline when tackling the challenges of Abstract Perception and
Long-range Compositional Reasoning. To address these deficiencies, we propose
Face-Enhanced Viper of Role Interactions (FEVoRI) and Context Query Reduction
(ConQueR), which enhance Visual Programming by fostering role interaction
awareness and progressively refining movie contexts and trope queries during
reasoning processes, significantly improving performance by 15 F1 points.
However, this performance still lags behind human levels (40 vs. 65 F1).
Additionally, we introduce a new protocol to evaluate the necessity of Abstract
Perception and Long-range Compositional Reasoning for task resolution. This is
done by analyzing the code generated through Visual Programming using an
Abstract Syntax Tree (AST), thereby confirming the increased complexity of TiM.
The dataset and code are available at: https://ander1119.github.io/TiM",2024-06-16 12:58:31+00:00,"['Hung-Ting Su', 'Chun-Tong Chao', 'Ya-Ching Hsu', 'Xudong Lin', 'Yulei Niu', 'Hung-Yi Lee', 'Winston H. Hsu']",http://arxiv.org/abs/2406.10923v1
PrAViC: Probabilistic Adaptation Framework for Real-Time Video Classification,"Video processing is generally divided into two main categories: processing of
the entire video, which typically yields optimal classification outcomes, and
real-time processing, where the objective is to make a decision as promptly as
possible. The latter is often driven by the need to identify rapidly potential
critical or dangerous situations. These could include machine failure, traffic
accidents, heart problems, or dangerous behavior. Although the models dedicated
to the processing of entire videos are typically well-defined and clearly
presented in the literature, this is not the case for online processing, where
a plethora of hand-devised methods exist. To address this, we present \our{}, a
novel, unified, and theoretically-based adaptation framework for dealing with
the online classification problem for video data. The initial phase of our
study is to establish a robust mathematical foundation for the theory of
classification of sequential data, with the potential to make a decision at an
early stage. This allows us to construct a natural function that encourages the
model to return an outcome much faster. The subsequent phase is to demonstrate
a straightforward and readily implementable method for adapting offline models
to online and recurrent operations. Finally, by comparing the proposed approach
to the non-online state-of-the-art baseline, it is demonstrated that the use of
\our{} encourages the network to make earlier classification decisions without
compromising accuracy.",2024-06-17 11:56:15+00:00,"['Magdalena Trdowicz', 'ukasz Struski', 'Marcin Mazur', 'Szymon Janusz', 'Arkadiusz Lewicki', 'Jacek Tabor']",http://arxiv.org/abs/2406.11443v1
GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation,"Referring Video Object Segmentation (RVOS) aims to segment the object
referred to by the query sentence throughout the entire video. Most existing
methods require end-to-end training with dense mask annotations, which could be
computation-consuming and less scalable. In this work, we aim to efficiently
adapt foundation segmentation models for addressing RVOS from weak supervision
with the proposed Grounded Prompting (GroPrompt) framework. More specifically,
we propose Text-Aware Prompt Contrastive Learning (TAP-CL) to enhance the
association between the position prompts and the referring sentences with only
box supervisions, including Text-Contrastive Prompt Learning (TextCon) and
Modality-Contrastive Prompt Learning (ModalCon) at frame level and video level,
respectively. With the proposed TAP-CL, our GroPrompt framework can generate
temporal-consistent yet text-aware position prompts describing locations and
movements for the referred object from the video. The experimental results in
the standard RVOS benchmarks (Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences, and
JHMDB-Sentences) demonstrate the competitive performance of our proposed
GroPrompt framework given only bounding box weak supervisions.",2024-06-18 17:54:17+00:00,"['Ci-Siang Lin', 'I-Jieh Liu', 'Min-Hung Chen', 'Chien-Yi Wang', 'Sifei Liu', 'Yu-Chiang Frank Wang']",http://arxiv.org/abs/2406.12834v2
Unleashing the Potential of Tracklets for Unsupervised Video Person Re-Identification,"With rich temporal-spatial information, video-based person re-identification
methods have shown broad prospects. Although tracklets can be easily obtained
with ready-made tracking models, annotating identities is still expensive and
impractical. Therefore, some video-based methods propose using only a few
identity annotations or camera labels to facilitate feature learning. They also
simply average the frame features of each tracklet, overlooking unexpected
variations and inherent identity consistency within tracklets. In this paper,
we propose the Self-Supervised Refined Clustering (SSR-C) framework without
relying on any annotation or auxiliary information to promote unsupervised
video person re-identification. Specifically, we first propose the
Noise-Filtered Tracklet Partition (NFTP) module to reduce the feature bias of
tracklets caused by noisy tracking results, and sequentially partition the
noise-filtered tracklets into ""sub-tracklets"". Then, we cluster and further
merge sub-tracklets using the self-supervised signal from tracklet partition,
which is enhanced through a progressive strategy to generate reliable pseudo
labels, facilitating intra-class cross-tracklet aggregation. Moreover, we
propose the Class Smoothing Classification (CSC) loss to efficiently promote
model learning. Extensive experiments on the MARS and DukeMTMC-VideoReID
datasets demonstrate that our proposed SSR-C for unsupervised video person
re-identification achieves state-of-the-art results and is comparable to
advanced supervised methods.",2024-06-20 12:30:12+00:00,"['Nanxing Meng', 'Qizao Wang', 'Bin Li', 'Xiangyang Xue']",http://arxiv.org/abs/2406.14261v1
Towards Timely Video Analytics Services at the Network Edge,"Real-time video analytics services aim to provide users with accurate
recognition results timely. However, existing studies usually fall into the
dilemma between reducing delay and improving accuracy. The edge computing
scenario imposes strict transmission and computation resource constraints,
making balancing these conflicting metrics under dynamic network conditions
difficult. In this regard, we introduce the age of processed information (AoPI)
concept, which quantifies the time elapsed since the generation of the latest
accurately recognized frame. AoPI depicts the integrated impact of recognition
accuracy, transmission, and computation efficiency. We derive closed-form
expressions for AoPI under preemptive and non-preemptive computation scheduling
policies w.r.t. the transmission/computation rate and recognition accuracy of
video frames. We then investigate the joint problem of edge server selection,
video configuration adaptation, and bandwidth/computation resource allocation
to minimize the long-term average AoPI over all cameras. We propose an online
method, i.e., Lyapunov-based block coordinate descent (LBCD), to solve the
problem, which decouples the original problem into two subproblems to optimize
the video configuration/resource allocation and edge server selection strategy
separately. We prove that LBCD achieves asymptotically optimal performance.
According to the testbed experiments and simulation results, LBCD reduces the
average AoPI by up to 10.94X compared to state-of-the-art baselines.",2024-06-21 01:45:14+00:00,"['Xishuo Li', 'Shan Zhang', 'Yuejiao Huang', 'Xiao Ma', 'Zhiyuan Wang', 'Hongbin Luo']",http://arxiv.org/abs/2406.14820v1
Long Context Transfer from Language to Vision,"Video sequences offer valuable temporal information, but existing large
multimodal models (LMMs) fall short in understanding extremely long videos.
Many works address this by reducing the number of visual tokens using visual
resamplers. Alternatively, in this paper, we approach this problem from the
perspective of the language model. By simply extrapolating the context length
of the language backbone, we enable LMMs to comprehend orders of magnitude more
visual tokens without any video training. We call this phenomenon long context
transfer and carefully ablate its properties. To effectively measure LMMs'
ability to generalize to long contexts in the vision modality, we develop
V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark
inspired by the language model's NIAH test. Our proposed Long Video Assistant
(LongVA) can process 2000 frames or over 200K visual tokens without additional
complexities. With its extended context length, LongVA achieves
state-of-the-art performance on Video-MME among 7B-scale models by densely
sampling more input frames. Our work is open-sourced at
https://github.com/EvolvingLMMs-Lab/LongVA.",2024-06-24 17:58:06+00:00,"['Peiyuan Zhang', 'Kaichen Zhang', 'Bo Li', 'Guangtao Zeng', 'Jingkang Yang', 'Yuanhan Zhang', 'Ziyue Wang', 'Haoran Tan', 'Chunyuan Li', 'Ziwei Liu']",http://arxiv.org/abs/2406.16852v2
BVI-AOM: A New Training Dataset for Deep Video Compression Optimization,"Deep learning is now playing an important role in enhancing the performance
of conventional hybrid video codecs. These learning-based methods typically
require diverse and representative training material for optimization in order
to achieve model generalization and optimal coding performance. However,
existing datasets either offer limited content variability or come with
restricted licensing terms constraining their use to research purposes only. To
address these issues, we propose a new training dataset, named BVI-AOM, which
contains 956 uncompressed sequences at various resolutions from 270p to 2160p,
covering a wide range of content and texture types. The dataset comes with more
flexible licensing terms and offers competitive performance when used as a
training set for optimizing deep video coding tools. The experimental results
demonstrate that when used as a training set to optimize two popular network
architectures for two different coding tools, the proposed dataset leads to
additional bitrate savings of up to 0.29 and 2.98 percentage points in terms of
PSNR-Y and VMAF, respectively, compared to an existing training dataset,
BVI-DVC, which has been widely used for deep video coding. The BVI-AOM dataset
is available at https://github.com/fan-aaron-zhang/bvi-aom",2024-08-06 15:54:55+00:00,"['Jakub Nawaa', 'Yuxuan Jiang', 'Fan Zhang', 'Xiaoqing Zhu', 'Joel Sole', 'David Bull']",http://arxiv.org/abs/2408.03265v3
SAM 2 in Robotic Surgery: An Empirical Evaluation for Robustness and Generalization in Surgical Video Segmentation,"The recent Segment Anything Model (SAM) 2 has demonstrated remarkable
foundational competence in semantic segmentation, with its memory mechanism and
mask decoder further addressing challenges in video tracking and object
occlusion, thereby achieving superior results in interactive segmentation for
both images and videos. Building upon our previous empirical studies, we
further explore the zero-shot segmentation performance of SAM 2 in
robot-assisted surgery based on prompts, alongside its robustness against
real-world corruption. For static images, we employ two forms of prompts:
1-point and bounding box, while for video sequences, the 1-point prompt is
applied to the initial frame. Through extensive experimentation on the MICCAI
EndoVis 2017 and EndoVis 2018 benchmarks, SAM 2, when utilizing bounding box
prompts, outperforms state-of-the-art (SOTA) methods in comparative
evaluations. The results with point prompts also exhibit a substantial
enhancement over SAM's capabilities, nearing or even surpassing existing
unprompted SOTA methodologies. Besides, SAM 2 demonstrates improved inference
speed and less performance degradation against various image corruption.
Although slightly unsatisfactory results remain in specific edges or regions,
SAM 2's robust adaptability to 1-point prompts underscores its potential for
downstream surgical tasks with limited prompt requirements.",2024-08-08 17:08:57+00:00,"['Jieming Yu', 'An Wang', 'Wenzhen Dong', 'Mengya Xu', 'Mobarakol Islam', 'Jie Wang', 'Long Bai', 'Hongliang Ren']",http://arxiv.org/abs/2408.04593v1
Breast tumor classification based on self-supervised contrastive learning from ultrasound videos,"Background: Breast ultrasound is prominently used in diagnosing breast
tumors. At present, many automatic systems based on deep learning have been
developed to help radiologists in diagnosis. However, training such systems
remains challenging because they are usually data-hungry and demand amounts of
labeled data, which need professional knowledge and are expensive. Methods: We
adopted a triplet network and a self-supervised contrastive learning technique
to learn representations from unlabeled breast ultrasound video clips. We
further designed a new hard triplet loss to to learn representations that
particularly discriminate positive and negative image pairs that are hard to
recognize. We also constructed a pretraining dataset from breast ultrasound
videos (1,360 videos from 200 patients), which includes an anchor sample
dataset with 11,805 images, a positive sample dataset with 188,880 images, and
a negative sample dataset dynamically generated from video clips. Further, we
constructed a finetuning dataset, including 400 images from 66 patients. We
transferred the pretrained network to a downstream benign/malignant
classification task and compared the performance with other state-of-the-art
models, including three models pretrained on ImageNet and a previous
contrastive learning model retrained on our datasets. Results and conclusion:
Experiments revealed that our model achieved an area under the receiver
operating characteristic curve (AUC) of 0.952, which is significantly higher
than the others. Further, we assessed the dependence of our pretrained model on
the number of labeled data and revealed that <100 samples were required to
achieve an AUC of 0.901. The proposed framework greatly reduces the demand for
labeled data and holds potential for use in automatic breast ultrasound image
diagnosis.",2024-08-20 07:16:01+00:00,"['Yunxin Tang', 'Siyuan Tang', 'Jian Zhang', 'Hao Chen']",http://arxiv.org/abs/2408.10600v1
CogVLM2: Visual Language Models for Image and Video Understanding,"Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in
pursuit of enhanced vision-language fusion, efficient higher-resolution
architecture, and broader modalities and applications. Here we propose the
CogVLM2 family, a new generation of visual language models for image and video
understanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image
understanding model, CogVLM2 inherits the visual expert architecture with
improved training recipes in both pre-training and post-training stages,
supporting input resolution up to $1344 \times 1344$ pixels. As a video
understanding model, CogVLM2-Video integrates multi-frame input with timestamps
and proposes automated temporal grounding data construction. Notably, CogVLM2
family has achieved state-of-the-art results on benchmarks like MMBench,
MM-Vet, TextVQA, MVBench and VCGBench. All models are open-sourced in
https://github.com/THUDM/CogVLM2 and https://github.com/THUDM/GLM-4,
contributing to the advancement of the field.",2024-08-29 12:59:12+00:00,"['Wenyi Hong', 'Weihan Wang', 'Ming Ding', 'Wenmeng Yu', 'Qingsong Lv', 'Yan Wang', 'Yean Cheng', 'Shiyu Huang', 'Junhui Ji', 'Zhao Xue', 'Lei Zhao', 'Zhuoyi Yang', 'Xiaotao Gu', 'Xiaohan Zhang', 'Guanyu Feng', 'Da Yin', 'Zihan Wang', 'Ji Qi', 'Xixuan Song', 'Peng Zhang', 'Debing Liu', 'Bin Xu', 'Juanzi Li', 'Yuxiao Dong', 'Jie Tang']",http://arxiv.org/abs/2408.16500v1
LaDTalk: Latent Denoising for Synthesizing Talking Head Videos with High Frequency Details,"Audio-driven talking head generation is a pivotal area within film-making and
Virtual Reality. Although existing methods have made significant strides
following the end-to-end paradigm, they still encounter challenges in producing
videos with high-frequency details due to their limited expressivity in this
domain. This limitation has prompted us to explore an effective post-processing
approach to synthesize photo-realistic talking head videos. Specifically, we
employ a pretrained Wav2Lip model as our foundation model, leveraging its
robust audio-lip alignment capabilities. Drawing on the theory of Lipschitz
Continuity, we have theoretically established the noise robustness of Vector
Quantised Auto Encoders (VQAEs). Our experiments further demonstrate that the
high-frequency texture deficiency of the foundation model can be temporally
consistently recovered by the Space-Optimised Vector Quantised Auto Encoder
(SOVQAE) we introduced, thereby facilitating the creation of realistic talking
head videos. We conduct experiments on both the conventional dataset and the
High-Frequency TalKing head (HFTK) dataset that we curated. The results
indicate that our method, LaDTalk, achieves new state-of-the-art video quality
and out-of-domain lip synchronization performance.",2024-10-01 18:32:02+00:00,"['Jian Yang', 'Xukun Wang', 'Wentao Wang', 'Guoming Li', 'Qihang Fang', 'Ruihong Yuan', 'Tianyang Wang', 'Xiaomei Zhang', 'Yeying Jin', 'Zhaoxin Fan']",http://arxiv.org/abs/2410.00990v2
ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding,"The rise of social media and short-form video (SFV) has facilitated a
breeding ground for misinformation. With the emergence of large language
models, significant research has gone into curbing this misinformation problem
with automatic false claim detection for text. Unfortunately, the automatic
detection of misinformation in SFV is a more complex problem that remains
largely unstudied. While text samples are monomodal (only containing words),
SFVs comprise three different modalities: words, visuals, and non-linguistic
audio. In this work, we introduce Video Masked Autoencoders for Misinformation
Guarding (ViMGuard), the first deep-learning architecture capable of
fact-checking an SFV through analysis of all three of its constituent
modalities. ViMGuard leverages a dual-component system. First, Video and Audio
Masked Autoencoders analyze the visual and non-linguistic audio elements of a
video to discern its intention; specifically whether it intends to make an
informative claim. If it is deemed that the SFV has informative intent, it is
passed through our second component: a Retrieval Augmented Generation system
that validates the factual accuracy of spoken words. In evaluation, ViMGuard
outperformed three cutting-edge fact-checkers, thus setting a new standard for
SFV fact-checking and marking a significant stride toward trustworthy news on
social platforms. To promote further testing and iteration, VimGuard was
deployed into a Chrome extension and all code was open-sourced on GitHub.",2024-10-22 00:30:08+00:00,"['Andrew Kan', 'Christopher Kan', 'Zaid Nabulsi']",http://arxiv.org/abs/2410.16592v1
TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models,"Existing benchmarks often highlight the remarkable performance achieved by
state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal
context for video understanding. However, how well do the models truly perform
visual temporal reasoning? Our study of existing benchmarks shows that this
capability of MFMs is likely overestimated as many questions can be solved by
using a single, few, or out-of-order frames. To systematically examine current
visual temporal reasoning tasks, we propose three principles with corresponding
metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame
Information Disparity. Following these principles, we introduce TOMATO,
Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted to
rigorously assess MFMs' temporal reasoning capabilities in video understanding.
TOMATO comprises 1,484 carefully curated, human-annotated questions spanning
six tasks (i.e., action count, direction, rotation, shape & trend, velocity &
frequency, and visual cues), applied to 1,417 videos, including 805
self-recorded and -generated videos, that encompass human-centric, real-world,
and simulated scenarios. Our comprehensive evaluation reveals a human-model
performance gap of 57.3% with the best-performing model. Moreover, our in-depth
analysis uncovers more fundamental limitations beyond this gap in current MFMs.
While they can accurately recognize events in isolated frames, they fail to
interpret these frames as a continuous sequence. We believe TOMATO will serve
as a crucial testbed for evaluating the next-generation MFMs and as a call to
the community to develop AI systems capable of comprehending human world
dynamics through the video modality.",2024-10-30 17:50:23+00:00,"['Ziyao Shangguan', 'Chuhan Li', 'Yuxuan Ding', 'Yanan Zheng', 'Yilun Zhao', 'Tesca Fitzgerald', 'Arman Cohan']",http://arxiv.org/abs/2410.23266v1
Can Loyalty to Creators Dilute Loyalty to Promoted Products? Examining the Heterogeneous Effects of Live-Streamed Content on Video Game Usage,"Social media platforms have led to online consumption communities, or
fandoms, that involve complex networks of ancillary creators and consumers
focused on some core product or intellectual property. For example, video game
communities include networks of players and content creators centered around a
specific video game. These networks are complex in that video game publishers
often sponsor creators, but creators and publishers may have divergent
incentives. Specifically, creators can potentially benefit from content that
builds their own following at the expense of the core game. Our research
investigates the relationship between consuming live-streamed content and
engagement with a specific video game. We examine the causal effect of viewing
live-streamed content on subsequent gameplay for a specific game, using an
unexpected service interruption of the livestreaming platform and time zone
differences among users. We find live-streamed content significantly increases
gameplay as a 10% increase in live-streamed viewing minutes results in a 3.08%
increase in gameplay minutes. We also explore how this effect varies by user
loyalty to different types of streamer channels (firm-owned, mega, and micro).
The positive effects of live-streamed content are greatest for micro-streamers
and smallest for mega-streamers. These findings are salient for firms
allocating sponsorship resources.",2024-11-02 01:55:58+00:00,"['Wooyong Jo', 'Mike Lewis', 'Yanwen Wang']",http://arxiv.org/abs/2411.01103v1
"A Multicast Scheme for Live Streaming Courses in Large-Scale, Geographically Dense Campus Networks","Video courses have become a significant component of modern education.
However, the increasing demand for live streaming video courses places
considerable strain on the service capabilities of campus networks. The
challenges associated with live streaming course videos in campus network
environments exhibit distinct spatial distribution characteristics. The
audience for specific video courses may be highly concentrated in certain
areas, leading to a large number of users attempting to access the same live
stream simultaneously. Utilizing a Content Delivery Network (CDN) to distribute
videos in these campus scenarios creates substantial unicast pressure on edge
CDN servers. This paper proposes a two-layer dynamic partitioning Recursive Bit
String (RBS) virtual domain network layer multicast architecture specifically
designed for large-scale, geographically dense multicast scenarios within
campus networks. This approach reduces redundant multicast messages by
approximately 10-30\% compared to the two-layer fixed partitioning method.
Additionally, it establishes multicast source authentication capabilities based
on Source Address Validation Improvement (SAVI) and facilitates secure
multicast group key exchange using a concise exchange protocol within the
WebRTC framework. In the next-generation data plane of programmable
software-defined networks, the RBS stateless multicast technology can be
integrated with the unique characteristics of large-scale, geographically dense
campus network scenarios to dynamically and efficiently extend multicast
coverage to every dormitory.",2024-11-10 02:09:17+00:00,"['Senxin Wu', 'Jinlong Hu', 'Ling Zhang']",http://arxiv.org/abs/2411.06334v1
Implementing an Optimized and Secured Multimedia Streaming Protocol in a Participatory Sensing Scenario,"Multimedia streaming protocols are becoming increasingly popular in
Crowdsensing due to their ability to deliver high-quality video content over
the internet in real-time. Streaming multimedia content, as in the context of
live video streaming, requires high bandwidth and large storage capacity to
ensure a sufficient throughput. Crowdsensing can distribute information about
shared video contents among multiple users in network, reducing storage
capacity and computational and bandwidth requirements. However, Crowdsensing
introduces several security constraints that must be taken into account to
ensure the confidentiality, integrity, and availability of the data. In the
specific case of video streaming, commonly named as visual crowdsensing (VCS)
within this context, data is transmitted over wireless networks, making it
vulnerable to security breaches and susceptible to eavesdropping and
interception by attackers. Multimedias often contains sensitive user data and
may be subject to various privacy laws, including data protection laws and laws
related to photography and video recording, based on local GDPR (General Data
Protection Regulation). For this reason the realization of a secure protocol
optimized for a distributed data streaming in real-time becomes increasingly
important in crowdsensing and smart-enviroment context. In this article, we
will discuss the use of a symmetric AES-CTR encryption based protocol for
securing data streaming over a crowd-sensed network.",2024-11-14 07:35:53+00:00,['Andrea Vaiuso'],http://arxiv.org/abs/2411.09252v2
ReWind: Understanding Long Videos with Instructed Learnable Memory,"Vision-Language Models (VLMs) are crucial for applications requiring
integrated understanding textual and visual information. However, existing VLMs
struggle with long videos due to computational inefficiency, memory
limitations, and difficulties in maintaining coherent understanding across
extended sequences. To address these challenges, we introduce ReWind, a novel
memory-based VLM designed for efficient long video understanding while
preserving temporal fidelity. ReWind operates in a two-stage framework. In the
first stage, ReWind maintains a dynamic learnable memory module with a novel
\textbf{read-perceive-write} cycle that stores and updates instruction-relevant
visual information as the video unfolds. This module utilizes learnable queries
and cross-attentions between memory contents and the input stream, ensuring low
memory requirements by scaling linearly with the number of tokens. In the
second stage, we propose an adaptive frame selection mechanism guided by the
memory content to identify instruction-relevant key moments. It enriches the
memory representations with detailed spatial information by selecting a few
high-resolution frames, which are then combined with the memory contents and
fed into a Large Language Model (LLM) to generate the final answer. We
empirically demonstrate ReWind's superior performance in visual question
answering (VQA) and temporal grounding tasks, surpassing previous methods on
long video benchmarks. Notably, ReWind achieves a +13\% score gain and a +12\%
accuracy improvement on the MovieChat-1K VQA dataset and an +8\% mIoU increase
on Charades-STA for temporal grounding.",2024-11-23 13:23:22+00:00,"['Anxhelo Diko', 'Tinghuai Wang', 'Wassim Swaileh', 'Shiyan Sun', 'Ioannis Patras']",http://arxiv.org/abs/2411.15556v1
Stable Mean Teacher for Semi-supervised Video Action Detection,"In this work, we focus on semi-supervised learning for video action
detection. Video action detection requires spatiotemporal localization in
addition to classification, and a limited amount of labels makes the model
prone to unreliable predictions. We present Stable Mean Teacher, a simple
end-to-end teacher-based framework that benefits from improved and temporally
consistent pseudo labels. It relies on a novel Error Recovery (EoR) module,
which learns from students' mistakes on labeled samples and transfers this
knowledge to the teacher to improve pseudo labels for unlabeled samples.
Moreover, existing spatiotemporal losses do not take temporal coherency into
account and are prone to temporal inconsistencies. To address this, we present
Difference of Pixels (DoP), a simple and novel constraint focused on temporal
consistency, leading to coherent temporal detections. We evaluate our approach
on four different spatiotemporal detection benchmarks: UCF101-24, JHMDB21, AVA,
and YouTube-VOS. Our approach outperforms the supervised baselines for action
detection by an average margin of 23.5% on UCF101-24, 16% on JHMDB21, and 3.3%
on AVA. Using merely 10% and 20% of data, it provides competitive performance
compared to the supervised baseline trained on 100% annotations on UCF101-24
and JHMDB21, respectively. We further evaluate its effectiveness on AVA for
scaling to large-scale datasets and YouTube-VOS for video object segmentation,
demonstrating its generalization capability to other tasks in the video domain.
Code and models are publicly available.",2024-12-10 00:25:33+00:00,"['Akash Kumar', 'Sirshapan Mitra', 'Yogesh Singh Rawat']",http://arxiv.org/abs/2412.07072v2
Multimodal Sentiment Analysis based on Video and Audio Inputs,"Despite the abundance of current researches working on the sentiment analysis
from videos and audios, finding the best model that gives the highest accuracy
rate is still considered a challenge for researchers in this field. The main
objective of this paper is to prove the usability of emotion recognition models
that take video and audio inputs. The datasets used to train the models are the
CREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned
models that been used are: Facebook/wav2vec2-large for audio and the
Google/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for
each emotion generated by the two previous models is utilized in the decision
making framework. After disparity in the results, if one of the models gets
much higher accuracy, another test framework is created. The methods used are
the Weighted Average method, the Confidence Level Threshold method, the Dynamic
Weighting Based on Confidence method, and the Rule-Based Logic method. This
limited approach gives encouraging results that make future research into these
methods viable.",2024-12-12 14:42:10+00:00,"['Antonio Fernandez', 'Suzan Awinat']",http://arxiv.org/abs/2412.09317v1
"Query-centric Audio-Visual Cognition Network for Moment Retrieval, Segmentation and Step-Captioning","Video has emerged as a favored multimedia format on the internet. To better
gain video contents, a new topic HIREST is presented, including video
retrieval, moment retrieval, moment segmentation, and step-captioning. The
pioneering work chooses the pre-trained CLIP-based model for video retrieval,
and leverages it as a feature extractor for other three challenging tasks
solved in a multi-task learning paradigm. Nevertheless, this work struggles to
learn the comprehensive cognition of user-preferred content, due to
disregarding the hierarchies and association relations across modalities. In
this paper, guided by the shallow-to-deep principle, we propose a query-centric
audio-visual cognition (QUAG) network to construct a reliable multi-modal
representation for moment retrieval, segmentation and step-captioning.
Specifically, we first design the modality-synergistic perception to obtain
rich audio-visual content, by modeling global contrastive alignment and local
fine-grained interaction between visual and audio modalities. Then, we devise
the query-centric cognition that uses the deep-level query to perform the
temporal-channel filtration on the shallow-level audio-visual representation.
This can cognize user-preferred content and thus attain a query-centric
audio-visual representation for three tasks. Extensive experiments show QUAG
achieves the SOTA results on HIREST. Further, we test QUAG on the query-based
video summarization task and verify its good generalization.",2024-12-18 06:43:06+00:00,"['Yunbin Tu', 'Liang Li', 'Li Su', 'Qingming Huang']",http://arxiv.org/abs/2412.13543v1
CIMGEN: Controlled Image Manipulation by Finetuning Pretrained Generative Models on Limited Data,"Content creation and image editing can benefit from flexible user controls. A
common intermediate representation for conditional image generation is a
semantic map, that has information of objects present in the image. When
compared to raw RGB pixels, the modification of semantic map is much easier.
One can take a semantic map and easily modify the map to selectively insert,
remove, or replace objects in the map. The method proposed in this paper takes
in the modified semantic map and alter the original image in accordance to the
modified map. The method leverages traditional pre-trained image-to-image
translation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a
limited dataset of reference images associated with the semantic maps. We
discuss the qualitative and quantitative performance of our technique to
illustrate its capacity and possible applications in the fields of image
forgery and image editing. We also demonstrate the effectiveness of the
proposed image forgery technique in thwarting the numerous deep learning-based
image forensic techniques, highlighting the urgent need to develop robust and
generalizable image forensic tools in the fight against the spread of fake
media.",2024-01-23 06:30:47+00:00,"['Chandrakanth Gudavalli', 'Erik Rosten', 'Lakshmanan Nataraj', 'Shivkumar Chandrasekaran', 'B. S. Manjunath']",http://arxiv.org/abs/2401.13006v1
PCAC-GAN: A Sparse-Tensor-Based Generative Adversarial Network for 3D Point Cloud Attribute Compression,"Learning-based methods have proven successful in compressing geometric
information for point clouds. For attribute compression, however, they still
lag behind non-learning-based methods such as the MPEG G-PCC standard. To
bridge this gap, we propose a novel deep learning-based point cloud attribute
compression method that uses a generative adversarial network (GAN) with sparse
convolution layers. Our method also includes a module that adaptively selects
the resolution of the voxels used to voxelize the input point cloud. Sparse
vectors are used to represent the voxelized point cloud, and sparse
convolutions process the sparse tensors, ensuring computational efficiency. To
the best of our knowledge, this is the first application of GANs to compress
point cloud attributes. Our experimental results show that our method
outperforms existing learning-based techniques and rivals the latest G-PCC test
model (TMC13v23) in terms of visual quality.",2024-07-08 07:23:03+00:00,"['Xiaolong Mao', 'Hui Yuan', 'Xin Lu', 'Raouf Hamzaoui', 'Wei Gao']",http://arxiv.org/abs/2407.05677v3
A Lightweight GAN-Based Image Fusion Algorithm for Visible and Infrared Images,"This paper presents a lightweight image fusion algorithm specifically
designed for merging visible light and infrared images, with an emphasis on
balancing performance and efficiency. The proposed method enhances the
generator in a Generative Adversarial Network (GAN) by integrating the
Convolutional Block Attention Module (CBAM) to improve feature focus and
utilizing Depthwise Separable Convolution (DSConv) for more efficient
computations. These innovations significantly reduce the model's computational
cost, including the number of parameters and inference latency, while
maintaining or even enhancing the quality of the fused images. Comparative
experiments using the M3FD dataset demonstrate that the proposed algorithm not
only outperforms similar image fusion methods in terms of fusion quality but
also offers a more resource-efficient solution suitable for deployment on
embedded devices. The effectiveness of the lightweight design is validated
through extensive ablation studies, confirming its potential for real-time
applications in complex environments.",2024-09-07 18:04:39+00:00,"['Zhizhong Wu', 'Jiajing Chen', 'LiangHao Tan', 'Hao Gong', 'Zhou Yuru', 'Ge Shi']",http://arxiv.org/abs/2409.15332v1
Enhancing Sentinel-2 Image Resolution: Evaluating Advanced Techniques based on Convolutional and Generative Neural Networks,"This paper investigates the enhancement of spatial resolution in Sentinel-2
bands that contain spectral information using advanced super-resolution
techniques by a factor of 2. State-of-the-art CNN models are compared with
enhanced GAN approaches in terms of quality and feasibility. Therefore, a
representative dataset comprising Sentinel-2 low-resolution images and
corresponding high-resolution aerial orthophotos is required. Literature study
reveals no feasible dataset for the land type of interest (forests), for which
reason an adequate dataset had to be generated in addition, accounting for
accurate alignment and image source optimization. The results reveal that while
CNN-based approaches produce satisfactory outcomes, they tend to yield blurry
images. In contrast, GAN-based models not only provide clear and detailed
images, but also demonstrate superior performance in terms of quantitative
assessment, underlying the potential of the framework beyond the specific land
type investigated.",2024-10-01 08:56:46+00:00,"['Patrick Kramer', 'Alexander Steinhardt', 'Barbara Pedretscher']",http://arxiv.org/abs/2410.00516v1
DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays with Generative Adversarial Networks,"Computed tomography (CT) provides highly detailed three-dimensional (3D)
medical images but is costly, time-consuming, and often inaccessible in
intraoperative settings (Organization et al. 2011). Recent advancements have
explored reconstructing 3D chest volumes from sparse 2D X-rays, such as
single-view or orthogonal double-view images. However, current models tend to
process 2D images in a planar manner, prioritizing visual realism over
structural accuracy. In this work, we introduce DuoLift Generative Adversarial
Networks (DuoLift-GAN), a novel architecture with dual branches that
independently elevate 2D images and their features into 3D representations.
These 3D outputs are merged into a unified 3D feature map and decoded into a
complete 3D chest volume, enabling richer 3D information capture. We also
present a masked loss function that directs reconstruction towards critical
anatomical regions, improving structural accuracy and visual quality. This
paper demonstrates that DuoLift-GAN significantly enhances reconstruction
accuracy while achieving superior visual realism compared to existing methods.",2024-11-12 17:11:18+00:00,"['Zhaoxi Zhang', 'Yueliang Ying']",http://arxiv.org/abs/2411.07941v2
Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation,"Text-driven human motion synthesis is capturing significant attention for its
ability to effortlessly generate intricate movements from abstract text cues,
showcasing its potential for revolutionizing motion design not only in film
narratives but also in virtual reality experiences and computer game
development. Existing methods often rely on 3D motion capture data, which
require special setups resulting in higher costs for data acquisition,
ultimately limiting the diversity and scope of human motion. In contrast, 2D
human videos offer a vast and accessible source of motion data, covering a
wider range of styles and activities. In this paper, we explore leveraging 2D
human motion extracted from videos as an alternative data source to improve
text-driven 3D motion generation. Our approach introduces a novel framework
that disentangles local joint motion from global movements, enabling efficient
learning of local motion priors from 2D data. We first train a single-view 2D
local motion generator on a large dataset of text-motion pairs. To enhance this
model to synthesize 3D motion, we fine-tune the generator with 3D data,
transforming it into a multi-view generator that predicts view-consistent local
joint motion and root dynamics. Experiments on the HumanML3D dataset and novel
text prompts demonstrate that our method efficiently utilizes 2D data,
supporting realistic 3D human motion generation and broadening the range of
motion types it supports. Our code will be made publicly available at
https://zju3dv.github.io/Motion-2-to-3/.",2024-12-17 17:34:52+00:00,"['Huaijin Pi', 'Ruoxi Guo', 'Zehong Shen', 'Qing Shuai', 'Zechen Hu', 'Zhumei Wang', 'Yajiao Dong', 'Ruizhen Hu', 'Taku Komura', 'Sida Peng', 'Xiaowei Zhou']",http://arxiv.org/abs/2412.13111v1
PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging,"Lensless cameras offer significant advantages in size, weight, and cost
compared to traditional lens-based systems. Without a focusing lens, lensless
cameras rely on computational algorithms to recover the scenes from multiplexed
measurements. However, current algorithms struggle with inaccurate forward
imaging models and insufficient priors to reconstruct high-quality images. To
overcome these limitations, we introduce a novel two-stage approach for
consistent and photorealistic lensless image reconstruction. The first stage of
our approach ensures data consistency by focusing on accurately reconstructing
the low-frequency content with a spatially varying deconvolution method that
adjusts to changes in the Point Spread Function (PSF) across the camera's field
of view. The second stage enhances photorealism by incorporating a generative
prior from pre-trained diffusion models. By conditioning on the low-frequency
content retrieved in the first stage, the diffusion model effectively
reconstructs the high-frequency details that are typically lost in the lensless
imaging process, while also maintaining image fidelity. Our method achieves a
superior balance between data fidelity and visual quality compared to existing
methods, as demonstrated with two popular lensless systems, PhlatCam and
DiffuserCam. Project website: https://phocolens.github.io/.",2024-09-26 16:07:24+00:00,"['Xin Cai', 'Zhiyuan You', 'Hailong Zhang', 'Wentao Liu', 'Jinwei Gu', 'Tianfan Xue']",http://arxiv.org/abs/2409.17996v2
HunyuanVideo: A Systematic Framework For Large Video Generative Models,"Recent advancements in video generation have significantly impacted daily
life for both individuals and industries. However, the leading video generation
models remain closed-source, resulting in a notable performance gap between
industry capabilities and those available to the public. In this report, we
introduce HunyuanVideo, an innovative open-source video foundation model that
demonstrates performance in video generation comparable to, or even surpassing,
that of leading closed-source models. HunyuanVideo encompasses a comprehensive
framework that integrates several key elements, including data curation,
advanced architectural design, progressive model scaling and training, and an
efficient infrastructure tailored for large-scale model training and inference.
As a result, we successfully trained a video generative model with over 13
billion parameters, making it the largest among all open-source models. We
conducted extensive experiments and implemented a series of targeted designs to
ensure high visual quality, motion dynamics, text-video alignment, and advanced
filming techniques. According to evaluations by professionals, HunyuanVideo
outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,
and three top-performing Chinese video generative models. By releasing the code
for the foundation model and its applications, we aim to bridge the gap between
closed-source and open-source communities. This initiative will empower
individuals within the community to experiment with their ideas, fostering a
more dynamic and vibrant video generation ecosystem. The code is publicly
available at https://github.com/Tencent/HunyuanVideo.",2024-12-03 23:52:37+00:00,"['Weijie Kong', 'Qi Tian', 'Zijian Zhang', 'Rox Min', 'Zuozhuo Dai', 'Jin Zhou', 'Jiangfeng Xiong', 'Xin Li', 'Bo Wu', 'Jianwei Zhang', 'Kathrina Wu', 'Qin Lin', 'Junkun Yuan', 'Yanxin Long', 'Aladdin Wang', 'Andong Wang', 'Changlin Li', 'Duojun Huang', 'Fang Yang', 'Hao Tan', 'Hongmei Wang', 'Jacob Song', 'Jiawang Bai', 'Jianbing Wu', 'Jinbao Xue', 'Joey Wang', 'Kai Wang', 'Mengyang Liu', 'Pengyu Li', 'Shuai Li', 'Weiyan Wang', 'Wenqing Yu', 'Xinchi Deng', 'Yang Li', 'Yi Chen', 'Yutao Cui', 'Yuanbo Peng', 'Zhentao Yu', 'Zhiyu He', 'Zhiyong Xu', 'Zixiang Zhou', 'Zunnan Xu', 'Yangyu Tao', 'Qinglin Lu', 'Songtao Liu', 'Dax Zhou', 'Hongfa Wang', 'Yong Yang', 'Di Wang', 'Yuhong Liu', 'Jie Jiang', 'Caesar Zhong']",http://arxiv.org/abs/2412.03603v6
Enhancing the Rate-Distortion-Perception Flexibility of Learned Image Codecs with Conditional Diffusion Decoders,"Learned image compression codecs have recently achieved impressive
compression performances surpassing the most efficient image coding
architectures. However, most approaches are trained to minimize rate and
distortion which often leads to unsatisfactory visual results at low bitrates
since perceptual metrics are not taken into account. In this paper, we show
that conditional diffusion models can lead to promising results in the
generative compression task when used as a decoder, and that, given a
compressed representation, they allow creating new tradeoff points between
distortion and perception at the decoder side based on the sampling method.",2024-03-05 11:48:35+00:00,"['Daniele Mari', 'Simone Milani']",http://arxiv.org/abs/2403.02887v1
Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution Using Divergence Measures,"Super-Resolution (SR) is a time-hallowed image processing problem that aims
to improve the quality of a Low-Resolution (LR) sample up to the standard of
its High-Resolution (HR) counterpart. We aim to address this by introducing
Super-Resolution Generator (SuRGe), a fully-convolutional Generative
Adversarial Network (GAN)-based architecture for SR. We show that distinct
convolutional features obtained at increasing depths of a GAN generator can be
optimally combined by a set of learnable convex weights to improve the quality
of generated SR samples. In the process, we employ the Jensen-Shannon and the
Gromov-Wasserstein losses respectively between the SR-HR and LR-SR pairs of
distributions to further aid the generator of SuRGe to better exploit the
available information in an attempt to improve SR. Moreover, we train the
discriminator of SuRGe with the Wasserstein loss with gradient penalty, to
primarily prevent mode collapse. The proposed SuRGe, as an end-to-end GAN
workflow tailor-made for super-resolution, offers improved performance while
maintaining low inference time. The efficacy of SuRGe is substantiated by its
superior performance compared to 18 state-of-the-art contenders on 10 benchmark
datasets.",2024-04-09 13:19:43+00:00,"['Arkaprabha Basu', 'Kushal Bose', 'Sankha Subhra Mullick', 'Anish Chakrabarty', 'Swagatam Das']",http://arxiv.org/abs/2404.06294v1
Adversarial Attack Against Images Classification based on Generative Adversarial Networks,"Adversarial attacks on image classification systems have always been an
important problem in the field of machine learning, and generative adversarial
networks (GANs), as popular models in the field of image generation, have been
widely used in various novel scenarios due to their powerful generative
capabilities. However, with the popularity of generative adversarial networks,
the misuse of fake image technology has raised a series of security problems,
such as malicious tampering with other people's photos and videos, and invasion
of personal privacy. Inspired by the generative adversarial networks, this work
proposes a novel adversarial attack method, aiming to gain insight into the
weaknesses of the image classification system and improve its anti-attack
ability. Specifically, the generative adversarial networks are used to generate
adversarial samples with small perturbations but enough to affect the
decision-making of the classifier, and the adversarial samples are generated
through the adversarial learning of the training generator and the classifier.
From extensive experiment analysis, we evaluate the effectiveness of the method
on a classical image classification dataset, and the results show that our
model successfully deceives a variety of advanced classifiers while maintaining
the naturalness of adversarial samples.",2024-12-21 15:23:34+00:00,['Yahe Yang'],http://arxiv.org/abs/2412.16662v2
DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space,"This paper explores image modeling from the frequency space and introduces
DCTdiff, an end-to-end diffusion generative paradigm that efficiently models
images in the discrete cosine transform (DCT) space. We investigate the design
space of DCTdiff and reveal the key design factors. Experiments on different
frameworks (UViT, DiT), generation tasks, and various diffusion samplers
demonstrate that DCTdiff outperforms pixel-based diffusion models regarding
generative quality and training efficiency. Remarkably, DCTdiff can seamlessly
scale up to high-resolution generation without using the latent diffusion
paradigm. Finally, we illustrate several intriguing properties of DCT image
modeling. For example, we provide a theoretical proof of why `image diffusion
can be seen as spectral autoregression', bridging the gap between diffusion and
autoregressive models. The effectiveness of DCTdiff and the introduced
properties suggest a promising direction for image modeling in the frequency
space. The code is at \url{https://github.com/forever208/DCTdiff}.",2024-12-19 16:44:01+00:00,"['Mang Ning', 'Mingxiao Li', 'Jianlin Su', 'Haozhe Jia', 'Lanmiao Liu', 'Martin Bene', 'Albert Ali Salah', 'Itir Onal Ertugrul']",http://arxiv.org/abs/2412.15032v1
KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems,"Embodied AI agents responsible for executing interconnected, long-sequence
household tasks often face difficulties with in-context memory, leading to
inefficiencies and errors in task execution. To address this issue, we
introduce KARMA, an innovative memory system that integrates long-term and
short-term memory modules, enhancing large language models (LLMs) for planning
in embodied agents through memory-augmented prompting. KARMA distinguishes
between long-term and short-term memory, with long-term memory capturing
comprehensive 3D scene graphs as representations of the environment, while
short-term memory dynamically records changes in objects' positions and states.
This dual-memory structure allows agents to retrieve relevant past scene
experiences, thereby improving the accuracy and efficiency of task planning.
Short-term memory employs strategies for effective and adaptive memory
replacement, ensuring the retention of critical information while discarding
less pertinent data. Compared to state-of-the-art embodied agents enhanced with
memory, our memory-augmented embodied AI agent improves success rates by 1.3x
and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator,
respectively, and enhances task execution efficiency by 3.4x and 62.7x.
Furthermore, we demonstrate that KARMA's plug-and-play capability allows for
seamless deployment on real-world robotic systems, such as mobile manipulation
platforms.Through this plug-and-play memory system, KARMA significantly
enhances the ability of embodied agents to generate coherent and contextually
appropriate plans, making the execution of complex household tasks more
efficient. The experimental videos from the work can be found at
https://youtu.be/4BT7fnw9ehs. Our code is available at
https://github.com/WZX0Swarm0Robotics/KARMA/tree/master.",2024-09-23 11:02:46+00:00,"['Zixuan Wang', 'Bo Yu', 'Junzhe Zhao', 'Wenhao Sun', 'Sai Hou', 'Shuai Liang', 'Xing Hu', 'Yinhe Han', 'Yiming Gan']",http://arxiv.org/abs/2409.14908v2
Interactive Visual Learning for Stable Diffusion,"Diffusion-based generative models' impressive ability to create convincing
images has garnered global attention. However, their complex internal
structures and operations often pose challenges for non-experts to grasp. We
introduce Diffusion Explainer, the first interactive visualization tool
designed to elucidate how Stable Diffusion transforms text prompts into images.
It tightly integrates a visual overview of Stable Diffusion's complex
components with detailed explanations of their underlying operations. This
integration enables users to fluidly transition between multiple levels of
abstraction through animations and interactive elements. Offering real-time
hands-on experience, Diffusion Explainer allows users to adjust Stable
Diffusion's hyperparameters and prompts without the need for installation or
specialized hardware. Accessible via users' web browsers, Diffusion Explainer
is making significant strides in democratizing AI education, fostering broader
public access. More than 7,200 users spanning 113 countries have used our
open-sourced tool at https://poloclub.github.io/diffusion-explainer/. A video
demo is available at https://youtu.be/MbkIADZjPnA.",2024-04-22 23:23:45+00:00,"['Seongmin Lee', 'Benjamin Hoover', 'Hendrik Strobelt', 'Zijie J. Wang', 'ShengYun Peng', 'Austin Wright', 'Kevin Li', 'Haekyu Park', 'Haoyang Yang', 'Polo Chau']",http://arxiv.org/abs/2404.16069v1
GenzIQA: Generalized Image Quality Assessment using Prompt-Guided Latent Diffusion Models,"The design of no-reference (NR) image quality assessment (IQA) algorithms is
extremely important to benchmark and calibrate user experiences in modern
visual systems. A major drawback of state-of-the-art NR-IQA methods is their
limited ability to generalize across diverse IQA settings with reasonable
distribution shifts. Recent text-to-image generative models such as latent
diffusion models generate meaningful visual concepts with fine details related
to text concepts. In this work, we leverage the denoising process of such
diffusion models for generalized IQA by understanding the degree of alignment
between learnable quality-aware text prompts and images. In particular, we
learn cross-attention maps from intermediate layers of the denoiser of latent
diffusion models to capture quality-aware representations of images. In
addition, we also introduce learnable quality-aware text prompts that enable
the cross-attention features to be better quality-aware. Our extensive cross
database experiments across various user-generated, synthetic, and low-light
content-based benchmarking databases show that latent diffusion models can
achieve superior generalization in IQA when compared to other methods in the
literature.",2024-06-07 05:46:39+00:00,"['Diptanu De', 'Shankhanil Mitra', 'Rajiv Soundararajan']",http://arxiv.org/abs/2406.04654v1
Text2Data: Low-Resource Data Generation with Textual Control,"Natural language serves as a common and straightforward signal for humans to
interact seamlessly with machines. Recognizing the importance of this
interface, the machine learning community is investing considerable effort in
generating data that is semantically coherent with textual instructions. While
strides have been made in text-to-data generation spanning image editing, audio
synthesis, video creation, and beyond, low-resource areas characterized by
expensive annotations or complex data structures, such as molecules, motion
dynamics, and time series, often lack textual labels. This deficiency impedes
supervised learning, thereby constraining the application of advanced
generative models for text-to-data tasks. In response to these challenges in
the low-resource scenario, we propose Text2Data, a novel approach that utilizes
unlabeled data to understand the underlying data distribution through an
unsupervised diffusion model. Subsequently, it undergoes controllable
finetuning via a novel constraint optimization-based learning objective that
ensures controllability and effectively counteracts catastrophic forgetting.
Comprehensive experiments demonstrate that Text2Data is able to achieve
enhanced performance regarding controllability across various modalities,
including molecules, motions and time series, when compared to existing
baselines.",2024-02-08 03:41:39+00:00,"['Shiyu Wang', 'Yihao Feng', 'Tian Lan', 'Ning Yu', 'Yu Bai', 'Ran Xu', 'Huan Wang', 'Caiming Xiong', 'Silvio Savarese']",http://arxiv.org/abs/2402.10941v2
TPC: Test-time Procrustes Calibration for Diffusion-based Human Image Animation,"Human image animation aims to generate a human motion video from the inputs
of a reference human image and a target motion video. Current diffusion-based
image animation systems exhibit high precision in transferring human identity
into targeted motion, yet they still exhibit irregular quality in their
outputs. Their optimal precision is achieved only when the physical
compositions (i.e., scale and rotation) of the human shapes in the reference
image and target pose frame are aligned. In the absence of such alignment,
there is a noticeable decline in fidelity and consistency. Especially, in
real-world environments, this compositional misalignment commonly occurs,
posing significant challenges to the practical usage of current systems. To
this end, we propose Test-time Procrustes Calibration (TPC), which enhances the
robustness of diffusion-based image animation systems by maintaining optimal
performance even when faced with compositional misalignment, effectively
addressing real-world scenarios. The TPC provides a calibrated reference image
for the diffusion model, enhancing its capability to understand the
correspondence between human shapes in the reference and target images. Our
method is simple and can be applied to any diffusion-based image animation
system in a model-agnostic manner, improving the effectiveness at test time
without additional training.",2024-10-31 15:34:49+00:00,"['Sunjae Yoon', 'Gwanhyeong Koo', 'Younghwan Lee', 'Chang D. Yoo']",http://arxiv.org/abs/2410.24037v1
Multi-Robot Motion Planning with Diffusion Models,"Diffusion models have recently been successfully applied to a wide range of
robotics applications for learning complex multi-modal behaviors from data.
However, prior works have mostly been confined to single-robot and small-scale
environments due to the high sample complexity of learning multi-robot
diffusion models. In this paper, we propose a method for generating
collision-free multi-robot trajectories that conform to underlying data
distributions while using only single-robot data. Our algorithm, Multi-robot
Multi-model planning Diffusion (MMD), does so by combining learned diffusion
models with classical search-based techniques -- generating data-driven motions
under collision constraints. Scaling further, we show how to compose multiple
diffusion models to plan in large environments where a single diffusion model
fails to generalize well. We demonstrate the effectiveness of our approach in
planning for dozens of robots in a variety of simulated scenarios motivated by
logistics environments. View video demonstrations in our supplementary
material, and our code at: https://github.com/yoraish/mmd.",2024-10-04 01:31:13+00:00,"['Yorai Shaoul', 'Itamar Mishani', 'Shivam Vats', 'Jiaoyang Li', 'Maxim Likhachev']",http://arxiv.org/abs/2410.03072v1
Mozart's Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models,"In recent years, AI-Generated Content (AIGC) has witnessed rapid
advancements, facilitating the creation of music, images, and other artistic
forms across a wide range of industries. However, current models for image- and
video-to-music synthesis struggle to capture the nuanced emotions and
atmosphere conveyed by visual content. To fill this gap, we propose Mozart's
Touch, a multi-modal music generation framework capable of generating music
aligned with cross-modal inputs such as images, videos, and text. The framework
consists of three key components: Multi-modal Captioning Module, Large Language
Model (LLM) understanding \& Bridging Module, and Music Generation Module.
Unlike traditional end-to-end methods, Mozart's Touch uses LLMs to accurately
interpret visual elements without requiring the training or fine-tuning of
music generation models, providing efficiency and transparency through clear,
interpretable prompts. We also introduce the ""LLM-Bridge"" method to resolve the
heterogeneous representation challenges between descriptive texts from
different modalities. Through a series of objective and subjective evaluations,
we demonstrate that Mozart's Touch outperforms current state-of-the-art models.
Our code and examples are available at
https://github.com/TiffanyBlews/MozartsTouch.",2024-05-05 03:15:52+00:00,"['Jiajun Li', 'Tianze Xu', 'Xuesong Chen', 'Xinrui Yao', 'Shuchang Liu']",http://arxiv.org/abs/2405.02801v3
Revisiting registration-based synthesis: A focus on unsupervised MR image synthesis,"Deep learning (DL) has led to significant improvements in medical image
synthesis, enabling advanced image-to-image translation to generate synthetic
images. However, DL methods face challenges such as domain shift and high
demands for training data, limiting their generalizability and applicability.
Historically, image synthesis was also carried out using deformable image
registration (DIR), a method that warps moving images of a desired modality to
match the anatomy of a fixed image. However, concerns about its speed and
accuracy led to its decline in popularity. With the recent advances of DL-based
DIR, we now revisit and reinvigorate this line of research. In this paper, we
propose a fast and accurate synthesis method based on DIR. We use the task of
synthesizing a rare magnetic resonance (MR) sequence, white matter nulled (WMn)
T1-weighted (T1-w) images, to demonstrate the potential of our approach. During
training, our method learns a DIR model based on the widely available MPRAGE
sequence, which is a cerebrospinal fluid nulled (CSFn) T1-w inversion recovery
gradient echo pulse sequence. During testing, the trained DIR model is first
applied to estimate the deformation between moving and fixed CSFn images.
Subsequently, this estimated deformation is applied to align the paired WMn
counterpart of the moving CSFn image, yielding a synthetic WMn image for the
fixed CSFn image. Our experiments demonstrate promising results for
unsupervised image synthesis using DIR. These findings highlight the potential
of our technique in contexts where supervised synthesis methods are constrained
by limited training data.",2024-02-19 17:00:54+00:00,"['Savannah P. Hays', 'Lianrui Zuo', 'Yihao Liu', 'Anqi Feng', 'Jiachen Zhuo', 'Jerry L. Prince', 'Aaron Carass']",http://arxiv.org/abs/2402.12288v1
COMBO: Compositional World Models for Embodied Multi-Agent Cooperation,"In this paper, we investigate the problem of embodied multi-agent
cooperation, where decentralized agents must cooperate given only egocentric
views of the world. To effectively plan in this setting, in contrast to
learning world dynamics in a single-agent scenario, we must simulate world
dynamics conditioned on an arbitrary number of agents' actions given only
partial egocentric visual observations of the world. To address this issue of
partial observability, we first train generative models to estimate the overall
world state given partial egocentric observations. To enable accurate
simulation of multiple sets of actions on this world state, we then propose to
learn a compositional world model for multi-agent cooperation by factorizing
the naturally composable joint actions of multiple agents and compositionally
generating the video conditioned on the world state. By leveraging this
compositional world model, in combination with Vision Language Models to infer
the actions of other agents, we can use a tree search procedure to integrate
these modules and facilitate online cooperative planning. We evaluate our
methods on three challenging benchmarks with 2-4 agents. The results show our
compositional world model is effective and the framework enables the embodied
agents to cooperate efficiently with different agents across various tasks and
an arbitrary number of agents, showing the promising future of our proposed
methods. More videos can be found at https://embodied-agi.cs.umass.edu/combo/.",2024-04-16 17:59:11+00:00,"['Hongxin Zhang', 'Zeyuan Wang', 'Qiushi Lyu', 'Zheyuan Zhang', 'Sunli Chen', 'Tianmin Shu', 'Behzad Dariush', 'Kwonjoon Lee', 'Yilun Du', 'Chuang Gan']",http://arxiv.org/abs/2404.10775v2
"Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot Motion Transfer","Given the remarkable results of motion synthesis with diffusion models, a
natural question arises: how can we effectively leverage these models for
motion editing? Existing diffusion-based motion editing methods overlook the
profound potential of the prior embedded within the weights of pre-trained
models, which enables manipulating the latent feature space; hence, they
primarily center on handling the motion space. In this work, we explore the
attention mechanism of pre-trained motion diffusion models. We uncover the
roles and interactions of attention elements in capturing and representing
intricate human motion patterns, and carefully integrate these elements to
transfer a leader motion to a follower one while maintaining the nuanced
characteristics of the follower, resulting in zero-shot motion transfer.
Editing features associated with selected motions allows us to confront a
challenge observed in prior motion diffusion approaches, which use general
directives (e.g., text, music) for editing, ultimately failing to convey subtle
nuances effectively. Our work is inspired by how a monkey closely imitates what
it sees while maintaining its unique motion patterns; hence we call it Monkey
See, Monkey Do, and dub it MoMo. Employing our technique enables accomplishing
tasks such as synthesizing out-of-distribution motions, style transfer, and
spatial editing. Furthermore, diffusion inversion is seldom employed for
motions; as a result, editing efforts focus on generated motions, limiting the
editability of real ones. MoMo harnesses motion inversion, extending its
application to both real and generated motions. Experimental results show the
advantage of our approach over the current art. In particular, unlike methods
tailored for specific applications through training, our approach is applied at
inference time, requiring no training. Our webpage is at
https://monkeyseedocg.github.io.",2024-06-10 17:47:14+00:00,"['Sigal Raab', 'Inbar Gat', 'Nathan Sala', 'Guy Tevet', 'Rotem Shalev-Arkushin', 'Ohad Fried', 'Amit H. Bermano', 'Daniel Cohen-Or']",http://arxiv.org/abs/2406.06508v1
Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis,"Recent works in implicit representations, such as Neural Radiance Fields
(NeRF), have advanced the generation of realistic and animatable head avatars
from video sequences. These implicit methods are still confronted by visual
artifacts and jitters, since the lack of explicit geometric constraints poses a
fundamental challenge in accurately modeling complex facial deformations. In
this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid
representation that encodes explicit dynamic meshes by neural networks to
ensure geometric consistency across various motions and viewpoints. DynTet is
parameterized by the coordinate-based networks which learn signed distance,
deformation, and material texture, anchoring the training data into a
predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently
decodes textured meshes with a consistent topology, enabling fast rendering
through a differentiable rasterizer and supervision via a pixel loss. To
enhance training efficiency, we incorporate classical 3D Morphable Models to
facilitate geometry learning and define a canonical space for simplifying
texture learning. These advantages are readily achievable owing to the
effective geometric representation employed in DynTet. Compared with prior
works, DynTet demonstrates significant improvements in fidelity, lip
synchronization, and real-time performance according to various metrics. Beyond
producing stable and visually appealing synthesis videos, our method also
outputs the dynamic meshes which is promising to enable many emerging
applications.",2024-02-27 09:56:15+00:00,"['Zicheng Zhang', 'Ruobing Zheng', 'Ziwen Liu', 'Congying Han', 'Tianqi Li', 'Meng Wang', 'Tiande Guo', 'Jingdong Chen', 'Bonan Li', 'Ming Yang']",http://arxiv.org/abs/2402.17364v1
Data Augmentation with Diffusion Models for Colon Polyp Localization on the Low Data Regime: How much real data is enough?,"The scarcity of data in medical domains hinders the performance of Deep
Learning models. Data augmentation techniques can alleviate that problem, but
they usually rely on functional transformations of the data that do not
guarantee to preserve the original tasks. To approximate the distribution of
the data using generative models is a way of reducing that problem and also to
obtain new samples that resemble the original data. Denoising Diffusion models
is a promising Deep Learning technique that can learn good approximations of
different kinds of data like images, time series or tabular data.
  Automatic colonoscopy analysis and specifically Polyp localization in
colonoscopy videos is a task that can assist clinical diagnosis and treatment.
The annotation of video frames for training a deep learning model is a time
consuming task and usually only small datasets can be obtained. The fine tuning
of application models using a large dataset of generated data could be an
alternative to improve their performance. We conduct a set of experiments
training different diffusion models that can generate jointly colonoscopy
images with localization annotations using a combination of existing open
datasets. The generated data is used on various transfer learning experiments
in the task of polyp localization with a model based on YOLO v9 on the low data
regime.",2024-11-28 05:25:33+00:00,"['Adrian Tormos', 'Blanca Llaurad', 'Fernando Nez', 'Axel Romero', 'Dario Garcia-Gasulla', 'Javier Bjar']",http://arxiv.org/abs/2411.18926v1
Attack on Scene Flow using Point Clouds,"Deep neural networks have made significant advancements in accurately
estimating scene flow using point clouds, which is vital for many applications
like video analysis, action recognition, and navigation. The robustness of
these techniques, however, remains a concern, particularly in the face of
adversarial attacks that have been proven to deceive state-of-the-art deep
neural networks in many domains. Surprisingly, the robustness of scene flow
networks against such attacks has not been thoroughly investigated. To address
this problem, the proposed approach aims to bridge this gap by introducing
adversarial white-box attacks specifically tailored for scene flow networks.
Experimental results show that the generated adversarial examples obtain up to
33.7 relative degradation in average end-point error on the KITTI and
FlyingThings3D datasets. The study also reveals the significant impact that
attacks targeting point clouds in only one dimension or color channel have on
average end-point error. Analyzing the success and failure of these attacks on
the scene flow networks and their 2D optical flow network variants shows a
higher vulnerability for the optical flow networks. Code is available at
https://github.com/aheldis/Attack-on-Scene-Flow-using-Point-Clouds.git.",2024-04-21 11:21:27+00:00,"['Haniyeh Ehsani Oskouie', 'Mohammad-Shahram Moin', 'Shohreh Kasaei']",http://arxiv.org/abs/2404.13621v6
Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers,"Recent advancements in diffusion models, particularly the architectural
transformation from UNet-based models to Diffusion Transformers (DiTs),
significantly improve the quality and scalability of image and video
generation. However, despite their impressive capabilities, the substantial
computational costs of these large-scale models pose significant challenges for
real-world deployment. Post-Training Quantization (PTQ) emerges as a promising
solution, enabling model compression and accelerated inference for pretrained
models, without the costly retraining. However, research on DiT quantization
remains sparse, and existing PTQ frameworks, primarily designed for traditional
diffusion models, tend to suffer from biased quantization, leading to notable
performance degradation. In this work, we identify that DiTs typically exhibit
significant spatial variance in both weights and activations, along with
temporal variance in activations. To address these issues, we propose Q-DiT, a
novel approach that seamlessly integrates two key techniques: automatic
quantization granularity allocation to handle the significant variance of
weights and activations across input channels, and sample-wise dynamic
activation quantization to adaptively capture activation changes across both
timesteps and samples. Extensive experiments conducted on ImageNet and VBench
demonstrate the effectiveness of the proposed Q-DiT. Specifically, when
quantizing DiT-XL/2 to W6A8 on ImageNet ($256 \times 256$), Q-DiT achieves a
remarkable reduction in FID by 1.09 compared to the baseline. Under the more
challenging W4A8 setting, it maintains high fidelity in image and video
generation, establishing a new benchmark for efficient, high-quality
quantization in DiTs. Code is available at
\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}.",2024-06-25 07:57:27+00:00,"['Lei Chen', 'Yuan Meng', 'Chen Tang', 'Xinzhu Ma', 'Jingyan Jiang', 'Xin Wang', 'Zhi Wang', 'Wenwu Zhu']",http://arxiv.org/abs/2406.17343v2
Generalized Event Cameras,"Event cameras capture the world at high time resolution and with minimal
bandwidth requirements. However, event streams, which only encode changes in
brightness, do not contain sufficient scene information to support a wide
variety of downstream tasks. In this work, we design generalized event cameras
that inherently preserve scene intensity in a bandwidth-efficient manner. We
generalize event cameras in terms of when an event is generated and what
information is transmitted. To implement our designs, we turn to single-photon
sensors that provide digital access to individual photon detections; this
modality gives us the flexibility to realize a rich space of generalized event
cameras. Our single-photon event cameras are capable of high-speed,
high-fidelity imaging at low readout rates. Consequently, these event cameras
can support plug-and-play downstream inference, without capturing new event
datasets or designing specialized event-vision models. As a practical
implication, our designs, which involve lightweight and near-sensor-compatible
computations, provide a way to use single-photon sensors without exorbitant
bandwidth costs.",2024-07-02 21:48:32+00:00,"['Varun Sundar', 'Matthew Dutson', 'Andrei Ardelean', 'Claudio Bruschini', 'Edoardo Charbon', 'Mohit Gupta']",http://arxiv.org/abs/2407.02683v1
Diffusion-based Unsupervised Audio-visual Speech Enhancement,"This paper proposes a new unsupervised audio-visual speech enhancement (AVSE)
approach that combines a diffusion-based audio-visual speech generative model
with a non-negative matrix factorization (NMF) noise model. First, the
diffusion model is pre-trained on clean speech conditioned on corresponding
video data to simulate the speech generative distribution. This pre-trained
model is then paired with the NMF-based noise model to estimate clean speech
iteratively. Specifically, a diffusion-based posterior sampling approach is
implemented within the reverse diffusion process, where after each iteration, a
speech estimate is obtained and used to update the noise parameters.
Experimental results confirm that the proposed AVSE approach not only
outperforms its audio-only counterpart but also generalizes better than a
recent supervised-generative AVSE method. Additionally, the new inference
algorithm offers a better balance between inference speed and performance
compared to the previous diffusion-based method. Code and demo available at:
https://jeaneudesayilo.github.io/fast_UdiffSE",2024-10-04 12:22:54+00:00,"['Jean-Eudes Ayilo', 'Mostafa Sadeghi', 'Romain Serizel', 'Xavier Alameda-Pineda']",http://arxiv.org/abs/2410.05301v2
Video2Reward: Generating Reward Function from Videos for Legged Robot Behavior Learning,"Learning behavior in legged robots presents a significant challenge due to
its inherent instability and complex constraints. Recent research has proposed
the use of a large language model (LLM) to generate reward functions in
reinforcement learning, thereby replacing the need for manually designed
rewards by experts. However, this approach, which relies on textual
descriptions to define learning objectives, fails to achieve controllable and
precise behavior learning with clear directionality. In this paper, we
introduce a new video2reward method, which directly generates reward functions
from videos depicting the behaviors to be mimicked and learned. Specifically,
we first process videos containing the target behaviors, converting the motion
information of individuals in the videos into keypoint trajectories represented
as coordinates through a video2text transforming module. These trajectories are
then fed into an LLM to generate the reward function, which in turn is used to
train the policy. To enhance the quality of the reward function, we develop a
video-assisted iterative reward refinement scheme that visually assesses the
learned behaviors and provides textual feedback to the LLM. This feedback
guides the LLM to continually refine the reward function, ultimately
facilitating more efficient behavior learning. Experimental results on tasks
involving bipedal and quadrupedal robot motion control demonstrate that our
method surpasses the performance of state-of-the-art LLM-based reward
generation methods by over 37.6% in terms of human normalized score. More
importantly, by switching video inputs, we find our method can rapidly learn
diverse motion behaviors such as walking and running.",2024-12-07 03:10:27+00:00,"['Runhao Zeng', 'Dingjie Zhou', 'Qiwei Liang', 'Junlin Liu', 'Hui Li', 'Changxin Huang', 'Jianqiang Li', 'Xiping Hu', 'Fuchun Sun']",http://arxiv.org/abs/2412.05515v1
AniDoc: Animation Creation Made Easier,"The production of 2D animation follows an industry-standard workflow,
encompassing four essential stages: character design, keyframe animation,
in-betweening, and coloring. Our research focuses on reducing the labor costs
in the above process by harnessing the potential of increasingly powerful
generative AI. Using video diffusion models as the foundation, AniDoc emerges
as a video line art colorization tool, which automatically converts sketch
sequences into colored animations following the reference character
specification. Our model exploits correspondence matching as an explicit
guidance, yielding strong robustness to the variations (e.g., posture) between
the reference character and each line art frame. In addition, our model could
even automate the in-betweening process, such that users can easily create a
temporally consistent animation by simply providing a character image as well
as the start and end sketches. Our code is available at:
https://yihao-meng.github.io/AniDoc_demo.",2024-12-18 18:59:59+00:00,"['Yihao Meng', 'Hao Ouyang', 'Hanlin Wang', 'Qiuyu Wang', 'Wen Wang', 'Ka Leong Cheng', 'Zhiheng Liu', 'Yujun Shen', 'Huamin Qu']",http://arxiv.org/abs/2412.14173v2
Flow as the Cross-Domain Manipulation Interface,"We present Im2Flow2Act, a scalable learning framework that enables robots to
acquire real-world manipulation skills without the need of real-world robot
training data. The key idea behind Im2Flow2Act is to use object flow as the
manipulation interface, bridging domain gaps between different embodiments
(i.e., human and robot) and training environments (i.e., real-world and
simulated). Im2Flow2Act comprises two components: a flow generation network and
a flow-conditioned policy. The flow generation network, trained on human
demonstration videos, generates object flow from the initial scene image,
conditioned on the task description. The flow-conditioned policy, trained on
simulated robot play data, maps the generated object flow to robot actions to
realize the desired object movements. By using flow as input, this policy can
be directly deployed in the real world with a minimal sim-to-real gap. By
leveraging real-world human videos and simulated robot play data, we bypass the
challenges of teleoperating physical robots in the real world, resulting in a
scalable system for diverse tasks. We demonstrate Im2Flow2Act's capabilities in
a variety of real-world tasks, including the manipulation of rigid,
articulated, and deformable objects.",2024-07-21 16:15:02+00:00,"['Mengda Xu', 'Zhenjia Xu', 'Yinghao Xu', 'Cheng Chi', 'Gordon Wetzstein', 'Manuela Veloso', 'Shuran Song']",http://arxiv.org/abs/2407.15208v2
"SkyScript-100M: 1,000,000,000 Pairs of Scripts and Shooting Scripts for Short Drama","Generating high-quality shooting scripts containing information such as scene
and shot language is essential for short drama script generation. We collect
6,660 popular short drama episodes from the Internet, each with an average of
100 short episodes, and the total number of short episodes is about 80,000,
with a total duration of about 2,000 hours and totaling 10 terabytes (TB). We
perform keyframe extraction and annotation on each episode to obtain about
10,000,000 shooting scripts. We perform 100 script restorations on the
extracted shooting scripts based on our self-developed large short drama
generation model SkyReels. This leads to a dataset containing 1,000,000,000
pairs of scripts and shooting scripts for short dramas, called SkyScript-100M.
We compare SkyScript-100M with the existing dataset in detail and demonstrate
some deeper insights that can be achieved based on SkyScript-100M. Based on
SkyScript-100M, researchers can achieve several deeper and more far-reaching
script optimization goals, which may drive a paradigm shift in the entire field
of text-to-video and significantly advance the field of short drama video
generation. The data and code are available at
https://github.com/vaew/SkyScript-100M.",2024-08-18 02:27:25+00:00,"['Jing Tang', 'Quanlu Jia', 'Yuqiang Xie', 'Zeyu Gong', 'Xiang Wen', 'Jiayi Zhang', 'Yalong Guo', 'Guibin Chen', 'Jiangping Yang']",http://arxiv.org/abs/2408.09333v2
PlaMo: Plan and Move in Rich 3D Physical Environments,"Controlling humanoids in complex physically simulated worlds is a
long-standing challenge with numerous applications in gaming, simulation, and
visual content creation. In our setup, given a rich and complex 3D scene, the
user provides a list of instructions composed of target locations and
locomotion types. To solve this task we present PlaMo, a scene-aware path
planner and a robust physics-based controller. The path planner produces a
sequence of motion paths, considering the various limitations the scene imposes
on the motion, such as location, height, and speed. Complementing the planner,
our control policy generates rich and realistic physical motion adhering to the
plan. We demonstrate how the combination of both modules enables traversing
complex landscapes in diverse forms while responding to real-time changes in
the environment. Video: https://youtu.be/wWlqSQlRZ9M .",2024-06-26 10:41:07+00:00,"['Assaf Hallak', 'Gal Dalal', 'Chen Tessler', 'Kelly Guo', 'Shie Mannor', 'Gal Chechik']",http://arxiv.org/abs/2406.18237v1
Neural MP: A Generalist Neural Motion Planner,"The current paradigm for motion planning generates solutions from scratch for
every new problem, which consumes significant amounts of time and computational
resources. For complex, cluttered scenes, motion planning approaches can often
take minutes to produce a solution, while humans are able to accurately and
safely reach any goal in seconds by leveraging their prior experience. We seek
to do the same by applying data-driven learning at scale to the problem of
motion planning. Our approach builds a large number of complex scenes in
simulation, collects expert data from a motion planner, then distills it into a
reactive generalist policy. We then combine this with lightweight optimization
to obtain a safe path for real world deployment. We perform a thorough
evaluation of our method on 64 motion planning tasks across four diverse
environments with randomized poses, scenes and obstacles, in the real world,
demonstrating an improvement of 23%, 17% and 79% motion planning success rate
over state of the art sampling, optimization and learning based planning
methods. Video results available at mihdalal.github.io/neuralmotionplanner",2024-09-09 17:59:45+00:00,"['Murtaza Dalal', 'Jiahui Yang', 'Russell Mendonca', 'Youssef Khaky', 'Ruslan Salakhutdinov', 'Deepak Pathak']",http://arxiv.org/abs/2409.05864v1
Intrinsic Single-Image HDR Reconstruction,"The low dynamic range (LDR) of common cameras fails to capture the rich
contrast in natural scenes, resulting in loss of color and details in saturated
pixels. Reconstructing the high dynamic range (HDR) of luminance present in the
scene from single LDR photographs is an important task with many applications
in computational photography and realistic display of images. The HDR
reconstruction task aims to infer the lost details using the context present in
the scene, requiring neural networks to understand high-level geometric and
illumination cues. This makes it challenging for data-driven algorithms to
generate accurate and high-resolution results. In this work, we introduce a
physically-inspired remodeling of the HDR reconstruction problem in the
intrinsic domain. The intrinsic model allows us to train separate networks to
extend the dynamic range in the shading domain and to recover lost color
details in the albedo domain. We show that dividing the problem into two
simpler sub-tasks improves performance in a wide variety of photographs.",2024-09-20 17:56:51+00:00,"['Sebastian Dille', 'Chris Careaga', 'Yaz Aksoy']",http://arxiv.org/abs/2409.13803v1
Action-based image editing guided by human instructions,"Text-based image editing is typically approached as a static task that
involves operations such as inserting, deleting, or modifying elements of an
input image based on human instructions. Given the static nature of this task,
in this paper, we aim to make this task dynamic by incorporating actions. By
doing this, we intend to modify the positions or postures of objects in the
image to depict different actions while maintaining the visual properties of
the objects. To implement this challenging task, we propose a new model that is
sensitive to action text instructions by learning to recognize contrastive
action discrepancies. The model training is done on new datasets defined by
extracting frames from videos that show the visual scenes before and after an
action. We show substantial improvements in image editing using action-based
text instructions and high reasoning capabilities that allow our model to use
the input image as a starting scene for an action while generating a new image
that shows the final scene of the action.",2024-12-05 19:01:41+00:00,"['Maria Mihaela Trusca', 'Mingxiao Li', 'Marie-Francine Moens']",http://arxiv.org/abs/2412.04558v2
Creative Portraiture: Exploring Creative Adversarial Networks and Conditional Creative Adversarial Networks,"Convolutional neural networks (CNNs) have been combined with generative
adversarial networks (GANs) to create deep convolutional generative adversarial
networks (DCGANs) with great success. DCGANs have been used for generating
images and videos from creative domains such as fashion design and painting. A
common critique of the use of DCGANs in creative applications is that they are
limited in their ability to generate creative products because the generator
simply learns to copy the training distribution. We explore an extension of
DCGANs, creative adversarial networks (CANs). Using CANs, we generate novel,
creative portraits, using the WikiArt dataset to train the network. Moreover,
we introduce our extension of CANs, conditional creative adversarial networks
(CCANs), and demonstrate their potential to generate creative portraits
conditioned on a style label. We argue that generating products that are
conditioned, or inspired, on a style label closely emulates real creative
processes in which humans produce imaginative work that is still rooted in
previous styles.",2024-12-10 01:18:26+00:00,"['Sebastian Hereu', 'Qianfei Hu']",http://arxiv.org/abs/2412.07091v1
MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation,"The recent Mamba model has shown remarkable adaptability for visual
representation learning, including in medical imaging tasks. This study
introduces MambaMIR, a Mamba-based model for medical image reconstruction, as
well as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our
proposed MambaMIR inherits several advantages, such as linear complexity,
global receptive fields, and dynamic weights, from the original Mamba model.
The innovated arbitrary-mask mechanism effectively adapt Mamba to our image
reconstruction task, providing randomness for subsequent Monte Carlo-based
uncertainty estimation. Experiments conducted on various medical image
reconstruction tasks, including fast MRI and SVCT, which cover anatomical
regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR
and MambaMIR-GAN achieve comparable or superior reconstruction results relative
to state-of-the-art methods. Additionally, the estimated uncertainty maps offer
further insights into the reliability of the reconstruction quality. The code
is publicly available at https://github.com/ayanglab/MambaMIR.",2024-02-28 16:24:08+00:00,"['Jiahao Huang', 'Liutao Yang', 'Fanwen Wang', 'Yang Nan', 'Angelica I. Aviles-Rivero', 'Carola-Bibiane Schnlieb', 'Daoqiang Zhang', 'Guang Yang']",http://arxiv.org/abs/2402.18451v3
"Onboard Processing of Hyperspectral Imagery: Deep Learning Advancements, Methodologies, Challenges, and Emerging Trends","Recent advancements in deep learning techniques have spurred considerable
interest in their application to hyperspectral imagery processing. This paper
provides a comprehensive review of the latest developments in this field,
focusing on methodologies, challenges, and emerging trends. Deep learning
architectures such as Convolutional Neural Networks (CNNs), Autoencoders, Deep
Belief Networks (DBNs), Generative Adversarial Networks (GANs), and Recurrent
Neural Networks (RNNs) are examined for their suitability in processing
hyperspectral data. Key challenges, including limited training data and
computational constraints, are identified, along with strategies such as data
augmentation and noise reduction using GANs. The paper discusses the efficacy
of different network architectures, highlighting the advantages of lightweight
CNN models and 1D CNNs for onboard processing. Moreover, the potential of
hardware accelerators, particularly Field Programmable Gate Arrays (FPGAs), for
enhancing processing efficiency is explored. The review concludes with insights
into ongoing research trends, including the integration of deep learning
techniques into Earth observation missions such as the CHIME mission, and
emphasizes the need for further exploration and refinement of deep learning
methodologies to address the evolving demands of hyperspectral image
processing.",2024-04-09 14:32:39+00:00,"['Nafiseh Ghasemi', 'Jon Alvarez Justo', 'Marco Celesti', 'Laurent Despoisse', 'Jens Nieke']",http://arxiv.org/abs/2404.06526v1
A Generalizable 3D Diffusion Framework for Low-Dose and Few-View Cardiac SPECT,"Myocardial perfusion imaging using SPECT is widely utilized to diagnose
coronary artery diseases, but image quality can be negatively affected in
low-dose and few-view acquisition settings. Although various deep learning
methods have been introduced to improve image quality from low-dose or few-view
SPECT data, previous approaches often fail to generalize across different
acquisition settings, limiting their applicability in reality. This work
introduced DiffSPECT-3D, a diffusion framework for 3D cardiac SPECT imaging
that effectively adapts to different acquisition settings without requiring
further network re-training or fine-tuning. Using both image and projection
data, a consistency strategy is proposed to ensure that diffusion sampling at
each step aligns with the low-dose/few-view projection measurements, the image
data, and the scanner geometry, thus enabling generalization to different
low-dose/few-view settings. Incorporating anatomical spatial information from
CT and total variation constraint, we proposed a 2.5D conditional strategy to
allow the DiffSPECT-3D to observe 3D contextual information from the entire
image volume, addressing the 3D memory issues in diffusion model. We
extensively evaluated the proposed method on 1,325 clinical 99mTc tetrofosmin
stress/rest studies from 795 patients. Each study was reconstructed into 5
different low-count and 5 different few-view levels for model evaluations,
ranging from 1% to 50% and from 1 view to 9 view, respectively. Validated
against cardiac catheterization results and diagnostic comments from nuclear
cardiologists, the presented results show the potential to achieve low-dose and
few-view SPECT imaging without compromising clinical performance. Additionally,
DiffSPECT-3D could be directly applied to full-dose SPECT images to further
improve image quality, especially in a low-dose stress-first cardiac SPECT
imaging protocol.",2024-12-21 10:33:31+00:00,"['Huidong Xie', 'Weijie Gan', 'Wei Ji', 'Xiongchao Chen', 'Alaa Alashi', 'Stephanie L. Thorn', 'Bo Zhou', 'Qiong Liu', 'Menghua Xia', 'Xueqi Guo', 'Yi-Hwa Liu', 'Hongyu An', 'Ulugbek S. Kamilov', 'Ge Wang', 'Albert J. Sinusas', 'Chi Liu']",http://arxiv.org/abs/2412.16573v1
Phys4DGen: A Physics-Driven Framework for Controllable and Efficient 4D Content Generation from a Single Image,"The task of 4D content generation involves creating dynamic 3D models that
evolve over time in response to specific input conditions, such as images.
Existing methods rely heavily on pre-trained video diffusion models to guide 4D
content dynamics, but these approaches often fail to capture essential physical
principles, as video diffusion models lack a robust understanding of real-world
physics. Moreover, these models face challenges in providing fine-grained
control over dynamics and exhibit high computational costs. In this work, we
propose Phys4DGen, a novel, high-efficiency framework that generates
physics-compliant 4D content from a single image with enhanced control
capabilities. Our approach uniquely integrates physical simulations into the 4D
generation pipeline, ensuring adherence to fundamental physical laws. Inspired
by the human ability to infer physical properties visually, we introduce a
Physical Perception Module (PPM) that discerns the material properties and
structural components of the 3D object from the input image, facilitating
accurate downstream simulations. Phys4DGen significantly accelerates the 4D
generation process by eliminating iterative optimization steps in the dynamics
modeling phase. It allows users to intuitively control the movement speed and
direction of generated 4D content by adjusting external forces, achieving
finely tunable, physically plausible animations. Extensive evaluations show
that Phys4DGen outperforms existing methods in both inference speed and
physical realism, producing high-quality, controllable 4D content. Our project
page is available at the link: \url{https://jiajinglin.github.io/Phys4DGen/}.",2024-11-25 12:12:38+00:00,"['Jiajing Lin', 'Zhenzhong Wang', 'Shu Jiang', 'Yongjie Hou', 'Min Jiang']",http://arxiv.org/abs/2411.16800v3
FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion,"Visual diffusion models achieve remarkable progress, yet they are typically
trained at limited resolutions due to the lack of high-resolution data and
constrained computation resources, hampering their ability to generate
high-fidelity images or videos at higher resolutions. Recent efforts have
explored tuning-free strategies to exhibit the untapped potential
higher-resolution visual generation of pre-trained models. However, these
methods are still prone to producing low-quality visual content with repetitive
patterns. The key obstacle lies in the inevitable increase in high-frequency
information when the model generates visual content exceeding its training
resolution, leading to undesirable repetitive patterns deriving from the
accumulated errors. To tackle this challenge, we propose FreeScale, a
tuning-free inference paradigm to enable higher-resolution visual generation
via scale fusion. Specifically, FreeScale processes information from different
receptive scales and then fuses it by extracting desired frequency components.
Extensive experiments validate the superiority of our paradigm in extending the
capabilities of higher-resolution visual generation for both image and video
models. Notably, compared with the previous best-performing method, FreeScale
unlocks the generation of 8k-resolution images for the first time.",2024-12-12 18:59:59+00:00,"['Haonan Qiu', 'Shiwei Zhang', 'Yujie Wei', 'Ruihang Chu', 'Hangjie Yuan', 'Xiang Wang', 'Yingya Zhang', 'Ziwei Liu']",http://arxiv.org/abs/2412.09626v1
Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation,"In this article, we propose the novel concept of Belief Scene Graphs, which
are utility-driven extensions of partial 3D scene graphs, that enable efficient
high-level task planning with partial information. We propose a graph-based
learning methodology for the computation of belief (also referred to as
expectation) on any given 3D scene graph, which is then used to strategically
add new nodes (referred to as blind nodes) that are relevant to a robotic
mission. We propose the method of Computation of Expectation based on
Correlation Information (CECI), to reasonably approximate real
Belief/Expectation, by learning histograms from available training data. A
novel Graph Convolutional Neural Network (GCN) model is developed, to learn
CECI from a repository of 3D scene graphs. As no database of 3D scene graphs
exists for the training of the novel CECI model, we present a novel methodology
for generating a 3D scene graph dataset based on semantically annotated
real-life 3D spaces. The generated dataset is then utilized to train the
proposed CECI model and for extensive validation of the proposed method. We
establish the novel concept of \textit{Belief Scene Graphs} (BSG), as a core
component to integrate expectations into abstract representations. This new
concept is an evolution of the classical 3D scene graph concept and aims to
enable high-level reasoning for task planning and optimization of a variety of
robotics missions. The efficacy of the overall framework has been evaluated in
an object search scenario, and has also been tested in a real-life experiment
to emulate human common sense of unseen-objects. For a video of the article,
showcasing the experimental demonstration, please refer to the following link:
https://youtu.be/hsGlSCa12iY",2024-02-06 09:37:42+00:00,"['Mario A. V. Saucedo', 'Akash Patel', 'Akshit Saradagi', 'Christoforos Kanellakis', 'George Nikolakopoulos']",http://arxiv.org/abs/2402.03840v2
Optimized two-stage AI-based Neural Decoding for Enhanced Visual Stimulus Reconstruction from fMRI Data,"AI-based neural decoding reconstructs visual perception by leveraging
generative models to map brain activity, measured through functional MRI
(fMRI), into latent hierarchical representations. Traditionally, ridge linear
models transform fMRI into a latent space, which is then decoded using latent
diffusion models (LDM) via a pre-trained variational autoencoder (VAE). Due to
the complexity and noisiness of fMRI data, newer approaches split the
reconstruction into two sequential steps, the first one providing a rough
visual approximation, the second on improving the stimulus prediction via LDM
endowed by CLIP embeddings. This work proposes a non-linear deep network to
improve fMRI latent space representation, optimizing the dimensionality alike.
Experiments on the Natural Scenes Dataset showed that the proposed architecture
improved the structural similarity of the reconstructed image by about 2\% with
respect to the state-of-the-art model, based on ridge linear transform. The
reconstructed image's semantics improved by about 4\%, measured by perceptual
similarity, with respect to the state-of-the-art. The noise sensitivity
analysis of the LDM showed that the role of the first stage was fundamental to
predict the stimulus featuring high structural similarity. Conversely,
providing a large noise stimulus affected less the semantics of the predicted
stimulus, while the structural similarity between the ground truth and
predicted stimulus was very poor. The findings underscore the importance of
leveraging non-linear relationships between BOLD signal and the latent
representation and two-stage generative AI for optimizing the fidelity of
reconstructed visual stimuli from noisy fMRI data.",2024-12-17 16:42:55+00:00,"['Lorenzo Veronese', 'Andrea Moglia', 'Luca Mainardi', 'Pietro Cerveri']",http://arxiv.org/abs/2412.13237v1
Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization,"Diffusion models are gaining widespread use in cutting-edge image, video, and
audio generation. Score-based diffusion models stand out among these methods,
necessitating the estimation of score function of the input data distribution.
In this study, we present a theoretical framework to analyze two-layer neural
network-based diffusion models by reframing score matching and denoising score
matching as convex optimization. We prove that training shallow neural networks
for score prediction can be done by solving a single convex program. Although
most analyses of diffusion models operate in the asymptotic setting or rely on
approximations, we characterize the exact predicted score function and
establish convergence results for neural network-based diffusion models with
finite data. Our results provide a precise characterization of what neural
network-based diffusion models learn in non-asymptotic settings.",2024-02-03 00:20:25+00:00,"['Fangzhao Zhang', 'Mert Pilanci']",http://arxiv.org/abs/2402.01965v3
Text Diffusion with Reinforced Conditioning,"Diffusion models have demonstrated exceptional capability in generating
high-quality images, videos, and audio. Due to their adaptiveness in iterative
refinement, they provide a strong potential for achieving better
non-autoregressive sequence generation. However, existing text diffusion models
still fall short in their performance due to a challenge in handling the
discreteness of language. This paper thoroughly analyzes text diffusion models
and uncovers two significant limitations: degradation of self-conditioning
during training and misalignment between training and sampling. Motivated by
our findings, we propose a novel Text Diffusion model called TREC, which
mitigates the degradation with Reinforced Conditioning and the misalignment by
Time-Aware Variance Scaling. Our extensive experiments demonstrate the
competitiveness of TREC against autoregressive, non-autoregressive, and
diffusion baselines. Moreover, qualitative analysis shows its advanced ability
to fully utilize the diffusion process in refining samples.",2024-02-19 09:24:02+00:00,"['Yuxuan Liu', 'Tianchi Yang', 'Shaohan Huang', 'Zihan Zhang', 'Haizhen Huang', 'Furu Wei', 'Weiwei Deng', 'Feng Sun', 'Qi Zhang']",http://arxiv.org/abs/2402.14843v1
Stale Diffusion: Hyper-realistic 5D Movie Generation Using Old-school Methods,"Two years ago, Stable Diffusion achieved super-human performance at
generating images with super-human numbers of fingers. Following the steady
decline of its technical novelty, we propose Stale Diffusion, a method that
solidifies and ossifies Stable Diffusion in a maximum-entropy state. Stable
Diffusion works analogously to a barn (the Stable) from which an infinite set
of horses have escaped (the Diffusion). As the horses have long left the barn,
our proposal may be seen as antiquated and irrelevant. Nevertheless, we
vigorously defend our claim of novelty by identifying as early adopters of the
Slow Science Movement, which will produce extremely important pearls of wisdom
in the future. Our speed of contributions can also be seen as a quasi-static
implementation of the recent call to pause AI experiments, which we
wholeheartedly support. As a result of a careful archaeological expedition to
18-months-old Git commit histories, we found that naturally-accumulating errors
have produced a novel entropy-maximising Stale Diffusion method, that can
produce sleep-inducing hyper-realistic 5D video that is as good as one's
imagination.",2024-04-01 12:19:54+00:00,"['Joao F. Henriques', 'Dylan Campbell', 'Tengda Han']",http://arxiv.org/abs/2404.01079v1
Simple and Effective Masked Diffusion Language Models,"While diffusion models excel at generating high-quality images, prior work
reports a significant performance gap between diffusion and autoregressive (AR)
methods in language modeling. In this work, we show that simple masked discrete
diffusion is more performant than previously thought. We apply an effective
training recipe that improves the performance of masked diffusion models and
derive a simplified, Rao-Blackwellized objective that results in additional
improvements. Our objective has a simple form -- it is a mixture of classical
masked language modeling losses -- and can be used to train encoder-only
language models that admit efficient samplers, including ones that can generate
arbitrary lengths of text semi-autoregressively like a traditional language
model. On language modeling benchmarks, a range of masked diffusion models
trained with modern engineering practices achieves a new state-of-the-art among
diffusion models, and approaches AR perplexity. We provide the code, along with
a blog post and video tutorial on the project page: https://s-sahoo.com/mdlm",2024-06-11 17:51:40+00:00,"['Subham Sekhar Sahoo', 'Marianne Arriola', 'Yair Schiff', 'Aaron Gokaslan', 'Edgar Marroquin', 'Justin T Chiu', 'Alexander Rush', 'Volodymyr Kuleshov']",http://arxiv.org/abs/2406.07524v2
AIS 2024 Challenge on Video Quality Assessment of User-Generated Content: Methods and Results,"This paper reviews the AIS 2024 Video Quality Assessment (VQA) Challenge,
focused on User-Generated Content (UGC). The aim of this challenge is to gather
deep learning-based methods capable of estimating the perceptual quality of UGC
videos. The user-generated videos from the YouTube UGC Dataset include diverse
content (sports, games, lyrics, anime, etc.), quality and resolutions. The
proposed methods must process 30 FHD frames under 1 second. In the challenge, a
total of 102 participants registered, and 15 submitted code and models. The
performance of the top-5 submissions is reviewed and provided here as a survey
of diverse deep models for efficient video quality assessment of user-generated
content.",2024-04-24 21:02:14+00:00,"['Marcos V. Conde', 'Saman Zadtootaghaj', 'Nabajeet Barman', 'Radu Timofte', 'Chenlong He', 'Qi Zheng', 'Ruoxi Zhu', 'Zhengzhong Tu', 'Haiqiang Wang', 'Xiangguang Chen', 'Wenhui Meng', 'Xiang Pan', 'Huiying Shi', 'Han Zhu', 'Xiaozhong Xu', 'Lei Sun', 'Zhenzhong Chen', 'Shan Liu', 'Zicheng Zhang', 'Haoning Wu', 'Yingjie Zhou', 'Chunyi Li', 'Xiaohong Liu', 'Weisi Lin', 'Guangtao Zhai', 'Wei Sun', 'Yuqin Cao', 'Yanwei Jiang', 'Jun Jia', 'Zhichao Zhang', 'Zijian Chen', 'Weixia Zhang', 'Xiongkuo Min', 'Steve Gring', 'Zihao Qi', 'Chen Feng']",http://arxiv.org/abs/2404.16205v1
High Frequency Matters: Uncertainty Guided Image Compression with Wavelet Diffusion,"Diffusion probabilistic models have recently achieved remarkable success in
generating high-quality images. However, balancing high perceptual quality and
low distortion remains challenging in image compression applications. To
address this issue, we propose an efficient Uncertainty-Guided image
compression approach with wavelet Diffusion (UGDiff). Our approach focuses on
high frequency compression via the wavelet transform, since high frequency
components are crucial for reconstructing image details. We introduce a wavelet
conditional diffusion model for high frequency prediction, followed by a
residual codec that compresses and transmits prediction residuals to the
decoder. This diffusion prediction-then-residual compression paradigm
effectively addresses the low fidelity issue common in direct reconstructions
by existing diffusion models. Considering the uncertainty from the random
sampling of the diffusion model, we further design an uncertainty-weighted
rate-distortion (R-D) loss tailored for residual compression, providing a more
rational trade-off between rate and distortion. Comprehensive experiments on
two benchmark datasets validate the effectiveness of UGDiff, surpassing
state-of-the-art image compression methods in R-D performance, perceptual
quality, subjective quality, and inference time. Our code is available at:
https://github.com/hejiaxiang1/Wavelet-Diffusion/tree/main",2024-07-17 13:21:31+00:00,"['Juan Song', 'Jiaxiang He', 'Lijie Yang', 'Mingtao Feng', 'Keyan Wang']",http://arxiv.org/abs/2407.12538v2
Recasting Generic Pretrained Vision Transformers As Object-Centric Scene Encoders For Manipulation Policies,"Generic re-usable pre-trained image representation encoders have become a
standard component of methods for many computer vision tasks. As visual
representations for robots however, their utility has been limited, leading to
a recent wave of efforts to pre-train robotics-specific image encoders that are
better suited to robotic tasks than their generic counterparts. We propose
Scene Objects From Transformers, abbreviated as SOFT, a wrapper around
pre-trained vision transformer (PVT) models that bridges this gap without any
further training. Rather than construct representations out of only the final
layer activations, SOFT individuates and locates object-like entities from PVT
attentions, and describes them with PVT activations, producing an
object-centric embedding. Across standard choices of generic pre-trained vision
transformers PVT, we demonstrate in each case that policies trained on
SOFT(PVT) far outstrip standard PVT representations for manipulation tasks in
simulated and real settings, approaching the state-of-the-art robotics-aware
representations. Code, appendix and videos:
https://sites.google.com/view/robot-soft/",2024-05-24 20:20:15+00:00,"['Jianing Qian', 'Anastasios Panagopoulos', 'Dinesh Jayaraman']",http://arxiv.org/abs/2405.15916v1
Towards General Text-guided Image Synthesis for Customized Multimodal Brain MRI Generation,"Multimodal brain magnetic resonance (MR) imaging is indispensable in
neuroscience and neurology. However, due to the accessibility of MRI scanners
and their lengthy acquisition time, multimodal MR images are not commonly
available. Current MR image synthesis approaches are typically trained on
independent datasets for specific tasks, leading to suboptimal performance when
applied to novel datasets and tasks. Here, we present TUMSyn, a Text-guided
Universal MR image Synthesis generalist model, which can flexibly generate
brain MR images with demanded imaging metadata from routinely acquired scans
guided by text prompts. To ensure TUMSyn's image synthesis precision,
versatility, and generalizability, we first construct a brain MR database
comprising 31,407 3D images with 7 MRI modalities from 13 centers. We then
pre-train an MRI-specific text encoder using contrastive learning to
effectively control MR image synthesis based on text prompts. Extensive
experiments on diverse datasets and physician assessments indicate that TUMSyn
can generate clinically meaningful MR images with specified imaging metadata in
supervised and zero-shot scenarios. Therefore, TUMSyn can be utilized along
with acquired MR scan(s) to facilitate large-scale MRI-based screening and
diagnosis of brain diseases.",2024-09-25 11:14:47+00:00,"['Yulin Wang', 'Honglin Xiong', 'Kaicong Sun', 'Shuwei Bai', 'Ling Dai', 'Zhongxiang Ding', 'Jiameng Liu', 'Qian Wang', 'Qian Liu', 'Dinggang Shen']",http://arxiv.org/abs/2409.16818v1
TexSenseGAN: A User-Guided System for Optimizing Texture-Related Vibrotactile Feedback Using Generative Adversarial Network,"Vibration rendering is essential for creating realistic tactile experiences
in human-virtual object interactions, such as in video game controllers and VR
devices. By dynamically adjusting vibration parameters based on user actions,
these systems can convey spatial features and contribute to texture
representation. However, generating arbitrary vibrations to replicate
real-world material textures is challenging due to the large parameter space.
This study proposes a human-in-the-loop vibration generation model based on
user preferences. To enable users to easily control the generation of vibration
samples with large parameter spaces, we introduced an optimization model based
on Differential Subspace Search (DSS) and Generative Adversarial Network (GAN).
With DSS, users can employ a one-dimensional slider to easily modify the
high-dimensional latent space to ensure that the GAN can generate desired
vibrations. We trained the generative model using an open dataset of tactile
vibration data and selected five types of vibrations as target samples for the
generation experiment. Extensive user experiments were conducted using the
generated and real samples. The results indicated that our system could
generate distinguishable samples that matched the target characteristics.
Moreover, we established a correlation between subjects' ability to distinguish
real samples and their ability to distinguish generated samples.",2024-07-16 08:07:41+00:00,"['Mingxin Zhang', 'Shun Terui', 'Yasutoshi Makino', 'Hiroyuki Shinoda']",http://arxiv.org/abs/2407.11467v5
EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting,"Scene reconstruction from casually captured videos has wide applications in
real-world scenarios. With recent advancements in differentiable rendering
techniques, several methods have attempted to simultaneously optimize scene
representations (NeRF or 3DGS) and camera poses. Despite recent progress,
existing methods relying on traditional camera input tend to fail in high-speed
(or equivalently low-frame-rate) scenarios. Event cameras, inspired by
biological vision, record pixel-wise intensity changes asynchronously with high
temporal resolution, providing valuable scene and motion information in blind
inter-frame intervals. In this paper, we introduce the event camera to aid
scene construction from a casually captured video for the first time, and
propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly
integrates the advantages of event cameras into 3DGS through three key
components. First, we leverage the Event Generation Model (EGM) to fuse events
and frames, supervising the rendered views observed by the event stream.
Second, we adopt the Contrast Maximization (CMax) framework in a piece-wise
manner to extract motion information by maximizing the contrast of the Image of
Warped Events (IWE), thereby calibrating the estimated poses. Besides, based on
the Linear Event Generation Model (LEGM), the brightness information encoded in
the IWE is also utilized to constrain the 3DGS in the gradient domain. Third,
to mitigate the absence of color information of events, we introduce
photometric bundle adjustment (PBA) to ensure view consistency across events
and frames. We evaluate our method on the public Tanks and Temples benchmark
and a newly collected real-world dataset, RealEv-DAVIS. Our project page is
https://lbh666.github.io/ef-3dgs/.",2024-10-20 13:44:24+00:00,"['Bohao Liao', 'Wei Zhai', 'Zengyu Wan', 'Zhixin Cheng', 'Wenfei Yang', 'Tianzhu Zhang', 'Yang Cao', 'Zheng-Jun Zha']",http://arxiv.org/abs/2410.15392v3
SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing,"Audio-driven talking face generation aims to synthesize video with lip
movements synchronized to input audio. However, current generative techniques
face challenges in preserving intricate regional textures (skin, teeth). To
address the aforementioned challenges, we propose a novel framework called
SegTalker to decouple lip movements and image textures by introducing
segmentation as intermediate representation. Specifically, given the mask of
image employed by a parsing network, we first leverage the speech to drive the
mask and generate talking segmentation. Then we disentangle semantic regions of
image into style codes using a mask-guided encoder. Ultimately, we inject the
previously generated talking segmentation and style codes into a mask-guided
StyleGAN to synthesize video frame. In this way, most of textures are fully
preserved. Moreover, our approach can inherently achieve background separation
and facilitate mask-guided facial local editing. In particular, by editing the
mask and swapping the region textures from a given reference image (e.g. hair,
lip, eyebrows), our approach enables facial editing seamlessly when generating
talking face video. Experiments demonstrate that our proposed approach can
effectively preserve texture details and generate temporally consistent video
while remaining competitive in lip synchronization. Quantitative and
qualitative results on the HDTF and MEAD datasets illustrate the superior
performance of our method over existing methods.",2024-09-05 15:11:40+00:00,"['Lingyu Xiong', 'Xize Cheng', 'Jintao Tan', 'Xianjia Wu', 'Xiandong Li', 'Lei Zhu', 'Fei Ma', 'Minglei Li', 'Huang Xu', 'Zhihu Hu']",http://arxiv.org/abs/2409.03605v1
RealisDance: Equip controllable character animation with realistic hands,"Controllable character animation is an emerging task that generates character
videos controlled by pose sequences from given character images. Although
character consistency has made significant progress via reference UNet, another
crucial factor, pose control, has not been well studied by existing methods
yet, resulting in several issues: 1) The generation may fail when the input
pose sequence is corrupted. 2) The hands generated using the DWPose sequence
are blurry and unrealistic. 3) The generated video will be shaky if the pose
sequence is not smooth enough. In this paper, we present RealisDance to handle
all the above issues. RealisDance adaptively leverages three types of poses,
avoiding failed generation caused by corrupted pose sequences. Among these pose
types, HaMeR provides accurate 3D and depth information of hands, enabling
RealisDance to generate realistic hands even for complex gestures. Besides
using temporal attention in the main UNet, RealisDance also inserts temporal
attention into the pose guidance network, smoothing the video from the pose
condition aspect. Moreover, we introduce pose shuffle augmentation during
training to further improve generation robustness and video smoothness.
Qualitative experiments demonstrate the superiority of RealisDance over other
existing methods, especially in hand quality.",2024-09-10 04:14:11+00:00,"['Jingkai Zhou', 'Benzhi Wang', 'Weihua Chen', 'Jingqi Bai', 'Dongyang Li', 'Aixi Zhang', 'Hao Xu', 'Mingyang Yang', 'Fan Wang']",http://arxiv.org/abs/2409.06202v1
4D Panoptic Scene Graph Generation,"We are living in a three-dimensional space while moving forward through a
fourth dimension: time. To allow artificial intelligence to develop a
comprehensive understanding of such a 4D environment, we introduce 4D Panoptic
Scene Graph (PSG-4D), a new representation that bridges the raw visual data
perceived in a dynamic 4D world and high-level visual understanding.
Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent
entities with precise location and status information, and edges, which capture
the temporal relations. To facilitate research in this new area, we build a
richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of
1M frames, each of which is labeled with 4D panoptic segmentation masks as well
as fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,
a Transformer-based model that can predict panoptic segmentation masks, track
masks along the time axis, and generate the corresponding scene graphs via a
relation component. Extensive experiments on the new dataset show that our
method can serve as a strong baseline for future research on PSG-4D. In the
end, we provide a real-world application example to demonstrate how we can
achieve dynamic scene understanding by integrating a large language model into
our PSG-4D system.",2024-05-16 17:56:55+00:00,"['Jingkang Yang', 'Jun Cen', 'Wenxuan Peng', 'Shuai Liu', 'Fangzhou Hong', 'Xiangtai Li', 'Kaiyang Zhou', 'Qifeng Chen', 'Ziwei Liu']",http://arxiv.org/abs/2405.10305v1
Diffusion Posterior Proximal Sampling for Image Restoration,"Diffusion models have demonstrated remarkable efficacy in generating
high-quality samples. Existing diffusion-based image restoration algorithms
exploit pre-trained diffusion models to leverage data priors, yet they still
preserve elements inherited from the unconditional generation paradigm. These
strategies initiate the denoising process with pure white noise and incorporate
random noise at each generative step, leading to over-smoothed results. In this
paper, we present a refined paradigm for diffusion-based image restoration.
Specifically, we opt for a sample consistent with the measurement identity at
each generative step, exploiting the sampling selection as an avenue for output
stability and enhancement. The number of candidate samples used for selection
is adaptively determined based on the signal-to-noise ratio of the timestep.
Additionally, we start the restoration process with an initialization combined
with the measurement signal, providing supplementary information to better
align the generative process. Extensive experimental results and analyses
validate that our proposed method significantly enhances image restoration
performance while consuming negligible additional computational resources.",2024-02-25 04:24:28+00:00,"['Hongjie Wu', 'Linchao He', 'Mingqin Zhang', 'Dongdong Chen', 'Kunming Luo', 'Mengting Luo', 'Ji-Zhe Zhou', 'Hu Chen', 'Jiancheng Lv']",http://arxiv.org/abs/2402.16907v2
Text-to-Model: Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization,"Generative artificial intelligence (GenAI) has made significant progress in
understanding world knowledge and generating content from human languages
across various modalities, like text-to-text large language models,
text-to-image stable diffusion, and text-to-video Sora. While in this paper, we
investigate the capability of GenAI for text-to-model generation, to see
whether GenAI can comprehend hyper-level knowledge embedded within AI itself
parameters. Specifically, we study a practical scenario termed
train-once-for-all personalization, aiming to generate personalized models for
diverse end-users and tasks using text prompts. Inspired by the recent
emergence of neural network diffusion, we present Tina, a text-conditioned
neural network diffusion for train-once-for-all personalization. Tina leverages
a diffusion transformer model conditioned on task descriptions embedded using a
CLIP model. Despite the astronomical number of potential personalized tasks
(e.g., $1.73\times10^{13}$), by our design, Tina demonstrates remarkable
in-distribution and out-of-distribution generalization even trained on small
datasets ($\sim 1000$). We further verify whether and how \Tina understands
world knowledge by analyzing its capabilities under zero-shot/few-shot image
prompts, different numbers of personalized classes, prompts of natural language
descriptions, and predicting unseen entities.",2024-05-23 03:11:18+00:00,"['Zexi Li', 'Lingzhi Gao', 'Chao Wu']",http://arxiv.org/abs/2405.14132v2
Using Spatial Diffusions for Optoacoustic Tomography Image Reconstruction,"Optoacoustic tomography image reconstruction has been a problem of interest
in recent years. By exploiting the exceptional generative power of the recently
proposed diffusion models we consider a scheme which is based on a conditional
diffusion process. Using a simple initial image reconstruction method such as
Delay and Sum, we consider a specially designed autoencoder architecture which
generates a latent representation which is used as conditional information in
the generative diffusion process. Numerical results show the merits of our
proposal in terms of quality metrics such as PSNR and SSIM, showing that the
conditional information generated in terms of the initial reconstructed image
is able to bias the generative process of the diffusion model in order to
enhance the image, correct artifacts and even recover some finer details that
the initial reconstruction method is not able to obtain.",2024-11-08 12:37:25+00:00,"['Martin G. Gonzalez', 'Matias Vera', 'Leonardo Rey Vega']",http://arxiv.org/abs/2411.15156v1
Analysing Diffusion Segmentation for Medical Images,"Denoising Diffusion Probabilistic models have become increasingly popular due
to their ability to offer probabilistic modeling and generate diverse outputs.
This versatility inspired their adaptation for image segmentation, where
multiple predictions of the model can produce segmentation results that not
only achieve high quality but also capture the uncertainty inherent in the
model. Here, powerful architectures were proposed for improving diffusion
segmentation performance. However, there is a notable lack of analysis and
discussions on the differences between diffusion segmentation and image
generation, and thorough evaluations are missing that distinguish the
improvements these architectures provide for segmentation in general from their
benefit for diffusion segmentation specifically. In this work, we critically
analyse and discuss how diffusion segmentation for medical images differs from
diffusion image generation, with a particular focus on the training behavior.
Furthermore, we conduct an assessment how proposed diffusion segmentation
architectures perform when trained directly for segmentation. Lastly, we
explore how different medical segmentation tasks influence the diffusion
segmentation behavior and the diffusion process could be adapted accordingly.
With these analyses, we aim to provide in-depth insights into the behavior of
diffusion segmentation that allow for a better design and evaluation of
diffusion segmentation methods in the future.",2024-03-21 14:45:54+00:00,"['Mathias ttl', 'Siyuan Mei', 'Frauke Wilm', 'Jana Steenpass', 'Matthias Rbner', 'Arndt Hartmann', 'Matthias Beckmann', 'Peter Fasching', 'Andreas Maier', 'Ramona Erber', 'Katharina Breininger']",http://arxiv.org/abs/2403.14440v1
InfinityDrive: Breaking Time Limits in Driving World Models,"Autonomous driving systems struggle with complex scenarios due to limited
access to diverse, extensive, and out-of-distribution driving data which are
critical for safe navigation. World models offer a promising solution to this
challenge; however, current driving world models are constrained by short time
windows and limited scenario diversity. To bridge this gap, we introduce
InfinityDrive, the first driving world model with exceptional generalization
capabilities, delivering state-of-the-art performance in high fidelity,
consistency, and diversity with minute-scale video generation. InfinityDrive
introduces an efficient spatio-temporal co-modeling module paired with an
extended temporal training strategy, enabling high-resolution (576$\times$1024)
video generation with consistent spatial and temporal coherence. By
incorporating memory injection and retention mechanisms alongside an adaptive
memory curve loss to minimize cumulative errors, achieving consistent video
generation lasting over 1500 frames (more than 2 minutes). Comprehensive
experiments in multiple datasets validate InfinityDrive's ability to generate
complex and varied scenarios, highlighting its potential as a next-generation
driving world model built for the evolving demands of autonomous driving. Our
project homepage:
https://metadrivescape.github.io/papers_project/InfinityDrive/page.html",2024-12-02 14:15:41+00:00,"['Xi Guo', 'Chenjing Ding', 'Haoxuan Dou', 'Xin Zhang', 'Weixuan Tang', 'Wei Wu']",http://arxiv.org/abs/2412.01522v2
General Flow as Foundation Affordance for Scalable Robot Learning,"We address the challenge of acquiring real-world manipulation skills with a
scalable framework. We hold the belief that identifying an appropriate
prediction target capable of leveraging large-scale datasets is crucial for
achieving efficient and universal learning. Therefore, we propose to utilize 3D
flow, which represents the future trajectories of 3D points on objects of
interest, as an ideal prediction target. To exploit scalable data resources, we
turn our attention to human videos. We develop, for the first time, a
language-conditioned 3D flow prediction model directly from large-scale RGBD
human video datasets. Our predicted flow offers actionable guidance, thus
facilitating zero-shot skill transfer in real-world scenarios. We deploy our
method with a policy based on closed-loop flow prediction. Remarkably, without
any in-domain finetuning, our method achieves an impressive 81\% success rate
in zero-shot human-to-robot skill transfer, covering 18 tasks in 6 scenes. Our
framework features the following benefits: (1) scalability: leveraging
cross-embodiment data resources; (2) wide application: multiple object
categories, including rigid, articulated, and soft bodies; (3) stable skill
transfer: providing actionable guidance with a small inference domain-gap.
Code, data, and supplementary materials are available
https://general-flow.github.io",2024-01-21 09:39:11+00:00,"['Chengbo Yuan', 'Chuan Wen', 'Tong Zhang', 'Yang Gao']",http://arxiv.org/abs/2401.11439v2
Diff-DTI: Fast Diffusion Tensor Imaging Using A Feature-Enhanced Joint Diffusion Model,"Magnetic resonance diffusion tensor imaging (DTI) is a critical tool for
neural disease diagnosis. However, long scan time greatly hinders the
widespread clinical use of DTI. To accelerate image acquisition, a
feature-enhanced joint diffusion model (Diff-DTI) is proposed to obtain
accurate DTI parameter maps from a limited number of diffusion-weighted images
(DWIs). Diff-DTI introduces a joint diffusion model that directly learns the
joint probability distribution of DWIs with DTI parametric maps for conditional
generation. Additionally, a feature enhancement fusion mechanism (FEFM) is
designed and incorporated into the generative process of Diff-DTI to preserve
fine structures in the generated DTI maps. A comprehensive evaluation of the
performance of Diff-DTI was conducted on the Human Connectome Project dataset.
The results demonstrate that Diff-DTI outperforms existing state-of-the-art
fast DTI imaging methods in terms of visual quality and quantitative metrics.
Furthermore, Diff-DTI has shown the ability to produce high-fidelity DTI maps
with only three DWIs, thus overcoming the requirement of a minimum of six DWIs
for DTI.",2024-05-24 07:39:06+00:00,"['Lang Zhang', 'Jinling He', 'Dong Liang', 'Hairong Zheng', 'Yanjie Zhu']",http://arxiv.org/abs/2405.15830v1
SePPO: Semi-Policy Preference Optimization for Diffusion Alignment,"Reinforcement learning from human feedback (RLHF) methods are emerging as a
way to fine-tune diffusion models (DMs) for visual generation. However,
commonly used on-policy strategies are limited by the generalization capability
of the reward model, while off-policy approaches require large amounts of
difficult-to-obtain paired human-annotated data, particularly in visual
generation tasks. To address the limitations of both on- and off-policy RLHF,
we propose a preference optimization method that aligns DMs with preferences
without relying on reward models or paired human-annotated data. Specifically,
we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO
leverages previous checkpoints as reference models while using them to generate
on-policy reference samples, which replace ""losing images"" in preference pairs.
This approach allows us to optimize using only off-policy ""winning images.""
Furthermore, we design a strategy for reference model selection that expands
the exploration in the policy space. Notably, we do not simply treat reference
samples as negative examples for learning. Instead, we design an anchor-based
criterion to assess whether the reference samples are likely to be winning or
losing images, allowing the model to selectively learn from the generated
reference samples. This approach mitigates performance degradation caused by
the uncertainty in reference sample quality. We validate SePPO across both
text-to-image and text-to-video benchmarks. SePPO surpasses all previous
approaches on the text-to-image benchmarks and also demonstrates outstanding
performance on the text-to-video benchmarks. Code will be released in
https://github.com/DwanZhang-AI/SePPO.",2024-10-07 17:56:53+00:00,"['Daoan Zhang', 'Guangchen Lan', 'Dong-Jun Han', 'Wenlin Yao', 'Xiaoman Pan', 'Hongming Zhang', 'Mingxiao Li', 'Pengcheng Chen', 'Yu Dong', 'Christopher Brinton', 'Jiebo Luo']",http://arxiv.org/abs/2410.05255v1
Evaluating and Mitigating IP Infringement in Visual Generative AI,"The popularity of visual generative AI models like DALL-E 3, Stable Diffusion
XL, Stable Video Diffusion, and Sora has been increasing. Through extensive
evaluation, we discovered that the state-of-the-art visual generative models
can generate content that bears a striking resemblance to characters protected
by intellectual property rights held by major entertainment companies (such as
Sony, Marvel, and Nintendo), which raises potential legal concerns. This
happens when the input prompt contains the character's name or even just
descriptive details about their characteristics. To mitigate such IP
infringement problems, we also propose a defense method against it. In detail,
we develop a revised generation paradigm that can identify potentially
infringing generated content and prevent IP infringement by utilizing guidance
techniques during the diffusion process. It has the capability to recognize
generated content that may be infringing on intellectual property rights, and
mitigate such infringement by employing guidance methods throughout the
diffusion process without retrain or fine-tune the pretrained models.
Experiments on well-known character IPs like Spider-Man, Iron Man, and Superman
demonstrate the effectiveness of the proposed defense method. Our data and code
can be found at https://github.com/ZhentingWang/GAI_IP_Infringement.",2024-06-07 06:14:18+00:00,"['Zhenting Wang', 'Chen Chen', 'Vikash Sehwag', 'Minzhou Pan', 'Lingjuan Lyu']",http://arxiv.org/abs/2406.04662v1
DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM,"Visual language tracking (VLT) has emerged as a cutting-edge research area,
harnessing linguistic data to enhance algorithms with multi-modal inputs and
broadening the scope of traditional single object tracking (SOT) to encompass
video understanding applications. Despite this, most VLT benchmarks still
depend on succinct, human-annotated text descriptions for each video. These
descriptions often fall short in capturing the nuances of video content
dynamics and lack stylistic variety in language, constrained by their uniform
level of detail and a fixed annotation frequency. As a result, algorithms tend
to default to a ""memorize the answer"" strategy, diverging from the core
objective of achieving a deeper understanding of video content. Fortunately,
the emergence of large language models (LLMs) has enabled the generation of
diverse text. This work utilizes LLMs to generate varied semantic annotations
(in terms of text lengths and granularities) for representative SOT benchmarks,
thereby establishing a novel multi-modal benchmark. Specifically, we (1)
propose a new visual language tracking benchmark with diverse texts, named
DTVLT, based on five prominent VLT and SOT benchmarks, including three
sub-tasks: short-term tracking, long-term tracking, and global instance
tracking. (2) We offer four granularity texts in our benchmark, considering the
extent and density of semantic information. We expect this multi-granular
generation strategy to foster a favorable environment for VLT and video
understanding research. (3) We conduct comprehensive experimental analyses on
DTVLT, evaluating the impact of diverse text on tracking performance and hope
the identified performance bottlenecks of existing algorithms can support
further research in VLT and video understanding. The proposed benchmark,
experimental results and toolkit will be released gradually on
http://videocube.aitestunion.com/.",2024-10-03 13:57:07+00:00,"['Xuchen Li', 'Shiyu Hu', 'Xiaokun Feng', 'Dailing Zhang', 'Meiqi Wu', 'Jing Zhang', 'Kaiqi Huang']",http://arxiv.org/abs/2410.02492v2
G4G:A Generic Framework for High Fidelity Talking Face Generation with Fine-grained Intra-modal Alignment,"Despite numerous completed studies, achieving high fidelity talking face
generation with highly synchronized lip movements corresponding to arbitrary
audio remains a significant challenge in the field. The shortcomings of
published studies continue to confuse many researchers. This paper introduces
G4G, a generic framework for high fidelity talking face generation with
fine-grained intra-modal alignment. G4G can reenact the high fidelity of
original video while producing highly synchronized lip movements regardless of
given audio tones or volumes. The key to G4G's success is the use of a diagonal
matrix to enhance the ordinary alignment of audio-image intra-modal features,
which significantly increases the comparative learning between positive and
negative samples. Additionally, a multi-scaled supervision module is introduced
to comprehensively reenact the perceptional fidelity of original video across
the facial region while emphasizing the synchronization of lip movements and
the input audio. A fusion network is then used to further fuse the facial
region and the rest. Our experimental results demonstrate significant
achievements in reenactment of original video quality as well as highly
synchronized talking lips. G4G is an outperforming generic framework that can
produce talking videos competitively closer to ground truth level than current
state-of-the-art methods.",2024-02-28 07:23:17+00:00,"['Juan Zhang', 'Jiahao Chen', 'Cheng Wang', 'Zhiwang Yu', 'Tangquan Qi', 'Di Wu']",http://arxiv.org/abs/2402.18122v2
Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels,"Video generative models are receiving particular attention given their
ability to generate realistic and imaginative frames. Besides, these models are
also observed to exhibit strong 3D consistency, significantly enhancing their
potential to act as world simulators. In this work, we present Vidu4D, a novel
reconstruction model that excels in accurately reconstructing 4D (i.e.,
sequential 3D) representations from single generated videos, addressing
challenges associated with non-rigidity and frame distortion. This capability
is pivotal for creating high-fidelity virtual contents that maintain both
spatial and temporal coherence. At the core of Vidu4D is our proposed Dynamic
Gaussian Surfels (DGS) technique. DGS optimizes time-varying warping functions
to transform Gaussian surfels (surface elements) from a static state to a
dynamically warped state. This transformation enables a precise depiction of
motion and deformation over time. To preserve the structural integrity of
surface-aligned Gaussian surfels, we design the warped-state geometric
regularization based on continuous warping fields for estimating normals.
Additionally, we learn refinements on rotation and scaling parameters of
Gaussian surfels, which greatly alleviates texture flickering during the
warping process and enhances the capture of fine-grained appearance details.
Vidu4D also contains a novel initialization state that provides a proper start
for the warping fields in DGS. Equipping Vidu4D with an existing video
generative model, the overall framework demonstrates high-fidelity text-to-4D
generation in both appearance and geometry.",2024-05-27 04:43:44+00:00,"['Yikai Wang', 'Xinzhou Wang', 'Zilong Chen', 'Zhengyi Wang', 'Fuchun Sun', 'Jun Zhu']",http://arxiv.org/abs/2405.16822v1
Multi-granularity Contrastive Cross-modal Collaborative Generation for End-to-End Long-term Video Question Answering,"Long-term Video Question Answering (VideoQA) is a challenging
vision-and-language bridging task focusing on semantic understanding of
untrimmed long-term videos and diverse free-form questions, simultaneously
emphasizing comprehensive cross-modal reasoning to yield precise answers. The
canonical approaches often rely on off-the-shelf feature extractors to detour
the expensive computation overhead, but often result in domain-independent
modality-unrelated representations. Furthermore, the inherent gradient blocking
between unimodal comprehension and cross-modal interaction hinders reliable
answer generation. In contrast, recent emerging successful video-language
pre-training models enable cost-effective end-to-end modeling but fall short in
domain-specific ratiocination and exhibit disparities in task formulation.
Toward this end, we present an entirely end-to-end solution for long-term
VideoQA: Multi-granularity Contrastive cross-modal collaborative Generation
(MCG) model. To derive discriminative representations possessing high visual
concepts, we introduce Joint Unimodal Modeling (JUM) on a clip-bone
architecture and leverage Multi-granularity Contrastive Learning (MCL) to
harness the intrinsically or explicitly exhibited semantic correspondences. To
alleviate the task formulation discrepancy problem, we propose a Cross-modal
Collaborative Generation (CCG) module to reformulate VideoQA as a generative
task instead of the conventional classification scheme, empowering the model
with the capability for cross-modal high-semantic fusion and generation so as
to rationalize and answer. Extensive experiments conducted on six publicly
available VideoQA datasets underscore the superiority of our proposed method.",2024-10-12 06:21:58+00:00,"['Ting Yu', 'Kunhao Fu', 'Jian Zhang', 'Qingming Huang', 'Jun Yu']",http://arxiv.org/abs/2410.09379v1
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench and additional
results are released in https://wang-sj16.github.io/motif/.",2024-12-20 18:57:06+00:00,"['Shijie Wang', 'Samaneh Azadi', 'Rohit Girdhar', 'Saketh Rambhatla', 'Chen Sun', 'Xi Yin']",http://arxiv.org/abs/2412.16153v2
DreamDance: Animating Human Images by Enriching 3D Geometry Cues from 2D Poses,"In this work, we present DreamDance, a novel method for animating human
images using only skeleton pose sequences as conditional inputs. Existing
approaches struggle with generating coherent, high-quality content in an
efficient and user-friendly manner. Concretely, baseline methods relying on
only 2D pose guidance lack the cues of 3D information, leading to suboptimal
results, while methods using 3D representation as guidance achieve higher
quality but involve a cumbersome and time-intensive process. To address these
limitations, DreamDance enriches 3D geometry cues from 2D poses by introducing
an efficient diffusion model, enabling high-quality human image animation with
various guidance. Our key insight is that human images naturally exhibit
multiple levels of correlation, progressing from coarse skeleton poses to
fine-grained geometry cues, and further from these geometry cues to explicit
appearance details. Capturing such correlations could enrich the guidance
signals, facilitating intra-frame coherency and inter-frame consistency.
Specifically, we construct the TikTok-Dance5K dataset, comprising 5K
high-quality dance videos with detailed frame annotations, including human
pose, depth, and normal maps. Next, we introduce a Mutually Aligned Geometry
Diffusion Model to generate fine-grained depth and normal maps for enriched
guidance. Finally, a Cross-domain Controller incorporates multi-level guidance
to animate human images effectively with a video diffusion model. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
in animating human images.",2024-11-30 08:42:13+00:00,"['Yatian Pang', 'Bin Zhu', 'Bin Lin', 'Mingzhe Zheng', 'Francis E. H. Tay', 'Ser-Nam Lim', 'Harry Yang', 'Li Yuan']",http://arxiv.org/abs/2412.00397v1
Approximately Invertible Neural Network for Learned Image Compression,"Learned image compression have attracted considerable interests in recent
years. It typically comprises an analysis transform, a synthesis transform,
quantization and an entropy coding model. The analysis transform and synthesis
transform are used to encode an image to latent feature and decode the
quantized feature to reconstruct the image, and can be regarded as coupled
transforms. However, the analysis transform and synthesis transform are
designed independently in the existing methods, making them unreliable in
high-quality image compression. Inspired by the invertible neural networks in
generative modeling, invertible modules are used to construct the coupled
analysis and synthesis transforms. Considering the noise introduced in the
feature quantization invalidates the invertible process, this paper proposes an
Approximately Invertible Neural Network (A-INN) framework for learned image
compression. It formulates the rate-distortion optimization in lossy image
compression when using INN with quantization, which differentiates from using
INN for generative modelling. Generally speaking, A-INN can be used as the
theoretical foundation for any INN based lossy compression method. Based on
this formulation, A-INN with a progressive denoising module (PDM) is developed
to effectively reduce the quantization noise in the decoding. Moreover, a
Cascaded Feature Recovery Module (CFRM) is designed to learn high-dimensional
feature recovery from low-dimensional ones to further reduce the noise in
feature channel compression. In addition, a Frequency-enhanced Decomposition
and Synthesis Module (FDSM) is developed by explicitly enhancing the
high-frequency components in an image to address the loss of high-frequency
information inherent in neural network based image compression. Extensive
experiments demonstrate that the proposed A-INN outperforms the existing
learned image compression methods.",2024-08-30 07:57:47+00:00,"['Yanbo Gao', 'Meng Fu', 'Shuai Li', 'Chong Lv', 'Xun Cai', 'Hui Yuan', 'Mao Ye']",http://arxiv.org/abs/2408.17073v1
Text-Driven Tumor Synthesis,"Tumor synthesis can generate examples that AI often misses or over-detects,
improving AI performance by training on these challenging cases. However,
existing synthesis methods, which are typically unconditional -- generating
images from random variables -- or conditioned only by tumor shapes, lack
controllability over specific tumor characteristics such as texture,
heterogeneity, boundaries, and pathology type. As a result, the generated
tumors may be overly similar or duplicates of existing training data, failing
to effectively address AI's weaknesses. We propose a new text-driven tumor
synthesis approach, termed TextoMorph, that provides textual control over tumor
characteristics. This is particularly beneficial for examples that confuse the
AI the most, such as early tumor detection (increasing Sensitivity by +8.5%),
tumor segmentation for precise radiotherapy (increasing DSC by +6.3%), and
classification between benign and malignant tumors (improving Sensitivity by
+8.2%). By incorporating text mined from radiology reports into the synthesis
process, we increase the variability and controllability of the synthetic
tumors to target AI's failure cases more precisely. Moreover, TextoMorph uses
contrastive learning across different texts and CT scans, significantly
reducing dependence on scarce image-report pairs (only 141 pairs used in this
study) by leveraging a large corpus of 34,035 radiology reports. Finally, we
have developed rigorous tests to evaluate synthetic tumors, including
Text-Driven Visual Turing Test and Radiomics Pattern Analysis, showing that our
synthetic tumors is realistic and diverse in texture, heterogeneity,
boundaries, and pathology.",2024-12-24 18:43:09+00:00,"['Xinran Li', 'Yi Shuai', 'Chen Liu', 'Qi Chen', 'Qilong Wu', 'Pengfei Guo', 'Dong Yang', 'Can Zhao', 'Pedro R. A. S. Bassi', 'Daguang Xu', 'Kang Wang', 'Yang Yang', 'Alan Yuille', 'Zongwei Zhou']",http://arxiv.org/abs/2412.18589v1
Fine-grained Dynamic Network for Generic Event Boundary Detection,"Generic event boundary detection (GEBD) aims at pinpointing event boundaries
naturally perceived by humans, playing a crucial role in understanding
long-form videos. Given the diverse nature of generic boundaries, spanning
different video appearances, objects, and actions, this task remains
challenging. Existing methods usually detect various boundaries by the same
protocol, regardless of their distinctive characteristics and detection
difficulties, resulting in suboptimal performance. Intuitively, a more
intelligent and reasonable way is to adaptively detect boundaries by
considering their special properties. In light of this, we propose a novel
dynamic pipeline for generic event boundaries named DyBDet. By introducing a
multi-exit network architecture, DyBDet automatically learns the subnet
allocation to different video snippets, enabling fine-grained detection for
various boundaries. Besides, a multi-order difference detector is also proposed
to ensure generic boundaries can be effectively identified and adaptively
processed. Extensive experiments on the challenging Kinetics-GEBD and TAPOS
datasets demonstrate that adopting the dynamic strategy significantly benefits
GEBD tasks, leading to obvious improvements in both performance and efficiency
compared to the current state-of-the-art.",2024-07-05 06:02:46+00:00,"['Ziwei Zheng', 'Lijun He', 'Le Yang', 'Fan Li']",http://arxiv.org/abs/2407.04274v1
Large Generative Model-assisted Talking-face Semantic Communication System,"The rapid development of generative Artificial Intelligence (AI) continually
unveils the potential of Semantic Communication (SemCom). However, current
talking-face SemCom systems still encounter challenges such as low bandwidth
utilization, semantic ambiguity, and diminished Quality of Experience (QoE).
This study introduces a Large Generative Model-assisted Talking-face Semantic
Communication (LGM-TSC) System tailored for the talking-face video
communication. Firstly, we introduce a Generative Semantic Extractor (GSE) at
the transmitter based on the FunASR model to convert semantically sparse
talking-face videos into texts with high information density. Secondly, we
establish a private Knowledge Base (KB) based on the Large Language Model (LLM)
for semantic disambiguation and correction, complemented by a joint knowledge
base-semantic-channel coding scheme. Finally, at the receiver, we propose a
Generative Semantic Reconstructor (GSR) that utilizes BERT-VITS2 and SadTalker
models to transform text back into a high-QoE talking-face video matching the
user's timbre. Simulation results demonstrate the feasibility and effectiveness
of the proposed LGM-TSC system.",2024-11-06 12:45:46+00:00,"['Feibo Jiang', 'Siwei Tu', 'Li Dong', 'Cunhua Pan', 'Jiangzhou Wang', 'Xiaohu You']",http://arxiv.org/abs/2411.03876v1
MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large Language Models,"Research on large language models has advanced significantly across text,
speech, images, and videos. However, multi-modal music understanding and
generation remain underexplored due to the lack of well-annotated datasets. To
address this, we introduce a dataset with 167.69 hours of multi-modal data,
including text, images, videos, and music annotations. Based on this dataset,
we propose MuMu-LLaMA, a model that leverages pre-trained encoders for music,
images, and videos. For music generation, we integrate AudioLDM 2 and MusicGen.
Our evaluation across four tasks--music understanding, text-to-music
generation, prompt-based music editing, and multi-modal music
generation--demonstrates that MuMu-LLaMA outperforms state-of-the-art models,
showing its potential for multi-modal music applications.",2024-12-09 16:59:35+00:00,"['Shansong Liu', 'Atin Sakkeer Hussain', 'Qilong Wu', 'Chenshuo Sun', 'Ying Shan']",http://arxiv.org/abs/2412.06660v1
Attention Normalization Impacts Cardinality Generalization in Slot Attention,"Object-centric scene decompositions are important representations for
downstream tasks in fields such as computer vision and robotics. The recently
proposed Slot Attention module, already leveraged by several derivative works
for image segmentation and object tracking in videos, is a deep learning
component which performs unsupervised object-centric scene decomposition on
input images. It is based on an attention architecture, in which latent slot
vectors, which hold compressed information on objects, attend to localized
perceptual features from the input image. In this paper, we demonstrate that
design decisions on normalizing the aggregated values in the attention
architecture have considerable impact on the capabilities of Slot Attention to
generalize to a higher number of slots and objects as seen during training. We
propose and investigate alternatives to the original normalization scheme which
increase the generalization capabilities of Slot Attention to varying slot and
object counts, resulting in performance gains on the task of unsupervised image
segmentation. The newly proposed normalizations represent minimal and easy to
implement modifications of the usual Slot Attention module, changing the value
aggregation mechanism from a weighted mean operation to a scaled weighted sum
operation.",2024-07-04 22:09:01+00:00,"['Markus Krimmel', 'Jan Achterhold', 'Joerg Stueckler']",http://arxiv.org/abs/2407.04170v2
PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation,"Realistic object interactions are crucial for creating immersive virtual
experiences, yet synthesizing realistic 3D object dynamics in response to novel
interactions remains a significant challenge. Unlike unconditional or
text-conditioned dynamics generation, action-conditioned dynamics requires
perceiving the physical material properties of objects and grounding the 3D
motion prediction on these properties, such as object stiffness. However,
estimating physical material properties is an open problem due to the lack of
material ground-truth data, as measuring these properties for real objects is
highly difficult. We present PhysDreamer, a physics-based approach that endows
static 3D objects with interactive dynamics by leveraging the object dynamics
priors learned by video generation models. By distilling these priors,
PhysDreamer enables the synthesis of realistic object responses to novel
interactions, such as external forces or agent manipulations. We demonstrate
our approach on diverse examples of elastic objects and evaluate the realism of
the synthesized interactions through a user study. PhysDreamer takes a step
towards more engaging and realistic virtual experiences by enabling static 3D
objects to dynamically respond to interactive stimuli in a physically plausible
manner. See our project page at https://physdreamer.github.io/.",2024-04-19 17:41:05+00:00,"['Tianyuan Zhang', 'Hong-Xing Yu', 'Rundi Wu', 'Brandon Y. Feng', 'Changxi Zheng', 'Noah Snavely', 'Jiajun Wu', 'William T. Freeman']",http://arxiv.org/abs/2404.13026v2
EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head,"We present a novel approach for synthesizing 3D talking heads with
controllable emotion, featuring enhanced lip synchronization and rendering
quality. Despite significant progress in the field, prior methods still suffer
from multi-view consistency and a lack of emotional expressiveness. To address
these issues, we collect EmoTalk3D dataset with calibrated multi-view videos,
emotional annotations, and per-frame 3D geometry. By training on the EmoTalk3D
dataset, we propose a \textit{`Speech-to-Geometry-to-Appearance'} mapping
framework that first predicts faithful 3D geometry sequence from the audio
features, then the appearance of a 3D talking head represented by 4D Gaussians
is synthesized from the predicted geometry. The appearance is further
disentangled into canonical and dynamic Gaussians, learned from multi-view
videos, and fused to render free-view talking head animation. Moreover, our
model enables controllable emotion in the generated talking heads and can be
rendered in wide-range views. Our method exhibits improved rendering quality
and stability in lip motion generation while capturing dynamic facial details
such as wrinkles and subtle expressions. Experiments demonstrate the
effectiveness of our approach in generating high-fidelity and
emotion-controllable 3D talking heads. The code and EmoTalk3D dataset are
released at https://nju-3dv.github.io/projects/EmoTalk3D.",2024-08-01 05:46:57+00:00,"['Qianyun He', 'Xinya Ji', 'Yicheng Gong', 'Yuanxun Lu', 'Zhengyu Diao', 'Linjia Huang', 'Yao Yao', 'Siyu Zhu', 'Zhan Ma', 'Songcen Xu', 'Xiaofei Wu', 'Zixiao Zhang', 'Xun Cao', 'Hao Zhu']",http://arxiv.org/abs/2408.00297v1
Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance,"In this study, we introduce a methodology for human image animation by
leveraging a 3D human parametric model within a latent diffusion framework to
enhance shape alignment and motion guidance in curernt human generative
techniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear)
model as the 3D human parametric model to establish a unified representation of
body shape and pose. This facilitates the accurate capture of intricate human
geometry and motion characteristics from source videos. Specifically, we
incorporate rendered depth images, normal maps, and semantic maps obtained from
SMPL sequences, alongside skeleton-based motion guidance, to enrich the
conditions to the latent diffusion model with comprehensive 3D shape and
detailed pose attributes. A multi-layer motion fusion module, integrating
self-attention mechanisms, is employed to fuse the shape and motion latent
representations in the spatial domain. By representing the 3D human parametric
model as the motion guidance, we can perform parametric shape alignment of the
human body between the reference image and the source video motion.
Experimental evaluations conducted on benchmark datasets demonstrate the
methodology's superior ability to generate high-quality human animations that
accurately capture both pose and shape variations. Furthermore, our approach
also exhibits superior generalization capabilities on the proposed in-the-wild
dataset. Project page: https://fudan-generative-vision.github.io/champ.",2024-03-21 18:52:58+00:00,"['Shenhao Zhu', 'Junming Leo Chen', 'Zuozhuo Dai', 'Qingkun Su', 'Yinghui Xu', 'Xun Cao', 'Yao Yao', 'Hao Zhu', 'Siyu Zhu']",http://arxiv.org/abs/2403.14781v2
Contextual AD Narration with Interleaved Multimodal Sequence,"The Audio Description (AD) task aims to generate descriptions of visual
elements for visually impaired individuals to help them access long-form video
contents, like movie. With video feature, text, character bank and context
information as inputs, the generated ADs are able to correspond to the
characters by name and provide reasonable, contextual descriptions to help
audience understand the storyline of movie. To achieve this goal, we propose to
leverage pre-trained foundation models through a simple and unified framework
to generate ADs with interleaved multimodal sequence as input, termed as
Uni-AD. To enhance the alignment of features across various modalities with
finer granularity, we introduce a simple and lightweight module that maps video
features into the textual feature space. Moreover, we also propose a
character-refinement module to provide more precise information by identifying
the main characters who play more significant role in the video context. With
these unique designs, we further incorporate contextual information and a
contrastive loss into our architecture to generate more smooth and contextual
ADs. Experiments on the MAD-eval dataset show that Uni-AD can achieve
state-of-the-art performance on AD generation, which demonstrates the
effectiveness of our approach. Code will be available at
https://github.com/MCG-NJU/Uni-AD.",2024-03-19 17:27:55+00:00,"['Hanlin Wang', 'Zhan Tong', 'Kecheng Zheng', 'Yujun Shen', 'Limin Wang']",http://arxiv.org/abs/2403.12922v1
UniForensics: Face Forgery Detection via General Facial Representation,"Previous deepfake detection methods mostly depend on low-level textural
features vulnerable to perturbations and fall short of detecting unseen forgery
methods. In contrast, high-level semantic features are less susceptible to
perturbations and not limited to forgery-specific artifacts, thus having
stronger generalization. Motivated by this, we propose a detection method that
utilizes high-level semantic features of faces to identify inconsistencies in
temporal domain. We introduce UniForensics, a novel deepfake detection
framework that leverages a transformer-based video classification network,
initialized with a meta-functional face encoder for enriched facial
representation. In this way, we can take advantage of both the powerful
spatio-temporal model and the high-level semantic information of faces.
Furthermore, to leverage easily accessible real face data and guide the model
in focusing on spatio-temporal features, we design a Dynamic Video
Self-Blending (DVSB) method to efficiently generate training samples with
diverse spatio-temporal forgery traces using real facial videos. Based on this,
we advance our framework with a two-stage training approach: The first stage
employs a novel self-supervised contrastive learning, where we encourage the
network to focus on forgery traces by impelling videos generated by the same
forgery process to have similar representations. On the basis of the
representation learned in the first stage, the second stage involves
fine-tuning on face forgery detection dataset to build a deepfake detector.
Extensive experiments validates that UniForensics outperforms existing face
forgery methods in generalization ability and robustness. In particular, our
method achieves 95.3\% and 77.2\% cross dataset AUC on the challenging
Celeb-DFv2 and DFDC respectively.",2024-07-26 20:51:54+00:00,"['Ziyuan Fang', 'Hanqing Zhao', 'Tianyi Wei', 'Wenbo Zhou', 'Ming Wan', 'Zhanyi Wang', 'Weiming Zhang', 'Nenghai Yu']",http://arxiv.org/abs/2407.19079v1
TKG-DM: Training-free Chroma Key Content Generation Diffusion Model,"Diffusion models have enabled the generation of high-quality images with a
strong focus on realism and textual fidelity. Yet, large-scale text-to-image
models, such as Stable Diffusion, struggle to generate images where foreground
objects are placed over a chroma key background, limiting their ability to
separate foreground and background elements without fine-tuning. To address
this limitation, we present a novel Training-Free Chroma Key Content Generation
Diffusion Model (TKG-DM), which optimizes the initial random noise to produce
images with foreground objects on a specifiable color background. Our proposed
method is the first to explore the manipulation of the color aspects in initial
noise for controlled background generation, enabling precise separation of
foreground and background without fine-tuning. Extensive experiments
demonstrate that our training-free method outperforms existing methods in both
qualitative and quantitative evaluations, matching or surpassing fine-tuned
models. Finally, we successfully extend it to other tasks (e.g., consistency
models and text-to-video), highlighting its transformative potential across
various generative applications where independent control of foreground and
background is crucial.",2024-11-23 15:07:15+00:00,"['Ryugo Morita', 'Stanislav Frolov', 'Brian Bernhard Moser', 'Takahiro Shirakawa', 'Ko Watanabe', 'Andreas Dengel', 'Jinjia Zhou']",http://arxiv.org/abs/2411.15580v2
Domain Generalization for 6D Pose Estimation Through NeRF-based Image Synthesis,"This work introduces a novel augmentation method that increases the diversity
of a train set to improve the generalization abilities of a 6D pose estimation
network. For this purpose, a Neural Radiance Field is trained from synthetic
images and exploited to generate an augmented set. Our method enriches the
initial set by enabling the synthesis of images with (i) unseen viewpoints,
(ii) rich illumination conditions through appearance extrapolation, and (iii)
randomized textures. We validate our augmentation method on the challenging
use-case of spacecraft pose estimation and show that it significantly improves
the pose estimation generalization capabilities. On the SPEED+ dataset, our
method reduces the error on the pose by 50% on both target domains.",2024-07-15 14:42:08+00:00,"['Antoine Legrand', 'Renaud Detry', 'Christophe De Vleeschouwer']",http://arxiv.org/abs/2407.10762v1
FUSECAPS: Investigating Feature Fusion Based Framework for Capsule Endoscopy Image Classification,"In order to improve model accuracy, generalization, and class imbalance
issues, this work offers a strong methodology for classifying endoscopic
images. We suggest a hybrid feature extraction method that combines
convolutional neural networks (CNNs), multi-layer perceptrons (MLPs), and
radiomics. Rich, multi-scale feature extraction is made possible by this
combination, which captures both deep and handmade representations. These
features are then used by a classification head to classify diseases, producing
a model with higher generalization and accuracy. In this framework we have
achieved a validation accuracy of 76.2% in the capsule endoscopy video frame
classification task.",2024-11-04 21:55:52+00:00,"['Bidisha Chakraborty', 'Shree Mitra']",http://arxiv.org/abs/2411.02637v1
Explicit-NeRF-QA: A Quality Assessment Database for Explicit NeRF Model Compression,"In recent years, Neural Radiance Fields (NeRF) have demonstrated significant
advantages in representing and synthesizing 3D scenes. Explicit NeRF models
facilitate the practical NeRF applications with faster rendering speed, and
also attract considerable attention in NeRF compression due to its huge storage
cost. To address the challenge of the NeRF compression study, in this paper, we
construct a new dataset, called Explicit-NeRF-QA. We use 22 3D objects with
diverse geometries, textures, and material complexities to train four typical
explicit NeRF models across five parameter levels. Lossy compression is
introduced during the model generation, pivoting the selection of key
parameters such as hash table size for InstantNGP and voxel grid resolution for
Plenoxels. By rendering NeRF samples to processed video sequences (PVS), a
large scale subjective experiment with lab environment is conducted to collect
subjective scores from 21 viewers. The diversity of content, accuracy of mean
opinion scores (MOS), and characteristics of NeRF distortion are
comprehensively presented, establishing the heterogeneity of the proposed
dataset. The state-of-the-art objective metrics are tested in the new dataset.
Best Person correlation, which is around 0.85, is collected from the
full-reference objective metric. All tested no-reference metrics report very
poor results with 0.4 to 0.6 correlations, demonstrating the need for further
development of more robust no-reference metrics. The dataset, including NeRF
samples, source 3D objects, multiview images for NeRF generation, PVSs, MOS, is
made publicly available at the following location:
https://github.com/YukeXing/Explicit-NeRF-QA.",2024-07-11 04:02:05+00:00,"['Yuke Xing', 'Qi Yang', 'Kaifa Yang', 'Yilin Xu', 'Zhu Li']",http://arxiv.org/abs/2407.08165v3
UniRS: Unifying Multi-temporal Remote Sensing Tasks through Vision Language Models,"The domain gap between remote sensing imagery and natural images has recently
received widespread attention and Vision-Language Models (VLMs) have
demonstrated excellent generalization performance in remote sensing multimodal
tasks. However, current research is still limited in exploring how remote
sensing VLMs handle different types of visual inputs. To bridge this gap, we
introduce \textbf{UniRS}, the first vision-language model \textbf{uni}fying
multi-temporal \textbf{r}emote \textbf{s}ensing tasks across various types of
visual input. UniRS supports single images, dual-time image pairs, and videos
as input, enabling comprehensive remote sensing temporal analysis within a
unified framework. We adopt a unified visual representation approach, enabling
the model to accept various visual inputs. For dual-time image pair tasks, we
customize a change extraction module to further enhance the extraction of
spatiotemporal features. Additionally, we design a prompt augmentation
mechanism tailored to the model's reasoning process, utilizing the prior
knowledge of the general-purpose VLM to provide clues for UniRS. To promote
multi-task knowledge sharing, the model is jointly fine-tuned on a mixed
dataset. Experimental results show that UniRS achieves state-of-the-art
performance across diverse tasks, including visual question answering, change
captioning, and video scene classification, highlighting its versatility and
effectiveness in unifying these multi-temporal remote sensing tasks. Our code
and dataset will be released soon.",2024-12-30 06:34:18+00:00,"['Yujie Li', 'Wenjia Xu', 'Guangzuo Li', 'Zijian Yu', 'Zhiwei Wei', 'Jiuniu Wang', 'Mugen Peng']",http://arxiv.org/abs/2412.20742v1
Arges: Spatio-Temporal Transformer for Ulcerative Colitis Severity Assessment in Endoscopy Videos,"Accurate assessment of disease severity from endoscopy videos in ulcerative
colitis (UC) is crucial for evaluating drug efficacy in clinical trials.
Severity is often measured by the Mayo Endoscopic Subscore (MES) and Ulcerative
Colitis Endoscopic Index of Severity (UCEIS) score. However, expert MES/UCEIS
annotation is time-consuming and susceptible to inter-rater variability,
factors addressable by automation. Automation attempts with frame-level labels
face challenges in fully-supervised solutions due to the prevalence of
video-level labels in clinical trials. CNN-based weakly-supervised models (WSL)
with end-to-end (e2e) training lack generalization to new disease scores and
ignore spatio-temporal information crucial for accurate scoring. To address
these limitations, we propose ""Arges"", a deep learning framework that utilizes
a transformer with positional encoding to incorporate spatio-temporal
information from frame features to estimate disease severity scores in
endoscopy video. Extracted features are derived from a foundation model
(ArgesFM), pre-trained on a large diverse dataset from multiple clinical trials
(61M frames, 3927 videos). We evaluate four UC disease severity scores,
including MES and three UCEIS component scores. Test set evaluation indicates
significant improvements, with F1 scores increasing by 4.1% for MES and 18.8%,
6.6%, 3.8% for the three UCEIS component scores compared to state-of-the-art
methods. Prospective validation on previously unseen clinical trial data
further demonstrates the model's successful generalization.",2024-10-01 09:23:14+00:00,"['Krishna Chaitanya', 'Pablo F. Damasceno', 'Shreyas Fadnavis', 'Pooya Mobadersany', 'Chaitanya Parmar', 'Emily Scherer', 'Natalia Zemlianskaia', 'Lindsey Surace', 'Louis R. Ghanem', 'Oana Gabriela Cula', 'Tommaso Mansi', 'Kristopher Standish']",http://arxiv.org/abs/2410.00536v1
Towards nation-wide analytical healthcare infrastructures: A privacy-preserving augmented knee rehabilitation case study,"The purpose of this paper is to contribute towards the near-future
privacy-preserving big data analytical healthcare platforms, capable of
processing streamed or uploaded timeseries data or videos from patients. The
experimental work includes a real-life knee rehabilitation video dataset
capturing a set of exercises from simple and personalised to more general and
challenging movements aimed for returning to sport. To convert video from
mobile into privacy-preserving diagnostic timeseries data, we employed Google
MediaPipe pose estimation. The developed proof-of-concept algorithms can
augment knee exercise videos by overlaying the patient with stick figure
elements while updating generated timeseries plot with knee angle estimation
streamed as CSV file format. For patients and physiotherapists, video with
side-to-side timeseries visually indicating potential issues such as excessive
knee flexion or unstable knee movements or stick figure overlay errors is
possible by setting a-priori knee-angle parameters. To address adherence to
rehabilitation programme and quantify exercise sets and repetitions, our
adaptive algorithm can correctly identify (91.67%-100%) of all exercises from
side- and front-view videos. Transparent algorithm design for adaptive visual
analysis of various knee exercise patterns contributes towards the
interpretable AI and will inform near-future privacy-preserving, non-vendor
locking, open-source developments for both end-user computing devices and as
on-premises non-proprietary cloud platforms that can be deployed within the
national healthcare system.",2024-12-30 06:14:48+00:00,"['Boris Bai', 'Claudiu Vasile', 'Chengwei Feng', 'Marian G. Ciuc']",http://arxiv.org/abs/2412.20733v1
Tuning-Free Visual Customization via View Iterative Self-Attention Control,"Fine-Tuning Diffusion Models enable a wide range of personalized generation
and editing applications on diverse visual modalities. While Low-Rank
Adaptation (LoRA) accelerates the fine-tuning process, it still requires
multiple reference images and time-consuming training, which constrains its
scalability for large-scale and real-time applications. In this paper, we
propose \textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this
challenge. Specifically, VisCtrl is a training-free method that injects the
appearance and structure of a user-specified subject into another subject in
the target image, unlike previous approaches that require fine-tuning the
model. Initially, we obtain the initial noise for both the reference and target
images through DDIM inversion. Then, during the denoising phase, features from
the reference image are injected into the target image via the self-attention
mechanism. Notably, by iteratively performing this feature injection process,
we ensure that the reference image features are gradually integrated into the
target image. This approach results in consistent and harmonious editing with
only one reference image in a few denoising steps. Moreover, benefiting from
our plug-and-play architecture design and the proposed Feature Gradual Sampling
strategy for multi-view editing, our method can be easily extended to edit in
complex visual domains. Extensive experiments show the efficacy of VisCtrl
across a spectrum of tasks, including personalized editing of images, videos,
and 3D scenes.",2024-06-10 13:41:10+00:00,"['Xiaojie Li', 'Chenghao Gu', 'Shuzhao Xie', 'Yunpeng Bai', 'Weixiang Zhang', 'Zhi Wang']",http://arxiv.org/abs/2406.06258v2
Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection,"The high performance of denoising diffusion models for image generation has
paved the way for their application in unsupervised medical anomaly detection.
As diffusion-based methods require a lot of GPU memory and have long sampling
times, we present a novel and fast unsupervised anomaly detection approach
based on latent Bernoulli diffusion models. We first apply an autoencoder to
compress the input images into a binary latent representation. Next, a
diffusion model that follows a Bernoulli noise schedule is employed to this
latent space and trained to restore binary latent representations from
perturbed ones. The binary nature of this diffusion model allows us to identify
entries in the latent space that have a high probability of flipping their
binary code during the denoising process, which indicates out-of-distribution
data. We propose a masking algorithm based on these probabilities, which
improves the anomaly detection scores. We achieve state-of-the-art performance
compared to other diffusion-based unsupervised anomaly detection algorithms
while significantly reducing sampling time and memory consumption. The code is
available at https://github.com/JuliaWolleb/Anomaly_berdiff.",2024-03-18 11:15:03+00:00,"['Julia Wolleb', 'Florentin Bieder', 'Paul Friedrich', 'Peter Zhang', 'Alicia Durrer', 'Philippe C. Cattin']",http://arxiv.org/abs/2403.11667v1
Gaussian is All You Need: A Unified Framework for Solving Inverse Problems via Diffusion Posterior Sampling,"Diffusion models can generate a variety of high-quality images by modeling
complex data distributions. Trained diffusion models can also be very effective
image priors for solving inverse problems. Most of the existing diffusion-based
methods integrate data consistency steps within the diffusion reverse sampling
process. The data consistency steps rely on an approximate likelihood function.
In this paper, we show that the existing approximations are either insufficient
or computationally inefficient. To address these issues, we propose a unified
likelihood approximation method that incorporates a covariance correction term
to enhance the performance and avoids propagating gradients through the
diffusion model. The correction term, when integrated into the reverse
diffusion sampling process, achieves better convergence towards the true data
posterior for selected distributions and improves performance on real-world
natural image datasets. Furthermore, we present an efficient way to factorize
and invert the covariance matrix of the likelihood function for several inverse
problems. We present comprehensive experiments to demonstrate the effectiveness
of our method over several existing approaches.",2024-09-13 15:20:03+00:00,"['Nebiyou Yismaw', 'Ulugbek S. Kamilov', 'M. Salman Asif']",http://arxiv.org/abs/2409.08906v1
Stable Diffusion Segmentation for Biomedical Images with Single-step Reverse Process,"Diffusion models have demonstrated their effectiveness across various
generative tasks. However, when applied to medical image segmentation, these
models encounter several challenges, including significant resource and time
requirements. They also necessitate a multi-step reverse process and multiple
samples to produce reliable predictions. To address these challenges, we
introduce the first latent diffusion segmentation model, named SDSeg, built
upon stable diffusion (SD). SDSeg incorporates a straightforward latent
estimation strategy to facilitate a single-step reverse process and utilizes
latent fusion concatenation to remove the necessity for multiple samples.
Extensive experiments indicate that SDSeg surpasses existing state-of-the-art
methods on five benchmark datasets featuring diverse imaging modalities.
Remarkably, SDSeg is capable of generating stable predictions with a solitary
reverse step and sample, epitomizing the model's stability as implied by its
name. The code is available at
https://github.com/lin-tianyu/Stable-Diffusion-Seg",2024-06-26 14:01:07+00:00,"['Tianyu Lin', 'Zhiguang Chen', 'Zhonghao Yan', 'Weijiang Yu', 'Fudan Zheng']",http://arxiv.org/abs/2406.18361v3
"Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via Editable Gaussian Splatting","We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic
manipulation, which leverages the editability of Gaussian Splatting (GSplat)
scene representations to enable multi-stage manipulation tasks. Splat-MOVER
consists of: (i) ASK-Splat, a GSplat representation that distills semantic and
grasp affordance features into the 3D scene. ASK-Splat enables geometric,
semantic, and affordance understanding of 3D scenes, which is critical in many
robotics tasks; (ii) SEE-Splat, a real-time scene-editing module using 3D
semantic masking and infilling to visualize the motions of objects that result
from robot interactions in the real-world. SEE-Splat creates a ""digital twin""
of the evolving environment throughout the manipulation task; and (iii)
Grasp-Splat, a grasp generation module that uses ASK-Splat and SEE-Splat to
propose affordance-aligned candidate grasps for open-world objects. ASK-Splat
is trained in real-time from RGB images in a brief scanning phase prior to
operation, while SEE-Splat and Grasp-Splat run in real-time during operation.
We demonstrate the superior performance of Splat-MOVER in hardware experiments
on a Kinova robot compared to two recent baselines in four single-stage,
open-vocabulary manipulation tasks and in four multi-stage manipulation tasks,
using the edited scene to reflect changes due to prior manipulation stages,
which is not possible with existing baselines. Video demonstrations and the
code for the project are available at https://splatmover.github.io.",2024-05-07 15:00:19+00:00,"['Ola Shorinwa', 'Johnathan Tucker', 'Aliyah Smith', 'Aiden Swann', 'Timothy Chen', 'Roya Firoozi', 'Monroe Kennedy III', 'Mac Schwager']",http://arxiv.org/abs/2405.04378v4
Crowd-Sourced NeRF: Collecting Data from Production Vehicles for 3D Street View Reconstruction,"Recently, Neural Radiance Fields (NeRF) achieved impressive results in novel
view synthesis. Block-NeRF showed the capability of leveraging NeRF to build
large city-scale models. For large-scale modeling, a mass of image data is
necessary. Collecting images from specially designed data-collection vehicles
can not support large-scale applications. How to acquire massive high-quality
data remains an opening problem. Noting that the automotive industry has a huge
amount of image data, crowd-sourcing is a convenient way for large-scale data
collection. In this paper, we present a crowd-sourced framework, which utilizes
substantial data captured by production vehicles to reconstruct the scene with
the NeRF model. This approach solves the key problem of large-scale
reconstruction, that is where the data comes from and how to use them. Firstly,
the crowd-sourced massive data is filtered to remove redundancy and keep a
balanced distribution in terms of time and space. Then a structure-from-motion
module is performed to refine camera poses. Finally, images, as well as poses,
are used to train the NeRF model in a certain block. We highlight that we
present a comprehensive framework that integrates multiple modules, including
data selection, sparse 3D reconstruction, sequence appearance embedding, depth
supervision of ground surface, and occlusion completion. The complete system is
capable of effectively processing and reconstructing high-quality 3D scenes
from crowd-sourced data. Extensive quantitative and qualitative experiments
were conducted to validate the performance of our system. Moreover, we proposed
an application, named first-view navigation, which leveraged the NeRF model to
generate 3D street view and guide the driver with a synthesized video.",2024-06-24 03:30:20+00:00,"['Tong Qin', 'Changze Li', 'Haoyang Ye', 'Shaowei Wan', 'Minzhen Li', 'Hongwei Liu', 'Ming Yang']",http://arxiv.org/abs/2406.16289v1
AgentStudio: A Toolkit for Building General Virtual Agents,"General virtual agents need to handle multimodal observations, master complex
action spaces, and self-improve in dynamic, open-domain environments. However,
existing environments are often domain-specific and require complex setups,
which limits agent development and evaluation in real-world settings. As a
result, current evaluations lack in-depth analyses that decompose fundamental
agent capabilities. We introduce AgentStudio, a trinity of environments, tools,
and benchmarks to address these issues. AgentStudio provides a lightweight,
interactive environment with highly generic observation and action spaces,
e.g., video observations and GUI/API actions. It integrates tools for creating
online benchmark tasks, annotating GUI elements, and labeling actions in
videos. Based on our environment and tools, we curate an online task suite that
benchmarks both GUI interactions and function calling with efficient
auto-evaluation. We also reorganize existing datasets and collect new ones
using our tools to establish three datasets: GroundUI, IDMBench, and
CriticBench. These datasets evaluate fundamental agent abilities, including GUI
grounding, learning from videos, and success detection, pointing to the
desiderata for robust, general, and open-ended virtual agents.",2024-03-26 17:54:15+00:00,"['Longtao Zheng', 'Zhiyuan Huang', 'Zhenghai Xue', 'Xinrun Wang', 'Bo An', 'Shuicheng Yan']",http://arxiv.org/abs/2403.17918v3
SSNVC: Single Stream Neural Video Compression with Implicit Temporal Information,"Recently, Neural Video Compression (NVC) techniques have achieved remarkable
performance, even surpassing the best traditional lossy video codec. However,
most existing NVC methods heavily rely on transmitting Motion Vector (MV) to
generate accurate contextual features, which has the following drawbacks. (1)
Compressing and transmitting MV requires specialized MV encoder and decoder,
which makes modules redundant. (2) Due to the existence of MV Encoder-Decoder,
the training strategy is complex. In this paper, we present a noval Single
Stream NVC framework (SSNVC), which removes complex MV Encoder-Decoder
structure and uses a one-stage training strategy. SSNVC implicitly use temporal
information by adding previous entropy model feature to current entropy model
and using previous two frame to generate predicted motion information at the
decoder side. Besides, we enhance the frame generator to generate higher
quality reconstructed frame. Experiments demonstrate that SSNVC can achieve
state-of-the-art performance on multiple benchmarks, and can greatly simplify
compression process as well as training process.",2024-06-11 18:20:39+00:00,"['Feng Wang', 'Haihang Ruan', 'Zhihuang Xie', 'Ronggang Wang', 'Xiangyu Yue']",http://arxiv.org/abs/2406.07645v1
MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset,"Recent studies in speech-driven 3D talking head generation have achieved
convincing results in verbal articulations. However, generating accurate
lip-syncs degrades when applied to input speech in other languages, possibly
due to the lack of datasets covering a broad spectrum of facial movements
across languages. In this work, we introduce a novel task to generate 3D
talking heads from speeches of diverse languages. We collect a new multilingual
2D video dataset comprising over 420 hours of talking videos in 20 languages.
With our proposed dataset, we present a multilingually enhanced model that
incorporates language-specific style embeddings, enabling it to capture the
unique mouth movements associated with each language. Additionally, we present
a metric for assessing lip-sync accuracy in multilingual settings. We
demonstrate that training a 3D talking head model with our proposed dataset
significantly enhances its multilingual performance. Codes and datasets are
available at https://multi-talk.github.io/.",2024-06-20 12:52:46+00:00,"['Kim Sung-Bin', 'Lee Chae-Yeon', 'Gihun Son', 'Oh Hyun-Bin', 'Janghoon Ju', 'Suekyeong Nam', 'Tae-Hyun Oh']",http://arxiv.org/abs/2406.14272v1
'What did the Robot do in my Absence?' Video Foundation Models to Enhance Intermittent Supervision,"This paper investigates the application of Video Foundation Models (ViFMs)
for generating robot data summaries to enhance intermittent human supervision
of robot teams. We propose a novel framework that produces both generic and
query-driven summaries of long-duration robot vision data in three modalities:
storyboards, short videos, and text. Through a user study involving 30
participants, we evaluate the efficacy of these summary methods in allowing
operators to accurately retrieve the observations and actions that occurred
while the robot was operating without supervision over an extended duration (40
min). Our findings reveal that query-driven summaries significantly improve
retrieval accuracy compared to generic summaries or raw data, albeit with
increased task duration. Storyboards are found to be the most effective
presentation modality, especially for object-related queries. This work
represents, to our knowledge, the first zero-shot application of ViFMs for
generating multi-modal robot-to-human communication in intermittent supervision
contexts, demonstrating both the promise and limitations of these models in
human-robot interaction (HRI) scenarios.",2024-11-15 07:50:30+00:00,"['Kavindie Katuwandeniya', 'Leimin Tian', 'Dana Kuli']",http://arxiv.org/abs/2411.10016v1
Passive Deepfake Detection Across Multi-modalities: A Comprehensive Survey,"In recent years, deepfakes (DFs) have been utilized for malicious purposes,
such as individual impersonation, misinformation spreading, and artists' style
imitation, raising questions about ethical and security concerns. However,
existing surveys have focused on accuracy performance of passive DF detection
approaches for single modalities, such as image, video or audio. This
comprehensive survey explores passive approaches across multiple modalities,
including image, video, audio, and multi-modal domains, and extend our
discussion beyond detection accuracy, including generalization, robustness,
attribution, and interpretability. Additionally, we discuss threat models for
passive approaches, including potential adversarial strategies and different
levels of adversary knowledge and capabilities. We also highlights current
challenges in DF detection, including the lack of generalization across
different generative models, the need for comprehensive trustworthiness
evaluation, and the limitations of existing multi-modal approaches. Finally, we
propose future research directions that address these unexplored and emerging
issues in the field of passive DF detection, such as adaptive learning, dynamic
benchmark, holistic trustworthiness evaluation, and multi-modal detectors for
talking-face video generation.",2024-11-26 22:04:49+00:00,"['Hong-Hanh Nguyen-Le', 'Van-Tuan Tran', 'Dinh-Thuc Nguyen', 'Nhien-An Le-Khac']",http://arxiv.org/abs/2411.17911v1
A Study of Data Augmentation Techniques to Overcome Data Scarcity in Wound Classification using Deep Learning,"Chronic wounds are a significant burden on individuals and the healthcare
system, affecting millions of people and incurring high costs. Wound
classification using deep learning techniques is a promising approach for
faster diagnosis and treatment initiation. However, lack of high quality data
to train the ML models is a major challenge to realize the potential of ML in
wound care. In fact, data limitations are the biggest challenge in studies
using medical or forensic imaging today. We study data augmentation techniques
that can be used to overcome the data scarcity limitations and unlock the
potential of deep learning based solutions. In our study we explore a range of
data augmentation techniques from geometric transformations of wound images to
advanced GANs, to enrich and expand datasets. Using the Keras, Tensorflow, and
Pandas libraries, we implemented the data augmentation techniques that can
generate realistic wound images. We show that geometric data augmentation can
improve classification performance, F1 scores, by up to 11% on top of
state-of-the-art models, across several key classes of wounds. Our experiments
with GAN based augmentation prove the viability of using DE-GANs to generate
wound images with richer variations. Our study and results show that data
augmentation is a valuable privacy-preserving tool with huge potential to
overcome the data scarcity limitations and we believe it will be part of any
real-world ML-based wound care system.",2024-11-04 00:24:50+00:00,"['Harini Narayanan', 'Sindhu Ghanta']",http://arxiv.org/abs/2411.02456v1
A Hybrid Approach for COVID-19 Detection: Combining Wasserstein GAN with Transfer Learning,"COVID-19 is extremely contagious and its rapid growth has drawn attention
towards its early diagnosis. Early diagnosis of COVID-19 enables healthcare
professionals and government authorities to break the chain of transition and
flatten the epidemic curve. With the number of cases accelerating across the
developed world, COVID-19 induced Viral Pneumonia cases is a big challenge.
Overlapping of COVID-19 cases with Viral Pneumonia and other lung infections
with limited dataset and long training hours is a serious problem to cater.
Limited amount of data often results in over-fitting models and due to this
reason, model does not predict generalized results. To fill this gap, we
proposed GAN-based approach to synthesize images which later fed into the deep
learning models to classify images of COVID-19, Normal, and Viral Pneumonia.
Specifically, customized Wasserstein GAN is proposed to generate 19% more Chest
X-ray images as compare to the real images. This expanded dataset is then used
to train four proposed deep learning models: VGG-16, ResNet-50, GoogLeNet and
MNAST. The result showed that expanded dataset utilized deep learning models to
deliver high classification accuracies. In particular, VGG-16 achieved highest
accuracy of 99.17% among all four proposed schemes. Rest of the models like
ResNet-50, GoogLeNet and MNAST delivered 93.9%, 94.49% and 97.75% testing
accuracies respectively. Later, the efficiency of these models is compared with
the state of art models on the basis of accuracy. Further, our proposed models
can be applied to address the issue of scant datasets for any problem of image
analysis.",2024-11-10 09:09:41+00:00,"['Sumera Rounaq', 'Shahid Munir Shah', 'Mahmoud Aljawarneh']",http://arxiv.org/abs/2411.06397v2
Generative Iris Prior Embedded Transformer for Iris Restoration,"Iris restoration from complexly degraded iris images, aiming to improve iris
recognition performance, is a challenging problem. Due to the complex
degradation, directly training a convolutional neural network (CNN) without
prior cannot yield satisfactory results. In this work, we propose a generative
iris prior embedded Transformer model (Gformer), in which we build a
hierarchical encoder-decoder network employing Transformer block and generative
iris prior. First, we tame Transformer blocks to model long-range dependencies
in target images. Second, we pretrain an iris generative adversarial network
(GAN) to obtain the rich iris prior, and incorporate it into the iris
restoration process with our iris feature modulator. Our experiments
demonstrate that the proposed Gformer outperforms state-of-the-art methods.
Besides, iris recognition performance has been significantly improved after
applying Gformer.",2024-06-28 23:20:57+00:00,"['Yubo Huang', 'Jia Wang', 'Peipei Li', 'Liuyu Xiang', 'Peigang Li', 'Zhaofeng He']",http://arxiv.org/abs/2407.00261v2
HPix: Generating Vector Maps from Satellite Images,"Vector maps find widespread utility across diverse domains due to their
capacity to not only store but also represent discrete data boundaries such as
building footprints, disaster impact analysis, digitization, urban planning,
location points, transport links, and more. Although extensive research exists
on identifying building footprints and road types from satellite imagery, the
generation of vector maps from such imagery remains an area with limited
exploration. Furthermore, conventional map generation techniques rely on
labor-intensive manual feature extraction or rule-based approaches, which
impose inherent limitations. To surmount these limitations, we propose a novel
method called HPix, which utilizes modified Generative Adversarial Networks
(GANs) to generate vector tile map from satellite images. HPix incorporates two
hierarchical frameworks: one operating at the global level and the other at the
local level, resulting in a comprehensive model. Through empirical evaluations,
our proposed approach showcases its effectiveness in producing highly accurate
and visually captivating vector tile maps derived from satellite images. We
further extend our study's application to include mapping of road intersections
and building footprints cluster based on their area.",2024-07-18 16:54:02+00:00,"['Aditya Taparia', 'Keshab Nath']",http://arxiv.org/abs/2407.13680v1
Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms,"A comprehensive understanding of surgical scenes allows for monitoring of the
surgical process, reducing the occurrence of accidents and enhancing efficiency
for medical professionals. Semantic modeling within operating rooms, as a scene
graph generation (SGG) task, is challenging since it involves consecutive
recognition of subtle surgical actions over prolonged periods. To address this
challenge, we propose a Tri-modal (i.e., images, point clouds, and language)
confluence with Temporal dynamics framework, termed TriTemp-OR. Diverging from
previous approaches that integrated temporal information via memory graphs, our
method embraces two advantages: 1) we directly exploit bi-modal temporal
information from the video streaming for hierarchical feature interaction, and
2) the prior knowledge from Large Language Models (LLMs) is embedded to
alleviate the class-imbalance problem in the operating theatre. Specifically,
our model performs temporal interactions across 2D frames and 3D point clouds,
including a scale-adaptive multi-view temporal interaction (ViewTemp) and a
geometric-temporal point aggregation (PointTemp). Furthermore, we transfer
knowledge from the biomedical LLM, LLaVA-Med, to deepen the comprehension of
intraoperative relations. The proposed TriTemp-OR enables the aggregation of
tri-modal features through relation-aware unification to predict relations so
as to generate scene graphs. Experimental results on the 4D-OR benchmark
demonstrate the superior performance of our model for long-term OR streaming.",2024-04-14 12:19:16+00:00,"['Diandian Guo', 'Manxi Lin', 'Jialun Pei', 'He Tang', 'Yueming Jin', 'Pheng-Ann Heng']",http://arxiv.org/abs/2404.09231v1
VideoDistill: Language-aware Vision Distillation for Video Question Answering,"Significant advancements in video question answering (VideoQA) have been made
thanks to thriving large image-language pretraining frameworks. Although these
image-language models can efficiently represent both video and language
branches, they typically employ a goal-free vision perception process and do
not interact vision with language well during the answer generation, thus
omitting crucial visual cues. In this paper, we are inspired by the human
recognition and learning pattern and propose VideoDistill, a framework with
language-aware (i.e., goal-driven) behavior in both vision perception and
answer generation process. VideoDistill generates answers only from
question-related visual embeddings and follows a thinking-observing-answering
approach that closely resembles human behavior, distinguishing it from previous
research. Specifically, we develop a language-aware gating mechanism to replace
the standard cross-attention, avoiding language's direct fusion into visual
representations. We incorporate this mechanism into two key components of the
entire framework. The first component is a differentiable sparse sampling
module, which selects frames containing the necessary dynamics and semantics
relevant to the questions. The second component is a vision refinement module
that merges existing spatial-temporal attention layers to ensure the extraction
of multi-grained visual semantics associated with the questions. We conduct
experimental evaluations on various challenging video question-answering
benchmarks, and VideoDistill achieves state-of-the-art performance in both
general and long-form VideoQA datasets. In Addition, we verify that
VideoDistill can effectively alleviate the utilization of language shortcut
solutions in the EgoTaskQA dataset.",2024-04-01 07:44:24+00:00,"['Bo Zou', 'Chao Yang', 'Yu Qiao', 'Chengbin Quan', 'Youjian Zhao']",http://arxiv.org/abs/2404.00973v1
ToonCrafter: Generative Cartoon Interpolation,"We introduce ToonCrafter, a novel approach that transcends traditional
correspondence-based cartoon video interpolation, paving the way for generative
interpolation. Traditional methods, that implicitly assume linear motion and
the absence of complicated phenomena like dis-occlusion, often struggle with
the exaggerated non-linear and large motions with occlusion commonly found in
cartoons, resulting in implausible or even failed interpolation results. To
overcome these limitations, we explore the potential of adapting live-action
video priors to better suit cartoon interpolation within a generative
framework. ToonCrafter effectively addresses the challenges faced when applying
live-action video motion priors to generative cartoon interpolation. First, we
design a toon rectification learning strategy that seamlessly adapts
live-action video priors to the cartoon domain, resolving the domain gap and
content leakage issues. Next, we introduce a dual-reference-based 3D decoder to
compensate for lost details due to the highly compressed latent prior spaces,
ensuring the preservation of fine details in interpolation results. Finally, we
design a flexible sketch encoder that empowers users with interactive control
over the interpolation results. Experimental results demonstrate that our
proposed method not only produces visually convincing and more natural
dynamics, but also effectively handles dis-occlusion. The comparative
evaluation demonstrates the notable superiority of our approach over existing
competitors.",2024-05-28 07:58:33+00:00,"['Jinbo Xing', 'Hanyuan Liu', 'Menghan Xia', 'Yong Zhang', 'Xintao Wang', 'Ying Shan', 'Tien-Tsin Wong']",http://arxiv.org/abs/2405.17933v1
PlanLLM: Video Procedure Planning with Refinable Large Language Models,"Video procedure planning, i.e., planning a sequence of action steps given the
video frames of start and goal states, is an essential ability for embodied AI.
Recent works utilize Large Language Models (LLMs) to generate enriched action
step description texts to guide action step decoding. Although LLMs are
introduced, these methods decode the action steps into a closed-set of one-hot
vectors, limiting the model's capability of generalizing to new steps or tasks.
Additionally, fixed action step descriptions based on world-level commonsense
may contain noise in specific instances of visual states. In this paper, we
propose PlanLLM, a cross-modal joint learning framework with LLMs for video
procedure planning. We propose an LLM-Enhanced Planning module which fully uses
the generalization ability of LLMs to produce free-form planning output and to
enhance action step decoding. We also propose Mutual Information Maximization
module to connect world-level commonsense of step descriptions and
sample-specific information of visual states, enabling LLMs to employ the
reasoning ability to generate step sequences. With the assistance of LLMs, our
method can both closed-set and open vocabulary procedure planning tasks. Our
PlanLLM achieves superior performance on three benchmarks, demonstrating the
effectiveness of our designs.",2024-12-26 09:51:05+00:00,"['Dejie Yang', 'Zijing Zhao', 'Yang Liu']",http://arxiv.org/abs/2412.19139v2
Denoising with a Joint-Embedding Predictive Architecture,"Joint-embedding predictive architectures (JEPAs) have shown substantial
promise in self-supervised representation learning, yet their application in
generative modeling remains underexplored. Conversely, diffusion models have
demonstrated significant efficacy in modeling arbitrary probability
distributions. In this paper, we introduce Denoising with a Joint-Embedding
Predictive Architecture (D-JEPA), pioneering the integration of JEPA within
generative modeling. By recognizing JEPA as a form of masked image modeling, we
reinterpret it as a generalized next-token prediction strategy, facilitating
data generation in an auto-regressive manner. Furthermore, we incorporate
diffusion loss to model the per-token probability distribution, enabling data
generation in a continuous space. We also adapt flow matching loss as an
alternative to diffusion loss, thereby enhancing the flexibility of D-JEPA.
Empirically, with increased GFLOPs, D-JEPA consistently achieves lower FID
scores with fewer training epochs, indicating its good scalability. Our base,
large, and huge models outperform all previous generative models across all
scales on ImageNet conditional generation benchmarks. Beyond image generation,
D-JEPA is well-suited for other continuous data modeling, including video and
audio.",2024-10-02 05:57:10+00:00,"['Dengsheng Chen', 'Jie Hu', 'Xiaoming Wei', 'Enhua Wu']",http://arxiv.org/abs/2410.03755v2
Diffusion assisted image reconstruction in optoacoustic tomography,"In this paper we consider the problem of acoustic inversion in the context of
the optoacoustic tomography image reconstruction problem. By leveraging the
ability of the recently proposed diffusion models for image generative tasks
among others, we devise an image reconstruction architecture based on a
conditional diffusion process. The scheme makes use of an initial image
reconstruction, which is preprocessed by an autoencoder to generate an adequate
representation. This representation is used as conditional information in a
generative diffusion process. Although the computational requirements for
training and implementing the architecture are not low, several design choices
discussed in the work were made to keep them manageable. Numerical results show
that the conditional information allows to properly bias the parameters of the
diffusion model to improve the quality of the initial reconstructed image,
eliminating artifacts or even reconstructing finer details of the ground-truth
image that are not recoverable by the initial image reconstruction method. We
also tested the proposal under experimental conditions and the obtained results
were in line with those corresponding to the numerical simulations.
Improvements in image quality up to 17 % in terms of peak signal-to-noise ratio
were observed.",2024-04-16 02:36:07+00:00,"['M. G. Gonzlez', 'M. Vera', 'A. Dreszman', 'L. J. Rey Vega']",http://arxiv.org/abs/2404.10239v1
Frequency-Domain Refinement with Multiscale Diffusion for Super Resolution,"The performance of single image super-resolution depends heavily on how to
generate and complement high-frequency details to low-resolution images.
Recently, diffusion-based models exhibit great potential in generating
high-quality images for super-resolution tasks. However, existing models
encounter difficulties in directly predicting high-frequency information of
wide bandwidth by solely utilizing the high-resolution ground truth as the
target for all sampling timesteps. To tackle this problem and achieve
higher-quality super-resolution, we propose a novel Frequency Domain-guided
multiscale Diffusion model (FDDiff), which decomposes the high-frequency
information complementing process into finer-grained steps. In particular, a
wavelet packet-based frequency complement chain is developed to provide
multiscale intermediate targets with increasing bandwidth for reverse diffusion
process. Then FDDiff guides reverse diffusion process to progressively
complement the missing high-frequency details over timesteps. Moreover, we
design a multiscale frequency refinement network to predict the required
high-frequency components at multiple scales within one unified network.
Comprehensive evaluations on popular benchmarks are conducted, and demonstrate
that FDDiff outperforms prior generative methods with higher-fidelity
super-resolution results.",2024-05-16 11:58:52+00:00,"['Xingjian Wang', 'Li Chai', 'Jiming Chen']",http://arxiv.org/abs/2405.10014v1
Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory for Gaussian Process Data,"Diffusion Transformer, the backbone of Sora for video generation,
successfully scales the capacity of diffusion models, pioneering new avenues
for high-fidelity sequential data generation. Unlike static data such as
images, sequential data consists of consecutive data frames indexed by time,
exhibiting rich spatial and temporal dependencies. These dependencies represent
the underlying dynamic model and are critical to validate the generated data.
In this paper, we make the first theoretical step towards bridging diffusion
transformers for capturing spatial-temporal dependencies. Specifically, we
establish score approximation and distribution estimation guarantees of
diffusion transformers for learning Gaussian process data with covariance
functions of various decay patterns. We highlight how the spatial-temporal
dependencies are captured and affect learning efficiency. Our study proposes a
novel transformer approximation theory, where the transformer acts to unroll an
algorithm. We support our theoretical results by numerical experiments,
providing strong evidence that spatial-temporal dependencies are captured
within attention layers, aligning with our approximation theory.",2024-07-23 02:42:43+00:00,"['Hengyu Fu', 'Zehao Dou', 'Jiawei Guo', 'Mengdi Wang', 'Minshuo Chen']",http://arxiv.org/abs/2407.16134v2
PRESTO: Fast Motion Planning Using Diffusion Models Based on Key-Configuration Environment Representation,"We introduce a learning-guided motion planning framework that generates seed
trajectories using a diffusion model for trajectory optimization. Given a
workspace, our method approximates the configuration space (C-space) obstacles
through an environment representation consisting of a sparse set of
task-related key configurations, which is then used as a conditioning input to
the diffusion model. The diffusion model integrates regularization terms that
encourage smooth, collision-free trajectories during training, and trajectory
optimization refines the generated seed trajectories to correct any colliding
segments. Our experimental results demonstrate that high-quality trajectory
priors, learned through our C-space-grounded diffusion model, enable the
efficient generation of collision-free trajectories in narrow-passage
environments, outperforming previous learning- and planning-based baselines.
Videos and additional materials can be found on the project page:
https://kiwi-sherbet.github.io/PRESTO.",2024-09-24 12:12:12+00:00,"['Mingyo Seo', 'Yoonyoung Cho', 'Yoonchang Sung', 'Peter Stone', 'Yuke Zhu', 'Beomjoon Kim']",http://arxiv.org/abs/2409.16012v2
CASC: Condition-Aware Semantic Communication with Latent Diffusion Models,"Diffusion-based semantic communication methods have shown significant
advantages in image transmission by harnessing the generative power of
diffusion models. However, they still face challenges, including generation
randomness that leads to distorted reconstructions and high computational
costs. To address these issues, we propose CASC, a condition-aware semantic
communication framework that incorporates a latent diffusion model (LDM)-based
denoiser. The LDM denoiser at the receiver utilizes the received noisy latent
codes as the conditioning signal to reconstruct the latent codes, enabling the
decoder to accurately recover the source image. By operating in the latent
space, the LDM reduces computational complexity compared to traditional
diffusion models (DMs). Additionally, we introduce a condition-aware neural
network (CAN) that dynamically adjusts the weights in the hidden layers of the
LDM based on the conditioning signal. This enables finer control over the
generation process, significantly improving the perceptual quality of the
reconstructed images. Experimental results show that CASC significantly
outperforms DeepJSCC in both perceptual quality and visual effect. Moreover,
CASC reduces inference time by 51.7% compared to existing DM-based semantic
communication systems, while maintaining comparable perceptual performance. The
ablation studies also validate the effectiveness of the CAN module in improving
the image reconstruction quality.",2024-11-10 18:18:57+00:00,"['Weixuan Chen', 'Qianqian Yang']",http://arxiv.org/abs/2411.06552v1
Bracket Diffusion: HDR Image Generation by Consistent LDR Denoising,"We demonstrate generating HDR images using the concerted action of multiple
black-box, pre-trained LDR image diffusion models. Relying on a pre-trained LDR
generative diffusion models is vital as, first, there is no sufficiently large
HDR image dataset available to re-train them, and, second, even if it was,
re-training such models is impossible for most compute budgets. Instead, we
seek inspiration from the HDR image capture literature that traditionally fuses
sets of LDR images, called ""exposure brackets'', to produce a single HDR image.
We operate multiple denoising processes to generate multiple LDR brackets that
together form a valid HDR result. The key to making this work is to introduce a
consistency term into the diffusion process to couple the brackets such that
they agree across the exposure range they share while accounting for possible
differences due to the quantization error. We demonstrate state-of-the-art
unconditional and conditional or restoration-type (LDR2HDR) generative modeling
results, yet in HDR.",2024-05-23 08:24:22+00:00,"['Mojtaba Bemana', 'Thomas Leimkhler', 'Karol Myszkowski', 'Hans-Peter Seidel', 'Tobias Ritschel']",http://arxiv.org/abs/2405.14304v2
Dynamic duos: the building blocks of dimensional mechanics,"Mechanics studies the relationships between space, time, and matter, which
can be expressed in terms of the dimensions of length $\mathcal{L}$, time
$\mathcal{T}$, and mass $\mathcal{M}$. Each dimension broadens the scope of
mechanics, from geometric quantities with dimensions of the form
$\mathcal{L}^x$ (like lengths or areas), to kinematic quantities of the form
$\mathcal{L}^x\mathcal{T}^y$ (like speeds or accelerations), and eventually
``mass-carrying'' quantities such as mass, force, momentum, energy, action,
power, viscosity, etc. These standard mechanical quantities have dimensions of
the form $\mathcal{M}\mathcal{L}^x\mathcal{T}^y$, where $x$ and $y$ are
integers. In this contribution, we use this dimensional structure to arrange
these mass-carrying quantities into a table indexed by $x$ and $y$. Ratios of
quantities in the same rows provide characteristic lengths, and in the same
columns characteristic times, encompassing a great variety of physical
phenomena from atomic to astronomical scales. Most generally, we show that
picking duos of mechanical quantities that are neither on the same row nor
column yields dynamics, where one mechanical quantity is understood as
impelling motion, while the other is impeding it. The force and the mass are
the prototypes of impelling and impeding factors, but many other duos are
possible. This review provides a novel synthesis revealing the power of
dimensional analysis, to understand processes governed by the interplay of two
mechanical quantities. This elementary decomposition of space, time and motion
into pairs of mechanical factors is the foundation of ``dimensional
mechanics'', a method that this review wishes to promote and advance. The
review is complemented by online video lectures, which initiate a discussion on
the elaborate interplay of two or more mechanical quantities.",2024-01-25 10:51:23+00:00,"['Marc-Antoine Fardin', 'Mathieu Hautefeuille', 'Vivek Sharma']",http://arxiv.org/abs/2401.15101v2
Post-mastoidectomy Surface Multi-View Synthesis from a Single Microscopy Image,"Cochlear Implant (CI) procedures involve performing an invasive mastoidectomy
to insert an electrode array into the cochlea. In this paper, we introduce a
novel pipeline that is capable of generating synthetic multi-view videos from a
single CI microscope image. In our approach, we use a patient's pre-operative
CT scan to predict the post-mastoidectomy surface using a method designed for
this purpose. We manually align the surface with a selected microscope frame to
obtain an accurate initial pose of the reconstructed CT mesh relative to the
microscope. We then perform UV projection to transfer the colors from the frame
to surface textures. Novel views of the textured surface can be used to
generate a large dataset of synthetic frames with ground truth poses. We
evaluated the quality of synthetic views rendered using Pytorch3D and PyVista.
We found both rendering engines lead to similarly high-quality synthetic
novel-view frames compared to ground truth with a structural similarity index
for both methods averaging about 0.86. A large dataset of novel views with
known poses is critical for ongoing training of a method to automatically
estimate microscope pose for 2D to 3D registration with the pre-operative CT to
facilitate augmented reality surgery. This dataset will empower various
downstream tasks, such as integrating Augmented Reality (AR) in the OR,
tracking surgical tools, and supporting other video analysis studies.",2024-08-31 16:45:24+00:00,"['Yike Zhang', 'Jack Noble']",http://arxiv.org/abs/2409.03190v2
Prototypical Transformer as Unified Motion Learners,"In this work, we introduce the Prototypical Transformer (ProtoFormer), a
general and unified framework that approaches various motion tasks from a
prototype perspective. ProtoFormer seamlessly integrates prototype learning
with Transformer by thoughtfully considering motion dynamics, introducing two
innovative designs. First, Cross-Attention Prototyping discovers prototypes
based on signature motion patterns, providing transparency in understanding
motion scenes. Second, Latent Synchronization guides feature representation
learning via prototypes, effectively mitigating the problem of motion
uncertainty. Empirical results demonstrate that our approach achieves
competitive performance on popular motion tasks such as optical flow and scene
depth. Furthermore, it exhibits generality across various downstream tasks,
including object tracking and video stabilization.",2024-06-03 17:41:28+00:00,"['Cheng Han', 'Yawen Lu', 'Guohao Sun', 'James C. Liang', 'Zhiwen Cao', 'Qifan Wang', 'Qiang Guan', 'Sohail A. Dianat', 'Raghuveer M. Rao', 'Tong Geng', 'Zhiqiang Tao', 'Dongfang Liu']",http://arxiv.org/abs/2406.01559v1
Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos,"Understanding how humans would behave during hand-object interaction is vital
for applications in service robot manipulation and extended reality. To achieve
this, some recent works have been proposed to simultaneously forecast hand
trajectories and object affordances on human egocentric videos. The joint
prediction serves as a comprehensive representation of future hand-object
interactions in 2D space, indicating potential human motion and motivation.
However, the existing approaches mostly adopt the autoregressive paradigm for
unidirectional prediction, which lacks mutual constraints within the holistic
future sequence, and accumulates errors along the time axis. Meanwhile, these
works basically overlook the effect of camera egomotion on first-person view
predictions. To address these limitations, we propose a novel diffusion-based
interaction prediction method, namely Diff-IP2D, to forecast future hand
trajectories and object affordances concurrently in an iterative
non-autoregressive manner. We transform the sequential 2D images into latent
feature space and design a denoising diffusion model to predict future latent
interaction features conditioned on past ones. Motion features are further
integrated into the conditional denoising process to enable Diff-IP2D aware of
the camera wearer's dynamics for more accurate interaction prediction.
Extensive experiments demonstrate that our method significantly outperforms the
state-of-the-art baselines on both the off-the-shelf metrics and our newly
proposed evaluation protocol. This highlights the efficacy of leveraging a
generative paradigm for 2D hand-object interaction prediction. The code of
Diff-IP2D is released as open source at https://github.com/IRMVLab/Diff-IP2D.",2024-05-07 14:51:05+00:00,"['Junyi Ma', 'Jingyi Xu', 'Xieyuanli Chen', 'Hesheng Wang']",http://arxiv.org/abs/2405.04370v4
LaMoD: Latent Motion Diffusion Model For Myocardial Strain Generation,"Motion and deformation analysis of cardiac magnetic resonance (CMR) imaging
videos is crucial for assessing myocardial strain of patients with abnormal
heart functions. Recent advances in deep learning-based image registration
algorithms have shown promising results in predicting motion fields from
routinely acquired CMR sequences. However, their accuracy often diminishes in
regions with subtle appearance changes, with errors propagating over time.
Advanced imaging techniques, such as displacement encoding with stimulated
echoes (DENSE) CMR, offer highly accurate and reproducible motion data but
require additional image acquisition, which poses challenges in busy clinical
flows. In this paper, we introduce a novel Latent Motion Diffusion model
(LaMoD) to predict highly accurate DENSE motions from standard CMR videos. More
specifically, our method first employs an encoder from a pre-trained
registration network that learns latent motion features (also considered as
deformation-based shape features) from image sequences. Supervised by the
ground-truth motion provided by DENSE, LaMoD then leverages a probabilistic
latent diffusion model to reconstruct accurate motion from these extracted
features. Experimental results demonstrate that our proposed method, LaMoD,
significantly improves the accuracy of motion analysis in standard CMR images;
hence improving myocardial strain analysis in clinical settings for cardiac
patients. Our code is publicly available at https://github.com/jr-xing/LaMoD.",2024-07-02 12:54:32+00:00,"['Jiarui Xing', 'Nivetha Jayakumar', 'Nian Wu', 'Yu Wang', 'Frederick H. Epstein', 'Miaomiao Zhang']",http://arxiv.org/abs/2407.02229v2
Cross-Modality Translation with Generative Adversarial Networks to Unveil Alzheimer's Disease Biomarkers,"Generative approaches for cross-modality transformation have recently gained
significant attention in neuroimaging. While most previous work has focused on
case-control data, the application of generative models to disorder-specific
datasets and their ability to preserve diagnostic patterns remain relatively
unexplored. Hence, in this study, we investigated the use of a generative
adversarial network (GAN) in the context of Alzheimer's disease (AD) to
generate functional network connectivity (FNC) and T1-weighted structural
magnetic resonance imaging data from each other. We employed a cycle-GAN to
synthesize data in an unpaired data transition and enhanced the transition by
integrating weak supervision in cases where paired data were available. Our
findings revealed that our model could offer remarkable capability, achieving a
structural similarity index measure (SSIM) of $0.89 \pm 0.003$ for T1s and a
correlation of $0.71 \pm 0.004$ for FNCs. Moreover, our qualitative analysis
revealed similar patterns between generated and actual data when comparing AD
to cognitively normal (CN) individuals. In particular, we observed
significantly increased functional connectivity in cerebellar-sensory motor and
cerebellar-visual networks and reduced connectivity in cerebellar-subcortical,
auditory-sensory motor, sensory motor-visual, and cerebellar-cognitive control
networks. Additionally, the T1 images generated by our model showed a similar
pattern of atrophy in the hippocampal and other temporal regions of Alzheimer's
patients.",2024-05-08 23:38:02+00:00,"['Reihaneh Hassanzadeh', 'Anees Abrol', 'Hamid Reza Hassanzadeh', 'Vince D. Calhoun']",http://arxiv.org/abs/2405.05462v1
Synthesizing beta-amyloid PET images from T1-weighted Structural MRI: A Preliminary Study,"Beta-amyloid positron emission tomography (A$\beta$-PET) imaging has become a
critical tool in Alzheimer's disease (AD) research and diagnosis, providing
insights into the pathological accumulation of amyloid plaques, one of the
hallmarks of AD. However, the high cost, limited availability, and exposure to
radioactivity restrict the widespread use of A$\beta$-PET imaging, leading to a
scarcity of comprehensive datasets. Previous studies have suggested that
structural magnetic resonance imaging (MRI), which is more readily available,
may serve as a viable alternative for synthesizing A$\beta$-PET images. In this
study, we propose an approach to utilize 3D diffusion models to synthesize
A$\beta$-PET images from T1-weighted MRI scans, aiming to overcome the
limitations associated with direct PET imaging. Our method generates
high-quality A$\beta$-PET images for cognitive normal cases, although it is
less effective for mild cognitive impairment (MCI) patients due to the
variability in A$\beta$ deposition patterns among subjects. Our preliminary
results suggest that incorporating additional data, such as a larger sample of
MCI cases and multi-modality information including clinical and demographic
details, cognitive and functional assessments, and longitudinal data, may be
necessary to improve A$\beta$-PET image synthesis for MCI patients.",2024-09-26 20:51:59+00:00,"['Qing Lyu', 'Jin Young Kim', 'Jeongchul Kim', 'Christopher T Whitlow']",http://arxiv.org/abs/2409.18282v2
Neural Differential Appearance Equations,"We propose a method to reproduce dynamic appearance textures with
space-stationary but time-varying visual statistics. While most previous work
decomposes dynamic textures into static appearance and motion, we focus on
dynamic appearance that results not from motion but variations of fundamental
properties, such as rusting, decaying, melting, and weathering. To this end, we
adopt the neural ordinary differential equation (ODE) to learn the underlying
dynamics of appearance from a target exemplar. We simulate the ODE in two
phases. At the ""warm-up"" phase, the ODE diffuses a random noise to an initial
state. We then constrain the further evolution of this ODE to replicate the
evolution of visual feature statistics in the exemplar during the generation
phase. The particular innovation of this work is the neural ODE achieving both
denoising and evolution for dynamics synthesis, with a proposed temporal
training scheme. We study both relightable (BRDF) and non-relightable (RGB)
appearance models. For both we introduce new pilot datasets, allowing, for the
first time, to study such phenomena: For RGB we provide 22 dynamic textures
acquired from free online sources; For BRDFs, we further acquire a dataset of
21 flash-lit videos of time-varying materials, enabled by a simple-to-construct
setup. Our experiments show that our method consistently yields realistic and
coherent results, whereas prior works falter under pronounced temporal
appearance variations. A user study confirms our approach is preferred to
previous work for such exemplars.",2024-09-23 11:29:19+00:00,"['Chen Liu', 'Tobias Ritschel']",http://arxiv.org/abs/2410.07128v2
Text-Video Retrieval with Global-Local Semantic Consistent Learning,"Adapting large-scale image-text pre-training models, e.g., CLIP, to the video
domain represents the current state-of-the-art for text-video retrieval. The
primary approaches involve transferring text-video pairs to a common embedding
space and leveraging cross-modal interactions on specific entities for semantic
alignment. Though effective, these paradigms entail prohibitive computational
costs, leading to inefficient retrieval. To address this, we propose a simple
yet effective method, Global-Local Semantic Consistent Learning (GLSCL), which
capitalizes on latent shared semantics across modalities for text-video
retrieval. Specifically, we introduce a parameter-free global interaction
module to explore coarse-grained alignment. Then, we devise a shared local
interaction module that employs several learnable queries to capture latent
semantic concepts for learning fine-grained alignment. Furthermore, an
Inter-Consistency Loss (ICL) is devised to accomplish the concept alignment
between the visual query and corresponding textual query, and an
Intra-Diversity Loss (IDL) is developed to repulse the distribution within
visual (textual) queries to generate more discriminative concepts. Extensive
experiments on five widely used benchmarks (i.e., MSR-VTT, MSVD, DiDeMo, LSMDC,
and ActivityNet) substantiate the superior effectiveness and efficiency of the
proposed method. Remarkably, our method achieves comparable performance with
SOTA as well as being nearly 220 times faster in terms of computational cost.
Code is available at: https://github.com/zchoi/GLSCL.",2024-05-21 11:59:36+00:00,"['Haonan Zhang', 'Pengpeng Zeng', 'Lianli Gao', 'Jingkuan Song', 'Yihang Duan', 'Xinyu Lyu', 'Hengtao Shen']",http://arxiv.org/abs/2405.12710v3
GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding,"Recently, Multimodal Large Language Models (MLLMs) have been used as agents
to control keyboard and mouse inputs by directly perceiving the Graphical User
Interface (GUI) and generating corresponding commands. However, current agents
primarily demonstrate strong understanding capabilities in static environments
and are mainly applied to relatively simple domains, such as Web or mobile
interfaces. We argue that a robust GUI agent should be capable of perceiving
temporal information on the GUI, including dynamic Web content and multi-step
tasks. Additionally, it should possess a comprehensive understanding of various
GUI scenarios, including desktop software and multi-window interactions. To
this end, this paper introduces a new dataset, termed GUI-World, which features
meticulously crafted Human-MLLM annotations, extensively covering six GUI
scenarios and eight types of GUI-oriented questions in three formats. We
evaluate the capabilities of current state-of-the-art MLLMs, including Image
LLMs and Video LLMs, in understanding various types of GUI content, especially
dynamic and sequential content. Our findings reveal that current models
struggle with dynamic GUI content without manually annotated keyframes or
operation history. On the other hand, Video LLMs fall short in all GUI-oriented
tasks given the sparse GUI video dataset. Therefore, we take the initial step
of leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,
demonstrating an improved understanding of various GUI tasks. However, due to
the limitations in the performance of base LLMs, we conclude that using video
LLMs as GUI agents remains a significant challenge. We believe our work
provides valuable insights for future research in dynamic GUI content
understanding. All the dataset and code are publicly available at:
https://gui-world.github.io.",2024-06-16 06:56:53+00:00,"['Dongping Chen', 'Yue Huang', 'Siyuan Wu', 'Jingyu Tang', 'Liuyi Chen', 'Yilin Bai', 'Zhigang He', 'Chenlong Wang', 'Huichi Zhou', 'Yiqiang Li', 'Tianshuo Zhou', 'Yue Yu', 'Chujie Gao', 'Qihui Zhang', 'Yi Gui', 'Zhen Li', 'Yao Wan', 'Pan Zhou', 'Jianfeng Gao', 'Lichao Sun']",http://arxiv.org/abs/2406.10819v2
dAJC: A 2.02mW 50Mbps Direct Analog to MJPEG Converter for Video Sensor Node using Low-Noise Switched Capacitor MAC-Quantizer with Auto-Calibration and Sparsity-Aware ADC,"With the advancement in the field of the Internet of Things(IoT) and Internet
of Bodies(IoB), video camera applications using Video Sensor Nodes(VSNs) have
gained importance in the field of autonomous driving, health monitoring, robot
control, and security camera applications. However, these applications
typically involve high data rates due to the transmission of high-resolution
video signals, resulting from high data volume generated from the
analog-to-digital converters (ADCs). This significant data deluge poses
processing and storage overheads, exacerbating the problem. To address this
challenge, we propose a low-power solution aimed at reducing the power
consumption in Video Sensor Nodes (VSNs) by shifting the computation from the
digital domain to the inherently energy-efficient analog domain. Unlike
standard architectures where computation and processing are typically performed
in digital signal processing (DSP) blocks after the ADCs, our approach
eliminates the need for such blocks. Instead, we leverage a switched
capacitor-based computation unit in the analog domain, resulting in a reduction
in power consumption. We achieve a $\sim4X$ reduction in power consumption
compared to digital implementations. Furthermore, we employ a sparsity-aware
ADC, which is enabled only for significant compressed samples that contribute
to a small fraction ($\le5\%$) of the total captured analog samples, we achieve
a $\sim20X$ lower ADC conversion energy without any considerable degradation,
contributing to the overall energy savings in the system.",2024-07-02 00:27:32+00:00,"['Gourab Barik', 'Gaurav Kumar K', 'Baibhab Chatterjee', 'Shovan Maity', 'Sumon Bose', 'Shreyas Sen']",http://arxiv.org/abs/2407.11023v1
Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos,"Recent developments in Large Language Models pre-trained on extensive corpora
have shown significant success in various natural language processing tasks
with minimal fine-tuning. This success offers new promise for robotics, which
has long been constrained by the high cost of action-labeled data. We ask:
given the abundant video data containing interaction-related knowledge
available as a rich ""corpus"", can a similar generative pre-training approach be
effectively applied to enhance robot learning? The key challenge is to identify
an effective representation for autoregressive pre-training that benefits robot
manipulation tasks. Inspired by the way humans learn new skills through
observing dynamic environments, we propose that effective robotic learning
should emphasize motion-related knowledge, which is closely tied to low-level
actions and is hardware-agnostic, facilitating the transfer of learned motions
to actual robot actions. To this end, we introduce Moto, which converts video
content into latent Motion Token sequences by a Latent Motion Tokenizer,
learning a bridging ""language"" of motion from videos in an unsupervised manner.
We pre-train Moto-GPT through motion token autoregression, enabling it to
capture diverse visual motion knowledge. After pre-training, Moto-GPT
demonstrates the promising ability to produce semantically interpretable motion
tokens, predict plausible motion trajectories, and assess trajectory
rationality through output likelihood. To transfer learned motion priors to
real robot actions, we implement a co-fine-tuning strategy that seamlessly
bridges latent motion token prediction and real robot control. Extensive
experiments show that the fine-tuned Moto-GPT exhibits superior robustness and
efficiency on robot manipulation benchmarks, underscoring its effectiveness in
transferring knowledge from video data to downstream visual manipulation tasks.",2024-12-05 18:57:04+00:00,"['Yi Chen', 'Yuying Ge', 'Weiliang Tang', 'Yizhuo Li', 'Yixiao Ge', 'Mingyu Ding', 'Ying Shan', 'Xihui Liu']",http://arxiv.org/abs/2412.04445v3
Uni-AdaFocus: Spatial-temporal Dynamic Computation for Video Recognition,"This paper presents a comprehensive exploration of the phenomenon of data
redundancy in video understanding, with the aim to improve computational
efficiency. Our investigation commences with an examination of spatial
redundancy, which refers to the observation that the most informative region in
each video frame usually corresponds to a small image patch, whose shape, size
and location shift smoothly across frames. Motivated by this phenomenon, we
formulate the patch localization problem as a dynamic decision task, and
introduce a spatially adaptive video recognition approach, termed AdaFocus. In
specific, a lightweight encoder is first employed to quickly process the full
video sequence, whose features are then utilized by a policy network to
identify the most task-relevant regions. Subsequently, the selected patches are
inferred by a high-capacity deep network for the final prediction. The full
model can be trained in end-to-end conveniently. Furthermore, AdaFocus can be
extended by further considering temporal and sample-wise redundancies, i.e.,
allocating the majority of computation to the most task-relevant frames, and
minimizing the computation spent on relatively ""easier"" videos. Our resulting
approach, Uni-AdaFocus, establishes a comprehensive framework that seamlessly
integrates spatial, temporal, and sample-wise dynamic computation, while it
preserves the merits of AdaFocus in terms of efficient end-to-end training and
hardware friendliness. In addition, Uni-AdaFocus is general and flexible as it
is compatible with off-the-shelf efficient backbones (e.g., TSM and X3D), which
can be readily deployed as our feature extractor, yielding a significantly
improved computational efficiency. Empirically, extensive experiments based on
seven benchmark datasets and three application scenarios substantiate that
Uni-AdaFocus is considerably more efficient than the competitive baselines.",2024-12-15 15:51:44+00:00,"['Yulin Wang', 'Haoji Zhang', 'Yang Yue', 'Shiji Song', 'Chao Deng', 'Junlan Feng', 'Gao Huang']",http://arxiv.org/abs/2412.11228v1
PRIME: Protect Your Videos From Malicious Editing,"With the development of generative models, the quality of generated content
keeps increasing. Recently, open-source models have made it surprisingly easy
to manipulate and edit photos and videos, with just a few simple prompts. While
these cutting-edge technologies have gained popularity, they have also given
rise to concerns regarding the privacy and portrait rights of individuals.
Malicious users can exploit these tools for deceptive or illegal purposes.
Although some previous works focus on protecting photos against generative
models, we find there are still gaps between protecting videos and images in
the aspects of efficiency and effectiveness. Therefore, we introduce our
protection method, PRIME, to significantly reduce the time cost and improve the
protection performance. Moreover, to evaluate our proposed protection method,
we consider both objective metrics and human subjective metrics. Our evaluation
results indicate that PRIME only costs 8.3% GPU hours of the cost of the
previous state-of-the-art method and achieves better protection results on both
human evaluation and objective metrics. Code can be found in
https://github.com/GuanlinLee/prime.",2024-02-02 09:07:00+00:00,"['Guanlin Li', 'Shuai Yang', 'Jie Zhang', 'Tianwei Zhang']",http://arxiv.org/abs/2402.01239v1
Learning Expressive And Generalizable Motion Features For Face Forgery Detection,"Previous face forgery detection methods mainly focus on appearance features,
which may be easily attacked by sophisticated manipulation. Considering the
majority of current face manipulation methods generate fake faces based on a
single frame, which do not take frame consistency and coordination into
consideration, artifacts on frame sequences are more effective for face forgery
detection. However, current sequence-based face forgery detection methods use
general video classification networks directly, which discard the special and
discriminative motion information for face manipulation detection. To this end,
we propose an effective sequence-based forgery detection framework based on an
existing video classification method. To make the motion features more
expressive for manipulation detection, we propose an alternative motion
consistency block instead of the original motion features module. To make the
learned features more generalizable, we propose an auxiliary anomaly detection
block. With these two specially designed improvements, we make a general video
classification network achieve promising results on three popular face forgery
datasets.",2024-03-08 09:25:48+00:00,"['Jingyi Zhang', 'Peng Zhang', 'Jingjing Wang', 'Di Xie', 'Shiliang Pu']",http://arxiv.org/abs/2403.05172v1
REACTO: Reconstructing Articulated Objects from a Single Video,"In this paper, we address the challenge of reconstructing general articulated
3D objects from a single video. Existing works employing dynamic neural
radiance fields have advanced the modeling of articulated objects like humans
and animals from videos, but face challenges with piece-wise rigid general
articulated objects due to limitations in their deformation models. To tackle
this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that
enhances the rigidity of each part while maintaining flexible deformation of
the joints. Our primary insight combines three distinct approaches: 1) an
enhanced bone rigging system for improved component modeling, 2) the use of
quasi-sparse skinning weights to boost part rigidity and reconstruction
fidelity, and 3) the application of geodesic point assignment for precise
motion and seamless deformation. Our method outperforms previous works in
producing higher-fidelity 3D reconstructions of general articulated objects, as
demonstrated on both real and synthetic datasets. Project page:
https://chaoyuesong.github.io/REACTO.",2024-04-17 08:01:55+00:00,"['Chaoyue Song', 'Jiacheng Wei', 'Chuan-Sheng Foo', 'Guosheng Lin', 'Fayao Liu']",http://arxiv.org/abs/2404.11151v1
A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual Deepfake Detection,"This paper addresses the challenge of developing a robust audio-visual
deepfake detection model. In practical use cases, new generation algorithms are
continually emerging, and these algorithms are not encountered during the
development of detection methods. This calls for the generalization ability of
the method. Additionally, to ensure the credibility of detection methods, it is
beneficial for the model to interpret which cues from the video indicate it is
fake. Motivated by these considerations, we then propose a multi-stream fusion
approach with one-class learning as a representation-level regularization
technique. We study the generalization problem of audio-visual deepfake
detection by creating a new benchmark by extending and re-splitting the
existing FakeAVCeleb dataset. The benchmark contains four categories of fake
videos (Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,
and Unsynchronized videos). The experimental results demonstrate that our
approach surpasses the previous models by a large margin. Furthermore, our
proposed framework offers interpretability, indicating which modality the model
identifies as more likely to be fake. The source code is released at
https://github.com/bok-bok/MSOC.",2024-06-20 10:33:15+00:00,"['Kyungbok Lee', 'You Zhang', 'Zhiyao Duan']",http://arxiv.org/abs/2406.14176v3
MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video,"We present the first automated multimodal summary generation system,
MMSummary, for medical imaging video, particularly with a focus on fetal
ultrasound analysis. Imitating the examination process performed by a human
sonographer, MMSummary is designed as a three-stage pipeline, progressing from
keyframe detection to keyframe captioning and finally anatomy segmentation and
measurement. In the keyframe detection stage, an innovative automated workflow
is proposed to progressively select a concise set of keyframes, preserving
sufficient video information without redundancy. Subsequently, we adapt a large
language model to generate meaningful captions for fetal ultrasound keyframes
in the keyframe captioning stage. If a keyframe is captioned as fetal biometry,
the segmentation and measurement stage estimates biometric parameters by
segmenting the region of interest according to the textual prior. The MMSummary
system provides comprehensive summaries for fetal ultrasound examinations and
based on reported experiments is estimated to reduce scanning time by
approximately 31.5%, thereby suggesting the potential to enhance clinical
workflow efficiency.",2024-08-07 13:30:58+00:00,"['Xiaoqing Guo', 'Qianhui Men', 'J. Alison Noble']",http://arxiv.org/abs/2408.03761v2
Towards Real-Time Generation of Delay-Compensated Video Feeds for Outdoor Mobile Robot Teleoperation,"Teleoperation is an important technology to enable supervisors to control
agricultural robots remotely. However, environmental factors in dense crop rows
and limitations in network infrastructure hinder the reliability of data
streamed to teleoperators. These issues result in delayed and variable frame
rate video feeds that often deviate significantly from the robot's actual
viewpoint. We propose a modular learning-based vision pipeline to generate
delay-compensated images in real-time for supervisors. Our extensive offline
evaluations demonstrate that our method generates more accurate images compared
to state-of-the-art approaches in our setting. Additionally, ours is one of the
few works to evaluate a delay-compensation method in outdoor field environments
with complex terrain on data from a real robot in real-time. Resulting videos
and code are provided at https://sites.google.com/illinois.edu/comp-teleop.",2024-09-16 01:39:50+00:00,"['Neeloy Chakraborty', 'Yixiao Fang', 'Andre Schreiber', 'Tianchen Ji', 'Zhe Huang', 'Aganze Mihigo', 'Cassidy Wall', 'Abdulrahman Almana', 'Katherine Driggs-Campbell']",http://arxiv.org/abs/2409.09921v2
Robust Audiovisual Speech Recognition Models with Mixture-of-Experts,"Visual signals can enhance audiovisual speech recognition accuracy by
providing additional contextual information. Given the complexity of visual
signals, an audiovisual speech recognition model requires robust generalization
capabilities across diverse video scenarios, presenting a significant
challenge. In this paper, we introduce EVA, leveraging the mixture-of-Experts
for audioVisual ASR to perform robust speech recognition for ``in-the-wild''
videos. Specifically, we first encode visual information into visual tokens
sequence and map them into speech space by a lightweight projection. Then, we
build EVA upon a robust pretrained speech recognition model, ensuring its
generalization ability. Moreover, to incorporate visual information
effectively, we inject visual information into the ASR model through a
mixture-of-experts module. Experiments show our model achieves state-of-the-art
results on three benchmarks, which demonstrates the generalization ability of
EVA across diverse video domains.",2024-09-19 00:08:28+00:00,"['Yihan Wu', 'Yifan Peng', 'Yichen Lu', 'Xuankai Chang', 'Ruihua Song', 'Shinji Watanabe']",http://arxiv.org/abs/2409.12370v1
Enhancing Motion Variation in Text-to-Motion Models via Pose and Video Conditioned Editing,"Text-to-motion models that generate sequences of human poses from textual
descriptions are garnering significant attention. However, due to data
scarcity, the range of motions these models can produce is still limited. For
instance, current text-to-motion models cannot generate a motion of kicking a
football with the instep of the foot, since the training data only includes
martial arts kicks. We propose a novel method that uses short video clips or
images as conditions to modify existing basic motions. In this approach, the
model's understanding of a kick serves as the prior, while the video or image
of a football kick acts as the posterior, enabling the generation of the
desired motion. By incorporating these additional modalities as conditions, our
method can create motions not present in the training set, overcoming the
limitations of text-motion datasets. A user study with 26 participants
demonstrated that our approach produces unseen motions with realism comparable
to commonly represented motions in text-motion datasets (e.g., HumanML3D), such
as walking, running, squatting, and kicking.",2024-10-11 15:59:10+00:00,"['Clayton Leite', 'Yu Xiao']",http://arxiv.org/abs/2410.08931v1
MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer,"Transferring visual-language knowledge from large-scale foundation models for
video recognition has proved to be effective. To bridge the domain gap,
additional parametric modules are added to capture the temporal information.
However, zero-shot generalization diminishes with the increase in the number of
specialized parameters, making existing works a trade-off between zero-shot and
close-set performance. In this paper, we present MoTE, a novel framework that
enables generalization and specialization to be balanced in one unified model.
Our approach tunes a mixture of temporal experts to learn multiple task views
with various degrees of data fitting. To maximally preserve the knowledge of
each expert, we propose \emph{Weight Merging Regularization}, which regularizes
the merging process of experts in weight space. Additionally with temporal
feature modulation to regularize the contribution of temporal feature during
test. We achieve a sound balance between zero-shot and close-set video
recognition tasks and obtain state-of-the-art or competitive results on various
datasets, including Kinetics-400 \& 600, UCF, and HMDB. Code is available at
\url{https://github.com/ZMHH-H/MoTE}.",2024-10-14 15:00:55+00:00,"['Minghao Zhu', 'Zhengpu Wang', 'Mengxian Hu', 'Ronghao Dang', 'Xiao Lin', 'Xun Zhou', 'Chengju Liu', 'Qijun Chen']",http://arxiv.org/abs/2410.10589v1
V2SFlow: Video-to-Speech Generation with Speech Decomposition and Rectified Flow,"In this paper, we introduce V2SFlow, a novel Video-to-Speech (V2S) framework
designed to generate natural and intelligible speech directly from silent
talking face videos. While recent V2S systems have shown promising results on
constrained datasets with limited speakers and vocabularies, their performance
often degrades on real-world, unconstrained datasets due to the inherent
variability and complexity of speech signals. To address these challenges, we
decompose the speech signal into manageable subspaces (content, pitch, and
speaker information), each representing distinct speech attributes, and predict
them directly from the visual input. To generate coherent and realistic speech
from these predicted attributes, we employ a rectified flow matching decoder
built on a Transformer architecture, which models efficient probabilistic
pathways from random noise to the target speech distribution. Extensive
experiments demonstrate that V2SFlow significantly outperforms state-of-the-art
methods, even surpassing the naturalness of ground truth utterances.",2024-11-29 05:55:20+00:00,"['Jeongsoo Choi', 'Ji-Hoon Kim', 'Jinyu Li', 'Joon Son Chung', 'Shujie Liu']",http://arxiv.org/abs/2411.19486v1
PERSE: Personalized 3D Generative Avatars from A Single Portrait,"We present PERSE, a method for building an animatable personalized generative
avatar from a reference portrait. Our avatar model enables facial attribute
editing in a continuous and disentangled latent space to control each facial
attribute, while preserving the individual's identity. To achieve this, our
method begins by synthesizing large-scale synthetic 2D video datasets, where
each video contains consistent changes in the facial expression and viewpoint,
combined with a variation in a specific facial attribute from the original
input. We propose a novel pipeline to produce high-quality, photorealistic 2D
videos with facial attribute editing. Leveraging this synthetic attribute
dataset, we present a personalized avatar creation method based on the 3D
Gaussian Splatting, learning a continuous and disentangled latent space for
intuitive facial attribute manipulation. To enforce smooth transitions in this
latent space, we introduce a latent space regularization technique by using
interpolated 2D faces as supervision. Compared to previous approaches, we
demonstrate that PERSE generates high-quality avatars with interpolated
attributes while preserving identity of reference person.",2024-12-30 18:59:58+00:00,"['Hyunsoo Cha', 'Inhee Lee', 'Hanbyul Joo']",http://arxiv.org/abs/2412.21206v1
Analyzing Tumors by Synthesis,"Computer-aided tumor detection has shown great potential in enhancing the
interpretation of over 80 million CT scans performed annually in the United
States. However, challenges arise due to the rarity of CT scans with tumors,
especially early-stage tumors. Developing AI with real tumor data faces issues
of scarcity, annotation difficulty, and low prevalence. Tumor synthesis
addresses these challenges by generating numerous tumor examples in medical
images, aiding AI training for tumor detection and segmentation. Successful
synthesis requires realistic and generalizable synthetic tumors across various
organs. This chapter reviews AI development on real and synthetic data and
summarizes two key trends in synthetic data for cancer imaging research:
modeling-based and learning-based approaches. Modeling-based methods, like
Pixel2Cancer, simulate tumor development over time using generic rules, while
learning-based methods, like DiffTumor, learn from a few annotated examples in
one organ to generate synthetic tumors in others. Reader studies with expert
radiologists show that synthetic tumors can be convincingly realistic. We also
present case studies in the liver, pancreas, and kidneys reveal that AI trained
on synthetic tumors can achieve performance comparable to, or better than, AI
only trained on real data. Tumor synthesis holds significant promise for
expanding datasets, enhancing AI reliability, improving tumor detection
performance, and preserving patient privacy.",2024-09-09 19:51:44+00:00,"['Qi Chen', 'Yuxiang Lai', 'Xiaoxi Chen', 'Qixin Hu', 'Alan Yuille', 'Zongwei Zhou']",http://arxiv.org/abs/2409.06035v1
FiGCLIP: Fine-Grained CLIP Adaptation via Densely Annotated Videos,"While contrastive language image pretraining (CLIP) have exhibited impressive
performance by learning highly semantic and generalized representations, recent
works have exposed a fundamental drawback in its syntactic properties, that
includes interpreting fine-grained attributes, actions, spatial relations,
states, and details that require compositional reasoning. One reason for this
is that natural captions often do not capture all the visual details of a
scene. This leads to unaddressed visual concepts being misattributed to the
wrong words. And the pooled image and text features, ends up acting as a bag of
words, hence losing the syntactic information. In this work, we ask: Is it
possible to enhance CLIP's fine-grained and syntactic abilities without
compromising its semantic properties? We show that this is possible by adapting
CLIP efficiently on a high-quality, comprehensive, and relatively small
dataset. We demonstrate our adaptation strategy on VidSitu, a video situation
recognition dataset annotated with verbs and rich semantic role labels (SRL).
We use the SRL and verb information to create rule-based detailed captions,
making sure they capture most of the visual concepts. Combined with hard
negatives and hierarchical losses, these annotations allow us to learn a
powerful visual representation, dubbed Fine-Grained CLIP (FiGCLIP), that
preserves semantic understanding while being detail-oriented. We evaluate on
five diverse vision-language tasks in both fine-tuning and zero-shot settings,
achieving consistent improvements over the base CLIP model.",2024-01-15 13:27:34+00:00,"['Darshan Singh S', 'Zeeshan Khan', 'Makarand Tapaswi']",http://arxiv.org/abs/2401.07669v1
MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting,"3D reconstruction and simulation, although interrelated, have distinct
objectives: reconstruction requires a flexible 3D representation that can adapt
to diverse scenes, while simulation needs a structured representation to model
motion principles effectively. This paper introduces the Mesh-adsorbed Gaussian
Splatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians
to roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D
representation. Such representation harnesses both the rendering flexibility of
3D Gaussians and the structured property of meshes. To achieve this, we
introduce RMD-Net, a network that learns motion priors from video data to
refine mesh deformations, alongside RGD-Net, which models the relative
displacement between the mesh and Gaussians to enhance rendering fidelity under
mesh constraints. To generalize to novel, user-defined deformations beyond
input video without reliance on temporal data, we propose MPE-Net, which
leverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to
the universality of meshes, MaGS is compatible with various deformation priors
such as ARAP, SMPL, and soft physics simulation. Extensive experiments on the
D-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves
state-of-the-art performance in both reconstruction and simulation.",2024-06-03 17:59:51+00:00,"['Shaojie Ma', 'Yawei Luo', 'Wei Yang', 'Yi Yang']",http://arxiv.org/abs/2406.01593v2
Learning Precise Affordances from Egocentric Videos for Robotic Manipulation,"Affordance, defined as the potential actions that an object offers, is
crucial for robotic manipulation tasks. A deep understanding of affordance can
lead to more intelligent AI systems. For example, such knowledge directs an
agent to grasp a knife by the handle for cutting and by the blade when passing
it to someone. In this paper, we present a streamlined affordance learning
system that encompasses data collection, effective model training, and robot
deployment. First, we collect training data from egocentric videos in an
automatic manner. Different from previous methods that focus only on the object
graspable affordance and represent it as coarse heatmaps, we cover both
graspable (e.g., object handles) and functional affordances (e.g., knife
blades, hammer heads) and extract data with precise segmentation masks. We then
propose an effective model, termed Geometry-guided Affordance Transformer
(GKT), to train on the collected data. GKT integrates an innovative Depth
Feature Injector (DFI) to incorporate 3D shape and geometric priors, enhancing
the model's understanding of affordances. To enable affordance-oriented
manipulation, we further introduce Aff-Grasp, a framework that combines GKT
with a grasp generation model. For comprehensive evaluation, we create an
affordance evaluation dataset with pixel-wise annotations, and design
real-world tasks for robot experiments. The results show that GKT surpasses the
state-of-the-art by 15.9% in mIoU, and Aff-Grasp achieves high success rates of
95.5% in affordance prediction and 77.1% in successful grasping among 179
trials, including evaluations with seen, unseen objects, and cluttered scenes.",2024-08-19 16:11:47+00:00,"['Gen Li', 'Nikolaos Tsagkas', 'Jifei Song', 'Ruaridh Mon-Williams', 'Sethu Vijayakumar', 'Kun Shao', 'Laura Sevilla-Lara']",http://arxiv.org/abs/2408.10123v1
VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models,"The rapid advancement of vision-language models (VLMs) has established a new
paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously
detect anomalies and provide comprehendible explanations for the decisions.
Existing work in this direction often assumes the complex reasoning required
for VAD exceeds the capabilities of pretrained VLMs. Consequently, these
approaches either incorporate specialized reasoning modules during inference or
rely on instruction tuning datasets through additional training to adapt VLMs
for VAD. However, such strategies often incur substantial computational costs
or data annotation overhead. To address these challenges in explainable VAD, we
introduce a verbalized learning framework named VERA that enables VLMs to
perform VAD without model parameter modifications. Specifically, VERA
automatically decomposes the complex reasoning required for VAD into
reflections on simpler, more focused guiding questions capturing distinct
abnormal patterns. It treats these reflective questions as learnable parameters
and optimizes them through data-driven verbal interactions between learner and
optimizer VLMs, using coarsely labeled training data. During inference, VERA
embeds the learned questions into model prompts to guide VLMs in generating
segment-level anomaly scores, which are then refined into frame-level scores
via the fusion of scene and temporal contexts. Experimental results on
challenging benchmarks demonstrate that the learned questions of VERA are
highly adaptable, significantly improving both detection performance and
explainability of VLMs for VAD.",2024-12-02 04:10:14+00:00,"['Muchao Ye', 'Weiyang Liu', 'Pan He']",http://arxiv.org/abs/2412.01095v1
Sora OpenAI's Prelude: Social Media Perspectives on Sora OpenAI and the Future of AI Video Generation,"The rapid advancement of Generative AI (Gen-AI) is transforming
Human-Computer Interaction (HCI), with significant implications across various
sectors. This study investigates the public's perception of Sora OpenAI, a
pioneering Gen-AI video generation tool, via social media discussions on Reddit
before its release. It centers on two main questions: the envisioned
applications and the concerns related to Sora's integration. The analysis
forecasts positive shifts in content creation, predicting that Sora will
democratize video marketing and innovate game development by making video
production more accessible and economical. Conversely, there are concerns about
deepfakes and the potential for disinformation, underscoring the need for
strategies to address disinformation and bias. This paper contributes to the
Gen-AI discourse by fostering discussion on current and future capabilities,
enriching the understanding of public expectations, and establishing a temporal
benchmark for user anticipation. This research underscores the necessity for
informed, ethical approaches to AI development and integration, ensuring that
technological advancements align with societal values and user needs.",2024-03-02 00:16:22+00:00,"['Reza Hadi Mogavi', 'Derrick Wang', 'Joseph Tu', 'Hilda Hadan', 'Sabrina A. Sgandurra', 'Pan Hui', 'Lennart E. Nacke']",http://arxiv.org/abs/2403.14665v1
Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to Biological Motion Perception,"Biological motion perception (BMP) refers to humans' ability to perceive and
recognize the actions of living beings solely from their motion patterns,
sometimes as minimal as those depicted on point-light displays. While humans
excel at these tasks without any prior training, current AI models struggle
with poor generalization performance. To close this research gap, we propose
the Motion Perceiver (MP). MP solely relies on patch-level optical flows from
video clips as inputs. During training, it learns prototypical flow snapshots
through a competitive binding mechanism and integrates invariant motion
representations to predict action labels for the given video. During inference,
we evaluate the generalization ability of all AI models and humans on 62,656
video stimuli spanning 24 BMP conditions using point-light displays in
neuroscience. Remarkably, MP outperforms all existing AI models with a maximum
improvement of 29% in top-1 action recognition accuracy on these conditions.
Moreover, we benchmark all AI models in point-light displays of two standard
video datasets in computer vision. MP also demonstrates superior performance in
these cases. More interestingly, via psychophysics experiments, we found that
MP recognizes biological movements in a way that aligns with human behaviors.
Our data and code are available at
https://github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver.",2024-05-26 09:11:46+00:00,"['Shuangpeng Han', 'Ziyu Wang', 'Mengmi Zhang']",http://arxiv.org/abs/2405.16493v2
May the Dance be with You: Dance Generation Framework for Non-Humanoids,"We hypothesize dance as a motion that forms a visual rhythm from music, where
the visual rhythm can be perceived from an optical flow. If an agent can
recognize the relationship between visual rhythm and music, it will be able to
dance by generating a motion to create a visual rhythm that matches the music.
Based on this, we propose a framework for any kind of non-humanoid agents to
learn how to dance from human videos. Our framework works in two processes: (1)
training a reward model which perceives the relationship between optical flow
(visual rhythm) and music from human dance videos, (2) training the
non-humanoid dancer based on that reward model, and reinforcement learning. Our
reward model consists of two feature encoders for optical flow and music. They
are trained based on contrastive learning which makes the higher similarity
between concurrent optical flow and music features. With this reward model, the
agent learns dancing by getting a higher reward when its action creates an
optical flow whose feature has a higher similarity with the given music
feature. Experiment results show that generated dance motion can align with the
music beat properly, and user study result indicates that our framework is more
preferred by humans compared to the baselines. To the best of our knowledge,
our work of non-humanoid agents which learn dance from human videos is
unprecedented. An example video can be found at https://youtu.be/dOUPvo-O3QY.",2024-05-30 06:43:55+00:00,['Hyemin Ahn'],http://arxiv.org/abs/2405.19743v1
SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding,"Temporal grounding, also known as video moment retrieval, aims at locating
video segments corresponding to a given query sentence. The compositional
nature of natural language enables the localization beyond predefined events,
posing a certain challenge to the compositional generalizability of existing
methods. Recent studies establish the correspondence between videos and queries
through a decompose-reconstruct manner to achieve compositional generalization.
However, they only consider dominant primitives and build negative queries
through random sampling and recombination, resulting in semantically
implausible negatives that hinder the models from learning rational
compositions. In addition, recent DETR-based methods still underperform in
compositional temporal grounding, showing irrational saliency responses when
given negative queries that have subtle differences from positive queries. To
address these limitations, we first propose a large language model-driven
method for negative query construction, utilizing GPT-3.5-Turbo to generate
semantically plausible hard negative queries. Subsequently, we introduce a
coarse-to-fine saliency ranking strategy, which encourages the model to learn
the multi-granularity semantic relationships between videos and hierarchical
negative queries to boost compositional generalization. Extensive experiments
on two challenging benchmarks validate the effectiveness and generalizability
of our proposed method. Our code is available at
https://github.com/zxccade/SHINE.",2024-07-06 16:08:17+00:00,"['Zixu Cheng', 'Yujiang Pu', 'Shaogang Gong', 'Parisa Kordjamshidi', 'Yu Kong']",http://arxiv.org/abs/2407.05118v2
Effectively Leveraging CLIP for Generating Situational Summaries of Images and Videos,"Situation recognition refers to the ability of an agent to identify and
understand various situations or contexts based on available information and
sensory inputs. It involves the cognitive process of interpreting data from the
environment to determine what is happening, what factors are involved, and what
actions caused those situations. This interpretation of situations is
formulated as a semantic role labeling problem in computer vision-based
situation recognition. Situations depicted in images and videos hold pivotal
information, essential for various applications like image and video
captioning, multimedia retrieval, autonomous systems and event monitoring.
However, existing methods often struggle with ambiguity and lack of context in
generating meaningful and accurate predictions. Leveraging multimodal models
such as CLIP, we propose ClipSitu, which sidesteps the need for full
fine-tuning and achieves state-of-the-art results in situation recognition and
localization tasks. ClipSitu harnesses CLIP-based image, verb, and role
embeddings to predict nouns fulfilling all the roles associated with a verb,
providing a comprehensive understanding of depicted scenarios. Through a
cross-attention Transformer, ClipSitu XTF enhances the connection between
semantic role queries and visual token representations, leading to superior
performance in situation recognition. We also propose a verb-wise role
prediction model with near-perfect accuracy to create an end-to-end framework
for producing situational summaries for out-of-domain images. We show that
situational summaries empower our ClipSitu models to produce structured
descriptions with reduced ambiguity compared to generic captions. Finally, we
extend ClipSitu to video situation recognition to showcase its versatility and
produce comparable performance to state-of-the-art methods.",2024-07-30 08:39:20+00:00,"['Dhruv Verma', 'Debaditya Roy', 'Basura Fernando']",http://arxiv.org/abs/2407.20642v2
ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer,"Lip-syncing videos with given audio is the foundation for various
applications including the creation of virtual presenters or performers. While
recent studies explore high-fidelity lip-sync with different techniques, their
task-orientated models either require long-term videos for clip-specific
training or retain visible artifacts. In this paper, we propose a unified and
effective framework ReSyncer, that synchronizes generalized audio-visual facial
information. The key design is revisiting and rewiring the Style-based
generator to efficiently adopt 3D facial dynamics predicted by a principled
style-injected Transformer. By simply re-configuring the information insertion
mechanisms within the noise and style space, our framework fuses motion and
appearance with unified training. Extensive experiments demonstrate that
ReSyncer not only produces high-fidelity lip-synced videos according to audio,
but also supports multiple appealing properties that are suitable for creating
virtual presenters and performers, including fast personalized fine-tuning,
video-driven lip-syncing, the transfer of speaking styles, and even face
swapping. Resources can be found at
https://guanjz20.github.io/projects/ReSyncer.",2024-08-06 16:31:45+00:00,"['Jiazhi Guan', 'Zhiliang Xu', 'Hang Zhou', 'Kaisiyuan Wang', 'Shengyi He', 'Zhanwang Zhang', 'Borong Liang', 'Haocheng Feng', 'Errui Ding', 'Jingtuo Liu', 'Jingdong Wang', 'Youjian Zhao', 'Ziwei Liu']",http://arxiv.org/abs/2408.03284v1
Multi-Reference Generative Face Video Compression with Contrastive Learning,"Generative face video coding (GFVC) has been demonstrated as a potential
approach to low-latency, low bitrate video conferencing. GFVC frameworks
achieve an extreme gain in coding efficiency with over 70% bitrate savings when
compared to conventional codecs at bitrates below 10kbps. In recent MPEG/JVET
standardization efforts, all the information required to reconstruct video
sequences using GFVC frameworks are adopted as part of the supplemental
enhancement information (SEI) in existing compression pipelines. In light of
this development, we aim to address a challenge that has been weakly addressed
in prior GFVC frameworks, i.e., reconstruction drift as the distance between
the reference and target frames increases. This challenge creates the need to
update the reference buffer more frequently by transmitting more Intra-refresh
frames, which are the most expensive element of the GFVC bitstream. To overcome
this problem, we propose instead multiple reference animation as a robust
approach to minimizing reconstruction drift, especially when used in a
bi-directional prediction mode. Further, we propose a contrastive learning
formulation for multi-reference animation. We observe that using a contrastive
learning framework enhances the representation capabilities of the animation
generator. The resulting framework, MRDAC (Multi-Reference Deep Animation
Codec) can therefore be used to compress longer sequences with fewer reference
frames or achieve a significant gain in reconstruction accuracy at comparable
bitrates to previous frameworks. Quantitative and qualitative results show
significant coding and reconstruction quality gains compared to previous GFVC
methods, and more accurate animation quality in presence of large pose and
facial expression changes.",2024-09-02 08:06:47+00:00,"['Goluck Konuko', 'Giuseppe Valenzise']",http://arxiv.org/abs/2409.01029v1
Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints,"Recognizing pain in video is crucial for improving patient-computer
interaction systems, yet traditional data collection in this domain raises
significant ethical and logistical challenges. This study introduces a novel
approach that leverages synthetic data to enhance video-based pain recognition
models, providing an ethical and scalable alternative. We present a pipeline
that synthesizes realistic 3D facial models by capturing nuanced facial
movements from a small participant pool, and mapping these onto diverse
synthetic avatars. This process generates 8,600 synthetic faces, accurately
reflecting genuine pain expressions from varied angles and perspectives.
  Utilizing advanced facial capture techniques, and leveraging public datasets
like CelebV-HQ and FFHQ-UV for demographic diversity, our new synthetic dataset
significantly enhances model training while ensuring privacy by anonymizing
identities through facial replacements.
  Experimental results demonstrate that models trained on combinations of
synthetic data paired with a small amount of real participants achieve superior
performance in pain recognition, effectively bridging the gap between synthetic
simulations and real-world applications. Our approach addresses data scarcity
and ethical concerns, offering a new solution for pain detection and opening
new avenues for research in privacy-preserving dataset generation. All
resources are publicly available to encourage further innovation in this field.",2024-09-24 18:33:57+00:00,"['Jonas Nasimzada', 'Jens Kleesiek', 'Ken Herrmann', 'Alina Roitberg', 'Constantin Seibold']",http://arxiv.org/abs/2409.16382v1
TextToon: Real-Time Text Toonify Head Avatar from Single Video,"We propose TextToon, a method to generate a drivable toonified avatar. Given
a short monocular video sequence and a written instruction about the avatar
style, our model can generate a high-fidelity toonified avatar that can be
driven in real-time by another video with arbitrary identities. Existing
related works heavily rely on multi-view modeling to recover geometry via
texture embeddings, presented in a static manner, leading to control
limitations. The multi-view video input also makes it difficult to deploy these
models in real-world applications. To address these issues, we adopt a
conditional embedding Tri-plane to learn realistic and stylized facial
representations in a Gaussian deformation field. Additionally, we expand the
stylization capabilities of 3D Gaussian Splatting by introducing an adaptive
pixel-translation neural network and leveraging patch-aware contrastive
learning to achieve high-quality images. To push our work into consumer
applications, we develop a real-time system that can operate at 48 FPS on a GPU
machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate
the efficacy of our approach in generating textual avatars over existing
methods in terms of quality and real-time animation. Please refer to our
project page for more details: https://songluchuan.github.io/TextToon/.",2024-09-23 15:04:45+00:00,"['Luchuan Song', 'Lele Chen', 'Celong Liu', 'Pinxin Liu', 'Chenliang Xu']",http://arxiv.org/abs/2410.07160v1
Animate-X: Universal Character Image Animation with Enhanced Motion Representation,"Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Our in-depth analysis suggests to
attribute this limitation to their insufficient modeling of motion, which is
unable to comprehend the movement pattern of the driving video, thus imposing a
pose sequence rigidly onto the target character. To this end, this paper
proposes Animate-X, a universal animation framework based on LDM for various
character types (collectively named X), including anthropomorphic characters.
To enhance motion representation, we introduce the Pose Indicator, which
captures comprehensive motion pattern from the driving video through both
implicit and explicit manner. The former leverages CLIP visual features of a
driving video to extract its gist of motion, like the overall movement pattern
and temporal relations among motions, while the latter strengthens the
generalization of LDM by simulating possible inputs in advance that may arise
during inference. Moreover, we introduce a new Animated Anthropomorphic
Benchmark (A^2Bench) to evaluate the performance of Animate-X on universal and
widely applicable animation images. Extensive experiments demonstrate the
superiority and effectiveness of Animate-X compared to state-of-the-art
methods.",2024-10-14 09:06:55+00:00,"['Shuai Tan', 'Biao Gong', 'Xiang Wang', 'Shiwei Zhang', 'Dandan Zheng', 'Ruobing Zheng', 'Kecheng Zheng', 'Jingdong Chen', 'Ming Yang']",http://arxiv.org/abs/2410.10306v2
Latent Action Pretraining from Videos,"We introduce Latent Action Pretraining for general Action models (LAPA), an
unsupervised method for pretraining Vision-Language-Action (VLA) models without
ground-truth robot action labels. Existing Vision-Language-Action models
require action labels typically collected by human teleoperators during
pretraining, which significantly limits possible data sources and scale. In
this work, we propose a method to learn from internet-scale videos that do not
have robot action labels. We first train an action quantization model
leveraging VQ-VAE-based objective to learn discrete latent actions between
image frames, then pretrain a latent VLA model to predict these latent actions
from observations and task descriptions, and finally finetune the VLA on
small-scale robot manipulation data to map from latent to robot actions.
Experimental results demonstrate that our method significantly outperforms
existing techniques that train robot manipulation policies from large-scale
videos. Furthermore, it outperforms the state-of-the-art VLA model trained with
robotic action labels on real-world manipulation tasks that require language
conditioning, generalization to unseen objects, and semantic generalization to
unseen instructions. Training only on human manipulation videos also shows
positive transfer, opening up the potential for leveraging web-scale data for
robotics foundation model.",2024-10-15 16:28:09+00:00,"['Seonghyeon Ye', 'Joel Jang', 'Byeongguk Jeon', 'Sejune Joo', 'Jianwei Yang', 'Baolin Peng', 'Ajay Mandlekar', 'Reuben Tan', 'Yu-Wei Chao', 'Bill Yuchen Lin', 'Lars Liden', 'Kimin Lee', 'Jianfeng Gao', 'Luke Zettlemoyer', 'Dieter Fox', 'Minjoon Seo']",http://arxiv.org/abs/2410.11758v1
MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis and Knowledge-based Departmental Consultation,"We present MMDS, a system capable of recognizing medical images and patient
facial details, and providing professional medical diagnoses. The system
consists of two core components:The first component is the analysis of medical
images and videos. We trained a specialized multimodal medical model capable of
interpreting medical images and accurately analyzing patients' facial emotions
and facial paralysis conditions. The model achieved an accuracy of 72.59% on
the FER2013 facial emotion recognition dataset, with a 91.1% accuracy in
recognizing the ""happy"" emotion. In facial paralysis recognition, the model
reached an accuracy of 92%, which is 30% higher than that of GPT-4o. Based on
this model, we developed a parser for analyzing facial movement videos of
patients with facial paralysis, achieving precise grading of the paralysis
severity. In tests on 30 videos of facial paralysis patients, the system
demonstrated a grading accuracy of 83.3%.The second component is the generation
of professional medical responses. We employed a large language model,
integrated with a medical knowledge base, to generate professional diagnoses
based on the analysis of medical images or videos. The core innovation lies in
our development of a department-specific knowledge base routing management
mechanism, in which the large language model categorizes data by medical
departments and, during the retrieval process, determines the appropriate
knowledge base to query. This significantly improves retrieval accuracy in the
RAG (retrieval-augmented generation) process.",2024-10-20 14:31:05+00:00,"['Yi Ren', 'HanZhi Zhang', 'Weibin Li', 'Jun Fu', 'Diandong Liu', 'Tianyi Zhang', 'Jie He', 'Licheng Jiao']",http://arxiv.org/abs/2410.15403v2
GLCF: A Global-Local Multimodal Coherence Analysis Framework for Talking Face Generation Detection,"Talking face generation (TFG) allows for producing lifelike talking videos of
any character using only facial images and accompanying text. Abuse of this
technology could pose significant risks to society, creating the urgent need
for research into corresponding detection methods. However, research in this
field has been hindered by the lack of public datasets. In this paper, we
construct the first large-scale multi-scenario talking face dataset (MSTF),
which contains 22 audio and video forgery techniques, filling the gap of
datasets in this field. The dataset covers 11 generation scenarios and more
than 20 semantic scenarios, closer to the practical application scenario of
TFG. Besides, we also propose a TFG detection framework, which leverages the
analysis of both global and local coherence in the multimodal content of TFG
videos. Therefore, a region-focused smoothness detection module (RSFDM) and a
discrepancy capture-time frame aggregation module (DCTAM) are introduced to
evaluate the global temporal coherence of TFG videos, aggregating multi-grained
spatial information. Additionally, a visual-audio fusion module (V-AFM) is
designed to evaluate audiovisual coherence within a localized temporal
perspective. Comprehensive experiments demonstrate the reasonableness and
challenges of our datasets, while also indicating the superiority of our
proposed method compared to the state-of-the-art deepfake detection approaches.",2024-12-18 09:34:59+00:00,"['Xiaocan Chen', 'Qilin Yin', 'Jiarui Liu', 'Wei Lu', 'Xiangyang Luo', 'Jiantao Zhou']",http://arxiv.org/abs/2412.13656v2
DensePANet: An improved generative adversarial network for photoacoustic tomography image reconstruction from sparse data,"Image reconstruction is an essential step of every medical imaging method,
including Photoacoustic Tomography (PAT), which is a promising modality of
imaging, that unites the benefits of both ultrasound and optical imaging
methods. Reconstruction of PAT images using conventional methods results in
rough artifacts, especially when applied directly to sparse PAT data. In recent
years, generative adversarial networks (GANs) have shown a powerful performance
in image generation as well as translation, rendering them a smart choice to be
applied to reconstruction tasks. In this study, we proposed an end-to-end
method called DensePANet to solve the problem of PAT image reconstruction from
sparse data. The proposed model employs a novel modification of UNet in its
generator, called FD-UNet++, which considerably improves the reconstruction
performance. We evaluated the method on various in-vivo and simulated datasets.
Quantitative and qualitative results show the better performance of our model
over other prevalent deep learning techniques.",2024-04-19 09:52:32+00:00,"['Hesam Hakimnejad', 'Zohreh Azimifar', 'Narjes Goshtasbi']",http://arxiv.org/abs/2404.13101v1
Exploring Variational Autoencoders for Medical Image Generation: A Comprehensive Study,"Variational autoencoder (VAE) is one of the most common techniques in the
field of medical image generation, where this architecture has shown advanced
researchers in recent years and has developed into various architectures. VAE
has advantages including improving datasets by adding samples in smaller
datasets and in datasets with imbalanced classes, and this is how data
augmentation works. This paper provides a comprehensive review of studies on
VAE in medical imaging, with a special focus on their ability to create
synthetic images close to real data so that they can be used for data
augmentation. This study reviews important architectures and methods used to
develop VAEs for medical images and provides a comparison with other generative
models such as GANs on issues such as image quality, and low diversity of
generated samples. We discuss recent developments and applications in several
medical fields highlighting the ability of VAEs to improve segmentation and
classification accuracy.",2024-11-11 20:12:13+00:00,"['Khadija Rais', 'Mohamed Amroune', 'Abdelmadjid Benmachiche', 'Mohamed Yassine Haouam']",http://arxiv.org/abs/2411.07348v1
AA-SGAN: Adversarially Augmented Social GAN with Synthetic Data,"Accurately predicting pedestrian trajectories is crucial in applications such
as autonomous driving or service robotics, to name a few. Deep generative
models achieve top performance in this task, assuming enough labelled
trajectories are available for training. To this end, large amounts of
synthetically generated, labelled trajectories exist (e.g., generated by video
games). However, such trajectories are not meant to represent pedestrian motion
realistically and are ineffective at training a predictive model. We propose a
method and an architecture to augment synthetic trajectories at training time
and with an adversarial approach. We show that trajectory augmentation at
training time unleashes significant gains when a state-of-the-art generative
model is evaluated over real-world trajectories.",2024-12-23 23:17:44+00:00,"['Mirko Zaffaroni', 'Federico Signoretta', 'Marco Grangetto', 'Attilio Fiandrotti']",http://arxiv.org/abs/2412.18038v1
Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration,"The diffusion model, a new generative modeling paradigm, has achieved
significant success in generating images, audio, video, and text. It has been
adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq,
termed S2S Diffusion. Existing S2S-Diffusion models predominantly rely on fixed
or hand-crafted rules to schedule noise during the diffusion and denoising
processes. However, these models are limited by non-contextualized noise, which
fails to fully consider the characteristics of Seq2Seq tasks. In this paper, we
propose the Meta-DiffuB framework - a novel scheduler-exploiter S2S-Diffusion
paradigm designed to overcome the limitations of existing S2S-Diffusion models.
We employ Meta-Exploration to train an additional scheduler model dedicated to
scheduling contextualized noise for each sentence. Our exploiter model, an
S2S-Diffusion model, leverages the noise scheduled by our scheduler model for
updating and generation. Meta-DiffuB achieves state-of-the-art performance
compared to previous S2S-Diffusion models and fine-tuned pre-trained language
models (PLMs) across four Seq2Seq benchmark datasets. We further investigate
and visualize the impact of Meta-DiffuB's noise scheduling on the generation of
sentences with varying difficulties. Additionally, our scheduler model can
function as a ""plug-and-play"" model to enhance DiffuSeq without the need for
fine-tuning during the inference stage.",2024-10-17 04:06:02+00:00,"['Yun-Yen Chuang', 'Hung-Min Hsu', 'Kevin Lin', 'Chen-Sheng Gu', 'Ling Zhen Li', 'Ray-I Chang', 'Hung-yi Lee']",http://arxiv.org/abs/2410.13201v1
PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI,"With recent developments in Embodied Artificial Intelligence (EAI) research,
there has been a growing demand for high-quality, large-scale interactive scene
generation. While prior methods in scene synthesis have prioritized the
naturalness and realism of the generated scenes, the physical plausibility and
interactivity of scenes have been largely left unexplored. To address this
disparity, we introduce PhyScene, a novel method dedicated to generating
interactive 3D scenes characterized by realistic layouts, articulated objects,
and rich physical interactivity tailored for embodied agents. Based on a
conditional diffusion model for capturing scene layouts, we devise novel
physics- and interactivity-based guidance mechanisms that integrate constraints
from object collision, room layout, and object reachability. Through extensive
experiments, we demonstrate that PhyScene effectively leverages these guidance
functions for physically interactable scene synthesis, outperforming existing
state-of-the-art scene synthesis methods by a large margin. Our findings
suggest that the scenes generated by PhyScene hold considerable potential for
facilitating diverse skill acquisition among agents within interactive
environments, thereby catalyzing further advancements in embodied AI research.
Project website: http://physcene.github.io.",2024-04-15 05:29:23+00:00,"['Yandan Yang', 'Baoxiong Jia', 'Peiyuan Zhi', 'Siyuan Huang']",http://arxiv.org/abs/2404.09465v2
Physics-Inspired Generative Models in Medical Imaging: A Review,"Physics-inspired Generative Models (GMs), in particular Diffusion Models
(DMs) and Poisson Flow Models (PFMs), enhance Bayesian methods and promise
great utility in medical imaging. This review examines the transformative role
of such generative methods. First, a variety of physics-inspired GMs, including
Denoising Diffusion Probabilistic Models (DDPMs), Score-based Diffusion Models
(SDMs), and Poisson Flow Generative Models (PFGMs and PFGM++), are revisited,
with an emphasis on their accuracy, robustness as well as acceleration. Then,
major applications of physics-inspired GMs in medical imaging are presented,
comprising image reconstruction, image generation, and image analysis. Finally,
future research directions are brainstormed, including unification of
physics-inspired GMs, integration with Vision-Language Models (VLMs), and
potential novel applications of GMs. Since the development of generative
methods has been rapid, this review will hopefully give peers and learners a
timely snapshot of this new family of physics-driven generative models and help
capitalize their enormous potential for medical imaging.",2024-07-15 16:08:22+00:00,"['Dennis Hein', 'Afshin Bozorgpour', 'Dorit Merhof', 'Ge Wang']",http://arxiv.org/abs/2407.10856v2
Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks,"Safe and successful deployment of robots requires not only the ability to
generate complex plans but also the capacity to frequently replan and correct
execution errors. This paper addresses the challenge of long-horizon trajectory
planning under temporally extended objectives in a receding horizon manner. To
this end, we propose DOPPLER, a data-driven hierarchical framework that
generates and updates plans based on instruction specified by linear temporal
logic (LTL). Our method decomposes temporal tasks into chain of options with
hierarchical reinforcement learning from offline non-expert datasets. It
leverages diffusion models to generate options with low-level actions. We
devise a determinantal-guided posterior sampling technique during batch
generation, which improves the speed and diversity of diffusion generated
options, leading to more efficient querying. Experiments on robot navigation
and manipulation tasks demonstrate that DOPPLER can generate sequences of
trajectories that progressively satisfy the specified formulae for obstacle
avoidance and sequential visitation. Demonstration videos are available online
at: https://philiptheother.github.io/doppler/.",2024-10-03 11:10:37+00:00,"['Zeyu Feng', 'Hao Luan', 'Kevin Yuchen Ma', 'Harold Soh']",http://arxiv.org/abs/2410.02389v1
3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and High-quality Medical Image Generation,"The generation of medical images presents significant challenges due to their
high-resolution and three-dimensional nature. Existing methods often yield
suboptimal performance in generating high-quality 3D medical images, and there
is currently no universal generative framework for medical imaging. In this
paper, we introduce the 3D Medical Diffusion (3D MedDiffusion) model for
controllable, high-quality 3D medical image generation. 3D MedDiffusion
incorporates a novel, highly efficient Patch-Volume Autoencoder that compresses
medical images into latent space through patch-wise encoding and recovers back
into image space through volume-wise decoding. Additionally, we design a new
noise estimator to capture both local details and global structure information
during diffusion denoising process. 3D MedDiffusion can generate fine-detailed,
high-resolution images (up to 512x512x512) and effectively adapt to various
downstream tasks as it is trained on large-scale datasets covering CT and MRI
modalities and different anatomical regions (from head to leg). Experimental
results demonstrate that 3D MedDiffusion surpasses state-of-the-art methods in
generative quality and exhibits strong generalizability across tasks such as
sparse-view CT reconstruction, fast MRI reconstruction, and data augmentation.",2024-12-17 16:25:40+00:00,"['Haoshen Wang', 'Zhentao Liu', 'Kaicong Sun', 'Xiaodong Wang', 'Dinggang Shen', 'Zhiming Cui']",http://arxiv.org/abs/2412.13059v1
GAN-HA: A generative adversarial network with a novel heterogeneous dual-discriminator network and a new attention-based fusion strategy for infrared and visible image fusion,"Infrared and visible image fusion (IVIF) aims to preserve thermal radiation
information from infrared images while integrating texture details from visible
images. Thermal radiation information is mainly expressed through image
intensities, while texture details are typically expressed through image
gradients. However, existing dual-discriminator generative adversarial networks
(GANs) often rely on two structurally identical discriminators for learning,
which do not fully account for the distinct learning needs of infrared and
visible image information. To this end, this paper proposes a novel GAN with a
heterogeneous dual-discriminator network and an attention-based fusion strategy
(GAN-HA). Specifically, recognizing the intrinsic differences between infrared
and visible images, we propose, for the first time, a novel heterogeneous
dual-discriminator network to simultaneously capture thermal radiation
information and texture details. The two discriminators in this network are
structurally different, including a salient discriminator for infrared images
and a detailed discriminator for visible images. They are able to learn rich
image intensity information and image gradient information, respectively. In
addition, a new attention-based fusion strategy is designed in the generator to
appropriately emphasize the learned information from different source images,
thereby improving the information representation ability of the fusion result.
In this way, the fused images generated by GAN-HA can more effectively maintain
both the salience of thermal targets and the sharpness of textures. Extensive
experiments on various public datasets demonstrate the superiority of GAN-HA
over other state-of-the-art (SOTA) algorithms while showcasing its higher
potential for practical applications.",2024-04-24 17:06:52+00:00,"['Guosheng Lu', 'Zile Fang', 'Jiaju Tian', 'Haowen Huang', 'Yuelong Xu', 'Zhuolin Han', 'Yaoming Kang', 'Can Feng', 'Zhigang Zhao']",http://arxiv.org/abs/2404.15992v3
TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic Scene,"Despite advancements in Neural Implicit models for 3D surface reconstruction,
handling dynamic environments with interactions between arbitrary rigid,
non-rigid, or deformable entities remains challenging. The generic
reconstruction methods adaptable to such dynamic scenes often require
additional inputs like depth or optical flow or rely on pre-trained image
features for reasonable outcomes. These methods typically use latent codes to
capture frame-by-frame deformations. Another set of dynamic scene
reconstruction methods, are entity-specific, mostly focusing on humans, and
relies on template models. In contrast, some template-free methods bypass these
requirements and adopt traditional LBS (Linear Blend Skinning) weights for a
detailed representation of deformable object motions, although they involve
complex optimizations leading to lengthy training times. To this end, as a
remedy, this paper introduces TFS-NeRF, a template-free 3D semantic NeRF for
dynamic scenes captured from sparse or single-view RGB videos, featuring
interactions among two entities and more time-efficient than other LBS-based
approaches. Our framework uses an Invertible Neural Network (INN) for LBS
prediction, simplifying the training process. By disentangling the motions of
interacting entities and optimizing per-entity skinning weights, our method
efficiently generates accurate, semantically separable geometries. Extensive
experiments demonstrate that our approach produces high-quality reconstructions
of both deformable and non-deformable objects in complex interactions, with
improved training efficiency compared to existing methods.",2024-09-26 01:34:42+00:00,"['Sandika Biswas', 'Qianyi Wu', 'Biplab Banerjee', 'Hamid Rezatofighi']",http://arxiv.org/abs/2409.17459v4
DanceFusion: A Spatio-Temporal Skeleton Diffusion Transformer for Audio-Driven Dance Motion Reconstruction,"This paper introduces DanceFusion, a novel framework for reconstructing and
generating dance movements synchronized to music, utilizing a Spatio-Temporal
Skeleton Diffusion Transformer. The framework adeptly handles incomplete and
noisy skeletal data common in short-form dance videos on social media platforms
like TikTok. DanceFusion incorporates a hierarchical Transformer-based
Variational Autoencoder (VAE) integrated with a diffusion model, significantly
enhancing motion realism and accuracy. Our approach introduces sophisticated
masking techniques and a unique iterative diffusion process that refines the
motion sequences, ensuring high fidelity in both motion generation and
synchronization with accompanying audio cues. Comprehensive evaluations
demonstrate that DanceFusion surpasses existing methods, providing
state-of-the-art performance in generating dynamic, realistic, and
stylistically diverse dance motions. Potential applications of this framework
extend to content creation, virtual reality, and interactive entertainment,
promising substantial advancements in automated dance generation. Visit our
project page at https://th-mlab.github.io/DanceFusion/.",2024-11-07 12:11:11+00:00,"['Li Zhao', 'Zhengmin Lu']",http://arxiv.org/abs/2411.04646v1
Deepfake Detection and the Impact of Limited Computing Capabilities,"The rapid development of technologies and artificial intelligence makes
deepfakes an increasingly sophisticated and challenging-to-identify technique.
To ensure the accuracy of information and control misinformation and mass
manipulation, it is of paramount importance to discover and develop artificial
intelligence models that enable the generic detection of forged videos. This
work aims to address the detection of deepfakes across various existing
datasets in a scenario with limited computing resources. The goal is to analyze
the applicability of different deep learning techniques under these
restrictions and explore possible approaches to enhance their efficiency.",2024-02-08 11:04:34+00:00,"['Paloma Cantero-Arjona', 'Alfonso Snchez-Macin']",http://arxiv.org/abs/2402.14825v1
Singular value decompositions of third-order reduced biquaternion tensors,"In this paper, we introduce the applications of third-order reduced
biquaternion tensors in color video processing. We first develop algorithms for
computing the singular value decomposition (SVD) of a third-order reduced
biquaternion tensor via a new Ht-product. As theoretical applications, we
define the Moore-Penrose inverse of a third-order reduced biquaternion tensor
and develop its characterizations. In addition, we discuss the general (or
Hermitian) solutions to reduced biquaternion tensor equation
$\mathcal{A}\ast_{Ht} \mathcal{X}=\mathcal{B}$ as well as its least-square
solution. Finally, we compress the color video by this SVD, and the
experimental data shows that our method is faster than the compared scheme.",2024-03-04 02:50:03+00:00,"['Cui-E Yu', 'Xin Liu', 'Yang Zhang']",http://arxiv.org/abs/2403.01690v2
DeepSpeak Dataset v1.0,"We describe a large-scale dataset--DeepSpeak--of real and deepfake footage of
people talking and gesturing in front of their webcams. The real videos in this
first version of the dataset consist of 17 hours of footage from 220 diverse
individuals. Constituting more than 26 hours of footage, the fake videos
consist of a range of different state-of-the-art face-swap and lip-sync
deepfakes with natural and AI-generated voices. We expect to release future
versions of this dataset with different and updated deepfake technologies. This
dataset is made freely available for research and non-commercial uses; requests
for commercial use will be considered.",2024-08-09 22:29:43+00:00,"['Sarah Barrington', 'Matyas Bohacek', 'Hany Farid']",http://arxiv.org/abs/2408.05366v2
NimbleD: Enhancing Self-supervised Monocular Depth Estimation with Pseudo-labels and Large-scale Video Pre-training,"We introduce NimbleD, an efficient self-supervised monocular depth estimation
learning framework that incorporates supervision from pseudo-labels generated
by a large vision model. This framework does not require camera intrinsics,
enabling large-scale pre-training on publicly available videos. Our
straightforward yet effective learning strategy significantly enhances the
performance of fast and lightweight models without introducing any overhead,
allowing them to achieve performance comparable to state-of-the-art
self-supervised monocular depth estimation models. This advancement is
particularly beneficial for virtual and augmented reality applications
requiring low latency inference. The source code, model weights, and
acknowledgments are available at https://github.com/xapaxca/nimbled .",2024-08-26 10:50:14+00:00,"['Albert Luginov', 'Muhammad Shahzad']",http://arxiv.org/abs/2408.14177v1
A Full Transformer-based Framework for Automatic Pain Estimation using Videos,"The automatic estimation of pain is essential in designing an optimal pain
management system offering reliable assessment and reducing the suffering of
patients. In this study, we present a novel full transformer-based framework
consisting of a Transformer in Transformer (TNT) model and a Transformer
leveraging cross-attention and self-attention blocks. Elaborating on videos
from the BioVid database, we demonstrate state-of-the-art performances, showing
the efficacy, efficiency, and generalization capability across all the primary
pain estimation tasks.",2024-12-19 17:45:08+00:00,"['Stefanos Gkikas', 'Manolis Tsiknakis']",http://arxiv.org/abs/2412.15095v1
Computational Complexity of Game Boy Games,"We analyze the computational complexity of several popular video games
released for the Nintendo Game Boy video game console. We analyze the
complexity of generalized versions of four popular Game Boy games: Donkey Kong,
Wario Land, Harvest Moon GB, and Mole Mania. We provide original proofs showing
that these games are \textbf{NP}-hard. Our proofs rely on Karp reductions from
four of Karp's original 21 \textbf{NP}-complete problems: \textsc{Sat},
\textsc{3-Cnf-Sat}, \textsc{Hamiltonian Cycle}, and \textsc{Knapsack}. We also
discuss proofs easily derived from known results demonstrating the
\textbf{NP}-hardness of Lock `n' Chase and The Lion King.",2024-12-20 00:25:43+00:00,"['Hayder Tirmazi', 'Ali Tirmazi', 'Tien Phuoc Tran']",http://arxiv.org/abs/2412.15469v1
ReNoise: Real Image Inversion Through Iterative Noising,"Recent advancements in text-guided diffusion models have unlocked powerful
image manipulation capabilities. However, applying these methods to real images
necessitates the inversion of the images into the domain of the pretrained
diffusion model. Achieving faithful inversion remains a challenge, particularly
for more recent models trained to generate images with a small number of
denoising steps. In this work, we introduce an inversion method with a high
quality-to-operation ratio, enhancing reconstruction accuracy without
increasing the number of operations. Building on reversing the diffusion
sampling process, our method employs an iterative renoising mechanism at each
inversion sampling step. This mechanism refines the approximation of a
predicted point along the forward diffusion trajectory, by iteratively applying
the pretrained diffusion model, and averaging these predictions. We evaluate
the performance of our ReNoise technique using various sampling algorithms and
models, including recent accelerated diffusion models. Through comprehensive
evaluations and comparisons, we show its effectiveness in terms of both
accuracy and speed. Furthermore, we confirm that our method preserves
editability by demonstrating text-driven image editing on real images.",2024-03-21 17:52:08+00:00,"['Daniel Garibi', 'Or Patashnik', 'Andrey Voynov', 'Hadar Averbuch-Elor', 'Daniel Cohen-Or']",http://arxiv.org/abs/2403.14602v1
Constrained Diffusion Implicit Models,"This paper describes an efficient algorithm for solving noisy linear inverse
problems using pretrained diffusion models. Extending the paradigm of denoising
diffusion implicit models (DDIM), we propose constrained diffusion implicit
models (CDIM) that modify the diffusion updates to enforce a constraint upon
the final output. For noiseless inverse problems, CDIM exactly satisfies the
constraints; in the noisy case, we generalize CDIM to satisfy an exact
constraint on the residual distribution of the noise. Experiments across a
variety of tasks and metrics show strong performance of CDIM, with analogous
inference acceleration to unconstrained DDIM: 10 to 50 times faster than
previous conditional diffusion methods. We demonstrate the versatility of our
approach on many problems including super-resolution, denoising, inpainting,
deblurring, and 3D point cloud reconstruction.",2024-11-01 04:51:24+00:00,"['Vivek Jayaram', 'Ira Kemelmacher-Shlizerman', 'Steven M. Seitz', 'John Thickstun']",http://arxiv.org/abs/2411.00359v1
FORA: Fast-Forward Caching in Diffusion Transformer Acceleration,"Diffusion transformers (DiT) have become the de facto choice for generating
high-quality images and videos, largely due to their scalability, which enables
the construction of larger models for enhanced performance. However, the
increased size of these models leads to higher inference costs, making them
less attractive for real-time applications. We present Fast-FORward CAching
(FORA), a simple yet effective approach designed to accelerate DiT by
exploiting the repetitive nature of the diffusion process. FORA implements a
caching mechanism that stores and reuses intermediate outputs from the
attention and MLP layers across denoising steps, thereby reducing computational
overhead. This approach does not require model retraining and seamlessly
integrates with existing transformer-based diffusion models. Experiments show
that FORA can speed up diffusion transformers several times over while only
minimally affecting performance metrics such as the IS Score and FID. By
enabling faster processing with minimal trade-offs in quality, FORA represents
a significant advancement in deploying diffusion transformers for real-time
applications. Code will be made publicly available at:
https://github.com/prathebaselva/FORA.",2024-07-01 16:14:37+00:00,"['Pratheba Selvaraju', 'Tianyu Ding', 'Tianyi Chen', 'Ilya Zharkov', 'Luming Liang']",http://arxiv.org/abs/2407.01425v1
IG-CFAT: An Improved GAN-Based Framework for Effectively Exploiting Transformers in Real-World Image Super-Resolution,"In the field of single image super-resolution (SISR), transformer-based
models, have demonstrated significant advancements. However, the potential and
efficiency of these models in applied fields such as real-world image
super-resolution have been less noticed and there are substantial opportunities
for improvement. Recently, composite fusion attention transformer (CFAT),
outperformed previous state-of-the-art (SOTA) models in classic image
super-resolution. In this paper, we propose a novel GAN-based framework by
incorporating the CFAT model to effectively exploit the performance of
transformers in real-world image super-resolution. In our proposed approach, we
integrate a semantic-aware discriminator to reconstruct fine details more
accurately and employ an adaptive degradation model to better simulate
real-world degradations. Moreover, we introduce a new combination of loss
functions by adding wavelet loss to loss functions of GAN-based models to
better recover high-frequency details. Empirical results demonstrate that
IG-CFAT significantly outperforms existing SOTA models in both quantitative and
qualitative metrics. Our proposed model revolutionizes the field of real-world
image super-resolution and demonstrates substantially better performance in
recovering fine details and generating realistic textures. The introduction of
IG-CFAT offers a robust and adaptable solution for real-world image
super-resolution tasks.",2024-06-19 20:21:26+00:00,"['Alireza Aghelan', 'Ali Amiryan', 'Abolfazl Zarghani', 'Modjtaba Rouhani']",http://arxiv.org/abs/2406.13815v4
LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision,"End-to-end audio-conditioned latent diffusion models (LDMs) have been widely
adopted for audio-driven portrait animation, demonstrating their effectiveness
in generating lifelike and high-resolution talking videos. However, direct
application of audio-conditioned LDMs to lip-synchronization (lip-sync) tasks
results in suboptimal lip-sync accuracy. Through an in-depth analysis, we
identified the underlying cause as the ""shortcut learning problem"", wherein the
model predominantly learns visual-visual shortcuts while neglecting the
critical audio-visual correlations. To address this issue, we explored
different approaches for integrating SyncNet supervision into audio-conditioned
LDMs to explicitly enforce the learning of audio-visual correlations. Since the
performance of SyncNet directly influences the lip-sync accuracy of the
supervised model, the training of a well-converged SyncNet becomes crucial. We
conducted the first comprehensive empirical studies to identify key factors
affecting SyncNet convergence. Based on our analysis, we introduce
StableSyncNet, with an architecture designed for stable convergence. Our
StableSyncNet achieved a significant improvement in accuracy, increasing from
91% to 94% on the HDTF test set. Additionally, we introduce a novel Temporal
Representation Alignment (TREPA) mechanism to enhance temporal consistency in
the generated videos. Experimental results show that our method surpasses
state-of-the-art lip-sync approaches across various evaluation metrics on the
HDTF and VoxCeleb2 datasets.",2024-12-12 13:20:52+00:00,"['Chunyu Li', 'Chao Zhang', 'Weikai Xu', 'Jingyu Lin', 'Jinghui Xie', 'Weiguo Feng', 'Bingyue Peng', 'Cunjian Chen', 'Weiwei Xing']",http://arxiv.org/abs/2412.09262v2
Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion,"This paper presents Diffusion Forcing, a new training paradigm where a
diffusion model is trained to denoise a set of tokens with independent
per-token noise levels. We apply Diffusion Forcing to sequence generative
modeling by training a causal next-token prediction model to generate one or
several future tokens without fully diffusing past ones. Our approach is shown
to combine the strengths of next-token prediction models, such as
variable-length generation, with the strengths of full-sequence diffusion
models, such as the ability to guide sampling to desirable trajectories. Our
method offers a range of additional capabilities, such as (1) rolling-out
sequences of continuous tokens, such as video, with lengths past the training
horizon, where baselines diverge and (2) new sampling and guiding schemes that
uniquely profit from Diffusion Forcing's variable-horizon and causal
architecture, and which lead to marked performance gains in decision-making and
planning tasks. In addition to its empirical success, our method is proven to
optimize a variational lower bound on the likelihoods of all subsequences of
tokens drawn from the true joint distribution. Project website:
https://boyuan.space/diffusion-forcing",2024-07-01 15:43:25+00:00,"['Boyuan Chen', 'Diego Marti Monso', 'Yilun Du', 'Max Simchowitz', 'Russ Tedrake', 'Vincent Sitzmann']",http://arxiv.org/abs/2407.01392v4
Solution for Authenticity Identification of Typical Target Remote Sensing Images,"In this paper, we propose a basic RGB single-mode model based on weakly
supervised training under pseudo labels, which performs high-precision
authenticity identification under multi-scene typical target remote sensing
images. Due to the imprecision of Mask generation, we divide the task into two
sub-tasks: generating pseudo-mask and fine-tuning model based on generated
Masks. In generating pseudo masks, we use MM-Fusion as the base model to
generate masks for large objects such as planes and ships. By manually
calibrating the Mask of a small object such as a car, a highly accurate
pseudo-mask is obtained. For the task of fine-tuning models based on generating
masks, we use the WSCL model as the base model. It is worth noting that due to
the difference between the generated pseudo-Masks and the real Masks, we
discard the image feature extractors such as SRM and Noiseprint++ in WSCL, and
select the unscaled original image for training alone, which greatly ensures
the match between the image and the original label. The final trained model
achieved a score of 90.7702 on the test set.",2024-05-03 04:04:55+00:00,"['Yipeng Lin', 'Xinger Li', 'Yang Yang']",http://arxiv.org/abs/2405.02362v1
Diff-PCC: Diffusion-based Neural Compression for 3D Point Clouds,"Stable diffusion networks have emerged as a groundbreaking development for
their ability to produce realistic and detailed visual content. This
characteristic renders them ideal decoders, capable of producing high-quality
and aesthetically pleasing reconstructions. In this paper, we introduce the
first diffusion-based point cloud compression method, dubbed Diff-PCC, to
leverage the expressive power of the diffusion model for generative and
aesthetically superior decoding. Different from the conventional autoencoder
fashion, a dual-space latent representation is devised in this paper, in which
a compressor composed of two independent encoding backbones is considered to
extract expressive shape latents from distinct latent spaces. At the decoding
side, a diffusion-based generator is devised to produce high-quality
reconstructions by considering the shape latents as guidance to stochastically
denoise the noisy point clouds. Experiments demonstrate that the proposed
Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711 dB
BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while
attaining superior subjective quality. Source code will be made publicly
available.",2024-08-20 04:55:29+00:00,"['Kai Liu', 'Kang You', 'Pan Gao']",http://arxiv.org/abs/2408.10543v1
Heuristically Adaptive Diffusion-Model Evolutionary Strategy,"Diffusion Models represent a significant advancement in generative modeling,
employing a dual-phase process that first degrades domain-specific information
via Gaussian noise and restores it through a trainable model. This framework
enables pure noise-to-data generation and modular reconstruction of, images or
videos. Concurrently, evolutionary algorithms employ optimization methods
inspired by biological principles to refine sets of numerical parameters
encoding potential solutions to rugged objective functions. Our research
reveals a fundamental connection between diffusion models and evolutionary
algorithms through their shared underlying generative mechanisms: both methods
generate high-quality samples via iterative refinement on random initial
distributions. By employing deep learning-based diffusion models as generative
models across diverse evolutionary tasks and iteratively refining diffusion
models with heuristically acquired databases, we can iteratively sample
potentially better-adapted offspring parameters, integrating them into
successive generations of the diffusion model. This approach achieves efficient
convergence toward high-fitness parameters while maintaining explorative
diversity. Diffusion models introduce enhanced memory capabilities into
evolutionary algorithms, retaining historical information across generations
and leveraging subtle data correlations to generate refined samples. We elevate
evolutionary algorithms from procedures with shallow heuristics to frameworks
with deep memory. By deploying classifier-free guidance for conditional
sampling at the parameter level, we achieve precise control over evolutionary
search dynamics to further specific genotypical, phenotypical, or
population-wide traits. Our framework marks a major heuristic and algorithmic
transition, offering increased flexibility, precision, and control in
evolutionary optimization processes.",2024-11-20 16:06:28+00:00,"['Benedikt Hartl', 'Yanbo Zhang', 'Hananel Hazan', 'Michael Levin']",http://arxiv.org/abs/2411.13420v1
Longitudinal Causal Image Synthesis,"Clinical decision-making relies heavily on causal reasoning and longitudinal
analysis. For example, for a patient with Alzheimer's disease (AD), how will
the brain grey matter atrophy in a year if intervened on the A-beta level in
cerebrospinal fluid? The answer is fundamental to diagnosis and follow-up
treatment. However, this kind of inquiry involves counterfactual medical images
which can not be acquired by instrumental or correlation-based image synthesis
models. Yet, such queries require counterfactual medical images, not obtainable
through standard image synthesis models. Hence, a causal longitudinal image
synthesis (CLIS) method, enabling the synthesis of such images, is highly
valuable. However, building a CLIS model confronts three primary yet unmet
challenges: mismatched dimensionality between high-dimensional images and
low-dimensional tabular variables, inconsistent collection intervals of
follow-up data, and inadequate causal modeling capability of existing causal
graph methods for image data. In this paper, we established a tabular-visual
causal graph (TVCG) for CLIS overcoming these challenges through a novel
integration of generative imaging, continuous-time modeling, and structural
causal models combined with a neural network. We train our CLIS based on the
ADNI dataset and evaluate it on two other AD datasets, which illustrate the
outstanding yet controllable quality of the synthesized images and the
contributions of synthesized MRI to the characterization of AD progression,
substantiating the reliability and utility in clinics.",2024-10-23 09:13:11+00:00,"['Yujia Li', 'Han Li', 'ans S. Kevin Zhou']",http://arxiv.org/abs/2410.17691v1
Progressive Boundary Guided Anomaly Synthesis for Industrial Anomaly Detection,"Unsupervised anomaly detection methods can identify surface defects in
industrial images by leveraging only normal samples for training. Due to the
risk of overfitting when learning from a single class, anomaly synthesis
strategies are introduced to enhance detection capability by generating
artificial anomalies. However, existing strategies heavily rely on anomalous
textures from auxiliary datasets. Moreover, their limitations in the coverage
and directionality of anomaly synthesis may result in a failure to capture
useful information and lead to significant redundancy. To address these issues,
we propose a novel Progressive Boundary-guided Anomaly Synthesis (PBAS)
strategy, which can directionally synthesize crucial feature-level anomalies
without auxiliary textures. It consists of three core components: Approximate
Boundary Learning (ABL), Anomaly Feature Synthesis (AFS), and Refined Boundary
Optimization (RBO). To make the distribution of normal samples more compact,
ABL first learns an approximate decision boundary by center constraint, which
improves the center initialization through feature alignment. AFS then
directionally synthesizes anomalies with more flexible scales guided by the
hypersphere distribution of normal features. Since the boundary is so loose
that it may contain real anomalies, RBO refines the decision boundary through
the binary classification of artificial anomalies and normal features.
Experimental results show that our method achieves state-of-the-art performance
and the fastest detection speed on three widely used industrial datasets,
including MVTec AD, VisA, and MPDD. The code will be available at:
https://github.com/cqylunlun/PBAS.",2024-12-23 10:26:26+00:00,"['Qiyu Chen', 'Huiyuan Luo', 'Han Gao', 'Chengkan Lv', 'Zhengtao Zhang']",http://arxiv.org/abs/2412.17458v1
Generating Realistic X-ray Scattering Images Using Stable Diffusion and Human-in-the-loop Annotations,"We fine-tuned a foundational stable diffusion model using X-ray scattering
images and their corresponding descriptions to generate new scientific images
from given prompts. However, some of the generated images exhibit significant
unrealistic artifacts, commonly known as ""hallucinations"". To address this
issue, we trained various computer vision models on a dataset composed of 60%
human-approved generated images and 40% experimental images to detect
unrealistic images. The classified images were then reviewed and corrected by
human experts, and subsequently used to further refine the classifiers in next
rounds of training and inference. Our evaluations demonstrate the feasibility
of generating high-fidelity, domain-specific images using a fine-tuned
diffusion model. We anticipate that generative AI will play a crucial role in
enhancing data augmentation and driving the development of digital twins in
scientific research facilities.",2024-08-22 20:23:04+00:00,"['Zhuowen Zhao', 'Xiaoya Chong', 'Tanny Chavez', 'Alexander Hexemer']",http://arxiv.org/abs/2408.12720v1
Bridging the Gap between Learning and Inference for Diffusion-Based Molecule Generation,"The efficacy of diffusion models in generating a spectrum of data modalities,
including images, text, and videos, has spurred inquiries into their utility in
molecular generation, yielding significant advancements in the field. However,
the molecular generation process with diffusion models involves multiple
autoregressive steps over a finite time horizon, leading to exposure bias
issues inherently. To address the exposure bias issue, we propose a training
framework named GapDiff. The core idea of GapDiff is to utilize model-predicted
conformations as ground truth probabilistically during training, aiming to
mitigate the data distributional disparity between training and inference,
thereby enhancing the affinity of generated molecules. We conduct experiments
using a 3D molecular generation model on the CrossDocked2020 dataset, and the
vina energy and diversity demonstrate the potency of our framework with
superior affinity. GapDiff is available at
\url{https://github.com/HUGHNew/gapdiff}.",2024-11-08 10:53:39+00:00,"['Peidong Liu', 'Wenbo Zhang', 'Xue Zhe', 'Jiancheng Lv', 'Xianggen Liu']",http://arxiv.org/abs/2411.05472v1
Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions,"The many variations of Implicit Neural Representations (INRs), where a neural
network is trained as a continuous representation of a signal, have tremendous
practical utility for downstream tasks including novel view synthesis, video
compression, and image super-resolution. Unfortunately, the inner workings of
these networks are seriously under-studied. Our work, eXplaining the Implicit
Neural Canvas (XINC), is a unified framework for explaining properties of INRs
by examining the strength of each neuron's contribution to each output pixel.
We call the aggregate of these contribution maps the Implicit Neural Canvas and
we use this concept to demonstrate that the INRs we study learn to ""see"" the
frames they represent in surprising ways. For example, INRs tend to have highly
distributed representations. While lacking high-level object semantics, they
have a significant bias for color and edges, and are almost entirely
space-agnostic. We arrive at our conclusions by examining how objects are
represented across time in video INRs, using clustering to visualize similar
neurons across layers and architectures, and show that this is dominated by
motion. These insights demonstrate the general usefulness of our analysis
framework. Our project page is available at https://namithap10.github.io/xinc.",2024-01-18 18:57:40+00:00,"['Namitha Padmanabhan', 'Matthew Gwilliam', 'Pulkit Kumar', 'Shishira R Maiya', 'Max Ehrlich', 'Abhinav Shrivastava']",http://arxiv.org/abs/2401.10217v2
Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure,"In this work, we study the generalizability of diffusion models by looking
into the hidden properties of the learned score functions, which are
essentially a series of deep denoisers trained on various noise levels. We
observe that as diffusion models transition from memorization to
generalization, their corresponding nonlinear diffusion denoisers exhibit
increasing linearity. This discovery leads us to investigate the linear
counterparts of the nonlinear diffusion models, which are a series of linear
models trained to match the function mappings of the nonlinear diffusion
denoisers. Surprisingly, these linear denoisers are approximately the optimal
denoisers for a multivariate Gaussian distribution characterized by the
empirical mean and covariance of the training dataset. This finding implies
that diffusion models have the inductive bias towards capturing and utilizing
the Gaussian structure (covariance information) of the training dataset for
data generation. We empirically demonstrate that this inductive bias is a
unique property of diffusion models in the generalization regime, which becomes
increasingly evident when the model's capacity is relatively small compared to
the training dataset size. In the case that the model is highly
overparameterized, this inductive bias emerges during the initial training
phases before the model fully memorizes its training data. Our study provides
crucial insights into understanding the notable strong generalization
phenomenon recently observed in real-world diffusion models.",2024-10-31 15:57:04+00:00,"['Xiang Li', 'Yixiang Dai', 'Qing Qu']",http://arxiv.org/abs/2410.24060v5
ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer,"We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion
Transformer, that innovatively combines autoregressive and diffusion paradigms
for modeling continuous visual information. By introducing a block-wise
autoregressive unit, ACDiT offers a flexible interpolation between token-wise
autoregression and full-sequence diffusion, bypassing the limitations of
discrete tokenization. The generation of each block is formulated as a
conditional diffusion process, conditioned on prior blocks. ACDiT is easy to
implement, as simple as creating a Skip-Causal Attention Mask (SCAM) on
standard diffusion transformer during training. During inference, the process
iterates between diffusion denoising and autoregressive decoding that can make
full use of KV-Cache. We show that ACDiT performs best among all autoregressive
baselines under similar model scales on image and video generation tasks. We
also demonstrate that benefiting from autoregressive modeling, pretrained ACDiT
can be transferred in visual understanding tasks despite being trained with the
diffusion objective. The analysis of the trade-off between autoregressive
modeling and diffusion demonstrates the potential of ACDiT to be used in
long-horizon visual generation tasks. We hope that ACDiT offers a novel
perspective on visual autoregressive generation and unlocks new avenues for
unified models.",2024-12-10 18:13:20+00:00,"['Jinyi Hu', 'Shengding Hu', 'Yuxuan Song', 'Yufei Huang', 'Mingxuan Wang', 'Hao Zhou', 'Zhiyuan Liu', 'Wei-Ying Ma', 'Maosong Sun']",http://arxiv.org/abs/2412.07720v2
From Pixel to Cancer: Cellular Automata in Computed Tomography,"AI for cancer detection encounters the bottleneck of data scarcity,
annotation difficulty, and low prevalence of early tumors. Tumor synthesis
seeks to create artificial tumors in medical images, which can greatly
diversify the data and annotations for AI training. However, current tumor
synthesis approaches are not applicable across different organs due to their
need for specific expertise and design. This paper establishes a set of generic
rules to simulate tumor development. Each cell (pixel) is initially assigned a
state between zero and ten to represent the tumor population, and a tumor can
be developed based on three rules to describe the process of growth, invasion,
and death. We apply these three generic rules to simulate tumor
development--from pixel to cancer--using cellular automata. We then integrate
the tumor state into the original computed tomography (CT) images to generate
synthetic tumors across different organs. This tumor synthesis approach allows
for sampling tumors at multiple stages and analyzing tumor-organ interaction.
Clinically, a reader study involving three expert radiologists reveals that the
synthetic tumors and their developing trajectories are convincingly realistic.
Technically, we analyze and simulate tumor development at various stages using
9,262 raw, unlabeled CT images sourced from 68 hospitals worldwide. The
performance in segmenting tumors in the liver, pancreas, and kidneys exceeds
prevailing literature benchmarks, underlining the immense potential of tumor
synthesis, especially for earlier cancer detection.
  The code and models are available at
https://github.com/MrGiovanni/Pixel2Cancer",2024-03-11 06:46:31+00:00,"['Yuxiang Lai', 'Xiaoxi Chen', 'Angtian Wang', 'Alan Yuille', 'Zongwei Zhou']",http://arxiv.org/abs/2403.06459v2
HSIGene: A Foundation Model For Hyperspectral Image Generation,"Hyperspectral image (HSI) plays a vital role in various fields such as
agriculture and environmental monitoring. However, due to the expensive
acquisition cost, the number of hyperspectral images is limited, degenerating
the performance of downstream tasks. Although some recent studies have
attempted to employ diffusion models to synthesize HSIs, they still struggle
with the scarcity of HSIs, affecting the reliability and diversity of the
generated images. Some studies propose to incorporate multi-modal data to
enhance spatial diversity, but the spectral fidelity cannot be ensured. In
addition, existing HSI synthesis models are typically uncontrollable or only
support single-condition control, limiting their ability to generate accurate
and reliable HSIs. To alleviate these issues, we propose HSIGene, a novel HSI
generation foundation model which is based on latent diffusion and supports
multi-condition control, allowing for more precise and reliable HSI generation.
To enhance the spatial diversity of the training data while preserving spectral
fidelity, we propose a new data augmentation method based on spatial
super-resolution, in which HSIs are upscaled first, and thus abundant training
patches could be obtained by cropping the high-resolution HSIs. In addition, to
improve the perceptual quality of the augmented data, we introduce a novel
two-stage HSI super-resolution framework, which first applies RGB bands
super-resolution and then utilizes our proposed Rectangular Guided Attention
Network (RGAN) for guided HSI super-resolution. Experiments demonstrate that
the proposed model is capable of generating a vast quantity of realistic HSIs
for downstream tasks such as denoising and super-resolution. The code and
models are available at https://github.com/LiPang/HSIGene.",2024-09-19 05:17:44+00:00,"['Li Pang', 'Xiangyong Cao', 'Datao Tang', 'Shuang Xu', 'Xueru Bai', 'Feng Zhou', 'Deyu Meng']",http://arxiv.org/abs/2409.12470v2
Context-Guided Spatio-Temporal Video Grounding,"Spatio-temporal video grounding (or STVG) task aims at locating a
spatio-temporal tube for a specific instance given a text query. Despite
advancements, current methods easily suffer the distractors or heavy object
appearance variations in videos due to insufficient object information from the
text, leading to degradation. Addressing this, we propose a novel framework,
context-guided STVG (CG-STVG), which mines discriminative instance context for
object in videos and applies it as a supplementary guidance for target
localization. The key of CG-STVG lies in two specially designed modules,
including instance context generation (ICG), which focuses on discovering
visual context information (in both appearance and motion) of the instance, and
instance context refinement (ICR), which aims to improve the instance context
from ICG by eliminating irrelevant or even harmful information from the
context. During grounding, ICG, together with ICR, are deployed at each
decoding stage of a Transformer architecture for instance context learning.
Particularly, instance context learned from one decoding stage is fed to the
next stage, and leveraged as a guidance containing rich and discriminative
object feature to enhance the target-awareness in decoding feature, which
conversely benefits generating better new instance context for improving
localization finally. Compared to existing methods, CG-STVG enjoys object
information in text query and guidance from mined instance visual context for
more accurate target localization. In our experiments on three benchmarks,
including HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in
m_tIoU and m_vIoU on all of them, showing its efficacy. The code will be
released at https://github.com/HengLan/CGSTVG.",2024-01-03 07:05:49+00:00,"['Xin Gu', 'Heng Fan', 'Yan Huang', 'Tiejian Luo', 'Libo Zhang']",http://arxiv.org/abs/2401.01578v1
Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion,"The landscape of deep learning research is moving towards innovative
strategies to harness the true potential of data. Traditionally, emphasis has
been on scaling model architectures, resulting in large and complex neural
networks, which can be difficult to train with limited computational resources.
However, independently of the model size, data quality (i.e. amount and
variability) is still a major factor that affects model generalization. In this
work, we propose a novel technique to exploit available data through the use of
automatic data augmentation for the tasks of image classification and semantic
segmentation. We introduce the first Differentiable Augmentation Search method
(DAS) to generate variations of images that can be processed as videos.
Compared to previous approaches, DAS is extremely fast and flexible, allowing
the search on very large search spaces in less than a GPU day. Our intuition is
that the increased receptive field in the temporal dimension provided by DAS
could lead to benefits also to the spatial receptive field. More specifically,
we leverage DAS to guide the reshaping of the spatial receptive field by
selecting task-dependant transformations. As a result, compared to standard
augmentation alternatives, we improve in terms of accuracy on ImageNet,
Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when
plugging-in our DAS over different light-weight video backbones.",2024-03-22 13:27:57+00:00,"['Sofia Casarin', 'Cynthia I. Ugwu', 'Sergio Escalera', 'Oswald Lanz']",http://arxiv.org/abs/2403.15194v1
Passive Screen-to-Camera Communication,"A recent technology known as transparent screens is transforming windows into
displays. These smart windows are present in buses, airports and offices. They
can remain transparent, as a normal window, or display relevant information
that overlays their panoramic views. In this paper, we propose transforming
these windows not only into screens but also into wireless transmitters. To
achieve this goal, we build upon the research area of screen-to-camera
communication. In this area, videos are modified in a way that smartphone
cameras can decode data out of them, while this data remains invisible to the
viewers. A person sees a normal video, but the camera sees the video plus
additional information. In this communication method, one of the biggest
disadvantages is the traditional screens' power consumption, more than 80% of
which is used to generate light. To solve this, we employ novel transparent
screens relying on ambient light to display pictures, hence eliminating the
power source. However, this comes at the cost of a lower image quality, since
they use variable and out-of-control environment light, instead of generating a
constant and strong light by LED panels. Our work, dubbed PassiveCam, overcomes
the challenge of creating the first screen-to-camera communication link using
passive displays. This paper presents two main contributions. First, we analyze
and modify existing screens and encoding methods to embed information reliably
in ambient light. Second, we develop an Android App that optimizes the decoding
process, obtaining a real-time performance. Our evaluation, which considers a
musical application, shows a Packet Success Rate (PSR) of close to 90%. In
addition, our real-time application achieves response times of 530 ms and 1071
ms when the camera is static and when it is hand-held, respectively.",2024-03-24 15:06:24+00:00,"['Seyed Keyarash Ghiasi', 'Marco Kaldenbach', 'Marco Zuniga']",http://arxiv.org/abs/2403.16185v1
TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning,"Traffic video description and analysis have received much attention recently
due to the growing demand for efficient and reliable urban surveillance
systems. Most existing methods only focus on locating traffic event segments,
which severely lack descriptive details related to the behaviour and context of
all the subjects of interest in the events. In this paper, we present
TrafficVLM, a novel multi-modal dense video captioning model for vehicle ego
camera view. TrafficVLM models traffic video events at different levels of
analysis, both spatially and temporally, and generates long fine-grained
descriptions for the vehicle and pedestrian at different phases of the event.
We also propose a conditional component for TrafficVLM to control the
generation outputs and a multi-task fine-tuning paradigm to enhance
TrafficVLM's learning capability. Experiments show that TrafficVLM performs
well on both vehicle and overhead camera views. Our solution achieved
outstanding results in Track 2 of the AI City Challenge 2024, ranking us third
in the challenge standings. Our code is publicly available at
https://github.com/quangminhdinh/TrafficVLM.",2024-04-14 14:51:44+00:00,"['Quang Minh Dinh', 'Minh Khoi Ho', 'Anh Quan Dang', 'Hung Phong Tran']",http://arxiv.org/abs/2404.09275v1
LADDER: An Efficient Framework for Video Frame Interpolation,"Video Frame Interpolation (VFI) is a crucial technique in various
applications such as slow-motion generation, frame rate conversion, video frame
restoration etc. This paper introduces an efficient video frame interpolation
framework that aims to strike a favorable balance between efficiency and
quality. Our framework follows a general paradigm consisting of a flow
estimator and a refinement module, while incorporating carefully designed
components. First of all, we adopt depth-wise convolution with large kernels in
the flow estimator that simultaneously reduces the parameters and enhances the
receptive field for encoding rich context and handling complex motion.
Secondly, diverging from a common design for the refinement module with a
UNet-structure (encoder-decoder structure), which we find redundant, our
decoder-only refinement module directly enhances the result from coarse to fine
features, offering a more efficient process. In addition, to address the
challenge of handling high-definition frames, we also introduce an innovative
HD-aware augmentation strategy during training, leading to consistent
enhancement on HD images. Extensive experiments are conducted on diverse
datasets, Vimeo90K, UCF101, Xiph and SNU-FILM. The results demonstrate that our
approach achieves state-of-the-art performance with clear improvement while
requiring much less FLOPs and parameters, reaching to a better spot for
balancing efficiency and quality.",2024-04-17 06:47:17+00:00,"['Tong Shen', 'Dong Li', 'Ziheng Gao', 'Lu Tian', 'Emad Barsoum']",http://arxiv.org/abs/2404.11108v1
3D Multi-frame Fusion for Video Stabilization,"In this paper, we present RStab, a novel framework for video stabilization
that integrates 3D multi-frame fusion through volume rendering. Departing from
conventional methods, we introduce a 3D multi-frame perspective to generate
stabilized images, addressing the challenge of full-frame generation while
preserving structure. The core of our approach lies in Stabilized Rendering
(SR), a volume rendering module, which extends beyond the image fusion by
incorporating feature fusion. The core of our RStab framework lies in
Stabilized Rendering (SR), a volume rendering module, fusing multi-frame
information in 3D space. Specifically, SR involves warping features and colors
from multiple frames by projection, fusing them into descriptors to render the
stabilized image. However, the precision of warped information depends on the
projection accuracy, a factor significantly influenced by dynamic regions. In
response, we introduce the Adaptive Ray Range (ARR) module to integrate depth
priors, adaptively defining the sampling range for the projection process.
Additionally, we propose Color Correction (CC) assisting geometric constraints
with optical flow for accurate color aggregation. Thanks to the three modules,
our RStab demonstrates superior performance compared with previous stabilizers
in the field of view (FOV), image quality, and video stability across various
datasets.",2024-04-19 13:43:14+00:00,"['Zhan Peng', 'Xinyi Ye', 'Weiyue Zhao', 'Tianqi Liu', 'Huiqiang Sun', 'Baopu Li', 'Zhiguo Cao']",http://arxiv.org/abs/2404.12887v1
Uncertainty-boosted Robust Video Activity Anticipation,"Video activity anticipation aims to predict what will happen in the future,
embracing a broad application prospect ranging from robot vision and autonomous
driving. Despite the recent progress, the data uncertainty issue, reflected as
the content evolution process and dynamic correlation in event labels, has been
somehow ignored. This reduces the model generalization ability and deep
understanding on video content, leading to serious error accumulation and
degraded performance. In this paper, we address the uncertainty learning
problem and propose an uncertainty-boosted robust video activity anticipation
framework, which generates uncertainty values to indicate the credibility of
the anticipation results. The uncertainty value is used to derive a temperature
parameter in the softmax function to modulate the predicted target activity
distribution. To guarantee the distribution adjustment, we construct a
reasonable target activity label representation by incorporating the activity
evolution from the temporal class correlation and the semantic relationship.
Moreover, we quantify the uncertainty into relative values by comparing the
uncertainty among sample pairs and their temporal-lengths. This relative
strategy provides a more accessible way in uncertainty modeling than
quantifying the absolute uncertainty values on the whole dataset. Experiments
on multiple backbones and benchmarks show our framework achieves promising
performance and better robustness/interpretability. Source codes are available
at https://github.com/qzhb/UbRV2A.",2024-04-29 12:31:38+00:00,"['Zhaobo Qi', 'Shuhui Wang', 'Weigang Zhang', 'Qingming Huang']",http://arxiv.org/abs/2404.18648v1
Behavior Imitation for Manipulator Control and Grasping with Deep Reinforcement Learning,"The existing Motion Imitation models typically require expert data obtained
through MoCap devices, but the vast amount of training data needed is difficult
to acquire, necessitating substantial investments of financial resources,
manpower, and time. This project combines 3D human pose estimation with
reinforcement learning, proposing a novel model that simplifies Motion
Imitation into a prediction problem of joint angle values in reinforcement
learning. This significantly reduces the reliance on vast amounts of training
data, enabling the agent to learn an imitation policy from just a few seconds
of video and exhibit strong generalization capabilities. It can quickly apply
the learned policy to imitate human arm motions in unfamiliar videos. The model
first extracts skeletal motions of human arms from a given video using 3D human
pose estimation. These extracted arm motions are then morphologically
retargeted onto a robotic manipulator. Subsequently, the retargeted motions are
used to generate reference motions. Finally, these reference motions are used
to formulate a reinforcement learning problem, enabling the agent to learn a
policy for imitating human arm motions. This project excels at imitation tasks
and demonstrates robust transferability, accurately imitating human arm motions
from other unfamiliar videos. This project provides a lightweight, convenient,
efficient, and accurate Motion Imitation model. While simplifying the complex
process of Motion Imitation, it achieves notably outstanding performance.",2024-05-02 13:43:22+00:00,['Liu Qiyuan'],http://arxiv.org/abs/2405.01284v1
SuperGaussian: Repurposing Video Models for 3D Super Resolution,"We present a simple, modular, and generic method that upsamples coarse 3D
models by adding geometric and appearance details. While generative 3D models
now exist, they do not yet match the quality of their counterparts in image and
video domains. We demonstrate that it is possible to directly repurpose
existing (pretrained) video models for 3D super-resolution and thus sidestep
the problem of the shortage of large repositories of high-quality 3D training
models. We describe how to repurpose video upsampling models, which are not 3D
consistent, and combine them with 3D consolidation to produce 3D-consistent
results. As output, we produce high quality Gaussian Splat models, which are
object centric and effective. Our method is category agnostic and can be easily
incorporated into existing 3D workflows. We evaluate our proposed SuperGaussian
on a variety of 3D inputs, which are diverse both in terms of complexity and
representation (e.g., Gaussian Splats or NeRFs), and demonstrate that our
simple method significantly improves the fidelity of the final 3D models. Check
our project website for details: supergaussian.github.io",2024-06-02 03:44:50+00:00,"['Yuan Shen', 'Duygu Ceylan', 'Paul Guerrero', 'Zexiang Xu', 'Niloy J. Mitra', 'Shenlong Wang', 'Anna Frhstck']",http://arxiv.org/abs/2406.00609v4
Advancing Compressed Video Action Recognition through Progressive Knowledge Distillation,"Compressed video action recognition classifies video samples by leveraging
the different modalities in compressed videos, namely motion vectors,
residuals, and intra-frames. For this purpose, three neural networks are
deployed, each dedicated to processing one modality. Our observations indicate
that the network processing intra-frames tend to converge to a flatter minimum
than the network processing residuals, which in turn converges to a flatter
minimum than the motion vector network. This hierarchy in convergence motivates
our strategy for knowledge transfer among modalities to achieve flatter minima,
which are generally associated with better generalization. With this insight,
we propose Progressive Knowledge Distillation (PKD), a technique that
incrementally transfers knowledge across the modalities. This method involves
attaching early exits (Internal Classifiers - ICs) to the three networks. PKD
distills knowledge starting from the motion vector network, followed by the
residual, and finally, the intra-frame network, sequentially improving IC
accuracy. Further, we propose the Weighted Inference with Scaled Ensemble
(WISE), which combines outputs from the ICs using learned weights, boosting
accuracy during inference. Our experiments demonstrate the effectiveness of
training the ICs with PKD compared to standard cross-entropy-based training,
showing IC accuracy improvements of up to 5.87% and 11.42% on the UCF-101 and
HMDB-51 datasets, respectively. Additionally, WISE improves accuracy by up to
4.28% and 9.30% on UCF-101 and HMDB-51, respectively.",2024-07-02 23:30:01+00:00,"['Efstathia Soufleri', 'Deepak Ravikumar', 'Kaushik Roy']",http://arxiv.org/abs/2407.02713v1
Bootstrapping Vision-language Models for Self-supervised Remote Physiological Measurement,"Facial video-based remote physiological measurement is a promising research
area for detecting human vital signs (e.g., heart rate, respiration frequency)
in a non-contact way. Conventional approaches are mostly supervised learning,
requiring extensive collections of facial videos and synchronously recorded
photoplethysmography (PPG) signals. To tackle it, self-supervised learning has
recently gained attentions; due to the lack of ground truth PPG signals, its
performance is however limited. In this paper, we propose a novel
self-supervised framework that successfully integrates the popular
vision-language models (VLMs) into the remote physiological measurement task.
Given a facial video, we first augment its positive and negative video samples
with varying rPPG signal frequencies. Next, we introduce a frequency-oriented
vision-text pair generation method by carefully creating contrastive
spatio-temporal maps from positive and negative samples and designing proper
text prompts to describe their relative ratios of signal frequencies. A
pre-trained VLM is employed to extract features for these formed vision-text
pairs and estimate rPPG signals thereafter. We develop a series of generative
and contrastive learning mechanisms to optimize the VLM, including the
text-guided visual map reconstruction task, the vision-text contrastive
learning task, and the frequency contrastive and ranking task. Overall, our
method for the first time adapts VLMs to digest and align the frequency-related
knowledge in vision and text modalities. Extensive experiments on four
benchmark datasets demonstrate that it significantly outperforms state of the
art self-supervised methods.",2024-07-11 13:45:50+00:00,"['Zijie Yue', 'Miaojing Shi', 'Hanli Wang', 'Shuai Ding', 'Qijun Chen', 'Shanlin Yang']",http://arxiv.org/abs/2407.08507v2
PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos,"Intelligent assistance involves not only understanding but also action.
Existing ego-centric video datasets contain rich annotations of the videos, but
not of actions that an intelligent assistant could perform in the moment. To
address this gap, we release PARSE-Ego4D, a new set of personal action
recommendation annotations for the Ego4D dataset. We take a multi-stage
approach to generating and evaluating these annotations. First, we used a
prompt-engineered large language model (LLM) to generate context-aware action
suggestions and identified over 18,000 action suggestions. While these
synthetic action suggestions are valuable, the inherent limitations of LLMs
necessitate human evaluation. To ensure high-quality and user-centered
recommendations, we conducted a large-scale human annotation study that
provides grounding in human preferences for all of PARSE-Ego4D. We analyze the
inter-rater agreement and evaluate subjective preferences of participants.
Based on our synthetic dataset and complete human annotations, we propose
several new tasks for action suggestions based on ego-centric videos. We
encourage novel solutions that improve latency and energy requirements. The
annotations in PARSE-Ego4D will support researchers and developers who are
working on building action recommendation systems for augmented and virtual
reality systems.",2024-06-14 09:39:53+00:00,"['Steven Abreu', 'Tiffany D. Do', 'Karan Ahuja', 'Eric J. Gonzalez', 'Lee Payne', 'Daniel McDuff', 'Mar Gonzalez-Franco']",http://arxiv.org/abs/2407.09503v2
End-to-end Open-vocabulary Video Visual Relationship Detection using Multi-modal Prompting,"Open-vocabulary video visual relationship detection aims to expand video
visual relationship detection beyond annotated categories by detecting unseen
relationships between both seen and unseen objects in videos. Existing methods
usually use trajectory detectors trained on closed datasets to detect object
trajectories, and then feed these trajectories into large-scale pre-trained
vision-language models to achieve open-vocabulary classification. Such heavy
dependence on the pre-trained trajectory detectors limits their ability to
generalize to novel object categories, leading to performance degradation. To
address this challenge, we propose to unify object trajectory detection and
relationship classification into an end-to-end open-vocabulary framework. Under
this framework, we propose a relationship-aware open-vocabulary trajectory
detector. It primarily consists of a query-based Transformer decoder, where the
visual encoder of CLIP is distilled for frame-wise open-vocabulary object
detection, and a trajectory associator. To exploit relationship context during
trajectory detection, a relationship query is embedded into the Transformer
decoder, and accordingly, an auxiliary relationship loss is designed to enable
the decoder to perceive the relationships between objects explicitly. Moreover,
we propose an open-vocabulary relationship classifier that leverages the rich
semantic knowledge of CLIP to discover novel relationships. To adapt CLIP well
to relationship classification, we design a multi-modal prompting method that
employs spatio-temporal visual prompting for visual representation and
vision-guided language prompting for language input. Extensive experiments on
two public datasets, VidVRD and VidOR, demonstrate the effectiveness of our
framework. Our framework is also applied to a more difficult cross-dataset
scenario to further demonstrate its generalization ability.",2024-09-19 06:25:01+00:00,"['Yongqi Wang', 'Shuo Yang', 'Xinxiao Wu', 'Jiebo Luo']",http://arxiv.org/abs/2409.12499v1
Skills Made to Order: Efficient Acquisition of Robot Cooking Skills Guided by Multiple Forms of Internet Data,"This study explores the utility of various internet data sources to select
among a set of template robot behaviors to perform skills. Learning
contact-rich skills involving tool use from internet data sources has typically
been challenging due to the lack of physical information such as contact
existence, location, areas, and force in this data. Prior works have generally
used internet data and foundation models trained on this data to generate
low-level robot behavior. We hypothesize that these data and models may be
better suited to selecting among a set of basic robot behaviors to perform
these contact-rich skills. We explore three methods of template selection:
querying large language models, comparing video of robot execution to retrieved
human video using features from a pretrained video encoder common in prior
work, and performing the same comparison using features from an optic flow
encoder trained on internet data. Our results show that LLMs are surprisingly
capable template selectors despite their lack of visual information, optical
flow encoding significantly outperforms video encoders trained with an order of
magnitude more data, and important synergies exist between various forms of
internet data for template selection. By exploiting these synergies, we create
a template selector using multiple forms of internet data that achieves a 79\%
success rate on a set of 16 different cooking skills involving tool-use.",2024-09-23 16:25:44+00:00,"['Mrinal Verghese', 'Christopher Atkeson']",http://arxiv.org/abs/2409.15172v1
ReXplain: Translating Radiology into Patient-Friendly Video Reports,"Radiology reports, designed for efficient communication between medical
experts, often remain incomprehensible to patients. This inaccessibility could
potentially lead to anxiety, decreased engagement in treatment decisions, and
poorer health outcomes, undermining patient-centered care. We present ReXplain
(Radiology eXplanation), an innovative AI-driven system that translates
radiology findings into patient-friendly video reports. ReXplain uniquely
integrates a large language model for medical text simplification and
text-anatomy association, an image segmentation model for anatomical region
identification, and an avatar generation tool for engaging interface
visualization. ReXplain enables producing comprehensive explanations with plain
language, highlighted imagery, and 3D organ renderings in the form of video
reports. To evaluate the utility of ReXplain-generated explanations, we
conducted two rounds of user feedback collection from six board-certified
radiologists. The results of this proof-of-concept study indicate that ReXplain
could accurately deliver radiological information and effectively simulate
one-on-one consultation, shedding light on enhancing patient-centered radiology
with potential clinical usage. This work demonstrates a new paradigm in
AI-assisted medical communication, potentially improving patient engagement and
satisfaction in radiology care, and opens new avenues for research in
multimodal medical communication.",2024-10-01 06:41:18+00:00,"['Luyang Luo', 'Jenanan Vairavamurthy', 'Xiaoman Zhang', 'Abhinav Kumar', 'Ramon R. Ter-Oganesyan', 'Stuart T. Schroff', 'Dan Shilo', 'Rydhwana Hossain', 'Mike Moritz', 'Pranav Rajpurkar']",http://arxiv.org/abs/2410.00441v2
Efficient Transfer Learning for Video-language Foundation Models,"Pre-trained vision-language models provide a robust foundation for efficient
transfer learning across various downstream tasks. In the field of video action
recognition, mainstream approaches often introduce additional modules to
capture temporal information. Although the additional modules increase the
capacity of model, enabling it to better capture video-specific inductive
biases, existing methods typically introduce a substantial number of new
parameters and are prone to catastrophic forgetting of previously acquired
generalizable knowledge. In this paper, we propose a parameter-efficient
Multi-modal Spatio-Temporal Adapter (MSTA) to enhance the alignment between
textual and visual representations, achieving a balance between generalizable
knowledge and task-specific adaptation. Furthermore, to mitigate over-fitting
and enhance generalizability, we introduce a spatio-temporal description-guided
consistency constraint.This constraint involves providing template inputs
(e.g., ""a video of \{\textbf{cls}\}"") to the trainable language branch and
LLM-generated spatio-temporal descriptions to the pre-trained language branch,
enforcing output consistency between the branches. This approach reduces
overfitting to downstream tasks and enhances the distinguishability of the
trainable branch within the spatio-temporal semantic space. We evaluate the
effectiveness of our approach across four tasks: zero-shot transfer, few-shot
learning, base-to-novel generalization, and fully-supervised learning. Compared
to many state-of-the-art methods, our MSTA achieves outstanding performance
across all evaluations, while using only 2-7\% of the trainable parameters in
the original model.",2024-11-18 01:25:58+00:00,"['Haoxing Chen', 'Zizheng Huang', 'Yan Hong', 'Yanshuo Wang', 'Zhongcai Lyu', 'Zhuoer Xu', 'Jun Lan', 'Zhangxuan Gu']",http://arxiv.org/abs/2411.11223v4
Video-Text Dataset Construction from Multi-AI Feedback: Promoting Weak-to-Strong Preference Learning for Video Large Language Models,"High-quality video-text preference data is crucial for Multimodal Large
Language Models (MLLMs) alignment. However, existing preference data is very
scarce. Obtaining VQA preference data for preference training is costly, and
manually annotating responses is highly unreliable, which could result in
low-quality pairs. Meanwhile, AI-generated responses controlled by temperature
adjustment lack diversity. To address these issues, we propose a high-quality
VQA preference dataset, called \textit{\textbf{M}ultiple \textbf{M}ultimodal
\textbf{A}rtificial \textbf{I}ntelligence \textbf{P}reference Datasets in
\textbf{V}QA} (\textbf{MMAIP-V}), which is constructed by sampling from the
response distribution set and using an external scoring function for response
evaluation. Furthermore, to fully leverage the preference knowledge in MMAIP-V
and ensure sufficient optimization, we propose \textit{\textbf{Iter}ative
\textbf{W}eak-to-\textbf{S}trong \textbf{R}einforcement \textbf{L}earning from
\textbf{AI} \textbf{F}eedback for video MLLMs} (\textbf{Iter-W2S-RLAIF}), a
framework that gradually enhances MLLMs' alignment capabilities by iteratively
updating the reference model and performing parameter extrapolation. Finally,
we propose an unbiased and information-complete evaluation scheme in VQA
evaluation. Experiments demonstrate that MMAIP-V is beneficial for MLLMs in
preference learning and Iter-W2S-RLAIF fully exploits the alignment information
in MMAIP-V. We believe that the proposed automatic VQA preference data
generation pipeline based on AI feedback can greatly promote future work in the
MLLMs alignment. \textbf{Code and dataset are available}
\href{https://anonymous.4open.science/r/MMAIP-V_Iter-W2S-RLAIF-702F}{MMAIP-V\_Iter-W2S-RLAIF-702F}.",2024-11-25 08:59:39+00:00,"['Hao Yi', 'Qingyang Li', 'Yulan Hu', 'Fuzheng Zhang', 'Di Zhang', 'Yong Liu']",http://arxiv.org/abs/2411.16201v1
Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning,"Large Multimodal Models (LMMs) have made significant breakthroughs with the
advancement of instruction tuning. However, while existing models can
understand images and videos at a holistic level, they still struggle with
instance-level understanding that requires a more nuanced comprehension and
alignment. Instance-level understanding is crucial, as it focuses on the
specific elements that we are most interested in. Excitingly, existing works
find that the state-of-the-art LMMs exhibit strong instance understanding
capabilities when provided with explicit visual cues. Motivated by this, we
introduce an automated annotation pipeline assisted by GPT-4o to extract
instance-level information from images and videos through explicit visual
prompting for instance guidance. Building upon this pipeline, we proposed
Inst-IT, a solution to enhance LMMs in Instance understanding via explicit
visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose
multimodal instance-level understanding, a large-scale instruction-tuning
dataset, and a continuous instruction-tuning training paradigm to effectively
enhance spatial-temporal instance understanding capabilities of existing LMMs.
Experimental results show that, with the boost of Inst-IT, our models not only
achieve outstanding performance on Inst-IT Bench but also demonstrate
significant improvements across various generic image and video understanding
benchmarks. This highlights that our dataset not only boosts instance-level
understanding but also strengthens the overall capabilities of generic image
and video comprehension.",2024-12-04 18:58:10+00:00,"['Wujian Peng', 'Lingchen Meng', 'Yitong Chen', 'Yiweng Xie', 'Yang Liu', 'Tao Gui', 'Hang Xu', 'Xipeng Qiu', 'Zuxuan Wu', 'Yu-Gang Jiang']",http://arxiv.org/abs/2412.03565v1
ShotVL: Human-Centric Highlight Frame Retrieval via Language Queries,"Existing works on human-centric video understanding typically focus on
analyzing specific moment or entire videos. However, many applications require
higher precision at the frame level. In this work, we propose a novel task,
BestShot, which aims to locate highlight frames within human-centric videos via
language queries. This task demands not only a deep semantic comprehension of
human actions but also precise temporal localization. To support this task, we
introduce the BestShot Benchmark. %The benchmark is meticulously constructed by
combining human detection and tracking, potential frame selection based on
human judgment, and detailed textual descriptions crafted by human input to
ensure precision. The benchmark is meticulously constructed by combining
human-annotated highlight frames, detailed textual descriptions and duration
labeling. These descriptions encompass three critical elements: (1) Visual
content; (2) Fine-grained action; and (3) Human Pose Description. Together,
these elements provide the necessary precision to identify the exact highlight
frames in videos.
  To tackle this problem, we have collected two distinct datasets: (i)
ShotGPT4o Dataset, which is algorithmically generated by GPT-4o and (ii)
Image-SMPLText Dataset, a dataset with large-scale and accurate per-frame pose
description leveraging PoseScript and existing pose estimation datasets. Based
on these datasets, we present a strong baseline model, ShotVL, fine-tuned from
InternVL, specifically for BestShot. We highlight the impressive zero-shot
capabilities of our model and offer comparative analyses with existing SOTA
models. ShotVL demonstrates a significant 52% improvement over InternVL on the
BestShot Benchmark and a notable 57% improvement on the THUMOS14 Benchmark, all
while maintaining the SOTA performance in general image classification and
retrieval.",2024-12-17 08:44:29+00:00,"['Wangyu Xue', 'Chen Qian', 'Jiayi Wu', 'Yang Zhou', 'Wentao Liu', 'Ju Ren', 'Siming Fan', 'Yaoxue Zhang']",http://arxiv.org/abs/2412.12675v1
Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking,"Multimodal tracking has garnered widespread attention as a result of its
ability to effectively address the inherent limitations of traditional RGB
tracking. However, existing multimodal trackers mainly focus on the fusion and
enhancement of spatial features or merely leverage the sparse temporal
relationships between video frames. These approaches do not fully exploit the
temporal correlations in multimodal videos, making it difficult to capture the
dynamic changes and motion information of targets in complex scenarios. To
alleviate this problem, we propose a unified multimodal spatial-temporal
tracking approach named STTrack. In contrast to previous paradigms that solely
relied on updating reference information, we introduced a temporal state
generator (TSG) that continuously generates a sequence of tokens containing
multimodal temporal information. These temporal information tokens are used to
guide the localization of the target in the next time state, establish
long-range contextual relationships between video frames, and capture the
temporal trajectory of the target. Furthermore, at the spatial level, we
introduced the mamba fusion and background suppression interactive (BSI)
modules. These modules establish a dual-stage mechanism for coordinating
information interaction and fusion between modalities. Extensive comparisons on
five benchmark datasets illustrate that STTrack achieves state-of-the-art
performance across various multimodal tracking scenarios. Code is available at:
https://github.com/NJU-PCALab/STTrack.",2024-12-20 09:10:17+00:00,"['Xiantao Hu', 'Ying Tai', 'Xu Zhao', 'Chen Zhao', 'Zhenyu Zhang', 'Jun Li', 'Bineng Zhong', 'Jian Yang']",http://arxiv.org/abs/2412.15691v1
WEM-GAN: Wavelet transform based facial expression manipulation,"Facial expression manipulation aims to change human facial expressions
without affecting face recognition. In order to transform the facial
expressions to target expressions, previous methods relied on expression labels
to guide the manipulation process. However, these methods failed to preserve
the details of facial features, which causes the weakening or the loss of
identity information in the output image. In our work, we propose WEM-GAN, in
short for wavelet-based expression manipulation GAN, which puts more efforts on
preserving the details of the original image in the editing process. Firstly,
we take advantage of the wavelet transform technique and combine it with our
generator with a U-net autoencoder backbone, in order to improve the
generator's ability to preserve more details of facial features. Secondly, we
also implement the high-frequency component discriminator, and use
high-frequency domain adversarial loss to further constrain the optimization of
our model, providing the generated face image with more abundant details.
Additionally, in order to narrow the gap between generated facial expressions
and target expressions, we use residual connections between encoder and
decoder, while also using relative action units (AUs) several times. Extensive
qualitative and quantitative experiments have demonstrated that our model
performs better in preserving identity features, editing capability, and image
generation quality on the AffectNet dataset. It also shows superior performance
in metrics such as Average Content Distance (ACD) and Expression Distance (ED).",2024-12-03 16:23:02+00:00,"['Dongya Sun', 'Yunfei Hu', 'Xianzhe Zhang', 'Yingsong Hu']",http://arxiv.org/abs/2412.02530v1
Generative AI: A Pix2pix-GAN-Based Machine Learning Approach for Robust and Efficient Lung Segmentation,"Chest radiography is climacteric in identifying different pulmonary diseases,
yet radiologist workload and inefficiency can lead to misdiagnoses. Automatic,
accurate, and efficient segmentation of lung from X-ray images of chest is
paramount for early disease detection. This study develops a deep learning
framework using a Pix2pix Generative Adversarial Network (GAN) to segment
pulmonary abnormalities from CXR images. This framework's image preprocessing
and augmentation techniques were properly incorporated with a U-Net-inspired
generator-discriminator architecture. Initially, it loaded the CXR images and
manual masks from the Montgomery and Shenzhen datasets, after which
preprocessing and resizing were performed. A U-Net generator is applied to the
processed CXR images that yield segmented masks; then, a Discriminator Network
differentiates between the generated and real masks. Montgomery dataset served
as the model's training set in the study, and the Shenzhen dataset was used to
test its robustness, which was used here for the first time. An adversarial
loss and an L1 distance were used to optimize the model in training. All
metrics, which assess precision, recall, F1 score, and Dice coefficient, prove
the effectiveness of this framework in pulmonary abnormality segmentation. It,
therefore, sets the basis for future studies to be performed shortly using
diverse datasets that could further confirm its clinical applicability in
medical imaging.",2024-12-14 13:12:09+00:00,['Sharmin Akter'],http://arxiv.org/abs/2412.10826v1
LCB-net: Long-Context Biasing for Audio-Visual Speech Recognition,"The growing prevalence of online conferences and courses presents a new
challenge in improving automatic speech recognition (ASR) with enriched textual
information from video slides. In contrast to rare phrase lists, the slides
within videos are synchronized in real-time with the speech, enabling the
extraction of long contextual bias. Therefore, we propose a novel long-context
biasing network (LCB-net) for audio-visual speech recognition (AVSR) to
leverage the long-context information available in videos effectively.
Specifically, we adopt a bi-encoder architecture to simultaneously model audio
and long-context biasing. Besides, we also propose a biasing prediction module
that utilizes binary cross entropy (BCE) loss to explicitly determine biased
phrases in the long-context biasing. Furthermore, we introduce a dynamic
contextual phrases simulation to enhance the generalization and robustness of
our LCB-net. Experiments on the SlideSpeech, a large-scale audio-visual corpus
enriched with slides, reveal that our proposed LCB-net outperforms general ASR
model by 9.4%/9.1%/10.9% relative WER/U-WER/B-WER reduction on test set, which
enjoys high unbiased and biased performance. Moreover, we also evaluate our
model on LibriSpeech corpus, leading to 23.8%/19.2%/35.4% relative
WER/U-WER/B-WER reduction over the ASR model.",2024-01-12 06:03:39+00:00,"['Fan Yu', 'Haoxu Wang', 'Xian Shi', 'Shiliang Zhang']",http://arxiv.org/abs/2401.06390v1
High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering,"Readily editable mesh blendshapes have been widely used in animation
pipelines, while recent advancements in neural geometry and appearance
representations have enabled high-quality inverse rendering. Building upon
these observations, we introduce a novel technique that reconstructs mesh-based
blendshape rigs from single or sparse multi-view videos, leveraging
state-of-the-art neural inverse rendering. We begin by constructing a
deformation representation that parameterizes vertex displacements into
differential coordinates with tetrahedral connections, allowing for
high-quality vertex deformation on high-resolution meshes. By constructing a
set of semantic regulations in this representation, we achieve joint
optimization of blendshapes and expression coefficients. Furthermore, to enable
a user-friendly multi-view setup with unsynchronized cameras, we propose a
neural regressor to model time-varying motion parameters. This approach
implicitly considers the time difference across multiple cameras, enhancing the
accuracy of motion modeling. Experiments demonstrate that, with the flexible
input of single or sparse multi-view videos, we reconstruct personalized
high-fidelity blendshapes. These blendshapes are both geometrically and
semantically accurate, and they are compatible with industrial animation
pipelines. Code and data are available at
https://github.com/grignarder/high-quality-blendshape-generation.",2024-01-16 14:41:31+00:00,"['Xin Ming', 'Jiawei Li', 'Jingwang Ling', 'Libo Zhang', 'Feng Xu']",http://arxiv.org/abs/2401.08398v2
GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features,"Moment retrieval (MR) and highlight detection (HD) aim to identify relevant
moments and highlights in video from corresponding natural language query.
Large language models (LLMs) have demonstrated proficiency in various computer
vision tasks. However, existing methods for MR\&HD have not yet been integrated
with LLMs. In this letter, we propose a novel two-stage model that takes the
output of LLMs as the input to the second-stage transformer encoder-decoder.
First, MiniGPT-4 is employed to generate the detailed description of the video
frame and rewrite the query statement, fed into the encoder as new features.
Then, semantic similarity is computed between the generated description and the
rewritten queries. Finally, continuous high-similarity video frames are
converted into span anchors, serving as prior position information for the
decoder. Experiments demonstrate that our approach achieves a state-of-the-art
result, and by using only span anchors and similarity scores as outputs,
positioning accuracy outperforms traditional methods, like Moment-DETR.",2024-03-03 08:24:28+00:00,"['Yunzhuo Sun', 'Yifang Xu', 'Zien Xie', 'Yukun Shu', 'Sidan Du']",http://arxiv.org/abs/2403.01437v2
RID-TWIN: An end-to-end pipeline for automatic face de-identification in videos,"Face de-identification in videos is a challenging task in the domain of
computer vision, primarily used in privacy-preserving applications. Despite the
considerable progress achieved through generative vision models, there remain
multiple challenges in the latest approaches. They lack a comprehensive
discussion and evaluation of aspects such as realism, temporal coherence, and
preservation of non-identifiable features. In our work, we propose RID-Twin: a
novel pipeline that leverages the state-of-the-art generative models, and
decouples identity from motion to perform automatic face de-identification in
videos. We investigate the task from a holistic point of view and discuss how
our approach addresses the pertinent existing challenges in this domain. We
evaluate the performance of our methodology on the widely employed VoxCeleb2
dataset, and also a custom dataset designed to accommodate the limitations of
certain behavioral variations absent in the VoxCeleb2 dataset. We discuss the
implications and advantages of our work and suggest directions for future
research.",2024-03-15 06:59:21+00:00,"['Anirban Mukherjee', 'Monjoy Narayan Choudhury', 'Dinesh Babu Jayagopi']",http://arxiv.org/abs/2403.10058v1
PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization,"This paper introduces a novel approach to temporal action localization (TAL)
in few-shot learning. Our work addresses the inherent limitations of
conventional single-prompt learning methods that often lead to overfitting due
to the inability to generalize across varying contexts in real-world videos.
Recognizing the diversity of camera views, backgrounds, and objects in videos,
we propose a multi-prompt learning framework enhanced with optimal transport.
This design allows the model to learn a set of diverse prompts for each action,
capturing general characteristics more effectively and distributing the
representation to mitigate the risk of overfitting. Furthermore, by employing
optimal transport theory, we efficiently align these prompts with action
features, optimizing for a comprehensive representation that adapts to the
multifaceted nature of video data. Our experiments demonstrate significant
improvements in action localization accuracy and robustness in few-shot
settings on the standard challenging datasets of THUMOS-14 and EpicKitchens100,
highlighting the efficacy of our multi-prompt optimal transport approach in
overcoming the challenges of conventional few-shot TAL methods.",2024-03-27 18:08:14+00:00,"['Edward Fish', 'Jon Weinbren', 'Andrew Gilbert']",http://arxiv.org/abs/2403.18915v1
A Unified Framework for Human-centric Point Cloud Video Understanding,"Human-centric Point Cloud Video Understanding (PVU) is an emerging field
focused on extracting and interpreting human-related features from sequences of
human point clouds, further advancing downstream human-centric tasks and
applications. Previous works usually focus on tackling one specific task and
rely on huge labeled data, which has poor generalization capability.
Considering that human has specific characteristics, including the structural
semantics of human body and the dynamics of human motions, we propose a unified
framework to make full use of the prior knowledge and explore the inherent
features in the data itself for generalized human-centric point cloud video
understanding. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on various human-related tasks, including action
recognition and 3D pose estimation. All datasets and code will be released
soon.",2024-03-29 07:53:06+00:00,"['Yiteng Xu', 'Kecheng Ye', 'Xiao Han', 'Yiming Ren', 'Xinge Zhu', 'Yuexin Ma']",http://arxiv.org/abs/2403.20031v1
Perception-Oriented Video Frame Interpolation via Asymmetric Blending,"Previous methods for Video Frame Interpolation (VFI) have encountered
challenges, notably the manifestation of blur and ghosting effects. These
issues can be traced back to two pivotal factors: unavoidable motion errors and
misalignment in supervision. In practice, motion estimates often prove to be
error-prone, resulting in misaligned features. Furthermore, the reconstruction
loss tends to bring blurry results, particularly in misaligned regions. To
mitigate these challenges, we propose a new paradigm called PerVFI
(Perception-oriented Video Frame Interpolation). Our approach incorporates an
Asymmetric Synergistic Blending module (ASB) that utilizes features from both
sides to synergistically blend intermediate features. One reference frame
emphasizes primary content, while the other contributes complementary
information. To impose a stringent constraint on the blending process, we
introduce a self-learned sparse quasi-binary mask which effectively mitigates
ghosting and blur artifacts in the output. Additionally, we employ a
normalizing flow-based generator and utilize the negative log-likelihood loss
to learn the conditional distribution of the output, which further facilitates
the generation of clear and fine details. Experimental results validate the
superiority of PerVFI, demonstrating significant improvements in perceptual
quality compared to existing methods. Codes are available at
\url{https://github.com/mulns/PerVFI}",2024-04-10 02:40:17+00:00,"['Guangyang Wu', 'Xin Tao', 'Changlin Li', 'Wenyi Wang', 'Xiaohong Liu', 'Qingqing Zheng']",http://arxiv.org/abs/2404.06692v1
Watching Popular Musicians Learn by Ear: A Hypothesis-Generating Study of Human-Recording Interactions in YouTube Videos,"Popular musicians often learn music by ear. It is unclear what role
technology plays for those with experience at this task. In search of
opportunities for the development of novel human-recording interactions, we
analyze 18 YouTube videos depicting real-world examples of by-ear learning, and
discuss why, during this preliminary phase of research, online videos are
appropriate data. From our observations we generate hypotheses that can inform
future work. For example, a musician's scope of learning may influence what
technological interactions would help them, they could benefit from tools that
accommodate their working memory, and transcription does not appear to play a
key role in ear learning. Based on these findings, we pose a number of research
questions, and discuss their methodological considerations to guide future
study.",2024-06-06 13:25:42+00:00,"['Christopher Liscio', 'Daniel G. Brown']",http://arxiv.org/abs/2406.04058v1
Using Deep Convolutional Neural Networks to Detect Rendered Glitches in Video Games,"In this paper, we present a method using Deep Convolutional Neural Networks
(DCNNs) to detect common glitches in video games. The problem setting consists
of an image (800x800 RGB) as input to be classified into one of five defined
classes, normal image, or one of four different kinds of glitches (stretched,
low resolution, missing and placeholder textures). Using a supervised approach,
we train a ShuffleNetV2 using generated data. This work focuses on detecting
texture graphical anomalies achieving arguably good performance with an
accuracy of 86.8\%, detecting 88\% of the glitches with a false positive rate
of 8.7\%, and with the models being able to generalize and detect glitches even
in unseen objects. We apply a confidence measure as well to tackle the issue
with false positives as well as an effective way of aggregating images to
achieve better detection in production. The main use of this work is the
partial automatization of graphical testing in the final stages of video game
development.",2024-06-12 13:59:45+00:00,"['Carlos Garcia Ling', 'Konrad Tollmar', 'Linus Gisslen']",http://arxiv.org/abs/2406.08231v1
Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised Temporal Action Localization,"Weakly-supervised Temporal Action Localization (WSTAL) aims to localize
actions in untrimmed videos using only video-level supervision. Latest WSTAL
methods introduce pseudo label learning framework to bridge the gap between
classification-based training and inferencing targets at localization, and
achieve cutting-edge results. In these frameworks, a classification-based model
is used to generate pseudo labels for a regression-based student model to learn
from. However, the quality of pseudo labels in the framework, which is a key
factor to the final result, is not carefully studied. In this paper, we propose
a set of simple yet efficient pseudo label quality enhancement mechanisms to
build our FuSTAL framework. FuSTAL enhances pseudo label quality at three
stages: cross-video contrastive learning at proposal Generation-Stage,
prior-based filtering at proposal Selection-Stage and EMA-based distillation at
Training-Stage. These designs enhance pseudo label quality at different stages
in the framework, and help produce more informative, less false and smoother
action proposals. With the help of these comprehensive designs at all stages,
FuSTAL achieves an average mAP of 50.8% on THUMOS'14, outperforming the
previous best method by 1.2%, and becomes the first method to reach the
milestone of 50%.",2024-07-12 03:53:55+00:00,"['Qianhan Feng', 'Wenshuo Li', 'Tong Lin', 'Xinghao Chen']",http://arxiv.org/abs/2407.08971v1
Arbitrary-Scale Video Super-Resolution with Structural and Textural Priors,"Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we first describe a strong baseline
for AVSR by putting together three variants of elementary building blocks: 1) a
flow-guided recurrent unit that aggregates spatiotemporal information from
previous frames, 2) a flow-refined cross-attention unit that selects
spatiotemporal information from future frames, and 3) a hyper-upsampling unit
that generates scaleaware and content-independent upsampling kernels. We then
introduce ST-AVSR by equipping our baseline with a multi-scale structural and
textural prior computed from the pre-trained VGG network. This prior has proven
effective in discriminating structure and texture across different locations
and scales, which is beneficial for AVSR. Comprehensive experiments show that
ST-AVSR significantly improves super-resolution quality, generalization
ability, and inference speed over the state-of-theart. The code is available at
https://github.com/shangwei5/ST-AVSR.",2024-07-13 15:27:39+00:00,"['Wei Shang', 'Dongwei Ren', 'Wanying Zhang', 'Yuming Fang', 'Wangmeng Zuo', 'Kede Ma']",http://arxiv.org/abs/2407.09919v1
Ophthalmic Biomarker Detection: Highlights from the IEEE Video and Image Processing Cup 2023 Student Competition,"The VIP Cup offers a unique experience to undergraduates, allowing students
to work together to solve challenging, real-world problems with video and image
processing techniques. In this iteration of the VIP Cup, we challenged students
to balance personalization and generalization when performing biomarker
detection in 3D optical coherence tomography (OCT) images. Balancing
personalization and generalization is an important challenge to tackle, as the
variation within OCT scans of patients between visits can be minimal while the
difference in manifestation of the same disease across different patients may
be substantial. The domain difference between OCT scans can arise due to
pathology manifestation across patients, clinical labels, and the visit along
the treatment process when the scan is taken. Hence, we provided a multimodal
OCT dataset to allow teams to effectively target this challenge. Overall, this
competition gave undergraduates an opportunity to learn about how artificial
intelligence can be a powerful tool for the medical field, as well as the
unique challenges one faces when applying machine learning to biomedical data.",2024-08-20 20:09:09+00:00,"['Ghassan AlRegib', 'Mohit Prabhushankar', 'Kiran Kokilepersaud', 'Prithwijit Chowdhury', 'Zoe Fowler', 'Stephanie Trejo Corona', 'Lucas Thomaz', 'Angshul Majumdar']",http://arxiv.org/abs/2408.11170v1
SAM & SAM 2 in 3D Slicer: SegmentWithSAM Extension for Annotating Medical Images,"Creating annotations for 3D medical data is time-consuming and often requires
highly specialized expertise. Various tools have been implemented to aid this
process. Segment Anything Model 2 (SAM 2) offers a general-purpose prompt-based
segmentation algorithm designed to annotate videos. In this paper, we adapt
this model to the annotation of 3D medical images and offer our implementation
in the form of an extension to the popular annotation software: 3D Slicer. Our
extension allows users to place point prompts on 2D slices to generate
annotation masks and propagate these annotations across entire volumes in
either single-directional or bi-directional manners. Our code is publicly
available on https://github.com/mazurowski-lab/SlicerSegmentWithSAM and can be
easily installed directly from the Extension Manager of 3D Slicer as well.",2024-08-27 17:39:33+00:00,"['Zafer Yildiz', 'Yuwen Chen', 'Maciej A. Mazurowski']",http://arxiv.org/abs/2408.15224v1
TapToTab : Video-Based Guitar Tabs Generation using AI and Audio Analysis,"The automation of guitar tablature generation from video inputs holds
significant promise for enhancing music education, transcription accuracy, and
performance analysis. Existing methods face challenges with consistency and
completeness, particularly in detecting fretboards and accurately identifying
notes. To address these issues, this paper introduces an advanced approach
leveraging deep learning, specifically YOLO models for real-time fretboard
detection, and Fourier Transform-based audio analysis for precise note
identification. Experimental results demonstrate substantial improvements in
detection accuracy and robustness compared to traditional techniques. This
paper outlines the development, implementation, and evaluation of these
methodologies, aiming to revolutionize guitar instruction by automating the
creation of guitar tabs from video recordings.",2024-09-13 08:17:15+00:00,"['Ali Ghaleb', 'Eslam ElSadawy', 'Ihab Essam', 'Mohamed Abdelhakim', 'Seif-Eldin Zaki', 'Natalie Fahim', 'Razan Bayoumi', 'Hanan Hindy']",http://arxiv.org/abs/2409.08618v1
TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans,"We introduce a novel framework that learns a dynamic neural radiance field
(NeRF) for full-body talking humans from monocular videos. Prior work
represents only the body pose or the face. However, humans communicate with
their full body, combining body pose, hand gestures, as well as facial
expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network
that represents the holistic 4D human motion. Given a monocular video of a
subject, we learn corresponding modules for the body, face, and hands, that are
combined together to generate the final result. To capture complex finger
articulation, we learn an additional deformation field for the hands. Our
multi-identity representation enables simultaneous training for multiple
subjects, as well as robust animation under completely unseen poses. It can
also generalize to novel identities, given only a short video as input. We
demonstrate state-of-the-art performance for animating full-body talking
humans, with fine-grained hand articulation and facial expressions.",2024-09-25 06:51:57+00:00,"['Aggelina Chatziagapi', 'Bindita Chaudhuri', 'Amit Kumar', 'Rakesh Ranjan', 'Dimitris Samaras', 'Nikolaos Sarafianos']",http://arxiv.org/abs/2409.16666v1
Show and Guide: Instructional-Plan Grounded Vision and Language Model,"Guiding users through complex procedural plans is an inherently multimodal
task in which having visually illustrated plan steps is crucial to deliver an
effective plan guidance. However, existing works on plan-following language
models (LMs) often are not capable of multimodal input and output. In this
work, we present MM-PlanLLM, the first multimodal LLM designed to assist users
in executing instructional tasks by leveraging both textual plans and visual
information. Specifically, we bring cross-modality through two key tasks:
Conversational Video Moment Retrieval, where the model retrieves relevant
step-video segments based on user queries, and Visually-Informed Step
Generation, where the model generates the next step in a plan, conditioned on
an image of the user's current progress. MM-PlanLLM is trained using a novel
multitask-multistage approach, designed to gradually expose the model to
multimodal instructional-plans semantic layers, achieving strong performance on
both multimodal and textual dialogue in a plan-grounded setting. Furthermore,
we show that the model delivers cross-modal temporal and plan-structure
representations aligned between textual plan steps and instructional video
moments.",2024-09-27 18:20:24+00:00,"['Diogo Glria-Silva', 'David Semedo', 'Joo Magalhes']",http://arxiv.org/abs/2409.19074v3
From Experts to the Public: Governing Multimodal Language Models in Politically Sensitive Video Analysis,"This paper examines the governance of multimodal large language models
(MM-LLMs) through individual and collective deliberation, focusing on analyses
of politically sensitive videos. We conducted a two-step study: first,
interviews with 10 journalists established a baseline understanding of expert
video interpretation; second, 114 individuals from the general public engaged
in deliberation using Inclusive.AI, a platform that facilitates democratic
decision-making through decentralized autonomous organization (DAO) mechanisms.
Our findings show that while experts emphasized emotion and narrative, the
general public prioritized factual clarity, objectivity of the situation, and
emotional neutrality. Additionally, we explored the impact of different
governance mechanisms: quadratic vs. weighted ranking voting and equal vs.
20-80 power distributions on users decision-making on how AI should behave.
Specifically, quadratic voting enhanced perceptions of liberal democracy and
political equality, and participants who were more optimistic about AI
perceived the voting process to have a higher level of participatory democracy.
Our results suggest the potential of applying DAO mechanisms to help
democratize AI governance.",2024-09-15 03:17:38+00:00,"['Tanusree Sharma', 'Yujin Potter', 'Zachary Kilhoffer', 'Yun Huang', 'Dawn Song', 'Yang Wang']",http://arxiv.org/abs/2410.01817v1
Multi class activity classification in videos using Motion History Image generation,"Human action recognition has been a topic of interest across multiple fields
ranging from security to entertainment systems. Tracking the motion and
identifying the action being performed on a real time basis is necessary for
critical security systems. In entertainment, especially gaming, the need for
immediate responses for actions and gestures are paramount for the success of
that system. We show that Motion History image has been a well established
framework to capture the temporal and activity information in multi dimensional
detail enabling various usecases including classification. We utilize MHI to
produce sample data to train a classifier and demonstrate its effectiveness for
action classification across six different activities in a single multi-action
video. We analyze the classifier performance and identify usecases where MHI
struggles to generate the appropriate activity image and discuss mechanisms and
future work to overcome those limitations.",2024-10-13 16:22:02+00:00,['Senthilkumar Gopal'],http://arxiv.org/abs/2410.09902v1
Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals,"Audio descriptions (AD) make videos accessible for blind and low vision (BLV)
users by describing visual elements that cannot be understood from the main
audio track. AD created by professionals or novice describers is time-consuming
and lacks scalability while offering little control to BLV viewers on
description length and content and when they receive it. To address this gap,
we explore user-driven AI-generated descriptions, where the BLV viewer controls
when they receive descriptions. In a study, 20 BLV participants activated audio
descriptions for seven different video genres with two levels of detail:
concise and detailed. Our results show differences in AD frequency and level of
detail BLV users wanted for different videos, their sense of control with this
style of AD delivery, its limitations, and variations among BLV users in their
AD needs and perception of AI-generated descriptions. We discuss the
implications of our findings for future AI-based AD tools.",2024-11-18 18:53:57+00:00,"['Maryam Cheema', 'Hasti Seifi', 'Pooyan Fazli']",http://arxiv.org/abs/2411.11835v1
Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation,"This paper tackles the problem of video question answering (VideoQA), a task
that often requires multi-step reasoning and a profound understanding of
spatial-temporal dynamics. While large video-language models perform well on
benchmarks, they often lack explainability and spatial-temporal grounding. In
this paper, we propose Agent-of-Thoughts Distillation (AoTD), a method that
enhances models by incorporating automatically generated Chain-of-Thoughts
(CoTs) into the instruction-tuning process. Specifically, we leverage an
agent-based system to decompose complex questions into sub-tasks, and address
them with specialized vision models, the intermediate results are then treated
as reasoning chains. We also introduce a verification mechanism using a large
language model (LLM) to ensure the reliability of generated CoTs. Extensive
experiments demonstrate that AoTD improves the performance on multiple-choice
and open-ended benchmarks.",2024-12-02 16:37:50+00:00,"['Yudi Shi', 'Shangzhe Di', 'Qirui Chen', 'Weidi Xie']",http://arxiv.org/abs/2412.01694v2
SceneFactor: Factored Latent 3D Diffusion for Controllable 3D Scene Generation,"We present SceneFactor, a diffusion-based approach for large-scale 3D scene
generation that enables controllable generation and effortless editing.
SceneFactor enables text-guided 3D scene synthesis through our factored
diffusion formulation, leveraging latent semantic and geometric manifolds for
generation of arbitrary-sized 3D scenes. While text input enables easy,
controllable generation, text guidance remains imprecise for intuitive,
localized editing and manipulation of the generated 3D scenes. Our factored
semantic diffusion generates a proxy semantic space composed of semantic 3D
boxes that enables controllable editing of generated scenes by adding,
removing, changing the size of the semantic 3D proxy boxes that guides
high-fidelity, consistent 3D geometric editing. Extensive experiments
demonstrate that our approach enables high-fidelity 3D scene synthesis with
effective controllable editing through our factored diffusion approach.",2024-12-02 18:47:41+00:00,"['Alexey Bokhovkin', 'Quan Meng', 'Shubham Tulsiani', 'Angela Dai']",http://arxiv.org/abs/2412.01801v2
Domain-Transferred Synthetic Data Generation for Improving Monocular Depth Estimation,"A major obstacle to the development of effective monocular depth estimation
algorithms is the difficulty in obtaining high-quality depth data that
corresponds to collected RGB images. Collecting this data is time-consuming and
costly, and even data collected by modern sensors has limited range or
resolution, and is subject to inconsistencies and noise. To combat this, we
propose a method of data generation in simulation using 3D synthetic
environments and CycleGAN domain transfer. We compare this method of data
generation to the popular NYUDepth V2 dataset by training a depth estimation
model based on the DenseDepth structure using different training sets of real
and simulated data. We evaluate the performance of the models on newly
collected images and LiDAR depth data from a Husky robot to verify the
generalizability of the approach and show that GAN-transformed data can serve
as an effective alternative to real-world data, particularly in depth
estimation.",2024-05-02 09:21:10+00:00,"['Seungyeop Lee', 'Knut Peterson', 'Solmaz Arezoomandan', 'Bill Cai', 'Peihan Li', 'Lifeng Zhou', 'David Han']",http://arxiv.org/abs/2405.01113v1
Towards Physics-informed Cyclic Adversarial Multi-PSF Lensless Imaging,"Lensless imaging has emerged as a promising field within inverse imaging,
offering compact, cost-effective solutions with the potential to revolutionize
the computational camera market. By circumventing traditional optical
components like lenses and mirrors, novel approaches like mask-based lensless
imaging eliminate the need for conventional hardware. However, advancements in
lensless image reconstruction, particularly those leveraging Generative
Adversarial Networks (GANs), are hindered by the reliance on data-driven
training processes, resulting in network specificity to the Point Spread
Function (PSF) of the imaging system. This necessitates a complete retraining
for minor PSF changes, limiting adaptability and generalizability across
diverse imaging scenarios. In this paper, we introduce a novel approach to
multi-PSF lensless imaging, employing a dual discriminator cyclic adversarial
framework. We propose a unique generator architecture with a sparse
convolutional PSF-aware auxiliary branch, coupled with a forward model
integrated into the training loop to facilitate physics-informed learning to
handle the substantial domain gap between lensless and lensed images.
Comprehensive performance evaluation and ablation studies underscore the
effectiveness of our model, offering robust and adaptable lensless image
reconstruction capabilities. Our method achieves comparable performance to
existing PSF-agnostic generative methods for single PSF cases and demonstrates
resilience to PSF changes without the need for retraining.",2024-07-09 10:07:28+00:00,"['Abeer Banerjee', 'Sanjay Singh']",http://arxiv.org/abs/2407.06727v1
Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation,"Dynamic scene understanding is one of the most conspicuous field of interest
among computer vision community. In order to enhance dynamic scene
understanding, pixel-wise segmentation with neural networks is widely accepted.
The latest researches on pixel-wise segmentation combined semantic and motion
information and produced good performance. In this work, we propose a state of
art architecture of neural networks to accurately and efficiently get the
moving object proposals (MOP). We first train an unsupervised convolutional
neural network (UnFlow) to generate optical flow estimation. Then we render the
output of optical flow net to a fully convolutional SegNet model. The main
contribution of our work is (1) Fine-tuning the pretrained optical flow model
on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural
networks with Encoder-Decoder architecture to segment objects. We developed the
codes with TensorFlow, and executed the training and evaluation processes on an
AWS EC2 instance.",2024-02-14 01:13:55+00:00,"['Ge Shi', 'Zhili Yang']",http://arxiv.org/abs/2402.08882v1
Gaussian Shadow Casting for Neural Characters,"Neural character models can now reconstruct detailed geometry and texture
from video, but they lack explicit shadows and shading, leading to artifacts
when generating novel views and poses or during relighting. It is particularly
difficult to include shadows as they are a global effect and the required
casting of secondary rays is costly. We propose a new shadow model using a
Gaussian density proxy that replaces sampling with a simple analytic formula.
It supports dynamic motion and is tailored for shadow computation, thereby
avoiding the affine projection approximation and sorting required by the
closely related Gaussian splatting. Combined with a deferred neural rendering
model, our Gaussian shadows enable Lambertian shading and shadow casting with
minimal overhead. We demonstrate improved reconstructions, with better
separation of albedo, shading, and shadows in challenging outdoor scenes with
direct sun light and hard shadows. Our method is able to optimize the light
direction without any input from the user. As a result, novel poses have fewer
shadow artifacts and relighting in novel scenes is more realistic compared to
the state-of-the-art methods, providing new ways to pose neural characters in
novel environments, increasing their applicability.",2024-01-11 18:50:31+00:00,"['Luis Bolanos', 'Shih-Yang Su', 'Helge Rhodin']",http://arxiv.org/abs/2401.06116v1
Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant Scenes,"Most scenes are illuminated by several light sources, where the traditional
assumption of uniform illumination is invalid. This issue is ignored in most
color constancy methods, primarily due to the complex spatial impact of
multiple light sources on the image. Moreover, most existing multi-illuminant
methods fail to preserve the smooth change of illumination, which stems from
spatial dependencies in natural images. Motivated by this, we propose a novel
multi-illuminant color constancy method, by learning pixel-wise illumination
maps caused by multiple light sources. The proposed method enforces smoothness
within neighboring pixels, by regularizing the training with the total
variation loss. Moreover, a bilateral filter is provisioned further to enhance
the natural appearance of the estimated images, while preserving the edges.
Additionally, we propose a label-smoothing technique that enables the model to
generalize well despite the uncertainties in ground truth. Quantitative and
qualitative experiments demonstrate that the proposed method outperforms the
state-of-the-art.",2024-02-05 11:42:19+00:00,"['Umut Cem Entok', 'Firas Laakom', 'Farhad Pakdaman', 'Moncef Gabbouj']",http://arxiv.org/abs/2402.02922v1
Event-based Asynchronous HDR Imaging by Temporal Incident Light Modulation,"Dynamic Range (DR) is a pivotal characteristic of imaging systems. Current
frame-based cameras struggle to achieve high dynamic range imaging due to the
conflict between globally uniform exposure and spatially variant scene
illumination. In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR
imaging system, based on key insights into the challenges in HDR imaging and
the unique event-generating mechanism of Dynamic Vision Sensors (DVS). Our
proposed AsynHDR system integrates the DVS with a set of LCD panels. The LCD
panels modulate the irradiance incident upon the DVS by altering their
transparency, thereby triggering the pixel-independent event streams. The HDR
image is subsequently decoded from the event streams through our
temporal-weighted algorithm. Experiments under standard test platform and
several challenging scenes have verified the feasibility of the system in HDR
imaging task.",2024-03-14 13:45:09+00:00,"['Yuliang Wu', 'Ganchao Tan', 'Jinze Chen', 'Wei Zhai', 'Yang Cao', 'Zheng-Jun Zha']",http://arxiv.org/abs/2403.09392v1
VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation,"This paper explores the potential of Large Language Models(LLMs) in zero-shot
anomaly detection for safe visual navigation. With the assistance of the
state-of-the-art real-time open-world object detection model Yolo-World and
specialized prompts, the proposed framework can identify anomalies within
camera-captured frames that include any possible obstacles, then generate
concise, audio-delivered descriptions emphasizing abnormalities, assist in safe
visual navigation in complex circumstances. Moreover, our proposed framework
leverages the advantages of LLMs and the open-vocabulary object detection model
to achieve the dynamic scenario switch, which allows users to transition
smoothly from scene to scene, which addresses the limitation of traditional
visual navigation. Furthermore, this paper explored the performance
contribution of different prompt components, provided the vision for future
improvement in visual accessibility, and paved the way for LLMs in video
anomaly detection and vision-language understanding.",2024-03-19 03:55:39+00:00,"['Hao Wang', 'Jiayou Qin', 'Ashish Bastola', 'Xiwen Chen', 'John Suchanek', 'Zihao Gong', 'Abolfazl Razi']",http://arxiv.org/abs/2403.12415v1
Pointsoup: High-Performance and Extremely Low-Decoding-Latency Learned Geometry Codec for Large-Scale Point Cloud Scenes,"Despite considerable progress being achieved in point cloud geometry
compression, there still remains a challenge in effectively compressing
large-scale scenes with sparse surfaces. Another key challenge lies in reducing
decoding latency, a crucial requirement in real-world application. In this
paper, we propose Pointsoup, an efficient learning-based geometry codec that
attains high-performance and extremely low-decoding-latency simultaneously.
Inspired by conventional Trisoup codec, a point model-based strategy is devised
to characterize local surfaces. Specifically, skin features are embedded from
local windows via an attention-based encoder, and dilated windows are
introduced as cross-scale priors to infer the distribution of quantized
features in parallel. During decoding, features undergo fast refinement,
followed by a folding-based point generator that reconstructs point coordinates
with fairly fast speed. Experiments show that Pointsoup achieves
state-of-the-art performance on multiple benchmarks with significantly lower
decoding complexity, i.e., up to 90$\sim$160$\times$ faster than the G-PCCv23
Trisoup decoder on a comparatively low-end platform (e.g., one RTX 2080Ti).
Furthermore, it offers variable-rate control with a single neural model
(2.9MB), which is attractive for industrial practitioners.",2024-04-21 06:31:29+00:00,"['Kang You', 'Kai Liu', 'Li Yu', 'Pan Gao', 'Dandan Ding']",http://arxiv.org/abs/2404.13550v1
Investigation of unsupervised and supervised hyperspectral anomaly detection,"Hyperspectral sensing is a valuable tool for detecting anomalies and
distinguishing between materials in a scene. Hyperspectral anomaly detection
(HS-AD) helps characterize the captured scenes and separates them into anomaly
and background classes. It is vital in agriculture, environment, and military
applications such as RSTA (reconnaissance, surveillance, and target
acquisition) missions. We previously designed an equal voting ensemble of
hyperspectral unmixing and three unsupervised HS-AD algorithms. We later
utilized a supervised classifier to determine the weights of a voting ensemble,
creating a hybrid of heterogeneous unsupervised HS-AD algorithms with a
supervised classifier in a model stacking, which improved detection accuracy.
However, supervised classification methods usually fail to detect novel or
unknown patterns that substantially deviate from those seen previously. In this
work, we evaluate our technique and other supervised and unsupervised methods
using general hyperspectral data to provide new insights.",2024-08-13 17:20:14+00:00,"['Mazharul Hossain', 'Aaron Robinson', 'Lan Wang', 'Chrysanthe Preza']",http://arxiv.org/abs/2408.07114v1
Towards Infusing Auxiliary Knowledge for Distracted Driver Detection,"Distracted driving is a leading cause of road accidents globally.
Identification of distracted driving involves reliably detecting and
classifying various forms of driver distraction (e.g., texting, eating, or
using in-car devices) from in-vehicle camera feeds to enhance road safety. This
task is challenging due to the need for robust models that can generalize to a
diverse set of driver behaviors without requiring extensive annotated datasets.
In this paper, we propose KiD3, a novel method for distracted driver detection
(DDD) by infusing auxiliary knowledge about semantic relations between entities
in a scene and the structural configuration of the driver's pose. Specifically,
we construct a unified framework that integrates the scene graphs, and driver
pose information with the visual cues in video frames to create a holistic
representation of the driver's actions.Our results indicate that KiD3 achieves
a 13.64% accuracy improvement over the vision-only baseline by incorporating
such auxiliary knowledge with visual information.",2024-08-29 15:28:42+00:00,"['Ishwar B Balappanawar', 'Ashmit Chamoli', 'Ruwan Wickramarachchi', 'Aditya Mishra', 'Ponnurangam Kumaraguru', 'Amit P. Sheth']",http://arxiv.org/abs/2408.16621v1
Event-based Mosaicing Bundle Adjustment,"We tackle the problem of mosaicing bundle adjustment (i.e., simultaneous
refinement of camera orientations and scene map) for a purely rotating event
camera. We formulate the problem as a regularized non-linear least squares
optimization. The objective function is defined using the linearized event
generation model in the camera orientations and the panoramic gradient map of
the scene. We show that this BA optimization has an exploitable block-diagonal
sparsity structure, so that the problem can be solved efficiently. To the best
of our knowledge, this is the first work to leverage such sparsity to speed up
the optimization in the context of event-based cameras, without the need to
convert events into image-like representations. We evaluate our method, called
EMBA, on both synthetic and real-world datasets to show its effectiveness (50%
photometric error decrease), yielding results of unprecedented quality. In
addition, we demonstrate EMBA using high spatial resolution event cameras,
yielding delicate panoramas in the wild, even without an initial map. Project
page: https://github.com/tub-rip/emba",2024-09-11 15:53:01+00:00,"['Shuang Guo', 'Guillermo Gallego']",http://arxiv.org/abs/2409.07365v1
Local Policies Enable Zero-shot Long-horizon Manipulation,"Sim2real for robotic manipulation is difficult due to the challenges of
simulating complex contacts and generating realistic task distributions. To
tackle the latter problem, we introduce ManipGen, which leverages a new class
of policies for sim2real transfer: local policies. Locality enables a variety
of appealing properties including invariances to absolute robot and object
pose, skill ordering, and global scene configuration. We combine these policies
with foundation models for vision, language and motion planning and demonstrate
SOTA zero-shot performance of our method to Robosuite benchmark tasks in
simulation (97%). We transfer our local policies from simulation to reality and
observe they can solve unseen long-horizon manipulation tasks with up to 8
stages with significant pose, object and scene configuration variation.
ManipGen outperforms SOTA approaches such as SayCan, OpenVLA, LLMTrajGen and
VoxPoser across 50 real-world manipulation tasks by 36%, 76%, 62% and 60%
respectively. Video results at https://mihdalal.github.io/manipgen/",2024-10-29 17:59:55+00:00,"['Murtaza Dalal', 'Min Liu', 'Walter Talbott', 'Chen Chen', 'Deepak Pathak', 'Jian Zhang', 'Ruslan Salakhutdinov']",http://arxiv.org/abs/2410.22332v2
Splats in Splats: Embedding Invisible 3D Watermark within Gaussian Splatting,"3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction
performance with explicit scene representations. Given the widespread
application of 3DGS in 3D reconstruction and generation tasks, there is an
urgent need to protect the copyright of 3DGS assets. However, existing
copyright protection techniques for 3DGS overlook the usability of 3D assets,
posing challenges for practical deployment. Here we describe WaterGS, the first
3DGS watermarking framework that embeds 3D content in 3DGS itself without
modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep
insight into spherical harmonics (SH) and devise an importance-graded SH
coefficient encryption strategy to embed the hidden SH coefficients.
Furthermore, we employ a convolutional autoencoder to establish a mapping
between the original Gaussian primitives' opacity and the hidden Gaussian
primitives' opacity. Extensive experiments indicate that WaterGS significantly
outperforms existing 3D steganography techniques, with 5.31% higher scene
fidelity and 3X faster rendering speed, while ensuring security, robustness,
and user experience. Codes and data will be released at
https://water-gs.github.io.",2024-12-04 08:40:11+00:00,"['Yijia Guo', 'Wenkai Huang', 'Yang Li', 'Gaolei Li', 'Hang Zhang', 'Liwen Hu', 'Jianhua Li', 'Tiejun Huang', 'Lei Ma']",http://arxiv.org/abs/2412.03121v1
Towards Physically-Based Sky-Modeling,"Accurate environment maps are a key component in rendering photorealistic
outdoor scenes with coherent illumination. They enable captivating visual arts,
immersive virtual reality and a wide range of engineering and scientific
applications. Recent works have extended sky-models to be more comprehensive
and inclusive of cloud formations but existing approaches fall short in
faithfully recreating key-characteristics in physically captured HDRI. As we
demonstrate, environment maps produced by sky-models do not relight scenes with
the same tones, shadows, and illumination coherence as physically captured HDR
imagery. Though the visual quality of DNN-generated LDR and HDR imagery has
greatly progressed in recent years, we demonstrate this progress to be
tangential to sky-modelling. Due to the Extended Dynamic Range (EDR) of 14EV
required for outdoor environment maps inclusive of the sun, sky-modelling
extends beyond the conventional paradigm of High Dynamic Range Imagery (HDRI).
In this work, we propose an all-weather sky-model, learning weathered-skies
directly from physically captured HDR imagery. Per user-controlled positioning
of the sun and cloud formations, our model (AllSky) allows for emulation of
physically captured environment maps with improved retention of the Extended
Dynamic Range (EDR) of the sky.",2024-12-16 15:32:05+00:00,['Ian J. Maquignaz'],http://arxiv.org/abs/2412.11883v1
Enhancing autonomous vehicle safety in rain: a data-centric approach for clear vision,"Autonomous vehicles face significant challenges in navigating adverse
weather, particularly rain, due to the visual impairment of camera-based
systems. In this study, we leveraged contemporary deep learning techniques to
mitigate these challenges, aiming to develop a vision model that processes live
vehicle camera feeds to eliminate rain-induced visual hindrances, yielding
visuals closely resembling clear, rain-free scenes. Using the Car Learning to
Act (CARLA) simulation environment, we generated a comprehensive dataset of
clear and rainy images for model training and testing. In our model, we
employed a classic encoder-decoder architecture with skip connections and
concatenation operations. It was trained using novel batching schemes designed
to effectively distinguish high-frequency rain patterns from low-frequency
scene features across successive image frames. To evaluate the model
performance, we integrated it with a steering module that processes front-view
images as input. The results demonstrated notable improvements in steering
accuracy, underscoring the model's potential to enhance navigation safety and
reliability in rainy weather conditions.",2024-12-29 20:27:12+00:00,"['Mark A. Seferian', 'Jidong J. Yang']",http://arxiv.org/abs/2412.20565v1
Neural Network Diffusion,"Diffusion models have achieved remarkable success in image and video
generation. In this work, we demonstrate that diffusion models can also
\textit{generate high-performing neural network parameters}. Our approach is
simple, utilizing an autoencoder and a diffusion model. The autoencoder
extracts latent representations of a subset of the trained neural network
parameters. Next, a diffusion model is trained to synthesize these latent
representations from random noise. This model then generates new
representations, which are passed through the autoencoder's decoder to produce
new subsets of high-performing network parameters. Across various architectures
and datasets, our approach consistently generates models with comparable or
improved performance over trained networks, with minimal additional cost.
Notably, we empirically find that the generated models are not memorizing the
trained ones. Our results encourage more exploration into the versatile use of
diffusion models. Our code is available
\href{https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion}{here}.",2024-02-20 16:59:03+00:00,"['Kai Wang', 'Dongwen Tang', 'Boya Zeng', 'Yida Yin', 'Zhaopan Xu', 'Yukun Zhou', 'Zelin Zang', 'Trevor Darrell', 'Zhuang Liu', 'Yang You']",http://arxiv.org/abs/2402.13144v3
CoSIGN: Few-Step Guidance of ConSIstency Model to Solve General INverse Problems,"Diffusion models have been demonstrated as strong priors for solving general
inverse problems. Most existing Diffusion model-based Inverse Problem Solvers
(DIS) employ a plug-and-play approach to guide the sampling trajectory with
either projections or gradients. Though effective, these methods generally
necessitate hundreds of sampling steps, posing a dilemma between inference time
and reconstruction quality. In this work, we try to push the boundary of
inference steps to 1-2 NFEs while still maintaining high reconstruction
quality. To achieve this, we propose to leverage a pretrained distillation of
diffusion model, namely consistency model, as the data prior. The key to
achieving few-step guidance is to enforce two types of constraints during the
sampling process of the consistency model: soft measurement constraint with
ControlNet and hard measurement constraint via optimization. Supporting both
single-step reconstruction and multistep refinement, the proposed framework
further provides a way to trade image quality with additional computational
cost. Within comparable NFEs, our method achieves new state-of-the-art in
diffusion-based inverse problem solving, showcasing the significant potential
of employing prior-based inverse problem solvers for real-world applications.
Code is available at: https://github.com/BioMed-AI-Lab-U-Michgan/cosign.",2024-07-17 15:57:50+00:00,"['Jiankun Zhao', 'Bowen Song', 'Liyue Shen']",http://arxiv.org/abs/2407.12676v1
GRIN: Zero-Shot Metric Depth with Pixel-Level Diffusion,"3D reconstruction from a single image is a long-standing problem in computer
vision. Learning-based methods address its inherent scale ambiguity by
leveraging increasingly large labeled and unlabeled datasets, to produce
geometric priors capable of generating accurate predictions across domains. As
a result, state of the art approaches show impressive performance in zero-shot
relative and metric depth estimation. Recently, diffusion models have exhibited
remarkable scalability and generalizable properties in their learned
representations. However, because these models repurpose tools originally
designed for image generation, they can only operate on dense ground-truth,
which is not available for most depth labels, especially in real-world
settings. In this paper we present GRIN, an efficient diffusion model designed
to ingest sparse unstructured training data. We use image features with 3D
geometric positional encodings to condition the diffusion process both globally
and locally, generating depth predictions at a pixel-level. With comprehensive
experiments across eight indoor and outdoor datasets, we show that GRIN
establishes a new state of the art in zero-shot metric monocular depth
estimation even when trained from scratch.",2024-09-15 23:32:04+00:00,"['Vitor Guizilini', 'Pavel Tokmakov', 'Achal Dave', 'Rares Ambrus']",http://arxiv.org/abs/2409.09896v1
Zero-shot Dynamic MRI Reconstruction with Global-to-local Diffusion Model,"Diffusion models have recently demonstrated considerable advancement in the
generation and reconstruction of magnetic resonance imaging (MRI) data. These
models exhibit great potential in handling unsampled data and reducing noise,
highlighting their promise as generative models. However, their application in
dynamic MRI remains relatively underexplored. This is primarily due to the
substantial amount of fully-sampled data typically required for training, which
is difficult to obtain in dynamic MRI due to its spatio-temporal complexity and
high acquisition costs. To address this challenge, we propose a dynamic MRI
reconstruction method based on a time-interleaved acquisition scheme, termed
the Glob-al-to-local Diffusion Model. Specifically, fully encoded
full-resolution reference data are constructed by merging under-sampled k-space
data from adjacent time frames, generating two distinct bulk training datasets
for global and local models. The global-to-local diffusion framework
alternately optimizes global information and local image details, enabling
zero-shot reconstruction. Extensive experiments demonstrate that the proposed
method performs well in terms of noise reduction and detail preservation,
achieving reconstruction quality comparable to that of supervised approaches.",2024-11-06 07:40:27+00:00,"['Yu Guan', 'Kunlong Zhang', 'Qi Qi', 'Dong Wang', 'Ziwen Ke', 'Shaoyu Wang', 'Dong Liang', 'Qiegen Liu']",http://arxiv.org/abs/2411.03723v1
Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models,"Recent advancements in visual generative models have enabled high-quality
image and video generation, opening diverse applications. However, evaluating
these models often demands sampling hundreds or thousands of images or videos,
making the process computationally expensive, especially for diffusion-based
models with inherently slow sampling. Moreover, existing evaluation methods
rely on rigid pipelines that overlook specific user needs and provide numerical
results without clear explanations. In contrast, humans can quickly form
impressions of a model's capabilities by observing only a few samples. To mimic
this, we propose the Evaluation Agent framework, which employs human-like
strategies for efficient, dynamic, multi-round evaluations using only a few
samples per round, while offering detailed, user-tailored analyses. It offers
four key advantages: 1) efficiency, 2) promptable evaluation tailored to
diverse user needs, 3) explainability beyond single numerical scores, and 4)
scalability across various models and tools. Experiments show that Evaluation
Agent reduces evaluation time to 10% of traditional methods while delivering
comparable results. The Evaluation Agent framework is fully open-sourced to
advance research in visual generative models and their efficient evaluation.",2024-12-10 18:52:39+00:00,"['Fan Zhang', 'Shulin Tian', 'Ziqi Huang', 'Yu Qiao', 'Ziwei Liu']",http://arxiv.org/abs/2412.09645v2
Cortical Surface Diffusion Generative Models,"Cortical surface analysis has gained increased prominence, given its
potential implications for neurological and developmental disorders.
Traditional vision diffusion models, while effective in generating natural
images, present limitations in capturing intricate development patterns in
neuroimaging due to limited datasets. This is particularly true for generating
cortical surfaces where individual variability in cortical morphology is high,
leading to an urgent need for better methods to model brain development and
diverse variability inherent across different individuals. In this work, we
proposed a novel diffusion model for the generation of cortical surface
metrics, using modified surface vision transformers as the principal
architecture. We validate our method in the developing Human Connectome Project
(dHCP), the results suggest our model demonstrates superior performance in
capturing the intricate details of evolving cortical surfaces. Furthermore, our
model can generate high-quality realistic samples of cortical surfaces
conditioned on postmenstrual age(PMA) at scan.",2024-02-07 11:12:09+00:00,"['Zhenshan Xie', 'Simon Dahan', 'Logan Z. J. Williams', 'M. Jorge Cardoso', 'Emma C. Robinson']",http://arxiv.org/abs/2402.04753v1
"Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation scans using Linked Denoising Diffusion Probabilistic Models","The rapid advancement of Artificial Intelligence (AI) in biomedical imaging
and radiotherapy is hindered by the limited availability of large imaging data
repositories. With recent research and improvements in denoising diffusion
probabilistic models (DDPM), high quality synthetic medical scans are now
possible. Despite this, there is currently no way of generating multiple
related images, such as a corresponding ground truth which can be used to train
models, so synthetic scans are often manually annotated before use. This
research introduces a novel architecture that is able to generate multiple,
related PET-CT-tumour mask pairs using paired networks and conditional
encoders. Our approach includes innovative, time step-controlled mechanisms and
a `noise-seeding' strategy to improve DDPM sampling consistency. While our
model requires a modified perceptual loss function to ensure accurate feature
alignment we show generation of clearly aligned synthetic images and
improvement in segmentation accuracy with generated images.",2024-03-26 14:21:49+00:00,"['Rowan Bradbury', 'Katherine A. Vallis', 'Bartlomiej W. Papiez']",http://arxiv.org/abs/2403.17734v1
Diffusion as Sound Propagation: Physics-inspired Model for Ultrasound Image Generation,"Deep learning (DL) methods typically require large datasets to effectively
learn data distributions. However, in the medical field, data is often limited
in quantity, and acquiring labeled data can be costly. To mitigate this data
scarcity, data augmentation techniques are commonly employed. Among these
techniques, generative models play a pivotal role in expanding datasets.
However, when it comes to ultrasound (US) imaging, the authenticity of
generated data often diminishes due to the oversight of ultrasound physics.
  We propose a novel approach to improve the quality of generated US images by
introducing a physics-based diffusion model that is specifically designed for
this image modality. The proposed model incorporates an US-specific scheduler
scheme that mimics the natural behavior of sound wave propagation in ultrasound
imaging. Our analysis demonstrates how the proposed method aids in modeling the
attenuation dynamics in US imaging. We present both qualitative and
quantitative results based on standard generative model metrics, showing that
our proposed method results in overall more plausible images. Our code is
available at https://github.com/marinadominguez/diffusion-for-us-images",2024-07-07 16:09:20+00:00,"['Marina Domnguez', 'Yordanka Velikova', 'Nassir Navab', 'Mohammad Farid Azampour']",http://arxiv.org/abs/2407.05428v1
SMGDiff: Soccer Motion Generation using diffusion probabilistic models,"Soccer is a globally renowned sport with significant applications in video
games and VR/AR. However, generating realistic soccer motions remains
challenging due to the intricate interactions between the human player and the
ball. In this paper, we introduce SMGDiff, a novel two-stage framework for
generating real-time and user-controllable soccer motions. Our key idea is to
integrate real-time character control with a powerful diffusion-based
generative model, ensuring high-quality and diverse output motion. In the first
stage, we instantly transform coarse user controls into diverse global
trajectories of the character. In the second stage, we employ a
transformer-based autoregressive diffusion model to generate soccer motions
based on trajectory conditioning. We further incorporate a contact guidance
module during inference to optimize the contact details for realistic ball-foot
interactions. Moreover, we contribute a large-scale soccer motion dataset
consisting of over 1.08 million frames of diverse soccer motions. Extensive
experiments demonstrate that our SMGDiff significantly outperforms existing
methods in terms of motion quality and condition alignment.",2024-11-25 09:25:53+00:00,"['Hongdi Yang', 'Chengyang Li', 'Zhenxuan Wu', 'Gaozheng Li', 'Jingya Wang', 'Jingyi Yu', 'Zhuo Su', 'Lan Xu']",http://arxiv.org/abs/2411.16216v1
Pixel-Wise Recognition for Holistic Surgical Scene Understanding,"This paper presents the Holistic and Multi-Granular Surgical Scene
Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that
models surgical scene understanding as a hierarchy of complementary tasks with
varying levels of granularity. Our approach encompasses long-term tasks, such
as surgical phase and step recognition, and short-term tasks, including
surgical instrument segmentation and atomic visual actions detection. To
exploit our proposed benchmark, we introduce the Transformers for Actions,
Phases, Steps, and Instrument Segmentation (TAPIS) model, a general
architecture that combines a global video feature extractor with localized
region proposals from an instrument segmentation model to tackle the
multi-granularity of our benchmark. Through extensive experimentation in ours
and alternative benchmarks, we demonstrate TAPIS's versatility and
state-of-the-art performance across different tasks. This work represents a
foundational step forward in Endoscopic Vision, offering a novel framework for
future research towards holistic surgical scene understanding.",2024-01-20 09:09:52+00:00,"['Nicols Ayobi', 'Santiago Rodrguez', 'Alejandra Prez', 'Isabela Hernndez', 'Nicols Aparicio', 'Eugnie Dessevres', 'Sebastin Pea', 'Jessica Santander', 'Juan Ignacio Caicedo', 'Nicols Fernndez', 'Pablo Arbelez']",http://arxiv.org/abs/2401.11174v3
SGIFormer: Semantic-guided and Geometric-enhanced Interleaving Transformer for 3D Instance Segmentation,"In recent years, transformer-based models have exhibited considerable
potential in point cloud instance segmentation. Despite the promising
performance achieved by existing methods, they encounter challenges such as
instance query initialization problems and excessive reliance on stacked
layers, rendering them incompatible with large-scale 3D scenes. This paper
introduces a novel method, named SGIFormer, for 3D instance segmentation, which
is composed of the Semantic-guided Mix Query (SMQ) initialization and the
Geometric-enhanced Interleaving Transformer (GIT) decoder. Specifically, the
principle of our SMQ initialization scheme is to leverage the predicted
voxel-wise semantic information to implicitly generate the scene-aware query,
yielding adequate scene prior and compensating for the learnable query set.
Subsequently, we feed the formed overall query into our GIT decoder to
alternately refine instance query and global scene features for further
capturing fine-grained information and reducing complex design intricacies
simultaneously. To emphasize geometric property, we consider bias estimation as
an auxiliary task and progressively integrate shifted point coordinates
embedding to reinforce instance localization. SGIFormer attains
state-of-the-art performance on ScanNet V2, ScanNet200 datasets, and the
challenging high-fidelity ScanNet++ benchmark, striking a balance between
accuracy and efficiency. The code, weights, and demo videos are publicly
available at https://rayyoh.github.io/sgiformer.",2024-07-16 10:17:28+00:00,"['Lei Yao', 'Yi Wang', 'Moyun Liu', 'Lap-Pui Chau']",http://arxiv.org/abs/2407.11564v1
Iterative approach to reconstructing neural disparity fields from light-field data,"This study proposes a neural disparity field (NDF) that establishes an
implicit, continuous representation of scene disparity based on a neural field
and an iterative approach to address the inverse problem of NDF reconstruction
from light-field data. NDF enables seamless and precise characterization of
disparity variations in three-dimensional scenes and can discretize disparity
at any arbitrary resolution, overcoming the limitations of traditional
disparity maps that are prone to sampling errors and interpolation
inaccuracies. The proposed NDF network architecture utilizes hash encoding
combined with multilayer perceptrons to capture detailed disparities in texture
levels, thereby enhancing its ability to represent the geometric information of
complex scenes. By leveraging the spatial-angular consistency inherent in
light-field data, a differentiable forward model to generate a central view
image from the light-field data is developed. Based on the forward model, an
optimization scheme for the inverse problem of NDF reconstruction using
differentiable propagation operators is established. Furthermore, an iterative
solution method is adopted to reconstruct the NDF in the optimization scheme,
which does not require training datasets and applies to light-field data
captured by various acquisition methods. Experimental results demonstrate that
high-quality NDF can be reconstructed from light-field data using the proposed
method. High-resolution disparity can be effectively recovered by NDF,
demonstrating its capability for the implicit, continuous representation of
scene disparities.",2024-07-22 05:06:06+00:00,"['Ligen Shi', 'Chang Liu', 'Xing Zhao', 'Jun Qiu']",http://arxiv.org/abs/2407.15380v1
PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance,"Recently, artificial intelligence techniques for education have been received
increasing attentions, while it still remains an open problem to design the
effective music instrument instructing systems. Although key presses can be
directly derived from sheet music, the transitional movements among key presses
require more extensive guidance in piano performance. In this work, we
construct a piano-hand motion generation benchmark to guide hand movements and
fingerings for piano playing. To this end, we collect an annotated dataset,
PianoMotion10M, consisting of 116 hours of piano playing videos from a
bird's-eye view with 10 million annotated hand poses. We also introduce a
powerful baseline model that generates hand motions from piano audios through a
position predictor and a position-guided gesture generator. Furthermore, a
series of evaluation metrics are designed to assess the performance of the
baseline model, including motion similarity, smoothness, positional accuracy of
left and right hands, and overall fidelity of movement distribution. Despite
that piano key presses with respect to music scores or audios are already
accessible, PianoMotion10M aims to provide guidance on piano fingering for
instruction purposes. The source code and dataset can be accessed at
https://github.com/agnJason/PianoMotion10M.",2024-06-13 17:05:23+00:00,"['Qijun Gan', 'Song Wang', 'Shengtao Wu', 'Jianke Zhu']",http://arxiv.org/abs/2406.09326v2
Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model,"As a fundamental backbone for video generation, diffusion models are
challenged by low inference speed due to the sequential nature of denoising.
Previous methods speed up the models by caching and reusing model outputs at
uniformly selected timesteps. However, such a strategy neglects the fact that
differences among model outputs are not uniform across timesteps, which hinders
selecting the appropriate model outputs to cache, leading to a poor balance
between inference efficiency and visual quality. In this study, we introduce
Timestep Embedding Aware Cache (TeaCache), a training-free caching approach
that estimates and leverages the fluctuating differences among model outputs
across timesteps. Rather than directly using the time-consuming model outputs,
TeaCache focuses on model inputs, which have a strong correlation with the
modeloutputs while incurring negligible computational cost. TeaCache first
modulates the noisy inputs using the timestep embeddings to ensure their
differences better approximating those of model outputs. TeaCache then
introduces a rescaling strategy to refine the estimated differences and
utilizes them to indicate output caching. Experiments show that TeaCache
achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%
Vbench score) degradation of visual quality.",2024-11-28 12:50:05+00:00,"['Feng Liu', 'Shiwei Zhang', 'Xiaofeng Wang', 'Yujie Wei', 'Haonan Qiu', 'Yuzhong Zhao', 'Yingya Zhang', 'Qixiang Ye', 'Fang Wan']",http://arxiv.org/abs/2411.19108v2
RDEIC: Accelerating Diffusion-Based Extreme Image Compression with Relay Residual Diffusion,"Diffusion-based extreme image compression methods have achieved impressive
performance at extremely low bitrates. However, constrained by the iterative
denoising process that starts from pure noise, these methods are limited in
both fidelity and efficiency. To address these two issues, we present Relay
Residual Diffusion Extreme Image Compression (RDEIC), which leverages
compressed feature initialization and residual diffusion. Specifically, we
first use the compressed latent features of the image with added noise, instead
of pure noise, as the starting point to eliminate the unnecessary initial
stages of the denoising process. Second, we directly derive a novel residual
diffusion equation from Stable Diffusion's original diffusion equation that
reconstructs the raw image by iteratively removing the added noise and the
residual between the compressed and target latent features. In this way, we
effectively combine the efficiency of residual diffusion with the powerful
generative capability of Stable Diffusion. Third, we propose a fixed-step
fine-tuning strategy to eliminate the discrepancy between the training and
inference phases, thereby further improving the reconstruction quality.
Extensive experiments demonstrate that the proposed RDEIC achieves
state-of-the-art visual quality and outperforms existing diffusion-based
extreme image compression methods in both fidelity and efficiency. The source
code will be provided in https://github.com/huai-chang/RDEIC.",2024-10-03 16:24:20+00:00,"['Zhiyuan Li', 'Yanhui Zhou', 'Hao Wei', 'Chenyang Ge', 'Ajmal Mian']",http://arxiv.org/abs/2410.02640v2
DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations,"Recent studies have introduced a new class of generative models for
synthesizing implicit neural representations (INRs) that capture arbitrary
continuous signals in various domains. These models opened the door for
domain-agnostic generative models, but they often fail to achieve high-quality
generation. We observed that the existing methods generate the weights of
neural networks to parameterize INRs and evaluate the network with fixed
positional embeddings (PEs). Arguably, this architecture limits the expressive
power of generative models and results in low-quality INR generation. To
address this limitation, we propose Domain-agnostic Latent Diffusion Model for
INRs (DDMI) that generates adaptive positional embeddings instead of neural
networks' weights. Specifically, we develop a Discrete-to-continuous space
Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and
the continuous signal functions in the shared latent space. Additionally, we
introduce a novel conditioning mechanism for evaluating INRs with the
hierarchically decomposed PEs to further enhance expressive power. Extensive
experiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance
Fields, and videos, with seven benchmark datasets, demonstrate the versatility
of DDMI and its superior performance compared to the existing INR generative
models.",2024-01-23 06:21:34+00:00,"['Dogyun Park', 'Sihyeon Kim', 'Sojin Lee', 'Hyunwoo J. Kim']",http://arxiv.org/abs/2401.12517v2
LEDiff: Latent Exposure Diffusion for HDR Generation,"While consumer displays increasingly support more than 10 stops of dynamic
range, most image assets such as internet photographs and generative AI content
remain limited to 8-bit low dynamic range (LDR), constraining their utility
across high dynamic range (HDR) applications. Currently, no generative model
can produce high-bit, high-dynamic range content in a generalizable way.
Existing LDR-to-HDR conversion methods often struggle to produce photorealistic
details and physically-plausible dynamic range in the clipped areas. We
introduce LEDiff, a method that enables a generative model with HDR content
generation through latent space fusion inspired by image-space exposure fusion
techniques. It also functions as an LDR-to-HDR converter, expanding the dynamic
range of existing low-dynamic range images. Our approach uses a small HDR
dataset to enable a pretrained diffusion model to recover detail and dynamic
range in clipped highlights and shadows. LEDiff brings HDR capabilities to
existing generative models and converts any LDR image to HDR, creating
photorealistic HDR outputs for image generation, image-based lighting (HDR
environment map generation), and photographic effects such as depth of field
simulation, where linear HDR data is essential for realistic quality.",2024-12-19 02:15:55+00:00,"['Chao Wang', 'Zhihao Xia', 'Thomas Leimkuehler', 'Karol Myszkowski', 'Xuaner Zhang']",http://arxiv.org/abs/2412.14456v2
Head and Neck Tumor Segmentation from [18F]F-FDG PET/CT Images Based on 3D Diffusion Model,"Head and neck (H&N) cancers are among the most prevalent types of cancer
worldwide, and [18F]F-FDG PET/CT is widely used for H&N cancer management.
Recently, the diffusion model has demonstrated remarkable performance in
various image-generation tasks. In this work, we proposed a 3D diffusion model
to accurately perform H&N tumor segmentation from 3D PET and CT volumes. The 3D
diffusion model was developed considering the 3D nature of PET and CT images
acquired. During the reverse process, the model utilized a 3D UNet structure
and took the concatenation of PET, CT, and Gaussian noise volumes as the
network input to generate the tumor mask. Experiments based on the HECKTOR
challenge dataset were conducted to evaluate the effectiveness of the proposed
diffusion model. Several state-of-the-art techniques based on U-Net and
Transformer structures were adopted as the reference methods. Benefits of
employing both PET and CT as the network input as well as further extending the
diffusion model from 2D to 3D were investigated based on various quantitative
metrics and the uncertainty maps generated. Results showed that the proposed 3D
diffusion model could generate more accurate segmentation results compared with
other methods. Compared to the diffusion model in 2D format, the proposed 3D
model yielded superior results. Our experiments also highlighted the advantage
of utilizing dual-modality PET and CT data over only single-modality data for
H&N tumor segmentation.",2024-01-31 04:34:31+00:00,"['Yafei Dong', 'Kuang Gong']",http://arxiv.org/abs/2401.17593v2
"Latent Diffusion, Implicit Amplification: Efficient Continuous-Scale Super-Resolution for Remote Sensing Images","Recent advancements in diffusion models have significantly improved
performance in super-resolution (SR) tasks. However, previous research often
overlooks the fundamental differences between SR and general image generation.
General image generation involves creating images from scratch, while SR
focuses specifically on enhancing existing low-resolution (LR) images by adding
typically missing high-frequency details. This oversight not only increases the
training difficulty but also limits their inference efficiency. Furthermore,
previous diffusion-based SR methods are typically trained and inferred at fixed
integer scale factors, lacking flexibility to meet the needs of up-sampling
with non-integer scale factors. To address these issues, this paper proposes an
efficient and elastic diffusion-based SR model (E$^2$DiffSR), specially
designed for continuous-scale SR in remote sensing imagery. E$^2$DiffSR employs
a two-stage latent diffusion paradigm. During the first stage, an autoencoder
is trained to capture the differential priors between high-resolution (HR) and
LR images. The encoder intentionally ignores the existing LR content to
alleviate the encoding burden, while the decoder introduces an SR branch
equipped with a continuous scale upsampling module to accomplish the
reconstruction under the guidance of the differential prior. In the second
stage, a conditional diffusion model is learned within the latent space to
predict the true differential prior encoding. Experimental results demonstrate
that E$^2$DiffSR achieves superior objective metrics and visual quality
compared to the state-of-the-art SR methods. Additionally, it reduces the
inference time of diffusion-based SR methods to a level comparable to that of
non-diffusion methods.",2024-10-30 09:14:13+00:00,"['Hanlin Wu', 'Jiangwei Mo', 'Xiaohui Sun', 'Jie Ma']",http://arxiv.org/abs/2410.22830v1
Neural radiance fields-based holography [Invited],"This study presents a novel approach for generating holograms based on the
neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data
is difficult in hologram computation. NeRF is a state-of-the-art technique for
3D light-field reconstruction from 2D images based on volume rendering. The
NeRF can rapidly predict new-view images that do not include a training
dataset. In this study, we constructed a rendering pipeline directly from a 3D
light field generated from 2D images by NeRF for hologram generation using deep
neural networks within a reasonable time. The pipeline comprises three main
components: the NeRF, a depth predictor, and a hologram generator, all
constructed using deep neural networks. The pipeline does not include any
physical calculations. The predicted holograms of a 3D scene viewed from any
direction were computed using the proposed pipeline. The simulation and
experimental results are presented.",2024-03-02 08:49:02+00:00,"['Minsung Kang', 'Fan Wang', 'Kai Kumano', 'Tomoyoshi Ito', 'Tomoyoshi Shimobaba']",http://arxiv.org/abs/2403.01137v2
AToM-Bot: Embodied Fulfillment of Unspoken Human Needs with Affective Theory of Mind,"We propose AToM-Bot, a novel task generation and execution framework for
proactive robot-human interaction, which leverages the human mental and
physical state inference capabilities of the Vision Language Model (VLM)
prompted by the Affective Theory of Mind (AToM). Without requiring explicit
commands by humans, AToM-Bot proactively generates and follows feasible tasks
to improve general human well-being. When around humans, AToM-Bot first detects
current human needs based on inferred human states and observations of the
surrounding environment. It then generates tasks to fulfill these needs, taking
into account its embodied constraints. We designed 16 daily life scenarios
spanning 4 common scenes and tasked the same visual stimulus to 59 human
subjects and our robot. We used the similarity between human open-ended answers
and robot output, and the human satisfaction scores to metric robot
performance. AToM-Bot received high human evaluations in need detection
(6.42/7, 91.7%), embodied solution (6.15/7, 87.8%) and task execution (6.17/7,
88.1%). We show that AToM-Bot excels in generating and executing feasible plans
to fulfill unspoken human needs. Videos and code are available at
https://affective-tom-bot.github.io.",2024-06-12 17:47:04+00:00,"['Wei Ding', 'Fanhong Li', 'Ziteng Ji', 'Zhengrong Xue', 'Jia Liu']",http://arxiv.org/abs/2406.08455v3
"Solutions to Deepfakes: Can Camera Hardware, Cryptography, and Deep Learning Verify Real Images?","The exponential progress in generative AI poses serious implications for the
credibility of all real images and videos. There will exist a point in the
future where 1) digital content produced by generative AI will be
indistinguishable from those created by cameras, 2) high-quality generative
algorithms will be accessible to anyone, and 3) the ratio of all synthetic to
real images will be large. It is imperative to establish methods that can
separate real data from synthetic data with high confidence. We define real
images as those that were produced by the camera hardware, capturing a
real-world scene. Any synthetic generation of an image or alteration of a real
image through generative AI or computer graphics techniques is labeled as a
synthetic image. To this end, this document aims to: present known strategies
in detection and cryptography that can be employed to verify which images are
real, weight the strengths and weaknesses of these strategies, and suggest
additional improvements to alleviate shortcomings.",2024-07-04 22:01:21+00:00,"['Alexander Vilesov', 'Yuan Tian', 'Nader Sehatbakhsh', 'Achuta Kadambi']",http://arxiv.org/abs/2407.04169v1
Diffusion and Multi-Domain Adaptation Methods for Eosinophil Segmentation,"Eosinophilic Esophagitis (EoE) represents a challenging condition for medical
providers today. The cause is currently unknown, the impact on a patient's
daily life is significant, and it is increasing in prevalence. Traditional
approaches for medical image diagnosis such as standard deep learning
algorithms are limited by the relatively small amount of data and difficulty in
generalization. As a response, two methods have arisen that seem to perform
well: Diffusion and Multi-Domain methods with current research efforts favoring
diffusion methods. For the EoE dataset, we discovered that a Multi-Domain
Adversarial Network outperformed a Diffusion based method with a FID of 42.56
compared to 50.65. Future work with diffusion methods should include a
comparison with Multi-Domain adaptation methods to ensure that the best
performance is achieved.",2024-03-17 19:58:35+00:00,"['Kevin Lin', 'Donald Brown', 'Sana Syed', 'Adam Greene']",http://arxiv.org/abs/2403.11323v1
FDDM: Frequency-Decomposed Diffusion Model for Rectum Cancer Dose Prediction in Radiotherapy,"Accurate dose distribution prediction is crucial in the radiotherapy
planning. Although previous methods based on convolutional neural network have
shown promising performance, they have the problem of over-smoothing, leading
to prediction without important high-frequency details. Recently, diffusion
model has achieved great success in computer vision, which excels in generating
images with more high-frequency details, yet suffers from time-consuming and
extensive computational resource consumption. To alleviate these problems, we
propose Frequency-Decomposed Diffusion Model (FDDM) that refines the
high-frequency subbands of the dose map. To be specific, we design a Coarse
Dose Prediction Module (CDPM) to first predict a coarse dose map and then
utilize discrete wavelet transform to decompose the coarse dose map into a
low-frequency subband and three high-frequency subbands. There is a notable
difference between the coarse predicted results and ground truth in
high-frequency subbands. Therefore, we design a diffusion-based module called
High-Frequency Refinement Module (HFRM) that performs diffusion operation in
the high-frequency components of the dose map instead of the original dose map.
Extensive experiments on an in-house dataset verify the effectiveness of our
approach.",2024-10-10 12:48:42+00:00,"['Xin Liao', 'Zhenghao Feng', 'Jianghong Xiao', 'Xingchen Peng', 'Yan Wang']",http://arxiv.org/abs/2410.07876v1
Dynamic 3D Point Cloud Sequences as 2D Videos,"Dynamic 3D point cloud sequences serve as one of the most common and
practical representation modalities of dynamic real-world environments.
However, their unstructured nature in both spatial and temporal domains poses
significant challenges to effective and efficient processing. Existing deep
point cloud sequence modeling approaches imitate the mature 2D video learning
mechanisms by developing complex spatio-temporal point neighbor grouping and
feature aggregation schemes, often resulting in methods lacking effectiveness,
efficiency, and expressive power. In this paper, we propose a novel generic
representation called \textit{Structured Point Cloud Videos} (SPCVs).
Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2D
manifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatial
smoothness and temporal consistency, where the pixel values correspond to the
3D coordinates of points. The structured nature of our SPCV representation
allows for the seamless adaptation of well-established 2D image/video
techniques, enabling efficient and effective processing and analysis of 3D
point cloud sequences. To achieve such re-organization, we design a
self-supervised learning pipeline that is geometrically regularized and driven
by self-reconstructive and deformation field learning objectives. Additionally,
we construct SPCV-based frameworks for both low-level and high-level 3D point
cloud sequence processing and analysis tasks, including action recognition,
temporal interpolation, and compression. Extensive experiments demonstrate the
versatility and superiority of the proposed SPCV, which has the potential to
offer new possibilities for deep learning on unstructured 3D point cloud
sequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.",2024-03-02 08:18:57+00:00,"['Yiming Zeng', 'Junhui Hou', 'Qijian Zhang', 'Siyu Ren', 'Wenping Wang']",http://arxiv.org/abs/2403.01129v2
"Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind","As humans move around, performing their daily tasks, they are able to recall
where they have positioned objects in their environment, even if these objects
are currently out of their sight. In this paper, we aim to mimic this spatial
cognition ability. We thus formulate the task of Out of Sight, Not Out of Mind
- 3D tracking active objects using observations captured through an egocentric
camera. We introduce a simple but effective approach to address this
challenging problem, called Lift, Match, and Keep (LMK). LMK lifts partial 2D
observations to 3D world coordinates, matches them over time using visual
appearance, 3D location and interactions to form object tracks, and keeps these
object tracks even when they go out-of-view of the camera. We benchmark LMK on
100 long videos from EPIC-KITCHENS. Our results demonstrate that spatial
cognition is critical for correctly locating objects over short and long time
scales. E.g., for one long egocentric video, we estimate the 3D location of 50
active objects. After 120 seconds, 57% of the objects are correctly localised
by LMK, compared to just 33% by a recent 3D method for egocentric videos and
17% by a general 2D tracking method.",2024-04-07 21:00:14+00:00,"['Chiara Plizzari', 'Shubham Goel', 'Toby Perrett', 'Jacob Chalk', 'Angjoo Kanazawa', 'Dima Damen']",http://arxiv.org/abs/2404.05072v2
Turbulence Strength $C_n^2$ Estimation from Video using Physics-based Deep Learning,"Images captured from a long distance suffer from dynamic image distortion due
to turbulent flow of air cells with random temperatures, and thus refractive
indices. This phenomenon, known as image dancing, is commonly characterized by
its refractive-index structure constant $C_n^2$ as a measure of the turbulence
strength. For many applications such as atmospheric forecast model,
long-range/astronomy imaging, and aviation safety, optical communication
technology, $C_n^2$ estimation is critical for accurately sensing the turbulent
environment. Previous methods for $C_n^2$ estimation include estimation from
meteorological data (temperature, relative humidity, wind shear, etc.) for
single-point measurements, two-ended pathlength measurements from optical
scintillometer for path-averaged $C_n^2$, and more recently estimating $C_n^2$
from passive video cameras for low cost and hardware complexity. In this paper,
we present a comparative analysis of classical image gradient methods for
$C_n^2$ estimation and modern deep learning-based methods leveraging
convolutional neural networks. To enable this, we collect a dataset of video
capture along with reference scintillometer measurements for ground truth, and
we release this unique dataset to the scientific community. We observe that
deep learning methods can achieve higher accuracy when trained on similar data,
but suffer from generalization errors to other, unseen imagery as compared to
classical methods. To overcome this trade-off, we present a novel physics-based
network architecture that combines learned convolutional layers with a
differentiable image gradient method that maintains high accuracy while being
generalizable across image datasets.",2024-08-29 15:31:51+00:00,"['Ripon Kumar Saha', 'Esen Salcin', 'Jihoo Kim', 'Joseph Smith', 'Suren Jayasuriya']",http://arxiv.org/abs/2408.16623v1
EchoFM: Foundation Model for Generalizable Echocardiogram Analysis,"Foundation models have recently gained significant attention because of their
generalizability and adaptability across multiple tasks and data distributions.
Although medical foundation models have emerged, solutions for cardiac imaging,
especially echocardiography videos, are still unexplored. In this paper, we
introduce EchoFM, a foundation model specifically designed to represent and
analyze echocardiography videos. In EchoFM, we propose a self-supervised
learning framework that captures both spatial and temporal variability patterns
through a spatio-temporal consistent masking strategy and periodic-driven
contrastive learning. This framework can effectively capture the
spatio-temporal dynamics of echocardiography and learn the representative video
features without any labels. We pre-train our model on an extensive dataset
comprising over 290,000 echocardiography videos covering 26 scan views across
different imaging modes, with up to 20 million frames of images. The
pre-trained EchoFM can then be easily adapted and fine-tuned for a variety of
downstream tasks, serving as a robust backbone model. Our evaluation was
systemically designed for four downstream tasks after the echocardiography
examination routine. Experiment results show that EchoFM surpasses
state-of-the-art methods, including specialized echocardiography methods,
self-supervised pre-training models, and general-purposed pre-trained
foundation models, across all downstream tasks.",2024-10-30 19:32:02+00:00,"['Sekeun Kim', 'Pengfei Jin', 'Sifan Song', 'Cheng Chen', 'Yiwei Li', 'Hui Ren', 'Xiang Li', 'Tianming Liu', 'Quanzheng Li']",http://arxiv.org/abs/2410.23413v2
Exploiting VLM Localizability and Semantics for Open Vocabulary Action Detection,"Action detection aims to detect (recognize and localize) human actions
spatially and temporally in videos. Existing approaches focus on the closed-set
setting where an action detector is trained and tested on videos from a fixed
set of action categories. However, this constrained setting is not viable in an
open world where test videos inevitably come beyond the trained action
categories. In this paper, we address the practical yet challenging
Open-Vocabulary Action Detection (OVAD) problem. It aims to detect any action
in test videos while training a model on a fixed set of action categories. To
achieve such an open-vocabulary capability, we propose a novel method OpenMixer
that exploits the inherent semantics and localizability of large
vision-language models (VLM) within the family of query-based detection
transformers (DETR). Specifically, the OpenMixer is developed by spatial and
temporal OpenMixer blocks (S-OMB and T-OMB), and a dynamically fused alignment
(DFA) module. The three components collectively enjoy the merits of strong
generalization from pre-trained VLMs and end-to-end learning from DETR design.
Moreover, we established OVAD benchmarks under various settings, and the
experimental results show that the OpenMixer performs the best over baselines
for detecting seen and unseen actions. We release the codes, models, and
dataset splits at https://github.com/Cogito2012/OpenMixer.",2024-11-17 00:39:59+00:00,"['Wentao Bao', 'Kai Li', 'Yuxiao Chen', 'Deep Patel', 'Martin Renqiang Min', 'Yu Kong']",http://arxiv.org/abs/2411.10922v1
CompactFlowNet: Efficient Real-time Optical Flow Estimation on Mobile Devices,"We present CompactFlowNet, the first real-time mobile neural network for
optical flow prediction, which involves determining the displacement of each
pixel in an initial frame relative to the corresponding pixel in a subsequent
frame. Optical flow serves as a fundamental building block for various
video-related tasks, such as video restoration, motion estimation, video
stabilization, object tracking, action recognition, and video generation. While
current state-of-the-art methods prioritize accuracy, they often overlook
constraints regarding speed and memory usage. Existing light models typically
focus on reducing size but still exhibit high latency, compromise significantly
on quality, or are optimized for high-performance GPUs, resulting in
sub-optimal performance on mobile devices. This study aims to develop a
mobile-optimized optical flow model by proposing a novel mobile
device-compatible architecture, as well as enhancements to the training
pipeline, which optimize the model for reduced weight, low memory utilization,
and increased speed while maintaining minimal error. Our approach demonstrates
superior or comparable performance to the state-of-the-art lightweight models
on the challenging KITTI and Sintel benchmarks. Furthermore, it attains a
significantly accelerated inference speed, thereby yielding real-time
operational efficiency on the iPhone 8, while surpassing real-time performance
levels on more advanced mobile devices.",2024-12-17 19:06:12+00:00,"['Andrei Znobishchev', 'Valerii Filev', 'Oleg Kudashev', 'Nikita Orlov', 'Humphrey Shi']",http://arxiv.org/abs/2412.13273v1
Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports,"Reasoning over sports videos for question answering is an important task with
numerous applications, such as player training and information retrieval.
However, this task has not been explored due to the lack of relevant datasets
and the challenging nature it presents. Most datasets for video question
answering (VideoQA) focus mainly on general and coarse-grained understanding of
daily-life videos, which is not applicable to sports scenarios requiring
professional action understanding and fine-grained motion analysis. In this
paper, we introduce the first dataset, named Sports-QA, specifically designed
for the sports VideoQA task. The Sports-QA dataset includes various types of
questions, such as descriptions, chronologies, causalities, and counterfactual
conditions, covering multiple sports. Furthermore, to address the
characteristics of the sports VideoQA task, we propose a new Auto-Focus
Transformer (AFT) capable of automatically focusing on particular scales of
temporal information for question answering. We conduct extensive experiments
on Sports-QA, including baseline studies and the evaluation of different
methods. The results demonstrate that our AFT achieves state-of-the-art
performance.",2024-01-03 02:22:34+00:00,"['Haopeng Li', 'Andong Deng', 'Jun Liu', 'Hossein Rahmani', 'Yulan Guo', 'Bernt Schiele', 'Mohammed Bennamoun', 'Qiuhong Ke']",http://arxiv.org/abs/2401.01505v4
Efficient UAVs Deployment and Resource Allocation in UAV-Relay Assisted Public Safety Networks for Video Transmission,"Wireless communication highly depends on the cellular ground base station
(GBS). A failure of the cellular GBS, fully or partially, during natural or
man-made disasters creates a communication gap in the disaster-affected areas.
In such situations, public safety communication (PSC) can significantly save
the national infrastructure, property, and lives. Throughout emergencies, the
PSC can provide mission-critical communication and video transmission services
in the affected area. Unmanned aerial vehicles (UAVs) as flying base stations
(UAV-BSs) are particularly suitable for PSC services as they are flexible,
mobile, and easily deployable. This manuscript considers a multi-UAV-assisted
PSC network with an observational UAV receiving videos from the affected area's
ground users (AGUs) and transmitting them to the nearby GBS via a relay UAV.
The objective of the proposed study is to maximize the average utility of the
video streams generated by the AGUs upon reaching the GBS. This is achieved by
optimizing the positions of the observational and relay UAVs, as well as the
distribution of communication resources, such as bandwidth, and transmit power,
while satisfying the system-designed constraints, such as transmission rate,
rate outage probability, transmit power budget, and available bandwidth. To
this end, a joint UAVs placement and resource allocation problem is
mathematically formulated. The proposed problem poses a significant challenge
for a solution. Considering the block coordinate descent and successive convex
approximation techniques, an efficient iterative algorithm is proposed.
Finally, simulation results are provided which show that our proposed approach
outperforms the existing methods.",2024-01-03 09:23:26+00:00,"['Naveed Khan', 'Ayaz Ahmad', 'Abdul Wakeel', 'Zeeshan Kaleem', 'Bushra Rashid', 'Waqas Khalid']",http://arxiv.org/abs/2401.01636v2
Scaling and Masking: A New Paradigm of Data Sampling for Image and Video Quality Assessment,"Quality assessment of images and videos emphasizes both local details and
global semantics, whereas general data sampling methods (e.g., resizing,
cropping or grid-based fragment) fail to catch them simultaneously. To address
the deficiency, current approaches have to adopt multi-branch models and take
as input the multi-resolution data, which burdens the model complexity. In this
work, instead of stacking up models, a more elegant data sampling method (named
as SAMA, scaling and masking) is explored, which compacts both the local and
global content in a regular input size. The basic idea is to scale the data
into a pyramid first, and reduce the pyramid into a regular data dimension with
a masking strategy. Benefiting from the spatial and temporal redundancy in
images and videos, the processed data maintains the multi-scale characteristics
with a regular input size, thus can be processed by a single-branch model. We
verify the sampling method in image and video quality assessment. Experiments
show that our sampling method can improve the performance of current
single-branch models significantly, and achieves competitive performance to the
multi-branch models without extra model complexity. The source code will be
available at https://github.com/Sissuire/SAMA.",2024-01-05 03:12:03+00:00,"['Yongxu Liu', 'Yinghui Quan', 'Guoyao Xiao', 'Aobo Li', 'Jinjian Wu']",http://arxiv.org/abs/2401.02614v1
Experimental Evaluation of Interactive Edge/Cloud Virtual Reality Gaming over Wi-Fi using Unity Render Streaming,"Virtual Reality (VR) streaming enables end-users to seamlessly immerse
themselves in interactive virtual environments using even low-end devices.
However, the quality of the VR experience heavily relies on Wireless Fidelity
(Wi-Fi) performance, since it serves as the last hop in the network chain. Our
study delves into the intricate interplay between Wi-Fi and VR traffic, drawing
upon empirical data and leveraging a Wi-Fi simulator. In this work, we further
evaluate Wi-Fi's suitability for VR streaming in terms of the Quality of
Service (QoS) it provides. In particular, we employ Unity Render Streaming to
remotely stream real-time VR gaming content over Wi-Fi 6 using Web Real-Time
Communication (WebRTC), considering a server physically located at the
network's edge, near the end user. Our findings demonstrate the system's
sustained network performance, showcasing minimal round-trip time (RTT) and
jitter at 60 and 90 frames per second (fps). In addition, we uncover the
characteristics and patterns of the generated traffic streams, unveiling a
distinctive video transmission approach inherent to WebRTC-based services: the
systematic packetization of video frames (VFs) and their transmission in
discrete batches at regular intervals, regardless of the targeted frame rate.
This interval-based transmission strategy maintains consistent video packet
delays across video frame rates but leads to increased Wi-Fi airtime
consumption. Our results demonstrate that shortening the interval between
batches is advantageous, as it enhances Wi-Fi efficiency and reduces delays in
delivering complete frames.",2024-02-01 12:06:41+00:00,"['Miguel Casasnovas', 'Costas Michaelides', 'Marc Carrascosa-Zamacois', 'Boris Bellalta']",http://arxiv.org/abs/2402.00540v2
UniVS: Unified and Universal Video Segmentation with Prompts as Queries,"Despite the recent advances in unified image segmentation (IS), developing a
unified video segmentation (VS) model remains a challenge. This is mainly
because generic category-specified VS tasks need to detect all objects and
track them across consecutive frames, while prompt-guided VS tasks require
re-identifying the target with visual/text prompts throughout the entire video,
making it hard to handle the different tasks with the same architecture. We
make an attempt to address these issues and present a novel unified VS
architecture, namely UniVS, by using prompts as queries. UniVS averages the
prompt features of the target from previous frames as its initial query to
explicitly decode masks, and introduces a target-wise prompt cross-attention
layer in the mask decoder to integrate prompt features in the memory pool. By
taking the predicted masks of entities from previous frames as their visual
prompts, UniVS converts different VS tasks into prompt-guided target
segmentation, eliminating the heuristic inter-frame matching process. Our
framework not only unifies the different VS tasks but also naturally achieves
universal training and testing, ensuring robust performance across different
scenarios. UniVS shows a commendable balance between performance and
universality on 10 challenging VS benchmarks, covering video instance,
semantic, panoptic, object, and referring segmentation tasks. Code can be found
at \url{https://github.com/MinghanLi/UniVS}.",2024-02-28 07:05:27+00:00,"['Minghan Li', 'Shuai Li', 'Xindong Zhang', 'Lei Zhang']",http://arxiv.org/abs/2402.18115v2
DIVERSE: A Dataset of YouTube Video Comment Stances with a Data Programming Model,"Public opinion of military organizations significantly influences their
ability to recruit talented individuals. As recruitment efforts increasingly
extend into digital spaces like social media, it becomes essential to assess
the stance of social media users toward online military content. However, there
is a notable lack of data for analyzing opinions on military recruiting efforts
online, compounded by challenges in stance labeling, which is crucial for
understanding public perceptions. Despite the importance of stance analysis for
successful online military recruitment, creating human-annotated, in-domain
stance labels is resource-intensive. In this paper, we address both the
challenges of stance labeling and the scarcity of data on public opinions of
online military recruitment by introducing and releasing the DIVERSE dataset:
https://doi.org/10.5281/zenodo.10493803. This dataset comprises all comments
from the U.S. Army's official YouTube Channel videos. We employed a
state-of-the-art weak supervision approach, leveraging large language models to
label the stance of each comment toward its respective video and the U.S. Army.
Our findings indicate that the U.S. Army's videos began attracting a
significant number of comments post-2021, with the stance distribution
generally balanced among supportive, oppositional, and neutral comments, with a
slight skew towards oppositional versus supportive comments.",2024-03-05 21:36:23+00:00,"['Iain J. Cruickshank', 'Amir Soofi', 'Lynnette Hui Xian Ng']",http://arxiv.org/abs/2403.03334v3
Detection of Object Throwing Behavior in Surveillance Videos,"Anomalous behavior detection is a challenging research area within computer
vision. Progress in this area enables automated detection of dangerous behavior
using surveillance camera feeds. A dangerous behavior that is often overlooked
in other research is the throwing action in traffic flow, which is one of the
unique requirements of our Smart City project to enhance public safety. This
paper proposes a solution for throwing action detection in surveillance videos
using deep learning. At present, datasets for throwing actions are not publicly
available. To address the use-case of our Smart City project, we first generate
the novel public 'Throwing Action' dataset, consisting of 271 videos of
throwing actions performed by traffic participants, such as pedestrians,
bicyclists, and car drivers, and 130 normal videos without throwing actions.
Second, we compare the performance of different feature extractors for our
anomaly detection method on the UCF-Crime and Throwing-Action datasets. The
explored feature extractors are the Convolutional 3D (C3D) network, the
Inflated 3D ConvNet (I3D) network, and the Multi-Fiber Network (MFNet).
Finally, the performance of the anomaly detection algorithm is improved by
applying the Adam optimizer instead of Adadelta, and proposing a mean normal
loss function that covers the multitude of normal situations in traffic. Both
aspects yield better anomaly detection performance. Besides this, the proposed
mean normal loss function lowers the false alarm rate on the combined dataset.
The experimental results reach an area under the ROC curve of 86.10 for the
Throwing-Action dataset, and 80.13 on the combined dataset, respectively.",2024-03-11 09:53:19+00:00,"['Ivo P. C. Kersten', 'Erkut Akdag', 'Egor Bondarev', 'Peter H. N. De With']",http://arxiv.org/abs/2403.06552v1
WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity,"For robotic surgical videos, instrument presence annotations are typically
recorded with video streams, which offering the potential to reduce the
manually annotated costs for segmentation. However, weakly supervised surgical
instrument segmentation with only instrument presence labels has been rarely
explored in surgical domain due to the highly under-constrained challenges.
Temporal properties can enhance representation learning by capturing sequential
dependencies and patterns over time even in incomplete supervision situations.
From this, we take the inherent temporal attributes of surgical video into
account and extend a two-stage weakly supervised segmentation paradigm from
different perspectives. Firstly, we make temporal equivariance constraint to
enhance pixel-wise temporal consistency between adjacent features. Secondly, we
constrain class-aware semantic continuity between global and local regions
across temporal dimension. Finally, we generate temporal-enhanced pseudo masks
from consecutive frames to suppress irrelevant regions. Extensive experiments
are validated on two surgical video datasets, including one cholecystectomy
surgery benchmark and one real robotic left lateral segment liver surgery
dataset. We annotate instance-wise instrument labels with fixed time-steps
which are double checked by a clinician with 3-years experience to evaluate
segmentation results. Experimental results demonstrate the promising
performances of our method, which consistently achieves comparable or favorable
results with previous state-of-the-art approaches.",2024-03-14 16:39:11+00:00,"['Qiyuan Wang', 'Yanzhe Liu', 'Shang Zhao', 'Rong Liu', 'S. Kevin Zhou']",http://arxiv.org/abs/2403.09551v2
Test-Time Zero-Shot Temporal Action Localization,"Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate
actions in untrimmed videos unseen during training. Existing ZS-TAL methods
involve fine-tuning a model on a large amount of annotated training data. While
effective, training-based ZS-TAL approaches assume the availability of labeled
data for supervised learning, which can be impractical in some applications.
Furthermore, the training process naturally induces a domain bias into the
learned model, which may adversely affect the model's generalization ability to
arbitrary videos. These considerations prompt us to approach the ZS-TAL problem
from a radically novel perspective, relaxing the requirement for training data.
To this aim, we introduce a novel method that performs Test-Time adaptation for
Temporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained
Vision and Language Model (VLM). T3AL operates in three steps. First, a
video-level pseudo-label of the action category is computed by aggregating
information from the entire video. Then, action localization is performed
adopting a novel procedure inspired by self-supervised learning. Finally,
frame-level textual descriptions extracted with a state-of-the-art captioning
model are employed for refining the action region proposals. We validate the
effectiveness of T3AL by conducting experiments on the THUMOS14 and the
ActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly
outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the
benefit of a test-time adaptation approach.",2024-04-08 11:54:49+00:00,"['Benedetta Liberatori', 'Alessandro Conti', 'Paolo Rota', 'Yiming Wang', 'Elisa Ricci']",http://arxiv.org/abs/2404.05426v2
Reconstructing Hand-Held Objects in 3D from Images and Videos,"Objects manipulated by the hand (i.e., manipulanda) are particularly
challenging to reconstruct from Internet videos. Not only does the hand occlude
much of the object, but also the object is often only visible in a small number
of image pixels. At the same time, two strong anchors emerge in this setting:
(1) estimated 3D hands help disambiguate the location and scale of the object,
and (2) the set of manipulanda is small relative to all possible objects. With
these insights in mind, we present a scalable paradigm for hand-held object
reconstruction that builds on recent breakthroughs in large language/vision
models and 3D object datasets. Given a monocular RGB video, we aim to
reconstruct hand-held object geometry in 3D, over time. In order to obtain the
best performing single frame model, we first present MCC-Hand-Object (MCC-HO),
which jointly reconstructs hand and object geometry given a single RGB image
and inferred 3D hand as inputs. Subsequently, we prompt a text-to-3D generative
model using GPT-4(V) to retrieve a 3D object model that matches the object in
the image(s); we call this alignment Retrieval-Augmented Reconstruction (RAR).
RAR provides unified object geometry across all frames, and the result is
rigidly aligned with both the input images and 3D MCC-HO observations in a
temporally consistent manner. Experiments demonstrate that our approach
achieves state-of-the-art performance on lab and Internet image/video datasets.
We make our code and models available on the project website:
https://janehwu.github.io/mcc-ho",2024-04-09 17:55:41+00:00,"['Jane Wu', 'Georgios Pavlakos', 'Georgia Gkioxari', 'Jitendra Malik']",http://arxiv.org/abs/2404.06507v3
An Animation-based Augmentation Approach for Action Recognition from Discontinuous Video,"Action recognition, an essential component of computer vision, plays a
pivotal role in multiple applications. Despite significant improvements brought
by Convolutional Neural Networks (CNNs), these models suffer performance
declines when trained with discontinuous video frames, which is a frequent
scenario in real-world settings. This decline primarily results from the loss
of temporal continuity, which is crucial for understanding the semantics of
human actions. To overcome this issue, we introduce the 4A (Action
Animation-based Augmentation Approach) pipeline, which employs a series of
sophisticated techniques: starting with 2D human pose estimation from RGB
videos, followed by Quaternion-based Graph Convolution Network for joint
orientation and trajectory prediction, and Dynamic Skeletal Interpolation for
creating smoother, diversified actions using game engine technology. This
innovative approach generates realistic animations in varied game environments,
viewed from multiple viewpoints. In this way, our method effectively bridges
the domain gap between virtual and real-world data. In experimental
evaluations, the 4A pipeline achieves comparable or even superior performance
to traditional training approaches using real-world data, while requiring only
10% of the original data volume. Additionally, our approach demonstrates
enhanced performance on In-the-wild videos, marking a significant advancement
in the field of action recognition.",2024-04-10 04:59:51+00:00,"['Xingyu Song', 'Zhan Li', 'Shi Chen', 'Xin-Qiang Cai', 'Kazuyuki Demachi']",http://arxiv.org/abs/2404.06741v4
Mumpy: Multilateral Temporal-view Pyramid Transformer for Video Inpainting Detection,"The task of video inpainting detection is to expose the pixel-level inpainted
regions within a video sequence. Existing methods usually focus on leveraging
spatial and temporal inconsistencies. However, these methods typically employ
fixed operations to combine spatial and temporal clues, limiting their
applicability in different scenarios. In this paper, we introduce a novel
Multilateral Temporal-view Pyramid Transformer ({\em MumPy}) that collaborates
spatial-temporal clues flexibly. Our method utilizes a newly designed
multilateral temporal-view encoder to extract various collaborations of
spatial-temporal clues and introduces a deformable window-based temporal-view
interaction module to enhance the diversity of these collaborations.
Subsequently, we develop a multi-pyramid decoder to aggregate the various types
of features and generate detection maps. By adjusting the contribution strength
of spatial and temporal clues, our method can effectively identify inpainted
regions. We validate our method on existing datasets and also introduce a new
challenging and large-scale Video Inpainting dataset based on the YouTube-VOS
dataset, which employs several more recent inpainting methods. The results
demonstrate the superiority of our method in both in-domain and cross-domain
evaluation scenarios.",2024-04-17 03:56:28+00:00,"['Ying Zhang', 'Yuezun Li', 'Bo Peng', 'Jiaran Zhou', 'Huiyu Zhou', 'Junyu Dong']",http://arxiv.org/abs/2404.11054v3
On-the-Fly Point Annotation for Fast Medical Video Labeling,"Purpose: In medical research, deep learning models rely on high-quality
annotated data, a process often laborious and timeconsuming. This is
particularly true for detection tasks where bounding box annotations are
required. The need to adjust two corners makes the process inherently
frame-by-frame. Given the scarcity of experts' time, efficient annotation
methods suitable for clinicians are needed. Methods: We propose an on-the-fly
method for live video annotation to enhance the annotation efficiency. In this
approach, a continuous single-point annotation is maintained by keeping the
cursor on the object in a live video, mitigating the need for tedious pausing
and repetitive navigation inherent in traditional annotation methods. This
novel annotation paradigm inherits the point annotation's ability to generate
pseudo-labels using a point-to-box teacher model. We empirically evaluate this
approach by developing a dataset and comparing on-the-fly annotation time
against traditional annotation method. Results: Using our method, annotation
speed was 3.2x faster than the traditional annotation technique. We achieved a
mean improvement of 6.51 +- 0.98 AP@50 over conventional method at equivalent
annotation budgets on the developed dataset. Conclusion: Without bells and
whistles, our approach offers a significant speed-up in annotation tasks. It
can be easily implemented on any annotation platform to accelerate the
integration of deep learning in video-based medical research.",2024-04-22 16:59:43+00:00,"['Meyer Adrien', 'Mazellier Jean-Paul', 'Jeremy Dana', 'Nicolas Padoy']",http://arxiv.org/abs/2404.14344v1
Skim then Focus: Integrating Contextual and Fine-grained Views for Repetitive Action Counting,"The key to action counting is accurately locating each video's repetitive
actions. Instead of estimating the probability of each frame belonging to an
action directly, we propose a dual-branch network, i.e., SkimFocusNet, working
in a two-step manner. The model draws inspiration from empirical observations
indicating that humans typically engage in coarse skimming of entire sequences
to grasp the general action pattern initially, followed by a finer,
frame-by-frame focus to determine if it aligns with the target action.
Specifically, SkimFocusNet incorporates a skim branch and a focus branch. The
skim branch scans the global contextual information throughout the sequence to
identify potential target action for guidance. Subsequently, the focus branch
utilizes the guidance to diligently identify repetitive actions using a
long-short adaptive guidance (LSAG) block. Additionally, we have observed that
videos in existing datasets often feature only one type of repetitive action,
which inadequately represents real-world scenarios. To more accurately describe
real-life situations, we establish the Multi-RepCount dataset, which includes
videos containing multiple repetitive motions. On Multi-RepCount, our
SkimFoucsNet can perform specified action counting, that is, to enable counting
a particular action type by referencing an exemplary video. This capability
substantially exhibits the robustness of our method. Extensive experiments
demonstrate that SkimFocusNet achieves state-of-the-art performances with
significant improvements. We also conduct a thorough ablation study to evaluate
the network components. The source code will be published upon acceptance.",2024-06-13 05:15:52+00:00,"['Zhengqi Zhao', 'Xiaohu Huang', 'Hao Zhou', 'Kun Yao', 'Errui Ding', 'Jingdong Wang', 'Xinggang Wang', 'Wenyu Liu', 'Bin Feng']",http://arxiv.org/abs/2406.08814v1
Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos,"Category-level 3D pose estimation is a fundamentally important problem in
computer vision and robotics, e.g. for embodied agents or to train 3D
generative models. However, so far methods that estimate the category-level
object pose require either large amounts of human annotations, CAD models or
input from RGB-D sensors. In contrast, we tackle the problem of learning to
estimate the category-level 3D pose only from casually taken object-centric
videos without human supervision. We propose a two-step pipeline: First, we
introduce a multi-view alignment procedure that determines canonical camera
poses across videos with a novel and robust cyclic distance formulation for
geometric and appearance matching using reconstructed coarse meshes and DINOv2
features. In a second step, the canonical poses and reconstructed meshes enable
us to train a model for 3D pose estimation from a single image. In particular,
our model learns to estimate dense correspondences between images and a
prototypical 3D template by predicting, for each pixel in a 2D image, a feature
vector of the corresponding vertex in the template mesh. We demonstrate that
our method outperforms all baselines at the unsupervised alignment of
object-centric videos by a large margin and provides faithful and robust
predictions in-the-wild. Our code and data is available at
https://github.com/GenIntel/uns-obj-pose3d.",2024-07-05 09:43:05+00:00,"['Leonhard Sommer', 'Artur Jesslen', 'Eddy Ilg', 'Adam Kortylewski']",http://arxiv.org/abs/2407.04384v1
TAPVid-3D: A Benchmark for Tracking Any Point in 3D,"We introduce a new benchmark, TAPVid-3D, for evaluating the task of
long-range Tracking Any Point in 3D (TAP-3D). While point tracking in two
dimensions (TAP) has many benchmarks measuring performance on real-world
videos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To
this end, leveraging existing footage, we build a new benchmark for 3D point
tracking featuring 4,000+ real-world videos, composed of three different data
sources spanning a variety of object types, motion patterns, and indoor and
outdoor environments. To measure performance on the TAP-3D task, we formulate a
collection of metrics that extend the Jaccard-based metric used in TAP to
handle the complexities of ambiguous depth scales across models, occlusions,
and multi-track spatio-temporal smoothness. We manually verify a large sample
of trajectories to ensure correct video annotations, and assess the current
state of the TAP-3D task by constructing competitive baselines using existing
tracking models. We anticipate this benchmark will serve as a guidepost to
improve our ability to understand precise 3D motion and surface deformation
from monocular video. Code for dataset download, generation, and model
evaluation is available at https://tapvid3d.github.io",2024-07-08 13:28:47+00:00,"['Skanda Koppula', 'Ignacio Rocco', 'Yi Yang', 'Joe Heyward', 'Joo Carreira', 'Andrew Zisserman', 'Gabriel Brostow', 'Carl Doersch']",http://arxiv.org/abs/2407.05921v2
A Comprehensive Review of Few-shot Action Recognition,"Few-shot action recognition aims to address the high cost and impracticality
of manually labeling complex and variable video data in action recognition. It
requires accurately classifying human actions in videos using only a few
labeled examples per class. Compared to few-shot learning in image scenarios,
few-shot action recognition is more challenging due to the intrinsic complexity
of video data. Recognizing actions involves modeling intricate temporal
sequences and extracting rich semantic information, which goes beyond mere
human and object identification in each frame. Furthermore, the issue of
intra-class variance becomes particularly pronounced with limited video
samples, complicating the learning of representative features for novel action
categories. To overcome these challenges, numerous approaches have driven
significant advancements in few-shot action recognition, which underscores the
need for a comprehensive survey. Unlike early surveys that focus on few-shot
image or text classification, we deeply consider the unique challenges of
few-shot action recognition. In this survey, we review a wide variety of recent
methods and summarize the general framework. Additionally, the survey presents
the commonly used benchmarks and discusses relevant advanced topics and
promising future directions. We hope this survey can serve as a valuable
resource for researchers, offering essential guidance to newcomers and
stimulating seasoned researchers with fresh insights.",2024-07-20 03:53:32+00:00,"['Yuyang Wanyan', 'Xiaoshan Yang', 'Weiming Dong', 'Changsheng Xu']",http://arxiv.org/abs/2407.14744v1
"UNQA: Unified No-Reference Quality Assessment for Audio, Image, Video, and Audio-Visual Content","As multimedia data flourishes on the Internet, quality assessment (QA) of
multimedia data becomes paramount for digital media applications. Since
multimedia data includes multiple modalities including audio, image, video, and
audio-visual (A/V) content, researchers have developed a range of QA methods to
evaluate the quality of different modality data. While they exclusively focus
on addressing the single modality QA issues, a unified QA model that can handle
diverse media across multiple modalities is still missing, whereas the latter
can better resemble human perception behaviour and also have a wider range of
applications. In this paper, we propose the Unified No-reference Quality
Assessment model (UNQA) for audio, image, video, and A/V content, which tries
to train a single QA model across different media modalities. To tackle the
issue of inconsistent quality scales among different QA databases, we develop a
multi-modality strategy to jointly train UNQA on multiple QA databases. Based
on the input modality, UNQA selectively extracts the spatial features, motion
features, and audio features, and calculates a final quality score via the four
corresponding modality regression modules. Compared with existing QA methods,
UNQA has two advantages: 1) the multi-modality training strategy makes the QA
model learn more general and robust quality-aware feature representation as
evidenced by the superior performance of UNQA compared to state-of-the-art QA
methods. 2) UNQA reduces the number of models required to assess multimedia
data across different modalities. and is friendly to deploy to practical
applications.",2024-07-29 04:56:56+00:00,"['Yuqin Cao', 'Xiongkuo Min', 'Yixuan Gao', 'Wei Sun', 'Weisi Lin', 'Guangtao Zhai']",http://arxiv.org/abs/2407.19704v1
Dynamic and Compressive Adaptation of Transformers From Images to Videos,"Recently, the remarkable success of pre-trained Vision Transformers (ViTs)
from image-text matching has sparked an interest in image-to-video adaptation.
However, most current approaches retain the full forward pass for each frame,
leading to a high computation overhead for processing entire videos. In this
paper, we present InTI, a novel approach for compressive image-to-video
adaptation using dynamic Inter-frame Token Interpolation. InTI aims to softly
preserve the informative tokens without disrupting their coherent
spatiotemporal structure. Specifically, each token pair at identical positions
within neighbor frames is linearly aggregated into a new token, where the
aggregation weights are generated by a multi-scale context-aware network. In
this way, the information of neighbor frames can be adaptively compressed in a
point-by-point manner, thereby effectively reducing the number of processed
frames by half each time. Importantly, InTI can be seamlessly integrated with
existing adaptation methods, achieving strong performance without extra-complex
design. On Kinetics-400, InTI reaches a top-1 accuracy of 87.1 with a
remarkable 37.5% reduction in GFLOPs compared to naive adaptation. When
combined with additional temporal modules, InTI achieves a top-1 accuracy of
87.6 with a 37% reduction in GFLOPs. Similar conclusions have been verified in
other common datasets.",2024-08-13 12:01:22+00:00,"['Guozhen Zhang', 'Jingyu Liu', 'Shengming Cao', 'Xiaotong Zhao', 'Kevin Zhao', 'Kai Ma', 'Limin Wang']",http://arxiv.org/abs/2408.06840v2
Classification of Endoscopy and Video Capsule Images using CNN-Transformer Model,"Gastrointestinal cancer is a leading cause of cancer-related incidence and
death, making it crucial to develop novel computer-aided diagnosis systems for
early detection and enhanced treatment. Traditional approaches rely on the
expertise of gastroenterologists to identify diseases; however, this process is
subjective, and interpretation can vary even among expert clinicians.
Considering recent advancements in classifying gastrointestinal anomalies and
landmarks in endoscopic and video capsule endoscopy images, this study proposes
a hybrid model that combines the advantages of Transformers and Convolutional
Neural Networks (CNNs) to enhance classification performance. Our model
utilizes DenseNet201 as a CNN branch to extract local features and integrates a
Swin Transformer branch for global feature understanding, combining both to
perform the classification task. For the GastroVision dataset, our proposed
model demonstrates excellent performance with Precision, Recall, F1 score,
Accuracy, and Matthews Correlation Coefficient (MCC) of 0.8320, 0.8386, 0.8324,
0.8386, and 0.8191, respectively, showcasing its robustness against class
imbalance and surpassing other CNNs as well as the Swin Transformer model.
Similarly, for the Kvasir-Capsule, a large video capsule endoscopy dataset, our
model outperforms all others, achieving overall Precision, Recall, F1 score,
Accuracy, and MCC of 0.7007, 0.7239, 0.6900, 0.7239, and 0.3871. Moreover, we
generated saliency maps to explain our model's focus areas, demonstrating its
reliable decision-making process. The results underscore the potential of our
hybrid CNN-Transformer model in aiding the early and accurate detection of
gastrointestinal (GI) anomalies.",2024-08-20 11:05:32+00:00,"['Aliza Subedi', 'Smriti Regmi', 'Nisha Regmi', 'Bhumi Bhusal', 'Ulas Bagci', 'Debesh Jha']",http://arxiv.org/abs/2408.10733v1
Cascaded Temporal Updating Network for Efficient Video Super-Resolution,"Existing video super-resolution (VSR) methods generally adopt a recurrent
propagation network to extract spatio-temporal information from the entire
video sequences, exhibiting impressive performance. However, the key components
in recurrent-based VSR networks significantly impact model efficiency, e.g.,
the alignment module occupies a substantial portion of model parameters, while
the bidirectional propagation mechanism significantly amplifies the inference
time. Consequently, developing a compact and efficient VSR method that can be
deployed on resource-constrained devices, e.g., smartphones, remains
challenging. To this end, we propose a cascaded temporal updating network
(CTUN) for efficient VSR. We first develop an implicit cascaded alignment
module to explore spatio-temporal correspondences from adjacent frames.
Moreover, we propose a unidirectional propagation updating network to
efficiently explore long-range temporal information, which is crucial for
high-quality video reconstruction. Specifically, we develop a simple yet
effective hidden updater that can leverage future information to update hidden
features during forward propagation, significantly reducing inference time
while maintaining performance. Finally, we formulate all of these components
into an end-to-end trainable VSR network. Extensive experimental results show
that our CTUN achieves a favorable trade-off between efficiency and performance
compared to existing methods. Notably, compared with BasicVSR, our method
obtains better results while employing only about 30% of the parameters and
running time. The source code and pre-trained models will be available at
https://github.com/House-Leo/CTUN.",2024-08-26 12:59:32+00:00,"['Hao Li', 'Jiangxin Dong', 'Jinshan Pan']",http://arxiv.org/abs/2408.14244v1
Learning to Play Video Games with Intuitive Physics Priors,"Video game playing is an extremely structured domain where algorithmic
decision-making can be tested without adverse real-world consequences. While
prevailing methods rely on image inputs to avoid the problem of hand-crafting
state space representations, this approach systematically diverges from the way
humans actually learn to play games. In this paper, we design object-based
input representations that generalize well across a number of video games.
Using these representations, we evaluate an agent's ability to learn games
similar to an infant - with limited world experience, employing simple
inductive biases derived from intuitive representations of physics from the
real world. Using such biases, we construct an object category representation
to be used by a Q-learning algorithm and assess how well it learns to play
multiple games based on observed object affordances. Our results suggest that a
human-like object interaction setup capably learns to play several video games,
and demonstrates superior generalizability, particularly for unfamiliar
objects. Further exploring such methods will allow machines to learn in a
human-centric way, thus incorporating more human-like learning benefits.",2024-09-20 20:30:27+00:00,"['Abhishek Jaiswal', 'Nisheeth Srivastava']",http://arxiv.org/abs/2409.13886v1
X-Prompt: Multi-modal Visual Prompt for Video Object Segmentation,"Multi-modal Video Object Segmentation (VOS), including RGB-Thermal,
RGB-Depth, and RGB-Event, has garnered attention due to its capability to
address challenging scenarios where traditional VOS methods struggle, such as
extreme illumination, rapid motion, and background distraction. Existing
approaches often involve designing specific additional branches and performing
full-parameter fine-tuning for fusion in each task. However, this paradigm not
only duplicates research efforts and hardware costs but also risks model
collapse with the limited multi-modal annotated data. In this paper, we propose
a universal framework named X-Prompt for all multi-modal video object
segmentation tasks, designated as RGB+X. The X-Prompt framework first
pre-trains a video object segmentation foundation model using RGB data, and
then utilize the additional modality of the prompt to adapt it to downstream
multi-modal tasks with limited data. Within the X-Prompt framework, we
introduce the Multi-modal Visual Prompter (MVP), which allows prompting
foundation model with the various modalities to segment objects precisely. We
further propose the Multi-modal Adaptation Experts (MAEs) to adapt the
foundation model with pluggable modality-specific knowledge without
compromising the generalization capacity. To evaluate the effectiveness of the
X-Prompt framework, we conduct extensive experiments on 3 tasks across 4
benchmarks. The proposed universal X-Prompt framework consistently outperforms
the full fine-tuning paradigm and achieves state-of-the-art performance. Code:
https://github.com/PinxueGuo/X-Prompt.git",2024-09-28 13:04:29+00:00,"['Pinxue Guo', 'Wanyun Li', 'Hao Huang', 'Lingyi Hong', 'Xinyu Zhou', 'Zhaoyu Chen', 'Jinglun Li', 'Kaixun Jiang', 'Wei Zhang', 'Wenqiang Zhang']",http://arxiv.org/abs/2409.19342v1
Recording dynamic facial micro-expressions with a multi-focus camera array,"We present an approach of utilizing a multi-camera array system for capturing
dynamic high-resolution videos of the human face, with improved imaging
performance as compared to traditional single-camera configurations. Employing
an array of 54 individual high-resolution cameras, each with its own 13
megapixel sensor (709 megapixels total), we uniquely focus each camera to a
different plane across the curved surface of the human face in order to capture
dynamic facial expressions. Post-processing methods then stitch together each
synchronized set of 54 images into a composite video frame. Our multi-focus
strategy overcomes the resolution and depth-of-field (DOF) limitations for
capturing macroscopically curved surfaces such as the human face, while
maintaining high lateral resolution. Specifically we demonstrate how our setup
achieves a generally uniform lateral resolution of 26.75 +/- 8.8 micrometer
across a composite DOF of ~43mm that covers the entire face (85 cm^2 + FOV).
Compared to a single-focus configuration this is almost a 10-fold increase in
effective DOF. We believe that our new approach for multi-focus camera array
video sets the stage for future video capture of a variety of dynamic and
macroscopically curved surfaces at microscopic resolution.",2024-10-02 19:30:21+00:00,"['Lucas Kreiss', 'Weiheng Tang', 'Ramana Balla', 'Xi Yang', 'Amey Chaware', 'Kanghyun Kim', 'Clare B. Cook', 'Aurelien Begue', 'Clay Dugo', 'Mark Harfouche', 'Kevin C. Zhou', 'Roarke Horstmeyer']",http://arxiv.org/abs/2410.01973v1
FLAASH: Flow-Attention Adaptive Semantic Hierarchical Fusion for Multi-Modal Tobacco Content Analysis,"The proliferation of tobacco-related content on social media platforms poses
significant challenges for public health monitoring and intervention. This
paper introduces a novel multi-modal deep learning framework named
Flow-Attention Adaptive Semantic Hierarchical Fusion (FLAASH) designed to
analyze tobacco-related video content comprehensively. FLAASH addresses the
complexities of integrating visual and textual information in short-form videos
by leveraging a hierarchical fusion mechanism inspired by flow network theory.
Our approach incorporates three key innovations, including a flow-attention
mechanism that captures nuanced interactions between visual and textual
modalities, an adaptive weighting scheme that balances the contribution of
different hierarchical levels, and a gating mechanism that selectively
emphasizes relevant features. This multi-faceted approach enables FLAASH to
effectively process and analyze diverse tobacco-related content, from product
showcases to usage scenarios. We evaluate FLAASH on the Multimodal Tobacco
Content Analysis Dataset (MTCAD), a large-scale collection of tobacco-related
videos from popular social media platforms. Our results demonstrate significant
improvements over existing methods, outperforming state-of-the-art approaches
in classification accuracy, F1 score, and temporal consistency. The proposed
method also shows strong generalization capabilities when tested on standard
video question-answering datasets, surpassing current models. This work
contributes to the intersection of public health and artificial intelligence,
offering an effective tool for analyzing tobacco promotion in digital media.",2024-10-25 17:20:22+00:00,"['Naga VS Raviteja Chappa', 'Page Daniel Dobbs', 'Bhiksha Raj', 'Khoa Luu']",http://arxiv.org/abs/2410.19896v2
"MSEG-VCUQ: Multimodal SEGmentation with Enhanced Vision Foundation Models, Convolutional Neural Networks, and Uncertainty Quantification for High-Speed Video Phase Detection Data","High-speed video (HSV) phase detection (PD) segmentation is crucial for
monitoring vapor, liquid, and microlayer phases in industrial processes. While
CNN-based models like U-Net have shown success in simplified shadowgraphy-based
two-phase flow (TPF) analysis, their application to complex HSV PD tasks
remains unexplored, and vision foundation models (VFMs) have yet to address the
complexities of either shadowgraphy-based or PD TPF video segmentation.
Existing uncertainty quantification (UQ) methods lack pixel-level reliability
for critical metrics like contact line density and dry area fraction, and the
absence of large-scale, multimodal experimental datasets tailored to PD
segmentation further impedes progress. To address these gaps, we propose
MSEG-VCUQ. This hybrid framework integrates U-Net CNNs with the
transformer-based Segment Anything Model (SAM) to achieve enhanced segmentation
accuracy and cross-modality generalization. Our approach incorporates
systematic UQ for robust error assessment and introduces the first open-source
multimodal HSV PD datasets. Empirical results demonstrate that MSEG-VCUQ
outperforms baseline CNNs and VFMs, enabling scalable and reliable PD
segmentation for real-world boiling dynamics.",2024-11-12 00:54:26+00:00,"['Chika Maduabuchi', 'Ericmoore Jossou', 'Matteo Bucci']",http://arxiv.org/abs/2411.07463v4
Principles of Visual Tokens for Efficient Video Understanding,"Video understanding has made huge strides in recent years, relying largely on
the power of transformers. As this architecture is notoriously expensive and
video data is highly redundant, research into improving efficiency has become
particularly relevant. Some creative solutions include token selection and
merging. While most methods succeed in reducing the cost of the model and
maintaining accuracy, an interesting pattern arises: most methods do not
outperform the baseline of randomly discarding tokens. In this paper we take a
closer look at this phenomenon and observe 5 principles of the nature of visual
tokens. For example, we observe that the value of tokens follows a clear
Pareto-distribution where most tokens have remarkably low value, and just a few
carry most of the perceptual information. We build on these and further
insights to propose a lightweight video model, LITE, that can select a small
number of tokens effectively, outperforming state-of-the-art and existing
baselines across datasets (Kinetics-400 and Something-Something-V2) in the
challenging trade-off of computation (GFLOPs) vs accuracy. Experiments also
show that LITE generalizes across datasets and even other tasks without the
need for retraining.",2024-11-20 14:09:47+00:00,"['Xinyue Hao', 'Gen Li', 'Shreyank N Gowda', 'Robert B Fisher', 'Jonathan Huang', 'Anurag Arnab', 'Laura Sevilla-Lara']",http://arxiv.org/abs/2411.13626v2
Self-supervised Video Instance Segmentation Can Boost Geographic Entity Alignment in Historical Maps,"Tracking geographic entities from historical maps, such as buildings, offers
valuable insights into cultural heritage, urbanization patterns, environmental
changes, and various historical research endeavors. However, linking these
entities across diverse maps remains a persistent challenge for researchers.
Traditionally, this has been addressed through a two-step process: detecting
entities within individual maps and then associating them via a heuristic-based
post-processing step. In this paper, we propose a novel approach that combines
segmentation and association of geographic entities in historical maps using
video instance segmentation (VIS). This method significantly streamlines
geographic entity alignment and enhances automation. However, acquiring
high-quality, video-format training data for VIS models is prohibitively
expensive, especially for historical maps that often contain hundreds or
thousands of geographic entities. To mitigate this challenge, we explore
self-supervised learning (SSL) techniques to enhance VIS performance on
historical maps. We evaluate the performance of VIS models under different
pretraining configurations and introduce a novel method for generating
synthetic videos from unlabeled historical map images for pretraining. Our
proposed self-supervised VIS method substantially reduces the need for manual
annotation. Experimental results demonstrate the superiority of the proposed
self-supervised VIS approach, achieving a 24.9\% improvement in AP and a 0.23
increase in F1 score compared to the model trained from scratch.",2024-11-26 13:31:51+00:00,"['Xue Xia', 'Randall Balestriero', 'Tao Zhang', 'Lorenz Hurni']",http://arxiv.org/abs/2411.17425v1
