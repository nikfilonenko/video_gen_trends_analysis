text
video video generative adversarial network fewshot learning based policy gradient development sophisticated model videotovideo synthesis facilitated recent advance deep reinforcement learning generative adversarial network gans paper propose new deep neural network approach based reinforcement learning unsupervised conditional videotovideo synthesis preserving unique style source video domain approach aim learn mapping source video domain target video domain train model using policy gradient employ convlstm layer capture spatial temporal information designing finegrained gan architecture incorporating spatiotemporal adversarial goal adversarial loss aid content translation preserving style unlike traditional videotovideo synthesis method requiring paired input proposed approach general require paired input thus dealing limited video target domain ie fewshot learning particularly effective experiment show produce temporally coherent video result result highlight potential approach advance videotovideo synthesis
contrastive sequentialdiffusion learning nonlinear multiscene instructional video synthesis generated video scene actioncentric sequence description recipe instruction doityourself project often include nonlinear pattern next video may need visually consistent immediately preceding video earlier one current multiscene video synthesis approach fail meet consistency requirement address propose contrastive sequential video diffusion method selects suitable previously generated scene guide condition denoising process next scene result multiscene video grounded scene description coherent wrt scene require visual consistency experiment actioncentered data real world demonstrate practicality improved consistency model compared previous work
recapture generative video camera control userprovided video using masked video finetuning recently breakthrough video modeling allowed controllable camera trajectory generated video however method directly applied userprovided video generated video model paper present recapture method generating new video novel camera trajectory single userprovided video method allows u regenerate reference video existing scene motion vastly different angle cinematic camera motion notably using method also plausibly hallucinate part scene observable reference video method work generating noisy anchor video new camera trajectory using multiview diffusion model depthbased point cloud rendering regenerating anchor video clean temporally consistent reangled video using proposed masked video finetuning technique
consistent dynamic extendable long video generation text texttovideo diffusion model enable generation highquality video follow text instruction making easy create diverse individual content however existing approach mostly focus highquality short video generation typically frame ending hardcuts naively extended case long video synthesis overcome limitation introduce autoregressive approach long video generation frame smooth transition key component arei shortterm memory block called conditional attention module cam condition current generation feature extracted previous chunk via attentional mechanism leading consistent chunk transition ii longterm memory block called appearance preservation module extract highlevel scene object feature first video chunk prevent model forgetting initial scene iii randomized blending approach enables apply video enhancer autoregressively infinitely long video without inconsistency chunk experiment show generates high motion amount contrast competing imagetovideo method prone video stagnation applied naively autoregressive manner thus propose highquality seamless texttolong video generator outperforms competitor consistency motion code available
freelong trainingfree long video generation spectralblend temporal attention video diffusion model made substantial progress various video generation application however training model long video generation task require significant computational data resource posing challenge developing long video diffusion model paper investigates straightforward trainingfree approach extend existing short video diffusion model eg pretrained video consistent long video generation eg frame preliminary observation found directly applying short video diffusion model generate long video lead severe video quality degradation investigation reveals degradation primarily due distortion highfrequency component long video characterized decrease spatial highfrequency component increase temporal highfrequency component motivated propose novel solution named freelong balance frequency distribution long video feature denoising process freelong blend lowfrequency component global video feature encapsulate entire video sequence highfrequency component local video feature focus shorter subsequence frame approach maintains global consistency incorporating diverse highquality spatiotemporal detail local video enhancing consistency fidelity long video generation evaluated freelong multiple base video diffusion model observed significant improvement additionally method support coherent multiprompt generation ensuring visual coherence seamless transition scene
svg stereoscopic video generation via denoising frame matrix video generation model demonstrated great capability producing impressive monocular video however generation stereoscopic video remains underexplored propose posefree trainingfree approach generating stereoscopic video using offtheshelf monocular video generation model method warp generated monocular video camera view stereoscopic baseline using estimated video depth employ novel frame matrix video inpainting framework framework leverage video generation model inpaint frame observed different timestamps view effective approach generates consistent semantically coherent stereoscopic video without scene optimization model finetuning moreover develop disocclusion boundary reinjection scheme improves quality video inpainting alleviating negative effect propagated disoccluded area latent space validate efficacy proposed method conducting experiment video various generative model including sora lumiere walt zeroscope experiment demonstrate method significant improvement previous method code released
collaborative video diffusion consistent multivideo generation camera control research video generation recently made tremendous progress enabling highquality video generated text prompt image adding control video generation process important goal moving forward recent approach condition video generation model camera trajectory make stride towards yet remains challenging generate video scene multiple different camera trajectory solution multivideo generation problem could enable largescale scene generation editable camera trajectory among application introduce collaborative video diffusion cvd important step towards vision cvd framework includes novel crossvideo synchronization module promotes consistency corresponding frame video rendered different camera pose using epipolar attention mechanism trained top stateoftheart cameracontrol module video generation cvd generates multiple video rendered different camera trajectory significantly better consistency baseline shown extensive experiment project page httpscollaborativevideodiffusiongithubio
dynamicscaler seamless scalable video generation panoramic scene increasing demand immersive arvr application spatial intelligence heightened need generate highquality scenelevel panoramic video however video diffusion model constrained limited resolution aspect ratio restricts applicability scenelevel dynamic content synthesis work propose dynamicscaler addressing challenge enabling spatially scalable panoramic dynamic scene synthesis preserve coherence across panoramic scene arbitrary size specifically introduce offset shifting denoiser facilitating efficient synchronous coherent denoising panoramic dynamic scene via diffusion model fixed resolution seamless rotating window ensures seamless boundary transition consistency across entire panoramic space accommodating varying resolution aspect ratio additionally employ global motion guidance mechanism ensure local detail fidelity global motion continuity extensive experiment demonstrate method achieves superior content motion quality panoramic scenelevel video generation offering trainingfree efficient scalable solution immersive dynamic scene creation constant vram consumption regardless output video resolution project page available urlhttpsdynamicscalerpagesdev
accelerating video diffusion model via distribution matching generative model particularly diffusion model made significant success data synthesis across various modality including image video asset however current diffusion model computationally intensive often requiring numerous sampling step limit practical application especially video generation work introduces novel framework diffusion distillation distribution matching dramatically reduces number inference step maintainingand potentially improvinggeneration quality approach focus distilling pretrained diffusion model efficient fewstep generator specifically targeting video generation leveraging combination video gan loss novel score distribution matching loss demonstrate potential generate highquality video frame substantially fewer sampling step specific proposed method incorporates denoising gan discriminator distil real data pretrained image diffusion model enhance frame quality promptfollowing capability experimental result using animatediff teacher model showcase method effectiveness achieving superior performance four sampling step compared existing technique
generative adversarial synthesis radar point cloud scene validation verification automotive radar datasets realistic traffic scenario required ever laborious acquire paper introduce radar scene synthesis using gans alternative real dataset acquisition simulationbased approach train pointnet based gan model generate realistic radar point cloud scene use binary classifier evaluate performance scene generated using model test set real scene demonstrate gan model achieves similar performance real scene test set
mimo controllable character video synthesis spatial decomposed modeling character video synthesis aim produce realistic video animatable character within lifelike scene fundamental problem computer vision graphic community work typically require multiview capture percase training severely limit applicability modeling arbitrary character short time recent method break limitation via pretrained diffusion model struggle pose generality scene interaction end propose mimo novel framework synthesize character video controllable attribute ie character motion scene provided simple user input also simultaneously achieve advanced scalability arbitrary character generality novel motion applicability interactive realworld scene unified framework core idea encode video compact spatial code considering inherent nature video occurrence concretely lift frame pixel using monocular depth estimator decompose video clip three spatial component ie main human underlying scene floating occlusion hierarchical layer based depth component encoded canonical identity code structured motion code full scene code utilized control signal synthesis process design spatial decomposed modeling enables flexible user control complex motion expression well synthesis scene interaction experimental result demonstrate effectiveness robustness proposed method
multisentence video grounding long video generation video generation witnessed great success recently application generating long video still remains challenging due difficulty maintaining temporal consistency generated video high memory cost generation tackle problem paper propose brave new idea multisentence video grounding long video generation connecting massive video moment retrieval video generation task first time providing new paradigm long video generation method work summarized three step design sequential scene text prompt query video grounding utilizing massive video moment retrieval search video moment segment meet text requirement video database ii based source frame retrieved video moment segment adopt video editing method create new video content preserving temporal consistency retrieved video since editing conducted segment segment even frame frame largely reduces memory cost iii also attempt video morphing personalized generation method improve subject consistency long video generation providing ablation experimental result subtasks long video generation approach seamlessly extends development imagevideo editing video morphing personalized generation video grounding long video generation offering effective solution generating long video low memory cost
image free stepping stone texttovideo generation texttovideo generation trailed behind texttoimage generation term quality diversity primarily due inherent complexity spatiotemporal modeling limited availability videotext datasets recent texttovideo diffusion model employ image intermediate step significantly enhancing overall performance incurring high training cost paper present novel video diffusion inference pipeline leverage advanced image technique enhance pretrained texttovideo diffusion model requires additional training instead vanilla texttovideo inference pipeline consists two stage anchor image synthesis anchor imageaugmented texttovideo synthesis correspondingly simple yet effective generationselection strategy employed achieve visuallyrealistic semanticallyfaithful anchor image innovative noiseinvariant video score distillation sampling nivsds developed animate image dynamic video distilling motion knowledge video diffusion model followed video regeneration process refine video extensive experiment show proposed method produce video higher visual realism textual fidelity furthermore also support seamlessly integrated existing imagetovideo diffusion model thereby improving overall video quality
exvideo extending video diffusion model via parameterefficient posttuning recently advancement video synthesis attracted significant attention video synthesis model animatediff stable video diffusion demonstrated practical applicability diffusion model creating dynamic visual content emergence sora spotlighted potential video generation technology nonetheless extension video length constrained limitation computational resource existing video synthesis model generate short video clip paper propose novel posttuning methodology video synthesis model called exvideo approach designed enhance capability current video synthesis model allowing produce content extended temporal duration incurring lower training expenditure particular design extension strategy across common temporal model architecture respectively including convolution temporal attention positional embedding evaluate efficacy proposed posttuning approach conduct extension training stable video diffusion model approach augments model capacity generate original number frame requiring gpu hour training dataset comprising video importantly substantial increase video length doesnt compromise model innate generalization capability model showcase advantage generating video diverse style resolution release source code enhanced model publicly
turn im real towards robust detection aigenerated video impressive achievement generative model creating highquality video raised concern digital integrity privacy vulnerability recent work combat deepfakes video developed detector highly accurate identifying gangenerated sample however robustness detector diffusiongenerated video generated video creation tool eg sora openai runway pika etc still unexplored paper propose novel framework detecting video synthesized multiple stateoftheart sota generative model stable video diffusion find sota method detecting diffusiongenerated image lack robustness identifying diffusiongenerated video analysis reveals effectiveness detector diminishes applied outofdomain video primarily struggle track temporal feature dynamic variation frame address abovementioned challenge collect new benchmark video dataset diffusiongenerated video using sota video creation tool extract representation within explicit knowledge diffusion model video frame train detector cnn lstm architecture evaluation show framework well capture temporal feature frame achieves detection accuracy indomain video improves accuracy outdomain video point
create anything multiview video diffusion model present method creating dynamic scene monocular video leverage multiview video diffusion model trained diverse combination datasets enable novel view synthesis specified camera pose timestamps combined novel sampling approach model transform single monocular video multiview video enabling robust reconstruction via optimization deformable gaussian representation demonstrate competitive performance novel view synthesis dynamic scene reconstruction benchmark highlight creative capability scene generation real generated video see project page result interactive demo
cono consistency noise injection tuningfree long video diffusion tuningfree long video diffusion proposed generate extendedduration video enriched content reusing knowledge pretrained short video diffusion model without retraining however work overlook finegrained longterm video consistency modeling resulting limited scene consistency ie unreasonable object background transition especially multiple text input mitigate propose consistency noise injection dubbed cono introduces lookback mechanism enhance finegrained scene transition different video clip design longterm consistency regularization eliminate content shift extending video content noise prediction particular lookback mechanism break noise scheduling process three essential part one internal noise prediction part injected two videoextending part intending achieve finegrained transition two video clip longterm consistency regularization focus explicitly minimizing pixelwise distance predicted noise extended video clip original one thereby preventing abrupt scene transition extensive experiment shown effectiveness strategy performing longvideo generation single multitext prompt condition project available
video diffusion model effective generator automatic generation recently attracted widespread attention recent method greatly accelerated generation speed usually produce lessdetailed object due limited model capacity data motivated recent advancement video diffusion model introduce leverage world simulation capacity pretrained video diffusion model facilitate generation fully unleash potential video diffusion perceive world introduce geometrical consistency prior extend video diffusion model multiview consistent generator benefiting stateoftheart video diffusion model could finetuned generate orbit frame surrounding object given single image tailored reconstruction pipeline generate highquality mesh gaussians within minute furthermore method extended scenelevel novel view synthesis achieving precise control camera path sparse input view extensive experiment demonstrate superior performance proposed approach especially term generation quality multiview consistency code available
video worth thousand image exploring latest trend long video generation image may convey thousand word video composed hundred thousand image frame tell intricate story despite significant progress multimodal large language model mllms generating extended video remains formidable challenge writing openais sora current stateoftheart system still limited producing video one minute length limitation stem complexity long video generation requires generative ai technique approximating density function essential aspect planning story development maintaining spatial temporal consistency present additional hurdle integrating generative ai divideandconquer approach could improve scalability longer video offering greater control survey examine current landscape long video generation covering foundational technique like gans diffusion model video generation strategy largescale training datasets quality metric evaluating long video future research area address limitation existing video generation capability believe would serve comprehensive foundation offering extensive information guide future advancement research field long video generation
videostudio generating consistentcontent multiscene video recent innovation breakthrough diffusion model significantly expanded possibility generating highquality video given prompt existing work tackle singlescene scenario one video event occurring single background extending generate multiscene video nevertheless trivial necessitates nicely manage logic preserving consistent visual appearance key content across video scene paper propose novel framework namely videostudio consistentcontent multiscene video generation technically videostudio leverage large language model llm convert input prompt comprehensive multiscene script benefit logical knowledge learnt llm script scene includes prompt describing event foregroundbackground entity well camera movement videostudio identifies common entity throughout script asks llm detail entity resultant entity description fed texttoimage model generate reference image entity finally videostudio output multiscene video generating scene video via diffusion process take reference image descriptive prompt event camera movement account diffusion model incorporates reference image condition alignment strengthen content consistency multiscene video extensive experiment demonstrate videostudio outperforms sota video generation model term visual quality content consistency user preference source code available urlhttpsgithubcomfuchenustcvideostudio
human motion generation generating realistic human video remains challenging task effective method currently relying human motion sequence control signal existing approach often use existing motion extracted video restricts application specific motion type global scene matching propose novel approach generate human motion sequence conditioned scene image allowing diverse motion adapts different scene approach utilizes diffusion model accepts scene image text prompt input producing motion sequence tailored scene train model collect largescale video dataset featuring singlehuman activity annotating video corresponding human motion target output experiment demonstrate method effectively predicts human motion aligns scene image projection furthermore show generated motion sequence improves human motion quality video synthesis task
synthesis dynamic scene using video diffusion recent frontier computer vision task video generation consists generating timevarying representation scene generate dynamic scene current method explicitly model temporal dynamic jointly optimizing consistency across time view scene paper instead investigate whether necessary explicitly enforce multiview consistency time current approach sufficient model generate representation timestep independently hence propose model leverage video diffusion generate video first generating seed video temporal dynamic independently generating representation timestep seed video evaluate two stateoftheart video generation method find achieves comparable result despite explicitly modeling temporal dynamic ablate quality depends number view generated per frame observe degradation fewer view performance degradation remains minor result thus suggest temporal knowledge may necessary generate highquality dynamic scene potentially enabling simpler generative algorithm task
spatialdreamer selfsupervised stereo video synthesis monocular input stereo video synthesis monocular input demanding task field spatial computing virtual reality main challenge task lie insufficiency highquality paired stereo video training difficulty maintaining spatiotemporal consistency frame existing method primarily address issue directly applying novel view synthesis nv technique video facing limitation inability effectively represent dynamic scene requirement large amount training data paper introduce novel selfsupervised stereo video synthesis paradigm via video diffusion model termed spatialdreamer meet challenge headon firstly address stereo video data insufficiency propose depth based video generation module dvg employ forwardbackward rendering mechanism generate paired video geometric temporal prior leveraging data generated dvg propose refinernet along selfsupervised synthetic framework designed facilitate efficient dedicated training importantly devise consistency control module consists metric stereo deviation strength temporal interaction learning module til geometric temporal consistency ensurance respectively evaluated proposed method various benchmark method result showcasing superior performance
vidpanos generative panoramic video casual panning video panoramic image stitching provides unified wideangle view scene extends beyond camera field view stitching frame panning video panoramic photograph wellunderstood problem stationary scene object moving still panorama capture scene present method synthesizing panoramic video casuallycaptured panning video original video captured wideangle camera pose panorama synthesis spacetime outpainting problem aim create full panoramic video length input video consistent completion spacetime volume requires powerful realistic prior video content motion adapt generative video model existing generative model however immediately extend panorama completion show instead apply video generation component panorama synthesis system demonstrate exploit strength model minimizing limitation system create video panorama range inthewild scene including people vehicle flowing water well stationary background feature
generative camera dolly extreme monocular dynamic novel view synthesis accurate reconstruction complex dynamic scene single viewpoint continues challenging task computer vision current dynamic novel view synthesis method typically require video many different camera viewpoint necessitating careful recording setup significantly restricting utility wild well term embodied ai application paper propose textbfgcd controllable monocular dynamic view synthesis pipeline leverage largescale diffusion prior given video scene generate synchronous video chosen perspective conditioned set relative camera pose parameter model require depth input explicitly model scene geometry instead performing endtoend videotovideo translation order achieve goal efficiently despite trained synthetic multiview video data zeroshot realworld generalization experiment show promising result multiple domain including robotics object permanence driving environment believe framework potentially unlock powerful application rich dynamic scene understanding perception robotics interactive video viewing experience virtual reality
longtake video dataset temporally dense caption efficacy video generation model heavily depends quality training datasets previous video generation model trained short video clip recently increasing interest training long video generation model directly longer video however lack highquality long video impedes advancement long video generation promote research long video generation desire new dataset four key feature essential training long video generation model long video covering least second longtake video without cut large motion diverse content temporally dense caption achieve introduce new pipeline selecting highquality longtake video generating temporally dense caption specifically define set metric quantitatively assess video quality including scene cut dynamic degree semanticlevel quality enabling u filter highquality longtake video large amount source video subsequently develop hierarchical video captioning pipeline annotate long video temporallydense caption pipeline curate first longtake video dataset comprising million longtake video covering second annotated temporally dense caption validate effectiveness finetuning video generation model generate long video dynamic motion believe work significantly contribute future research long video generation
svsgan leveraging gans semantic video synthesis recent year growing interest semantic image synthesis si use generative adversarial network gans diffusion model field seen innovation implementation specialized loss function tailored task diverging general approach imagetoimage translation concept semantic video synthesis generation temporally coherent realistic sequence image semantic newly formalized paper existing method already explored aspect field approach rely generic loss function designed videotovideo translation require additional data achieve temporal coherence paper introduce svsgan framework specifically designed svs featuring custom architecture loss function approach includes triplepyramid generator utilizes spade block additionally employ unetbased network image discriminator performs semantic segmentation oasis loss combination tailored architecture objective engineering framework aim bridge existing gap si svs outperforming current stateoftheart model datasets like cityscape
finegained zeroshot video sampling incorporating temporal dimension pretrained image diffusion model video generation prevalent approach however method computationally demanding necessitates largescale video datasets critically heterogeneity image video datasets often result catastrophic forgetting image expertise recent attempt directly extract video snippet image diffusion model somewhat mitigated problem nevertheless method generate brief video clip simple movement fail capture finegrained motion nongrid deformation paper propose novel zeroshot video sampling algorithm denoted capable directly sampling highquality video clip existing image synthesis method stable diffusion without training optimization specifically utilizes dependency noise model temporal momentum attention ensure content consistency animation coherence respectively ability enables excel related task conditional contextspecialized video generation instructionguided video editing experimental result demonstrate achieves stateoftheart performance zeroshot video generation occasionally outperforming recent supervised method homepage urlhttpsdensechengithubiozss
gaussianstolife textdriven animation gaussian splatting scene stateoftheart novel view synthesis method achieve impressive result multiview capture static scene however reconstructed scene still lack liveliness key component creating engaging experience recently novel video diffusion model generate realistic video complex motion enable animation image however naively used animate scene lack multiview consistency breathe life static world propose method animating part highquality scene gaussian splatting representation key idea leverage powerful video diffusion model generative component model combine robust technique lift video meaningful motion find contrast prior work enables realistic animation complex preexisting scene enables animation large variety object class related work mostly focused priorbased character animation single object model enables creation consistent immersive experience arbitrary scene
driving scene synthesis freeform trajectory generative prior driving scene synthesis along freeform trajectory essential driving simulation enable closedloop evaluation endtoend driving policy existing method excel novel view synthesis recorded trajectory face challenge novel trajectory due limited view driving video vastness driving environment tackle challenge propose novel freeform driving view synthesis approach dubbed drivex leveraging video generative prior optimize model across variety trajectory concretely crafted inverse problem enables video diffusion model utilized prior manytrajectory optimization parametric model eg gaussian splatting seamlessly use generative prior iteratively conduct process optimization resulting model produce highfidelity virtual driving environment outside recorded trajectory enabling freeform trajectory driving simulation beyond real driving scene drivex also utilized simulate virtual driving world aigenerated video
highfidelity texttovideo synthesis compressed representation present texttovideo generation model capable producing realistic scene textual description building recent advancement openais sora explore latent diffusion model ldm architecture introduce video variational autoencoder vidvae vidvae compress video data spatially temporally significantly reducing length visual token computational demand associated generating longsequence video address computational cost propose divideandmerge strategy maintains temporal consistency across video segment diffusion transformer dit model incorporates spatial temporal selfattention layer enabling robust generalization across different timeframes aspect ratio devised data processing pipeline beginning collected highquality videotext pair pipeline includes multiple step clipping text detection motion estimation aesthetic scoring dense captioning based inhouse videollm model training vidvae dit model required approximately day respectively model support video generation endtoend way demonstrates competitive performance stateoftheart model
progressive autoregressive video diffusion model current frontier video diffusion model demonstrated remarkable result generating highquality video however generate short video clip normally around second frame due computation limitation training work show existing model naturally extended autoregressive video diffusion model without changing architecture key idea assign latent frame progressively increasing noise level rather single noise level allows finegrained condition among latents large overlap attention window progressive video denoising allows model autoregressively generate video frame without quality degradation abrupt scene change present stateoftheart result long video generation minute frame fps video paper available httpsdesaixiegithubiopavdm
drivegenvlm realworld video generation vision language model based autonomous driving advancement autonomous driving technology necessitates increasingly sophisticated method understanding predicting realworld scenario vision language model vlms emerging revolutionary tool significant potential influence autonomous driving paper propose drivegenvlm framework generate driving video use vlms understand achieve employ video generation framework grounded denoising diffusion probabilistic model ddpm aimed predicting realworld video sequence explore adequacy generated video use vlms employing pretrained model known efficient incontext learning egocentric video eilev diffusion model trained waymo open dataset evaluated using frechet video distance fvd score ensure quality realism generated video corresponding narration provided eilev generated video may beneficial autonomous driving domain narration enhance traffic scene understanding aid navigation improve planning capability integration video generation vlms drivegenvlm framework represents significant step forward leveraging advanced ai model address complex challenge autonomous driving
raccoon versatile instructional video editing framework autogenerated narrative recent video generative model primarily rely carefully written text prompt specific task like inpainting style editing require laborintensive textual description input video hindering flexibility adapt personalraw video user specification paper proposes raccoon versatile userfriendly videotoparagraphtovideo generative framework support multiple video editing capability removal addition modification unified pipeline raccoon consists two principal stage videotoparagraph paragraphtovideo stage automatically describe video scene wellstructured natural language capturing holistic context focused object detail subsequently stage user optionally refine description guide video diffusion model enabling various modification input video removing changing subject andor adding new object proposed approach stand method several significant contribution raccoon suggests multigranular spatiotemporal pooling strategy generate wellstructured video description capturing broad context object detail without requiring complex human annotation simplifying precise video content editing based text user video generative model incorporates autogenerated narrative instruction enhance quality accuracy generated content raccoon also plan imagine new object given video user simply prompt model receive detailed video editing plan complex video editing proposed framework demonstrates impressive versatile capability videotoparagraph generation video content editing incorporated sota video generative model enhancement
illumination histogram consistency metric quantitative assessment video sequence advance deep generative model greatly accelerate process video procession video enhancement synthesis learning spatiotemporal video model requires capture temporal dynamic scene addition visual appearance individual frame illumination consistency reflects variation illumination dynamic video sequence play vital role video processing unfortunately date wellaccepted quantitative metric proposed video illumination consistency evaluation paper propose illumination histogram consistency ihc metric quantitatively automatically evaluate illumination consistency video sequence ihc measure illumination variation video sequence based illumination histogram discrepancy across frame video sequence specifically given video sequence first estimate illumination map individual frame using retinex model using illumination map mean illumination histogram video sequence computed mean operation across frame next compute illumination histogram discrepancy individual frame mean illumination histogram sum illumination histogram discrepancy represent illumination variation video sequence finally obtain ihc score illumination histogram discrepancy via normalization subtraction operation experiment conducted illustrate performance proposed ihc metric capability measure illumination variation video sequence source code available urlhttpsgithubcomlongchencvihcmetric
dynamic content generation multiframe multiview consistency present stable video latent video diffusion model multiframe multiview consistent dynamic content generation unlike previous method rely separately trained generative model video generation novel view synthesis design unified diffusion model generate novel view video dynamic object specifically given monocular reference video generates novel view video frame temporally consistent use generated novel view video optimize implicit representation dynamic nerf efficiently without need cumbersome sdsbased optimization used prior work train unified novel view video generation model curate dynamic object dataset existing objaverse dataset extensive experimental result multiple datasets user study demonstrate stateoftheart performance novelview video synthesis well generation compared prior work
analyzing improving camera control video diffusion transformer numerous work recently integrated camera control foundational texttovideo model resulting camera control often imprecise video generation quality suffers work analyze camera motion first principle perspective uncovering insight enable precise camera manipulation without compromising synthesis quality first determine motion induced camera movement video lowfrequency nature motivates u adjust train test pose conditioning schedule accelerating training convergence improving visual motion quality probing representation unconditional video diffusion transformer observe implicitly perform camera pose estimation hood subportion layer contain camera information suggested u limit injection camera conditioning subset architecture prevent interference video feature leading reduction training parameter improved training speed higher visual quality finally complement typical dataset camera control learning curated dataset diverse dynamic video stationary camera help model distinguish camera scene motion improves dynamic generated poseconditioned video compound finding design advanced camera control architecture new stateoftheart model generative video modeling camera control
redefining temporal modeling video diffusion vectorized timestep approach diffusion model revolutionized image generation extension video generation shown promise however current video diffusion modelsvdms rely scalar timestep variable applied clip level limit ability model complex temporal dependency needed various task like imagetovideo generation address limitation propose frameaware video diffusion modelfvdm introduces novel vectorized timestep variablevtv unlike conventional vdms approach allows frame follow independent noise schedule enhancing model capacity capture finegrained temporal dependency fvdms flexibility demonstrated across multiple task including standard video generation imagetovideo generation video interpolation long video synthesis diverse set vtv configuration achieve superior quality generated video overcoming challenge catastrophic forgetting finetuning limited generalizability zeroshot methodsour empirical evaluation show fvdm outperforms stateoftheart method video generation quality also excelling extended task addressing fundamental shortcoming existing vdms fvdm set new paradigm video synthesis offering robust framework significant implication generative modeling multimedia application
nvssolver video diffusion model zeroshot novel view synthesizer harnessing potent generative capability pretrained large video diffusion model propose nvssolver new novel view synthesis nv paradigm operates textitwithout need training nvssolver adaptively modulates diffusion sampling process given view enable creation remarkable visual experience single multiple view static scene monocular video dynamic scene specifically built upon theoretical modeling iteratively modulate score function given scene prior represented warped input view control video diffusion process moreover theoretically exploring boundary estimation error achieve modulation adaptive fashion according view pose number diffusion step extensive evaluation static dynamic scene substantiate significant superiority nvssolver stateoftheart method quantitatively qualitatively textit source code
zeroshot scene texturing video diffusion model mesh widely used computer vision graphic efficiency animation minimal memory use playing crucial role movie game ar vr however creating temporally consistent realistic texture mesh sequence remains laborintensive professional artist hand video diffusion model excel textdriven video generation often lack geometry awareness struggle achieving multiview consistent texturing mesh work present zeroshot approach integrates inherent geometry knowledge mesh sequence expressiveness video diffusion model produce multiview temporally consistent texture given untextured mesh sequence text prompt input method enhances multiview consistency synchronizing diffusion process across different view latent aggregation uv space ensure temporal consistency leverage prior knowledge conditional video generation model texture synthesis however straightforwardly combining video diffusion model uv texture aggregation lead blurry result analyze underlying cause propose simple yet effective modification ddim sampling process address issue additionally introduce reference latent texture strengthen correlation frame denoising process best knowledge first method specifically designed scene texturing extensive experiment demonstrate superiority producing multiview multiframe consistent video based untextured mesh sequence
video diffusion transformer incontext learner paper investigates solution enabling incontext capability video diffusion transformer minimal tuning required activation specifically propose simple pipeline leverage incontext generation textbfi concatenate video along spacial time dimension textbfii jointly caption multiscene video clip one source textbfiii apply taskspecific finetuning using carefully curated small datasets series diverse controllable task demonstrate qualitatively existing advanced texttovideo model effectively perform incontext generation notably allows creation consistent multiscene video exceeding second duration without additional computational overhead importantly method requires modification original model result highfidelity video output better align prompt specification maintain role consistency framework present valuable tool research community offer critical insight advancing productlevel controllable video generation system data code model weight publicly available httpsgithubcomfeizcvideoincontext
controllable panorama video generation video diffusion model panorama video recently attracts interest study application courtesy immersive experience due expensive cost capturing panoramic video generating desirable panorama video prompt urgently required lately emerging texttovideo diffusion method demonstrate notable effectiveness standard video generation however due significant gap content motion pattern panoramic standard video method encounter challenge yielding satisfactory panoramic video paper propose pipeline named video diffusion model generating panoramic video based given prompt motion condition specifically introduce lightweight accompanied enhancement technique transform pretrained model panorama video generation propose new panorama dataset named consisting panoramic videotext pair training addressing absence captioned panoramic video datasets extensive experiment demonstrate superiority effectiveness panorama video generation project page
divot diffusion power video tokenizer comprehension generation recent year significant surge interest unifying image comprehension generation within large language model llm growing interest prompted u explore extending unification video core challenge lie developing versatile video tokenizer capture spatial characteristic temporal dynamic video obtain representation llm representation decoded realistic video clip enable video generation work introduce divot diffusionpowered video tokenizer leverage diffusion process selfsupervised video representation learning posit video diffusion model effectively denoise video clip taking feature video tokenizer condition tokenizer successfully captured robust spatial temporal information additionally video diffusion model inherently function detokenizer decoding video representation building upon divot tokenizer present divotvicuna videototext autoregression texttovideo generation modeling distribution continuousvalued divot feature gaussian mixture model experimental result demonstrate diffusionbased video tokenizer integrated pretrained llm achieves competitive performance across various video comprehension generation benchmark instruction tuned divotvicuna also excels video storytelling generating interleaved narrative corresponding video
dimensionx create scene single image controllable video diffusion paper introduce textbfdimensionx framework designed generate photorealistic scene single image video diffusion approach begin insight spatial structure scene temporal evolution scene effectively represented sequence video frame recent video diffusion model shown remarkable success producing vivid visuals face limitation directly recovering scene due limited spatial temporal controllability generation overcome propose stdirector decouples spatial temporal factor video diffusion learning dimensionaware loras dimensionvariant data controllable video diffusion approach enables precise manipulation spatial structure temporal dynamic allowing u reconstruct representation sequential frame combination spatial temporal dimension additionally bridge gap generated video realworld scene introduce trajectoryaware mechanism generation identitypreserving denoising strategy generation extensive experiment various realworld synthetic datasets demonstrate dimensionx achieves superior result controllable video generation well scene generation compared previous method
osv one step enough highquality image video generation video diffusion model shown great potential generating highquality video making increasingly popular focus however inherent iterative nature lead substantial computational time cost effort made accelerate video diffusion reducing inference step technique like consistency distillation gan training approach often fall short either performance training stability work introduce twostage training framework effectively combine consistency distillation gan training address challenge additionally propose novel video discriminator design eliminates need decoding video latents improves final performance model capable producing highquality video merely onestep flexibility perform multistep refinement performance enhancement quantitative evaluation benchmark show model significantly outperforms existing method notably performancefvd exceeds performance consistency distillation based method animatelcm fvd approach performance advanced stable video diffusion fvd
lvcd referencebased lineart video colorization diffusion model propose first video diffusion framework referencebased lineart video colorization unlike previous work rely solely image generative model colorize lineart frame frame approach leverage largescale pretrained video diffusion model generate colorized animation video approach lead temporally consistent result better equipped handle large motion firstly introduce sketchguided controlnet provides additional control finetune imagetovideo diffusion model controllable video synthesis enabling generation animation video conditioned lineart propose reference attention facilitate transfer color reference frame frame containing fast expansive motion finally present novel scheme sequential sampling incorporating overlapped blending module prevreference attention extend video diffusion model beyond original fixedlength limitation long video colorization qualitative quantitative result demonstrate method significantly outperforms stateoftheart technique term frame video quality well temporal consistency moreover method capable generating highquality long temporalconsistent animation video large motion achievable previous work code model available httpsluckyhztgithubiolvcd
highresolution long video generation autonomous driving adaptive control rapid advancement diffusion model greatly improved video synthesis especially controllable video generation vital application like autonomous driving although dit vae become standard framework video generation introduces challenge controllable driving video generation especially geometry control rendering existing control method ineffective address issue propose novel approach integrates mvdit block spatialtemporal conditional encoding enable multiview video generation precise geometric control additionally introduce efficient method obtaining contextual description video support diverse textual control along progressive training strategy using mixed video data enhance training efficiency generalizability consequently enables multiview driving video synthesis resolution frame count compared current sota rich contextual control geometric control extensive experiment demonstrate ability unlocking broader application autonomous driving
vstar generative temporal nursing longer dynamic video synthesis despite tremendous progress field texttovideo synthesis opensourced diffusion model struggle generate longer video dynamically varying evolving content tend synthesize quasistatic video ignoring necessary visual changeovertime implied text prompt time scaling model enable longer dynamic video synthesis often remains computationally intractable address challenge introduce concept generative temporal nursing gtn aim alter generative process fly inference improve control temporal dynamic enable generation longer video propose method gtn dubbed vstar consists two key ingredient video synopsis prompting vsp automatic generation video synopsis based original single prompt leveraging llm give accurate textual guidance different visual state longer video temporal attention regularization tar regularization technique refine temporal attention unit pretrained diffusion model enables control video dynamic experimentally showcase superiority proposed approach generating longer visually appealing video existing opensourced model additionally analyze temporal attention map realized without vstar demonstrating importance applying method mitigate neglect desired visual change time
ctrlv higher fidelity video generation boundingbox controlled object motion controllable video generation attracted significant attention largely due advance video diffusion model domain autonomous driving essential develop highly accurate prediction object motion paper tackle crucial challenge exert precise control object motion realistic video synthesis accomplish control object movement using bounding box extend control rendering box pixel space employ distinct specialized model forecast trajectory object bounding box based previous desired future position adapt enhance separate video diffusion network create video content based high quality trajectory forecast method ctrlv leverage modified finetuned stable video diffusion svd model solve trajectory video generation extensive experiment conducted kitti virtualkitti nuscenes datasets validate effectiveness approach producing realistic controllable video generation
read watch scream sound generation text video despite impressive progress multimodal generative model videotoaudio generation still suffers limited performance limit flexibility prioritize sound synthesis specific object within scene conversely texttoaudio generation method generate highquality audio pose challenge ensuring comprehensive scene depiction timevarying control tackle challenge propose novel videoandtexttoaudio generation method called video serf conditional control texttoaudio generation model especially method estimate structural information sound namely energy video receiving key content cue user prompt employ wellperforming texttoaudio model consolidate video control much efficient training multimodal diffusion model massive tripletpaired audiovideotext data addition separating generative component audio becomes flexible system allows user freely adjust energy surrounding environment primary sound source according preference experimental result demonstrate method show superiority term quality controllability training efficiency code demo available httpsnaveraigithubiorewas
sfv single forward video generation model diffusionbased video generation model demonstrated remarkable success obtaining highfidelity video iterative denoising process however model require multiple denoising step sampling resulting high computational cost work propose novel approach obtain singlestep video generation model leveraging adversarial training finetune pretrained video diffusion model show adversarial training multisteps video diffusion model ie stable video diffusion svd trained perform single forward pas synthesize highquality video capturing temporal spatial dependency video data extensive experiment demonstrate method achieves competitive generation quality synthesized video significantly reduced computational overhead denoising process ie around speedup compared svd speedup compared existing work even better generation quality paving way realtime video synthesis editing visualization result made publicly available httpssnapresearchgithubiosfv
towards photorealistic scene generation via video diffusion model existing dynamic scene generation method mostly rely distilling knowledge pretrained generative model typically finetuned synthetic object datasets result generated scene often objectcentric lack photorealism address limitation introduce novel pipeline designed photorealistic scene generation discarding dependency multiview generative model instead fully utilizing video generative model trained diverse realworld datasets method begin generating reference video using video generation model learn canonical representation video using freezetime video delicately generated reference video handle inconsistency freezetime video jointly learn perframe deformation model imperfection learn temporal deformation based canonical representation capture dynamic interaction reference video pipeline facilitates generation dynamic scene enhanced photorealism structural integrity viewable multiple perspective thereby setting new standard scene generation
sned superposition network architecture search efficient video diffusion model aigenerated content garnered significant attention achieving photorealistic video synthesis remains formidable challenge despite promising advance diffusion model video generation quality complex model architecture substantial computational demand training inference create significant gap model realworld application paper present sned superposition network architecture search method efficient video diffusion model method employ supernet training paradigm target various model cost resolution option using weightsharing method moreover propose supernet training sampling warmup fast training optimization showcase flexibility method conduct experiment involving pixelspace latentspace video diffusion model result demonstrate framework consistently produce comparable result across different model option high efficiency according experiment pixelspace video diffusion model achieve consistent video generation result simultaneously across x x resolution large range model size number parameter pixelspace video diffusion model
videorepair improving texttovideo generation via misalignment evaluation localized refinement recent texttovideo diffusion model demonstrated impressive generation capability across various domain however model often generate video misalignment text prompt especially prompt describe complex scene multiple object attribute address introduce videorepair novel modelagnostic trainingfree video refinement framework automatically identifies finegrained textvideo misalignment generates explicit spatial textual feedback enabling diffusion model perform targeted localized refinement videorepair consists two stage video refinement planning first detect misalignment generating finegrained evaluation question answering using mllm based video evaluation output identify accurately generated object construct localized prompt precisely refine misaligned region localized refinement enhance video alignment repairing misaligned region original video preserving correctly generated area achieved framewise region decomposition using regionpreserving segmentation rps module two popular video generation benchmark evalcrafter videorepair substantially outperforms recent baseline across various textvideo alignment metric provide comprehensive analysis videorepair component qualitative example
lumiere spacetime diffusion model video generation introduce lumiere texttovideo diffusion model designed synthesizing video portray realistic diverse coherent motion pivotal challenge video synthesis end introduce spacetime unet architecture generates entire temporal duration video single pas model contrast existing video model synthesize distant keyframes followed temporal superresolution approach inherently make global temporal consistency difficult achieve deploying spatial importantly temporal upsampling leveraging pretrained texttoimage diffusion model model learns directly generate fullframerate lowresolution video processing multiple spacetime scale demonstrate stateoftheart texttovideo generation result show design easily facilitates wide range content creation task video editing application including imagetovideo video inpainting stylized generation
comparative analysis generative model enhancing image synthesis vaes gans stable diffusion paper examines three major generative modelling framework variational autoencoders vaes generative adversarial network gans stable diffusion model vaes effective learning latent representation frequently yield blurry result gans generate realistic image face issue mode collapse stable diffusion model producing highquality image strong semantic coherence demanding term computational resource additionally paper explores incorporating grounding dino grounded sam stable diffusion improves image accuracy utilising sophisticated segmentation inpainting technique analysis guide selecting suitable model various application highlight area research
searching prior make texttovideo synthesis better significant advancement video diffusion model brought substantial progress field texttovideo synthesis however existing synthesis model struggle accurately generate complex motion dynamic leading reduction video realism one possible solution collect massive data train model would extremely expensive alleviate problem paper reformulate typical generation process searchbased generation pipeline instead scaling model training employ existing video motion prior database specifically divide generation process two step given prompt input search existing textvideo datasets find video text label closely match prompt motion propose tailored search algorithm emphasizes object motion feature ii retrieved video processed distilled motion prior finetune pretrained base model followed generating desired video using input prompt utilizing prior gleaned searched video enhance realism generated video motion operation finished single nvidia rtx gpu validate method stateoftheart model across diverse prompt input code public
multimodal semantic communication generative audiodriven video conferencing paper study efficient multimodal data communication scheme video conferencing considered system speaker give talk audience talking head video audio transmitted since speaker frequently change posture highfidelity transmission audio speech music required redundant visual video data exists removed generating video audio end propose wavetovideo system efficient video transmission framework reduces transmitted data generating talking head video audio particular fullduration audio shortduration video data synchronously transmitted wireless channel neural network nns extracting encoding audio video semantics receiver combine decoded audio video data well us generative adversarial network gan based model generate lip movement video speaker simulation result show proposed system reduce amount transmitted data maintaining perceptual quality generated conferencing video
matten video generation mambaattention paper introduce matten cuttingedge latent diffusion model mambaattention architecture video generation minimal computational cost matten employ spatialtemporal attention local video content modeling bidirectional mamba global video content modeling comprehensive experimental evaluation demonstrates matten competitive performance current transformerbased ganbased model benchmark performance achieving superior fvd score efficiency additionally observe direct positive correlation complexity designed model improvement video quality indicating excellent scalability matten
realistic geometryaware transition compositional synthesis recent advance diffusion model demonstrated exceptional capability image video generation improving effectiveness synthesis existing generation method generate highquality object scene based userfriendly condition benefiting gaming video industry however method struggle synthesize significant object deformation complex transition interaction within scene address challenge propose novel synthesis framework enables realistic complex scene transition specifically first use multimodal large language model mllms produce physicaware scene description scene initialization effective transition timing planning propose geometryaware transition network realize complex scenelevel transition based plan involves expressive geometrical object deformation extensive experiment demonstrate consistently outperforms existing stateoftheart method generating scene accurate highquality transition validating effectiveness code
zerosmooth trainingfree diffuser adaptation high frame rate video generation video generation made remarkable progress recent year especially since advent video diffusion model many video generation model produce plausible synthetic video eg stable video diffusion svd however video model generate low frame rate video due limited gpu memory well difficulty modeling large set frame training video always uniformly sampled specified interval temporal compression previous method promote frame rate either training video interpolation model pixel space postprocessing stage training interpolation model latent space specific base video model paper propose trainingfree video interpolation method generative video diffusion model generalizable different model plugandplay manner investigate nonlinearity feature space video diffusion model transform video model selfcascaded video diffusion model incorporating designed hidden state correction module selfcascaded architecture correction module proposed retain temporal consistency key frame interpolated frame extensive evaluation preformed multiple popular video model demonstrate effectiveness propose method especially trainingfree method even comparable trained interpolation model supported huge compute resource largescale datasets
amg avatar motion guided video generation human video generation task gained significant attention advancement deep generative model generating realistic video human movement challenging nature due intricacy human body topology sensitivity visual artifact extensively studied medium generation method take advantage massive human medium datasets struggle control whereas avatarbased approach offering freedom control lack photorealism harmonized seamlessly background scene propose amg method combine photorealism controllability conditioning video diffusion model controlled rendering avatar additionally introduce novel data processing pipeline reconstructs render human avatar movement dynamic camera video amg first method enables multiperson diffusion video generation precise control camera position human motion background style also demonstrate extensive evaluation outperforms existing human video generation method conditioned pose sequence driving video term realism adaptability
hybrid video diffusion model triplane wavelet representation generating highquality video synthesize desired realistic content challenging task due intricate highdimensionality complexity video several recent diffusionbased method shown comparable performance compressing video lowerdimensional latent space using traditional video autoencoder architecture however method employ standard framewise convolution fail fully exploit spatiotemporal nature video address issue propose novel hybrid video diffusion model called hvdm capture spatiotemporal dependency effectively hvdm trained hybrid video autoencoder extract disentangled representation video including global context information captured projected latent ii local volume information captured convolution wavelet decomposition iii frequency information improving video reconstruction based disentangled representation hybrid autoencoder provide comprehensive video latent enriching generated video fine structure detail experiment video generation benchamarks skytimelapse taichi demonstrate proposed approach achieves stateoftheart video generation quality showing wide range video application eg long video generation imagetovideo video dynamic control
moviebench hierarchical movie level dataset long video generation recent advancement video generation model like stable video diffusion show promising result primarily focus short singlescene video model struggle generating long video involve multiple scene coherent narrative consistent character furthermore publicly available dataset tailored analysis evaluation training long video generation model paper present moviebench hierarchical movielevel dataset long video generation address challenge providing unique contribution movielength video featuring rich coherent storyline multiscene narrative consistency character appearance audio across scene hierarchical data structure contains highlevel movie information detailed shotlevel description experiment demonstrate moviebench brings new insight challenge maintaining character id consistency across multiple scene various character dataset public continuously maintained aiming advance field long video generation data found httpsweijiawugithubiomoviebench
dreamrunner finegrained compositional storytovideo generation retrievalaugmented motion adaptation storytelling video generation svg aim produce coherent visually rich multiscene video follow structured narrative existing method primarily employ llm highlevel planning decompose story scenelevel description independently generated stitched together however approach struggle generating highquality video aligned complex singlescene description visualizing complex description involves coherent composition multiple character event complex motion synthesis muticharacter customization address challenge propose dreamrunner novel storytovideo generation method first structure input script using large language model llm facilitate coarsegrained scene planning well finegrained objectlevel layout motion planning next dreamrunner present retrievalaugmented testtime adaptation capture target motion prior object scene supporting diverse motion customization based retrieved video thus facilitating generation new video complex scripted motion lastly propose novel spatialtemporal regionbased attention prior injection module finegrained objectmotion binding framebyframe semantic control compare dreamrunner various svg baseline demonstrating stateoftheart performance character consistency text alignment smooth transition additionally dreamrunner exhibit strong finegrained conditionfollowing ability compositional texttovideo generation significantly outperforming baseline finally validate dreamrunners robust ability generate multiobject interaction qualitative example
ditctrl exploring attention control multimodal diffusion transformer tuningfree multiprompt longer video generation soralike video generation model achieved remarkable progress multimodal diffusion transformer mmdit architecture however current video generation model predominantly focus singleprompt struggling generate coherent scene multiple sequential prompt better reflect realworld dynamic scenario pioneering work explored multiprompt video generation face significant challenge including strict training data requirement weak prompt following unnatural transition address problem propose ditctrl trainingfree multiprompt video generation method mmdit architecture first time key idea take multiprompt video generation task temporal video editing smooth transition achieve goal first analyze mmdits attention mechanism finding full attention behaves similarly crossselfattention block unetlike diffusion model enabling maskguided precise semantic control across different prompt attention sharing multiprompt video generation based careful design video generated ditctrl achieves smooth transition consistent object motion given multiple sequential prompt without additional training besides also present mpvbench new benchmark specially designed multiprompt video generation evaluate performance multiprompt generation extensive experiment demonstrate method achieves stateoftheart performance without additional training
novel multiview synthesis generation single image using latent video diffusion present stable video latent video diffusion model highresolution imagetomultiview generation orbital video around object recent work generation propose technique adapt generative model novel view synthesis nv optimization however method several disadvantage due either limited view inconsistent nv thereby affecting performance object generation work propose adapts imagetovideo diffusion model novel multiview synthesis generation thereby leveraging generalization multiview consistency video model adding explicit camera control nv also propose improved optimization technique use nv output generation extensive experimental result multiple datasets metric well user study demonstrate stateoftheart performance nv well reconstruction compared prior work
anything scene photorealistic video object insertion realistic video simulation shown significant potential across diverse application virtual reality film production particularly true scenario capturing video realworld setting either impractical expensive existing approach video simulation often fail accurately model lighting environment represent object geometry achieve high level photorealism paper propose anything scene novel generic framework realistic video simulation seamlessly insert object existing dynamic video strong emphasis physical realism proposed general framework encompasses three key process integrating realistic object given scene video proper placement ensure geometric realism estimating sky environmental lighting distribution simulating realistic shadow enhance light realism employing style transfer network refines final video output maximize photorealism experimentally demonstrate anything scene framework produce simulated video great geometric realism lighting realism photorealism significantly mitigating challenge associated video data generation framework offer efficient costeffective solution acquiring highquality video furthermore application extend well beyond video data augmentation showing promising potential virtual reality video editing various videocentric application please check project website httpsanythinginanyscenegithubio access project code highresolution video result
motioncraft physicsbased zeroshot video generation generating video realistic physically plausible motion one main recent challenge computer vision diffusion model achieving compelling result image generation video diffusion model limited heavy training huge model resulting video still biased training dataset work propose motioncraft new zeroshot video generator craft physicsbased realistic video motioncraft able warp noise latent space image diffusion model stable diffusion applying optical flow derived physic simulation show warping noise latent space result coherent application desired motion allowing model generate missing element consistent scene evolution would otherwise result artefact missing content flow applied pixel space compare method stateoftheart reporting qualitative quantitative improvement demonstrating effectiveness approach generate video finelyprescribed complex motion dynamic project page httpsmezzelfogithubiomotioncraft
comuni decomposing common unique video signal diffusionbased video generation since video record object moving coherently adjacent video frame commonness similar object appearance uniqueness slightly changed posture prevent redundant modeling common video signal propose novel diffusionbased framework named comuni decomposes common unique video signal enable efficient video generation approach separate decomposition video signal task video generation thus reducing computation complexity generative model particular introduce cuvae decompose video signal encode latent feature train cuvae selfsupervised manner employ cascading merge module reconstitute video signal timeagnostic video decoder reconstruct video frame propose culdm model latent feature video generation adopts two specific diffusion stream simultaneously model common unique latent feature utilize additional joint module cross modeling common unique latent feature novel position embedding method ensure content consistency motion coherence generated video position embedding method incorporates spatial temporal absolute position information joint module extensive experiment demonstrate necessity decomposing common unique video signal video generation effectiveness efficiency proposed method
aligned monocular depth estimation dynamic video recent development monocular depth estimation method enable highquality depth estimation singleview image fail estimate consistent video depth across different frame recent work address problem applying video diffusion model generate video depth conditioned input video trainingexpensive produce scaleinvariant depth value without camera pose paper propose novel videodepth estimation method called estimate temporal consistent depth map dynamic video key idea utilize recent model align estimated monocular depth map different timesteps first finetune model additional estimated monocular depth input dynamic scene apply optimization reconstruct depth map camera pose extensive experiment demonstrate estimate consistent video depth camera pose monocular video superior performance baseline method
msg score comprehensive evaluation multiscene video generation paper address metric required generating multiscene video based continuous scenario opposed traditional short video generation scenariobased video require comprehensive evaluation considers multiple factor character consistency artistic coherence aesthetic quality alignment generated content intended prompt additionally video generation unlike single image movement character across frame introduces potential issue like distortion unintended change must effectively evaluated corrected context probabilistic model like diffusion generating desired scene requires repeated sampling manual selection akin film director chooses best shot numerous take propose scorebased evaluation benchmark automates process enabling objective efficient assessment complexity approach allows generation highquality multiscene video selecting best outcome based automated scoring rather manual inspection
continuous video process modeling video continuous multidimensional process video prediction diffusion model made significant stride image generation mastering task unconditional image synthesis textimage translation imagetoimage conversion however capability fall short realm video prediction mainly treat video collection independent image relying external constraint temporal attention mechanism enforce temporal coherence paper introduce novel model class treat video continuous multidimensional process rather series discrete frame also report reduction sampling step required sample new frame thus making framework efficient inference time extensive experimentation establish stateoftheart performance video prediction validated benchmark datasets including kth bair navigate project page httpswwwcsumdedugauravshcvpsuppwebsitehtml video result
streetcrafter street view synthesis controllable video diffusion model paper aim tackle problem photorealistic view synthesis vehicle sensor data recent advancement neural scene representation achieved notable success rendering highquality autonomous driving scene performance significantly degrades viewpoint deviate training trajectory mitigate problem introduce streetcrafter novel controllable video diffusion model utilizes lidar point cloud rendering pixellevel condition fully exploit generative prior novel view synthesis preserving precise camera control moreover utilization pixellevel lidar condition allows u make accurate pixellevel edits target scene addition generative prior streetcrafter effectively incorporated dynamic scene representation achieve realtime rendering experiment waymo open dataset pandaset demonstrate model enables flexible control viewpoint change enlarging view synthesis region satisfying rendering outperforms existing method
spatialme stereo video conversion using depthwarping blendinpainting stereo video conversion aim transform monocular video immersive stereo format despite advancement novel view synthesis still remains two major challenge difficulty achieving highfidelity stable result ii insufficiency highquality stereo video data paper introduce spatialme novel stereo video conversion framework based depthwarping blendinpainting specifically propose maskbased hierarchy feature update mhfu refiner integrate refine output designed multibranch inpainting module using feature update unit fuu mask mechanism also propose disparity expansion strategy address problem foreground bleeding furthermore conduct highquality realworld stereo video dataset alleviate data shortage contains stereo video captured realworld resolution x covering various indoor outdoor scene extensive experiment demonstrate superiority approach generating stereo video stateoftheart method
hyperglm hypergraph video scene graph generation anticipation multimodal llm advanced visionlanguage task still struggle understanding video scene bridge gap video scene graph generation vidsgg emerged capture multiobject relationship across video frame however prior method rely pairwise connection limiting ability handle complex multiobject interaction reasoning end propose multimodal llm scene hypergraph hyperglm promoting reasoning multiway interaction higherorder relationship approach uniquely integrates entity scene graph capture spatial relationship object procedural graph model causal transition forming unified hypergraph significantly hyperglm enables reasoning injecting unified hypergraph llm additionally introduce new video scene graph reasoning vsgr dataset featuring frame thirdperson egocentric drone view support five task scene graph generation scene graph anticipation video question answering video captioning relation reasoning empirically hyperglm consistently outperforms stateoftheart method across five task effectively modeling reasoning complex relationship diverse video scene
visage video synthesis using action graph surgery surgical data science sd field analyzes patient data surgery improve surgical outcome skill however surgical data scarce heterogeneous complex limit applicability existing machine learning method work introduce novel task future video generation laparoscopic surgery task augment enrich existing surgical data enable various application simulation analysis robotaided surgery ultimately involves understanding current state operation also accurately predicting dynamic often unpredictable nature surgical procedure proposed method visage video synthesis using action graph surgery leverage power action scene graph capture sequential nature laparoscopic procedure utilizes diffusion model synthesize temporally coherent video sequence visage predicts future frame given single initial frame action graph triplet incorporating domainspecific knowledge action graph visage ensures generated video adhere expected visual motion pattern observed real laparoscopic procedure result experiment demonstrate highfidelity video generation laparoscopy procedure enables various application sd
cfsynthesis controllable freeview human video synthesis human video synthesis aim create lifelike character various environment wide application vr storytelling content creation diffusionbased method made significant progress struggle generalize complex pose varying scene background address limitation introduce cfsynthesis novel framework generating highquality human video customizable attribute including identity motion scene configuration method leverage texturesmplbased representation ensure consistent stable character appearance across free viewpoint additionally introduce novel foregroundbackground separation strategy effectively decomposes scene foreground background enabling seamless integration userdefined background experimental result multiple datasets show cfsynthesis achieves stateoftheart performance complex human animation also adapts effectively motion freeview userspecified scenario
compressing scene dynamic generative approach paper proposes learn generative prior motion pattern instead video content generative video compression prior derived small motion dynamic common scene swinging tree wind floating boat sea utilizing compact motion prior novel generative scene dynamic compression framework built realize ultralow bitrate communication highquality reconstruction diverse scene content encoder side motion prior characterized compact representation densetosparse manner decoder side decoded motion prior serve trajectory hint scene dynamic reconstruction via diffusionbased flowdriven generator experimental result illustrate proposed method achieve superior ratedistortion performance outperform stateoftheart conventional video codec versatile video coding vvc scene dynamic sequence project page found httpsgithubcomxyzyszgnvdc
dreamforge motionaware autoregressive video generation multiview driving scene recent advance diffusion model improved controllable streetscape generation supported downstream perception planning task however challenge remain accurately modeling driving scene generating long video alleviate issue propose dreamforge advanced diffusionbased autoregressive video generation model tailored longterm generation enhance lane foreground generation introduce perspective guidance integrate objectwise position encoding incorporate local correlation improve foreground object modeling also propose motionaware temporal attention capture motion cue appearance change video leveraging motion frame autoregressive generation paradigmwe autoregressively generate long video frame using model trained short sequence achieving superior quality compared baseline video evaluation finally integrate method realistic simulator drivearena provide reliable openloop closedloop evaluation visionbased driving agent project page httpspjlabadggithubiodrivearenadreamforge
viewcrafter taming video diffusion model highfidelity novel view synthesis despite recent advancement neural reconstruction dependence dense multiview capture restricts broader applicability work propose textbfviewcrafter novel method synthesizing highfidelity novel view generic scene single sparse image prior video diffusion model method take advantage powerful generation capability video diffusion model coarse clue offered pointbased representation generate highquality video frame precise camera pose control enlarge generation range novel view tailored iterative view synthesis strategy together camera trajectory planning algorithm progressively extend clue area covered novel view viewcrafter facilitate various application immersive experience realtime rendering efficiently optimizing representation using reconstructed point generated novel view scenelevel generation imaginative content creation extensive experiment diverse datasets demonstrate strong generalization capability superior performance method synthesizing highfidelity consistent novel view
motion consistency model accelerating video diffusion disentangled motionappearance distillation image diffusion distillation achieves highfidelity generation sampling step however applying technique directly video diffusion often result unsatisfactory frame quality due limited visual quality public video datasets affect performance teacher student video diffusion model study aim improve video diffusion distillation improving frame appearance using abundant highquality image data propose motion consistency model mcm singlestage video diffusion distillation method disentangles motion appearance learning specifically mcm includes video consistency model distills motion video teacher model image discriminator enhances frame appearance match highquality image data combination present two challenge conflicting frame learning objective video distillation learns lowquality video frame image discriminator target highquality image traininginference discrepancy due differing quality video sample used training inference address challenge introduce disentangled motion distillation mixed trajectory distillation former applies distillation objective solely motion representation latter mitigates traininginference discrepancy mixing distillation trajectory low highquality video domain extensive experiment show mcm achieves stateoftheart video diffusion distillation performance additionally method enhance frame quality video diffusion model producing frame high aesthetic score specific style without corresponding video data
fashionvdm video diffusion model virtual tryon present fashionvdm video diffusion model vdm generating virtual tryon video given input garment image person video method aim generate highquality tryon video person wearing given garment preserving person identity motion imagebased virtual tryon shown impressive result however existing video virtual tryon vvt method still lacking garment detail temporal consistency address issue propose diffusionbased architecture video virtual tryon split classifierfree guidance increased control conditioning input progressive temporal training strategy singlepass video generation also demonstrate effectiveness joint imagevideo training video tryon especially video data limited qualitative quantitative experiment show approach set new stateoftheart video virtual tryon additional result visit project page httpsjohannakarrasgithubiofashionvdm
tango cospeech gesture video reenactment hierarchical audio motion embedding diffusion interpolation present tango framework generating cospeech bodygesture video given fewminute singlespeaker reference video target speech audio tango produce highfidelity video synchronized body gesture tango build gesture video reenactment gvr split retrieves video clip using directed graph structure representing video frame node valid transition edge address two key limitation gvr audiomotion misalignment visual artifact gangenerated transition frame particular propose retrieving gesture using latent feature distance improve crossmodal alignment ensure latent feature could effectively model relationship speech audio gesture motion implement hierarchical joint embedding space aumoclip ii introduce diffusionbased model generate highquality transition frame diffusion model appearance consistent interpolation acinterp built upon animateanyone includes reference motion module homography background flow preserve appearance consistency generated reference video integrating component graphbased retrieval framework tango reliably produce realistic audiosynchronized video outperforms existing generative retrieval method code pretrained model available urlhttpspantomatrixgithubiotango
dynamic multiobject scene generation monocular video viewpredictive generative model provide strong prior lifting objectcentric image video rendering score distillation objective question remains lifting complete multiobject dynamic scene two challenge direction first rendering error gradient often insufficient recover fast object motion second view predictive generative model work much better object whole scene score distillation objective currently applied scene level directly present first approach generate dynamic scene multiple object monocular video via novel view synthesis key insight decomposerecompose approach factorizes video scene background object track also factorizing object motion component objectcentric deformation objecttoworldframe transformation camera motion decomposition permit rendering error gradient object viewpredictive model recover object completion deformation bounding box track guide large object movement scene show extensive result challenging davis kubric selfcaptured video quantitative comparison user preference study besides scene generation obtains accurate persistent point track projecting inferred trajectory release code hope work stimulate research finegrained understanding video
slow bidirectional fast autoregressive video diffusion model current video diffusion model achieve impressive generation quality struggle interactive application due bidirectional attention dependency generation single frame requires model process entire sequence including future address limitation adapting pretrained bidirectional diffusion transformer autoregressive transformer generates frame onthefly reduce latency extend distribution matching distillation dmd video distilling diffusion model generator enable stable highquality distillation introduce student initialization scheme based teacher ode trajectory well asymmetric distillation strategy supervises causal student model bidirectional teacher approach effectively mitigates error accumulation autoregressive generation allowing longduration video synthesis despite training short clip model achieves total score vbenchlong benchmark surpassing previous video generation model enables fast streaming generation highquality video fps single gpu thanks kv caching approach also enables streaming videotovideo translation imagetovideo dynamic prompting zeroshot manner release code based opensource model future
motioncom automatic motionaware image composition llm video diffusion prior work present motioncom trainingfree motionaware diffusion based image composition enabling automatic seamless integration target object new scene dynamically coherent result without finetuning optimization traditional approach area suffer two significant limitation require manual planning object placement often generate static composition lacking motion realism motioncom address issue utilizing large vision language model lvlm intelligent planning video diffusion prior motioninfused image synthesis streamlining composition process multimodal chainofthought cot prompting lvlm automates strategic placement planning foreground object considering potential motion interaction within scene complementing propose novel method motionpaint distill motionaware information pretrained video diffusion model generation phase ensuring object seamlessly integrated also endowed realistic motion extensive quantitative qualitative result highlight motioncoms superiority showcasing efficiency streamlining planning process capability produce composition authentically depict motion interaction
immersive video generation perspective anchor video offer hyperimmersive experience allows viewer explore dynamic scene full degree achieve userfriendly personalized content creation video format seek lift standard perspective video equirectangular video end introduce first video generation framework creates highquality video rich diverse motion pattern video anchor learns finegrained spherical visual motion pattern limited video data several key design firstly adopt dualbranch design including perspective panorama video denoising branch provide local global constraint video generation motion module spatial lora layer finetuned extended web video additionally antipodal mask devised capture longrange motion dependency enhancing reversed camera motion antipodal pixel across hemisphere handle diverse perspective video input propose elevationaware design adapt varying video masking due changing elevation across frame extensive experiment show achieves superior graphic quality motion coherence among stateoftheart video generation method believe hold promise advancing personalized immersive video creation
gamegenx interactive openworld game video generation introduce gamegenx first diffusion transformer model specifically designed generating interactively controlling openworld game video model facilitates highquality opendomain generation simulating extensive array game engine feature innovative character dynamic environment complex action diverse event additionally provides interactive controllability predicting altering future content based current clip thus allowing gameplay simulation realize vision first collected built openworld video game dataset scratch first largest dataset openworld game video generation control comprises million diverse gameplay video clip sampling game informative caption gamegenx undergoes twostage training process consisting foundation model pretraining instruction tuning firstly model pretrained via texttovideo generation video continuation endowing capability longsequence highquality opendomain game video generation achieve interactive controllability designed instructnet incorporate gamerelated multimodal control signal expert allows model adjust latent representation based user input unifying character interaction scene content control first time video generation instruction tuning instructnet updated pretrained foundation model frozen enabling integration interactive controllability without loss diversity quality generated video content
venhancer generative spacetime enhancement video generation present venhancer generative spacetime enhancement framework improves existing texttovideo result adding detail spatial domain synthetic detailed motion temporal domain given generated lowquality video approach increase spatial temporal resolution simultaneously arbitrary upsampling space time scale unified video diffusion model furthermore venhancer effectively remove generated spatial artifact temporal flickering generated video achieve basing pretrained video diffusion model train video controlnet inject diffusion model condition low framerate lowresolution video effectively train video controlnet design spacetime data augmentation well videoaware conditioning benefiting design venhancer yield stable training share elegant endtoend training manner extensive experiment show venhancer surpasses existing stateoftheart video superresolution spacetime superresolution method enhancing aigenerated video moreover venhancer exisiting opensource stateoftheart texttovideo method reach top one video generation benchmark vbench
towards chunkwise generation long video generating longduration video always significant challenge due inherent complexity spatiotemporal domain substantial gpu memory demand required calculate huge size tensor diffusion based generative model achieve stateoftheart performance video generation task typically trained predefined video resolution length inference noise tensor specific resolution length specified first model perform denoising entire video tensor simultaneously frame together approach easily raise outofmemory oom problem specified resolution andor length exceed certain limit one solution problem generate many short video chunk autoregressively strong interchunk spatiotemporal relation concatenate together form long video approach long video generation task divided multiple short video generation subtasks cost subtask reduced feasible level paper conduct detailed survey long video generation autoregressive chunkbychunk strategy address common problem caused applying short imagetovideo model long video task design efficient kstep search solution mitigate problem
representing long volumetric video temporal gaussian hierarchy paper aim address challenge reconstructing long volumetric video multiview rgb video recent dynamic view synthesis method leverage powerful representation like feature grid point cloud sequence achieve highquality rendering result however typically limited short video clip often suffer large memory footprint dealing longer video solve issue propose novel representation named temporal gaussian hierarchy compactly model long volumetric video key observation generally various degree temporal redundancy dynamic scene consist area changing different speed motivated approach build multilevel hierarchy gaussian primitive level separately describes scene region different degree content change adaptively share gaussian primitive represent unchanged scene content different temporal segment thus effectively reducing number gaussian primitive addition treelike structure gaussian hierarchy allows u efficiently represent scene particular moment subset gaussian primitive leading nearly constant gpu memory usage training rendering regardless video length extensive experimental result demonstrate superiority method alternative method term training cost rendering speed storage usage knowledge work first approach capable efficiently handling minute volumetric video data maintaining stateoftheart rendering quality project page available
triergon finegrained videotoaudio generation multimodal condition lufs control videotoaudio generation utilizes visualonly video feature produce realistic sound correspond scene however current model often lack finegrained control generated audio especially term loudness variation incorporation multimodal condition overcome limitation introduce triergon diffusionbased model incorporates textual auditory pixellevel visual prompt enable detailed semantically rich audio synthesis additionally introduce loudness unit relative full scale lufs embedding allows precise manual control loudness change time individual audio channel enabling model effectively address intricate correlation video audio realworld foley workflow triergon capable creating khz highfidelity stereo audio clip varying length second significantly outperforms existing stateoftheart method typically generate mono audio fixed duration
talc timealigned caption multiscene texttovideo generation texttovideo generative model often produce singlescene video clip depict entity performing particular action eg red panda climbing tree however pertinent generate multiscene video since ubiquitous realworld eg red panda climbing tree followed red panda sleep top tree generate multiscene video pretrained model introduce simple effective timealigned caption talc framework specifically enhance textconditioning mechanism architecture recognize temporal alignment video scene scene description instance condition visual feature earlier later scene generated video representation first scene description eg red panda climbing tree second scene description eg red panda sleep top tree respectively result show model generate multiscene video adhere multiscene text description visually consistent eg entity background finetune pretrained model multiscene videotext data using talc framework show talcfinetuned model outperforms baseline achieving relative gain overall score average visual consistency text adherence using human evaluation
video recap recursive captioning hourlong video video captioning model designed process short video clip second output text describing lowlevel visual concept eg object scene atomic action however realworld video last minute hour complex hierarchical structure spanning different temporal granularity propose video recap recursive video captioning model process video input dramatically different length second hour output video caption multiple hierarchy level recursive videolanguage architecture exploit synergy different video hierarchy process hourlong video efficiently utilize curriculum learning training scheme learn hierarchical structure video starting cliplevel caption describing atomic action focusing segmentlevel description concluding generating summary hourlong video furthermore introduce dataset augmenting manually collected longrange video summary recursive model flexibly generate caption different hierarchy level also useful complex video understanding task videoqa egoschema data code model available httpssitesgooglecomviewvidrecap
diffted oneshot audiodriven ted talk video generation diffusionbased cospeech gesture audiodriven talking video generation advanced significantly existing method often depend videotovideo translation technique traditional generative network like gans typically generate taking head cospeech gesture separately leading less coherent output furthermore gesture produced method often appear overly smooth subdued lacking diversity many gesturecentric approach integrate talking head generation address limitation introduce diffted new approach oneshot audiodriven tedstyle talking video generation single image specifically leverage diffusion model generate sequence keypoints thinplate spline motion model precisely controlling avatar animation ensuring temporally coherent diverse gesture innovative approach utilizes classifierfree guidance empowering gesture flow naturally audio input without relying pretrained classifier experiment demonstrate diffted generates temporally coherent talking video diverse cospeech gesture
latte latent diffusion transformer video generation propose novel latent diffusion transformer namely latte video generation latte first extract spatiotemporal token input video adopts series transformer block model video distribution latent space order model substantial number token extracted video four efficient variant introduced perspective decomposing spatial temporal dimension input video improve quality generated video determine best practice latte rigorous experimental analysis including video clip patch embedding model variant timestepclass information injection temporal positional embedding learning strategy comprehensive evaluation demonstrates latte achieves stateoftheart performance across four standard video generation datasets ie faceforensics skytimelapse taichihd addition extend latte texttovideo generation task latte achieves comparable result compared recent model strongly believe latte provides valuable insight future research incorporating transformer diffusion model video generation
vivid video virtual tryon using diffusion model video virtual tryon aim transfer clothing item onto video target person directly applying technique imagebased tryon video domain framewise manner cause temporalinconsistent outcome previous videobased tryon solution generate low visual quality blurring result work present vivid novel framework employing powerful diffusion model tackle task video virtual tryon specifically design garment encoder extract finegrained clothing semantic feature guiding model capture garment detail inject target video proposed attention feature fusion mechanism ensure spatialtemporal consistency introduce lightweight pose encoder encode pose signal enabling model learn interaction clothing human posture insert hierarchical temporal module texttoimage stable diffusion model coherent lifelike video synthesis furthermore collect new dataset largest diverse type garment highest resolution task video virtual tryon date extensive experiment demonstrate approach able yield satisfactory video tryon result dataset code weight publicly available project page
synthesis synchronized sound effect temporal semantic control sound designer foley artist usually sonorize scene movie video game manually annotating sonorizing action interest video case intent leave full creative control sound designer tool allows bypass repetitive part work thus able focus creative aspect sound production achieve presenting twostage model consisting rmsmapper estimate envelope representative audio characteristic associated input video stablefoley diffusion model based stable audio open generates audio semantically temporally aligned target video temporal alignment guaranteed use envelope controlnet input semantic alignment achieved use sound representation chosen designer crossattention conditioning diffusion process train test model greatest hit dataset commonly used evaluate model addition test model case study interest introduce walking map dataset video extracted video game depicting animated character walking different location sample code available demo page
one prompt scene generation via videoassisted consistencyenhanced mae artificial intelligence generated content aigc advance variety method developed generate text image video object single multimodal input contributing effort emulate humanlike cognitive content creation however generating realistic largescale scene single input present challenge due complexity involved ensuring consistency across extrapolated view generated model benefiting recent video generation model implicit neural representation propose scene generation model ensures realism diversity video generation framework also us implicit neural field combined masked autoencoders mae effectively ensures consistency unseen area across view specifically initially warp input image image generated text simulate adjacent view filling invisible area mae model however filled image usually fail maintain view consistency thus utilize produced view optimize neural radiance field enhancing geometric consistency moreover enhance detail texture fidelity generated view employ ganbased loss image derived input image video generation model extensive experiment demonstrate method generate realistic consistent scene single prompt qualitative quantitative result indicate approach surpasses existing stateoftheart method show encourage video example
vlasik consistent glassesremoval video using synthetic data diffusionbased generative model recently shown remarkable image video editing capability however local video editing particularly removal small attribute like glass remains challenge existing method either alter video excessively generate unrealistic artifact fail perform requested edit consistently throughout video work focus consistent identitypreserving removal glass video using case study consistent local attribute removal video due lack paired data adopt weakly supervised approach generate synthetic imperfect data using adjusted pretrained diffusion model show despite data imperfection learning generated data leveraging prior pretrained diffusion model model able perform desired edit consistently preserving original video content furthermore exemplify generalization ability method local video editing task applying successfully facial stickerremoval approach demonstrates significant improvement existing method showcasing potential leveraging synthetic data strong video prior local video editing task
video creation demonstration explore novel video creation experience namely video creation demonstration given demonstration video context image different scene generate physically plausible video continues naturally context image carry action concept demonstration enable capability present deltadiffusion selfsupervised training approach learns unlabeled video conditional future frame prediction unlike existing video generation control based explicit signal adopts form implicit latent control maximal flexibility expressiveness required general video leveraging video foundation model appearance bottleneck design top extract action latents demonstration video conditioning generation process minimal appearance leakage empirically deltadiffusion outperforms related baseline term human preference largescale machine evaluation demonstrates potential towards interactive world simulation sampled video generation result available httpsdeltadiffusiongithubio
egosonics generating synchronized audio silent egocentric video introduce egosonics method generate semantically meaningful synchronized audio track conditioned silent egocentric video generating audio silent egocentric video could open new application virtual reality assistive technology augmenting existing datasets existing work limited domain like speech music impact sound capture broad range audio frequency found egocentric video egosonics address limitation building strength latent diffusion model conditioned audio synthesis first encode process paired audiovideo data make suitable generation encoded data used train model generate audio track capture semantics input video proposed syncronet build top controlnet provide control signal enables generation temporally synchronized audio extensive evaluation comprehensive user study show model outperforms existing work audio quality proposed synchronization evaluation method furthermore demonstrate downstream application model improving video summarization
mvoc trainingfree multiple video object composition method diffusion model video composition core task video editing although image composition based diffusion model highly successful straightforward extend achievement video object composition task exhibit corresponding interaction effect also ensure object composited video maintain motion identity consistency necessary composite physical harmony video address challenge propose multiple video object composition mvoc method based diffusion model specifically first perform ddim inversion video object obtain corresponding noise feature secondly combine edit object image editing method obtain first frame composited video finally use imagetovideo generation model composite video feature attention injection video object dependence module trainingfree conditional guidance operation video generation enables coordination feature attention map various object nonindependent composited video final generative model constrains object generated video consistent original object motion identity also introduces interaction effect object extensive experiment demonstrated proposed method outperforms existing stateoftheart approach project page httpssobeymilgithubiomvoccom
leveraging compressed frame size ultrafast video classification classifying video distinct category sport music video crucial multimedia understanding retrieval especially immense volume video content constantly generated traditional method require video decompression extract pixellevel feature like color texture motion thereby increasing computational storage demand moreover method often suffer performance degradation lowquality video present novel approach examines postcompression bitstream video perform classification eliminating need bitstream decoding validate approach built comprehensive data set comprising youtube video clip totaling hour spanning distinct category evaluation indicate precision accuracy recall rate consistently many exceeding reaching algorithm operates approximately time faster realtime video outperforming traditional dynamic time warping dtw algorithm seven order magnitude
worlddreamer towards general world model video generation via predicting masked token world model play crucial role understanding predicting dynamic world essential video generation however existing world model confined specific scenario gaming driving limiting ability capture complexity general world dynamic environment therefore introduce worlddreamer pioneering world model foster comprehensive comprehension general world physic motion significantly enhances capability video generation drawing inspiration success large language model worlddreamer frame world modeling unsupervised visual sequence modeling challenge achieved mapping visual input discrete token predicting masked one process incorporate multimodal prompt facilitate interaction within world model experiment show worlddreamer excels generating video across different scenario including natural scene driving environment worlddreamer showcase versatility executing task texttovideo conversion imagetovideo synthesis video editing result underscore worlddreamers effectiveness capturing dynamic element within diverse general world environment
reconx reconstruct scene sparse view video diffusion model advancement scene reconstruction transformed image real world model producing realistic result hundred input photo despite great success denseview reconstruction scenario rendering detailed scene insufficient captured view still illposed optimization problem often resulting artifact distortion unseen area paper propose reconx novel scene reconstruction paradigm reframes ambiguous reconstruction challenge temporal generation task key insight unleash strong generative prior large pretrained video diffusion model sparseview reconstruction however view consistency struggle accurately preserved directly generated video frame pretrained model address given limited input view proposed reconx first construct global point cloud encodes contextual space structure condition guided condition video diffusion model synthesizes video frame detailpreserved exhibit high degree consistency ensuring coherence scene various perspective finally recover scene generated video confidenceaware gaussian splatting optimization scheme extensive experiment various realworld datasets show superiority reconx stateoftheart method term quality generalizability
enhancing unbounded gaussian splatting viewconsistent diffusion prior novelview synthesis aim generate novel view scene multiple input image video recent advancement like gaussian splatting achieved notable success producing photorealistic rendering efficient pipeline however generating highquality novel view challenging setting sparse input view remains difficult due insufficient information undersampled area often resulting noticeable artifact paper present novel pipeline enhancing representation quality representation leverage video diffusion prior address challenging view consistency problem reformulating achieving temporal consistency within video generation process restores viewconsistent latent feature rendered novel view integrates input view spatialtemporal decoder enhanced view used finetune initial model significantly improving rendering performance extensive experiment largescale datasets unbounded scene demonstrate yield superior reconstruction performance highfidelity rendering result compared stateoftheart method project webpage
unianimate taming unified video diffusion model consistent human image animation recent diffusionbased human image animation technique demonstrated impressive success synthesizing video faithfully follow given reference identity sequence desired movement pose despite still two limitation extra reference model required align identity image main video branch significantly increase optimization burden model parameter ii generated video usually short time eg frame hampering practical application address shortcoming present unianimate framework enable efficient longterm human video generation first reduce optimization difficulty ensure temporal coherence map reference image along posture guidance noise video common feature space incorporating unified video diffusion model second propose unified noise input support random noised input well first frame conditioned input enhances ability generate longterm video finally efficiently handle long sequence explore alternative temporal modeling architecture based state space model replace original computationconsuming temporal transformer extensive experimental result indicate unianimate achieves superior synthesis result existing stateoftheart counterpart quantitative qualitative evaluation notably unianimate even generate highly consistent oneminute video iteratively employing first frame conditioning strategy code model publicly available project page httpsunianimategithubio
surgen textguided diffusion model surgical video generation diffusionbased video generation model made significant stride producing output improved visual fidelity temporal coherence user control advancement hold great promise improving surgical education enabling realistic diverse interactive simulation environment study introduce surgen textguided diffusion model tailored surgical video synthesis surgen produce video highest resolution longest duration among existing surgical video generation model validate visual temporal quality output using standard image video generation metric additionally assess alignment corresponding text prompt deep learning classifier trained surgical data result demonstrate potential diffusion model serve valuable educational tool surgical trainee
investigating effectiveness crossattention unlock zeroshot editing texttovideo diffusion model recent advance image video diffusion model content creation plethora technique proposed customizing generated content particular manipulating crossattention layer texttoimage diffusion model shown great promise controlling shape location object scene transferring imageediting technique video domain however extremely challenging object motion temporal consistency difficult capture accurately work take first look role crossattention texttovideo diffusion model zeroshot video editing oneshot model shown potential controlling motion camera movement demonstrate zeroshot control object shape position movement model show despite limitation current model crossattention guidance promising approach editing video
teaching video diffusion model track point improves video generation recent foundational video generator produce visually rich output still struggle appearance drift object gradually degrade change inconsistently across frame breaking visual coherence hypothesize explicit supervision term spatial tracking feature level propose spatially aware video generator combine video diffusion loss point tracking across frame providing enhanced spatial supervision diffusion feature merges video generation point tracking task single network making minimal change existing video generation architecture using stable video diffusion backbone demonstrates possible unify video generation point tracking typically handled separate task extensive evaluation show effectively reduces appearance drift resulting temporally stable visually coherent video generation project page
slicedit zeroshot video editing texttoimage diffusion model using spatiotemporal slice texttoimage diffusion model achieve stateoftheart result image synthesis editing however leveraging pretrained model video editing considered major challenge many existing work attempt enforce temporal consistency edited video explicit correspondence mechanism either pixel space deep feature method however struggle strong nonrigid motion paper introduce fundamentally different approach based observation spatiotemporal slice natural video exhibit similar characteristic natural image thus diffusion model normally used prior video frame also serve strong prior enhancing temporal consistency applying spatiotemporal slice based observation present slicedit method textbased video editing utilizes pretrained diffusion model process spatial spatiotemporal slice method generates video retain structure motion original video adhering target text extensive experiment demonstrate slicedits ability edit wide range realworld video confirming clear advantage compared existing competing method webpage httpsmatankleinergithubioslicedit
scene copilot procedural text video generation human loop video generation achieved impressive quality still suffers artifact temporal inconsistency violation physical law leveraging scene fundamentally resolve issue providing precise control scene entity facilitate easy generation diverse photorealistic scene propose scene copilot framework combining large language model llm procedural scene generator specifically scene copilot consists scene codex blendergpt human loop scene codex designed translate textual user input command understandable scene generator blendergpt provides user intuitive direct way precisely control generated scene final output video furthermore user utilize blender ui receive instant visual feedback additionally curated procedural dataset object code format enhance system capability component work seamlessly together support user generating desired scene extensive experiment demonstrate capability framework customizing scene video generation
animate motion turning still image dynamic video recent year diffusion model made remarkable stride texttovideo generation sparking quest enhanced control video output accurately reflect user intention traditional effort predominantly focus employing either semantic cue like image depth map motionbased condition like moving sketch object bounding box semantic input offer rich scene context lack detailed motion specificity conversely motion input provide precise trajectory information miss broader semantic narrative first time integrate semantic motion cue within diffusion model video generation demonstrated fig end introduce scene motion conditional diffusion smcd novel methodology managing multimodal input incorporates recognized motion conditioning module investigates various approach integrate scene condition promoting synergy different modality model training separate condition two modality introducing twostage training pipeline experimental result demonstrate design significantly enhances video quality motion precision semantic coherence
vividdream generating scene ambient dynamic introduce vividdream method generating explorable scene ambient dynamic single input image text prompt vividdream first expands input image static point cloud iterative inpainting geometry merging ensemble animated video generated using video diffusion model quality refinement technique conditioned rendering static scene sampled camera trajectory optimize canonical scene representation using animated video ensemble pervideo motion embeddings visibility mask mitigate inconsistency resulting scene enables freeview exploration scene plausible ambient scene dynamic experiment demonstrate vividdream provide human viewer compelling experience generated based diverse real image text prompt
replace anyone video recent advancement controllable humancentric video generation particularly rise diffusion model demonstrated considerable progress however achieving precise localized control human motion eg replacing inserting individual video exhibiting desired motion pattern still remains challenging work propose replaceanyone framework focus localizing manipulating human motion video diverse intricate background specifically formulate task imageconditioned posedriven video inpainting paradigm employing unified video diffusion architecture facilitates imageconditioned posedriven video generation inpainting within masked video region moreover introduce diverse mask form involving regular irregular shape avoid shape leakage allow granular local control additionally implement twostage training methodology initially training imageconditioned pose driven video generation model followed joint training video inpainting within masked area way approach enables seamless replacement insertion character maintaining desired pose motion reference appearance within single framework experimental result demonstrate effectiveness method generating realistic coherent video content
dreamfactory pioneering multiscene long video generation multiagent framework current video generation model excel creating short realistic clip struggle longer multiscene video introduce textttdreamfactory llmbased framework tackle challenge textttdreamfactory leverage multiagent collaboration principle key frame iteration design method ensure consistency style across long video utilizes chain thought cot address uncertainty inherent large language model textttdreamfactory generates long stylistically coherent complex video evaluating longform video present challenge propose novel metric crossscene face distance score crossscene style consistency score research area contribute multiscene video dataset containing humanrated video
enhanced creativity ideation stable video synthesis paper explores innovative application stable video diffusion svd diffusion model revolutionizes creation dynamic video content static image digital medium design industry accelerate svd emerges powerful generative tool enhances productivity introduces novel creative possibility paper examines technical underpinnings diffusion model practical effectiveness potential future development particularly context video generation svd operates probabilistic framework employing gradual denoising process transform random noise coherent video frame address challenge visual consistency natural movement stylistic reflection generated video showcasing high generalization capability integration svd design task promise enhanced creativity rapid prototyping significant time cost efficiency particularly impactful area requiring frametoframe consistency natural motion capture creative diversity animation visual effect advertising educational content creation paper concludes svd catalyst design innovation offering wide array application promising avenue future research development field digital medium design
matter detecting aigenerated video like sora recent advancement diffusionbased video generation showcased remarkable result yet gap synthetic realworld video remains underexplored study examine gap three fundamental perspective appearance motion geometry comparing realworld video generated stateoftheart ai model stable video diffusion achieve train three classifier using convolutional network targeting distinct aspect vision foundation model feature appearance optical flow motion monocular depth geometry classifier exhibit strong performance fake video detection qualitatively quantitatively indicates aigenerated video still easily detectable significant gap real fake video persists furthermore utilizing gradcam pinpoint systematic failure aigenerated video appearance motion geometry finally propose ensembleofexperts model integrates appearance optical flow depth information fake video detection resulting enhanced robustness generalization ability model capable detecting video generated sora high accuracy even without exposure sora video training suggests gap real fake video generalized across various video generative model project page
animatelcm computationefficient personalized style video generation without personalized video data paper introduces effective method computationefficient personalized style video generation without requiring access personalized video data reduces necessary generation time similarly sized video diffusion model second around second maintaining level performance method effectiveness lie duallevel decoupling learning approach separating learning video style video generation acceleration allows personalized style video generation without personalized style video data separating acceleration image generation acceleration video motion generation enhancing training efficiency mitigating negative effect lowquality video data
firstframeguided video editing via imagetovideo diffusion model remarkable generative capability diffusion model motivated extensive research image video editing compared video editing face additional challenge time dimension image editing witnessed development diverse highquality approach capable software like photoshop light gap introduce novel generic solution extends applicability image editing tool video propagating edits single frame entire video using pretrained imagetovideo model method dubbed adaptively preserve visual motion integrity source video depending extent edits effectively handling global edits local edits moderate shape change existing method fully achieve core method two main process coarse motion extraction align basic motion pattern original video appearance refinement precise adjustment using finegrained attention matching also incorporate skipinterval strategy mitigate quality degradation autoregressive generation across multiple video clip experimental result demonstrate framework superior performance finegrained video editing proving capability produce highquality temporally consistent output
captioning video multiple crossmodality teacher quality data annotation upperbounds quality downstream model exist large text corpus imagetext pair highquality videotext data much harder collect first manual labeling timeconsuming requires annotator watch entire video second video temporal dimension consisting several scene stacked together showing multiple action accordingly establish video dataset highquality caption propose automatic approach leveraging multimodal input textual video description subtitle individual video frame specifically curate highresolution video publicly available dataset split semantically consistent video clip apply multiple crossmodality teacher model obtain caption video next finetune retrieval model small subset best caption video manually selected employ model whole dataset select best caption annotation way get video paired highquality text caption dub dataset show value proposed dataset three downstream task video captioning video text retrieval textdriven video generation model trained proposed data score substantially better majority metric across task
latent gaussian diffusion propose first approach generative modeling gaussians latent gaussian diffusion formulation enables effective generative modeling scaling generation entire roomscale scene efficiently rendered enable effective synthesis gaussians propose latent diffusion formulation operating compressed latent space gaussians compressed latent space learned vectorquantized variational autoencoder vqvae employ sparse convolutional architecture efficiently operate roomscale scene way complexity costly generation process via diffusion substantially reduced allowing higher detail objectlevel generation well scalability large scene leveraging gaussian representation generated scene rendered arbitrary viewpoint realtime demonstrate approach significantly improves visual quality prior work unconditional objectlevel radiance field synthesis showcase applicability roomscale scene generation
posecrafter oneshot personalized video synthesis following flexible pose control paper introduce posecrafter oneshot method personalized video generation following control flexible pose built upon stable diffusion controlnet carefully design inference process produce highquality video without corresponding groundtruth frame first select appropriate reference frame training video invert initialize latent variable generation insert corresponding training pose target pose sequence enhance faithfulness trained temporal attention module furthermore alleviate face hand degradation resulting discrepancy pose training video inference pose implement simple latent editing affine transformation matrix involving facial hand landmark extensive experiment several datasets demonstrate posecrafter achieves superior result baseline pretrained vast collection video commonly used metric besides posecrafter follow pose different individual artificial edits simultaneously retain human identity opendomain training video project page available httpsmlgsaigithubioposecrafterdemo
framebridge improving imagetovideo generation bridge model imagetovideo generation gaining increasing attention wide application video synthesis recently diffusionbased model achieved remarkable progress given novel design network architecture cascaded framework motion representation however restricted noisetodata generation process diffusionbased method inevitably suffer difficulty generate video sample appearance consistency temporal coherence uninformative gaussian noise may limit synthesis quality work present framebridge taking given static image prior video target establishing tractable bridge model formulating synthesis framestoframes generation task modelling datatodata process fully exploit information input image facilitate generative model learn image animation process two popular setting training model namely finetuning pretrained texttovideo model training scratch propose two technique snraligned finetuning saf neural prior improve finetuning efficiency diffusionbased model framebridge synthesis quality bridgebased model respectively experiment conducted demonstrate framebridge achieves superior quality comparison diffusion counterpart zeroshot fvd v msrvtt nonzeroshot fvd v proposed saf neural prior effectively enhance ability bridgebased model scenario finetuning training scratch demo sample visited httpsframebridgedemogithubio
taming large video diffusion transformer camera control modern texttovideo synthesis model demonstrate coherent photorealistic generation complex video text description however existing model lack finegrained control camera movement critical downstream application related content creation visual effect vision recently new method demonstrate ability generate video controllable camera pose technique leverage pretrained unetbased diffusion model explicitly disentangle spatial temporal generation still existing approach enables camera control new transformerbased video diffusion model process spatial temporal information jointly propose tame video transformer camera control using controlnetlike conditioning mechanism incorporates spatiotemporal camera embeddings based plucker coordinate approach demonstrates stateoftheart performance controllable video generation finetuning dataset best knowledge work first enable camera control transformerbased video diffusion model
wonderland navigating scene single image paper address challenging question efficiently create highquality widescope scene single arbitrary image existing method face several constraint requiring multiview data timeconsuming perscene optimization low visual quality background distorted reconstruction unseen area propose novel pipeline overcome limitation specifically introduce largescale reconstruction model us latents video diffusion model predict gaussian splattings scene feedforward manner video diffusion model designed create video precisely following specified camera trajectory allowing generate compressed video latents contain multiview information maintaining consistency train reconstruction model operate video latent space progressive training strategy enabling efficient generation highquality widescope generic scene extensive evaluation across various datasets demonstrate model significantly outperforms existing method singleview scene generation particularly outofdomain image first time demonstrate reconstruction model effectively built upon latent space diffusion model realize efficient scene generation
streetscapes largescale consistent street view generation using autoregressive video diffusion present method generating streetscapeslong sequence view onthefly synthesized cityscale scene generation conditioned language input eg city name weather well underlying maplayout hosting desired trajectory compared recent model video generation view synthesis method scale much longerrange camera trajectory spanning several city block maintaining visual quality consistency achieve goal build recent work video diffusion used within autoregressive framework easily scale long sequence particular introduce new temporal imputation method prevents autoregressive approach drifting distribution realistic city imagery train streetscapes system compelling source dataposed imagery google street view along contextual map datawhich allows user generate city view conditioned desired city layout controllable camera pose please see result project page httpsboyangdengcomstreetscapes
cavia cameracontrollable multiview video diffusion viewintegrated attention recent year remarkable breakthrough imagetovideo generation however consistency camera controllability generated frame remained unsolved recent study attempted incorporate camera control generation process result often limited simple trajectory lack ability generate consistent video multiple distinct camera path scene address limitation introduce cavia novel framework cameracontrollable multiview video generation capable converting input image multiple spatiotemporally consistent video framework extends spatial temporal attention module viewintegrated attention module improving viewpoint temporal consistency flexible design allows joint training diverse curated data source including scenelevel static video objectlevel synthetic multiview dynamic video realworld monocular dynamic video best knowledge cavia first kind allows user precisely specify camera motion obtaining object motion extensive experiment demonstrate cavia surpasses stateoftheart method term geometric consistency perceptual quality project page
unlearning concept texttovideo diffusion model advancement computer vision natural language processing texttovideo generation enabled texttovideo diffusion model become prevalent model trained using large amount data internet however training data often contain copyrighted content including cartoon character icon artist style private portrait unsafe video since filtering data retraining model challenging method unlearning specific concept texttovideo diffusion model investigated however due high computational complexity relative large optimization scale little work unlearning method texttovideo diffusion model propose novel conceptunlearning method transferring unlearning capability text encoder texttoimage diffusion model texttovideo diffusion model specifically method optimizes text encoder using fewshot unlearning several generated image used use optimized text encoder texttovideo diffusion model generate video method cost low computation resource small optimization scale discus generated video unlearning concept experiment demonstrates method unlearn copyrighted cartoon character artist style object people facial characteristic method unlearn concept within second rtx since concept unlearning method texttovideo diffusion model make concept unlearning feasible accessible texttovideo domain
qbenchvideo benchmarking video quality understanding lmms rising interest research large multimodal model lmms video understanding many study emphasized general video comprehension capability neglecting systematic exploration video quality understanding address oversight introduce qbenchvideo paper new benchmark specifically designed evaluate lmms proficiency discerning video quality ensure video source diversity qbenchvideo encompasses video natural scene aigenerated content aigc computer graphic cg b building traditional multiplechoice question format yesorno whathow category include openended question better evaluate complex scenario additionally incorporate video pair quality comparison question enhance comprehensiveness c beyond traditional technical aesthetic temporal distortion expanded evaluation aspect include dimension aigc distortion address increasing demand video generation finally collect total questionanswer pair test opensource proprietary lmms finding indicate lmms foundational understanding video quality performance remains incomplete imprecise notable discrepancy compared human performance qbenchvideo seek catalyze community interest stimulate research unlock untapped potential lmms close gap video quality understanding
decoupled video generation chain trainingfree diffusion model expert video generation model hold substantial potential area filmmaking however current video diffusion model need high computational cost produce suboptimal result due extreme complexity video generation task paper propose textbfconfiner efficient video generation framework decouples video generation easier subtasks structure textbfcontrol spatialtemporal retextbffinement generate highquality video chain offtheshelf diffusion model expert expert responsible decoupled subtask refinement introduce coordinated denoising merge multiple diffusion expert capability single sampling furthermore design confinerlong framework generate long coherent video three constraint strategy confiner experimental result indicate inference cost confiner surpasses representative model like lavie modelscope across objective subjective metric confinerlong generate highquality coherent video frame
stereocrafterzero zeroshot stereo video generation noisy restart generating highquality stereo video mimic human binocular vision requires consistent depth perception temporal coherence across frame despite advance image video synthesis using diffusion model producing highquality stereo video remains challenging task due difficulty maintaining consistent temporal spatial coherence left right view introduce stereocrafterzero novel framework zeroshot stereo video generation leverage video diffusion prior without requiring paired training data key innovation include noisy restart strategy initialize stereoaware latent representation iterative refinement process progressively harmonizes latent space addressing issue like temporal flickering view inconsistency addition propose use dissolved depth map streamline latent space operation reducing highfrequency depth information comprehensive evaluation including quantitative metric user study demonstrate stereocrafterzero produce highquality stereo video enhanced depth consistency temporal smoothness even depth estimation imperfect framework robust adaptable across various diffusion model setting new benchmark zeroshot stereo video generation enabling immersive visual experience code httpsgithubcomshijianjianstereocrafterzero
opensora democratizing efficient video production vision language two foundational sens human build cognitive ability intelligence significant breakthrough made ai language ability artificial visual intelligence especially ability generate simulate world see far lagging behind facilitate development accessibility artificial visual intelligence created opensora opensource video generation model designed produce highfidelity video content opensora support wide spectrum visual generation task including texttoimage generation texttovideo generation imagetovideo generation model leverage advanced deep learning architecture traininginference technique enable flexible video synthesis could generate video content second resolution arbitrary aspect ratio specifically introduce spatialtemporal diffusion transformer stdit efficient diffusion framework video decouples spatial temporal attention also introduce highly compressive autoencoder make representation compact accelerate training ad hoc training strategy initiative aim foster innovation creativity inclusivity within community ai content creation embracing opensource principle opensora democratizes full access traininginferencedata preparation code well model weight resource publicly available httpsgithubcomhpcaitechopensora
effect short videosharing service video copy detection short videosharing service allow user post second video eg youtube short tiktok attracted lot attention recent year however conventional video copy detection vcd method mainly focus general videosharing service eg youtube bilibili effect short videosharing service video copy detection still unclear considering illegally copied video short videosharing service servicedistinctive characteristic especially time length pro con vcd service required analyzed paper examine effect short videosharing service vcd constructing dataset short videosharing service characteristic novel dataset automatically constructed publicly available dataset reference video fixed shorttimelength query video automation procedure assure reproducibility data privacy preservation paper experimental result focusing segmentlevel videolevel situation see three effect segmentlevel vcd short videosharing service difficult general videosharing service videolevel vcd short videosharing service easier general videosharing service video alignment component mainly suppress detection performance short videosharing service
motionflow attentiondriven motion transfer video diffusion model texttovideo model demonstrated impressive capability producing diverse captivating video content showcasing notable advancement generative ai however model generally lack finegrained control motion pattern limiting practical applicability introduce motionflow novel framework designed motion transfer video diffusion model method utilizes crossattention map accurately capture manipulate spatial temporal dynamic enabling seamless motion transfer across various context approach require training work testtime leveraging inherent capability pretrained video diffusion model contrast traditional approach struggle comprehensive scene change maintaining consistent motion motionflow successfully handle complex transformation attentionbased mechanism qualitative quantitative experiment demonstrate motionflow significantly outperforms existing model fidelity versatility even drastic scene alteration
generative omnimatte learning decompose video layer given video set input object mask omnimatte method aim decompose video semantically meaningful layer containing individual object along associated effect shadow reflection existing omnimatte method assume static background accurate pose depth estimation produce poor decomposition assumption violated furthermore due lack generative prior natural video existing method complete dynamic occluded region present novel generative layered video decomposition framework address omnimatte problem method assume stationary scene require camera pose depth information produce clean complete layer including convincing completion occluded dynamic region core idea train video diffusion model identify remove scene effect caused specific object show model finetuned existing video inpainting model small carefully curated dataset demonstrate highquality decomposition editing result wide range casually captured video containing soft shadow glossy reflection splashing water
letstalk latent diffusion transformer talking video synthesis portrait image animation using audio rapidly advanced enabling creation increasingly realistic expressive animated face challenge multimodalityguided video generation task involve fusing various modality ensuring consistency timing portrait seek produce vivid talking head address challenge present letstalk latent diffusion transformer talking video synthesis diffusion transformer incorporates modular temporal spatial attention mechanism merge multimodality enhance spatialtemporal consistency handle multimodal condition first summarize three fusion scheme ranging shallow deep fusion compactness thoroughly explore impact applicability propose suitable solution according modality difference image audio video generation portrait utilize deep fusion scheme symbiotic fusion ensure portrait consistency audio implement shallow fusion scheme direct fusion achieve audioanimation alignment preserving diversity extensive experiment demonstrate approach generates temporally coherent realistic video enhanced diversity liveliness
videostar selftraining enables video instruction tuning supervision performance large vision language model lvlms dependent size quality training datasets existing video instruction tuning datasets lack diversity derived prompting large language model video caption generate questionanswer pair therefore mostly descriptive meanwhile many labeled video datasets diverse label supervision exist however find integration lvlms nontrivial herein present video selftraining augmented reasoning videostar first video selftraining approach videostar allows utilization labeled video dataset video instruction tuning videostar lvlm cycle instruction generation finetuning show improves general video understanding ii adapts lvlms novel downstream task existing supervision generation lvlm prompted propose answer answer filtered contain original video label lvlm retrained generated dataset training generated answer contain correct video label videostar utilizes existing video label weak supervision video instruction tuning result demonstrate videostarenhanced lvlms exhibit improved performance general video qa tempcompass performance improved ii downstream task videostar improved accuracy action quality assessment finediving
worldconsistent video diffusion explicit modeling recent advancement diffusion model set new benchmark image video generation enabling realistic visual synthesis across single multiframe context however model still struggle efficiently explicitly generating content address propose worldconsistent video diffusion wvd novel framework incorporates explicit supervision using xyz image encode global coordinate image pixel specifically train diffusion transformer learn joint distribution rgb xyz frame approach support multitask adaptability via flexible inpainting strategy example wvd estimate xyz frame groundtruth rgb generate novel rgb frame using xyz projection along specified camera trajectory wvd unifies task like generation multiview stereo cameracontrolled video generation approach demonstrates competitive performance across multiple benchmark providing scalable solution video image generation single pretrained model
hifivfs high fidelity video face swapping face swapping aim generate result combine identity source attribute target existing method primarily focus imagebased face swapping processing video frame handled independently making difficult ensure temporal stability model perspective face swapping gradually shifting generative adversarial network gans diffusion model dm dm shown possess stronger generative capability current diffusionbased approach often employ inpainting technique struggle preserve finegrained attribute like lighting makeup address challenge propose high fidelity video face swapping hifivfs framework leverage strong generative capability temporal prior stable video diffusion svd build finegrained attribute module extract identitydisentangled finegrained attribute feature identity desensitization adversarial learning additionally introduce detailed identity injection enhance identity similarity extensive experiment demonstrate method achieves stateoftheart sota video face swapping qualitatively quantitatively
controlling space time diffusion model present cascaded diffusion model novel view synthesis nv conditioned one image general scene set camera pose timestamps overcome challenge due limited availability training data advocate joint training camera pose posetime video time pose data propose new architecture enables advocate calibration sfm posed data using monocular metric depth estimator metric scale camera control model evaluation introduce new metric enrich overcome shortcoming current evaluation scheme demonstrating stateoftheart result fidelity pose control compared existing diffusion model nv time adding ability handle temporal dynamic also used improved panorama stitching poseconditioned video video translation several task overview see
pursuing highresolution generation video diffusion model despite tremendous progress generation existing method still struggle produce multiview consistent image highresolution texture detail especially paradigm diffusion lack awareness work present highresolution model new video diffusion based paradigm redefines single image multiview image sequential image generation ie orbital video generation methodology delf underlying temporal consistency knowledge video diffusion model generalizes well geometry consistency across multiple view generation technically first empowers pretrained video diffusion model prior camera pose condition yielding multiview image lowresolution texture detail videotovideo refiner learnt scale multiview image highresolution texture detail highresolution multiview image augmented novel view gaussian splatting finally leveraged obtain highfidelity mesh via reconstruction extensive experiment novel view synthesis single view reconstruction demonstrate manages produce superior multiview consistency image highlydetailed texture source code data available
multidiff consistent novel view synthesis single image introduce multidiff novel approach consistent novel view synthesis scene single rgb image task synthesizing novel view single reference image highly illposed nature exist multiple plausible explanation unobserved area address issue incorporate strong prior form monocular depth predictor videodiffusion model monocular depth enables u condition model warped reference image target view increasing geometric stability videodiffusion prior provides strong proxy scene allowing model learn continuous pixelaccurate correspondence across generated image contrast approach relying autoregressive image generation prone drift error accumulation multidiff jointly synthesizes sequence frame yielding highquality multiview consistent result even longterm scene generation large camera movement reducing inference time order magnitude additional consistency image quality improvement introduce novel structured noise distribution experimental result demonstrate multidiff outperforms stateoftheart method challenging realworld datasets scannet finally model naturally support multiview consistent editing without need tuning
transformerbased image video inpainting current challenge future direction image inpainting currently hot topic within field computer vision offer viable solution various application including photographic restoration video editing medical imaging deep learning advancement notably convolutional neural network cnns generative adversarial network gans significantly enhanced inpainting task improved capability fill missing damaged region image video incorporation contextually appropriate detail advancement improved aspect including efficiency information preservation achieving realistic texture structure recently visual transformer exploited offer improvement image video inpainting advent transformerbased architecture initially designed natural language processing also integrated computer vision task method utilize selfattention mechanism excel capturing longrange dependency within data therefore particularly effective task requiring comprehensive understanding global context image video paper provide comprehensive review current image video inpainting approach specific focus transformerbased technique goal highlight significant improvement provide guideline new researcher field image video inpainting using visual transformer categorized transformerbased technique architectural configuration type damage performance metric furthermore present organized synthesis current challenge suggest direction future research field image video inpainting
video diffusion model strong video inpainter propagationbased video inpainting using optical flow pixel feature level recently garnered significant attention however limitation inaccuracy optical flow prediction propagation noise time issue result nonuniform noise time consistency problem throughout video particularly pronounced removed area large involves substantial movement address issue propose novel first frame filling video diffusion inpainting model fffvdi design fffvdi inspired capability pretrained imagetovideo diffusion model transform first frame image highly natural video apply video inpainting task propagate noise latent information future frame fill masked area first frame noise latent code next finetune pretrained imagetovideo diffusion model generate inpainted video proposed model address limitation existing method rely optical flow quality producing much natural temporally consistent video proposed approach first effectively integrate imagetovideo diffusion model video inpainting task various comparative experiment demonstrate proposed model robustly handle diverse inpainting type high quality
enhancing temporal consistency video editing reconstructing video gaussian splatting recent advancement zeroshot video diffusion model shown promise textdriven video editing challenge remain achieving high temporal consistency address introduce gaussian splatting video refiner designed enhance temporal consistency zeroshot video editor approach utilizes twostage gaussian optimizing process tailored editing dynamic monocular video first stage employ improved version colmap referred mccolmap process original video using masked clipped approach video clip mccolmap generates point cloud dynamic foreground object complex background point cloud utilized initialize two set gaussians aiming represent foreground background view foreground background view merged learnable parameter map reconstruct full view second stage leverage reconstruction ability developed first stage impose temporal constraint video diffusion model demonstrate efficacy stage conduct extensive experiment across two related task video reconstruction video editing trained iteration significantly improves video reconstruction quality psnr psnr increase training efficiency time faster nerfbased stateofart method davis dataset respectively moreover enhances video editing ensuring temporal consistency across dynamic monocular video
contextaware talking face video generation paper consider novel practical case talking face video generation specifically focus scenario involving multipeople interaction talking context audience surroundings present situation video generation take context consideration order generate video content naturally aligned driving audio spatially coherent context achieve provide twostage crossmodal controllable video generation pipeline taking facial landmark explicit compact control signal bridge driving audio talking context generated video inside pipeline devise video diffusion model allowing efficient contort spatial condition landmark context video well audio condition temporally coherent generation experimental result verify advantage proposed method baseline term audiovideo synchronization video fidelity frame consistency
vlogger make dream vlog work present vlogger generic ai system generating minutelevel video blog ie vlog user description different short video second vlog often contains complex storyline diversified scene challenging existing video generation approach break bottleneck vlogger smartly leverage large language model llm director decomposes long video generation task vlog four key stage invoke various foundation model play critical role vlog professional including script actor showmaker voicer design mimicking human being vlogger generate vlogs explainable cooperation topdown planning bottomup shooting moreover introduce novel video diffusion model showmaker serf videographer vlogger generating video snippet shooting scene incorporating script actor attentively textual visual prompt effectively enhance spatialtemporal coherence snippet besides design concise mixed training paradigm showmaker boosting capacity generation prediction finally extensive experiment show method achieves stateoftheart performance zeroshot generation prediction task importantly vlogger generate vlogs openworld description without loss video coherence script actor code model available httpsgithubcomzhuangshaobinvlogger
effivedefficient video editing via textinstruction diffusion model largescale texttovideo model shown remarkable ability direct application video editing remains challenging due limited available datasets current video editing method commonly require pervideo finetuning diffusion model specific inversion optimization ensure highfidelity edits paper introduce effived efficient diffusionbased model directly support instructionguided video editing achieve present two efficient workflow gather video editing pair utilizing augmentation fundamental visionlanguage technique workflow transform vast image editing datasets openworld video highquality dataset training effived experimental result reveal effived generates highquality editing video also executes rapidly finally demonstrate data collection method significantly improves editing performance potentially tackle scarcity video editing data code found httpsgithubcomalibabaeffived
efficient video audio mapper visual scene detection videotoaudio generation aim produce corresponding audio given silent video input task particularly challenging due crossmodality sequential nature audiovisual feature involved recent work made significant progress bridging domain gap video audio generating audio semantically aligned video content however critical limitation approach inability effectively recognize handle multiple scene within video often leading suboptimal audio generation case paper first reimplement stateoftheart model slightly modified lightweight architecture achieving result outperform baseline propose improved model incorporates scene detector address challenge switching multiple visual scene result vggsound show model recognize handle multiple scene within video achieve superior performance baseline fidelity relevance
odvae omnidimensional video compressor improving latent video diffusion model variational autoencoder vae compressing video latent representation crucial preceding component latent video diffusion model lvdms reconstruction quality sufficient vaes compression video efficient lvdms however lvdms utilize image vae whose compression video spatial dimension often ignored temporal dimension conduct temporal compression video vae obtain concise latent representation promising accurate reconstruction seldom explored fill gap propose omnidimension compression vae named odvae temporally spatially compress video although odvaes sufficient compression brings great challenge video reconstruction still achieve high reconstructed accuracy fine design obtain better tradeoff video reconstruction quality compression speed four variant odvae introduced analyzed addition novel tail initialization designed train odvae efficiently novel inference strategy proposed enable odvae handle video arbitrary length limited gpu memory comprehensive experiment video reconstruction lvdmbased video generation demonstrate effectiveness efficiency proposed method
moviedreamer hierarchical generation coherent long visual sequence recent advancement video generation primarily leveraged diffusion model shortduration content however approach often fall short modeling complex narrative maintaining character consistency extended period essential longform video production like movie propose moviedreamer novel hierarchical framework integrates strength autoregressive model diffusionbased rendering pioneer longduration video generation intricate plot progression high visual fidelity approach utilizes autoregressive model global narrative coherence predicting sequence visual token subsequently transformed highquality video frame diffusion rendering method akin traditional movie production process complex story factorized manageable scene capturing employ multimodal script enriches scene description detailed character information visual style enhancing continuity character identity across scene present extensive experiment across various movie genre demonstrating approach achieves superior visual narrative quality also effectively extends duration generated content significantly beyond current capability homepage httpsaimuofagithubiomoviedreamer
transforming static image using generative model video salient object detection many video processing task leveraging largescale image datasets common strategy image data abundant facilitates comprehensive knowledge transfer typical approach simulating video static image involves applying spatial transformation affine transformation spline warping create sequence mimic temporal progression however task like video salient object detection appearance motion cue critical basic imagetovideo technique fail produce realistic optical flow capture independent motion property object study show imagetovideo diffusion model generate realistic transformation static image understanding contextual relationship image component ability allows model generate plausible optical flow preserving semantic integrity reflecting independent motion scene element augmenting individual image way create largescale imageflow pair significantly enhance model training approach achieves stateoftheart performance across public benchmark datasets outperforming existing approach
temporal plugin unsupervised video denoising pretrained image denoisers recent advancement deep learning shown impressive result image video denoising leveraging extensive pair noisy noisefree data supervision however challenge acquiring paired video dynamic scene hamper practical deployment deep video denoising technique contrast obstacle less pronounced image denoising paired data readily available thus welltrained image denoiser could serve reliable spatial prior video denoising paper propose novel unsupervised video denoising framework named temporal plugin tap integrates tunable temporal module pretrained image denoiser incorporating temporal module method harness temporal information across noisy frame complementing power spatial denoising furthermore introduce progressive finetuning strategy refines temporal module using generated pseudo clean video frame progressively enhancing network denoising performance compared unsupervised video denoising method framework demonstrates superior performance srgb raw video denoising datasets
predicting longhorizon future conditioning geometry time work explores task generating future sensor observation conditioned past motivated predictive coding concept neuroscience well robotic application selfdriving vehicle predictive video modeling challenging future may multimodal learning scale remains computationally expensive video processing address challenge key insight leverage largescale pretraining image diffusion model handle multimodality repurpose image model video prediction conditioning new frame timestamps model trained video static dynamic scene allow trained modestlysized datasets introduce invariance factoring illumination texture forcing model predict pseudo depth readily obtained inthewild video via offtheshelf monocular depth network fact show simply modifying network predict grayscale pixel already improves accuracy video prediction given extra controllability timestamp conditioning propose sampling schedule work better traditional autoregressive hierarchical sampling strategy motivated probabilistic metric object forecasting literature create benchmark video prediction diverse set video spanning indoor outdoor scene large vocabulary object experiment illustrate effectiveness learning condition timestamps show importance predicting future invariant modality
opticalflow guided prompt optimization coherent video generation texttovideo diffusion model made significant stride many still face challenge generating video temporal consistency within diffusion framework guidance technique proven effective enhancing output quality inference however applying method video diffusion model introduces additional complexity handling computation across entire sequence address propose novel framework called motionprompt guide video generation process via optical flow specifically train discriminator distinguish optical flow random pair frame real video generated one given prompt influence entire video optimize learnable token embeddings reverse sampling step using gradient trained discriminator applied random frame pair approach allows method generate visually coherent video sequence closely reflect natural motion dynamic without compromising fidelity generated content demonstrate effectiveness approach across various model
fastercache trainingfree video diffusion model acceleration high quality paper present textbftextitfastercache novel trainingfree strategy designed accelerate inference video diffusion model highquality generation analyzing existing cachebased method observe textitdirectly reusing adjacentstep feature degrades video quality due loss subtle variation perform pioneering investigation acceleration potential classifierfree guidance cfg reveal significant redundancy conditional unconditional feature within timestep capitalizing observation introduce fastercache substantially accelerate diffusionbased video generation key contribution include dynamic feature reuse strategy preserve feature distinction temporal continuity cfgcache optimizes reuse conditional unconditional output enhance inference speed without compromising video quality empirically evaluate fastercache recent video diffusion model experimental result show fastercache significantly accelerate video generation eg speedup keeping video quality comparable baseline consistently outperform existing method inference speed video quality
diffbgm diffusion model video background music generation editing video piece attractive background music indispensable however video background music generation task face several challenge example lack suitable training datasets difficulty flexibly controlling music generation process sequentially aligning video music work first propose highquality musicvideo dataset detailed annotation shot detection provide multimodal information video music present evaluation metric assess music quality including music diversity alignment music video retrieval precision metric finally propose diffbgm framework automatically generate background music given video us different signal control different aspect music generation process ie us dynamic video feature control music rhythm semantic feature control melody atmosphere propose align video music sequentially introducing segmentaware crossattention layer experiment verify effectiveness proposed method code model available httpsgithubcomsizheleediffbgm
overcoming data limitation highquality video diffusion model texttovideo generation aim produce video based given prompt recently several commercial video model able generate plausible video minimal noise excellent detail high aesthetic score however model rely largescale wellfiltered highquality video accessible community many existing research work train model using lowquality dataset struggle generate highquality video model optimized fit work explore training scheme video model extended stable diffusion investigate feasibility leveraging lowquality video synthesized highquality image obtain highquality video model first analyze connection spatial temporal module video model distribution shift lowquality video observe full training module result stronger coupling spatial temporal module training temporal module based stronger coupling shift distribution higher quality without motion degradation finetuning spatial module highquality image resulting generic highquality video model evaluation conducted demonstrate superiority proposed method particularly picture quality motion concept composition
salova segmentaugmented long video assistant targeted retrieval routing longform video analysis despite advance large multimodal model applying long untrimmed video content remains challenging due limitation context length substantial memory overhead constraint often lead significant information loss reduced relevance model response exponential growth video data across web platform understanding longform video crucial advancing generalized intelligence paper introduce salova segmentaugmented long video assistant novel videollm framework designed enhance comprehension lengthy video content targeted retrieval process address two main challenge achieve present scenewalk dataset highquality collection long video densely captioned segment level enable model capture scene continuity maintain rich descriptive context ii develop robust architectural design integrating dynamic routing mechanism spatiotemporal projector efficiently retrieve process relevant video segment based user query framework mitigates limitation current videolmms allowing precise identification retrieval relevant video segment response query thereby improving contextual relevance generated response extensive experiment salova demonstrates enhanced capability processing complex longform video showing significant capability maintain contextual integrity across extended sequence
disentangling foreground background motion enhanced realism human video generation recent advancement human video synthesis enabled generation highquality video application stable diffusion model however existing method predominantly concentrate animating solely human element foreground guided pose information leaving background entirely static contrary authentic highquality video background often dynamically adjust harmony foreground movement eschewing stagnancy introduce technique concurrently learns foreground background dynamic segregating movement using distinct motion representation human figure animated leveraging posebased motion capturing intricate action conversely background employ sparse tracking point model motion thereby reflecting natural interaction foreground activity environmental change training realworld video enhanced innovative motion depiction approach model generates video exhibiting coherent movement foreground subject surrounding context extend video generation longer sequence without accumulating error adopt clipbyclip generation strategy introducing global feature step ensure seamless continuity across segment ingeniously link final frame produced clip input noise spawn succeeding one maintaining narrative flow throughout sequential generation process infuse feature representation initial reference image network effectively curtailing cumulative color inconsistency may otherwise arise empirical evaluation attest superiority method producing video exhibit harmonious interplay foreground action responsive background dynamic surpassing prior methodology regard
vidu highly consistent dynamic skilled texttovideo generator diffusion model introduce vidu highperformance texttovideo generator capable producing video second single generation vidu diffusion model uvit backbone unlocks scalability capability handling long video vidu exhibit strong coherence dynamism capable generating realistic imaginative video well understanding professional photography technique par sora powerful reported texttovideo generator finally perform initial experiment controllable video generation including cannytovideo generation video prediction subjectdriven generation demonstrate promising result
image scene learning imagine world million video threedimensional understanding object scene play key role human ability interact world active area research computer vision graphic robotics large scale synthetic objectcentric datasets shown effective training model understanding object however applying similar approach realworld object scene difficult due lack largescale data video potential source realworld data finding diverse yet corresponding view content shown difficult scale furthermore standard video come fixed viewpoint determined time capture restricts ability access scene variety diverse potentially useful perspective argue large scale video address limitation provide scalable corresponding frame diverse view paper introduce video dataset process efficiently finding corresponding frame diverse viewpoint scale train diffusionbased model odin empowered largest realworld multiview dataset date odin able freely generate novel view realworld scene unlike previous method odin move camera environment enabling model infer geometry layout scene additionally show improved performance standard novel view synthesis reconstruction benchmark
vast unified framework controllable consistent video generation generating highquality video textual description pose challenge maintaining temporal coherence control subject motion propose vast video storyboard text twostage framework address challenge enable highquality video generation first stage storyforge transforms textual description detailed storyboards capturing human pose object layout represent structural essence scene second stage visionforge generates video storyboards producing highquality video smooth motion temporal consistency spatial coherence decoupling text understanding video generation vast enables precise control subject dynamic scene composition experiment vbench benchmark demonstrate vast outperforms existing method visual quality semantic expression setting new standard dynamic coherent video generation
actanywhere subjectaware video background generation generating video background tailor foreground subject motion important problem movie industry visual effect community task involves synthesizing background aligns motion appearance foreground subject also complies artist creative intention introduce actanywhere generative model automates process traditionally requires tedious manual effort model leverage power largescale video diffusion model specifically tailored task actanywhere take sequence foreground subject segmentation input image describes desired scene condition produce coherent video realistic foregroundbackground interaction adhering condition frame train model largescale dataset humanscene interaction video extensive evaluation demonstrate superior performance model significantly outperforming baseline moreover show actanywhere generalizes diverse outofdistribution sample including nonhuman subject please visit project webpage httpsactanywheregithubio
onlinevpo align video diffusion model online videocentric preference optimization recent year field texttovideo generation made significant stride despite progress still gap theoretical advancement practical application amplified issue like degraded image quality flickering artifact recent advancement enhancing video diffusion model vdm feedback learning shown promising result however method still exhibit notable limitation misaligned feedback inferior scalability tackle issue introduce onlinevpo efficient preference learning approach tailored specifically video diffusion model method feature two novel design firstly instead directly using imagebased reward feedback leverage video quality assessment vqa model trained synthetic data reward model provide distribution modalityaligned feedback video diffusion model additionally introduce online dpo algorithm address offpolicy optimization scalability issue existing video preference learning framework employing video reward model offer concise video feedback fly onlinevpo offer effective efficient preference guidance extensive experiment opensource videodiffusion model demonstrate onlinevpo simple yet effective importantly scalable preference learning algorithm video diffusion model offering valuable insight future advancement domain
covert hiding visual editing robust generative video steganography traditional video steganography method based modifying covert space embedding whereas propose innovative approach embeds secret message within semantic feature steganography video editing process although existing traditional video steganography method display certain level security embedding capacity lack adequate robustness common distortion online social network osns paper introduce endtoend robust generative video steganography network rogvs achieves visual editing modifying semantic feature video embed secret message employ faceswapping scenario showcase visual editing effect first design secret message embedding module adaptively hide secret message semantic feature video extensive experiment display proposed rogvs method applied facial video datasets demonstrate superiority existing video image steganography technique term robustness capacity
zerohsi zeroshot humanscene interaction video generation humanscene interaction hsi generation crucial application embodied ai virtual reality robotics yet existing method synthesize interaction unseen environment inthewild scene reconstructed scene rely paired scene captured human motion data training unavailable unseen environment present zerohsi novel approach enables zeroshot humanscene interaction synthesis eliminating need training mocap data key insight distill humanscene interaction stateoftheart video generation model trained vast amount natural human movement interaction use differentiable rendering reconstruct humanscene interaction zerohsi synthesize realistic human motion static scene environment dynamic object without requiring groundtruth motion data evaluate zerohsi curated dataset different type various indoor outdoor scene different interaction prompt demonstrating ability generate diverse contextually appropriate humanscene interaction
avlink temporallyaligned diffusion feature crossmodal audiovideo generation propose avlink unified framework videotoaudio audiotovideo generation leverage activation frozen video audio diffusion model temporallyaligned crossmodal conditioning key framework fusion block facilitates bidirectional information exchange video audio diffusion model temporallyaligned self attention operation unlike prior work us dedicated model task relies pretrained feature extractor avlink achieves task single selfcontained framework directly leveraging feature obtained complementary modality ie video feature generate audio audio feature generate video extensive automatic subjective evaluation demonstrate method achieves substantial improvement audiovideo synchronization outperforming expensive baseline moviegen videotoaudio model
promptus prompt streaming replace video streaming stable diffusion exponential growth video traffic traditional video streaming system approaching limit compression efficiency communication capacity reduce bitrate maintaining quality propose promptus disruptive novel system streaming prompt instead video content stable diffusion convert video frame series prompt delivery ensure pixel alignment gradient descentbased prompt fitting framework proposed achieve adaptive bitrate prompt lowrank decompositionbased bitrate control algorithm introduced interframe compression prompt temporal smoothingbased prompt interpolation algorithm proposed evaluation across various video domain real network trace demonstrate promptus enhance perceptual quality lpips compared vae respectively decrease ratio severely distorted frame moreover promptus achieves realtime video generation prompt fps best knowledge promptus first attempt replace video codecs prompt inversion first use prompt streaming instead video streaming work open new paradigm efficient video communication beyond shannon limit
deco decoupled humancentered diffusion video editing motion consistency diffusion model usher new era video editing flexibly manipulating video content text prompt despite widespread application demand editing humancentered video model face significant challenge handling complex object like human paper introduce deco novel video editing framework specifically designed treat human background separate editable target ensuring global spatialtemporal consistency maintaining coherence individual component specifically propose decoupled dynamic human representation utilizes parametric human body prior generate tailored human preserving consistent motion original video addition consider background layered atlas apply textguided image editing approach enhance geometry texture human optimization extend calculation score distillation sampling normal space image space moreover tackle inconsistent lighting edited target leveraging lightingaware video harmonizer problem previously overlooked decomposeeditcombine approach extensive qualitative numerical experiment demonstrate deco outperforms prior video editing method humancentered video especially longer video
towards motion video diffusion model textconditioned video diffusion model emerged powerful tool realm video generation editing ability capture nuance human movement remains underexplored indeed ability model faithfully model array text prompt lead wide host application human character animation work take initial step investigate whether model effectively guide synthesis realistic human body animation specifically propose synthesize human motion deforming smplx body representation guided score distillation sampling sd calculated using video diffusion model analyzing fidelity resulting animation gain insight extent obtain motion using publicly available texttovideo diffusion model using sd finding shed light potential limitation model generating diverse plausible human motion paving way research exciting area
crossmodal video summarization temporal prompt instruction tuning video summarization aim create short accurate cohesive summary longer video despite existence various video summarization datasets notable limitation limited amount source video hamper effective training advanced large visionlanguage model vlms additionally existing datasets created videotovideo summarization overlooking contemporary need multimodal video content summarization recent effort made expand unimodal multimodal video summarization categorizing task three subtasks based summary modality videotovideo videototext combination video text summarization however textual summary previous multimodal datasets inadequate address issue introduce crossmodal video summarization dataset featuring diverse video sourced youtube length ranging second average summarization ratio video summary paired textual summary reference specific frame index facilitating generation aligned video textual summary addition propose new video summarization framework named specifically study first framework unifies different video summarization task one large language model llm text decoder achieves taskcontrollable video summarization temporal prompt task instruction experiment show outperforms strong baseline model multiple video summarization task furthermore propose enhanced evaluation metric summarization task
gendds generating diverse driving video scenario prompttovideo generative model autonomous driving training requires diverse range datasets encompassing various traffic condition weather scenario road type traditional data augmentation method often struggle generate datasets represent rare occurrence address challenge propose gendds novel approach generating driving scenario generation leveraging capability stable diffusion xl sdxl advanced latent diffusion model methodology involves use descriptive prompt guide synthesis process aimed producing realistic diverse driving scenario power latest computer vision technique controlnet hotshotxl built complete pipeline video generation together sdxl employ kitti dataset includes realworld driving video train model series experiment demonstrate model generate highquality driving video closely replicate complexity variability realworld driving scenario research contributes development sophisticated training data autonomous driving system open new avenue creating virtual environment simulation validation purpose
emo emote portrait alive generating expressive portrait video diffusion model weak condition work tackle challenge enhancing realism expressiveness talking head video generation focusing dynamic nuanced relationship audio cue facial movement identify limitation traditional technique often fail capture full spectrum human expression uniqueness individual facial style address issue propose emo novel framework utilizes direct audiotovideo synthesis approach bypassing need intermediate model facial landmark method ensures seamless frame transition consistent identity preservation throughout video resulting highly expressive lifelike animation experimental result demonsrate emo able produce convincing speaking video also singing video various style significantly outperforming existing stateoftheart methodology term expressiveness realism
divd deblurring improved video diffusion model video deblurring present considerable challenge owing complexity blur frequently result combination camera shake object motion field video deblurring many previous work primarily concentrated distortionbased metric psnr however approach often result weak correlation human perception yield reconstruction lack realism diffusion model video diffusion model respectively excelled field image video generation particularly achieving remarkable result term image authenticity realistic perception however due computational complexity challenge inherent adapting diffusion model still uncertainty regarding potential video diffusion model video deblurring task explore viability video diffusion model task video deblurring introduce diffusion model specifically purpose field leveraging highly correlated information adjacent frame addressing challenge temporal misalignment crucial research direction tackle challenge many improvement based video diffusion model introduced work result model outperforms existing model achieves stateoftheart result range perceptual metric model preserve significant amount detail image maintaining competitive distortion metric furthermore best knowledge first time diffusion model applied video deblurring overcome limitation mentioned
consistent human image video generation spatially conditioned diffusion consistent humancentric image video synthesis aim generate image video new pose preserving appearance consistency given reference image crucial lowcost visual content creation recent advance based diffusion model typically rely separate network reference appearance feature extraction target visual generation leading inconsistent domain gap reference target paper frame task spatiallyconditioned inpainting problem target image inpainted maintain appearance consistency reference approach enables reference feature guide generation posecompliant target within unified denoising network thereby mitigating domain gap additionally better maintain reference appearance information impose causal feature interaction framework reference feature query target feature query appearance information reference target enhance computational efficiency flexibility practical implementation decompose spatiallyconditioned generation process two stage reference appearance extraction conditioned target generation stage share single denoising network interaction restricted selfattention layer proposed method ensures flexible control appearance generated human image video finetuning existing base diffusion model human video data method demonstrates strong generalization unseen human identity pose without requiring additional perinstance finetuning experimental result validate effectiveness approach showing competitive performance compared existing method consistent human image video synthesis
efficient video diffusion model via contentframe motionlatent decomposition video diffusion model recently made great progress generation quality still limited high memory computational requirement current video diffusion model often attempt process highdimensional video directly tackle issue propose contentmotion latent diffusion model cmd novel efficient extension pretrained image diffusion model video generation specifically propose autoencoder succinctly encodes video combination content frame like image lowdimensional motion latent representation former represents common content latter represents underlying motion video respectively generate content frame finetuning pretrained image diffusion model generate motion latent representation training new lightweight diffusion model key innovation design compact latent space directly utilizes pretrained image diffusion model done previous latent video diffusion model lead considerably better quality generation reduced computational cost instance cmd sample video faster prior approach generating video resolution length second moreover cmd achieves fvd score better previous stateoftheart
cvvae compatible video vae latent generative video model spatiotemporal compression video utilizing network variational autoencoders vae play crucial role openais sora numerous video generative model instance many llmlike video model learn distribution discrete token derived vaes within vqvae framework diffusionbased video model capture distribution continuous latent extracted vaes without quantization temporal compression simply realized uniform frame sampling result unsmooth motion consecutive frame currently lack commonly used continuous video vae latent diffusionbased video model research community moreover since current diffusionbased approach often implemented using pretrained texttoimage model directly training video vae without considering compatibility existing model result latent space gap take huge computational resource training bridge gap even model initialization address issue propose method training video vae latent video model namely cvvae whose latent space compatible given image vae eg image vae stable diffusion sd compatibility achieved proposed novel latent space regularization involves formulating regularization loss using image vae benefiting latent space compatibility video model trained seamlessly pretrained video model truly spatiotemporally compressed latent space rather simply sampling video frame equal interval cvvae existing video model generate four time frame minimal finetuning extensive experiment conducted demonstrate effectiveness proposed video vae
invi object insertion video using offtheshelf diffusion model introduce invi approach inserting replacing object within video referred inpainting using offtheshelf texttoimage latent diffusion model invi target controlled manipulation object blending seamlessly background video unlike existing video editing method focus comprehensive restyling entire scene alteration achieve goal tackle two key challenge firstly high quality control blending employ twostep process involving inpainting matching process begin inserting object single frame using controlnetbased inpainting diffusion model generating subsequent frame conditioned feature inpainted frame anchor minimize domain gap background object secondly ensure temporal coherence replace diffusion model selfattention layer extendedattention layer anchor frame feature serve key value layer enhancing consistency across frame approach remove need videospecific finetuning presenting efficient adaptable solution experimental result demonstrate invi achieves realistic object insertion consistent blending coherence across frame outperforming existing method
track answer extending textvqa image video spatiotemporal clue video textbased visual question answering video textvqa practical task aim answer question jointly reasoning textual visual information given video inspired development textvqa image domain existing video textvqa approach leverage language model eg process textrich multiple frame generate answer autoregressively nevertheless spatiotemporal relationship among visual entity including scene text object disrupted model susceptible interference unrelated information resulting irrational reasoning inaccurate answering tackle challenge propose tea stand textbftrack thtextbfe textbfanswer method better extends generative textvqa framework image video tea recovers spatiotemporal relationship complementary way incorporates ocraware clue enhance quality reasoning question extensive experiment several public video textvqa datasets validate effectiveness generalization framework tea outperforms existing textvqa method videolanguage pretraining method video large language model great margin
jvid joint videoimage diffusion visualquality temporalconsistency video generation introduce joint videoimage diffusion model jvid novel approach generating highquality temporally coherent video achieve integrating two diffusion model latent image diffusion model lidm trained image latent video diffusion model lvdm trained video data method combine model reverse diffusion process lidm enhances image quality lvdm ensures temporal consistency unique combination allows u effectively handle complex spatiotemporal dynamic video generation result demonstrate quantitative qualitative improvement producing realistic coherent video
frame familiar frame understanding replication video diffusion model building momentum image generation diffusion model increasing interest videobased diffusion model however video generation pose greater challenge due higherdimensional nature scarcity training data complex spatiotemporal relationship involved image generation model due extensive data requirement already strained computational resource limit instance model reproducing element training sample leading concern even legal dispute sample replication video diffusion model operate even constrained datasets tasked generating spatial temporal content may prone replicating sample training set compounding issue model often evaluated using metric inadvertently reward replication paper present systematic investigation phenomenon sample replication video diffusion model scrutinize various recent diffusion model video synthesis assessing tendency replicate spatial temporal content unconditional conditional generation scenario study identifies strategy less likely lead replication furthermore propose new evaluation strategy take replication account offering accurate measure model ability generate original content
long video diffusion generation segmented crossattention contentrich video data curation introduce presto novel video diffusion model designed generate video longrange coherence rich content extending video generation method maintain scenario diversity long duration present significant challenge address propose segmented crossattention sca strategy split hidden state segment along temporal dimension allowing segment crossattend corresponding subcaption sca requires additional parameter enabling seamless incorporation current ditbased architecture facilitate highquality long video generation build longtakehd dataset consisting contentrich video scenario coherence annotated overall video caption five progressive subcaptions experiment show presto achieves vbench semantic score dynamic degree outperforming existing stateoftheart video generation method demonstrates proposed presto significantly enhances content richness maintains longrange coherence capture intricate textual detail detail displayed project page httpsprestovideogithubio
vidprom millionscale real promptgallery dataset texttovideo diffusion model arrival sora mark new era texttovideo diffusion model bringing significant advancement video generation potential application however sora along texttovideo diffusion model highly reliant prompt publicly available dataset feature study texttovideo prompt paper introduce vidprom first largescale dataset comprising million unique texttovideo prompt real user additionally dataset includes million video generated four stateoftheart diffusion model alongside related data initially discus curation largescale dataset process timeconsuming costly subsequently underscore need new prompt dataset specifically designed texttovideo generation illustrating vidprom differs diffusiondb largescale promptgallery dataset image generation extensive diverse dataset also open many exciting new research area instance suggest exploring texttovideo prompt engineering efficient video generation video copy detection diffusion model develop better efficient safer model project including collected dataset vidprom related code publicly available httpsvidpromgithubio ccbync license
extreme video compression pretrained diffusion model diffusion model achieved remarkable success generating high quality image video data recently also used image compression high perceptual quality paper present novel approach extreme video compression leveraging predictive power diffusionbased generative model decoder conditional diffusion model take several neural compressed frame generates subsequent frame reconstruction quality drop desired level new frame encoded restart prediction entire video sequentially encoded achieve visually pleasing reconstruction considering perceptual quality metric learned perceptual image patch similarity lpips frechet video distance fvd bit rate low bit per pixel bpp experimental result demonstrate effectiveness proposed scheme compared standard codecs low bpp regime result showcase potential exploiting temporal relation video data using generative model code available httpsgithubcomelesionkyrieextremevideocompressionwithpredictionusingpretraindeddiffusionmodels
customizeavideo oneshot motion customization texttovideo diffusion model image customization extensively studied texttoimage diffusion model leading impressive outcome application emergence texttovideo diffusion model temporal counterpart motion customization yet well investigated address challenge oneshot video motion customization propose customizeavideo model motion single reference video adapts new subject scene spatial temporal variety leverage lowrank adaptation lora temporal attention layer tailor pretrained diffusion model specific motion modeling disentangle spatial temporal information training introduce novel concept appearance absorber detach original appearance reference video prior motion learning proposed module trained staged pipeline inferred plugandplay fashion enabling easy extension various downstream task custom video generation editing video appearance customization multiple motion combination project page found httpscustomizeavideogithubio
grid diffusion model texttovideo generation recent advance diffusion model significantly improved texttoimage generation however generating video text challenging task generating image text due much larger dataset higher computational cost required existing video generation method use either unet architecture considers temporal dimension autoregressive generation method require large datasets limited term computational cost compared texttoimage generation tackle challenge propose simple effective novel grid diffusion texttovideo generation without temporal dimension architecture large textvideo paired dataset generate highquality video using fixed amount gpu memory regardless number frame representing video grid image additionally since method reduces dimension video dimension image various imagebased method applied video textguided video manipulation image manipulation proposed method outperforms existing method quantitative qualitative evaluation demonstrating suitability model realworld video generation
warped diffusion solving video inverse problem image diffusion model using image model naively solving inverse video problem often suffers flickering texturesticking temporal inconsistency generated video tackle problem paper view frame continuous function space video sequence continuous warping transformation different frame perspective allows u train function space diffusion model image utilize solve temporally correlated inverse problem function space diffusion model need equivariant respect underlying spatial transformation ensure temporal consistency introduce simple posthoc testtime guidance towards selfequivariant solution method allows u deploy stateoftheart latent diffusion model stable diffusion xl solve video inverse problem demonstrate effectiveness method video inpainting video superresolution outperforming existing technique based noise transformation provide generated video result
towards multitask multimodal model video generative perspective advancement language foundation model primarily fueled recent surge artificial intelligence contrast generative learning nontextual modality especially video significantly trail behind language modeling thesis chronicle endeavor build multitask model generating video modality diverse condition well understanding compression application given high dimensionality visual data pursue concise accurate latent representation videonative spatialtemporal tokenizers preserve high fidelity unveil novel approach mapping bidirectionally visual observation interpretable lexical term furthermore scalable visual token representation prof beneficial across generation compression understanding task achievement mark first instance language model surpassing diffusion model visual synthesis video tokenizer outperforming industrystandard codecs within multimodal latent space study design multitask generative model masked multitask transformer excels quality efficiency flexibility video generation enable frozen language model trained solely text generate visual content finally build scalable generative multimodal transformer trained scratch enabling generation video containing highfidelity motion corresponding audio given diverse condition throughout course shown effectiveness integrating multiple task crafting highfidelity latent representation generating multiple modality work suggests intriguing potential future exploration generating nontextual data enabling realtime interactive experience across various medium form
reenact anything semantic video motion transfer using motiontextual inversion recent year seen tremendous improvement quality video generation editing approach several technique focus editing appearance address motion current approach using text trajectory bounding box limited simple motion specify motion single motion reference video instead propose use pretrained imagetovideo model rather texttovideo model approach allows u preserve exact appearance position target object scene help disentangle appearance motion method called motiontextual inversion leverage observation imagetovideo model extract appearance mainly latent image input textimage embedding injected via crossattention predominantly control motion thus represent motion using textimage embedding token operating inflated motiontext embedding containing multiple textimage embedding token per frame achieve high temporal motion granularity optimized motion reference video embedding applied various target image generate video semantically similar motion approach require spatial alignment motion reference video target image generalizes across various domain applied various task fullbody face reenactment well controlling motion inanimate object camera empirically demonstrate effectiveness method semantic video motion transfer task significantly outperforming existing method context
generative video diffusion unseen novel semantic video moment retrieval video moment retrieval vmr aim locate likely video moment corresponding text query untrimmed video training existing method limited lack diverse generalisable vmr datasets hindering ability generalise momenttext association query containing novel semantic concept unseen visually textually training source domain model generalisation novel semantics existing method rely heavily assuming access video text sentence pair target domain addition source domain pairwise training data neither practical scalable work introduce generalisable approach assuming text sentence describing new semantics available model training without seen video target domain end propose finegrained video editing framework termed fve explores generative video diffusion facilitate finegrained video editing seen source concept unseen target sentence consisting new concept enables generative hypothesis unseen video moment corresponding novel concept target domain finegrained generative video diffusion retains original video structure subject specific source domain introducing semantic distinction unseen novel vocabulary target domain critical challenge enable generative finegrained diffusion process meaningful optimising vmr synthesising visually pleasing video solve problem introducing hybrid selection mechanism integrates three quantitative metric selectively incorporate synthetic video moment novel video hypothesis enlarged addition original source training data whilst minimising potential
rethinking clipbased video learner crossdomain openvocabulary action recognition building upon impressive success clip contrastive languageimage pretraining recent pioneer work proposed adapt powerful clip video data leading efficient effective video learner openvocabulary action recognition inspired human perform action diverse environment work delf intriguing question clipbased video learner effectively generalize video domain encountered training answer establish crossdomain openvocabulary action recognition benchmark named xovaction conduct comprehensive evaluation five stateoftheart clipbased video learner various type domain gap evaluation demonstrates previous method exhibit limited action recognition performance unseen video domain revealing potential challenge crossdomain openvocabulary action recognition task paper focus one critical challenge task namely scene bias accordingly contribute novel sceneaware videotext alignment method key idea distinguish video representation apart sceneencoded text representation aiming learn sceneagnostic video representation recognizing action across domain extensive experiment demonstrate effectiveness method benchmark code available httpsgithubcomkunyulinxovaction
optical flow representation alignment mamba diffusion model medical video generation medical video generation model expected profound impact healthcare industry including limited medical education training surgical planning simulation current video diffusion model typically build image diffusion architecture incorporating temporal operation convolution temporal attention although approach effective oversimplification limit spatiotemporal performance consumes substantial computational resource counter propose medical simulation video generator medsora incorporates three key element video diffusion framework integrates advantage attention mamba balancing low computational load highquality video generation ii optical flow representation alignment method implicitly enhances attention interframe pixel iii video variational autoencoder vae frequency compensation address information loss medical feature occurs transforming pixel space latent feature back pixel frame extensive experiment application demonstrate medsora exhibit superior visual quality generating medical video outperforming advanced baseline method result code available httpswongzbbgithubiomedsora
controllable longer image animation diffusion model generating realistic animated video static image important area research computer vision method based physical simulation motion prediction achieved notable advance often limited specific object texture motion trajectory failing exhibit highly complex environment physical dynamic paper introduce opendomain controllable image animation method using motion prior video diffusion model method achieves precise control direction speed motion movable region extracting motion field information video learning moving trajectory strength current pretrained video generation model typically limited producing short video typically less frame contrast propose efficient longduration video generation method based noise reschedule specifically tailored image animation task facilitating creation video frame length maintaining consistency content scenery motion coordination specifically decompose denoise process two distinct phase shaping scene contour refining motion detail reschedule noise control generated frame sequence maintaining longdistance noise correlation conducted extensive experiment baseline encompassing commercial tool academic methodology demonstrate superiority method project page
tcbench benchmarking temporal compositionality texttovideo imagetovideo generation video generation many unique challenge beyond image generation temporal dimension introduces extensive possible variation across frame consistency continuity may violated study move beyond evaluating simple action argue generated video incorporate emergence new concept relation transition like realworld video time progress assess temporal compositionality video generation model propose tcbench benchmark meticulously crafted text prompt corresponding ground truth video robust evaluation metric prompt articulate initial final state scene effectively reducing ambiguity frame development simplifying assessment transition completion addition collecting aligned realworld video corresponding prompt expand tcbenchs applicability textconditional model imageconditional one perform generative frame interpolation also develop new metric measure completeness component transition generated video demonstrate significantly higher correlation human judgment existing metric comprehensive experimental result reveal video generator achieve less compositional change highlighting enormous space future improvement analysis indicates current video generation model struggle interpret description compositional change synthesize various component across different time step
fastvideoedit leveraging consistency model efficient texttovideo editing diffusion model demonstrated remarkable capability texttoimage texttovideo generation opening possibility video editing based textual input however computational cost associated sequential sampling diffusion model pose challenge efficient video editing existing approach relying image generation model video editing suffer timeconsuming oneshot finetuning additional condition extraction ddim inversion making realtime application impractical work propose fastvideoedit efficient zeroshot video editing approach inspired consistency model cm leveraging selfconsistency property cm eliminate need timeconsuming inversion additional condition extraction reducing editing time method enables direct mapping source video target video strong preservation ability utilizing special variance schedule result improved speed advantage fewer sampling step used maintaining comparable generation quality experimental result validate stateoftheart performance speed advantage fastvideoedit across evaluation metric encompassing editing speed temporal consistency textvideo alignment
towards retrieval augmented generation large video library video content creator need efficient tool repurpose content task often requires complex manual automated search crafting new video large video library remains challenge paper introduce task video library question answering vlqa interoperable architecture applies retrieval augmented generation rag video library propose system us large language model llm generate search query retrieving relevant video moment indexed speech visual metadata answer generation module integrates user query metadata produce response specific video timestamps approach show promise multimedia content retrieval aiassisted video content creation
factorizeddreamer training highquality video generator limited lowquality data texttovideo generation gained significant attention due wide application video generation editing enhancement translation etc however highquality hq video synthesis extremely challenging diverse complex motion existed real world existing work struggle address problem collecting largescale hq video inaccessible community work show publicly available limited lowquality lq data sufficient train hq video generator without recaptioning finetuning factorize whole generation process two step generating image conditioned highly descriptive caption synthesizing video conditioned generated image concise caption motion detail specifically present emphfactorizeddreamer factorized spatiotemporal framework several critical design generation including adapter combine text image embeddings pixelaware cross attention module capture pixellevel image information text encoder better understand motion description predictnet supervise optical flow present noise schedule play key role ensuring quality stability video generation model lower requirement detailed caption hq video directly trained limited lq datasets noisy brief caption largely alleviating cost collect largescale hq videotext pair extensive experiment variety imagetovideo generation task demonstrate effectiveness proposed factorizeddreamer source code available urlhttpsgithubcomyangxyfactorizeddreamer
dreamhead learning spatialtemporal correspondence via hierarchical diffusion audiodriven talking head synthesis audiodriven talking head synthesis strives generate lifelike video portrait provided audio diffusion model recognized superior quality robust generalization explored task however establishing robust correspondence temporal audio cue corresponding spatial facial expression diffusion model remains significant challenge talking head generation bridge gap present dreamhead hierarchical diffusion framework learns spatialtemporal correspondence talking head synthesis without compromising model intrinsic quality adaptabilitydreamhead learns predict dense facial landmark audio intermediate signal model spatial temporal correspondencesspecifically first hierarchy audiotolandmark diffusion first designed predict temporally smooth accurate landmark sequence given audio sequence signal second hierarchy landmarktoimage diffusion proposed produce spatially consistent facial portrait video modeling spatial correspondence dense facial landmark appearance extensive experiment show proposed dreamhead effectively learn spatialtemporal consistency designed hierarchical diffusion produce highfidelity audiodriven talking head video multiple identity
lvmark robust watermark latent video diffusion model rapid advancement generative model made possible create hyperrealistic video applicability increase unauthorized use raised significant concern leading growing demand technique protect ownership generative model existing watermarking method effectively embed watermark imagegenerative model fail account temporal information resulting poor performance applied videogenerative model address issue introduce novel watermarking method called lvmark embeds watermark video diffusion model key component lvmark selective weight modulation strategy efficiently embeds watermark message video diffusion model preserving quality generated video accurately decode message presence malicious attack design watermark decoder leverage spatiotemporal information wavelet domain crossattention module best knowledge approach first highlight potential videogenerative model watermarking valuable tool enhancing effectiveness ownership protection videogenerative model
efficient long video tokenization via coordinatebased patch reconstruction efficient tokenization video remains challenge training vision model process long video one promising direction develop tokenizer encode long video clip would enable tokenizer leverage temporal coherence video better tokenization however training existing tokenizers long video often incurs huge training cost trained reconstruct frame paper introduce coordtok video tokenizer learns mapping coordinatebased representation corresponding patch input video inspired recent advance generative model particular coordtok encodes video factorized triplane representation reconstructs patch correspond randomly sampled xyt coordinate allows training large tokenizer model directly long video without requiring excessive training resource experiment show coordtok drastically reduce number token encoding long video clip instance coordtok encode video resolution token baseline need token achieve similar reconstruction quality show efficient video tokenization enables memoryefficient training diffusion transformer generate frame
directorllm humancentric video generation paper introduce directorllm novel video generation model employ large language model llm orchestrate human pose within video foundational texttovideo model rapidly evolve demand highquality human motion interaction grows address need enhance authenticity human motion extend llm text generator video director human motion simulator utilizing opensource resource llama train directorllm generate detailed instructional signal human pose guide video generation approach offloads simulation human motion video generator llm effectively creating informative outline humancentric scene signal used condition video renderer facilitating realistic promptfollowing video generation independent llm module applied different video renderers including unet dit minimal effort experiment automatic evaluation benchmark human evaluation show model outperforms existing one generating video higher human motion fidelity improved prompt faithfulness enhanced rendered subject naturalness
vividzoo multiview video generation diffusion model diffusion model shown impressive performance imagevideo generation diffusionbased texttomultiviewvideo generation remains underexplored new challenge posed generation lie lack massive captioned multiview video complexity modeling multidimensional distribution end propose novel diffusionbased pipeline generates highquality multiview video centered around dynamic object text specifically factor problem viewpointspace time component factorization allows u combine reuse layer advanced pretrained multiview image video diffusion model ensure multiview consistency well temporal coherence generated multiview video largely reducing training cost introduce alignment module align latent space layer pretrained multiview video diffusion model addressing reused layer incompatibility arises domain gap multiview data support future research contribute captioned multiview video dataset experimental result demonstrate method generates highquality multiview video exhibiting vivid motion temporal coherence multiview consistency given variety text prompt
synchronized video storytelling generating video narration structured storyline video storytelling engaging multimedia content utilizes video accompanying narration attract audience key challenge creating narration recorded visual scene previous study dense video captioning video story generation made progress however practical application typically require synchronized narration ongoing visual scene work introduce new task synchronized video storytelling aim generate synchronous informative narration video narration associated video clip relate visual content integrate relevant knowledge appropriate word count corresponding clip duration specifically structured storyline beneficial guide generation process ensuring coherence integrity support exploration task introduce new benchmark dataset esyncvidstory rich annotation since existing multimodal llm effective addressing task oneshot fewshot setting propose framework named videonarrator generate storyline input video simultaneously generate narration guidance generated predefined storyline introduce set evaluation metric thoroughly assess generation automatic human evaluation validate effectiveness approach dataset code evaluation released
video diffusion alignment via reward gradient made significant progress towards building foundational video diffusion model model trained using largescale unsupervised data become crucial adapt model specific downstream task adapting model via supervised finetuning requires collecting target datasets video challenging tedious work utilize pretrained reward model learned via preference top powerful vision discriminative model adapt video diffusion model model contain dense gradient information respect generated rgb pixel critical efficient learning complex search space video show backpropagating gradient reward model video diffusion model allow compute sample efficient alignment video diffusion model show result across variety reward model video diffusion model demonstrating approach learn much efficiently term reward query computation prior gradientfree approach code model weightsand visualization available httpsvadervidgithubio
genlit reformulating singleimage relighting video generation manipulating illumination within single image represents fundamental challenge computer vision graphic problem traditionally addressed using inverse rendering technique require explicit asset reconstruction costly ray tracing simulation meanwhile recent advancement visual foundation model suggest new paradigm could soon practical possible one replaces explicit physical model network trained massive amount image video data paper explore potential exploiting video diffusion model particular stable video diffusion svd understanding physical world perform relighting task given single image specifically introduce genlit framework distills ability graphic engine perform light manipulation video generation model enabling user directly insert manipulate point light world within given image generate result directly video sequence find model finetuned small synthetic dataset object able generalize real image enabling singleimage relighting realistic ray tracing effect cast shadow result reveal ability video foundation model capture rich information lighting material shape finding suggest model minimal training used physicallybased rendering without explicit physically asset reconstruction complex ray tracing suggests potential model controllable physically accurate image synthesis task
highly dynamic realistic portrait image animation video diffusion transformer existing methodology animating portrait image face significant challenge particularly handling nonfrontal perspective rendering dynamic object around portrait generating immersive realistic background paper introduce first application pretrained transformerbased video generative model demonstrates strong generalization capability generates highly dynamic realistic video portrait animation effectively addressing challenge adoption new video backbone model make previous unetbased method identity maintenance audio conditioning video extrapolation inapplicable address limitation design identity reference network consisting causal vae combined stacked series transformer layer ensuring consistent facial identity across video sequence additionally investigate various speech audio conditioning motion frame mechanism enable generation continuous video driven speech audio method validated experiment benchmark newly proposed wild datasets demonstrating substantial improvement prior method generating realistic portrait characterized diverse orientation within dynamic immersive scene visualization source code available
anchored diffusion video face reenactment video generation drawn significant interest recently pushing development largescale model capable producing realistic video coherent motion due memory constraint model typically generate short video segment combined long video merging process pose significant challenge requires ensuring smooth transition overall consistency paper introduce anchored diffusion novel method synthesizing relatively long seamless video extend diffusion transformer dit incorporate temporal information creating sequencedit sdit model generating short video segment unlike previous work train model video sequence random nonuniform temporal spacing incorporate temporal information via external guidance increasing flexibility allowing capture short longterm relationship furthermore inference leverage transformer architecture modify diffusion process generating batch nonuniform sequence anchored common frame ensuring consistency regardless temporal distance demonstrate method focus face reenactment task creating video source image replicates facial expression movement driving video comprehensive experiment show approach outperforms current technique producing longer consistent highquality video offering editing capability
kubrick multimodal agent collaboration synthetic video generation texttovideo generation dominated endtoend diffusionbased autoregressive model one hand novel model provide plausible versatility criticized physical correctness shading illumination camera motion temporal consistency hand film industry relies manuallyedited computergenerated imagery cgi using modeling software humandirected synthetic video animation address aforementioned shortcoming extremely tedious requires tight collaboration movie maker rendering expert paper introduce automatic synthetic video generation pipeline based vision large language model vlm agent collaboration given natural language description video multiple vlm agent autodirect various process generation pipeline cooperate create blender script render video best aligns given description based film making inspiration augmented blenderbased movie making knowledge director agent decomposes input textbased video description subprocesses subprocess programmer agent produce pythonbased blender script based customized function composing api calling reviewer agent augmented knowledge video reviewing character motion coordinate intermediate screenshots us compositional reasoning ability provide feedback programmer agent programmer agent iteratively improves script yield best overall video outcome generated video show better quality commercial video generation model metric video quality instructionfollowing performance moreover framework outperforms approach comprehensive user study quality consistency rationality
lova longform videotoaudio generation videotoaudio generation important video editing postprocessing enabling creation semanticsaligned audio silent video however existing method focus generating shortform audio short video segment less second giving little attention scenario longform video input current unetbased diffusion model inevitable problem handling longform audio generation inconsistency within final concatenated audio paper first highlight importance longform problem besides propose lova novel model longform videotoaudio generation based diffusion transformer dit architecture lova prof effective generating longform audio compared existing autoregressive model unetbased diffusion model extensive objective subjective experiment demonstrate lova achieves comparable performance benchmark outperforms baseline benchmark longform video input
individual content motion dynamic preserved pruning video diffusion model high computational cost slow inference time major obstacle deploying video diffusion model vdm practical application overcome introduce new video diffusion model compression approach using individual content motion dynamic preserved pruning consistency loss first empirically observe deeper vdm layer crucial maintaining quality textbfmotion dynamic eg coherence entire video shallower layer focused textbfindividual content eg individual frame therefore prune redundant block shallower layer preserving deeper layer resulting lightweight vdm variant called vdmini additionally propose textbfindividual content motion dynamic icmd consistency loss gain comparable generation performance larger vdm ie teacher vdmini ie student particularly first use individual content distillation icd loss ensure consistency feature generated frame teacher student model next introduce multiframe content adversarial mca loss enhance motion dynamic across generated video whole method significantly accelerates inference time maintaining highquality video generation extensive experiment demonstrate effectiveness vdmini two important video generation task texttovideo imagetovideo respectively achieve average time time speed method sfv method maintaining quality generated video two benchmark ie vbench
tavgbench benchmarking text audiblevideo generation text audiblevideo generation tavg task involves generating video accompanying audio based text description achieving requires skillful alignment audio video element support research field developed comprehensive text audiblevideo generation benchmark tavgbench contains million clip total duration thousand hour propose automatic annotation pipeline ensure audible video detailed description audio video content also introduce audiovisual harmoni score avhscore provide quantitative measure alignment generated audio video modality additionally present baseline model tavg called tavdiffusion us twostream latent diffusion model provide fundamental starting point research area achieve alignment audio video employing crossattention contrastive learning extensive experiment evaluation tavgbench demonstrate effectiveness proposed model conventional metric proposed metric
latent tree scene diffusion present novel latent diffusion model largescale scene generation recent advance diffusion model shown impressive result object generation limited spatial extent quality extended scene generate complex diverse scene structure introduce latent tree representation effectively encode lowerfrequency geometry higherfrequency detail coarsetofine hierarchy learn generative diffusion process latent scene space modeling latent component scene resolution level synthesize largescale scene varying size train diffusion model scene patch synthesize arbitrarysized output scene shared diffusion generation across multiple scene patch extensive experiment demonstrate efficacy benefit largescale highquality unconditional scene generation probabilistic completion partial scene observation
exploring interplay video generation world model autonomous driving survey world model video generation pivotal technology domain autonomous driving playing critical role enhancing robustness reliability autonomous system world model simulate dynamic realworld environment video generation model produce realistic video sequence increasingly integrated improve situational awareness decisionmaking capability autonomous vehicle paper investigates relationship two technology focusing structural parallel particularly diffusionbased model contribute accurate coherent simulation driving scenario examine leading work jepa genie sora exemplify different approach world model design thereby highlighting lack universally accepted definition world model diverse interpretation underscore field evolving understanding world model optimized various autonomous driving task furthermore paper discusses key evaluation metric employed domain chamfer distance scene reconstruction frechet inception distance fid assessing quality generated video content analyzing interplay video generation world model survey identifies critical challenge future research direction emphasizing potential technology jointly advance performance autonomous driving system finding presented paper aim provide comprehensive understanding integration video generation world model drive innovation development safer reliable autonomous vehicle
labelefficient data augmentation video diffusion model guidewire segmentation cardiac fluoroscopy accurate segmentation guidewires interventional cardiac fluoroscopy video crucial computeraided navigation task although deep learning method demonstrated high accuracy robustness wire segmentation require substantial annotated datasets generalizability underscoring need extensive labeled data enhance model performance address challenge propose segmentationguided frameconsistency video diffusion model sfvd generate large collection labeled fluoroscopy video augmenting training data wire segmentation network sfvd leverage video limited annotation independently modeling scene distribution motion distribution first sample scene distribution generating fluoroscopy image wire positioned according specified input mask sample motion distribution progressively generating subsequent frame ensuring frametoframe coherence frameconsistency strategy segmentationguided mechanism refines process adjusting wire contrast ensuring diverse range visibility synthesized image evaluation fluoroscopy dataset confirms superior quality generated video show significant improvement guidewire segmentation
vjt video transformer joint task deblurring lowlight enhancement denoising video restoration task aim recover highquality video lowquality observation contains various important subtasks video denoising deblurring lowlight enhancement since video often face different type degradation blur low light noise even worse kind degradation could happen simultaneously taking video extreme environment pose significant challenge one want remove artifact time paper best knowledge first propose efficient endtoend video transformer approach joint task video deblurring lowlight enhancement denoising work build novel multitier transformer tier us different level degraded video target learn feature video effectively moreover carefully design new tiertotier feature fusion scheme learn video feature incrementally accelerate training process suitable adaptive weighting scheme also provide new multiscenelowlightblurnoise mlbn dataset generated according characteristic joint task based realblur dataset youtube video simulate realistic scene far possible conducted extensive experiment compared many previous stateoftheart method show effectiveness approach clearly
snap video scaled spatiotemporal transformer texttovideo synthesis contemporary model generating image show remarkable quality versatility swayed advantage research community repurposes generate video since video content highly redundant argue naively bringing advance image model video generation domain reduces motion fidelity visual quality impairs scalability work build snap video videofirst model systematically address challenge first extend edm framework take account spatially temporally redundant pixel naturally support video generation second show unet workhorse behind image generation scale poorly generating video requiring significant computational overhead hence propose new transformerbased architecture train time faster unets faster inference allows u efficiently train texttovideo model billion parameter first time reach stateoftheart result number benchmark generate video substantially higher quality temporal consistency motion complexity user study showed model favored large margin recent method see website httpssnapresearchgithubiosnapvideo
compositional video generation flow equalization largescale texttovideo diffusion model recently demonstrated unprecedented capability transform natural language description stunning photorealistic video despite promising result significant challenge remains model struggle fully grasp complex compositional interaction multiple concept action issue arises word dominantly influence final video overshadowing conceptsto tackle problem introduce textbfvico generic framework compositional video generation explicitly ensures concept represented properly core vico analyzes input token influence generated video adjusts model prevent single concept dominating specifically vico extract attention weight layer build spatialtemporal attention graph estimate influence emphmaxflow source text token video target token although direct computation attention flow diffusion model typically infeasible devise efficient approximation based subgraph flow employ fast vectorized implementation turn make flow computation manageable differentiable updating noisy latent balance flow vico capture complex interaction consequently produce video closely adhere textual description apply method multiple diffusionbased video model compositional video editing empirical result demonstrate framework significantly enhances compositional richness accuracy generated video visit website athrefhttpsadamdadgithubiovicourlhttpsadamdadgithubiovico
videoagent selfimproving video generation video generation used generate visual plan controlling robotic system given image observation language instruction previous work generated video plan converted robot control executed however major bottleneck leveraging video generation control lie quality generated video often suffer hallucinatory content unrealistic physic resulting low task success control action extracted generated video scaling dataset model size provides partial solution integrating external feedback natural essential grounding video generation real world observation propose videoagent selfimproving generated video plan based external feedback instead directly executing generated video plan videoagent first refines generated video plan using novel procedure call selfconditioning consistency allowing inferencetime compute turned better generated video plan refined video plan executed videoagent collect additional data environment improve video plan generation experiment simulated robotic manipulation metaworld ithor show videoagent drastically reduces hallucination thereby boosting success rate downstream manipulation task illustrate videoagent effectively refine realrobot video providing early indicator robot effective tool grounding video generation physical world video demo code found httpsvideoasagentgithubio
dcgaussian improving gaussian splatting reflective dash cam video present dcgaussian new method generating novel view invehicle dash cam video neural rendering technique made significant stride driving scenario existing method primarily designed video collected autonomous vehicle however video limited quantity diversity compared dash cam video widely used across various type vehicle capture broader range scenario dash cam video often suffer severe obstruction reflection occlusion windshield significantly impede application neural rendering technique address challenge develop dcgaussian based recent realtime neural rendering technique gaussian splatting approach includes adaptive image decomposition module model reflection occlusion unified manner additionally introduce illuminationaware obstruction modeling manage reflection occlusion varying lighting condition lastly employ geometryguided gaussian enhancement strategy improve rendering detail incorporating additional geometry prior experiment selfcaptured public dash cam video show method achieves stateoftheart performance novel view synthesis also accurately reconstructing captured scene getting rid obstruction see project page code data httpslinhanwanggithubiodcgaussian
enhancing multitext long video generation consistency without tuning timefrequency analysis prompt alignment theory despite considerable progress achieved long video generation problem still significant room improve consistency video particularly term smoothness transition scene address issue enhance consistency coherence video generated either single multiple prompt propose timefrequency based temporal attention reweighting algorithm tiara meticulously edits attention score matrix based discrete shorttime fourier transform method supported theoretical guarantee firstofitskind frequencybased method diffusion model video generated multiple prompt investigate key factor affecting prompt interpolation quality propose promptblend advanced prompt interpolation pipeline efficacy proposed method validated via extensive experimental result exhibiting consistent impressive improvement baseline method code released upon acceptance
fourplane factorized video autoencoders latent variable generative model emerged powerful tool generative task including image video synthesis model enabled pretrained autoencoders map high resolution data compressed lower dimensional latent space generative model subsequently developed requiring fewer computational resource despite effectiveness direct application latent variable model higher dimensional domain video continues pose challenge efficient training inference paper propose autoencoder project volumetric data onto fourplane factorized latent space grows sublinearly input size making ideal higher dimensional data like video design factorized model support straightforward adoption number conditional generation task latent diffusion model ldms classconditional generation frame prediction video interpolation result show proposed fourplane latent space retains rich representation needed highfidelity reconstruction despite heavy compression simultaneously enabling ldms operate significant improvement speed memory
reattentional controllable video diffusion editing editing video textual guidance garnered popularity due streamlined process mandate user solely edit text prompt corresponding source video recent study explored exploited largescale texttoimage diffusion model textguided video editing resulting remarkable video editing capability however may still suffer limitation mislocated object incorrect number object therefore controllability video editing remains formidable challenge paper aim challenge limitation proposing reattentional controllable video diffusion editing reatco method specially align spatial placement target object edited text prompt trainingfree manner propose reattentional diffusion rad refocus crossattention activation response edited text prompt target video denoising stage resulting spatially locationaligned semantically highfidelity manipulated video particular faithfully preserve invariant region content less border artifact propose invariant regionguided joint sampling irjs strategy mitigate intrinsic sampling error wrt invariant region denoising timestep constrain generated content harmonized invariant region content experimental result verify reatco consistently improves controllability video diffusion editing achieves superior video editing performance
video diffusion model survey diffusion generative model recently become powerful technique creating modifying highquality coherent video content survey provides comprehensive overview critical component diffusion model video generation including application architectural design temporal dynamic modeling paper begin discussing core principle mathematical formulation explores various architectural choice method maintaining temporal consistency taxonomy application presented categorizing model based input modality text prompt image video audio signal advancement texttovideo generation discussed illustrate stateoftheart capability limitation current approach additionally survey summarizes recent development training evaluation practice including use diverse video image datasets adoption various evaluation metric assess model performance survey concludes examination ongoing challenge generating longer video managing computational cost offer insight potential future direction field consolidating latest research development survey aim serve valuable resource researcher practitioner working video diffusion model website httpsgithubcomndrwmlnkawesomevideodiffusionmodels
cage unsupervised visual composition animation controllable video generation field video generation expanded significantly recent year controllable compositional video generation garnering considerable interest method rely leveraging annotation text object bounding box motion cue require substantial human effort thus limit scalability contrast address challenge controllable compositional video generation without annotation introducing novel unsupervised approach model trained scratch dataset unannotated video inference time compose plausible novel scene animate object placing object part desired location space time core innovation method lie unified control format training process video generation conditioned randomly selected subset pretrained selfsupervised local feature conditioning compels model learn inpaint missing information video spatially temporally thereby learning inherent compositionality scene dynamic moving object abstraction level imposed invariance conditioning input minor visual perturbation enable control object motion simply using feature desired future location call model cage stand visual composition animation video generation conduct extensive experiment validate effectiveness cage across various scenario demonstrating capability accurately follow control generate highquality video exhibit coherent scene composition realistic animation
ardup active region video diffusion universal policy sequential decisionmaking formulated textconditioned video generation problem video planner guided textdefined goal generates future frame visualizing planned action control action subsequently derived work introduce active region video diffusion universal policy ardup novel framework videobased policy learning emphasizes generation active region ie potential interaction area enhancing conditional policy focus interactive area critical task execution innovative framework integrates active region conditioning latent diffusion model video planning employ latent representation direct action decoding inverse dynamic modeling utilizing motion cue video automatic active region discovery method eliminates need manual annotation active region validate ardups efficacy via extensive experiment simulator cliport realworld dataset bridgedata achieving notable improvement success rate generating convincingly realistic video plan
beyouroutpainter mastering video outpainting inputspecific adaptation video outpainting challenging task aiming generating video content outside viewport input video maintaining interframe intraframe consistency existing method fall short either generation quality flexibility introduce motia mastering video outpainting inputspecific adaptation diffusionbased pipeline leverage intrinsic dataspecific pattern source video imagevideo generative prior effective outpainting motia comprises two main phase inputspecific adaptation patternaware outpainting inputspecific adaptation phase involves conducting efficient effective pseudo outpainting learning singleshot source video process encourages model identify learn pattern within source video well bridging gap standard generative process outpainting subsequent phase patternaware outpainting dedicated generalization learned pattern generate outpainting outcome additional strategy including spatialaware insertion noise travel proposed better leverage diffusion model generative prior acquired video pattern source video extensive evaluation underscore motias superiority outperforming existing stateoftheart method widely recognized benchmark notably advancement achieved without necessitating extensive taskspecific tuning
videoinfinity distributed long video generation diffusion model recently achieved remarkable result video generation despite encouraging performance generated video typically constrained small number frame resulting clip lasting merely second primary challenge producing longer video include substantial memory requirement extended processing time required single gpu straightforward solution would split workload across multiple gpus however lead two issue ensuring gpus communicate effectively share timing context information modifying existing video diffusion model usually trained short sequence create longer video without additional training tackle paper introduce videoinfinity distributed inference pipeline enables parallel processing across multiple gpus longform video generation specifically propose two coherent mechanism clip parallelism dualscope attention clip parallelism optimizes gathering sharing context information across gpus minimizes communication overhead dualscope attention modulates temporal selfattention balance local global context efficiently across device together two mechanism join force distribute workload enable fast generation long video x nvidia ada gpu setup method generates video frame approximately minute enabling long video generation speed time faster prior method
idol unified dualmodal latent diffusion humancentric joint videodepth generation significant advance made humancentric video generation yet joint videodepth generation problem remains underexplored existing monocular depth estimation method may generalize well synthesized image video multiviewbased method difficulty controlling human appearance motion work present idol unified dualmodal latent diffusion highquality humancentric joint videodepth generation idol consists two novel design first enable dualmodal generation maximize information exchange video depth generation propose unified dualmodal unet parametersharing framework joint video depth denoising wherein modality label guide denoising target crossmodal attention enables mutual information flow second ensure precise videodepth spatial alignment propose motion consistency loss enforces consistency video depth feature motion field leading harmonized output additionally crossattention map consistency loss applied align crossattention map video denoising depth denoising facilitating spatial alignment extensive experiment tiktok datasets show superior performance significantly surpassing existing method term video fvd depth accuracy
onlyflow optical flow based motion conditioning video diffusion model consider problem texttovideo generation task precise control various application camera movement control videotovideo editing method tacking problem rely providing userdefined control binary mask camera movement embeddings approach propose onlyflow approach leveraging optical flow firstly extracted input video condition motion generated video using text prompt input video onlyflow allows user generate video respect motion input video well text prompt implemented optical flow estimation model applied input video fed trainable optical flow encoder output feature map injected texttovideo backbone model perform quantitative qualitative user preference study show onlyflow positively compare stateoftheart method wide range task even though onlyflow specifically trained task onlyflow thus constitutes versatile lightweight yet efficient method controlling motion texttovideo generation model code made available github huggingface
consistent scene generation text prompt recent advance diffusion model revolutionized content creation yet generating photorealistic dynamic scene remains significant challenge existing dynamic generation method typically rely distilling knowledge pretrained generative model often finetuned synthetic object datasets consequently resulting scene tend objectcentric lack photorealism texttovideo model generate realistic scene motion often struggle spatial understanding provide limited control camera viewpoint rendering address limitation present novel scene generation framework departs conventional multiview generative model favor streamlined architecture harness video generative model trained diverse realworld datasets method first generates reference video using video generation model employ strategic camera array selection rendering apply progressive warping inpainting technique ensure spatial temporal consistency across multiple viewpoint finally optimize multiview image using dynamic renderer enabling flexible camera control based user preference adopting trainingfree architecture efficiently produce realistic scene viewed arbitrary trajectory code made publicly available project page
topa extending large language model video understanding via textonly prealignment recent advancement image understanding benefited extensive use web imagetext pair however video understanding remains challenge despite availability substantial web videotext data difficulty primarily arises inherent complexity video inefficient language supervision recent webcollected videotext datasets paper introduce textonly prealignment topa novel approach extend large language model llm video understanding without need pretraining real video data specifically first employ advanced llm automatically generate textual video comprising continuous textual frame along corresponding annotation simulate real videotext data annotated textual video used prealign languageonly llm video modality bridge gap textual real video employ clip model feature extractor align image text modality textonly prealignment continuous textual frame encoded sequence clip text feature analogous continuous clip image feature thus aligning llm real video representation extensive experiment including zeroshot evaluation finetuning various video understanding task demonstrate topa effective efficient framework aligning video content llm particular without training video data model achieves accuracy challenging longform video understanding benchmark egoschema performance surpasses previous videotext pretraining approach prof competitive recent video agent
realmdreamer textdriven scene generation inpainting depth diffusion introduce realmdreamer technique generating forwardfacing scene text description method optimizes gaussian splatting representation match complex text prompt using pretrained diffusion model key insight leverage inpainting diffusion model conditioned initial scene estimate provide low variance supervision unknown region distillation conjunction imbue highfidelity geometry geometric distillation depth diffusion model conditioned sample inpainting model find initialization optimization crucial provide principled methodology notably technique doesnt require video multiview data synthesize various highquality scene different style complex layout generality method allows synthesis single image measured comprehensive user study method outperforms existing approach preferred project page httpsrealmdreamergithubio
wildvidfit video virtual tryon wild via imagebased controlled diffusion model video virtual tryon aim generate realistic sequence maintain garment identity adapt person pose body shape source video traditional imagebased method relying warping blending struggle complex human movement occlusion limiting effectiveness video tryon application moreover videobased model require extensive highquality data substantial computational resource tackle issue reconceptualize video tryon process generating video conditioned garment description human motion solution wildvidfit employ imagebased controlled diffusion model streamlined onestage approach model conditioned specific garment individual trained still image rather video leverage diffusion guidance pretrained model including video masked autoencoder segment smoothness improvement selfsupervised model feature alignment adjacent frame latent space integration markedly boost model ability maintain temporal coherence enabling effective video tryon within imagebased framework experiment vitonhd dresscode datasets along test vvt tiktok datasets demonstrate wildvidfits capability generate fluid coherent video project page website wildvidfitprojectgithubio
univg towards unifiedmodal video generation diffusion based video generation received extensive attention achieved considerable success within academic industrial community however current effort mainly concentrated singleobjective singletask video generation generation driven text image combination text image fully meet need realworld application scenario user likely input image text condition flexible manner either individually combination address propose unifiedmodal video genearation system capable handling multiple video generation task across text image modality end revisit various video generation task within system perspective generative freedom classify highfreedom lowfreedom video generation category highfreedom video generation employ multicondition cross attention generate video align semantics input image text lowfreedom video generation introduce biased gaussian noise replace pure random gaussian noise help better preserve content input condition method achieves lowest frechet video distance fvd public academic benchmark msrvtt surpasses current opensource method human evaluation par current closesource method sample visit httpsunivgbaidugithubio
trainingfree condition video diffusion model single frame spatialsemantic echocardiogram synthesis conditional video diffusion model cdm shown promising result video synthesis potentially enabling generation realistic echocardiogram address problem data scarcity however current cdms require paired segmentation map echocardiogram dataset present new method called freeecho generating realistic echocardiogram single enddiastolic segmentation map without additional training data method based temporal attention layer model conditioned segmentation map using trainingfree conditioning method based sdedit evaluate model two public echocardiogram datasets camus echonetdynamic show model generate plausible echocardiogram spatially aligned input segmentation map achieving performance comparable trainingbased cdms work open new possibility generating echocardiogram single segmentation map used data augmentation domain adaptation application medical imaging code available
inflation diffusion efficient temporal adaptation texttovideo superresolution propose efficient diffusionbased texttovideo superresolution sr tuning approach leverage readily learned capacity pixel level image diffusion model capture spatial information video generation accomplish goal design efficient architecture inflating weighting texttoimage sr model video generation framework additionally incorporate temporal adapter ensure temporal coherence across video frame investigate different tuning approach based inflated architecture report tradeoff computational cost superresolution quality empirical evaluation quantitative qualitative shutterstock video dataset demonstrates approach able perform texttovideo sr generation good visual quality temporal consistency evaluate temporal coherence also present visualization video format
feedforward scene synthesis sparse view introduce feedforward approach novel view synthesis nv diverse realworld scene using sparse observation setting inherently illposed due minimal overlap among input view insufficient visual information provided making challenging conventional method achieve highquality result address effectively combining geometryaware reconstruction temporally consistent video generation specifically refactors feedforward gaussian splatting model render feature directly latent space pretrained stable video diffusion svd model feature act pose visual cue guide denoising process produce photorealistic view model endtoend trainable support rendering arbitrary view sparse input view evaluate performance introduce new benchmark using challenging dataset achieves superior visual quality compared stateoftheart method widesweeping even nv task experiment existing benchmark also confirm effectiveness model video result available project page
vca video curious agent long video understanding long video understanding pose unique challenge due temporal complexity low information density recent work address task sampling numerous frame incorporating auxiliary tool using llm result high computational cost work introduce curiositydriven video agent selfexploration capability dubbed vca built upon vlms vca autonomously navigates video segment efficiently build comprehensive understanding complex video sequence instead directly sampling frame vca employ treesearch structure explore video segment collect frame rather relying external feedback reward vca leverage vlms selfgenerated intrinsic reward guide exploration enabling capture crucial information reasoning experimental result multiple long video benchmark demonstrate approach superior effectiveness efficiency
harnessing metalearning improving fullframe video stabilization video stabilization longstanding computer vision problem particularly pixellevel synthesis solution video stabilization synthesize full frame add complexity task technique aim stabilize video synthesizing full frame enhancing stability considered video intensifies complexity task due distinct mix unique motion profile visual content present video sequence making robust generalization fixed parameter difficult study introduce novel approach enhance performance pixellevel synthesis solution video stabilization adapting model individual input video sequence proposed adaptation exploit lowlevel visual cue accessible testtime improve stability quality resulting video highlight efficacy methodology testtime adaptation simple finetuning one model followed significant stability gain via integration metalearning technique notably significant improvement achieved single adaptation step versatility proposed algorithm demonstrated consistently improving performance various pixellevel synthesis model video stabilization realworld scenario
rethinking video deblurring waveletaware dynamic transformer diffusion model current video deblurring method limitation recovering highfrequency information since regression loss conservative highfrequency detail since diffusion model dm strong capability generating highfrequency detail consider introducing dm video deblurring task however found directly applying dm video deblurring task following problem dm require many iteration step generate video gaussian noise consumes many computational resource dm easily misled blurry artifact video resulting irrational content distortion deblurred video address issue propose novel video deblurring framework vddiff integrates diffusion model waveletaware dynamic transformer wadt specifically perform diffusion model highly compact latent space generate prior feature containing highfrequency information conforms ground truth distribution design wadt preserve recover lowfrequency information video utilizing highfrequency information generated diffusion model extensive experiment show proposed vddiff outperforms sota method gopro dvd bsd realworld video datasets
llmenhanced graph prior meet indoor scene explicit regularization compositional scene synthesis diverse application across spectrum industry robotics film video game closely mirror complexity realworld multiobject environment conventional work typically employ shape retrieval based framework naturally suffer limited shape diversity recent progress made object shape generation generative model diffusion model increase shape fidelity however approach separately treat shape generation layout generation synthesized scene usually hampered layout collision suggests scenelevel fidelity still underexplored paper aim generating realistic reasonable indoor scene scene graph enrich prior given scene graph input large language model utilized aggregate globalwise feature local nodewise edgewise feature unified graph encoder graph feature extracted guide joint layoutshape generation additional regularization introduced explicitly constrain produced layout benchmarked sgfront dataset method achieves better scene synthesis especially term scenelevel fidelity source code released publication
sar image synthesis diffusion model recent year diffusion model dm become popular method generating synthetic data achieving sample higher quality quickly became superior generative adversarial network gans current stateoftheart method generative modeling however potential yet exploited radar lack available training data longstanding problem work specific type dm namely denoising diffusion probabilistic model ddpm adapted sar domain investigate network choice specific diffusion parameter conditional unconditional sar image generation experiment show ddpm qualitatively quantitatively outperforms stateoftheart ganbased method sar image generation finally show ddpm profit pretraining largescale clutter data generating sar image even higher quality
feedforward bullettime reconstruction dynamic scene monocular video recent advancement static feedforward scene reconstruction demonstrated significant progress highquality novel view synthesis however model often struggle generalizability across diverse environment fail effectively handle dynamic content present btimer short bullettimer first motionaware feedforward model realtime reconstruction novel view synthesis dynamic scene approach reconstructs full scene gaussian splatting representation given target bullet timestamp aggregating information context frame formulation allows btimer gain scalability generalization leveraging static dynamic scene datasets given casual monocular dynamic video btimer reconstructs bullettime scene within reaching stateoftheart performance static dynamic scene datasets even compared optimizationbased approach
exploring aigc video quality focus visual harmony videotext consistency domain distribution gap recent advancement texttovideo artificial intelligence generated content aigc remarkable compared traditional video assessment aigc video encounter various challenge visual inconsistency defy common sense discrepancy content textual prompt distribution gap various generative model etc target challenge work categorize assessment aigc video quality three dimension visual harmony videotext consistency domain distribution gap dimension design specific module provide comprehensive quality assessment aigc video furthermore research identifies significant variation visual quality fluidity style among video generated different texttovideo model predicting source generative model make aigc video feature discriminative enhances quality assessment performance proposed method used thirdplace winner ntire quality assessment aigenerated content track video demonstrating effectiveness code available httpsgithubcomcoobiwtrivqa
vcbench controllable benchmark symbolic abstract challenge video cognition recent advancement large videolanguage model lvlms driven development benchmark designed assess cognitive ability videobased task however existing benchmark heavily rely webcollected video paired human annotation modelgenerated question limit control video content fall short evaluating advanced cognitive ability involving symbolic element abstract concept address limitation introduce vcbench controllable benchmark assess lvlms cognitive ability involving symbolic abstract concept varying difficulty level generating video data pythonbased engine vcbench allows precise control video content creating dynamic taskoriented video feature complex scene abstract concept task pair tailored question template target specific cognitive challenge providing rigorous evaluation test evaluation reveals even stateoftheart sota model struggle simple video cognition task involving abstract concept performance sharply dropping video complexity rise finding reveal current limitation lvlms advanced cognitive task highlight critical role vcbench driving research toward robust lvlms complex video cognition challenge
conclvd controllable chinese landscape video generation via diffusion model chinese landscape painting gem chinese cultural artistic heritage showcase splendor nature deep observation imagination painter limited traditional technique artwork confined static imagery ancient time leaving dynamism landscape subtlety artistic sentiment viewer imagination recently emerging texttovideo diffusion method shown significant promise video generation providing hope creation dynamic chinese landscape painting however challenge lack specific datasets intricacy artistic style creation extensive highquality video pose difficulty model generating chinese landscape painting video paper propose clvhd chinese landscape videohigh definition novel dataset chinese landscape painting video conclvd controllable chinese landscape video diffusion model utilizes stable diffusion specifically present motion module featuring dual attention mechanism capture dynamic transformation landscape imagery alongside noise adapter leverage unsupervised contrastive learning latent space following generation keyframes employ optical flow frame interpolation enhance video smoothness method retains essence landscape painting imagery also achieves dynamic transition significantly advancing field artistic video generation source code dataset available
videofoley twostage videotosound generation via temporal event condition foley sound foley sound synthesis crucial multimedia production enhancing user experience synchronizing audio video temporally semantically recent study automating laborintensive process videotosound generation face significant challenge system lacking explicit temporal feature suffer poor alignment controllability timestampbased model require costly subjective human annotation propose videofoley videotosound system using root mean square rms intuitive condition semantic timbre prompt audio text rms framelevel intensity envelope closely related audio semantics act temporal event feature guide audio generation video annotationfree selfsupervised learning framework consists two stage incorporating novel idea including rms discretization rmscontrolnet pretrained texttoaudio model extensive evaluation show videofoley achieves stateoftheart performance audiovisual alignment controllability sound timing intensity timbre nuance source code model weight demo available companion website httpsjnwnleegithubiovideofoleydemo
ppvf efficient privacypreserving online video fetching framework correlated differential privacy online video streaming evolved integral component contemporary internet landscape yet disclosure user request present formidable privacy challenge user stream preferred online video request automatically seized video content provider potentially leaking user privacy unfortunately current protection method wellsuited preserving user request privacy content provider maintaining highquality online video service tackle challenge introduce novel privacypreserving video fetching ppvf framework utilizes trusted edge device prefetch cache video ensuring privacy user request optimizing efficiency edge caching specifically design ppvf three core component textitonline privacy budget scheduler employ theoretically guaranteed online algorithm select nonrequested video candidate assigned privacy budget alternative video chosen online algorithm theoretically guaranteed consider video utility available privacy budget textitnoisy video request generator generates redundant video request addition original one utilizing correlated differential privacy obfuscate request privacy textitonline video utility predictor leverage federated learning collaboratively evaluate video utility online fashion aiding video selection noise generation finally conduct extensive experiment using realworld video request trace tencent video result demonstrate ppvf effectively safeguard user request privacy upholding high video caching performance
omni world model consistent long video generation video generation model vgms received extensive attention recently serve promising candidate generalpurpose large vision model generate short video time existing method achieve long video generation iteratively calling vgms using lastframe output condition nextround generation however last frame contains shortterm finegrained information scene resulting inconsistency long horizon address propose omni world model produce longterm coherent comprehensive condition consistent long video generation video observation underlying evolving world propose model longterm development latent space use vgms film video specifically represent world latent state variable decoded explicit video observation observation serve basis anticipating temporal dynamic turn update state variable interaction evolving dynamic persistent state enhances diversity consistency long video extensive experiment show achieves comparable performance sota method vbenchlong validating ability generate highquality video observation code httpsgithubcomhuangyhowl
heartbeat towards controllable echocardiography video synthesis multimodal conditionsguided diffusion model echocardiography echo video widely used cardiac examination clinical procedure heavily relies operator experience need year training maybe assistance deep learningbased system enhanced accuracy efficiency however challenging since acquiring sufficient customized data eg abnormal case novice training deep model development clinically unrealistic hence controllable echo video synthesis highly desirable paper propose novel diffusionbased framework named heartbeat towards controllable highfidelity echo video synthesis highlight threefold first heartbeat serf unified framework enables perceiving multimodal condition simultaneously guide controllable generation second factorize multimodal condition local global one two insertion strategy separately provided fine coarsegrained control composable flexible manner way user synthesize echo video conform mental imagery combining multimodal control signal third propose decouple visual concept temporal dynamic learning using twostage training scheme simplifying model training one interesting thing heartbeat easily generalize maskguided cardiac mri synthesis shot showcasing scalability broader application extensive experiment two public datasets show efficacy proposed heartbeat
trainingfree camera control video generation propose trainingfree robust solution offer camera movement control offtheshelf video diffusion model unlike previous work method require supervised finetuning cameraannotated datasets selfsupervised training via data augmentation instead plugandplay pretrained video diffusion model generate cameracontrollable video single image text prompt input inspiration work come layout prior intermediate latents encode generated result thus rearranging noisy pixel cause output content relocate well camera moving could also seen type pixel rearrangement caused perspective change video reorganized following specific camera motion noisy latents change accordingly building propose camtrol enables robust camera control video diffusion model achieved twostage process first model image layout rearrangement explicit camera movement point cloud space second generate video camera motion leveraging layout prior noisy latents formed series rearranged image extensive experiment demonstrated superior performance video generation camera motion alignment compared finetuned method furthermore show capability camtrol generalize various base model well impressive application scalable motion control dealing complicated trajectory unsupervised video generation video available httpslifedecodergithubiocamtrol
image conductor precision control interactive video synthesis filmmaking animation production often require sophisticated technique coordinating camera transition object movement typically involving laborintensive realworld capturing despite advancement generative ai video creation achieving precise control motion interactive video asset generation remains challenging end propose image conductor method precise control camera transition object movement generate video asset single image wellcultivated training strategy proposed separate distinct camera object motion camera lora weight object lora weight address cinematographic variation illposed trajectory introduce camerafree guidance technique inference enhancing object movement eliminating camera transition additionally develop trajectoryoriented video motion data curation pipeline training quantitative qualitative experiment demonstrate method precision finegrained control generating motioncontrollable video image advancing practical application interactive video synthesis project webpage available httpsliyaoweistugithubioprojectimageconductor
vimi grounding video generation multimodal instruction existing texttovideo diffusion model rely solely textonly encoders pretraining limitation stem absence largescale multimodal prompt video datasets resulting lack visual grounding restricting versatility application multimodal integration address construct largescale multimodal prompt dataset employing retrieval method pair incontext example given text prompt utilize twostage training strategy enable diverse video generation task within model first stage propose multimodal conditional video generation framework pretraining augmented datasets establishing foundational model grounded video generation secondly finetune model first stage three video generation task incorporating multimodal instruction process refines model ability handle diverse input task ensuring seamless integration multimodal information twostage training process vimi demonstrates multimodal understanding capability producing contextually rich personalized video grounded provided input shown figure compared previous visual grounded video generation method vimi synthesize consistent temporally coherent video large motion retaining semantic control lastly vimi also achieves stateoftheart texttovideo generation result benchmark
tugofwar deepfake generation detection multimodal generative model rapidly evolving leading surge generation realistic video audio offer exciting possibility also serious risk deepfake video convincingly impersonate individual particularly garnered attention due potential misuse spreading misinformation creating fraudulent content survey paper examines dual landscape deepfake video generation detection emphasizing need effective countermeasure potential abuse provide comprehensive overview current deepfake generation technique including face swapping reenactment audiodriven animation leverage cuttingedge technology like gans diffusion model produce highly realistic fake video additionally analyze various detection approach designed differentiate authentic altered video detecting visual artifact deploying advanced algorithm pinpoint inconsistency across video audio signal effectiveness detection method heavily relies diversity quality datasets used training evaluation discus evolution deepfake datasets highlighting importance robust diverse frequently updated collection enhance detection accuracy generalizability deepfakes become increasingly indistinguishable authentic content developing advanced detection technique keep pace generation technology crucial advocate proactive approach tugofwar deepfake creator detector emphasizing need continuous research collaboration standardization evaluation metric creation comprehensive benchmark
vitondit learning inthewild video tryon human dance video via diffusion transformer video tryon stand promising area tremendous realworld potential prior work limited transferring product clothing image onto person video simple pose background underperforming casually captured video recently sora revealed scalability diffusion transformer dit generating lifelike video featuring realworld scenario inspired explore propose first ditbased video tryon framework practical inthewild application named vitondit specifically vitondit consists garment extractor spatialtemporal denoising dit identity preservation controlnet faithfully recover clothing detail extracted garment feature fused selfattention output denoising dit controlnet also introduce novel random selection strategy training interpolated autoregressive iar technique inference facilitate long video generation unlike existing attempt require laborious restrictive construction paired training dataset severely limiting scalability vitondit alleviates relying solely unpaired human dance video carefully designed multistage training strategy furthermore curate challenging benchmark dataset evaluate performance casual video tryon extensive experiment demonstrate superiority vitondit generating spatiotemporal consistent tryon result inthewild video complicated human pose
trajectoryconditioned generation recent technique generation synthesize dynamic scene using supervision pretrained texttovideo model however existing representation motion deformation model timedependent neural representation limited amount motion generatethey synthesize motion extending far beyond bounding box used volume rendering lack flexible motion model contributes gap realism generation method recent nearphotorealistic video generation model propose trajectoryconditioned generation factor motion global local component represent global motion scene bounding box using rigid transformation along trajectory parameterized spline learn local deformation conform global trajectory using supervision texttovideo model approach enables synthesis scene animated along arbitrary trajectory compositional scene generation significant improvement realism amount generated motion evaluate qualitatively user study video result viewed website
generative video propagation largescale video generation model inherent ability realistically model natural scene paper demonstrate careful design generative video propagation framework various video task addressed unified way leveraging generative power model specifically framework genprop encodes original video selective content encoder propagates change made first frame using imagetovideo generation model propose data generation scheme cover multiple video task based instancelevel video segmentation datasets model trained incorporating mask prediction decoder head optimizing regionaware loss aid encoder preserve original content generation model propagates modified region novel design open new possibility editing scenario genprop allows substantial change object shape insertion inserted object exhibit independent motion removal genprop effectively remove effect like shadow reflection whole video tracking genprop capable tracking object associated effect together experiment result demonstrate leading performance model various video task provide indepth analysis proposed framework
omnitokenizer joint imagevideo tokenizer visual generation tokenizer serving translator map intricate visual data compact latent space lie core visual generative model based finding existing tokenizers tailored image video input paper present omnitokenizer transformerbased tokenizer joint image video tokenization omnitokenizer designed spatialtemporal decoupled architecture integrates window causal attention spatial temporal modeling exploit complementary nature image video data propose progressive training strategy omnitokenizer first trained image data fixed resolution develop spatial encoding capacity jointly trained image video data multiple resolution learn temporal dynamic omnitokenizer first time handle image video input within unified framework prof possibility realizing synergy extensive experiment demonstrate omnitokenizer achieves stateoftheart sota reconstruction performance various image video datasets eg reconstruction fid imagenet reconstruction fvd beating previous sota method respectively additionally also show integrated omnitokenizer language modelbased approach diffusion model realize advanced visual synthesis performance underscoring superiority versatility method code available httpsgithubcomfoundationvisionomnitokenizer
mavin multiaction video generation diffusion model via transition video infilling diffusionbased video generation achieved significant progress yet generating multiple action occur sequentially remains formidable task directly generating video sequential action extremely challenging due scarcity finegrained action annotation difficulty establishing temporal semantic correspondence maintaining longterm consistency tackle propose intuitive straightforward solution splicing multiple singleaction video segment sequentially core challenge lie generating smooth natural transition segment given inherent complexity variability action transition introduce mavin multiaction video infilling model designed generate transition video seamlessly connect two given video forming cohesive integrated sequence mavin incorporates several innovative technique address challenge transition video infilling task firstly consecutive noising strategy coupled variablelength sampling employed handle large infilling gap varied generation length secondly boundary frame guidance bfg proposed address lack semantic guidance transition generation lastly gaussian filter mixer gfm dynamically manages noise initialization inference mitigating traintest discrepancy preserving generation flexibility additionally introduce new metric cliprs clip relative smoothness evaluate temporal coherence smoothness complementing traditional qualitybased metric experimental result horse tiger scenario demonstrate mavin superior performance generating smooth coherent video transition compared existing method
bora biomedical generalist video generation model generative model hold promise revolutionizing medical education robotassisted surgery data augmentation medical ai development diffusion model generate realistic image text prompt recent advancement demonstrated ability create diverse highquality video however model often struggle generating accurate representation medical procedure detailed anatomical structure paper introduces bora first spatiotemporal diffusion probabilistic model designed textguided biomedical video generation bora leverage transformer architecture pretrained generalpurpose video generation task finetuned model alignment instruction tuning using newly established medical video corpus includes paired textvideo data various biomedical field best knowledge first attempt establish comprehensive annotated biomedical video dataset bora capable generating highquality video data across four distinct biomedical domain adhering medical expert standard demonstrating consistency diversity generalist video generative model hold significant potential enhancing medical consultation decisionmaking particularly resourcelimited setting additionally bora could pave way immersive medical training procedure planning extensive experiment distinct medical modality endoscopy ultrasound mri cell tracking validate effectiveness model understanding biomedical instruction superior performance across subject compared stateoftheart generation model
dicode diffusioncompressed deep token autoregressive video generation language model video inherently temporal sequence nature work explore potential modeling video chronological scalable manner autoregressive ar language model inspired success natural language processing introduce dicode novel approach leverage diffusioncompressed deep token generate video language model autoregressive manner unlike existing method employ lowlevel representation limited compression rate dicode utilizes deep token considerable compression rate reduction token count significant compression made possible tokenizer trained leveraging prior knowledge video diffusion model deep token enable dicode employ vanilla ar language model video generation akin translating one visual language another treating video temporal sequence dicode fully harness capability language model autoregressive generation dicode scalable using readily available ar architecture capable generating video ranging second one minute using gpus training evaluate dicode quantitatively qualitatively demonstrating performs comparably existing method term quality ensuring efficient training showcase scalability release series dicode configuration varying parameter size observe consistent improvement performance model size increase believe dicodes exploration academia represents promising initial step toward scalable video modeling ar language model paving way development larger powerful video generation model
camco cameracontrollable imagetovideo generation recently video diffusion model emerged expressive generative tool highquality video content creation readily available general user however model often offer precise control camera pose video generation limiting expression cinematic language user control address issue introduce camco allows finegrained camera pose control imagetovideo generation equip pretrained imagetovideo generator accurately parameterized camera pose input using plucker coordinate enhance consistency video produced integrate epipolar attention module attention block enforces epipolar constraint feature map additionally finetune camco realworld video camera pose estimated structurefrommotion algorithm better synthesize object motion experiment show camco significantly improves consistency camera control capability compared previous model effectively generating plausible object motion project page
lumisculpt consistency lighting control network video generation lighting play pivotal role ensuring naturalness video generation significantly influencing aesthetic quality generated content however due deep coupling lighting temporal feature video remains challenging disentangle model independent coherent lighting attribute limiting ability control lighting video generation paper inspired established controllable model propose lumisculpt first time enables precise consistent lighting control generation modelslumisculpt equips video generation strong interactive capability allowing input custom lighting reference image sequence furthermore core learnable plugandplay module lumisculpt facilitates remarkable control lighting intensity position trajectory latent video diffusion model based advanced dit backboneadditionally effectively train lumisculpt address issue insufficient lighting data construct lumihuman new lightweight flexible dataset portrait lighting image video experimental result demonstrate lumisculpt achieves precise highquality lighting control video generation
exposing aigenerated video benchmark dataset localandglobal temporal defect based detection method generative model made significant advancement creation realistic video cause security issue however emerging risk adequately addressed due absence benchmark dataset aigenerated video paper first construct video dataset using advanced diffusionbased video generation algorithm various semantic content besides typical video lossy operation network transmission adopted generate degraded sample analyzing local global temporal defect current aigenerated video novel detection framework adaptively learning local motion information global appearance variation constructed expose fake video finally experiment conducted evaluate generalization robustness different spatial temporal domain detection method result serve baseline demonstrate research challenge future study
shaping stabilized video mitigating unintended change conceptaugmented video editing textdriven video editing utilizing generative diffusion model garnered significant attention due potential application however existing approach constrained limited word embeddings provided pretraining hinders nuanced editing targeting open concept specific attribute directly altering keywords target prompt often result unintended disruption attention mechanism achieve flexible editing easily work proposes improved conceptaugmented video editing approach generates diverse stable target video flexibly devising abstract conceptual pair specifically framework involves conceptaugmented textual inversion dual prior supervision mechanism former enables plugandplay guidance stable diffusion video editing effectively capturing target attribute stylized result dual prior supervision mechanism significantly enhances video stability fidelity comprehensive evaluation demonstrate approach generates stable lifelike video outperforming stateoftheart method
immersepro endtoend stereo video synthesis via implicit disparity learning introduce textitimmersepro innovative framework specifically designed transform singleview video stereo video framework utilizes novel dualbranch architecture comprising disparity branch context branch video data leveraging spatialtemporal attention mechanism textitimmersepro employ implicit disparity guidance enabling generation stereo pair video sequence without need explicit disparity map thus reducing potential error associated disparity estimation model addition technical advancement introduce youtubesbs dataset comprehensive collection stereo video sourced youtube dataset unprecedented scale featuring million stereo pair designed facilitate training benchmarking stereo video generation model experiment demonstrate effectiveness textitimmersepro producing highquality stereo video offering significant improvement existing method compared best competitor stereofrommono quantitatively improve result ssim psnr
sportshhi dataset humanhuman interaction detection sport video videobased visual relation detection task video scene graph generation play important role finegrained video understanding however current video visual relation detection datasets two main limitation hinder progress research area first explore complex humanhuman interaction multiperson scenario second relation type existing datasets relatively lowlevel semantics often recognized appearance simple prior information without need detailed spatiotemporal context reasoning nevertheless comprehending highlevel interaction human crucial understanding complex multiperson video sport surveillance video address issue propose new video visual relation detection task video humanhuman interaction detection build dataset named sportshhi sportshhi contains highlevel interaction class basketball volleyball sport human bounding box interaction instance annotated keyframes benchmark propose twostage baseline method conduct extensive experiment reveal key factor successful humanhuman interaction detector hope sportshhi stimulate research human interaction understanding video promote development spatiotemporal context modeling technique video visual relation detection
zeroshot surgical tool segmentation monocular video using segment anything model segment anything model sam latest generation foundation model image video segmentation trained expansive segment anything video sav dataset comprises million mask across video sam advance predecessor capability supporting zeroshot segmentation various prompt eg point box mask robust zeroshot performance efficient memory usage make sam particularly appealing surgical tool segmentation video especially given scarcity labeled data diversity surgical procedure study evaluate zeroshot video segmentation performance sam model across different type surgery including endoscopy microscopy also assess performance video featuring single multiple tool varying length demonstrate sam applicability effectiveness surgical domain found sam demonstrates strong capability segmenting various surgical video new tool enter scene additional prompt necessary maintain segmentation accuracy specific challenge inherent surgical video impact robustness sam
noise crystallization liquid noise zeroshot video generation using image diffusion model although powerful image generation consistent controllable video longstanding problem diffusion model video model require extensive training computational resource leading high cost large environmental impact moreover video model currently offer limited control output motion paper introduces novel approach video generation augmenting image diffusion model create sequential animation frame maintaining fine detail technique applied existing image model without training video parameter zeroshot altering input noise latent diffusion model two complementary method presented noise crystallization ensures consistency limited large movement due reduced latent embedding size liquid noise trade consistency greater flexibility without resolution limitation core concept also allow application relighting seamless upscaling improved video style transfer furthermore exploration vae embedding used latent diffusion model performed resulting interesting theoretical insight method humaninterpretable latent space
latentcolorization latent diffusionbased speaker video colorization current research predominantly focus imagebased colorization domain videobased colorization remains relatively unexplored existing video colorization technique operate framebyframe basis often overlooking critical aspect temporal coherence successive frame approach result inconsistency across frame leading undesirable effect like flickering abrupt color transition frame address challenge harness generative capability finetuned latent diffusion model designed specifically video colorization introducing novel solution achieving temporal consistency video colorization well demonstrating strong improvement established image quality metric compared existing method furthermore perform subjective study user preferred approach existing state art dataset encompasses combination conventional datasets video televisionmovies short leveraging power finetuned latent diffusionbased colorization system temporal consistency mechanism improve performance automatic video colorization addressing challenge temporal inconsistency short demonstration result seen example video available httpsyoutubevdbzszdfuxm
live stream translation via unidirectional attention video diffusion model large language model shown remarkable efficacy generating streaming data text audio thanks temporally unidirectional attention mechanism model correlation current token previous token however video streaming remains much less explored despite growing need live video processing stateoftheart video diffusion model leverage bidirectional temporal attention model correlation current frame surrounding ie including future frame hinders processing streaming video address problem present first attempt designing video diffusion model unidirectional temporal attention specifically targeting live streaming video translation compared previous work approach ensures temporal consistency smoothness correlating current frame predecessor initial warmup frame without future frame additionally use highly efficient denoising scheme featuring kvcache mechanism pipelining facilitate streaming video translation interactive framerates extensive experiment demonstrate effectiveness proposed attention mechanism pipeline outperforming previous method term temporal smoothness andor efficiency
humanvdm learning singleimage human gaussian splatting video diffusion model generating lifelike human single rgb image remains challenging task computer vision requires accurate modeling geometry highquality texture plausible unseen part existing method typically use multiview diffusion model generation often face inconsistent view issue hinder highquality human generation address propose humanvdm novel method generating human single rgb image using video diffusion model humanvdm provides temporally consistent view human generation using gaussian splatting consists three module viewconsistent human video diffusion module video augmentation module gaussian splatting module first single image fed human video diffusion module generate coherent human video next video augmentation module applies superresolution video interpolation enhance texture geometric smoothness generated video finally human gaussian splatting module learns lifelike human guidance highresolution viewconsistent image experiment demonstrate humanvdm achieves highquality human single image outperforming stateoftheart method generation quality quantity project page httpshumanvdmgithubiohumanvdm
transforming drone video bevs videobased geolocalization existing approach drone visual geolocalization predominantly adopt imagebased setting single droneview snapshot matched image platform task formulation however underutilizes inherent video output drone sensitive occlusion viewpoint disparity address limitation formulate new videobased drone geolocalization task propose paradigm paradigm transforms video bird eye view bev simplifying subsequent textbfinterplatform matching process particular employ gaussian splatting reconstruct scene obtain bev projection different existing transform method eg polar transform bevs preserve finegrained detail without significant distortion facilitate discriminative textbfintraplatform representation learning paradigm also incorporates diffusionbased module generating hard negative sample validate approach introduce univ new videobased geolocalization dataset extends imagebased dataset univ feature flight path elevation angle increased frame rate frame per second fps extensive experiment univ dataset show paradigm achieves competitive recall rate outperforms conventional videobased method compared competitive method proposed approach exhibit robustness lower elevation occlusion
easyanimate highperformance long video generation method based transformer architecture paper present easyanimate advanced method video generation leverage power transformer architecture highperformance outcome expanded dit framework originally designed image synthesis accommodate complexity video generation incorporating motion module block used capture temporal dynamic thereby ensuring production consistent frame seamless motion transition motion module adapted various dit baseline method generate video different style also generate video different frame rate resolution training inference phase suitable image video moreover introduce slice vae novel approach condense temporal axis facilitating generation long duration video currently easyanimate exhibit proficiency generate video frame provide holistic ecosystem video production based dit encompassing aspect data preprocessing vae training dit model training baseline model lora model endtoend video inference code available httpsgithubcomaigcappseasyanimate continuously working enhance performance method
hierarchical patch diffusion model highresolution video generation diffusion model demonstrated remarkable performance image video synthesis however scaling highresolution input challenging requires restructuring diffusion pipeline multiple independent component limiting scalability complicating downstream application make efficient training unlocks endtoend optimization highresolution video improve pdms two principled way first enforce consistency patch develop deep context fusion architectural technique propagates context information lowscale highscale patch hierarchical manner second accelerate training inference propose adaptive computation allocates network capacity computation towards coarse image detail resulting model set new stateoftheart fvd score inception score classconditional video generation surpassing recent method show rapidly finetuned base lowresolution generator highresolution time time texttovideo synthesis best knowledge model first diffusionbased architecture trained high resolution entirely endtoend project webpage httpssnapresearchgithubiohpdm
diving deep motion representation videotext model video informative image capture dynamic scene representing motion video capture dynamic activity work introduce generated motion description capture finegrained motion description activity apply three action datasets evaluated several videotext model task retrieval motion description found fall far behind human expert performance two action datasets raising question whether videotext model understand motion video address introduce method improving motion understanding videotext model utilizing motion description method prof effective two action datasets motion description retrieval task result draw attention need quality caption involving finegrained motion information existing datasets demonstrate effectiveness proposed pipeline understanding finegrained motion videotext retrieval
toolchain comprehensive audiovideo analysis using deep learning based multimodal approach use case riot violent context detection paper present toolchain comprehensive audiovideo analysis leveraging deep learning based multimodal approach end different specific task speech text acoustic scene classification asc acoustic event detection aed visual object detection vod image captioning ic video captioning vc conducted integrated toolchain combining individual task analyzing audio visual data extracted input video toolchain offer various audiovideobased application two general application audiovideo clustering comprehensive audiovideo summary specific application riot violent context detection furthermore toolchain present flexible adaptable architecture effective integrate new model audiovideobased application
multigranularity video object segmentation current benchmark video segmentation limited annotating salient object ie foreground instance despite impressive architectural design previous work trained benchmark struggled adapt realworld scenario thus developing new video segmentation dataset aimed tracking multigranularity segmentation target video scene necessary work aim generate multigranularity video segmentation dataset annotated salient nonsalient mask achieve propose largescale densely annotated multigranularity video object segmentation mugvos dataset includes various type granularity mask annotation automatically collected training set assist tracking salient nonsalient object also curated humanannotated test set reliable evaluation addition present memorybased mask propagation model mmpm trained evaluated mugvos dataset lead best performance among existing video object segmentation method segment sambased video segmentation method project page available httpscvlabkaistgithubiomugvos
motion control enhanced complex action video generation existing texttovideo model often struggle generating video sufficiently pronounced complex action key limitation lie text prompt inability precisely convey intricate motion detail address propose novel framework mvideo designed produce longduration video precise fluid action mvideo overcomes limitation text prompt incorporating mask sequence additional motion condition input providing clearer accurate representation intended action leveraging foundational vision model groundingdino mvideo automatically generates mask sequence enhancing efficiency robustness result demonstrate training mvideo effectively aligns text prompt motion condition produce video simultaneously meet criterion dual control mechanism allows dynamic video generation enabling alteration either text prompt motion condition independently tandem furthermore mvideo support motion condition editing composition facilitating generation video complex action mvideo thus advance motion generation setting strong benchmark improved action depiction current video diffusion model project page available
mimosa humanai cocreation computational spatial audio effect video spatial audio offer immersive video consumption experience viewer however creating editing spatial audio often expensive requires specialized equipment skill posing high barrier amateur video creator present mimosa humanai cocreation tool enables amateur user computationally generate manipulate spatial audio effect video monaural stereo audio mimosa automatically ground sound source corresponding sounding object visual scene enables user validate fix error location sounding object user also augment spatial audio effect flexibly manipulating sounding source position creatively customizing audio effect design mimosa exemplifies humanai collaboration approach instead utilizing stateof art endtoend blackbox ml model us multistep pipeline aligns interpretable intermediate result user workflow lab user study participant demonstrates mimosa usability usefulness expressiveness capability creating immersive spatial audio effect collaboration user
improving multicenter generalizability ganbased fat suppression using federated learning generative adversarial network ganbased synthesis fat suppressed f mri nonfs proton density sequence potential accelerate acquisition knee mri however gans trained singlesite data poor generalizability external data show federated learning improve multicenter generalizability gans synthesizing f mri facilitating privacypreserving multiinstitutional collaboration
splatter video video gaussian representation versatile processing video representation longstanding problem crucial various downstream task trackingdepth predictionsegmentationview synthesisand editing however current method either struggle model complex motion due absence structure rely implicit representation illsuited manipulation task address challenge introduce novel explicit representationvideo gaussian representation embeds video gaussians proposed representation model video appearance canonical space using explicit gaussians proxy associate gaussian motion video motion approach offer intrinsic explicit representation layered atlas volumetric pixel matrix obtain representation distill prior optical flow depth foundation model regularize learning illposed setting extensive application demonstrate versatility new video representation proven effective numerous video processing task including tracking consistent video depth feature refinement motion appearance editing stereoscopic video generation project page
intentiondriven egotoexo video generation egotoexo video generation refers generating corresponding exocentric video according egocentric video providing valuable application arvr embodied ai benefiting advancement diffusion model technique notable progress achieved video generation however existing method build upon spatiotemporal consistency assumption adjacent frame satisfied egotoexo scenario due drastic change view end paper proposes intentiondriven egotoexo video generation framework ide leverage action intention consisting human movement action description viewindependent representation guide video generation preserving consistency content motion specifically egocentric head trajectory first estimated multiview stereo matching crossview feature perception module introduced establish correspondence exo ego view guiding trajectory transformation module infer human fullbody movement head trajectory meanwhile present action description unit map action semantics feature space consistent exocentric image finally inferred human movement highlevel action description jointly guide generation exocentric motion interaction content ie corresponding optical flow occlusion map backward process diffusion model ultimately warping corresponding exocentric video conduct extensive experiment relevant dataset diverse exoego video pair ide outperforms stateoftheart model subjective objective assessment demonstrating efficacy egotoexo video generation
video editing via factorized diffusion distillation introduce emu video edit eve model establishes new stateofthe art video editing without relying supervised video editing data develop eve separately train image editing adapter video generation adapter attach texttoimage model align adapter towards video editing introduce new unsupervised distillation procedure factorized diffusion distillation procedure distills knowledge one teacher simultaneously without supervised data utilize procedure teach eve edit video jointly distilling knowledge precisely edit individual frame image editing adapter ii ensure temporal consistency among edited frame using video generation adapter finally demonstrate potential approach unlocking capability align additional combination adapter
tell hear see video audio generation text content visual audio scene multifaceted video paired various audio viceversa thereby videotoaudio generation task imperative introduce steering approach controlling generated audio videotoaudio generation wellestablished generative task existing method lack controllability work propose vatt multimodal generative framework take video optional text prompt input generates audio optional textual description audio framework two advantage videotoaudio generation process refined controlled via text complement context visual information ii model suggest audio generate video generating audio caption vatt consists two key module vatt converter llm finetuned instruction includes projection layer map video feature llm vector space vatt audio transformer generates audio token visual frame optional text prompt using iterative parallel decoding audio token converted waveform pretrained neural codec experiment show vatt compared existing videotoaudio generation method objective metric achieves competitive performance audio caption provided audio caption provided prompt vatt achieves even refined performance lowest kld score furthermore subjective study show vatt audio chosen preferred generated audio audio generated existing method vatt enables controllable videotoaudio generation text well suggesting text prompt video audio caption unlocking novel application textguided videotoaudio generation videotoaudio captioning
dynamic tryon taming video virtual tryon dynamic attention mechanism video tryon stand promising area tremendous realworld potential previous research video tryon primarily focused transferring product clothing image video simple human pose performing poorly complex movement better preserve clothing detail approach armed additional garment encoder resulting higher computational resource consumption primary challenge domain twofold leveraging garment encoders capability video tryon lowering computational requirement ensuring temporal consistency synthesis human body part especially rapid movement tackle issue propose novel video tryon framework based diffusion transformerdit named dynamic tryon reduce computational overhead adopt straightforward approach utilizing dit backbone garment encoder employing dynamic feature fusion module store integrate garment feature ensure temporal consistency human body part introduce limbaware dynamic attention module enforces dit backbone focus region human limb denoising process extensive experiment demonstrate superiority dynamic tryon generating stable smooth tryon result even video featuring complicated human posture
eva embodied world model future video anticipation world model integrate raw data various modality image language simulate comprehensive interaction world thereby displaying crucial role field like mixed reality robotics yet applying world model accurate video prediction quite challenging due complex dynamic intention various scene practice paper inspired human rethinking process decompose complex video prediction four metatasks enable world model handle issue finegrained manner alongside task introduce new benchmark named embodied video anticipation benchmark evabench provide wellrounded evaluation evabench focused evaluating video prediction ability human robot action presenting significant challenge language model generation model targeting embodied video prediction propose embodied video anticipator eva unified framework aiming video understanding generation eva integrates video generation model visual language model effectively combining reasoning capability highquality generation moreover enhance generalization framework tailordesigned multistage pretraining paradigm adaptatively ensemble lora produce highfidelity result extensive experiment evabench highlight potential eva significantly improve performance embodied scene paving way largescale pretrained model realworld prediction task
flexifilm long video generation flexible condition generating long consistent video emerged significant yet challenging problem existing diffusionbased video generation model derived image generation model demonstrate promising performance generating short video simple conditioning mechanism sampling strategyoriginally designed image generationcause severe performance degradation adapted long video generation result prominent temporal inconsistency overexposure thus work introduce flexifilm new diffusion model tailored long video generation framework incorporates temporal conditioner establish consistent relationship generation multimodal condition resampling strategy tackle overexposure empirical result demonstrate flexifilm generates long consistent video second length outperforming competitor qualitative quantitative analysis project page httpsyichengithubioflexifilmpage
video interpolation diffusion model present vidim generative model video interpolation creates short video given start end frame order achieve high fidelity generate motion unseen input data vidim us cascaded diffusion model first generate target video low resolution generate highresolution video conditioned lowresolution generated video compare vidim previous stateoftheart method video interpolation demonstrate work fail setting underlying motion complex nonlinear ambiguous vidim easily handle case additionally demonstrate classifierfree guidance start end frame conditioning superresolution model original highresolution frame without additional parameter unlocks highfidelity result vidim fast sample jointly denoises frame generated requires less billion parameter per diffusion model produce compelling result still enjoys scalability improved quality larger parameter count
ufo enhancing diffusionbased video generation uniform frame organizer recently diffusionbased video generation model achieved significant success however existing model often suffer issue like weak consistency declining image quality time overcome challenge inspired aesthetic principle propose noninvasive plugin called uniform frame organizer ufo compatible diffusionbased video generation model ufo comprises series adaptive adapter adjustable intensity significantly enhance consistency foreground background video improve image quality without altering original model parameter integrated training ufo simple efficient requires minimal resource support stylized training modular design allows combination multiple ufo enabling customization personalized video generation model furthermore ufo also support direct transferability across different model specification without need specific retraining experimental result indicate ufo effectively enhances video generation quality demonstrates superiority public video generation benchmark code publicly available httpsgithubcomdelongliubuptufo
realtime onestep diffusionbased expressive portrait video generation latent diffusion model made great stride generating expressive portrait video accurate lipsync natural motion single reference image audio input however model far realtime often requiring many sampling step take minute generate even one second videosignificantly limiting practical use introduce osalcm onestep avatar latent consistency model paving way realtime diffusionbased avatar method achieves comparable video quality existing method requires one sampling step making faster accomplish propose novel avatar discriminator design guide lipaudio consistency motion expressiveness enhance video quality limited sampling step additionally employ secondstage training architecture using editing finetuned method eft transforming video generation editing task training effectively address temporal gap challenge singlestep generation experiment demonstrate osalcm outperforms existing opensource portrait video generation model operating efficiently single sampling step
perceptual video quality assessment survey perceptual video quality assessment play vital role field video processing due existence quality degradation introduced various stage video signal acquisition compression transmission display advancement internet communication cloud service technology video content traffic growing exponentially emphasizes requirement accurate rapid assessment video quality therefore numerous subjective objective video quality assessment study conducted past two decade generic video specific video streaming usergenerated content ugc virtual augmented reality vr ar high frame rate hfr audiovisual etc survey provides uptodate comprehensive review video quality assessment study specifically first review subjective video quality assessment methodology database necessary validating performance video quality metric second objective video quality assessment algorithm general purpose surveyed concluded according methodology utilized quality measure third overview objective video quality assessment measure specific application emerging topic finally performance stateoftheart video quality assessment measure compared analyzed survey provides systematic overview classical work recent progress realm video quality assessment help researcher quickly access field conduct relevant research
faster projected gan towards faster fewshot image generation order solve problem long training time large consumption computing resource huge parameter amount gan network image generation paper proposes improved gan network model named faster projected gan based projected gan proposed network mainly focus improvement generator projected gan introducing depth separable convolution dsc number parameter projected gan reduced training speed accelerated memory saved experimental result show artpainting landscape fewshot image datasets speed increase memory saving achieved time fid loss less loss amount model parameter better controlled time significant training speed improvement achieved small sample image generation task special scene earthquake scene public datasets
idanimator zeroshot identitypreserving human video generation generating highfidelity human video specified identity attracted significant attention content generation community however existing technique struggle strike balance training efficiency identity preservation either requiring tedious casebycase finetuning usually missing identity detail video generation process study present textbfidanimator zeroshot humanvideo generation approach perform personalized video generation given single reference facial image without training idanimator inherits existing diffusionbased video generation backbone face adapter encode idrelevant embeddings learnable facial latent query facilitate extraction identity information video generation introduce idoriented dataset construction pipeline incorporates unified human attribute action captioning technique constructed facial image pool based pipeline random reference training strategy devised precisely capture idrelevant embeddings idpreserving loss thus improving fidelity generalization capacity model idspecific video generation extensive experiment demonstrate superiority idanimator generate personalized human video previous model moreover method highly compatible popular pretrained model like animatediff various community backbone model showing high extendability realworld application video generation identity preservation highly desired code checkpoint released httpsgithubcomidanimatoridanimator
wfvae enhancing video vae waveletdriven energy flow latent video diffusion model video variational autoencoder vae encodes video lowdimensional latent space becoming key component latent video diffusion model lvdms reduce model training cost however resolution duration generated video increase encoding cost video vaes becomes limiting bottleneck training lvdms moreover blockwise inference method adopted lvdms lead discontinuity latent space processing longduration video key addressing computational bottleneck lie decomposing video distinct component efficiently encoding critical information wavelet transform decompose video multiple frequencydomain component improve efficiency significantly thus propose wavelet flow vae wfvae autoencoder leverage multilevel wavelet transform facilitate lowfrequency energy flow latent representation furthermore introduce method called causal cache maintains integrity latent space blockwise inference compared stateoftheart video vaes wfvae demonstrates superior performance psnr lpips metric achieving higher throughput lower memory consumption maintaining competitive reconstruction quality code model available httpsgithubcompkuyuangroupwfvae
videoqa era llm empirical study video large language model videollms flourishing advanced many videolanguage task golden testbed video question answering videoqa play pivotal role videollm developing work conduct timely comprehensive study videollms behavior videoqa aiming elucidate success failure mode provide insight towards humanlike video understanding question answering analysis demonstrate videollms excel videoqa correlate contextual cue generate plausible response question varied video content however model falter handling video temporality reasoning temporal content ordering grounding qarelevant temporal moment moreover model behave unintuitively unresponsive adversarial video perturbation sensitive simple variation candidate answer question also necessarily generalize better finding demonstrate videollms qa capability standard condition yet highlight severe deficiency robustness interpretability suggesting urgent need rationale videollm developing
towards long video understanding via finedetailed video story generation long video understanding become critical task computer vision driving advancement across numerous application surveillance content retrieval existing video understanding method suffer two challenge dealing long video understanding intricate longcontext relationship modeling interference redundancy tackle challenge introduce finedetailed video story generation fdvs interprets long video detailed textual representation specifically achieve finegrained modeling longtemporal content propose bottomup video interpretation mechanism progressively interprets video content clip video avoid interference redundant information video introduce semantic redundancy reduction mechanism remove redundancy visual textual level method transforms long video hierarchical textual representation contain multigranularity information video representation fdvs applicable various task without finetuning evaluate proposed method across eight datasets spanning three task performance demonstrates effectiveness versatility method
subjective objective quality assessment method stereoscopic video visibility affecting distortion present two major contribution work create full hd resolution stereoscopic video dataset comprised reference distorted video test stimulus produced simulating five level fog haze ambiance pristine left right video sequence perform subjective analysis created video dataset viewer compute difference mean opinion score dmos quality representative dataset opinion unaware ou distortion unaware du video quality assessment model developed video construct cyclopean frame individual view video partition nonoverlapping block analyze natural scene statistic nss patch pristine test video empirically model nss feature univariate generalized gaussian distribution uggd compute uggd model parameter alpha beta multiple spatial scale multiple orientation spherical steerable pyramid decomposition show uggd parameter distortion discriminable perform multivariate gaussian mvg modeling pristine distorted video feature set compute corresponding mean vector covariance matrix mvg fit compute bhattacharyya distance measure mean vector covariance matrix estimate perceptual deviation test video pristine video set finally pool distance measure estimate overall quality score video performance proposed objective algorithm verified popular video datasets irccyn proposed vad stereo dataset algorithm delivers consistent performance across datasets show competitive performance offtheshelf image video quality assessment algorithm
gradientfree path integral control enhancing texttovideo generation large visionlanguage model diffusion model achieved impressive result generative task like texttoimage texttovideo synthesis however achieving accurate text alignment generation remains challenging due complex temporal dependency across frame existing reinforcement learning rlbased approach enhance text alignment often require differentiable reward function constrained limited prompt hindering scalability applicability paper propose novel gradientfree framework aligning generated video text prompt without requiring additional model training leveraging principle path integral control approximates guidance diffusion model using nondifferentiable reward function thereby enabling integration powerful blackbox large visionlanguage model lvlms reward model additionally framework support flexible ensembling multiple reward model including largescale imagebased model synergistically enhance alignment without incurring substantial computational overhead demonstrate significantly improves text alignment across various dimension enhances overall quality generated video
boximator generating rich controllable motion video synthesis generating rich controllable motion pivotal challenge video synthesis propose boximator new approach finegrained motion control boximator introduces two constraint type hard box soft box user select object conditional frame using hard box use either type box roughly rigorously define object position shape motion path future frame boximator function plugin existing video diffusion model training process preserve base model knowledge freezing original weight training control module address training challenge introduce novel selftracking technique greatly simplifies learning boxobject correlation empirically boximator achieves stateoftheart video quality fvd score improving two base model enhanced incorporating box constraint robust motion controllability validated drastic increase bounding box alignment metric human evaluation also show user favor boximator generation result base model
exploring pretrained texttovideo diffusion model referring video object segmentation paper explore visual representation produced pretrained texttovideo diffusion model video understanding task hypothesize latent representation learned pretrained generative model encapsulates rich semantics coherent temporal correspondence thereby naturally facilitating video understanding hypothesis validated classic referring video object segmentation rvos task introduce novel framework termed vdit tailored dedicatedly designed component built upon fixed pretrained model specifically vdit us textual information conditional input ensuring semantic consistency across time precise temporal instance matching incorporates image token supplementary textual input enriching feature set generate detailed nuanced mask besides instead using standard gaussian noise propose predict videospecific noise extra noise prediction module help preserve feature fidelity elevates segmentation quality extensive experiment surprisingly observe fixed generative diffusion model unlike commonly used video backbone eg video swin transformer pretrained discriminative imagevideo pretasks exhibit better potential maintain semantic alignment temporal consistency existing standard benchmark vdit achieves highly competitive result surpassing many existing stateoftheart method code available httpsgithubcombuxiangzhirenvdit
memo memoryguided diffusion expressive talking video generation recent advance video diffusion model unlocked new potential realistic audiodriven talking video generation however achieving seamless audiolip synchronization maintaining longterm identity consistency producing natural audioaligned expression generated talking video remain significant challenge address challenge propose memoryguided emotionaware diffusion memo endtoend audiodriven portrait animation approach generate identityconsistent expressive talking video approach built around two key module memoryguided temporal module enhances longterm identity consistency motion smoothness developing memory state store information longer past context guide temporal modeling via linear attention emotionaware audio module replaces traditional cross attention multimodal attention enhance audiovideo interaction detecting emotion audio refine facial expression via emotion adaptive layer norm extensive quantitative qualitative result demonstrate memo generates realistic talking video across diverse image audio type outperforming stateoftheart method overall quality audiolip synchronization identity consistency expressionemotion alignment
undive generalized underwater video enhancement using generative prior rise marine exploration underwater imaging gained significant attention research topic underwater video enhancement become crucial realtime computer vision task marine exploration however existing method focus enhancing individual frame neglect video temporal dynamic leading visually poor enhancement furthermore lack groundtruth reference limit use abundant available underwater video data many application address issue propose twostage framework enhancing underwater video first stage us denoising diffusion probabilistic model learn generative prior unlabeled data capturing robust descriptive feature representation second stage prior incorporated physicsbased image formulation spatial enhancement also enforcing temporal consistency video frame method enables realtime computationallyefficient processing highresolution underwater video lower resolution offer efficient enhancement presence diverse watertypes extensive experiment four datasets show approach generalizes well outperforms existing enhancement method code available githubcomsuhassrinathundive
multimodal emotion recognition fusing video semantic mooc learning scenario massive open online course mooc learning scenario semantic information instructional video crucial impact learner emotional state learner mainly acquire knowledge watching instructional video semantic information video directly affect learner emotional state however study paid attention potential influence semantic information instructional video learner emotional state deeply explore impact video semantic information learner emotion paper innovatively proposes multimodal emotion recognition method fusing video semantic information physiological signal generate video description pretrained large language model llm obtain highlevel semantic information instructional video using crossattention mechanism modal interaction semantic information fused eye movement photoplethysmography ppg signal obtain feature containing critical information three mode accurate recognition learner emotional state realized emotion classifier experimental result show method significantly improved emotion recognition performance providing new perspective efficient method emotion recognition research mooc learning scenario method proposed paper contributes deeper understanding impact instructional video learner emotional state also provides beneficial reference future research emotion recognition mooc learning scenario
reanimating image using neural representation dynamic stimulus computer vision model made incredible stride static image recognition still match human performance task require understanding complex dynamic motion notably true realworld scenario embodied agent face complex motionrich environment approach brainnrds brainneural representation dynamic stimulus leverage stateoftheart video diffusion model decouple static image representation motion generation enabling u utilize fmri brain activity deeper understanding human response dynamic visual stimulus conversely also demonstrate information brain representation motion enhance prediction optical flow artificial system novel approach lead four main finding visual motion represented finegrained objectlevel resolution optical flow decoded brain activity generated participant viewing video stimulus video encoders outperform imagebased model predicting videodriven brain activity braindecoded motion signal enable realistic video reanimation based initial frame video extend prior work achieve full video decoding videodriven brain activity brainnrds advance understanding brain represents spatial temporal information dynamic visual scene finding demonstrate potential combining brain imaging video diffusion model developing robust biologicallyinspired computer vision system show additional decoding encoding example site httpsbrainnrdsgithubio
mardini masked autoregressive diffusion video generation scale introduce mardini new family video diffusion model integrate advantage masked autoregression mar unified diffusion model dm framework mar handle temporal planning dm focus spatial generation asymmetric network design marbased planning model containing parameter generates planning signal masked frame using lowresolution input ii lightweight generation model us signal produce highresolution frame via diffusion denoising mardinis mar enables video generation conditioned number masked frame frame position single model handle video interpolation eg masking middle frame imagetovideo generation eg masking second frame onward video expansion eg masking half frame efficient design allocates computational resource lowresolution planning model making computationally expensive important spatiotemporal attention feasible scale mardini set new stateoftheart video interpolation meanwhile within inference step efficiently generates video par much expensive advanced imagetovideo model
matchdiffusion trainingfree generation matchcuts matchcuts powerful cinematic tool create seamless transition scene delivering strong visual metaphorical connection however crafting matchcuts challenging resourceintensive process requiring deliberate artistic planning matchdiffusion present first trainingfree method matchcut generation using texttovideo diffusion model matchdiffusion leverage key property diffusion model early denoising step define scene broad structure later step add detail guided insight matchdiffusion employ joint diffusion initialize generation two prompt shared noise aligning structure motion applies disjoint diffusion allowing video diverge introduce unique detail approach produce visually coherent video suited matchcuts user study metric demonstrate matchdiffusions effectiveness potential democratize matchcut creation
endora video generation model endoscopy simulator generative model hold promise revolutionizing medical education robotassisted surgery data augmentation machine learning despite progress generating medical image complex domain clinical video generation largely remained untappedthis paper introduces model innovative approach generate medical video simulate clinical endoscopy scene present novel generative model design integrates meticulously crafted spatialtemporal video transformer advanced vision foundation model prior explicitly modeling spatialtemporal dynamic video generation also pioneer first public benchmark endoscopy simulation video generation model adapting existing stateoftheart method endeavorendora demonstrates exceptional visual quality generating endoscopy video surpassing stateoftheart method extensive testing moreover explore endoscopy simulator empower downstream video analysis task even generate medical scene multiview consistency nutshell endora mark notable breakthrough deployment generative ai clinical endoscopy research setting substantial stage advance medical content generation detail please visit project page httpsendoramedvidgengithubio
draw audio leveraging multiinstruction videotoaudio synthesis foley term commonly used filmmaking referring addition daily sound effect silent film video enhance auditory experience videotoaudio particular type automatic foley task present inherent challenge related audiovisual synchronization challenge encompass maintaining content consistency input video generated audio well alignment temporal loudness property within video address issue construct controllable videotoaudio synthesis model termed draw audio support multiple input instruction drawn mask loudness signal ensure content consistency synthesized audio target video introduce maskattention module mam employ masked video instruction enable model focus region interest additionally implement timeloudness module tlm us auxiliary loudness signal ensure synthesis sound aligns video loudness temporal dimension furthermore extended largescale dataset named vggsoundcaption annotating caption prompt extensive experiment challenging benchmark across two largescale datasets verify draw audio achieves stateoftheart project page httpsyannqigithubiodrawanaudio
sora agi world model complete survey texttovideo generation evolution video generation text starting animating mnist number simulating physical world sora progressed breakneck speed past seven year often seen superficial expansion predecessor texttoimage generation model texttovideo generation model developed upon carefully engineered constituent systematically discus element consisting limited core building block vision language temporal supporting feature perspective contribution achieving world model employ prisma framework curate impactful research article renowned scientific database primarily studying video synthesis using text condition upon minute exploration manuscript observe texttovideo generation involves intricate technology beyond plain extension texttoimage generation additional review shortcoming soragenerated video pinpoint call indepth study various enabling aspect video generation dataset evaluation metric efficient architecture humancontrolled generation finally conclude study texttovideo generation may still infancy requiring contribution crossdiscipline research community towards advancement first step realize artificial general intelligence agi
video summarization using denoising diffusion probabilistic model video summarization aim eliminate visual redundancy retaining key part video construct concise comprehensive synopsis existing method use discriminative model predict importance score video frame however method susceptible annotation inconsistency caused inherent subjectivity different annotator annotating video paper introduce generative framework video summarization learns generate summary probability distribution perspective effectively reducing interference subjective annotation noise specifically propose novel diffusion summarization method based denoising diffusion probabilistic model ddpm learns probability distribution training data noise prediction generates summary iterative denoising method resistant subjective annotation noise less prone overfitting training data discriminative method strong generalization ability moreover facilitate training ddpm limited data employ unsupervised video summarization model implement earlier denoising process extensive experiment various datasets tvsum summe fpvsum demonstrate effectiveness method
generating video unposed internet photo address problem generating video unposed internet photo handful input image serve keyframes model interpolates simulate path moving camera given random image model ability capture underlying geometry recognize scene identity relate frame term camera position orientation reflects fundamental understanding structure scene layout however existing video model luma dream machine fail task design selfsupervised method take advantage consistency video variability multiview internet photo train scalable video model without annotation camera parameter validate method outperforms baseline term geometric appearance consistency also show model benefit application enable camera control gaussian splatting result suggest scale scenelevel learning using data video multiview internet photo
textanimator controllable visual text video generation video generation challenging yet pivotal task various industry gaming ecommerce advertising one significant unresolved aspect within effective visualization text within generated video despite progress achieved generation current method still effectively visualize text video directly mainly focus summarizing semantic scene information understanding depicting action recent advance imagelevel visual text generation show promise transitioning technique video domain face problem notably preserving textual fidelity motion coherence paper propose innovative approach termed textanimator visual text video generation textanimator contains text embedding injection module precisely depict structure visual text generated video besides develop camera control module text refinement module improve stability generated visual text controlling camera movement well motion visualized text quantitative qualitative experimental result demonstrate superiority approach accuracy generated visual text stateoftheart video generation method project page found httpslaulampaulgithubiotextanimatorhtml
uvcg leveraging temporal consistency universal video protection security risk aidriven video editing garnered significant attention although recent study indicate adding perturbation image protect malicious edits directly applying imagebased method perturb frame video becomes ineffective video editing technique leverage consistency interframe information restore individually perturbed content address challenge leverage temporal consistency video content propose straightforward efficient yet highly effective broadly applicable approach universal video consistency guard uvcg uvcg embeds content another videotarget video within protected video introducing continuous imperceptible perturbation ability force encoder editing model map continuous input misaligned continuous output thereby inhibiting generation video consistent intended textual prompt additionally leveraging similarity perturbation adjacent frame improve computational efficiency perturbation generation employing perturbationreuse strategy applied uvcg across various version latent diffusion model ldm assessed effectiveness generalizability across multiple ldmbased editing pipeline result confirm effectiveness transferability efficiency approach safeguarding video content unauthorized modification
towards better metric texttovideo generation generative model demonstrated remarkable capability synthesizing highquality text image video video generation contemporary texttovideo model exhibit impressive capability crafting visually stunning video nonetheless evaluating video pose significant challenge current research predominantly employ automated metric fvd clip score however metric provide incomplete analysis particularly temporal assessment video content thus rendering unreliable indicator true video quality furthermore user study potential reflect human perception accurately hampered timeintensive laborious nature outcome often tainted subjective bias paper investigate limitation inherent existing metric introduce novel evaluation pipeline texttovideo score metric integrates two pivotal criterion textvideo alignment scrutinizes fidelity video representing given text description video quality evaluates video overall production caliber mixture expert moreover evaluate proposed metric facilitate future improvement present tvge dataset collecting human judgement texttovideo generated video two criterion experiment tvge dataset demonstrate superiority proposed offering better metric texttovideo generation
videogpt integrating image video encoders enhanced video understanding building advance language model large multimodal model lmms contributed significant improvement video understanding current video lmms utilize advanced large language model llm rely either image video encoders process visual input limitation image encoders excel capturing rich spatial detail frame sequence lack explicit temporal context important video intricate action sequence hand video encoders provide temporal context often limited computational constraint lead processing sparse frame lower resolution resulting reduced contextual spatial understanding end introduce videogpt combine complementary benefit image encoder detailed spatial understanding video encoder global temporal context modeling model process video dividing smaller segment applies adaptive pooling strategy feature extracted image video encoders architecture showcase improved performance across multiple video benchmark including vcgbench mvbench zeroshot questionanswering develop videoinstruction set using novel semiautomatic annotation pipeline improves model performance additionally comprehensively evaluate video lmms present vcgbenchdiverse covering broad video category lifestyle sport science gaming surveillance video benchmark questionanswer pair evaluates generalization existing lmms dense video captioning spatial temporal understanding complex reasoning ensuring comprehensive assessment across diverse video type dynamic code httpsgithubcommbzuaioryxvideogptplus
motionaura generating highquality motion consistent video using discrete diffusion spatiotemporal complexity video data present significant challenge task compression generation inpainting present four key contribution address challenge spatiotemporal video processing first introduce mobile inverted vectorquantization variational autoencoder combine variational autoencoders vaes masked token modeling enhance spatiotemporal video compression model achieves superior temporal consistency stateoftheart sota reconstruction quality employing novel training strategy full frame masking second present motionaura texttovideo generation framework utilizes vectorquantized diffusion model discretize latent space capture complex motion dynamic producing temporally coherent video aligned text prompt third propose spectral transformerbased denoising network process video data frequency domain using fourier transform method effectively capture global context longrange dependency highquality video generation denoising lastly introduce downstream task sketch guided video inpainting task leverage lowrank adaptation lora parameterefficient finetuning model achieve sota performance range benchmark work offer robust framework spatiotemporal modeling userdriven video content manipulation release code datasets model opensource
towards understanding unsafe video generation video generation model vgms demonstrated capability synthesize highquality output important understand potential produce unsafe content violent terrifying video work provide comprehensive understanding unsafe video generation first confirm possibility model could indeed generate unsafe video choose unsafe content generation prompt collected lexica three opensource sota vgms generate unsafe video filtering duplicate poorly generated content created initial set unsafe video original pool video clustering thematic coding analysis generated video identify unsafe video category distortedweird terrifying pornographic violentbloody political irb approval recruit online participant help label generated video based annotation submitted participant identified unsafe video initial video set labeled information corresponding prompt created first dataset unsafe video generated vgms study possible defense mechanism prevent generation unsafe video existing defense method image generation focus filtering either input prompt output result propose new approach called latent variable defense lvd work within model internal sampling process lvd achieve defense accuracy reducing time computing resource sampling large number unsafe prompt
easycontrol transfer controlnet video diffusion controllable generation interpolation following advancement textguided image generation technology exemplified stable diffusion video generation gaining increased attention academic community however relying solely text guidance video generation serious limitation video contain much richer content image especially term motion information hardly adequately described plain text fortunately computer vision various visual representation serve additional control signal guide generation help signal video generation controlled finer detail allowing greater flexibility different application integrating various control however nontrivial paper propose universal framework called easycontrol propagating injecting condition feature condition adapter method enables user control video generation single condition map framework various condition including raw pixel depth hed etc integrated different unetbased pretrained video diffusion model low practical cost conduct comprehensive experiment public datasets quantitative qualitative result indicate method outperforms stateoftheart method easycontrol significantly improves various evaluation metric across multiple validation datasets compared previous work specifically sketchtovideo generation task easycontrol achieves improvement fvd respectively compared videocomposer fidelity model demonstrates powerful image retention ability resulting high fvd msrvtt compared imagetovideo model
dreamitate realworld visuomotor policy learning via video generation key challenge manipulation learning policy robustly generalize diverse visual environment promising mechanism learning robust policy leverage video generative model pretrained largescale datasets internet video paper propose visuomotor policy learning framework finetunes video diffusion model human demonstration given task test time generate example execution task conditioned image novel scene use synthesized execution directly control robot key insight using common tool allows u effortlessly bridge embodiment gap human hand robot manipulator evaluate approach four task increasing complexity demonstrate harnessing internetscale generative model allows learned policy achieve significantly higher degree generalization existing behavior cloning approach
zeroshot image conditioning texttovideo diffusion model textconditioned imagetovideo generation aim synthesize realistic video starting given image eg woman photo text description eg woman drinking water existing framework often require costly training videotext datasets specific model design text image conditioning paper propose zeroshot tuningfree method empowers pretrained texttovideo diffusion model conditioned provided image enabling generation without optimization finetuning introducing external module approach leverage pretrained diffusion foundation model generative prior guide video generation additional image input propose repeatandslide strategy modulates reverse denoising process allowing frozen diffusion model synthesize video framebyframe starting provided image ensure temporal continuity employ ddpm inversion strategy initialize gaussian noise newly synthesized frame resampling technique help preserve visual detail conduct comprehensive experiment domainspecific opendomain datasets consistently outperforms recent opendomain model furthermore show seamlessly extend task video infilling prediction provided image autoregressive design also support long video generation
aicl action incontext learning video diffusion model opendomain video generation model constrained scale training video datasets less common action still generated researcher explore video editing method achieve action generation editing spatial information action video however method mechanically generates identical action without understanding align characteristic opendomain scenario paper propose aicl empowers generative model ability understand action information reference video similar human incontext learning extensive experiment demonstrate aicl effectively capture action achieves stateoftheart generation performance across three typical video diffusion model five metric using randomly selected category nontraining datasets
magictime timelapse video generation model metamorphic simulator recent advance texttovideo generation achieved remarkable success synthesizing highquality general video textual description largely overlooked problem existing model adequately encoded physical knowledge real world thus generated video tend limited motion poor variation paper propose textbfmagictime metamorphic timelapse video generation model learns realworld physic knowledge timelapse video implement metamorphic generation first design magicadapter scheme decouple spatial temporal training encode physical knowledge metamorphic video transform pretrained model generate metamorphic video second introduce dynamic frame extraction strategy adapt metamorphic timelapse video wider variation range cover dramatic object metamorphic process thus embodying physical knowledge general video finally introduce magic textencoder improve understanding metamorphic video prompt furthermore create timelapse videotext dataset called textbfchronomagic specifically curated unlock metamorphic video generation ability extensive experiment demonstrate superiority effectiveness magictime generating highquality dynamic metamorphic video suggesting timelapse video generation promising path toward building metamorphic simulator physical world
semantically consistent video inpainting conditional diffusion model current stateoftheart method video inpainting typically rely optical flow attentionbased approach inpaint masked region propagating visual information across frame approach led significant progress standard benchmark struggle task require synthesis novel content present frame paper reframe video inpainting conditional generative modeling problem present framework solving problem conditional video diffusion model introduce inpaintingspecific sampling scheme capture crucial longrange dependency context devise novel method conditioning known pixel incomplete frame highlight advantage using generative approach task showing method capable generating diverse highquality inpaintings synthesizing new content spatially temporally semantically consistent provided context
videoelevator elevating video generation quality versatile texttoimage diffusion model texttoimage diffusion model demonstrated unprecedented capability creating realistic aesthetic image contrary texttovideo diffusion model still lag far behind frame quality text alignment owing insufficient quality quantity training video paper introduce videoelevator trainingfree plugandplay method elevates performance using superior capability different conventional sampling ie temporal spatial modeling videoelevator explicitly decomposes sampling step temporal motion refining spatial quality elevating specifically temporal motion refining us encapsulated enhance temporal consistency followed inverting noise distribution required spatial quality elevating harness inflated directly predict less noisy latent adding photorealistic detail conducted experiment extensive prompt combination various result show videoelevator improves performance baseline foundational also facilitates stylistic video synthesis personalized code available httpsgithubcomybybzhangvideoelevator
mofavideo controllable image animation via generative motion field adaption frozen imagetovideo diffusion model present mofavideo advanced controllable image animation method generates video given image using various additional controllable signal human landmark reference manual trajectory another even provided video combination different previous method work specific motion domain show weak control ability diffusion prior achieve goal design several domainaware motion field adapter ie mofaadapters control generated motion video generation pipeline mofaadapters consider temporal motion consistency video generate dense motion flow given sparse control condition first multiscale feature given image wrapped guided feature stable video diffusion generation naively train two motion adapter manual trajectory human landmark individually since contain sparse information control training mofaadapters different domain also work together controllable video generation project page
loong generating minutelevel long video autoregressive language model desirable challenging generate contentrich long video scale minute autoregressive large language model llm achieved great success generating coherent long sequence token domain natural language processing exploration autoregressive llm video generation limited generating short video several second work conduct deep analysis challenge prevent autoregressive llmbased video generator generating long video based observation analysis propose loong new autoregressive llmbased video generator generate minutelong video specifically model text token video token unified sequence autoregressive llm train model scratch propose progressive shorttolong training loss reweighting scheme mitigate loss imbalance problem long video training investigate inference strategy including video token reencoding sampling strategy diminish error accumulation inference proposed loong trained video extended generate minutelevel long video conditioned text prompt demonstrated result sample available httpsepiphqnygithubioloongvideo
generate scene evaluating improving texttovision generation scene graph programming dalle sora gained attention producing implausible image astronaut riding horse space despite proliferation texttovision model inundated internet synthetic visuals image asset current benchmark predominantly evaluate model realworld scene paired caption introduce generate scene framework systematically enumerates scene graph representing vast array visual scene spanning realistic imaginative composition generate scene leverage scene graph programming method dynamically constructing scene graph varying complexity structured taxonomy visual element taxonomy includes numerous object attribute relation enabling synthesis almost infinite variety scene graph using structured representation generate scene translates scene graph caption enabling scalable evaluation texttovision model standard metric conduct extensive evaluation across multiple texttoimage texttovideo model presenting key finding model performance find ditbackbone texttoimage model align closely input caption unetbackbone model texttovideo model struggle balancing dynamic consistency texttovideo model show notable gap human preference alignment demonstrate effectiveness generate scene conducting three practical application leveraging caption generated generate scene selfimproving framework model iteratively enhance performance using generated data distillation process transfer specific strength proprietary model opensource counterpart improvement content moderation identifying generating challenging synthetic data
omnidrag enabling motion control omnidirectional imagetovideo generation virtual reality gain popularity demand controllable creation immersive dynamic omnidirectional video odvs increasing previous texttoodv generation method achieve impressive result struggle content inaccuracy inconsistency due reliance solely textual input although recent motion control technique provide finegrained control video generation directly applying method odvs often result spatial distortion unsatisfactory performance especially complex spherical motion tackle challenge propose omnidrag first approach enabling scene objectlevel motion control accurate highquality omnidirectional imagetovideo generation building pretrained video diffusion model introduce omnidirectional control module jointly finetuned temporal attention layer effectively handle complex spherical motion addition develop novel spherical motion estimator accurately extract motioncontrol signal allows user perform dragstyle odv generation simply drawing handle target point also present new dataset named addressing scarcity odv data large scene object motion experiment demonstrate significant superiority omnidrag achieving holistic scenelevel finegrained objectlevel control odv generation project page available
multiscale temporal map diffusion model natural language video localization natural language video localization nlvl grounding phrase natural language description corresponding video segment complex yet critical task video understanding despite ongoing advancement many existing solution lack capability globally capture temporal dynamic video data study present novel approach nlvl aim address issue method involves direct generation global temporal map via conditional denoising diffusion process based input video language query main challenge inherent sparsity discontinuity temporal map devising diffusion decoder address challenge introduce multiscale technique develop innovative diffusion decoder approach effectively encapsulates interaction query video data across various time scale experiment charade didemo datasets underscore potency design
largescale videoaction dataset egocentric video generation video generation emerged promising tool world simulation leveraging visual data replicate realworld environment within context egocentric video generation center human perspective hold significant potential enhancing application virtual reality augmented reality gaming however generation egocentric video present substantial challenge due dynamic nature egocentric viewpoint intricate diversity action complex variety scene encountered existing datasets inadequate addressing challenge effectively bridge gap present first highquality dataset specifically curated egocentric video generation encompasses million egocentric video clip enriched detailed action annotation including finegrained kinematic control highlevel textual description ensure integrity usability dataset implement sophisticated data cleaning pipeline designed maintain frame consistency action coherence motion smoothness egocentric condition furthermore introduce egodreamer capable generating egocentric video driven simultaneously action description kinematic control signal dataset associated action annotation data cleansing metadata released advancement research egocentric video generation
multimodal fusion coherence modeling video topic segmentation video topic segmentation vt task segment video intelligible nonoverlapping topic facilitating efficient comprehension video content quick access specific content vt also critical various downstream video understanding task traditional vt method using shallow feature unsupervised approach struggle accurately discern nuance topical transition recently supervised approach achieved superior performance video action scene segmentation unsupervised approach work improve supervised vt thoroughly exploring multimodal fusion multimodal coherence modeling specifically enhance multimodal fusion exploring different architecture using crossattention mixture expert generally strengthen multimodality alignment fusion pretrain finetune model multimodal contrastive learning propose new pretraining task tailored vt task novel finetuning task enhancing multimodal coherence modeling vt evaluate proposed approach educational video form lecture due vital role topic segmentation educational video boosting learning experience additionally introduce largescale chinese lecture video dataset augment existing english corpus promoting research vt experiment english chinese lecture datasets demonstrate model achieves superior vt performance compared competitive unsupervised supervised baseline
driveeditor unified informationguided framework controllable object editing driving scene visioncentric autonomous driving system require diverse data robust training evaluation augmented manipulating object position appearance within existing scene capture recent advancement diffusion model shown promise video editing application object manipulation driving scenario remains challenging due imprecise positional control difficulty preserving highfidelity object appearance address challenge position appearance control introduce driveeditor diffusionbased framework object editing driving video driveeditor offer unified framework comprehensive object editing operation including repositioning replacement deletion insertion diverse manipulation achieved shared set varying input processed identical position control appearance maintenance module position control module project given bounding box preserving depth information hierarchically injects diffusion process enabling precise control object position orientation appearance maintenance module preserve consistent attribute single reference image employing threetiered approach lowlevel detail preservation highlevel semantic maintenance integration prior novel view synthesis model extensive qualitative quantitative evaluation nuscenes dataset demonstrate driveeditors exceptional fidelity controllability generating diverse driving scene edits well remarkable ability facilitate downstream task project page httpsyvanlianggithubiodriveeditor
latentreframe enabling camera control video diffusion model without training precise camera pose control crucial video generation diffusion model existing method require finetuning additional datasets containing paired video camera pose annotation dataintensive computationally costly disrupt pretrained model distribution introduce latentreframe enables camera control pretrained video diffusion model without finetuning unlike existing method latentreframe operates sampling stage maintaining efficiency preserving original model distribution approach reframes latent code video frame align input camera trajectory timeaware point cloud latent code inpainting harmonization refine model latent space ensuring highquality video generation experimental result demonstrate latentreframe achieves comparable superior camera control precision video quality trainingbased method without need finetuning additional datasets
mygo consistent controllable multiview driving video generation camera control highquality driving video generation crucial providing training data autonomous driving model however current generative model rarely focus enhancing camera motion control multiview task essential driving video generation therefore propose mygo endtoend framework video generation introducing motion onboard camera condition make progress camera controllability multiview consistency mygo employ additional plugin module inject camera parameter pretrained video diffusion model retains extensive knowledge pretrained model much possible furthermore use epipolar constraint neighbor view information generation process view enhance spatialtemporal consistency experimental result show mygo achieved stateoftheart result general cameracontrolled video generation multiview driving video generation task lay foundation accurate environment simulation autonomous driving project page
avid adapting video diffusion model world model largescale generative model achieved remarkable success number domain however sequential decisionmaking problem robotics actionlabelled data often scarce therefore scalingup foundation model decisionmaking remains challenge potential solution lie leveraging widelyavailable unlabelled video train world model simulate consequence action world model accurate used optimize decisionmaking downstream task imagetovideo diffusion model already capable generating highly realistic synthetic video however model actionconditioned powerful model closedsource mean finetuned work propose adapt pretrained video diffusion model actionconditioned world model without access parameter pretrained model approach avid train adapter small domainspecific dataset actionlabelled video avid us learned mask modify intermediate output pretrained model generate accurate actionconditioned video evaluate avid video game realworld robotics data show outperforms existing baseline diffusion model result demonstrate utilized correctly pretrained video model potential powerful tool embodied ai
cameractrl enabling camera control texttovideo generation controllability play crucial role video generation allows user create edit content precisely existing model however lack control camera pose serf cinematic language express deeper narrative nuance alleviate issue introduce cameractrl enabling accurate camera pose control video diffusion model approach explores effective camera trajectory parameterization along plugandplay camera pose control module trained top video diffusion model leaving module base model untouched moreover comprehensive study effect various training datasets conducted suggesting video diverse camera distribution similar appearance base model indeed enhance controllability generalization experimental result demonstrate effectiveness cameractrl achieving precise camera control different video generation model marking step forward pursuit dynamic customized video storytelling textual camera pose input
depthcrafter generating consistent long depth sequence openworld video estimating video depth openworld scenario challenging due diversity video appearance content motion camera movement length present depthcrafter innovative method generating temporally consistent long depth sequence intricate detail openworld video without requiring supplementary information camera pose optical flow generalization ability openworld video achieved training videotodepth model pretrained imagetovideo diffusion model meticulously designed threestage training strategy training approach enables model generate depth sequence variable length one time frame harvest precise depth detail rich content diversity realistic synthetic datasets also propose inference strategy process extremely long video segmentwise estimation seamless stitching comprehensive evaluation multiple datasets reveal depthcrafter achieves stateoftheart performance openworld video depth estimation zeroshot setting furthermore depthcrafter facilitates various downstream application including depthbased visual effect conditional video generation
videotoaudio generation semantic temporal alignment visual auditory perception two crucial way human experience world texttovideo generation made remarkable progress past year absence harmonious audio generated video limit broader application paper propose semantic temporal aligned videotoaudio approach enhances audio generation video extracting local temporal global semantic video feature combining refined video feature text crossmodal guidance address issue information redundancy video propose onset prediction pretext task local temporal feature extraction attentive pooling module global semantic feature extraction supplement insufficient semantic information video propose latent diffusion model texttoaudio prior initialization crossmodal guidance also introduce audioaudio align new metric assess audiotemporal alignment subjective objective metric demonstrate method surpasses existing videotoaudio model generating audio better quality semantic consistency temporal alignment ablation experiment validated effectiveness module audio sample available
adaptive caching faster video generation diffusion transformer generating temporallyconsistent highfidelity video computationally expensive especially longer temporal span morerecent diffusion transformer dit despite making significant headway context heightened challenge rely larger model heavier attention mechanism resulting slower inference speed paper introduce trainingfree method accelerate video dit termed adaptive caching adacache motivated fact video created equal meaning video require fewer denoising step attain reasonable quality others building cache computation diffusion process also devise caching schedule tailored video generation maximizing qualitylatency tradeoff introduce motion regularization moreg scheme utilize video information within adacache essentially controlling compute allocation based motion content altogether plugandplay contribution grant significant inference speedup eg opensora video generation without sacrificing generation quality across multiple video dit baseline
needle video haystack scalable synthetic evaluator video mllms video understanding crucial next step multimodal large language model mllms various benchmark introduced better evaluating mllms nevertheless current video benchmark still inefficient evaluating video model iterative development due high cost constructing datasets difficulty isolating specific skill paper propose videoniah video needle haystack benchmark construction framework synthetic video generation videoniah decouples video content queryresponses inserting unrelated visual needle original video framework automates generation queryresponse pair using predefined rule minimizing manual labor query focus specific aspect video understanding enabling skillspecific evaluation separation video content query also allow increased video variety evaluation across different length utilizing videoniah compile video benchmark vnbench includes task retrieval ordering counting evaluate three key aspect video understanding temporal perception chronological ordering spatiotemporal coherence conduct comprehensive evaluation proprietary opensource model uncovering significant difference video understanding capability across various task additionally perform indepth analysis test result model configuration based finding provide advice improving video mllm training offering valuable insight guide future research model development code data available
zeroshot video editing adaptive sliding score distillation rapidly evolving field texttovideo generation catalyzed renewed interest controllable video editing research application editing prompt guide diffusion model denoising gained prominence mirroring advancement image editing noisebased inference process inherently compromise original video integrity resulting unintended overediting temporal discontinuity address challenge study proposes novel paradigm videobased score distillation facilitating direct manipulation original video content specifically distinguishing imagebased score distillation propose adaptive sliding score distillation strategy incorporates global local video guidance reduce impact editing error combined proposed imagebased joint guidance mechanism ability mitigate inherent instability model singlestep sampling additionally design weighted attention fusion module preserve key feature original video avoid overediting extensive experiment demonstrate strategy effectively address existing challenge achieving superior performance compared current stateoftheart method
dive taming dino subjectdriven video editing building success diffusion model image generation editing video editing recently gained substantial attention however maintaining temporal consistency motion alignment still remains challenging address issue paper proposes dinoguided video editing dive framework designed facilitate subjectdriven editing source video conditioned either target text prompt reference image specific identity core dive lie leveraging powerful semantic feature extracted pretrained model implicit correspondence guide editing process specifically ensure temporal motion consistency dive employ dino feature align motion trajectory source video extensive experiment diverse realworld video demonstrate framework achieve highquality editing result robust motion consistency highlighting potential dino contribute video editing precise subject editing dive incorporates dino feature reference image pretrained texttoimage model learn lowrank adaptation loras effectively registering target subject identity project page httpsdinovideoeditinggithubio
compositional video generation llm director significant progress made texttovideo generation use powerful generative model largescale internet data however substantial challenge remain precisely controlling individual concept within generated video motion appearance specific character movement viewpoint work propose novel paradigm generates concept representation separately composes prior large language model llm diffusion model specifically given input textual prompt scheme consists three stage leverage llm director first decompose complex query several subprompts indicate individual concept within videotextiteg scene object motion let llm invoke pretrained expert model obtain corresponding representation concept compose representation prompt multimodal llm produce coarse guidance scale coordinate trajectory object make generated frame adhere natural image distribution leverage diffusion prior use score distillation sampling refine composition extensive experiment demonstrate method generate highfidelity video text diverse motion flexible control concept project page
learning multimodal forgery representation diffusion generated video detection large number synthesized video diffusion model pose threat information security authenticity leading increasing demand generated content detection however existing videolevel detection algorithm primarily focus detecting facial forgery often fail identify diffusiongenerated content diverse range semantics advance field video forensics propose innovative algorithm named multimodal detectionmmdet detecting diffusiongenerated video mmdet utilizes profound perceptual comprehensive ability large multimodal model lmms generating multimodal forgery representation mmfr lmms multimodal space enhancing ability detect unseen forgery content besides mmdet leverage inandacross frame attention iafa mechanism feature augmentation spatiotemporal domain dynamic fusion strategy help refine forgery representation fusion moreover construct comprehensive diffusion video dataset called diffusion video forensics dvf across wide range forgery video mmdet achieves stateoftheart performance dvf demonstrating effectiveness algorithm source code dvf available httpsgithubcomsparklexfantasymmdet
novel view extrapolation video diffusion prior field novel view synthesis made significant stride thanks development radiance field method however radiance field technique far better novel view interpolation novel view extrapolation synthesis novel view far beyond observed training view design viewextrapolator novel view synthesis approach leverage generative prior stable video diffusion svd realistic novel view extrapolation redesigning svd denoising process viewextrapolator refines artifactprone view rendered radiance field greatly enhancing clarity realism synthesized novel view viewextrapolator generic novel view extrapolator work different type rendering view rendered point cloud single view monocular video available additionally viewextrapolator requires finetuning svd making dataefficient computationefficient extensive experiment demonstrate superiority viewextrapolator novel view extrapolation project page urlhttpskunhaoliugithubioviewextrapolator
motrans customized motion transfer textdriven video diffusion model existing pretrained texttovideo model demonstrated impressive ability generating realistic video basic motion camera movement however model exhibit significant limitation generating intricate humancentric motion current effort primarily focus finetuning model small set video containing specific motion often fail effectively decouple motion appearance limited reference video thereby weakening modeling capability motion pattern end propose motrans customized motion transfer method enabling video generation similar motion new context specifically introduce multimodal large language model mllmbased recaptioner expand initial prompt focus appearance appearance injection module adapt appearance prior video frame motion modeling process complementary multimodal representation recaptioned prompt video frame promote modeling appearance facilitate decoupling appearance motion addition devise motionspecific embedding enhancing modeling specific motion experimental result demonstrate method effectively learns specific motion pattern singular multiple reference video performing favorably existing method customized video generation
vgtvp multimodal procedural planning via visually grounded textvideo prompting large language model llmbased agent shown promise procedural task potential multimodal instruction augmented text video assist user remains underexplored address gap propose visually grounded textvideo prompting vgtvp method novel llmempowered multimodal procedural planning mpp framework generates cohesive text video procedural plan given specified highlevel objective main challenge achieving textual visual informativeness temporal coherence accuracy procedural plan vgtvp leverage zeroshot reasoning capability llm videototext generation ability video captioning model texttovideo generation ability diffusion model vgtvp improves interaction modality proposing novel fusion captioning foc method using texttovideo bridge videototext bridge allow llm guide generation visuallygrounded text plan textualgrounded video plan address scarcity datasets suitable mpp curated new dataset called dailylife task procedural plan dailypp conduct comprehensive experiment benchmark evaluate human preference regarding textual visual informativeness temporal coherence plan accuracy vgtvp method outperforms unimodal baseline dailypp dataset
syncflow toward temporally aligned joint audiovideo generation text video audio closely correlated modality human naturally perceive together recent advancement enabled generation audio video text producing modality simultaneously still typically relies either cascaded process multimodal contrastive encoders approach however often lead suboptimal result due inherent information loss inference conditioning paper introduce syncflow system capable simultaneously generating temporally synchronized audio video text core syncflow proposed dualdiffusiontransformer ddit architecture enables joint video audio modelling proper information fusion efficiently manage computational cost joint audio video modelling syncflow utilizes multistage training strategy separate video audio learning joint finetuning empirical evaluation demonstrate syncflow produce audio video output correlated baseline method significantly enhanced audio quality audiovisual correspondence moreover demonstrate strong zeroshot capability syncflow including zeroshot videotoaudio generation adaptation novel video resolution without training
enhancing visual consistency imagetovideo generation imagetovideo generation aim use initial frame alongside text prompt create video sequence grand challenge generation maintain visual consistency throughout video existing method often struggle preserve integrity subject background style first frame well ensure fluid logical progression within video narrative mitigate issue propose diffusionbased method enhance visual consistency generation specifically introduce spatiotemporal attention first frame maintain spatial motion consistency noise initialization lowfrequency band first frame enhance layout consistency two approach enable generate highly consistent video also extend proposed approach show potential improve consistency autoregressive long video generation camera motion control verify effectiveness method propose comprehensive evaluation benchmark generation automatic human evaluation result demonstrate superiority existing method
controllable video compression multimodal generative model traditional neural video codecs commonly encounter limitation controllability generality ultralowbitrate coding scenario overcome challenge propose controllable video compression framework incorporating multimodal generative model framework utilizes semanticmotion composite strategy keyframe selection retain critical information keyframe corresponding video clip dialoguebased large multimodal model lmm approach extract hierarchical spatiotemporal detail enabling interframe intraframe representation improved video fidelity enhancing encoding interpretability employ conditional diffusionbased textguided keyframe compression method achieving high fidelity frame reconstruction decoding textual description derived lmms guide diffusion process restore original video content accurately experimental result demonstrate significantly outperforms stateoftheart vvc standard ultralow bitrate scenario particularly preserving semantic perceptual fidelity
repurposing pretrained video diffusion model eventbased video interpolation video frame interpolation aim recover realistic missing frame observed frame generating highframerate video lowframerate video however without additional guidance large motion frame make problem illposed eventbased video frame interpolation evfi address challenge using sparse hightemporalresolution event measurement motion guidance guidance allows evfi method significantly outperform frameonly method however date evfi method relied limited set paired eventframe training data severely limiting performance generalization capability work overcome limited data challenge adapting pretrained video diffusion model trained internetscale datasets evfi experimentally validate approach realworld evfi datasets including new one introduce method outperforms existing method generalizes across camera far better existing approach
put shoe lifting egocentric perspective exocentric video investigate exocentrictoegocentric crossview translation aim generate firstperson egocentric view actor based video recording capture actor thirdperson exocentric perspective end propose generative framework called decouples translation process two stage highlevel structure transformation explicitly encourages crossview correspondence exocentric egocentric view diffusionbased pixellevel hallucination incorporates hand layout prior enhance fidelity generated egocentric view pave way future advancement field curate comprehensive exotoego crossview translation benchmark consists diverse collection synchronized egoexo tabletop activity video pair sourced three public datasets aria pilot experimental result validate delivers photorealistic video result clear hand manipulation detail outperforms several baseline term synthesis quality generalization ability new action
temporally consistent object editing video using extended attention image generation editing seen great deal advancement rise largescale diffusion model allow user control different modality text mask depth map etc however controlled editing video still lag behind prior work area focused using diffusion model globally change style existing video hand many practical application editing localized part video critical work propose method edit video using pretrained inpainting image diffusion model systematically redesign forward path model replacing selfattention module extended version attention module creates framelevel dependency way ensure edited information consistent across video frame matter shape position masked area qualitatively compare result stateoftheart term accuracy several video editing task like object retargeting object replacement object removal task simulation demonstrate superior performance proposed strategy
lingen towards highresolution minutelength texttovideo generation linear computational complexity texttovideo generation enhances content creation highly computationally intensive computational cost diffusion transformer dit scale quadratically number pixel make minutelength video generation extremely expensive limiting existing model generating video second length propose linearcomplexity texttovideo generation lingen framework whose cost scale linearly number pixel first time lingen enables highresolution minutelength video generation single gpu without compromising quality replaces computationallydominant quadraticcomplexity block selfattention linearcomplexity block called mate consists mabranch tebranch mabranch target shorttolongrange correlation combining bidirectional block token rearrangement method rotary major scan review token developed long video generation tebranch novel temporal swin attention block focus temporal correlation adjacent token mediumrange token mate block address adjacency preservation issue mamba improves consistency generated video significantly experimental result show lingen outperforms dit win rate video quality flop latency reduction furthermore automatic metric human evaluation demonstrate yield comparable video quality stateoftheart model win rate respect lumalabs kling respectively pave way hourlength movie generation realtime interactive video generation provide video generation result example project website httpslineargengithubio
ssm meet video diffusion model efficient longterm video generation structured state space given remarkable achievement image generation diffusion model research community shown increasing interest extending model video generation recent diffusion model video generation predominantly utilized attention layer extract temporal feature however attention layer limited computational cost increase quadratically sequence length limitation present significant challenge generating longer video sequence using diffusion model overcome challenge propose leveraging statespace model ssms temporal feature extractor ssms eg mamba recently gained attention promising alternative due lineartime memory consumption relative sequence length line previous research suggesting using bidirectional ssms effective understanding spatial feature image generation found bidirectionality also beneficial capturing temporal feature video data rather relying traditional unidirectional ssms conducted comprehensive evaluation multiple longterm video datasets minerl navigate across various model size sequence frame ssmbased model require less memory achieve fvd attentionbased model moreover ssmbased model often deliver better performance comparable gpu memory usage code available
fréchet video motion distance metric evaluating motion consistency video significant advancement made video generative model recently unlike image generation video generation present greater challenge requiring generating highquality frame also ensuring temporal consistency across frame despite impressive progress research metric evaluating quality generated video especially concerning temporal motion consistency remains underexplored bridge research gap propose frechet video motion distance fvmd metric focus evaluating motion consistency video generation specifically design explicit motion feature based key point tracking measure similarity feature via frechet distance conduct sensitivity analysis injecting noise real video verify effectiveness fvmd carry largescale human study demonstrating metric effectively detects temporal noise aligns better human perception generated video quality existing metric additionally motion feature consistently improve performance video quality assessment vqa model indicating approach also applicable unary video quality evaluation code available
genrec unifying video generation recognition diffusion model video diffusion model able generate highquality video learning strong spatialtemporal prior largescale datasets paper aim investigate whether prior derived generative process suitable video recognition eventually joint optimization generation recognition building upon stable video diffusion introduce genrec first unified framework trained randomframe conditioning process learn generalized spatialtemporal representation resulting framework naturally support generation recognition importantly robust even visual input contain limited information extensive experiment demonstrate efficacy genrec recognition generation particular genrec achieves competitive recognition performance offering accuracy respectively genrec also performs best classconditioned imagetovideo generation achieving fvd score datasets furthermore genrec demonstrates extraordinary robustness scenario limited frame observed code available
shape motion reconstruction single video monocular dynamic reconstruction challenging longstanding vision problem due highly illposed nature task existing approach limited either depend template effective quasistatic scene fail model motion explicitly work introduce method capable reconstructing generic dynamic scene featuring explicit fullsequencelong motion casually captured monocular video tackle underconstrained nature problem two key insight first exploit lowdimensional structure motion representing scene motion compact set motion base point motion expressed linear combination base facilitating soft decomposition scene multiple rigidlymoving group second utilize comprehensive set datadriven prior including monocular depth map longrange track devise method effectively consolidate noisy supervisory signal resulting globally consistent representation dynamic scene experiment show method achieves stateoftheart performance longrange motion estimation novel view synthesis dynamic scene project page httpsshapeofmotiongithubio
akira augmentation kit ray optical video generation recent advance textconditioned video diffusion greatly improved video quality however method offer limited sometimes control user camera aspect including dynamic camera motion zoom distorted lens focus shift motion optical aspect crucial adding controllability cinematic element generation framework ultimately resulting visual content draw focus enhances mood guide emotion according filmmaker control paper aim close gap controllable video generation camera optic achieve propose akira augmentation kit ray novel augmentation framework build train camera adapter complex camera model existing video generation backbone enables finetuned control camera motion well complex optical parameter focal length distortion aperture achieve cinematic effect zoom fisheye effect bokeh extensive experiment demonstrate akiras effectiveness combining composing camera optic outperforming stateoftheart method work set new landmark controlled optically enhanced video generation paving way future optical video generation method
slowfastvgen slowfast learning actiondriven long video generation human being endowed complementary learning system bridge slow learning general world dynamic fast storage episodic memory new experience previous video generation model however primarily focus slow learning pretraining vast amount data overlooking fast learning phase crucial episodic memory storage oversight lead inconsistency across temporally distant frame generating longer video frame fall beyond model context window end introduce slowfastvgen novel dualspeed learning system actiondriven long video generation approach incorporates masked conditional video diffusion model slow learning world dynamic alongside inferencetime fast learning strategy based temporal lora module specifically fast learning process update temporal lora parameter based local input output thereby efficiently storing episodic memory parameter propose slowfast learning loop algorithm seamlessly integrates inner fast learning loop outer slow learning loop enabling recall prior multiepisode experience contextaware skill learning facilitate slow learning approximate world model collect largescale dataset video language action annotation covering wide range scenario extensive experiment show slowfastvgen outperforms baseline across various metric actiondriven video generation achieving fvd score compared maintaining consistency longer video average scene cut versus slowfast learning loop algorithm significantly enhances performance longhorizon planning task well project website httpsslowfastvgengithubio
soaf scene occlusionaware neural acoustic field paper tackle problem novel view audiovisual synthesis along arbitrary trajectory indoor scene given audiovideo recording known trajectory scene existing method often overlook effect room geometry particularly wall occlusion sound propagation making less accurate multiroom environment work propose new approach called scene occlusionaware acoustic field soaf accurate sound generation approach derives prior sound energy field using distanceaware parametric soundpropagation modelling transforms based scene transmittance learned input video extract feature local acoustic field centred around receiver using fibonacci sphere generate binaural audio novel view directionaware attention mechanism extensive experiment real dataset rwavs synthetic dataset soundspaces demonstrate method outperforms previous stateoftheart technique audio generation project page httpsgithubcomhuiyugaosoaf
lifting single image gaussians video generation prior singleimage reconstruction remains fundamental challenge computer vision due inherent geometric ambiguity limited viewpoint information recent advance latent video diffusion model lvdms offer promising prior learned largescale video data however leveraging prior effectively face three key challenge degradation quality across large camera motion difficulty achieving precise camera control geometric distortion inherent diffusion process damage consistency address challenge proposing framework effectively release lvdms generative prior ensuring consistency specifically design articulated trajectory strategy generate video frame decomposes video sequence large camera motion one controllable small motion use robust neural matching model ie calibrate camera pose generated frame produce corresponding point cloud finally propose distortionaware gaussian splatting representation learn independent distortion frame output undistorted canonical gaussians extensive experiment demonstrate achieves stateoftheart performance two challenging datasets ie llff tank temple generalizes well diverse inthewild image cartoon illustration complex realworld scene
timeseries initialization conditioning videoagnostic stabilization video superresolution using recurrent network recurrent neural network rnn video super resolution vsr generally trained randomly clipped cropped short video extracted original training video due various challenge learning rnns however since rnn optimized superresolve short video vsr long video degraded due domain gap preliminary experiment reveal degradation change depending video property video length dynamic avoid degradation paper proposes training strategy rnn vsr work efficiently stably independently video length dynamic proposed training strategy stabilizes vsr training vsr network various rnn hidden state changed depending video property since computing variety hidden state timeconsuming computational cost reduced reusing hidden state efficient training addition training stability improved framenumber conditioning experimental result demonstrate proposed method performed better base method video various length dynamic
towards holistic languagevideo representation language modelenhanced msrvideo text dataset robust holistic languagevideo representation key pushing video understanding forward despite improvement training strategy quality languagevideo dataset less attention current plain simple text description visualonly focus languagevideo task result limited capacity realworld natural language video retrieval task query much complex paper introduces method automatically enhance videolanguage datasets making modality contextaware sophisticated representation learning need hence helping downstream task multifaceted video captioning method capture entity action speech transcript aesthetic emotional cue providing detailed correlating information text side video side training also develop agentlike strategy using language model generate highquality factual textual description reducing human intervention enabling scalability method effectiveness improving languagevideo representation evaluated textvideo retrieval using msrvtt dataset several multimodal retrieval model
egocvr egocentric benchmark finegrained composed video retrieval composed video retrieval video textual description modifies video content provided input model aim retrieve relevant video modified content database video challenging task first step acquire largescale training datasets collect highquality benchmark evaluation work introduce egocvr new evaluation benchmark finegrained composed video retrieval using largescale egocentric video datasets egocvr consists query specifically focus highquality temporal video understanding find existing composed video retrieval framework achieve necessary highquality temporal video understanding task address shortcoming adapt simple trainingfree method propose generic reranking framework composed video retrieval demonstrate achieves strong result egocvr code benchmark freely available httpsgithubcomexplainablemlegocvr
videoorion tokenizing object dynamic video present videoorion video large language model videollm explicitly capture key semantic information video spatialtemporal dynamic object throughout video videoorion employ expert vision model extract object dynamic detectsegmenttrack pipeline encoding set object token aggregating spatialtemporal object feature method address persistent challenge videollms efficiently compressing highdimensional video data semantic token comprehensible llm compared prior method resort downsampling original video aggregating visual token using resamplers leading information loss entangled semantics videoorion offer natural efficient way derive compact disentangled semantic representation also enables explicit object modeling video content minimal computational cost moreover introduced object token naturally allow videoorion accomplish videobased referring task experimental result show videoorion learn make good use object token achieves competitive result general video question answering videobased referring benchmark
abductive egoview accident video understanding safe driving perception present mmau novel dataset multimodal accident video understanding mmau contains inthewild egoview accident video temporally aligned text description annotate million object box pair videobased accident reason covering accident category mmau support various accident understanding task particularly multimodal video diffusion understand accident causeeffect chain safe driving mmau present abductive accident video understanding framework safe driving perception adversasd adversasd performs video diffusion via objectcentric video diffusion oavd method driven abductive clip model model involves contrastive interaction loss learn pair cooccurrence normal nearaccident accident frame corresponding text description accident reason prevention advice accident category oavd enforces causal region learning fixing content original frame background video generation find dominant causeeffect chain certain accident extensive experiment verify abductive ability adversasd superiority oavd stateoftheart diffusion model additionally provide careful benchmark evaluation object detection accident reason answering since adversasd relies precise object accident reason information
cardiff video salient object ranking chain thought reasoning saliency prediction diffusion video saliency prediction aim identify region video attract human attention gaze driven bottomup feature video topdown process like memory cognition among topdown influence language play crucial role guiding attention shaping visual information interpreted existing method primarily focus modeling perceptual information neglecting reasoning process facilitated language ranking cue crucial outcome process practical guidance saliency prediction paper propose cardiff caption rank generate diffusion framework imitates process integrating multimodal large language model mllm grounding module diffusion model enhance video saliency prediction specifically introduce novel prompting method vsorcot video salient object ranking chain thought utilizes mllm grounding module caption video content infer salient object along ranking position process derives ranking map sufficiently leveraged diffusion model decode saliency map given video accurately extensive experiment show effectiveness vsorcot improving performance video saliency prediction proposed cardiff performs better stateoftheart model mv dataset demonstrates crossdataset capability dataset zeroshot evaluation
collaboratively selfsupervised video representation learning action recognition considering close connection action recognition human pose estimation design collaboratively selfsupervised video representation csvr learning framework specific action recognition jointly factoring generative pose prediction discriminative context matching pretext task specifically csvr consists three branch generative pose prediction branch discriminative context matching branch video generating branch among first one encodes dynamic motion feature utilizing conditionalgan predict human pose future frame second branch extract static context feature contrasting positive negative video feature iframe feature pair third branch designed generate current future video frame purpose collaboratively improving dynamic motion feature static context feature extensive experiment demonstrate method achieves stateoftheart performance multiple popular video datasets
multistage highaesthetic video generation growing demand highfidelity video generation textual description catalyzed significant research field work introduce integrates texttoimage model video motion generator reference image embedding module frame interpolation module endtoend video generation pipeline benefiting architecture design generate aesthetically pleasing highresolution video remarkable fidelity smoothness demonstrates superior performance leading texttovideo system runway pika morph moon valley stable video diffusion model via user evaluation large scale
savgbench benchmarking spatially aligned audiovideo generation work address lack multimodal generative model capable producing highquality video spatially aligned audio recent advancement generative model successful video generation often overlook spatial alignment audio visuals essential immersive experience tackle problem establish new research direction benchmarking spatially aligned audiovideo generation savg propose three key component benchmark dataset baseline metric introduce spatially aligned audiovisual dataset derived audiovisual dataset consisting multichannel audio video spatiotemporal annotation sound event propose baseline audiovisual diffusion model focused stereo audiovisual joint learning accommodate spatial sound finally present metric evaluate video spatial audio quality including new spatial audiovisual alignment metric experimental result demonstrates gap exist baseline model ground truth term video audio quality spatial alignment modality
highfrequency enhanced hybrid neural representation video compression neural representation video nerv simplified video codec process achieved swift decoding speed encoding video content neural network presenting promising solution video compression however existing work overlook crucial issue video reconstructed method lack highfrequency detail address problem paper introduces highfrequency enhanced hybrid neural representation network method focus leveraging highfrequency information improve synthesis fine detail network specifically design wavelet highfrequency encoder incorporates wavelet frequency decomposer wfd block generate highfrequency feature embeddings next design highfrequency feature modulation hfm block leverage extracted highfrequency embeddings enhance fitting process decoder finally refined harmonic decoder block dynamic weighted frequency loss reduce potential loss highfrequency information experiment bunny uvg datasets demonstrate method outperforms method showing notable improvement detail preservation compression performance
humanvbench exploring humancentric video understanding capability mllms synthetic benchmark data domain multimodal large language model mllms achieving humancentric video understanding remains formidable challenge existing benchmark primarily emphasize object action recognition often neglecting intricate nuance human emotion behavior speechvisual alignment within video content present humanvbench innovative benchmark meticulously crafted bridge gap evaluation video mllms humanvbench comprises carefully designed task explore two primary dimension inner emotion outer manifestation spanning static dynamic basic complex well singlemodal crossmodal aspect two advanced automated pipeline video annotation distractorincluded qa generation humanvbench utilizes diverse stateoftheart sota technique streamline benchmark data synthesis quality assessment minimizing human annotation dependency tailored humancentric multimodal attribute comprehensive evaluation across sota video mllms reveals notable limitation current performance especially crossmodal emotion perception underscoring necessity refinement toward achieving humanlike understanding humanvbench opensourced facilitate future advancement realworld application video mllms
depth video scalable synthetic data video depth estimation long hindered scarcity consistent scalable ground truth data leading inconsistent unreliable result paper introduce depth video model tackle challenge two key innovation first develop scalable synthetic data pipeline capturing realtime video depth data diverse virtual environment yielding video clip duration precise depth annotation second leverage powerful prior generative video diffusion model handle realworld video effectively integrating advanced technique rotary position encoding flow matching enhance flexibility efficiency unlike previous model limited fixedlength video sequence approach introduces novel mixedduration training strategy handle video varying length performs robustly across different frame rateseven single frame inference propose depth interpolation method enables model infer highresolution video depth across sequence frame model outperforms previous generative depth model term spatial accuracy temporal consistency code model weight opensourced
blended latent diffusion attention control realworld video editing due lack fully publicly available texttovideo model current video editing method tend build pretrained texttoimage generation model however still face grand challenge dealing local editing video temporal information first although existing method attempt focus local area editing predefined mask preservation outsidearea background nonideal due spatially entire generation frame addition specially providing mask user additional costly undertaking autonomous masking strategy integrated editing process desirable last least imagelevel pretrained model hasnt learned temporal information across frame video vital expressing motion dynamic paper propose adapt imagelevel blended latent diffusion model perform local video editing task specifically leverage ddim inversion acquire latents background latents instead randomly noised one better preserve background information input video introduce autonomous mask manufacture mechanism derived crossattention map diffusion step finally enhance temporal consistency across video frame transforming selfattention block unet temporalspatial block extensive experiment proposed approach demonstrates effectiveness different realworld video editing task
learning see dazzle machine vision susceptible laser dazzle intense laser light blind distort perception environment oversaturation permanent damage sensor pixel employ wavefrontcoded phase mask diffuse energy laser light introduce sandwich generative adversarial network sgan restore image complex image degradation varying laserinduced image saturation maskinduced image blurring unknown lighting condition various noise corruption sgan architecture combine discriminative generative method wrapping two gans around learnable image deconvolution module addition make use fourier feature representation reduce spectral bias neural network improve learning highfrequency image detail endtoend training includes realistic physicsbased synthesis large set training data publicly available image trained sgan suppress peak laser irradiance high time sensor saturation threshold point camera sensor may experience damage without mask trained model evaluated synthetic data set data collected laboratory proposed image restoration model quantitatively qualitatively outperforms stateoftheart method wide range scene content laser power incident laser angle ambient illumination strength noise characteristic
vgmshield mitigating misuse video generative model rapid advancement video generation people conveniently utilize video generation model create video tailored specific desire nevertheless also growing concern potential misuse creating disseminating false information work introduce vgmshield set three straightforward pioneering mitigation lifecycle fake video generation start textitfake video detection trying understand whether uniqueness generated video whether differentiate real video investigate textittracing problem map fake video back model generates towards propose leverage pretrained model focus spatialtemporal dynamic backbone identify inconsistency video experiment seven stateoftheart opensource model demonstrate current model still perfectly handle spatialtemporal relationship thus accomplish detection tracing nearly perfect accuracy furthermore anticipating future generative model improvement propose prevention method add invisible perturbation image make generated video look unreal together fake video detection tracing multifaceted set solution effectively mitigate misuse video generative model
vmas videotomusic generation via semantic alignment web music video present framework learning generate background music video input unlike existing work rely symbolic musical annotation limited quantity diversity method leverage largescale web video accompanied background music enables model learn generate realistic diverse music accomplish goal develop generative videomusic transformer novel semantic videomusic alignment scheme model us joint autoregressive contrastive learning objective encourages generation music aligned highlevel video content also introduce novel videobeat alignment scheme match generated music beat lowlevel motion video lastly capture finegrained visual cue video needed realistic background music generation introduce new temporal video encoder architecture allowing u efficiently process video consisting many densely sampled frame train framework newly curated discomv dataset consisting videomusic sample order magnitude larger prior datasets used video music generation method outperforms existing approach discomv musiccaps datasets according various music generation evaluation metric including human evaluation result available
largescale highquality multiracial human face video dataset generating talking face video various condition recently become highly popular research area within generative task however building highquality face video generation model requires wellperforming pretrained backbone key obstacle universal model fail adequately address existing work rely universal video image generation model optimize control mechanism neglect evident upper bound video quality due limited capability backbone result lack highquality human face video datasets work investigate unsatisfactory result related study gather trim existing public talking face video datasets additionally collect annotate largescale dataset resulting comprehensive highquality multiracial face collection named using dataset craft several effective pretrained backbone model face video generation specifically conduct experiment several wellestablished video generation model including texttovideo imagetovideo unconditional video generation various setting obtain corresponding performance benchmark compared trained public datasets demonstrate superiority dataset experiment also allow u investigate empirical strategy crafting domainspecific video generation task costeffective setting make curated dataset along pretrained talking face video generation model publicly available resource contribution hopefully advance research field
decof generated video detection via frame consistency first benchmark dataset escalating quality video generated advanced video generation method result new security challenge relevant research effort opensource dataset generated video detection generated video detection method proposed far end propose opensource dataset detection method generated video first time first propose scalable dataset consisting prompt covering various forgery target scene behavior action well various generation model different architecture generation method including popular commercial model like openais sora google veo second found via probing experiment spatial artifactbased detector lack generalizability hence propose simple yet effective textbfdetection model based textbfframe textbfconsistency textbfdecof focus temporal artifact eliminating impact spatial artifact feature learning extensive experiment demonstrate efficacy decof detecting video generated unseen video generation model confirm powerful generalizability across several commercially proprietary model code dataset released urlhttpsgithubcomwuwuwuyuedecof
image secretly last frame pseudo video diffusion model viewed special case hierarchical variational autoencoders hvaes shown profound success generating photorealistic image contrast standard hvaes often produce image inferior quality compared diffusion model paper hypothesize success diffusion model partly attributed additional selfsupervision information intermediate latent state provided corrupted image along original image form pseudo video based hypothesis explore possibility improving type generative model pseudo video specifically first extend given image generative model video generative model counterpart train video generative model pseudo video constructed applying data augmentation original image furthermore analyze potential issue firstorder markov data augmentation method typically used diffusion model propose use expressive data augmentation construct useful information pseudo video empirical result celeba datasets demonstrate improved image generation quality achieved additional selfsupervised information pseudo video
raformer redundancyaware transformer video wire inpainting video wire inpainting vwi prominent application video inpainting aimed flawlessly removing wire film tv series offering significant time labor saving compared manual framebyframe removal however wire removal pose greater challenge due wire longer slimmer object typically targeted general video inpainting task often intersecting people background object irregularly add complexity inpainting process recognizing limitation posed existing video wire datasets characterized small size poor quality limited variety scene introduce new vwi dataset novel mask generation strategy namely wire removal video dataset pseudo wireshaped pws mask dataset comprises video average length frame designed facilitate development efficacy inpainting model building upon research proposes redundancyaware transformer raformer method address unique challenge wire removal video inpainting unlike conventional approach indiscriminately process frame patch raformer employ novel strategy selectively bypass redundant part static background segment devoid valuable information inpainting core raformer redundancyaware attention raa module isolates accentuates essential content coarsegrained windowbased attention mechanism complemented soft feature alignment sfa module refines feature achieves endtoend feature alignment extensive experiment traditional video inpainting datasets proposed dataset demonstrate raformer outperforms stateoftheart method
interest summary queryfocused long video summarization generating concise informative video summary long video important yet subjective due varying scene importance user ability specify scene importance text query enhances relevance summary paper introduces approach queryfocused video summarization aiming align video summary closely user query end propose fully convolutional sequence network attention fcsnaqfvs novel approach designed task leveraging temporal convolutional attention mechanism model effectively extract highlight relevant content based userspecified query experimental validation benchmark dataset queryfocused video summarization demonstrates effectiveness approach
turbsegres segmentthenrestore pipeline dynamic video atmospheric turbulence tackling image degradation due atmospheric turbulence particularly dynamic environment remains challenge longrange imaging system existing technique primarily designed static scene scene small motion paper present first segmentthenrestore pipeline restoring video dynamic scene turbulent environment leverage mean optical flow unsupervised motion segmentation method separate dynamic static scene component prior restoration camera shake compensation segmentation introduce foregroundbackground enhancement leveraging statistic turbulence strength transformer model trained novel noisebased procedural turbulence generator fast dataset augmentation benchmarked existing restoration method approach restores geometric distortion enhances sharpness video make code simulator data publicly available advance field video restoration turbulence riponcsgithubioturbsegres
semisupervised video semantic segmentation using unreliable pseudo label pixellevel scene understanding one fundamental problem computer vision aim recognizing object class mask semantics pixel given image compared image scene parsing video scene parsing introduces temporal information effectively improve consistency accuracy predictionbecause realworld actually videobased rather static state paper adopt semisupervised video semantic segmentation method based unreliable pseudo label ensemble teacher network model student network model generate pseudo label retrain student network method achieves miou score development test final test respectively finally obtain place video scene parsing wild challenge cvpr
oneshot learning meet depth diffusion multiobject video creating editable video depict complex interaction multiple object various artistic style long challenging task filmmaking progress often hampered scarcity data set contain paired text description corresponding video showcase interaction paper introduces novel depthconditioning approach significantly advance field enabling generation coherent diverse video single textvideo pair using pretrained depthaware texttoimage model method finetunes pretrained model capture continuous motion employing customdesigned spatial temporal attention mechanism inference use ddim inversion provide structural guidance video generation innovative technique allows continuously controllable depth video facilitating generation multiobject interaction maintaining concept generation compositional strength original model across various artistic style photorealism animation impressionism
trajectory attention finegrained video motion control recent advancement video generation greatly driven video diffusion model camera motion control emerging crucial challenge creating viewcustomized visual content paper introduces trajectory attention novel approach performs attention along available pixel trajectory finegrained camera motion control unlike existing method often yield imprecise output neglect temporal correlation approach possesses stronger inductive bias seamlessly injects trajectory information video generation process importantly approach model trajectory attention auxiliary branch alongside traditional temporal attention design enables original temporal attention trajectory attention work synergy ensuring precise motion control new content generation capability critical trajectory partially available experiment camera motion control image video demonstrate significant improvement precision longrange consistency maintaining highquality generation furthermore show approach extended video motion control task firstframeguided video editing excels maintaining content consistency large spatial temporal range
openhumanvid largescale highquality dataset enhancing humancentric video generation recent advancement visual generation technology markedly increased scale availability video datasets crucial training effective video generation model however significant lack highquality humancentric video datasets present challenge progress field bridge gap introduce openhumanvid largescale highquality humancentric video dataset characterized precise detailed caption encompass human appearance motion state along supplementary human motion condition including skeleton sequence speech audio validate efficacy dataset associated training strategy propose extension existing classical diffusion transformer architecture conduct pretraining model proposed dataset finding yield two critical insight first incorporation largescale highquality dataset substantially enhances evaluation metric generated human video preserving performance general video generation task second effective alignment text human appearance human motion facial motion essential producing highquality video output based insight corresponding methodology straightforward extended network trained proposed dataset demonstrates obvious improvement generation humancentric video project page httpsfudangenerativevisiongithubioopenhumanvid
lost melody empirical observation texttovideo generation storytelling perspective texttovideo generation task witnessed notable progress generated outcome reflecting text prompt high fidelity impressive visual quality however current texttovideo generation model invariably focused conveying visual element single scene far indifferent another important potential medium namely storytelling paper examine texttovideo generation storytelling perspective hardly investigated make empirical remark spotlight limitation current texttovideo generation scheme also propose evaluation framework storytelling aspect video discus potential future direction
lifelong learning video diffusion model single video stream work demonstrates training autoregressive video diffusion model single continuous video stream possible remarkably also competitive standard offline training approach given number gradient step demonstration reveals main result achieved using experience replay retains subset preceding video stream also contribute three new single video generative modeling datasets suitable evaluating lifelong video model learning lifelong bouncing ball lifelong maze lifelong plaicraft dataset contains million consecutive frame synthetic environment increasing complexity
towards realistic driving simulation video generation model driving simulation essential developing realistic autonomous driving simulator despite advancement existing method generating driving scene significant challenge remain view transformation spatialtemporal dynamic modeling address limitation propose spatialtemporal simulation driving model reconstruct realworld scene design controllable generative network achieve simulation construct continuous point cloud scene using surroundview data autonomous vehicle decouples spatialtemporal relationship produce coherent keyframe video additionally leverage video generation model obtain photorealistic controllable driving simulation video perspective expand range view generation train vehicle motion video based decomposed camera pose enhancing modeling capability distant scene furthermore reconstruct vehicle camera trajectory integrate point across consecutive view enabling comprehensive scene understanding along temporal dimension following extensive multilevel scene training simulate desired viewpoint achieve deep understanding scene evolution static spatialtemporal condition compared existing method approach show promising performance multiview scene consistency background coherence accuracy contributes ongoing advancement realistic autonomous driving simulation code httpsgithubcomwzzhengstag
foundation model video understanding survey video foundation model vifms aim learn generalpurpose representation various video understanding task leveraging largescale datasets powerful model vifms achieve capturing robust generic feature video data survey analyzes video foundational model offering comprehensive overview benchmark evaluation metric across distinct video task categorized main category additionally offer indepth performance analysis model common video task categorize vifms three category imagebased vifms adapt existing image model video task videobased vifms utilize videospecific encoding method universal foundational model ufms combine multiple modality image video audio text etc within single framework comparing performance various vifms different task survey offer valuable insight strength weakness guiding future advancement video understanding analysis surprisingly reveals imagebased foundation model consistently outperform videobased model video understanding task additionally ufms leverage diverse modality demonstrate superior performance video task share comprehensive list vifms studied work
motionmaster trainingfree camera motion transfer video generation emergence diffusion model greatly propelled progress image video generation recently effort made controllable video generation including texttovideo generation video motion control among camera motion control important topic however existing camera motion control method rely training temporal camera module necessitate substantial computation resource due large amount parameter video generation model moreover existing method predefine camera motion type training limit flexibility camera control therefore reduce training cost achieve flexible camera control propose comd novel trainingfree video motion transfer model disentangles camera motion object motion source video transfer extracted camera motion new video first propose oneshot camera motion disentanglement method extract camera motion single source video separate moving object background estimate camera motion moving object region based motion background solving poisson equation furthermore propose fewshot camera motion disentanglement method extract common camera motion multiple video similar camera motion employ windowbased clustering technique extract common feature temporal attention map multiple video finally propose motion combination method combine different type camera motion together enabling model controllable flexible camera control extensive experiment demonstrate trainingfree approach effectively decouple cameraobject motion apply decoupled camera motion wide range controllable video generation task achieving flexible diverse camera motion control
largescale highquality dataset texttovideo generation texttovideo generation recently garnered significant attention thanks large multimodality model sora however generation still face two important challenge lacking precise open sourced highquality dataset previous popular video datasets eg either low quality large research institution therefore challenging crucial collect precise highquality textvideo pair generation ignoring fully utilize textual information recent method focused vision transformer using simple cross attention module video generation fall short thoroughly extracting semantic information text prompt address issue introduce precise highquality dataset expressive caption openscenario dataset contains million textvideo pair facilitating research generation furthermore curate video create advancing highdefinition video generation additionally propose novel multimodal video diffusion transformer mvdit capable mining structure information visual token semantic information text token extensive experiment ablation study verify superiority previous datasets effectiveness mvdit
mind time temporallycontrolled multievent video generation realworld video consist sequence event generating sequence precise temporal control infeasible existing video generator rely single paragraph text input tasked generating multiple event described using single prompt method often ignore event fail arrange correct order address limitation present mint multievent video generator temporal control key insight bind event specific period generated video allows model focus one event time enable timeaware interaction event caption video token design timebased positional encoding method dubbed rerope encoding help guide crossattention operation finetuning pretrained video diffusion transformer temporally grounded data approach produce coherent video smoothly connected event first time literature model offer control timing event generated video extensive experiment demonstrate mint outperforms existing commercial opensource model large margin
video generation replace cinematographer research cinematic language generated video recent advancement texttovideo generation leveraged diffusion model enhance visual coherence video generated textual description however research primarily focused object motion limited attention given cinematic language video crucial cinematographer convey emotion narrative pacing address limitation propose threefold approach enhance ability model generate controllable cinematic language specifically introduce cinematic language dataset encompasses shot framing angle camera movement enabling model learn diverse cinematic style building facilitate robust cinematic alignment evaluation present cameraclip model finetuned proposed dataset excels understanding complex cinematic language generated video provide valuable guidance multishot composition process finally propose cliplora costguided dynamic lora composition method facilitates smooth transition realistic blending cinematic language dynamically fusing multiple pretrained cinematic loras within single video experiment demonstrate cameraclip outperforms existing model assessing alignment cinematic language video achieving score additionally cliplora improves ability multishot composition potentially bridging gap automatically generated video shot professional cinematographer
stereocrafter diffusionbased generation long highfidelity stereoscopic monocular video paper present novel framework converting video immersive stereoscopic addressing growing demand content immersive experience leveraging foundation model prior approach overcomes limitation traditional method boost performance ensure highfidelity generation required display device proposed system consists two main step depthbased video splatting warping extracting occlusion mask stereo video inpainting utilize pretrained stable video diffusion backbone introduce finetuning protocol stereo video inpainting task handle input video varying length resolution explore autoregressive strategy tiled processing finally sophisticated data processing pipeline developed reconstruct largescale highquality dataset support training framework demonstrates significant improvement video conversion offering practical solution creating immersive content device like apple vision pro display summary work contributes field presenting effective method generating highquality stereoscopic video monocular input potentially transforming experience digital medium
stream spatiotemporal evaluation analysis metric video generative model image generative model made significant progress generating realistic diverse image supported comprehensive guidance various evaluation metric however current video generative model struggle generate even short video clip limited tool provide insight improvement current video evaluation metric simple adaptation image metric switching embeddings video embedding network may underestimate unique characteristic video analysis reveals widely used frechet video distance fvd stronger emphasis spatial aspect temporal naturalness video inherently constrained input size embedding network used limiting frame additionally demonstrates considerable instability diverges human evaluation address limitation propose stream new video evaluation metric uniquely designed independently evaluate spatial temporal aspect feature allows comprehensive analysis evaluation video generative model various perspective unconstrained video length provide analytical experimental evidence demonstrating stream provides effective evaluation tool visual temporal quality video offering insight area improvement video generative model best knowledge stream first evaluation metric separately assess temporal spatial aspect video code available
dvos selfsupervised densepattern video object segmentation video object segmentation approach primarily rely largescale pixelaccurate humanannotated datasets model development dense video object segmentation dvos scenario video frame encompasses hundred small dense partially occluded object accordingly laborintensive manual annotation even single frame often take hour hinders development dvos many application furthermore video dense pattern following large number object move different direction pose additional challenge address challenge proposed semiselfsupervised spatiotemporal approach dvos utilizing diffusionbased method multitask learning emulating real video optical flow simulating motion developed methodology synthesize computationally annotated video used training dvos model model performance improved utilizing weakly labeled computationally generated imprecise data demonstrate utility efficacy proposed approach developed dvos model wheat head segmentation handheld dronecaptured video capturing wheat crop field different location across various growth stage spanning heading maturity despite using manually annotated video frame proposed approach yielded highperforming model achieving dice score tested dronecaptured external test set showed efficacy proposed approach wheat head segmentation application extended crop dvos domain crowd analysis microscopic image analysis
freemask rethinking importance attention mask zeroshot video editing texttovideo diffusion model made remarkable advancement driven ability generate temporally coherent video research zeroshot video editing using fundamental model expanded rapidly enhance editing quality structural control frequently employed video editing among technique crossattention mask control stand effectiveness efficiency however crossattention mask naively applied video editing introduce artifact blurring flickering experiment uncover critical factor overlooked previous video editing research crossattention mask consistently clear vary model structure denoising timestep address issue propose metric mask matching cost mmc quantifies variability propose freemask method selecting optimal mask tailored specific video editing task using mmcselected mask improve masked fusion mechanism within comprehensive attention feature eg temp cross selfattention module approach seamlessly integrated existing zeroshot video editing framework better performance requiring control assistance parameter finetuning enabling adaptive decoupling unedited semantic layout mask precision control extensive experiment demonstrate freemask achieves superior semantic fidelity temporal consistency editing quality compared stateoftheart method
vividface diffusionbased hybrid framework highfidelity video face swapping video face swapping becoming increasingly popular across various application yet existing method primarily focus static image struggle video face swapping temporal consistency complex scenario paper present first diffusionbased framework specifically designed video face swapping approach introduces novel imagevideo hybrid training framework leverage abundant static image data temporal video sequence addressing inherent limitation videoonly training framework incorporates specially designed diffusion model coupled vidfacevae effectively process type data better maintain temporal coherence generated video disentangle identity pose feature construct attributeidentity disentanglement triplet aidt dataset triplet three face image two image sharing pose two sharing identity enhanced comprehensive occlusion augmentation dataset also improves robustness occlusion additionally integrate reconstruction technique input conditioning network handling large pose variation extensive experiment demonstrate framework achieves superior performance identity preservation temporal consistency visual quality compared existing method requiring fewer inference step approach effectively mitigates key challenge video face swapping including temporal flickering identity preservation robustness occlusion pose variation
selfcalibrating novel view synthesis monocular video using gaussian splatting gaussian splatting g significantly elevated scene reconstruction efficiency novel view synthesis nv accuracy compared neural radiance field nerf particularly dynamic scene however current nv method whether based g nerf primarily rely camera parameter provided colmap even utilize sparse point cloud generated colmap initialization lack accuracy well timeconsuming sometimes result poor dynamic scene representation especially scene large object movement extreme camera condition eg small translation combined large rotation study simultaneously optimize estimation camera parameter scene supervised additional information like depth optical flow etc obtained offtheshelf model using unverified information ground truth reduce robustness accuracy frequently occur long monocular video eg hundred frame propose novel approach learns highfidelity g scene representation selfcalibration camera parameter includes extraction point feature robustly represent structure use subsequent joint optimization camera parameter structure towards overall scene optimization demonstrate accuracy time efficiency method extensive quantitative qualitative experimental result several standard benchmark result show significant improvement stateoftheart method novel view synthesis source code released soon
editboard towards comprehensive evaluation benchmark textbased video editing model rapid development diffusion model significantly advanced aigenerated content aigc particularly texttoimage texttovideo generation textbased video editing leveraging generative capability emerged promising field enabling precise modification video based text prompt despite proliferation innovative video editing model conspicuous lack comprehensive evaluation benchmark holistically assess model performance across various dimension existing evaluation limited inconsistent typically summarizing overall performance single score obscures model effectiveness individual editing task address gap propose editboard first comprehensive evaluation benchmark textbased video editing model editboard encompasses nine automatic metric across four dimension evaluating model four task category introducing three new metric assess fidelity taskoriented benchmark facilitates objective evaluation detailing model performance providing insight model strength weakness opensourcing editboard aim standardize evaluation advance development robust video editing model
data collectionfree masked video modeling pretraining video transformer generally requires large amount data presenting significant challenge term data collection cost concern related privacy licensing inherent bias synthesizing data one promising way solve issue yet pretraining solely synthetic data challenge paper introduce effective selfsupervised learning framework video leverage readily available less costly static image specifically define pseudo motion generator pmg module recursively applies image transformation generate pseudomotion video image pseudomotion video leveraged masked video modeling approach applicable synthetic image well thus entirely freeing video pretraining data collection cost concern real data experiment action recognition task demonstrate framework allows effective learning spatiotemporal feature pseudomotion video significantly improving existing method also use static image partially outperforming using real synthetic video result uncover fragment video transformer learn masked video modeling
virbo multimodal multilingual avatar video generation digital marketing widespread popularity internet celebrity marketing world short video production gradually become popular way presenting product information however traditional video production industry usually includes series procedure script writing video filming professional studio video clipping special effect rendering customized postprocessing forth mention multilingual video accessible could speak multilingual language complicated procedure usually need professional team complete made short video production costly time money paper present intelligent system support automatic generation talking avatar video namely virbo simply userspecified script virbo could use deep generative model generate target talking video meanwhile system also support multimodal input customize video specified face specified voice special effect system also integrated multilingual customization module support generate multilingual talking avatar video batch hundred delicate template creative special effect series user study demo test found virbo generate talking avatar video maintained high quality video professional team reducing entire production cost significantly intelligent system effectively promote video production industry facilitate internet marketing neglecting language barrier cost challenge
learning summarize video generating caption rapid growth video data internet video summarization becoming important ai technology however due high labelling cost video summarization existing study conducted smallscale datasets leading limited performance generalization capacity work introduce use dense video caption supervision signal train video summarization model motivated propose model learns summarize video generating caption exploit dense video caption annotation weaklysupervised approach allows u train model largescale dense video caption datasets achieve better performance generalization capacity improve generalization capacity introduce clip strong visionlanguage model prior mechanism enhance learning important object caption may ignore video practice perform zeroshot video summarization finetuned groundtruth summary video caption target dataset examine performance weaklysupervised finetuning video caption propose two new datasets tvsumcaption summecaption derived two common video summarization datasets publicly released conduct extensive experiment result demonstrate method achieves significant improvement performance generalization capacity compared previous method
learning actionable discrete diffusion policy via largescale actionless video pretraining learning generalist embodied agent capable completing multiple task pose challenge primarily stemming scarcity actionlabeled robotic datasets contrast vast amount human video exist capturing intricate task interaction physical world promising prospect arise utilizing actionless human video pretraining transferring knowledge facilitate robot policy learning limited robot demonstration however remains challenge due domain gap human robot moreover difficult extract useful information representing dynamic world human video noisy multimodal data structure paper introduce novel framework tackle challenge leverage unified discrete diffusion combine generative pretraining human video policy finetuning small number actionlabeled robot video start compressing human robot video unified video token pretraining stage employ discrete diffusion model maskandreplace diffusion strategy predict future video token latent space finetuning stage harness imagined future video guide lowlevel action learning limited set robot data experiment demonstrate method generates highfidelity future video planning enhances finetuned policy compared previous stateoftheart approach superior performance project website available httpsvideodiffgithubio
moonshot towards controllable video generation editing multimodal condition existing video diffusion model vdms limited mere text condition thereby usually lacking control visual appearance geometry structure generated video work present moonshot new video generation model condition simultaneously multimodal input image text model builts upon core module called multimodal video block mvb consists conventional spatialtemporal layer representing video feature decoupled crossattention layer address image text input appearance conditioning addition carefully design model architecture optionally integrate pretrained image controlnet module geometry visual condition without needing extra training overhead opposed prior method experiment show versatile multimodal conditioning mechanism moonshot demonstrates significant improvement visual quality temporal consistency compared existing model addition model easily repurposed variety generative application personalized video generation image animation video editing unveiling potential serve fundamental architecture controllable video generation model made public httpsgithubcomsalesforcelavis
tvg trainingfree transition video generation method diffusion model transition video play crucial role medium production enhancing flow coherence visual narrative traditional method like morphing often lack artistic appeal require specialized skill limiting effectiveness recent advance diffusion modelbased video generation offer new possibility creating transition face challenge poor interframe relationship modeling abrupt content change propose novel trainingfree transition video generation tvg approach using videolevel diffusion model address limitation without additional training method leverage gaussian process regression mathcalgpr model latent representation ensuring smooth dynamic transition frame additionally introduce interpolationbased conditional control frequencyaware bidirectional fusion fbif architecture enhance temporal control transition reliability evaluation benchmark datasets custom image pair demonstrate effectiveness approach generating highquality smooth transition video code provided httpssobeymilgithubiotvgcom
dreamphysics learning physicsbased dynamic video diffusion prior dynamic interaction attracting lot attention recently however creating content remains challenging one solution animate scene physicsbased simulation requires manually assigning precise physical property object simulated result would become unnatural another solution learn deformation object distillation video generative model however tends produce video small discontinuous motion due inappropriate extraction application physic prior work combine strength complementing shortcoming two solution propose learn physical property material field video diffusion prior utilize physicsbased materialpointmethod mpm simulator generate content realistic motion particular propose motion distillation sampling emphasize video motion information distillation addition facilitate optimization propose kanbased material field frame boosting experimental result demonstrate method enjoys realistic motion stateofthearts
neural graph matching video retrieval largescale videodriven ecommerce rapid development short video industry traditional ecommerce encountered new paradigm videodriven ecommerce leverage attractive video product showcase provides video item service user benefitting dynamic visualized introduction itemsvideodriven ecommerce shown huge potential stimulating consumer confidence promoting sale paper focus video retrieval task facing following challenge howto handle heterogeneity among user item video mine complementarity item video better user understanding paper first leverage dual graph model coexisting uservideo useritem interaction videodriven ecommerce innovatively reduce user preference understanding graph matching problem solve propose novel bilevel graph matching networkgmn mainly consists node preferencelevel graph matching given user nodelevel graph matching aim match video item preferencelevel graph matching aim match multiple user preference extracted video item proposed gmn generate improve user embedding aggregating matched node preference dual graph bilevel manner comprehensive experiment show superiority proposed gmn significant improvement stateoftheart approach eg developed wellknown videodriven ecommerce platform serving hundred million user every day
youtube video analytics patient engagement evidence colonoscopy preparation video video effective way deliver contextualized justintime medical information patient education however video analysis topic identification retrieval extraction analysis medical information understandability patient perspective extremely challenging task study demonstrates data analysis pipeline utilizes method retrieve medical information youtube video preparing colonoscopy exam much maligned disliked procedure patient find challenging get adequately prepared first use youtube data api collect metadata desired video select search keywords use google video intelligence api analyze text frame object data annotate youtube video material medical information video understandability overall recommendation develop bidirectional long shortterm memory bilstm model identify medical term video build three classifier group video based level encoded medical information video understandability whether video recommended study provides healthcare stakeholder guideline scalable approach generating new educational video content enhance management vast number health condition
agentbased video trimming information becomes accessible usergenerated video increasing length placing burden viewer sift vast content valuable insight trend underscore need algorithm extract key video information efficiently despite significant advancement highlight detection moment retrieval video summarization current approach primarily focus selecting specific time interval often overlooking relevance segment potential segment arranging paper introduce novel task called video trimming vt focus detecting wasted footage selecting valuable segment composing final video coherent story address task propose agentbased video trimming avt structured three phase video structuring clip filtering story composition specifically employ video captioning agent convert video slice structured textual description filtering module dynamically discard lowquality footage based structured information clip video arrangement agent select compile valid clip coherent final narrative evaluation develop video evaluation agent assess trimmed video conducting assessment parallel human evaluation additionally curate new benchmark dataset video trimming using raw user video internet result avt received favorable evaluation user study demonstrated superior map precision youtube highlight tvsum dataset highlight detection task code model available httpsylingfenggithubioavt
annotated biomedical video generation using denoising diffusion probabilistic model flow field segmentation tracking living cell play vital role within biomedical domain particularly cancer research drug development developmental biology usually tedious timeconsuming task traditionally done biomedical expert recently automatize process deep learning based segmentation tracking method proposed method require largescale datasets full potential constrained scarcity annotated data biomedical imaging domain address limitation propose biomedical video diffusion model bvdm capable generating realisticlooking synthetic microscopy video trained single real video bvdm generate video arbitrary length pixellevel annotation used training datahungry model composed denoising diffusion probabilistic model ddpm generating highfidelity synthetic cell microscopy image flow prediction model fpm predicting nonrigid transformation consecutive video frame inference initially ddpm imposes realistic cell texture synthetic cell mask generated based real data statistic flow prediction model predicts flow field consecutive mask applies ddpm output previous time frame create next one keeping temporal consistency bvdm outperforms stateoftheart synthetic live cell microscopy video generation model furthermore demonstrate sufficiently large synthetic dataset enhances performance cell segmentation tracking model compared using limited amount available real data
improved video vae latent video diffusion model variational autoencoder vae aim compress pixel data lowdimensional latent space playing important role openais sora latent video diffusion generation model existing video vaes inflate pretrained image vae causal structure temporalspatial compression paper present two astonishing finding initialization welltrained image vae latent dimension suppresses improvement subsequent temporal compression capability adoption causal reasoning lead unequal information interaction unbalanced performance frame alleviate problem propose keyframebased temporal compression ktc architecture group causal convolution gcconv module improve video vae ivvae specifically ktc architecture divide latent space two branch one half completely inherits compression prior keyframes lowerdimension image vae half involves temporal compression group causal convolution reducing temporalspatial conflict accelerating convergence speed video vae gcconv half us standard convolution within frame group ensure interframe equivalence employ causal logical padding group maintain flexibility processing variable frame video extensive experiment five benchmark demonstrate sota video reconstruction generation capability proposed ivvae
sectorshaped diffusion model video generation diffusion model achieved great success image generation however leveraging idea video generation face significant challenge maintaining consistency continuity across video frame mainly caused lack effective framework align frame video desired temporal feature preserving consistent semantic stochastic feature work propose novel sectorshaped diffusion model whose sectorshaped diffusion region formed set rayshaped reverse diffusion process starting noise point generate group intrinsically related data sharing semantic stochastic feature varying temporal feature appropriate guided condition apply video generation task explore use optical flow temporal condition experimental result show outperforms many existing method task video generation without temporalfeature modelling module texttovideo generation task temporal condition explicitly given propose twostage generation strategy decouple generation temporal feature semanticcontent feature show without additional training model integrated another temporal condition generative model still achieve comparable performance existing work result viewd
mastering trajectory multientity motion video generation paper aim manipulate multientity motion video generation previous method controllable video generation primarily leverage control signal manipulate object motion achieved remarkable synthesis result however control signal inherently limited expressing nature object motion overcome problem introduce robust controller regulates multientity dynamic space given userdesired pose location rotation sequence entity core approach plugandplay grounded object injector fuse multiple input entity respective trajectory gated selfattention mechanism addition exploit injector architecture preserve video diffusion prior crucial generalization ability mitigate video quality degradation introduce domain adaptor training employ annealed sampling strategy inference address lack suitable training data construct dataset first correlate collected human animal asset gptgenerated trajectory capture motion evenlysurround camera diverse ue platform extensive experiment show set new stateoftheart accuracy generalization controlling multientity motion project page
vidgpt introducing gptstyle autoregressive generation video diffusion model advance diffusion model today video generation achieved impressive quality generating temporal consistent long video still challenging majority video diffusion model vdms generate long video autoregressive manner ie generating subsequent clip conditioned last frame previous clip however existing approach involve bidirectional computation restricts receptive context autoregression step result model lacking longterm dependency inspired huge success large language model llm following gpt generative pretrained transformer bring causal ie unidirectional generation vdms use past frame prompt generate future frame causal generation introduce causal temporal attention vdm force generated frame depend previous frame frame prompt inject conditional frame concatenating noisy frame frame generated along temporal axis consequently present video diffusion gpt vidgpt based two key design autoregression step able acquire longterm context prompting frame concatenated previously generated frame additionally bring kvcache mechanism vdms eliminates redundant computation overlapped frame significantly boosting inference speed extensive experiment demonstrate vidgpt achieves stateoftheart performance quantitatively qualitatively long video generation code available httpsgithubcomdawnlxcausalvideogen
physgen rigidbody physicsgrounded imagetovideo generation present physgen novel imagetovideo generation method convert single image input condition eg force torque applied object image produce realistic physically plausible temporally consistent video key insight integrate modelbased physical simulation datadriven video generation process enabling plausible imagespace dynamic heart system three core component image understanding module effectively capture geometry material physical parameter image ii imagespace dynamic simulation model utilizes rigidbody physic inferred parameter simulate realistic behavior iii imagebased rendering refinement module leverage generative video diffusion produce realistic video footage featuring simulated motion resulting video realistic physic appearance even precisely controllable showcasing superior result existing datadriven imagetovideo generation work quantitative comparison comprehensive user study physgens resulting video used various downstream application turning image realistic animation allowing user interact image create various dynamic project page httpsstevenlswgithubiophysgen
reducio generating video within second using extremely compressed motion latents commercial video generation model exhibited realistic highfidelity result still restricted limited access one crucial obstacle largescale application expensive training inference cost paper argue video contain much redundant information image thus encoded motion latents based content image towards goal design imageconditioned vae encode video extremely compressed motion latent space magic reducio charm enables reduction latents compared common vae without sacrificing quality training diffusion model compact representation easily allows generating resolution video adopt twostage video generation paradigm performs texttoimage textimagetovideo sequentially extensive experiment show reduciodit achieves strong performance evaluation though trained limited gpu resource importantly method significantly boost efficiency video ldms training inference train reduciodit around training hour total generate video clip within second single gpu code released httpsgithubcommicrosoftreduciovae
ytcommentqa video question answerability instructional video instructional video provide detailed howto guide various task viewer often posing question regarding content addressing question vital comprehending content yet receiving immediate answer difficult numerous computational model developed video question answering video qa task primarily trained question generated based video content aiming produce answer within content however realworld situation user may pose question go beyond video informational boundary highlighting necessity determine video provide answer discerning whether question answered video content challenging due multimodal nature video visual verbal information intertwined bridge gap present ytcommentqa dataset contains naturallygenerated question youtube categorized answerability required modality answer visual script experiment answerability classification task demonstrate complexity ytcommentqa emphasize need comprehend combined role visual script information video reasoning dataset available httpsgithubcomlgresearchytcommentqa
learning video representation without natural video show useful video representation learned synthetic video natural image without incorporating natural video training propose progression video datasets synthesized simple generative process model growing set natural video property eg motion acceleration shape transformation downstream performance video model pretrained generated datasets gradually increase dataset progression videomae model pretrained synthetic video close performance gap action classification training scratch selfsupervised pretraining natural video outperforms pretrained model introducing crop static image pretraining stage result similar performance pretraining outperforms pretrained model outofdistribution datasets analyzing lowlevel property datasets identify correlation frame diversity frame similarity natural data downstream performance approach provides controllable transparent alternative video data curation process pretraining
sokbench situated video reasoning benchmark aligned openworld knowledge learning commonsense reasoning visual context scene realworld crucial step toward advanced artificial intelligence however existing video reasoning benchmark still inadequate since mainly designed factual situated reasoning rarely involve broader knowledge real world work aim delve deeper reasoning evaluation specifically within dynamic openworld structured context knowledge propose new benchmark sokbench consisting question situation instancelevel annotation depicted video reasoning process required understand apply situated knowledge general knowledge problemsolving create dataset propose automatic scalable generation method generate questionanswer pair knowledge graph rationale instructing combination llm mllms concretely first extract observable situated entity relation process video situated knowledge extend openworld knowledge beyond visible content task generation facilitated multiple dialogue iteration subsequently corrected refined designed selfpromptings demonstration corpus explicit situated fact implicit commonsense generate associated questionanswer pair reasoning process finally followed manual review quality assurance evaluated recent mainstream large visionlanguage model benchmark found several insightful conclusion information please refer benchmark wwwbobbywucomsokbench
crossmodal angiography video generation static fundus photography clinical knowledge guidance fundus fluorescein angiography ffa critical tool assessing retinal vascular dynamic aiding diagnosis eye disease however invasive nature less accessibility compared color fundus cf image pose significant challenge current cf ffa translation method limited static generation work pioneer dynamic ffa video generation static cf image introduce autoregressive gan smooth memorysaving framebyframe ffa synthesis enhance focus dynamic lesion change ffa region design knowledge mask based clinical experience leveraging mask approach integrates innovative knowledge maskguided technique including knowledgeboosted attention knowledgeaware discriminator maskenhanced patchnce loss aimed refining generation critical area addressing pixel misalignment challenge method achieves best fvd psnr compared common video generation approach human assessment ophthalmologist confirms high generation quality notably knowledge mask surpasses supervised lesion segmentation mask offering promising noninvasive alternative traditional ffa research clinical application code available
generative inbetweening adapting imagetovideo model keyframe interpolation present method generating video sequence coherent motion pair input key frame adapt pretrained largescale imagetovideo diffusion model originally trained generate video moving forward time single input image key frame interpolation ie produce video two input frame accomplish adaptation lightweight finetuning technique produce version model instead predicts video moving backwards time single input image model along original forwardmoving model subsequently used dualdirectional diffusion sampling process combine overlapping model estimate starting two keyframes experiment show method outperforms existing diffusionbased method traditional frame interpolation technique
medical imaging complexity effect gan performance proliferation machine learning model diverse clinical application led growing need highfidelity medical image training data data often scarce due cost constraint privacy concern alleviating burden medical image synthesis via generative adversarial network gans emerged powerful method synthetically generating photorealistic image based existing set real medical image however exact image set size required efficiently train gan unclear work experimentally establish benchmark measure relationship sample dataset size fidelity generated image given datasets distribution image complexity analyze statistical metric based delentropy image complexity measure rooted shannon entropy information theory pipeline conduct experiment two stateoftheart gans stylegan spadegan trained multiple medical imaging datasets variable sample size across gans general performance improved increasing training set size suffered increasing complexity
deep generative model medical image synthesis deep generative modeling emerged powerful tool synthesizing realistic medical image driving advance medical image analysis disease diagnosis treatment planning chapter explores various deep generative model medical image synthesis focus variational autoencoders vaes generative adversarial network gans denoising diffusion model ddms discus fundamental principle recent advance well strength weakness model examine application clinically relevant problem including unconditional conditional generation task like imagetoimage translation image reconstruction additionally review commonly used evaluation metric assessing image fidelity diversity utility privacy provide overview current challenge field
cogvideox texttovideo diffusion model expert transformer present cogvideox largescale texttovideo generation model based diffusion transformer generate continuous video aligned text prompt frame rate fps resolution pixel previous video generation model often limited movement short duration difficult generate video coherent narrative based text propose several design address issue first propose variational autoencoder vae compress video along spatial temporal dimension improve compression rate video fidelity second improve textvideo alignment propose expert transformer expert adaptive layernorm facilitate deep fusion two modality third employing progressive training multiresolution frame pack technique cogvideox adept producing coherent longduration different shape video characterized significant motion addition develop effective textvideo data processing pipeline includes various data preprocessing strategy video captioning method greatly contributing generation quality semantic alignment result show cogvideox demonstrates stateoftheart performance across multiple machine metric human evaluation model weight causal vae video caption model cogvideox publicly available httpsgithubcomthudmcogvideo
gaussian splatting decoder generative adversarial network nerfbased generative adversarial network gans like giraffe shown high rendering quality large representational variety however rendering neural radiance field pose challenge application first significant computational demand nerf rendering preclude use lowpower device mobile vrar headset second implicit representation based neural network difficult incorporate explicit scene vr environment video game gaussian splatting overcomes limitation providing explicit representation rendered efficiently high frame rate work present novel approach combine high rendering quality nerfbased gans flexibility computational advantage training decoder map implicit nerf representation explicit gaussian splatting attribute integrate representational diversity quality gans ecosystem gaussian splatting first time additionally approach allows high resolution gan inversion realtime gan editing gaussian splatting scene project page
distilling visionlanguage model million video recent advance visionlanguage model largely attributed abundance imagetext data aim replicate success videolanguage model simply enough humancurated videotext data available thus resort finetuning videolanguage model strong imagelanguage baseline synthesized instructional data resulting video model videoinstructiontuning viit used autolabel million video generate highquality caption show adapted videolanguage model performs well wide range videolanguage benchmark instance surpasses best prior result openended nextqa besides model generates detailed description previously unseen video provide better textual supervision existing method experiment show videolanguage dualencoder model contrastively trained autogenerated caption better strongest baseline also leverage visionlanguage model best model outperforms stateoftheart method msrvtt zeroshot texttovideo retrieval side product generate largest video caption dataset date
singer vivid audiodriven singing video generation multiscale spectral diffusion model recent advancement generative model significantly enhanced talking face video generation yet singing video generation remains underexplored difference human talking singing limit performance existing talking face video generation model applied singing fundamental difference talking singingspecifically audio characteristic behavioral expressionslimit effectiveness existing model observe difference singing talking audio manifest term frequency amplitude address designed multiscale spectral module help model learn singing pattern spectral domain additionally develop spectralfiltering module aid model learning human behavior associated singing audio two module integrated diffusion model enhance singing video generation performance resulting proposed model singer furthermore lack highquality realworld singing face video hindered development singing video generation community address gap collected inthewild audiovisual singing dataset facilitate research area experiment demonstrate singer capable generating vivid singing video outperforms stateoftheart method objective subjective evaluation
sonicvisionlm playing sound vision language model growing interest task generating sound silent video primarily practicality streamlining video postproduction however existing method videosound generation attempt directly create sound visual representation challenging due difficulty aligning visual representation audio representation paper present sonicvisionlm novel framework aimed generating wide range sound effect leveraging visionlanguage modelsvlms instead generating audio directly video use capability powerful vlms provided silent video approach first identifies event within video using vlm suggest possible sound match video content shift approach transforms challenging task aligning image audio wellstudied subproblems aligning imagetotext texttoaudio popular diffusion model improve quality audio recommendation llm collected extensive dataset map text description specific sound effect developed timecontrolled audio adapter approach surpasses current stateoftheart method converting video audio enhancing synchronization visuals improving alignment audio video component project page httpsyusiissygithubiosonicvisionlmgithubio
syncammaster synchronizing multicamera video generation diverse viewpoint recent advancement video diffusion model shown exceptional ability simulating realworld dynamic maintaining consistency progress inspires u investigate potential model ensure dynamic consistency across various viewpoint highly desirable feature application virtual filming unlike existing method focused multiview generation single object reconstruction interest lie generating openworld video arbitrary viewpoint incorporating dof camera pose achieve propose plugandplay module enhances pretrained texttovideo model multicamera video generation ensuring consistent content across different viewpoint specifically introduce multiview synchronization module maintain appearance geometry consistency across viewpoint given scarcity highquality training data design hybrid training scheme leverage multicamera image monocular video supplement unreal enginerendered multicamera video furthermore method enables intriguing extension rerendering video novel viewpoint also release multiview synchronized video dataset named syncamvideodataset project page httpsjianhongbaigithubiosyncammaster
motionbridge dynamic video inbetweening flexible control generating plausible smooth transition two image frame video inbetweening essential tool video editing long video synthesis traditional work lack capability generate complex large motion recent video generation technique powerful creating highquality result often lack fine control detail intermediate frame lead result align creative mind introduce motionbridge unified video inbetweening framework allows flexible control including trajectory stroke keyframes mask guide pixel text however learning multimodal control unified framework challenging task thus design two generator extract control signal faithfully encode feature dualbranch embedders resolve ambiguity introduce curriculum training strategy smoothly learn various control extensive qualitative quantitative experiment demonstrated multimodal control enable dynamic customizable contextually accurate visual narrative
objectcentric diffusion efficient video editing diffusionbased video editing reached impressive quality transform either global style local structure attribute given video input following textual edit prompt however solution typically incur heavy memory computational cost generate temporallycoherent frame either form diffusion inversion andor crossframe attention paper conduct analysis inefficiency suggest simple yet effective modification allow significant speedup whilst maintaining quality moreover introduce objectcentric diffusion fix generation artifact reduce latency allocating computation towards foreground edited region arguably important perceptual quality achieve two novel proposal objectcentric sampling decoupling diffusion step spent salient background region spending former ii objectcentric token merging reduces cost crossframe attention fusing redundant token unimportant background region technique readily applicable given video editing model without retraining drastically reduce memory computational cost evaluate proposal inversionbased controlsignalbased editing pipeline show latency reduction comparable synthesis quality project page qualcommairesearchgithubioobjectcentricdiffusion
genuine knowledge practice diffusion testtime adaptation video adverse weather removal realworld vision task frequently suffer appearance unexpected adverse weather condition including rain haze snow raindrop last decade convolutional neural network vision transformer yielded outstanding result singleweather video removal however due absence appropriate adaptation fail generalize weather condition although viwsnet proposed remove adverse weather condition video single set pretrained weight seriously blinded seen weather traintime degenerate coming unseen weather testtime work introduce testtime adaptation adverse weather removal video propose first framework integrates testtime adaptation iterative diffusion reverse process specifically devise diffusionbased network novel temporal noise model efficiently explore framecorrelated information degraded video clip training stage inference stage introduce proxy task named diffusion tubelet selfcalibration learn primer distribution test video stream optimize model approximating temporal noise model online adaptation experimental result benchmark datasets demonstrate testtime adaptation method diffusionbased networkdifftta outperforms stateoftheart method term restoring video degraded seen weather condition generalizable capability also validated unseen weather condition synthesized realworld video
mmego towards building egocentric multimodal llm research aim comprehensively explore building multimodal foundation model egocentric video understanding achieve goal work three front first lack qa data egocentric video understanding develop data engine efficiently generates highquality qa sample egocentric video ranging second one hour long based humanannotated data currently largest egocentric qa dataset second contribute challenging egocentric qa benchmark video question evaluate model ability recognizing memorizing visual detail across video varying length introduce new debiasing evaluation method help mitigate unavoidable language bias present model evaluated third propose specialized multimodal architecture featuring novel memory pointer prompting mechanism design includes global glimpse step gain overarching understanding entire video identify key visual information followed fallback step utilizes key visual information generate response enables model effectively comprehend extended video content data benchmark model successfully build mmego egocentric multimodal llm show powerful performance egocentric video understanding
emotion future motion simulation via event sequence diffusion forecasting typical object future motion critical task interpreting interacting dynamic environment computer vision eventbased sensor could capture change scene exceptional temporal granularity may potentially offer unique opportunity predict future motion level detail precision previously unachievable inspired propose integrate strong learning capacity video diffusion model rich motion information event camera motion simulation framework specifically initially employ pretrained stable video diffusion model adapt event sequence dataset process facilitates transfer extensive knowledge rgb video eventcentric domain moreover introduce alignment mechanism utilizes reinforcement learning technique enhance reverse generation trajectory diffusion model ensuring improved performance accuracy extensive testing validation demonstrate effectiveness method various complex scenario showcasing potential revolutionize motion flow prediction computer vision application autonomous vehicle guidance robotic navigation interactive medium finding suggest promising direction future research enhancing interpretative power predictive accuracy computer vision system
instructional video generation despite recent stride video generation stateoftheart method still struggle element visual detail one particularly challenging case class egocentric instructional video intricate motion hand coupled mostly stable nondistracting environment necessary convey appropriate visual action instruction address challenge introduce new method instructional video generation diffusionbased method incorporates two distinct innovation first propose automatic method generate expected region motion guided visual context action text second introduce critical hand structure loss guide diffusion model focus smooth consistent hand pose evaluate method augmented instructional datasets based epickitchens demonstrating significant improvement stateoftheart method term instructional clarity especially hand motion target region across diverse environment action video result found httpsexcitedbuttergithubioinstructionalvideogeneration
surgsora decoupled rgbdflow diffusion model controllable surgical video generation medical video generation transformative potential enhancing surgical understanding pathology insight precise controllable visual representation however current model face limitation controllability authenticity bridge gap propose surgsora motioncontrollable surgical video generation framework us single input frame usercontrollable motion cue surgsora consists three key module dual semantic injector dsi extract objectrelevant rgb depth feature input frame integrates segmentation cue capture detailed spatial feature complex anatomical structure decoupled flow mapper dfm fuse optical flow semanticrgbd feature multiple scale enhance temporal understanding object spatial dynamic trajectory controller tc allows user specify motion direction estimate sparse optical flow guiding video generation process fused feature used condition frozen stable diffusion model produce realistic temporally coherent surgical video extensive evaluation demonstrate surgsora outperforms stateoftheart method controllability authenticity showing potential advance surgical video generation medical education training research
uncovering hidden subspace video diffusion model using reidentification latent video diffusion model easily deceive casual observer domain expert alike thanks produced image quality temporal consistency beyond entertainment creates opportunity around safe data sharing fully synthetic datasets crucial healthcare well domain relying sensitive personal information however privacy concern approach fully addressed yet model trained synthetic data specific downstream task still perform worse trained real data discrepancy may partly due sampling space subspace training video effectively reducing training data size downstream model additionally reduced temporal consistency generating long video could contributing factor paper first show training privacypreserving model latent space computationally efficient generalize better furthermore investigate downstream degradation factor propose use reidentification model previously employed privacy preservation filter demonstrate sufficient train model latent space video generator subsequently use model evaluate subspace covered synthetic video datasets thus introduce new way measure faithfulness generative machine learning model focus specific application healthcare echocardiography illustrate effectiveness novel method finding indicate training video learned latent video diffusion model could explain lack performance training downstream task synthetic data
aid adapting diffusion model instructionguided video prediction textguided video prediction tvp involves predicting motion future frame initial frame according instruction wide application virtual reality robotics content creation previous tvp method make significant breakthrough adapting stable diffusion task however struggle frame consistency temporal stability primarily due limited scale video datasets observe pretrained diffusion model possess good prior video dynamic lack textual control hence transferring model leverage video dynamic prior injecting instruction control generate controllable video meaningful challenging task achieve introduce multimodal large language model mllm predict future video state based initial frame text instruction specifically design dual query transformer dqformer architecture integrates instruction frame conditional embeddings future frame prediction additionally develop longshort term temporal adapter spatial adapter quickly transfer general video diffusion model specific scenario minimal training cost experimental result show method significantly outperforms stateoftheart technique four datasets something something epic bridge data notably aid achieves fvd improvement bridge respectively demonstrating effectiveness various domain example found website httpschenhsinggithubioaid
narcan natural refined canonical image integration diffusion prior video editing propose video editing framework narcan integrates hybrid deformation field diffusion prior generate highquality natural canonical image represent input video approach utilizes homography model global motion employ multilayer perceptrons mlps capture local residual deformation enhancing model ability handle complex video dynamic introducing diffusion prior early stage training model ensures generated image retain highquality natural appearance making produced canonical image suitable various downstream task video editing capability achieved current canonicalbased method furthermore incorporate lowrank adaptation lora finetuning introduce noise diffusion prior update scheduling technique accelerates training process time extensive experimental result show method outperforms existing approach various video editing task produce coherent highquality edited video sequence see project page video result
videomaker zeroshot customized video generation inherent force video diffusion model zeroshot customized video generation gained significant attention due substantial application potential existing method rely additional model extract inject reference subject feature assuming video diffusion model vdm alone insufficient zeroshot customized video generation however method often struggle maintain consistent subject appearance due suboptimal feature extraction injection technique paper reveal vdm inherently possesses force extract inject subject feature departing previous heuristic approach introduce novel framework leverage vdms inherent force enable highquality zeroshot customized video generation specifically feature extraction directly input reference image vdm use intrinsic feature extraction process provides finegrained feature also significantly aligns vdms pretrained knowledge feature injection devise innovative bidirectional interaction subject feature generated content spatial selfattention within vdm ensuring vdm better subject fidelity maintaining diversity generated video experiment customized human object video generation validate effectiveness framework
learning localize action instructional video llmbased multipathway textvideo alignment learning localize temporal boundary procedure step instructional video challenging due limited availability annotated largescale training video recent work focus learning crossmodal alignment video segment asrtranscripted narration text contrastive learning however method fail account alignment noise ie irrelevant narration instructional task video unreliable timestamps narration address challenge work proposes novel training framework motivated strong capability large language model llm procedure understanding text summarization first apply llm filter taskirrelevant information summarize taskrelated procedure step llmsteps narration generate reliable pseudomatching llmsteps video training propose multipathway textvideo alignment mptva strategy key idea measure alignment llmsteps video via multiple pathway including stepnarrationvideo alignment using narration timestamps direct steptovideo alignment based longterm semantic similarity direct steptovideo alignment focusing shortterm finegrained semantic similarity learned general video domain result different pathway fused generate reliable pseudo stepvideo matching conducted extensive experiment across various task problem setting evaluate proposed method approach surpasses stateoftheart method three downstream task procedure step grounding step localization narration grounding
freesurgs sfmfree gaussian splatting surgical scene reconstruction realtime reconstruction surgical scene play vital role computerassisted surgery holding promise enhance surgeon visibility recent advancement gaussian splatting shown great potential realtime novel view synthesis general scene relies accurate pose point cloud generated structurefrommotion sfm initialization however sfm fails recover accurate camera pose geometry surgical scene due challenge minimal texture photometric inconsistency tackle problem paper propose first sfmfree method surgical scene reconstruction jointly optimizing camera pose scene representation based video continuity key method exploit immediate optical flow prior guide projection flow derived gaussians unlike previous method relying photometric loss formulate pose estimation problem minimizing flow loss projection flow optical flow consistency check introduced filter flow outlier detecting rigid reliable point satisfy epipolar geometry gaussian optimization randomly sample frame optimize scene representation grow gaussian progressively experiment scared dataset demonstrate superior performance existing method novel view synthesis pose estimation high efficiency code available httpsgithubcomwrldfreesurgs
urban scene generation satellite image diffusion directly generating scene satellite imagery offer exciting possibility integration application like game map service however challenge arise significant view change scene scale previous effort mainly focused image video generation lacking exploration adaptability scene generation arbitrary view existing generation work either operate object level difficult utilize geometry obtained satellite imagery overcome limitation propose novel architecture direct scene generation introducing diffusion model sparse representation combining neural rendering technique specifically approach generates texture color point level given geometry using diffusion model first transformed scene representation feedforward manner representation utilized render arbitrary view would excel singleframe quality interframe consistency experiment two cityscale datasets show model demonstrates proficiency generating photorealistic streetview image sequence crossview urban scene satellite imagery
trainingfree semantic video composition via pretrained diffusion model video composition task aim integrate specified foreground background different video harmonious composite current approach predominantly trained video adjusted foreground color lighting struggle address deep semantic disparity beyond superficial adjustment domain gap therefore propose trainingfree pipeline employing pretrained diffusion model imbued semantic prior knowledge process composite video broader semantic disparity specifically process video frame cascading manner handle frame two process diffusion model inversion process propose balanced partial inversion obtain generation initial point balance reversibility modifiability generation process propose interframe augmented attention augment foreground continuity across frame experimental result reveal pipeline successfully ensures visual harmony interframe coherence output demonstrating efficacy managing broader semantic disparity
efficient diffusion model comprehensive survey principle practice one popular soughtafter generative model recent year diffusion model sparked interest many researcher steadily shown excellent advantage various generative task image synthesis video generation molecule design scene rendering multimodal generation relying dense theoretical principle reliable application practice remarkable success recent effort diffusion model come largely progressive design principle efficient architecture training inference deployment methodology however comprehensive indepth review summarize principle practice help rapid understanding application diffusion model survey provide new efficiencyoriented perspective existing effort mainly focus profound principle efficient practice architecture design model training fast inference reliable deployment guide theoretical research algorithm migration model application new scenario readerfriendly way urlhttpsgithubcomponyzymefficientdmssurvey
automatic generation interactive game scene user casual sketch content generation heart many computer graphic application including video gaming filmmaking virtual augmented reality etc paper proposes novel deeplearning based approach automatically generating interactive playable game scene user casual prompt handdrawn sketch sketchbased input offer natural convenient way convey user design intention content creation process circumvent datadeficient challenge learning ie lack large training data scene method leverage pretrained denoising diffusion model generate image scene conceptual guidance process adopt isometric projection mode factor unknown camera pose obtaining scene layout generated isometric image use pretrained image understanding method segment image meaningful part offground object tree building extract scene layout segment layout subsequently fed procedural content generation pcg engine video game engine like unity unreal create scene resulting scene seamlessly integrated game development environment readily playable extensive test demonstrate method efficiently generate highquality interactive game scene layout closely follow user intention
customvideo customizing texttovideo generation multiple subject customized texttovideo generation aim generate highquality video guided text prompt subject reference current approach personalizing texttovideo generation suffer tackling multiple subject challenging practical scenario work aim promote multisubject guided texttovideo customization propose customvideo novel framework generate identitypreserving video guidance multiple subject specific firstly encourage cooccurrence multiple subject via composing single image upon basic texttovideo diffusion model design simple yet effective attention control strategy disentangle different subject latent space diffusion model moreover help model focus specific area object segment object given reference image provide corresponding object mask attention learning also collect multisubject texttovideo generation dataset comprehensive benchmark individual subject different category meaningful pair extensive qualitative quantitative user study result demonstrate superiority method compared previous stateoftheart approach project page httpskyfafydwangprojectscustomvideo
magicme identityspecific video customized diffusion creating content specified identity id attracted significant interest field generative model field texttoimage generation subjectdriven creation achieved great progress identity controlled via reference image however extension video generation well explored work propose simple yet effective subject identity controllable video generation framework termed video custom diffusion vcd specified identity defined image vcd reinforces identity characteristic injects framewise correlation initialization stage stable video output achieve propose three novel component essential highquality identity preservation stable video generation noise initialization method gaussian noise prior better interframe stability id module based extended textual inversion trained cropped identity disentangle id information background face vcd tiled vcd module reinforce face upscale video higher resolution preserving identity feature conducted extensive experiment verify vcd able generate stable video better id baseline besides transferability encoded identity id module vcd also working well personalized texttoimage model available publicly code available httpsgithubcomzhendongmagicme
motionzero zeroshot moving object control framework diffusionbased video generation recent largescale pretrained diffusion model demonstrated powerful generative ability produce highquality video detailed text description however exerting control motion object video generated video diffusion model challenging problem paper propose novel zeroshot moving object trajectory control framework motionzero enable boundingboxtrajectoriescontrolled texttovideo diffusion model end initial noise prior module designed provide positionbased prior improve stability appearance moving object accuracy position addition based attention map unet spatial constraint directly applied denoising process diffusion model ensures positional spatial consistency moving object inference furthermore temporal consistency guaranteed proposed shift temporal attention mechanism method flexibly applied various stateoftheart video diffusion model without training process extensive experiment demonstrate proposed method control motion trajectory object generate highquality video project page httpsvpxecnugithubiomotionzerowebsite
textbased talking video editing cascaded conditional diffusion textbased talkinghead video editing aim efficiently insert delete substitute segment talking video userfriendly text editing approach challenging generalizable talkingface representation seamless audiovisual transition identitypreserved talking face previous work either require minute talkingface video training data expensive testtime optimization customized talking video editing directly generate video sequence without considering incontext information leading poor generalizable representation incoherent transition even inconsistent identity paper propose efficient cascaded conditional diffusionbased framework consists two stage audio denselandmark motion motion video textittextbfin first stage first propose dynamic weighted incontext diffusion module synthesize denselandmark motion given edited audio textittextbfin second stage introduce warpingguided conditional diffusion module module first interpolates start end frame editing interval generate smooth intermediate frame help audiotodense motion image intermediate frame warped obtain coarse intermediate frame conditioned warped intermedia frame diffusion model adopted generate detailed highresolution target frame guarantee coherent identitypreserved transition cascaded conditional diffusion model decomposes complex talking editing task two flexible generation task provides generalizable talkingface representation seamless audiovisual transition identitypreserved face small dataset experiment show effectiveness superiority proposed method
fast memoryefficient video diffusion using streamlined inference rapid progress artificial intelligencegenerated content aigc especially diffusion model significantly advanced development highquality video generation however current video diffusion model exhibit demanding computational requirement high peak memory usage especially generating longer higherresolution video limitation greatly hinder practical application video diffusion model standard hardware platform tackle issue present novel trainingfree framework named streamlined inference leverage temporal spatial property video diffusion model approach integrates three core component feature slicer operator grouping step rehash specifically feature slicer effectively partition input feature subfeatures operator grouping process subfeature group consecutive operator resulting significant memory reduction without sacrificing quality speed step rehash exploit similarity adjacent step diffusion accelerates inference skipping unnecessary step extensive experiment demonstrate approach significantly reduces peak memory computational overhead making feasible generate highquality video single consumer gpu eg reducing peak memory animatediff featuring faster inference
promptavideo prompt video diffusion model via preferencealigned llm texttovideo model made remarkable advancement optimization highquality textvideo pair textual prompt play pivotal role determining quality output video however achieving desired output often entail multiple revision iterative inference refine userprovided prompt current automatic method refining prompt encounter challenge modalityinconsistency costdiscrepancy modelunaware applied texttovideo diffusion model address problem introduce llmbased prompt adaptation framework termed promptavideo excels crafting videocentric laborfree preferencealigned prompt tailored specific video diffusion model approach involves meticulously crafted twostage optimization alignment system initially conduct rewardguided prompt evolution pipeline automatically create optimal prompt pool leverage supervised finetuning sft llm multidimensional reward employed generate pairwise data sft model followed direct preference optimization dpo algorithm facilitate preference alignment extensive experimentation comparative analysis validate effectiveness promptavideo across diverse generation model highlighting potential push boundary video generation
foodmem near realtime precise food video segmentation food segmentation including video vital addressing realworld health agriculture food biotechnology issue current limitation lead inaccurate nutritional analysis inefficient crop management suboptimal food processing impacting food security public health improving segmentation technique enhance dietary assessment agricultural productivity food production process study introduces development robust framework highquality nearrealtime segmentation tracking food item video using minimal hardware resource present foodmem novel framework designed segment food item video sequence unbounded scene foodmem consistently generate mask food portion video sequence overcoming limitation existing semantic segmentation model flickering prohibitive inference speed video processing context address issue foodmem leverage twophase solution transformer segmentation phase create initial segmentation mask memorybased tracking phase monitor food mask complex scene framework outperforms current stateoftheart food segmentation model yielding superior performance across various condition camera angle lighting reflection scene complexity food diversity result reduced segmentation noise elimination artifact completion missing segment also introduce new annotated food dataset encompassing challenging scenario absent previous benchmark extensive experiment conducted vegetable fruit datasets demonstrate foodmem enhances stateoftheart mean average precision food video segmentation x faster average
expressedit video editing natural language sketching informational video serve crucial source explaining conceptual procedural knowledge novice expert alike producing informational video editor edit video overlaying textimages trimming footage enhance video quality make engaging however video editing difficult timeconsuming especially novice video editor often struggle expressing implementing editing idea address challenge first explored multimodalitynatural language nl sketching natural modality human use expressioncan utilized support video editor expressing video editing idea gathered multimodal expression editing command video editor revealed pattern use nl sketching describing edit intent based finding present expressedit system enables editing video via nl text sketching video frame powered llm vision model system interprets temporal spatial operational reference nl command spatial reference sketching system implement interpreted edits user iterate observational study showed expressedit enhanced ability novice video editor express implement edit idea system allowed participant perform edits efficiently generate idea generating edits based user multimodal edit command supporting iteration editing command work offer insight design future multimodal interface aibased pipeline video editing
raven rethinking adversarial video generation efficient triplane network present novel unconditional video generative model designed address longterm spatial temporal dependency attention computational dataset efficiency capture long spatiotemporal dependency approach incorporates hybrid explicitimplicit triplane representation inspired generative framework developed threedimensional object representation employ single latent code model entire video clip individual video frame synthesized intermediate triplane representation derived primary latent code novel strategy half computational complexity measured flop compared efficient stateoftheart method consequently approach facilitates efficient temporally coherent generation video moreover joint frame modeling approach contrast autoregressive method mitigates generation visual artifact enhance model capability integrating optical flowbased module within generative adversarial network gan based generator architecture thereby compensating constraint imposed smaller generator size result model synthesizes highfidelity video clip resolution pixel duration extending second frame rate fps efficacy versatility approach empirically validated qualitative quantitative assessment across three different datasets comprising synthetic real video clip make training inference code public
sora detector unified hallucination detection large texttovideo model rapid advancement texttovideo generative model enabled synthesis highfidelity video content guided textual description despite significant progress model often susceptible hallucination generating content contradict input text pose challenge reliability practical deployment address critical issue introduce soradetector novel unified framework designed detect hallucination across diverse large model including cuttingedge sora model framework built upon comprehensive analysis hallucination phenomenon categorizing based manifestation video content leveraging stateoftheart keyframe extraction technique multimodal large language model soradetector first evaluates consistency extracted video content summary textual prompt construct static dynamic knowledge graph kg frame detect hallucination single frame across frame sora detector provides robust quantifiable measure consistency static dynamic hallucination addition developed sora detector agent automate hallucination detection process generate complete video quality report input video lastly present novel metaevaluation benchmark meticulously crafted facilitate evaluation advancement hallucination detection extensive experiment video generated sora large model demonstrate efficacy approach accurately detecting hallucination code dataset accessed via github
robot shape location retention video generation using diffusion model diffusion model marked significant milestone enhancement image video generation technology however generating video precisely retain shape location moving object robot remains challenge paper present diffusion model specifically tailored generate video accurately maintain shape location mobile robot development offer substantial benefit working detecting dangerous interaction human robot facilitating creation training data collision detection model circumventing need collecting data real world often involves legal ethical issue model incorporate technique embedding accessible robot pose information applying semantic mask regulation within convnext backbone network technique designed refine intermediate output therefore improving retention performance shape location extensive experimentation model demonstrated notable improvement maintaining shape location different robot well enhancing overall video generation quality compared benchmark diffusion model code opensourced hrefhttpsgithubcompengpaulwangdiffusionrobotsgithub
generative video model help pose estimation pairwise pose estimation image little overlap open challenge computer vision existing method even trained largescale datasets struggle scenario due lack identifiable correspondence visual overlap inspired human ability infer spatial relationship diverse scene propose novel approach interpose leverage rich prior encoded within pretrained generative video model propose use video model hallucinate intermediate frame two input image effectively creating dense visual transition significantly simplifies problem pose estimation since current video model still produce implausible motion inconsistent geometry introduce selfconsistency score evaluates consistency pose prediction sampled video demonstrate approach generalizes among three stateoftheart video model show consistent improvement stateoftheart four diverse datasets encompassing indoor outdoor objectcentric scene finding suggest promising avenue improving pose estimation model leveraging large generative model trained vast amount video data readily available data see project page result httpsinterposegithubio
animate thought decoupled reconstruction dynamic natural vision slow brain activity reconstructing human dynamic vision brain activity challenging task great scientific significance although prior video reconstruction method made substantial progress still suffer several limitation including difficulty simultaneously reconciling semantic eg categorical description structure eg size color consistent motion information eg order frame low temporal resolution fmri pose challenge decoding multiple frame video dynamic single fmri frame reliance video generation model introduces ambiguity regarding whether dynamic observed reconstructed video genuinely derived fmri data hallucination generative model overcome limitation propose twostage model named mindanimator fmritofeature stage decouple semantic structure motion feature fmri specifically employ fmrivisionlanguage trimodal contrastive learning decode semantic feature fmri design sparse causal attention mechanism decoding multiframe video motion feature nextframeprediction task featuretovideo stage feature integrated video using inflated stable diffusion effectively eliminating external video data interference extensive experiment multiple videofmri datasets demonstrate model achieves stateoftheart performance comprehensive visualization analysis elucidate interpretability model neurobiological perspective project page httpsmindanimatordesigngithubio
makima tuningfree multiattribute opendomain video editing via maskguided attention modulation diffusionbased texttoimage model demonstrated remarkable result global video editing task however focus primarily global video modification achieving desired attributespecific change remains challenging task specifically multiattribute editing mae video contemporary video editing approach either require extensive finetuning rely additional network controlnet modeling multiobject appearance yet remain infancy offering coarsegrained mae solution paper present makima tuningfree mae framework built upon pretrained model opendomain video editing approach preserve video structure appearance information incorporating attention map feature inversion process denoising facilitate precise editing multiple attribute introduce maskguided attention modulation enhancing correlation spatially corresponding token suppressing crossattribute interference selfattention crossattention layer balance video frame generation quality efficiency implement consistent feature propagation generates frame sequence editing keyframes propagating feature throughout sequence extensive experiment demonstrate makima outperforms existing baseline opendomain multiattribute video editing task achieving superior result editing accuracy temporal consistency maintaining computational efficiency
sparse input view synthesis representation reliable prior novel view synthesis refers problem synthesizing novel viewpoint scene given image viewpoint fundamental problem computer vision graphic enables vast variety application metaverse freeview watching event video gaming video stabilization video compression recent representation radiance field multiplane image significantly improve quality image rendered novel viewpoint however model require dense sampling input view high quality render performance go significantly input view available thesis focus sparse input novel view synthesis problem static dynamic scene first part work mainly focus sparse input novel view synthesis static scene using neural radiance field nerf study design reliable dense prior better regularize nerf situation particular propose prior visibility pixel pair input view show visibility prior related relative depth object dense reliable existing prior absolute depth compute visibility prior using plane sweep volume without need train neural network large datasets evaluate approach multiple datasets show model outperforms existing approach sparse input novel view synthesis second part aim improve regularization learning scenespecific prior suffer generalization issue achieve learning prior given scene alone without pretraining large datasets particular design augmented nerfs obtain better depth supervision certain region scene main nerf extend framework also apply newer faster radiance field model tensorf zipnerf extensive experiment multiple datasets show superiority approach sparse input novel view synthesis design sparse input fast dynamic radiance field severely constrained lack suitable representation reliable prior motion address first challenge designing explicit motion model based factorized volume compact optimizes quickly also introduce reliable sparse flow prior constrain motion field since find popularly employed dense optical flow prior unreliable show benefit motion representation reliable prior multiple datasets final part thesis study application view synthesis frame rate upsampling video gaming specifically consider problem temporal view synthesis goal predict future frame given past frame camera motion key challenge predicting future motion object estimating past motion extrapolating explore use multiplane image representation scene depth reliably estimate object motion particularly occluded region design new database effectively evaluate approach temporal view synthesis dynamic scene show achieve stateoftheart performance
teaching video diffusion model latent physical phenomenon knowledge video diffusion model exhibited tremendous progress various video generation task however existing model struggle capture latent physical knowledge failing infer physical phenomenon challenging articulate natural language generating video following fundamental physical law still opening challenge address challenge propose novel method teach video diffusion model latent physical phenomenon knowledge enabling accurate generation physically informed phenomenon specifically first pretrain masked autoencoders mae reconstruct physical phenomenon resulting output embeddings encapsulate latent physical phenomenon knowledge leveraging embeddings could generate pseudolanguage prompt feature based aligned spatial relationship clip vision language encoders particularly given diffusion model typically use clip language encoder text prompt embeddings approach integrates clip visual feature informed latent physical knowledge quaternion hidden space enables modeling spatial relationship produce physical knowledgeinformed pseudolanguage prompt incorporating prompt feature finetuning video diffusion model parameterefficient manner physical knowledgeinformed video successfully generated validate method extensively numerical simulation realworld observation physical phenomenon demonstrating remarkable performance across diverse scenario
freetraj tuningfree trajectory control video diffusion model diffusion model demonstrated remarkable capability video generation spark interest introducing trajectory control generation process existing work mainly focus trainingbased method eg conditional adapter argue diffusion model allows decent control generated content without requiring training study introduce tuningfree framework achieve trajectorycontrollable video generation imposing guidance noise construction attention computation specifically first show several instructive phenomenon analyze initial noise influence motion trajectory generated content subsequently propose freetraj tuningfree approach enables trajectory control modifying noise sampling attention mechanism furthermore extend freetraj facilitate longer larger video generation controllable trajectory equipped design user flexibility provide trajectory manually opt trajectory automatically generated llm trajectory planner extensive experiment validate efficacy approach enhancing trajectory controllability video diffusion model
wavelet diffusion gan image superresolution recent year diffusion model emerged superior alternative generative adversarial network gans highfidelity image generation wide application texttoimage generation imagetoimage translation superresolution however realtime feasibility hindered slow training inference speed study address challenge proposing waveletbased conditional diffusion gan scheme singleimage superresolution sisr approach utilizes diffusion gan paradigm reduce timesteps required reverse diffusion process discrete wavelet transform dwt achieve dimensionality reduction decreasing training inference time significantly result experimental validation celebahq dataset confirm effectiveness proposed scheme approach outperforms stateoftheart methodology successfully ensuring highfidelity output overcoming inherent drawback associated diffusion model timesensitive application
loopy taming audiodriven portrait avatar longterm motion dependency introduction diffusionbased video generation technique audioconditioned human video generation recently achieved significant breakthrough naturalness motion synthesis portrait detail due limited control audio signal driving human motion existing method often add auxiliary spatial signal stabilize movement may compromise naturalness freedom motion paper propose endtoend audioonly conditioned video diffusion model named loopy specifically designed inter intraclip temporal module audiotolatents module enabling model leverage longterm motion information data learn natural motion pattern improving audioportrait movement correlation method remove need manually specified spatial motion template used existing method constrain motion inference extensive experiment show loopy outperforms recent audiodriven portrait diffusion model delivering lifelike highquality result across various scenario
makeyouranchor diffusionbased avatar generation framework despite remarkable process talkingheadbased avatarcreating solution directly generating anchorstyle video fullbody motion remains challenging study propose makeyouranchor novel system necessitating oneminute video clip individual training subsequently enabling automatic generation anchorstyle video precise torso hand movement specifically finetune proposed structureguided diffusion model input video render mesh condition human appearance adopt twostage training strategy diffusion model effectively binding movement specific appearance produce arbitrary long temporal video extend unet framewise diffusion model style without additional training cost simple yet effective batchoverlapped temporal denoising module proposed bypass constraint video length inference finally novel identityspecific face enhancement module introduced improve visual quality facial region output video comparative experiment demonstrate effectiveness superiority system term visual quality temporal coherence identity preservation outperforming sota diffusionnondiffusion method project page urlhttpsgithubcomictmcgmakeyouranchor
beyond raw video understanding edited video large multimodal model emerging video lmms large multimodal model achieved significant improvement generic video understanding form vqa visual question answering raw video captured camera however large portion video realworld application edited video textiteg user usually cut add effectsmodifications raw video publishing social medium platform edited video usually high view count covered existing benchmark video lmms textitie activitynetqa videochatgpt benchmark paper leverage edited video popular short video platform textitie tiktok build video vqa benchmark named editvidqa covering four typical editing category ie effect funny meme game funny meme video benchmark nuanced understanding highlevel reasoning effect game evaluate understanding capability artificial design opensource video lmms perform poorly editvidqa benchmark indicating huge domain gap edited short video social medium regular raw video improve generalization ability lmms collect training set proposed benchmark based raw video smallscale tiktokcapcut edited video boost performance proposed editvidqa benchmark indicating effectiveness highquality training data also identified serious issue existing evaluation protocol using judge namely sorry attack sorrystyle naive answer achieve extremely high rating gpt judge eg correctness score videochatgpt evaluation protocol avoid sorry attack evaluate result judge keyword filtering dataset released httpsgithubcomxenonlambeditvidqa
unictrl improving spatiotemporal consistency texttovideo diffusion model via trainingfree unified attention control video diffusion model developed video generation usually integrating text image conditioning enhance control generated content despite progress ensuring consistency across frame remains challenge particularly using text prompt control condition address problem introduce unictrl novel plugandplay method universally applicable improve spatiotemporal consistency motion diversity video generated texttovideo model without additional training unictrl ensures semantic consistency across different frame crossframe selfattention control meanwhile enhances motion quality spatiotemporal consistency motion injection spatiotemporal synchronization experimental result demonstrate unictrls efficacy enhancing various texttovideo model confirming effectiveness universality
crossconditioned diffusion model medical image image translation multimodal magnetic resonance imaging mri provides rich complementary information analyzing disease however practical challenge acquiring multiple mri modality cost scan time safety consideration often result incomplete datasets affect quality diagnosis performance deep learning model trained data recent advancement generative adversarial network gans denoising diffusion model shown promise natural medical imagetoimage translation task however complexity training gans computational expense associated diffusion model hinder development application task address issue introduce crossconditioned diffusion model cdm medical imagetoimage translation core idea cdm use distribution target modality guidance improve synthesis quality achieving higher generation efficiency compared conventional diffusion model first propose modalityspecific representation model mrm model distribution target modality design modalitydecoupled diffusion network mdn efficiently effectively learn distribution mrm finally crossconditioned unet cunet condition embedding module designed synthesize target modality source modality input target distribution guidance extensive experiment conducted upenngbm benchmark datasets demonstrate superiority method
neurosymbolic evaluation texttovideo model using formal verification recent advancement texttovideo model sora moviegen cogvideox pushing boundary synthetic video generation adoption seen field like robotics autonomous driving entertainment model become prevalent various metric benchmark emerged evaluate quality generated video however metric emphasize visual quality smoothness neglecting temporal fidelity texttovideo alignment crucial safetycritical application address gap introduce neusv novel synthetic video evaluation metric rigorously assesses texttovideo alignment using neurosymbolic formal verification technique approach first convert prompt formally defined temporal logic tl specification translates generated video automaton representation evaluates texttovideo alignment formally checking video automaton tl specification furthermore present dataset temporally extended prompt evaluate stateoftheart video generation model benchmark find neusv demonstrates higher correlation human evaluation compared existing metric evaluation reveals current video generation model perform poorly temporally complex prompt highlighting need future work improving texttovideo generation capability
spectral motion alignment video motion transfer using diffusion model evolution diffusion model greatly impacted video generation understanding particularly texttovideo diffusion model vdms significantly facilitated customization input video target appearance motion etc despite advance challenge persist accurately distilling motion information video frame existing work leverage consecutive frame residual target motion vector inherently lack global motion context vulnerable framewise distortion address present spectral motion alignment sma novel framework refines aligns motion vector using fourier wavelet transforms sma learns motion pattern incorporating frequencydomain regularization facilitating learning wholeframe global motion dynamic mitigating spatial artifact extensive experiment demonstrate smas efficacy improving motion transfer maintaining computational efficiency compatibility across various video customization framework
ganesh generalizable nerf lensless imaging lensless imaging offer significant opportunity develop ultracompact camera removing conventional bulky lens system however without focusing element sensor output longer direct image complex multiplexed scene representation traditional method attempted address challenge employing learnable inversion refinement model method primarily designed reconstruction generalize well reconstruction introduce ganesh novel framework designed enable simultaneous refinement novel view synthesis multiview lensless image unlike existing method require scenespecific training approach support onthefly inference without retraining scene moreover framework allows u tune model specific scene enhancing rendering refinement quality facilitate research area also present first multiview lensless dataset lenslessscenes extensive experiment demonstrate method outperforms current approach reconstruction accuracy refinement quality code video result available
genmac compositional texttovideo generation multiagent collaboration texttovideo generation model shown significant progress recent year however still struggle generating complex dynamic scene based compositional text prompt attribute binding multiple object temporal dynamic associated different object interaction object key motivation complex task decomposed simpler one handled rolespecialized mllm agent multiple agent collaborate together achieve collective intelligence complex goal propose genmac iterative multiagent framework enables compositional texttovideo generation collaborative workflow includes three stage design generation redesign iterative loop generation redesign stage progressively verify refine generated video redesign stage challenging stage aim verify generated video suggest correction redesign text prompt framewise layout guidance scale next iteration generation avoid hallucination single mllm agent decompose stage four sequentiallyexecuted mllmbased agent verification agent suggestion agent correction agent output structuring agent furthermore tackle diverse scenario compositional texttovideo generation design selfrouting mechanism adaptively select proper correction agent collection correction agent specialized one scenario extensive experiment demonstrate effectiveness genmac achieving stateofthe art performance compositional texttovideo generation
ivmixed sampler leveraging image diffusion model enhanced video synthesis multistep sampling mechanism key feature visual diffusion model significant potential replicate success openais strawberry enhancing performance increasing inference computational cost sufficient prior study demonstrated correctly scaling computation sampling process successfully lead improved generation quality enhanced image editing compositional generalization rapid advancement developing inferenceheavy algorithm improved image generation relatively little work explored inference scaling law video diffusion model vdms furthermore existing research show minimal performance gain perceptible naked eye address design novel trainingfree algorithm ivmixed sampler leverage strength image diffusion model idms assist vdms surpass current capability core ivmixed sampler use idms significantly enhance quality video frame vdms ensure temporal coherence video sampling process experiment demonstrated ivmixed sampler achieves stateoftheart performance benchmark including msrvttfvd example opensource animatediff ivmixed sampler reduces umtfvd score closing closedsource
beyond gfvc progressive face video compression framework adaptive visual token recently deep generative model greatly advanced progress face video coding towards promising ratedistortion performance diverse application functionality beyond traditional hybrid video coding paradigm generative face video compression gfvc relying strong capability deep generative model philosophy early modelbased coding mbc facilitate compact representation realistic reconstruction visual face signal thus achieving ultralow bitrate face video communication however gfvc algorithm sometimes faced unstable reconstruction quality limited bitrate range address problem paper proposes novel progressive face video compression framework namely pfvc utilizes adaptive visual token realize exceptional tradeoff reconstruction robustness bandwidth intelligence particular encoder proposed pfvc project highdimensional face signal adaptive visual token progressive manner whilst decoder reconstruct adaptive visual token motion estimation signal synthesis different granularity level experimental result demonstrate proposed pfvc framework achieve better coding flexibility superior ratedistortion performance comparison latest versatile video coding vvc codec stateoftheart gfvc algorithm project page found
listen move improving gans coherency agnostic soundtovideo generation deep generative model demonstrated ability create realistic audiovisual content sometimes driven domain different nature however smooth temporal dynamic video generation challenging problem work focus generic soundtovideo generation proposes three main feature enhance image quality temporal coherency generative adversarial model triple sound routing scheme multiscale residual dilated recurrent network extended sound analysis novel recurrent directional convolutional layer video prediction proposed feature improves quality coherency baseline neural architecture typically used sota video prediction layer providing extra temporal refinement
deformationaware gan medical image synthesis substantially misaligned pair medical image synthesis generates additional imaging modality costly invasive harmful acquire help facilitate clinical workflow training pair substantially misaligned eg lung mrict pair respiratory motion accurate image synthesis remains critical challenge recent work explored directional registration module adjust misalignment generative adversarial network gans however substantial misalignment lead suboptimal data mapping caused correspondence ambiguity degraded image fidelity caused morphology influence discriminator address challenge propose novel deformationaware gan dagan dynamically correct misalignment image synthesis based multiobjective inverse consistency specifically generative process three level inverse consistency cohesively optimise symmetric registration image generation improved correspondence adversarial process improve image fidelity misalignment design deformationaware discriminator disentangle mismatched spatial morphology judgement image fidelity experimental result show dagan achieved superior performance public dataset simulated misalignment realworld lung mrict dataset respiratory motion misalignment result indicate potential wide range medical image synthesis task radiotherapy planning
scaling foundation model multimodal video understanding introduce new family video foundation model vifm achieve stateoftheart result video recognition videotext task videocentric dialogue core design progressive training approach unifies masked video modeling crossmodal contrastive learning next token prediction scaling video encoder size parameter data level prioritize spatiotemporal consistency semantically segmenting video generating videoaudiospeech caption improves alignment video text extensive experiment validate design demonstrate superior performance video audio task notably model outperforms others various videorelated dialogue long video understanding benchmark highlighting ability reason comprehend longer context code model available
storyboard guided alignment finegrained video action recognition finegrained video action recognition conceptualized videotext matching problem previous approach often rely global video semantics consolidate video embeddings lead misalignment videotext pair due lack understanding action semantics atomic granularity level tackle challenge propose multigranularity framework based two observation video different global semantics may share similar atomic action appearance ii atomic action within video momentary slow even nondirectly related global video semantics inspired concept storyboarding disassembles script individual shot enhance global video semantics generating finegrained description using pretrained large language model detailed description capture common atomic action depicted video filtering metric proposed select description correspond atomic action present video description employing global semantics finegrained description identify key frame video utilize aggregate embeddings thereby making embedding accurate extensive experiment various video action recognition datasets demonstrate superior performance proposed method supervised fewshot zeroshot setting
consistent controllable imagetovideo generation explicit motion modeling introduce novel framework consistent controllable imagetovideo generation contrast previous method directly learn complicated imagetovideo mapping factorizes two stage explicit motion modeling first stage propose diffusionbased motion field predictor focus deducing trajectory reference image pixel second stage propose motionaugmented temporal attention enhance limited temporal attention video latent diffusion model module effectively propagate reference image feature synthesized frame guidance predicted trajectory first stage compared existing method generate consistent video even presence large motion viewpoint variation training sparse trajectory controlnet first stage support user precisely control motion trajectory motion region sparse trajectory region annotation offer controllability process solely relying textual instruction additionally second stage naturally support zeroshot videotovideo translation qualitative quantitative comparison demonstrate advantage prior approach consistent controllable imagetovideo generation please see project page
towards detection aisynthesized human face image past year image generation manipulation achieved remarkable progress due rapid development generative ai based deep learning recent study devoted significant effort address problem face image manipulation caused deepfake technique however problem detecting purely synthesized face image explored lesser extent particular recent popular diffusion model dm shown remarkable success image synthesis existing detector struggle generalize synthesized image created different generative model work comprehensive benchmark including human face image produced generative adversarial network gans variety dm established evaluate generalization ability robustness stateoftheart detector forgery trace introduced different generative model analyzed frequency domain draw various insight paper demonstrates detector trained frequency representation generalize well unseen generative model
videoclusternet selfsupervised adaptive face clustering video rise digital medium content production need analyzing movie tv series episode locate main cast character precisely gaining importancespecifically video face clustering aim group together detected video face track common facial identity problem challenging due large range pose expression appearance lighting variation given face across video frame generic pretrained face identification id model fail adapt well video production domain given high dynamic range content also unique cinematic style furthermore traditional clustering algorithm depend hyperparameters requiring individual tuning across datasets paper present novel video face clustering approach learns adapt generic face id model new video face track fully selfsupervised fashion also propose parameterfree clustering algorithm capable automatically adapting finetuned model embedding space input video due lack comprehensive movie face clustering benchmark also present firstofkind movie dataset moviefacecluster dataset handpicked film industry professional contains extremely challenging face id scenario experiment show method effectiveness handling difficult mainstream movie scene benchmark dataset stateoftheart performance traditional tv series datasets
step enhancing videollms compositional reasoning spatiotemporal graphguided selftraining video large language model videollms recently shown strong performance basic video understanding task captioning coarsegrained question answering struggle compositional reasoning requires multistep spatiotemporal inference across object relation interaction event hurdle enhancing capability include extensive manual labor lack spatiotemporal compositionality existing data absence explicit reasoning supervision paper propose step novel graphguided selftraining method enables videollms generate reasoningrich finetuning data raw video improve specifically first induce spatiotemporal scene graph stsg representation diverse video capture finegrained multigranular video semantics stsgs guide derivation multistep reasoning questionanswer qa data chainofthought cot rationale answer rationale integrated training objective aiming enhance model reasoning ability supervision explicit reasoning step experimental result demonstrate effectiveness step across model varying scale significant improvement task requiring three reasoning step furthermore achieves superior performance minimal amount selfgenerated rationaleenriched training sample compositional reasoning comprehensive understanding benchmark highlighting broad applicability vast potential
combining genre classification harmonicpercussive feature diffusion model musicvideo generation study present novel method generating music visualiser using diffusion model combining audio input userselected artwork process involves two main stage image generation video creation first music captioning genre classification performed followed retrieval artistic style description diffusion model generates image based user input image derived artistic style description video generation stage utilises diffusion model interpolate frame controlled audio energy vector derived key musical feature harmonic percussives method demonstrates promising result across various genre new metric audiovisual synchrony av introduced quantitatively evaluate synchronisation visual audio element comparative analysis show significantly higher av value video generated using proposed method audio energy vector compared linear interpolation approach potential application diverse field including independent music video creation film production live music event enhancing audiovisual experience public space
robodreamer learning compositional world model robot imagination texttovideo model demonstrated substantial potential robotic decisionmaking enabling imagination realistic plan future action well accurate environment simulation however one major issue model generalization model limited synthesizing video subject language instruction similar seen training time heavily limiting decisionmaking seek powerful world model synthesize plan unseen combination object action order solve previously unseen task new environment resolve issue introduce robodreamer innovative approach learning compositional world model factorizing video generation leverage natural compositionality language parse instruction set lowerlevel primitive condition set model generate video illustrate factorization naturally enables compositional generalization allowing u formulate new natural language instruction combination previously seen component show factorization enables u add additional multimodal goal allowing u specify video wish generate given natural language instruction goal image approach successfully synthesize video plan unseen goal rtx enables successful robot execution simulation substantially outperforms monolithic baseline approach video generation
denoising reuse exploiting interframe motion consistency efficient video latent generation video generation using diffusionbased model constrained high computational cost due framewise iterative diffusion process work present diffusion reuse motion dr mo network accelerate latent video generation key discovery coarsegrained noise earlier denoising step demonstrated high motion consistency across consecutive video frame following observation dr mo propagates coarsegrained noise onto next frame incorporating carefully designed lightweight interframe motion eliminating massive computational redundancy framewise diffusion model sensitive finegrained noise still acquired via later denoising step essential retain visual quality deciding intermediate step switch motionbased propagation denoising crucial problem key tradeoff efficiency quality dr mo employ metanetwork named denoising step selector ds dynamically determine desirable intermediate step across video frame extensive evaluation video generation editing task shown dr mo substantially accelerate diffusion model video task improved visual quality
vidhalluc evaluating temporal hallucination multimodal large language model video understanding multimodal large language model mllms recently shown significant advancement video understanding excelling content reasoning instructionfollowing task however problem hallucination model generate inaccurate misleading content remains underexplored video domain building observation visual encoder mllms often struggle differentiate video pair visually distinct semantically similar introduce vidhalluc largest benchmark designed examine hallucination mllms video understanding task vidhalluc assesses hallucination across three critical dimension action temporal sequence scene transition vidhalluc consists video paired based semantic similarity visual difference focusing case hallucination likely occur comprehensive testing experiment show mllms vulnerable hallucination across dimension furthermore propose dinoheal trainingfree method reduces hallucination incorporating spatial saliency information reweight visual feature inference result demonstrate dinoheal consistently improves performance vidhalluc achieving average improvement mitigating hallucination among task vidhalluc benchmark dinoheal code accessed via hrefhttpsvidhallucgithubiotextthis link
fada fast diffusion avatar synthesis mixedsupervised multicfg distillation diffusionbased audiodriven talking avatar method recently gained attention highfidelity vivid expressive result however slow inference speed limit practical application despite development various distillation technique diffusion model found naive diffusion distillation method yield satisfactory result distilled model exhibit reduced robustness openset input image decreased correlation audio video compared teacher model undermining advantage diffusion model address propose fada fast diffusion avatar synthesis mixedsupervised multicfg distillation first designed mixedsupervised loss leverage data varying quality enhance overall model capability well robustness additionally propose multicfg distillation learnable token utilize correlation audio reference image condition reducing threefold inference run caused multicfg acceptable quality degradation extensive experiment across multiple datasets show fada generates vivid video comparable recent diffusion modelbased method achieving nfe speedup time demo available webpage httpfadavatargithubio
unified editing panorama scene video disentangled selfattention injection texttoimage model achieved impressive capability image generation editing application across various modality often necessitates training separate model inspired existing method single image editing self attention injection video editing shared attention propose novel unified editing framework combine strength approach utilizing basic image texttoimage diffusion model specifically design sampling method facilitates editing consecutive image maintaining semantic consistency utilizing shared selfattention feature reference consecutive image sampling process experimental result confirm method enables editing across diverse modality including scene video panorama image
diffusionpromoted hdr video reconstruction high dynamic range hdr video reconstruction aim generate hdr video low dynamic range ldr frame captured alternating exposure existing work solely rely regressionbased paradigm leading adverse effect ghosting artifact missing detail saturated region paper propose diffusionpromoted method hdr video reconstruction termed hdrvdiff incorporates diffusion model capture hdr distribution hdrvdiff reconstruct hdr video realistic detail alleviating ghosting artifact however direct introduction video diffusion model would impose massive computational burden instead alleviate burden first propose hdr latent diffusion model hdrldm learn distribution prior single hdr frame specifically hdrldm incorporates tonemapping strategy compress hdr frame latent space novel exposure embedding aggregate exposure information diffusion process propose temporalconsistent alignment module tcam learn temporal information complement hdrldm conduct coarsetofine feature alignment different scale among video frame finally design zeroinit crossattention zica mechanism effectively integrate learned distribution prior temporal information generating hdr frame extensive experiment validate hdrvdiff achieves stateoftheart result several representative datasets
diffpano scalable consistent text panorama generation spherical epipolaraware diffusion diffusionbased method achieved remarkable achievement image object generation however generation scene even image remains constrained due limited number scene datasets complexity scene difficulty generating consistent multiview image address issue first establish largescale panoramic videotext dataset containing million consecutive panoramic keyframes corresponding panoramic depth camera pose text description propose novel textdriven panoramic generation framework termed diffpano achieve scalable consistent diverse panoramic scene generation specifically benefiting powerful generative capability stable diffusion finetune singleview texttopanorama diffusion model lora established panoramic videotext dataset design spherical epipolaraware multiview diffusion model ensure multiview consistency generated panoramic image extensive experiment demonstrate diffpano generate scalable consistent diverse panoramic image given unseen text description camera pose
nerfnqa noreference quality assessment scene generated nerf neural view synthesis method neural view synthesis nv demonstrated efficacy generating highfidelity dense viewpoint video using image set sparse view however existing quality assessment method like psnr ssim lpips tailored scene dense viewpoint synthesized nv nerf variant thus often fall short capturing perceptual quality including spatial angular aspect nvssynthesized scene furthermore lack dense ground truth view make full reference quality assessment nvssynthesized scene challenging instance datasets llff provide sparse image insufficient complete fullreference assessment address issue propose nerfnqa first noreference quality assessment method denselyobserved scene synthesized nv nerf variant nerfnqa employ joint quality assessment strategy integrating viewwise pointwise approach evaluate quality nvsgenerated scene viewwise approach assesses spatial quality individual synthesized view overall interview consistency pointwise approach focus angular quality scene surface point compound interpoint quality extensive evaluation conducted compare nerfnqa mainstream visual quality assessment method field image video lightfield assessment result demonstrate nerfnqa outperforms existing assessment method significantly show substantial superiority assessing nvssynthesized scene without reference implementation paper available httpsgithubcomvincentqqunerfnqa
video generation consistency tuning currently various study exploring generation long video however generated frame video often exhibit jitter noise therefore order generate video without noise propose novel framework composed four module separate tuning module average fusion module combined tuning module interframe consistency module applying newly proposed module subsequently consistency background foreground video frame optimized besides experimental result demonstrate video generated method exhibit high quality comparison stateoftheart method
ltxvideo realtime video latent diffusion introduce ltxvideo transformerbased latent diffusion model adopts holistic approach video generation seamlessly integrating responsibility videovae denoising transformer unlike existing method treat component independent ltxvideo aim optimize interaction improved efficiency quality core carefully designed videovae achieves high compression ratio spatiotemporal downscaling x x pixel per token enabled relocating patchifying operation transformer input vaes input operating highly compressed latent space enables transformer efficiently perform full spatiotemporal selfattention essential generating highresolution video temporal consistency however high compression inherently limit representation fine detail address vae decoder tasked latenttopixel conversion final denoising step producing clean result directly pixel space approach preserve ability generate fine detail without incurring runtime cost separate upsampling module model support diverse use case including texttovideo imagetovideo generation capability trained simultaneously achieves fasterthanrealtime generation producing second fps video resolution second nvidia gpu outperforming existing model similar scale source code pretrained model publicly available setting new benchmark accessible scalable video generation
masked generative videotoaudio transformer enhanced synchronicity videotoaudio generation leverage visualonly video feature render plausible sound match scene importantly generated sound onset match visual action aligned otherwise unnatural synchronization artifact arise recent work explored progression conditioning sound generator still image video feature focusing quality semantic matching ignoring synchronization sacrificing amount quality focus improving synchronization work propose generative model named maskvat interconnects fullband highquality general audio codec sequencetosequence masked generative model combination allows modeling high audio quality semantic matching temporal synchronicity time result show combining highquality codec proper pretrained audiovisual feature sequencetosequence parallel structure able yield highly synchronized result one hand whilst competitive state art noncodec generative audio model sample video generated audio available httpsmaskvatgithubio
adapting imagetovideo diffusion model largemotion frame interpolation development video generation model advanced significantly recent year adopt largescale imagetovideo diffusion model video frame interpolation present conditional encoder designed adapt imagetovideo model largemotion frame interpolation enhance performance integrate dualbranch feature extractor propose crossframe attention mechanism effectively capture spatial temporal information enabling accurate interpolation intermediate frame approach demonstrates superior performance frechet video distance fvd metric evaluated stateoftheart approach particularly handling large motion scenario highlighting advancement generativebased methodology
speechguided diffusion model realtime mri video vocal tract speech understanding speech production visually kinematically inform second language learning system design well creation speaking character video game animation work introduce datadriven method visually represent articulator motion magnetic resonance imaging mri video human vocal tract speech based arbitrary audio speech input leverage large pretrained speech model embedded prior knowledge generalize visual domain unseen data using speechtovideo diffusion model finding demonstrate visual generation significantly benefit pretrained speech representation also observed evaluating phoneme isolation challenging becomes straightforward assessed within context spoken word limitation current result include presence unsmooth tongue motion video distortion tongue contact palate
motionstone decoupled motion intensity modulation diffusion transformer imagetovideo generation imagetovideo generation conditioned static image enhanced recently motion intensity additional control signal motionaware model appealing generate diverse motion pattern yet lack reliable motion estimator training model largescale video set wild traditional metric eg ssim optical flow hard generalize arbitrary video tough human annotator label abstract motion intensity neither furthermore motion intensity shall reveal local object motion global camera movement studied paper address challenge new motion estimator capable measuring decoupled motion intensity object camera video leverage contrastive learning randomly paired video distinguish video greater motion intensity paradigm friendly annotation easy scale achieve stable performance motion estimation present new model named motionstone developed decoupled motion estimator experimental result demonstrate stability proposed motion estimator stateoftheart performance motionstone generation advantage warrant decoupled motion estimator serve general plugin enhancer data processing video generation training
scaling video summarization pretraining large language model longform video content constitutes significant portion internet traffic making automated video summarization essential research problem however existing video summarization datasets notably limited size constraining effectiveness stateoftheart method generalization work aim overcome limitation capitalizing abundance longform video dense speechtovideo alignment remarkable capability recent large language model llm summarizing long text introduce automated scalable pipeline generating largescale video summarization dataset using llm oracle summarizers leveraging generated dataset analyze limitation existing approach propose new video summarization model effectively address facilitate research field work also present new benchmark dataset contains long video highquality summary annotated professional extensive experiment clearly indicate proposed approach set new stateoftheart video summarization across several benchmark
wdm wavelet diffusion model highresolution medical image synthesis due threedimensional nature ct mrscans generative modeling medical image particularly challenging task existing approach mostly apply patchwise slicewise cascaded generation technique fit highdimensional data limited gpu memory however approach may introduce artifact potentially restrict model applicability certain downstream task work present wdm waveletbased medical image synthesis framework applies diffusion model wavelet decomposed image presented approach simple yet effective way scaling diffusion model high resolution trained single gpu experimental result brat lidcidri unconditional image generation resolution time time demonstrate stateoftheart image fidelity fid sample diversity msssim score compared recent gans diffusion model latent diffusion model proposed method one capable generating highquality image resolution time time outperforming comparing method
learning scalable generative model video diffusion model paper present novel method building scalable generative model utilizing pretrained video diffusion model primary obstacle developing foundation generative model limited availability data unlike image text video data readily accessible difficult acquire result significant disparity scale compared vast quantity type data address issue propose using video diffusion model trained extensive volume text image video knowledge source data unlocking multiview generative capability finetuning generate largescale synthetic multiview dataset train feedforward generative model proposed model trained nearly synthetic multiview data generate asset single image second achieves superior performance compared current sota feedforward generative model user preferring result time
dualpath collaborative generation network emotional video captioning emotional video captioning emerging task aim describe factual content intrinsic emotion expressed video essential evc task effectively perceive subtle ambiguous visual emotional cue caption generation neglected traditional video captioning existing emotional video captioning method perceive global visual emotional cue first combine video feature guide emotional caption generation neglect two characteristic evc task firstly method neglect dynamic subtle change intrinsic emotion video make difficult meet need common scene diverse changeable emotion secondly method incorporate emotional cue step guidance role emotion overemphasized make factual content less ignored generation end propose dualpath collaborative generation network dynamically perceives visual emotional cue evolution generating emotional caption collaborative learning specifically dynamic emotion perception path propose dynamic emotion evolution module first aggregate visual feature historical caption feature summarize global visual emotional cue dynamically selects emotional cue required recomposed stage besides adaptive caption generation path balance description factual content emotional cue propose emotion adaptive decoder thus method generate emotionrelated word necessary time step caption generation balance guidance factual content emotional cue well extensive experiment three challenging datasets demonstrate superiority approach proposed module
flexcache flexible approximate cache system video diffusion texttovideo application receive increasing attention public among diffusion model emerged prominent approach offering impressive quality visual content generation however still suffers substantial computational complexity often requiring several minute generate single video prior research addressed computational overhead texttoimage diffusion model technique developed directly suitable video diffusion model due significantly larger cache requirement enhanced computational demand associated video generation present flexcache flexible approximate cache system address challenge two main design first compress cache saving storage compression strategy reduce time consumption average find approximate cache system achieve higher hit rate computation saving decoupling object background design tailored cache replacement policy support two technique mentioned better evaluation flexcache reach time higher throughput lower cost compared stateoftheart diffusion approximate cache system
customttt motion appearance customized video generation via testtime training benefiting largescale pretraining textvideo pair current texttovideo diffusion model generate highquality video text description besides given reference image video parameterefficient finetuning method ie lora generate highquality customized concept eg specific subject motion reference video however combining trained multiple concept different reference single network show obvious artifact end propose customttt joint custom appearance motion given video easily detail first analyze prompt influence current video diffusion model find loras needed specific layer appearance motion customization besides since lora trained individually propose novel testtime training technique update parameter combination utilizing trained customized model conduct detailed experiment verify effectiveness proposed method method outperforms several stateoftheart work qualitative quantitative evaluation
tweediemix improving multiconcept fusion diffusionbased imagevideo generation despite significant advancement customizing texttoimage video generation model generating image video effectively integrate multiple personalized concept remains challenging task address present tweediemix novel method composing customized diffusion model inference phase analyzing property reverse diffusion sampling approach divide sampling process two stage initial step apply multiple objectaware sampling technique ensure inclusion desired target object later step blend appearance custom concept denoised image space using tweedies formula result demonstrate tweediemix generate multiple personalized concept higher fidelity existing method moreover framework effortlessly extended imagetovideo diffusion model enabling generation video feature multiple personalized concept result source code anonymous project page
improving video understanding generation better caption present series aiming facilitate video understanding large videolanguage model lvlms video generation texttovideo model via dense precise caption series comprises annotated dense caption video various length source developed carefully designed data filtering annotating strategy sharecaptionervideo efficient capable captioning model arbitrary video highquality aesthetic video annotated simple yet superb lvlm reached sota performance three advancing video benchmark achieve taking aside nonscalable costly human annotator find using caption video naive multiframe frameconcatenation input strategy lead less detailed sometimes temporalconfused result argue challenge designing highquality video captioning strategy lie three aspect interframe precise temporal change understanding intraframe detailed content description framenumber scalability arbitrarylength video end meticulously designed differential video captioning strategy stable scalable efficient generating caption video arbitrary resolution aspect ratio length based construct contains highquality video spanning wide range category resulting caption encompass rich world knowledge object attribute camera movement crucially detailed precise temporal description event based develop sharecaptionervideo superior captioner capable efficiently generating highquality caption arbitrary video
dive ditbased video generation enhanced control generating highfidelity temporally consistent video autonomous driving scenario face significant challenge eg problematic maneuver corner case despite recent video generation work proposed tackcle mentioned problem ie model built top diffusion transformer dit work still missing targeted exploring potential multiview video generation scenario noticeably propose first ditbased framework specifically designed generating temporally multiview consistent video precisely match given birdseye view layout control specifically proposed framework leverage parameterfree spatial viewinflated attention mechanism guarantee crossview consistency joint crossattention module controlnettransformer integrated improve precision control demonstrate advantage extensively investigate qualitative comparison nuscenes dataset particularly challenging corner case summary effectiveness proposed method producing long controllable highly consistent video difficult condition proven effective
using diffusion prior video amodal segmentation object permanence human fundamental cue help understanding persistence object even fully occluded scene present day method object segmentation account amodal nature world work segmentation visible modal object amodal method exist singleimage segmentation method handle highlevels occlusion better inferred using temporal information multiframe method focused solely segmenting rigid object end propose tackle video amodal segmentation formulating conditional generation task capitalizing foundational knowledge video generative model method simple repurpose model condition sequence modal mask frame object along contextual pseudodepth map learn object boundary may occluded therefore extended hallucinate complete extent object followed content completion stage able inpaint occluded region object benchmark approach alongside wide array stateoftheart method four datasets show dramatic improvement upto amodal segmentation object occluded region
odvista omnidirectional video dataset superresolution quality enhancement task omnidirectional video increasingly deployed largely due latest advancement immersive virtual reality vr extended reality xr technology however adoption video streaming encounter challenge related bandwidth latency particularly mobility condition unmanned aerial vehicle uavs adaptive resolution compression aim preserve quality maintaining low latency constraint yet downscaling encoding still degrade quality introduce artifact machine learning mlbased superresolution sr quality enhancement technique offer promising solution enhancing detail recovery reducing compression artifact however current publicly available video sr datasets lack compression artifact limit research field bridge gap paper introduces omnidirectional video streaming dataset odvista comprises highresolution high quality video downscaled encoded four bitrate range using highefficiency video coding standard evaluation show dataset feature wide variety scene also span different level content complexity crucial robust solution perform well realworld scenario generalize across diverse visual environment additionally evaluate performance considering quality enhancement runtime two handcrafted two mlbased sr model validation testing set odvista
hawk learning understand openworld video anomaly video anomaly detection vad system autonomously monitor identify disturbance reducing need manual labor associated cost however current vad system often limited superficial semantic understanding scene minimal user interaction additionally prevalent data scarcity existing datasets restricts applicability openworld scenario paper introduce hawk novel framework leverage interactive large visual language model vlm interpret video anomaly precisely recognizing difference motion information abnormal normal video hawk explicitly integrates motion modality enhance anomaly identification reinforce motion attention construct auxiliary consistency loss within motion video space guiding video branch focus motion modality moreover improve interpretation motiontolanguage establish clear supervisory relationship motion linguistic representation furthermore annotated anomaly video language description enabling effective training across diverse openworld scenario also created questionanswering pair user openworld question final result demonstrate hawk achieves sota performance surpassing existing baseline video description generation questionanswering codesdatasetdemo released httpsgithubcomjqtangusthawk
llmenhanced world model diverse driving video generation world model demonstrated superiority autonomous driving particularly generation multiview driving video however significant challenge still exist generating customized driving video paper propose build upon framework drivedreamer incorporates large language model llm generate userdefined driving video specifically llm interface initially incorporated convert user query agent trajectory subsequently hdmap adhering traffic regulation generated based trajectory ultimately propose unified multiview model enhance temporal spatial coherence generated driving video first world model generate customized driving video generate uncommon driving video eg vehicle abruptly cut userfriendly manner besides experimental result demonstrate generated video enhance training driving perception method eg detection tracking furthermore video generation quality surpasses stateoftheart method showcasing fid fvd score representing relative improvement
vimo generating motion casual video although human innate ability imagine multiple possible action video remains extraordinary challenge computer due intricate camera movement montage existing motion generation method predominantly rely manually collected motion datasets usually tediously sourced motion capture mocap system multiview camera unavoidably resulting limited size severely undermines generalizability inspired recent advance diffusion model probe simple effective way capture motion video propose novel videotomotiongeneration framework vimo could leverage immense trove untapped video content produce abundant diverse human motion distinct prior work video could causal including complicated camera movement occlusion striking experimental result demonstrate proposed model could generate natural motion even video rapid movement varying perspective frequent occlusion might exist also show work could enable three important downstream application generating dancing motion according arbitrary music source video style extensive experimental result prove model offer effective scalable way generate diversity realistic motion code demo public soon
followyourcanvas higherresolution video outpainting extensive content generation paper explores higherresolution video outpainting extensive content generation point common issue faced existing method attempting largely outpaint video generation lowquality content limitation imposed gpu memory address challenge propose diffusionbased method called textitfollowyourcanvas build upon two core design first instead employing common practice singleshot outpainting distribute task across spatial window seamlessly merge allows u outpaint video size resolution without constrained gpu memory second source video relative positional relation injected generation process window make generated spatial layout within window harmonize source video coupling two design enables u generate higherresolution outpainting video rich content keeping spatial temporal consistency followyourcanvas excels largescale video outpainting eg producing highquality aesthetically pleasing result achieves best quantitative result across various resolution scale setup code released httpsgithubcommayuelalafollowyourcanvas
loopanimate loopable salient object animation research diffusion modelbased video generation advanced rapidly however limitation object fidelity generation length hinder practical application additionally specific domain like animated wallpaper require seamless looping first last frame video match seamlessly address challenge paper proposes loopanimate novel method generating video consistent start end frame enhance object fidelity introduce framework decouples multilevel image appearance textual semantic information building upon imagetoimage diffusion model approach incorporates pixellevel featurelevel information input image injecting image appearance textual semantic embeddings different position diffusion model existing unetbased video generation model require input entire video training encode temporal positional information however due limitation gpu memory number frame typically restricted address paper proposes threestage training strategy progressively increasing frame number reducing finetuning module additionally introduce temporal e nhanced motion moduletemm extend capacity encoding temporal positional information frame proposed loopanimate first time extends singlepass generation length unetbased video generation model frame maintaining highquality video generation experiment demonstrate loopanimate achieves stateoftheart performance objective metric fidelity temporal consistency subjective evaluation result
stereotalker audiodriven human synthesis priorguided mixtureofexperts paper introduces stereotalker novel oneshot audiodriven human video synthesis system generates talking video precise lip synchronization expressive body gesture temporally consistent photorealistic quality continuous viewpoint control process follows twostage approach first stage system map audio input highfidelity motion sequence encompassing upperbody gesture facial expression enrich motion diversity authenticity large language model llm prior integrated textaligned semantic audio feature leveraging llm crossmodal generalization power enhance motion quality second stage improve diffusionbased video generation model incorporating priorguided mixtureofexperts moe mechanism viewguided moe focus viewspecific attribute maskguided moe enhances regionbased rendering stability additionally mask prediction module devised derive human mask motion data enhancing stability accuracy mask enabling mask guiding inference also introduce comprehensive human video dataset identity covering diverse body gesture detailed annotation facilitating broad generalization code data pretrained model released research purpose
finevq finegrained user generated content video quality assessment rapid growth usergenerated content ugc video produced urgent need effective video quality assessment vqa algorithm monitor video quality guide optimization recommendation procedure however current vqa model generally give overall rating ugc video lack finegrained label serving video processing recommendation application address challenge promote development ugc video establish first largescale finegrained video quality assessment database termed finevd comprises ugc video finegrained quality score description across multiple dimension based database propose finegrained video quality assessment finevq model learn finegrained quality ugc video capability quality rating quality scoring quality attribution extensive experimental result demonstrate proposed finevq produce finegrained videoquality result achieve stateoftheart performance finevd commonly used ugcvqa datasets finevd finevq made publicly available
step difference instructional video comparing user video reference howto video key requirement arvr technology delivering personalized assistance tailored user progress however current approach languagebased assistance answer question single video propose approach first automatically generates large amount visual instruction tuning data involving pair video leveraging existing step annotation accompanying narration train videoconditioned language model jointly reason across multiple raw video model achieves stateoftheart performance identifying difference video pair ranking video based severity difference show promising ability perform general reasoning multiple video project page httpsgithubcomfacebookresearchstepdiff
navero unlocking finegrained semantics videolanguage compositionality study capability videolanguage vidl model understanding composition object attribute action relation composition understanding becomes particularly challenging video data since compositional relation rapidly change time video first build benchmark named aaro evaluate composition understanding related action top spatial concept benchmark constructed generating negative text incorrect action description given video model expected pair positive text corresponding video furthermore propose training method called navero utilizes videotext data augmented negative text enhance composition understanding also develop negativeaugmented visuallanguage matching loss used explicitly benefit generated negative text compare navero stateoftheart method term compositional understanding well videotext retrieval performance navero achieves significant improvement method videolanguage imagelanguage composition understanding maintaining strong performance traditional textvideo retrieval task
textconditioned hdr image generation realtime ondevice video portrait relighting paper present approach realtime video portrait relighting mobile device utilizing textconditioned generation high dynamic range image hdri map method proposes diffusionbased image generation hdr domain taking advantage standard technique facilitates generation highquality realistic lighting condition textual description offering flexibility control portrait video relighting task unlike previous relighting framework proposed system performs video relighting directly ondevice enabling realtime inference real hdri map ondevice processing ensures privacy guarantee low runtime providing immediate response change lighting condition user input approach pave way new possibility realtime video application including video conferencing gaming augmented reality allowing dynamic textbased control lighting condition
trackgo flexible efficient method controllable video generation recent year seen substantial progress diffusionbased controllable video generation however achieving precise control complex scenario including finegrained object part sophisticated motion trajectory coherent background movement remains challenge paper introduce trackgo novel approach leverage freeform mask arrow conditional video generation method offer user flexible precise mechanism manipulating video content also propose trackadapter control implementation efficient lightweight adapter designed seamlessly integrated temporal selfattention layer pretrained video generation model design leverage observation attention map layer accurately activate region corresponding motion video experimental result demonstrate new approach enhanced trackadapter achieves stateoftheart performance key metric fvd fid objmc score
artificial intelligence biomedical video generation prominent subfield artificial intelligence generated content aigc video generation achieved notable advancement recent year introduction soraalike model represents pivotal breakthrough video generation technology significantly enhancing quality synthesized video particularly realm biomedicine video generation technology shown immense potential medical concept explanation disease simulation biomedical data augmentation article thoroughly examine latest development video generation model explore application challenge future opportunity biomedical sector conducted extensive review compiled comprehensive list datasets various source facilitate development evaluation video generative model biomedicine given rapid progress field also created github repository regularly update advance biomedical video generation
intragen trajectorycontrolled video generation object interaction advance video generation significantly improved realism quality created scene fueled interest developing intuitive tool let user leverage video generation world simulator texttovideo generation one approach enabling video creation text description yet due inherent ambiguity text limited temporal information offered text prompt researcher explored additional control signal like trajectoryguided system accurate generation nonetheless method evaluate whether model generate realistic interaction multiple object lacking introduce intragen pipeline improved trajectorybased generation object interaction scenario propose new datasets novel trajectory quality metric evaluate performance proposed intragen achieve object interaction introduce multimodal interaction encoding pipeline object id injection mechanism enriches objectenvironment interaction result demonstrate improvement visual fidelity quantitative performance code datasets available httpsgithubcominsaitinstituteintragen
video diffusion model trainingfree motion interpreter controller video generation primarily aim model authentic customized motion across frame making understanding controlling motion crucial topic diffusionbased study video motion focus motion customization trainingbased paradigm however demand substantial training resource necessitates retraining diverse model crucially approach explore video diffusion model encode crossframe motion information feature lacking interpretability transparency effectiveness answer question paper introduces novel perspective understand localize manipulate motionaware feature video diffusion model analysis using principal component analysis pca work discloses robust motionaware feature already exists video diffusion model present new motion feature moft eliminating content correlation information filtering motion channel moft provides distinct set benefit including ability encode comprehensive motion information clear interpretability extraction without need training generalizability across diverse architecture leveraging moft propose novel trainingfree video motion control framework method demonstrates competitive performance generating natural faithful motion providing architectureagnostic insight applicability variety downstream task
kinetic typography diffusion model paper introduces method realistic kinetic typography generates userpreferred animatable text content draw recent advance guided video diffusion model achieve visuallypleasing text appearance first construct kinetic typography dataset comprising video dataset made variety combination template designed professional motion graphic designer involves changing letter position glyph size ie flying glitch chromatic aberration reflecting effect etc next propose video diffusion model kinetic typography three requirement aesthetic appearance motion effect readable letter paper identifies requirement present static dynamic caption used spatial temporal guidance video diffusion model respectively static caption describes overall appearance video color texture glyph represent shape letter dynamic caption account movement letter background add one guidance zero convolution determine text content visible video apply zero convolution text content impose diffusion model lastly glyph loss minimizing difference predicted word groundtruth proposed make prediction letter readable experiment show model generates kinetic typography video legible artistic letter motion based text prompt
seeclear semantic distillation enhances pixel condensation video superresolution diffusionbased video superresolution vsr renowned generating perceptually realistic video yet grapple maintaining detail consistency across frame due stochastic fluctuation traditional approach pixellevel alignment ineffective diffusionprocessed frame iterative disruption overcome introduce seecleara novel vsr framework leveraging conditional video generation orchestrated instancecentric channelwise semantic control framework integrates semantic distiller pixel condenser synergize extract upscale semantic detail lowresolution frame instancecentric alignment module incam utilizes videoclipwise token dynamically relate pixel within across frame enhancing coherency additionally channelwise texture aggregation memory category infuses extrinsic knowledge capitalizing longstanding semantic texture method also innovates blurring diffusion process resshift mechanism finely balancing sharpness diffusion effect comprehensive experiment confirm framework advantage stateoftheart diffusionbased vsr technique code available
reinforcement learningbased automatic video editing method using pretrained visionlanguage model era video automatic video editing technique attract attention industry academia since reduce workload lower requirement human editor existing automatic editing system mainly scene eventspecific eg soccer game broadcasting yet automatic system general editing eg movie vlog editing cover various scene event rarely studied converting eventdriven editing method general scene nontrivial paper propose twostage scheme general editing firstly unlike previous work extract scenespecific feature leverage pretrained visionlanguage model vlm extract editingrelevant representation editing context moreover close gap professionallooking video automatic production generated simple guideline propose reinforcement learning rlbased editing framework formulate editing problem train virtual editor make better sequential editing decision finally evaluate proposed method general editing task real movie dataset experimental result demonstrate effectiveness benefit proposed context representation learning ability rlbased editing framework
moditalker motiondisentangled diffusion model highfidelity talking head generation conventional ganbased model talking head generation often suffer limited quality unstable training recent approach based diffusion model aimed address limitation improve fidelity however still face challenge including extensive sampling time difficulty maintaining temporal consistency due high stochasticity diffusion model overcome challenge propose novel motiondisentangled diffusion model highquality talking head generation dubbed moditalker introduce two module audiotomotion atom designed generate synchronized lip motion audio motiontovideo mtov designed produce highquality head video following generated motion atom excels capturing subtle lip movement leveraging audio attention mechanism addition mtov enhances temporal consistency leveraging efficient triplane representation experiment conducted standard benchmark demonstrate model achieves superior performance compared existing model also provide comprehensive ablation study user study result
contextaware video anomaly detection longterm datasets video anomaly detection research generally evaluated short isolated benchmark video minute long however realworld environment security camera observe scene month year time notion anomalous behavior critically depends context time day day week schedule event propose contextaware video anomaly detection algorithm trinity specifically targeted scenario trinity especially wellsuited crowded scene individual easily tracked anomaly due speed direction absence group motion trinity contrastive learning framework aim learn alignment context appearance motion us alignment quality classify video normal anomalous evaluate algorithm conventional benchmark public webcambased dataset collected span three month activity
genad generalized predictive model autonomous driving paper introduce first largescale video prediction model autonomous driving discipline eliminate restriction highcost data collection empower generalization ability model acquire massive data web pair diverse highquality text description resultant dataset accumulates hour driving video spanning area world diverse weather condition traffic scenario inheriting merit recent latent diffusion model model dubbed genad handle challenging dynamic driving scene novel temporal reasoning block showcase generalize various unseen driving datasets zeroshot manner surpassing general drivingspecific video prediction counterpart furthermore genad adapted actionconditioned prediction model motion planner holding great potential realworld driving application
generative expansion small datasets expansive graph approach limited data availability machine learning significantly impact performance generalization traditional augmentation method enhance moderately sufficient datasets gans struggle convergence generating diverse sample diffusion model effective high computational cost introduce expansive synthesis model generating largescale informationrich datasets minimal sample us expander graph mapping feature interpolation preserve data distribution feature relationship model leverage neural network nonlinear latent space captured koopman operator create linear feature space dataset expansion autoencoder selfattention layer optimal transport refines distributional consistency validate comparing classifier trained generated data trained original datasets result show comparable performance demonstrating model potential augment training data effectively work advance data generation addressing scarcity machine learning application
animating model multiview video diffusion recent advance generation mainly focus generating content distilling pretrained text singleview imageconditioned model inconvenient take advantage various offtheshelf asset multiview attribute result suffer spatiotemporal inconsistency owing inherent ambiguity supervision signal work present novel framework animating static model core idea twofold propose novel multiview video diffusion model mvvdm conditioned multiview rendering static object trained presented largescale multiview video dataset mvvideo based mvvdm introduce framework combining reconstruction score distillation sampling leverage multiview video diffusion prior animating object specifically mvvdm design new spatiotemporal attention module enhance spatial temporal consistency integrating video diffusion model additionally leverage static model multiview rendering condition preserve identity animating model effective twostage pipeline proposed first reconstruct motion directly generated multiview video followed introduced refine appearance motion benefiting accurate motion learning could achieve straightforward mesh animation qualitative quantitative experiment demonstrate significantly outperforms previous approach data code model openreleased
nuclass net novel approach video quality enhancement video content experienced surge popularity asserting dominance internet traffic internet thing iot network video compression long regarded primary mean efficiently managing substantial multimedia traffic generated videocapturing device nevertheless video compression algorithm entail significant computational demand order achieve substantial compression ratio complexity present formidable challenge implementing efficient video coding standard resourceconstrained embedded system iot edge node camera tackle challenge paper introduces nuclass net innovative deeplearning model designed mitigate compression artifact stemming lossy compression codecs enhancement significantly elevates perceptible quality lowbitrate video employing nuclass net video encoder within videocapturing node reduce output quality thereby generating lowbitrate video effectively curtailing computation bandwidth requirement edge decoder side typically less encumbered resource limitation nuclass net applied video decoder compensate artifact approximate quality original video experimental result affirm efficacy proposed model enhancing perceptible quality video especially streamed low bit rate
vanebench video anomaly evaluation benchmark conversational lmms recent development large multimodal video model videolmms significantly enhanced ability interpret analyze video data despite impressive capability current videolmms evaluated anomaly detection task critical deployment practical scenario eg towards identifying deepfakes manipulated video content traffic accident crime paper introduce vanebench benchmark designed assess proficiency videolmms detecting localizing anomaly inconsistency video dataset comprises array video synthetically generated using existing stateoftheart texttovideo generation model encompassing variety subtle anomaly inconsistency grouped five category unnatural transformation unnatural appearance passthrough disappearance sudden appearance additionally benchmark feature realworld sample existing anomaly detection datasets focusing crimerelated irregularity atypical pedestrian behavior unusual event task structured visual questionanswering challenge gauge model ability accurately detect localize anomaly within video evaluate nine existing videolmms open closed source benchmarking task find model encounter difficulty effectively identifying subtle anomaly conclusion research offer significant insight current capability videolmms realm anomaly detection highlighting importance work evaluating improving model realworld application code data available httpshananshafigithubiovanebenchmark
scbench sport commentary benchmark video llm recently significant advance made video large language model video llm academia industry however method evaluate benchmark performance different video llm especially finegrained temporal visual capability remain limited one hand current benchmark use relatively simple video eg subtitled movie clip model understand entire video processing frame hand datasets lack diversity task format comprising qa multichoice qa overlook model capacity generating indepth precise text sport video feature intricate visual information sequential event emotionally charged commentary present critical challenge video llm making sport commentary ideal benchmarking task inspired challenge propose novel task sport video commentary generation developed textbfscbench video llm construct benchmark introduce textbfscores sixdimensional metric specifically designed task upon propose gptbased evaluation method textbfcommentaryset dataset consisting annotated video clip groundtruth label tailored metric based scbench conduct comprehensive evaluation multiple video llm eg vila videollava etc chainofthought baseline method result found achieves best performance surpassing secondbest work provides fresh perspective future research aiming enhance model overall capability complex visual understanding task dataset released soon
video prediction model general visual encoders study explores potential opensource video conditional generation model encoders downstream task focusing instance segmentation using bair robot pushing dataset researcher propose using video prediction model general visual encoders leveraging ability capture critical spatial temporal information essential task instance segmentation inspired human vision study particularly gestalt principle common fate approach aim develop latent space representative motion image effectively discern foreground background information researcher utilize vectorquantized variational autoencoder vqvae video generative encoder model conditioned input frame coupled downstream segmentation task experiment involve adapting pretrained video generative model analyzing latent space training custom decoder foregroundbackground segmentation finding demonstrate promising result leveraging generative pretext learning downstream task working towards enhanced scene analysis segmentation computer vision application
msc multiscale spatiotemporal causal attention autoregressive video diffusion diffusion transformer enable flexible generative modeling video however still technically challenging computationally expensive generate highresolution video rich semantics complex motion similar language video data also autoregressive nature counterintuitive use attention mechanism bidirectional dependency model propose multiscale causal msc framework address problem specifically introduce multiple resolution spatial dimension highlow frequency temporal dimension realize efficient attention calculation furthermore attention block multiple scale combined controlled way allow causal conditioning noisy image frame diffusion training based idea noise destroys information different rate different resolution theoretically show approach greatly reduce computational complexity enhance efficiency training causal attention diffusion framework also used autoregressive long video generation without violating natural order frame sequence
vibidsampler enhancing video interpolation using bidirectional diffusion sampler recent progress largescale texttovideo imagetovideo diffusion model greatly enhanced video generation especially term keyframe interpolation however current imagetovideo diffusion model powerful generating video single conditioning frame need adaptation twoframe start end conditioned generation essential effective bounded interpolation unfortunately existing approach fuse temporally forward backward path parallel often suffer offmanifold issue leading artifact requiring multiple iterative renoising step work introduce novel bidirectional sampling strategy address offmanifold issue without requiring extensive renoising finetuning method employ sequential sampling along forward backward path conditioned start end frame respectively ensuring coherent onmanifold generation intermediate frame additionally incorporate advanced guidance technique cfg dd enhance interpolation process integrating method achieves stateoftheart performance efficiently generating highquality smooth video keyframes single gpu method interpolate frame x resolution second establishing leading solution keyframe interpolation
harivo harnessing texttoimage model video generation present method create diffusionbased video model pretrained texttoimage model recently animatediff proposed freezing model training temporal layer advance method proposing unique architecture incorporating mapping network framewise token tailored video generation maintaining diversity creativity original model key innovation include novel loss function temporal smoothness mitigating gradient sampling technique ensuring realistic temporally consistent video generation despite limited public video data successfully integrated videospecific inductive bias architecture loss function method built frozen stablediffusion model simplifies training process allows seamless integration offtheshelf model like controlnet dreambooth project page httpskwonminkigithubioharivo
starvid enhancing semantic alignment video diffusion model via spatial syntactic guided attention refocusing recent advance texttovideo generation diffusion model garnered significant attention however typically perform well scene single object motion struggling compositional scenario multiple object distinct motion accurately reflect semantic content text prompt address challenge propose textbfstarvid plugandplay trainingfree method improves semantic alignment multiple subject motion text prompt model starvid first leverage spatial reasoning capability large language model llm twostage motion trajectory planning based text prompt trajectory serve spatial prior guiding spatialaware loss refocus crossattention ca map distinctive region furthermore propose syntaxguided contrastive constraint strengthen correlation ca map verb corresponding noun enhancing motionsubject binding qualitative quantitative evaluation demonstrate proposed framework significantly outperforms baseline method delivering video higher quality improved semantic consistency
mmdisco multimodal discriminatorguided cooperative diffusion joint audio video generation study aim construct audiovideo generative model minimal computational cost leveraging pretrained singlemodal generative model audio video achieve propose novel method guide singlemodal model cooperatively generate wellaligned sample across modality specifically given two pretrained base diffusion model train lightweight joint guidance module adjust score separately estimated base model match score joint distribution audio video show guidance computed using gradient optimal discriminator distinguishes real audiovideo pair fake one independently generated base model based analysis construct joint guidance module training discriminator additionally adopt loss function stabilize discriminator gradient make work noise estimator standard diffusion model empirical evaluation several benchmark datasets demonstrate method improves singlemodal fidelity multimodal alignment relatively parameter code available httpsgithubcomsonyresearchmmdisco
mojito motion trajectory intensity control video generation recent advancement diffusion model shown great promise producing highquality video content however efficiently training video diffusion model capable integrating directional guidance controllable motion intensity remains challenging underexplored area tackle challenge paper introduces mojito diffusion model incorporates motion trajectory intensity control texttovideo generation specifically mojito feature directional motion control dmc module leverage crossattention efficiently direct generated object motion without training alongside motion intensity modulator mim us optical flow map generated video guide varying level motion intensity extensive experiment demonstrate mojitos effectiveness achieving precise trajectory intensity control high computational efficiency generating motion pattern closely match specified direction intensity providing realistic dynamic align well natural motion realworld scenario
videodpo omnipreference alignment video diffusion generation recent progress generative diffusion model greatly advanced texttovideo generation texttovideo model trained largescale diverse datasets produce varied output generation often deviate user preference highlighting need preference alignment pretrained model although direct preference optimization dpo demonstrated significant improvement language image generation pioneer adaptation video diffusion model propose videodpo pipeline making several key adjustment unlike previous image alignment method focus solely either visual quality ii semantic alignment text video comprehensively consider dimension construct preference score accordingly term omniscore design pipeline automatically collect preference pair data based proposed omniscore discover reweighting pair based score significantly impact overall preference alignment experiment demonstrate substantial improvement visual quality semantic alignment ensuring preference aspect neglected code data shared httpsvideodpogithubio
lightningdrag lightning fast accurate dragbased image editing emerging video accuracy speed critical image editing task pan et al introduced dragbased image editing framework achieves pixellevel control using generative adversarial network gans flurry subsequent study enhanced framework generality leveraging largescale diffusion model however method often suffer inordinately long processing time exceeding minute per edit low success rate addressing issue head present lightningdrag rapid approach enabling high quality dragbased image editing second unlike previous method redefine dragbased editing conditional generation task eliminating need timeconsuming latent optimization gradientbased guidance inference addition design pipeline allows u train model largescale paired video frame contain rich motion information object translation changing pose orientation zooming etc learning video approach significantly outperform previous method term accuracy consistency despite trained solely video model generalizes well perform local shape deformation presented training data eg lengthening hair twisting rainbow etc extensive qualitative quantitative evaluation benchmark datasets corroborate superiority approach code model released httpsgithubcommagicresearchlightningdrag
drsm efficient neural decomposition dynamic reconstruction stationary monocular camera popularity monocular video generated video sharing live broadcasting application reconstructing editing dynamic scene stationary monocular camera become special anticipated technology contrast scene reconstruction exploit multiview observation problem modeling dynamic scene single view significantly underconstrained illposed inspired recent progress neural rendering present novel framework tackle decomposition problem dynamic scene monocular camera framework utilizes decomposed static dynamic feature plane represent scene emphasizes learning dynamic region dense ray casting inadequate clue singleview occlusion also particular challenge scene reconstruction overcome difficulty propose deep supervised optimization ray casting strategy experiment various video method generates higherfidelity result existing method singleview dynamic scene representation
see matter novel visual physicsbased metric evaluating video generation quality video generation model advance rapidly assessing quality generated video become increasingly critical existing metric frechet video distance fvd inception score clipsim measure quality primarily latent space rather human visual perspective often overlooking key aspect like appearance motion consistency physical law paper propose novel metric vamp visual appearance motion plausibility evaluates visual appearance physical plausibility generated video vamp composed two main component appearance score assesses color shape texture consistency across frame motion score evaluates realism object movement validate vamp two experiment corrupted video evaluation generated video evaluation corrupted video evaluation introduce various type corruption real video measure correlation corruption severity vamp score generated video evaluation use stateoftheart model generate video carefully designed prompt compare vamp performance human evaluator ranking result demonstrate vamp effectively capture visual fidelity temporal consistency offering comprehensive evaluation video quality traditional method
far video generation world model physical law perspective openais sora highlight potential video generation developing world model adhere fundamental physical law however ability video generation model discover law purely visual data without human prior questioned world model learning true law give prediction robust nuance correctly extrapolate unseen scenario work evaluate across three key scenario indistribution outofdistribution combinatorial generalization developed simulation testbed object movement collision generate video deterministically governed one classical mechanic law provides unlimited supply data largescale experimentation enables quantitative evaluation whether generated video adhere physical law trained diffusionbased video generation model predict object movement based initial frame scaling experiment show perfect generalization within distribution measurable scaling behavior combinatorial generalization failure outofdistribution scenario experiment reveal two key insight generalization mechanism model model fail abstract general physical rule instead exhibit casebased generalization behavior ie mimicking closest training example generalizing new case model observed prioritize different factor referencing training data color size velocity shape study suggests scaling alone insufficient video generation model uncover fundamental physical law despite role soras broader success see project page httpsphyworldgithubio
mvbind selfsupervised music recommendation video via embedding space binding recent year witnessed rapid development short video usually contain visual audio modality background music important short video significantly influence emotion viewer however present background music short video generally chosen video producer lack automatic music recommendation method short video paper introduces mvbind innovative musicvideo embedding space binding model crossmodal retrieval mvbind operates selfsupervised approach acquiring inherent knowledge intermodal relationship directly data without need manual annotation additionally compensate lack corresponding musicalvisual pair dataset short video construct dataset video mainly consists meticulously selected short video dataset mvbind manifest significantly improved performance compared baseline method constructed dataset code released facilitate future research
humanannotated video dataset training evaluation video summarization method paper introduce new dataset video summarization transformation video content concise summary consumed via traditional device tv set smartphones dataset includes groundtruth humangenerated summary used training objectively evaluating video summarization method using dataset train assess two stateoftheart summarization method originally proposed summarization serve baseline future comparison summarization method specifically tailored video finally present interactive tool developed facilitate data annotation process assist annotation activity rely video fragment selection
vidcompress memoryenhanced temporal compression video understanding large language model videobased multimodal large language model videollms possess significant potential video understanding task however videollms treat video sequential set individual frame result insufficient temporalspatial interaction hinders finegrained comprehension difficulty processing longer video due limited visual token capacity address challenge propose vidcompress novel videollm featuring memoryenhanced temporal compression vidcompress employ dualcompressor approach memoryenhanced compressor capture shortterm longterm temporal relationship video compress visual token using multiscale transformer memorycache mechanism textperceived compressor generates condensed visual token utilizing qformer integrating temporal context query embeddings cross attention experiment several videoqa datasets comprehensive benchmark demonstrate vidcompress efficiently model complex temporalspatial relation significantly outperforms existing videollms
syncvis synchronized video instance segmentation recent detrbased method advanced development video instance segmentation vi transformer efficiency capability modeling spatial temporal information despite harvesting remarkable progress existing work follow asynchronous design model video sequence via either videolevel query adopting querysensitive cascade structure resulting difficulty handling complex challenging video scenario work analyze cause phenomenon limitation current solution propose conduct synchronized modeling via new framework named syncvis specifically syncvis explicitly introduces videolevel query embeddings design two key module synchronize videolevel query framelevel query embeddings synchronized videoframe modeling paradigm synchronized embedding optimization strategy former attempt promote mutual learning frame videolevel embeddings latter divide large video sequence small clip easier optimization extensive experimental evaluation conducted challenging youtubevis ovis benchmark syncvis achieves stateoftheart result demonstrates effectiveness generality proposed approach code available
snapgenv generating fivesecond video within five second mobile device witnessed unprecedented success diffusionbased video generation past year recently proposed model community wielded power generate cinematic highresolution video smooth motion arbitrary input prompt however supertask image generation video generation model require computation thus hosted mostly cloud server limiting broader adoption among content creator work propose comprehensive acceleration framework bring power largescale video diffusion model hand edge user network architecture scope initialize compact image backbone search design arrangement temporal layer maximize hardware efficiency addition propose dedicated adversarial finetuning algorithm efficient model reduce denoising step model parameter generate video iphone pm within second compared serverside model take minute powerful gpus generate single video accelerate generation magnitude delivering onpar quality
pretraining action recognition automatically generated fractal datasets recent year interest synthetic data grown particularly context pretraining image modality support range computer vision task including object classification medical imaging etc previous work demonstrated synthetic sample automatically produced various generative process replace real counterpart yield strong visual representation approach resolve issue associated real data collection labeling cost copyright privacy extend trend video domain applying task action recognition employing fractal geometry present method automatically produce largescale datasets short synthetic video clip utilized pretraining neural model generated video clip characterized notable variety stemmed innate ability fractal generate complex multiscale structure narrow domain gap identify key property real video carefully emulate pretraining thorough ablation determine attribute strengthen downstream result offer general guideline pretraining synthetic video proposed approach evaluated finetuning pretrained model established action recognition datasets well four video benchmark related group action recognition finegrained action recognition dynamic scene compared standard kinetics pretraining reported result come close even superior portion downstream datasets code sample synthetic video available
mobile video diffusion video diffusion model achieved impressive realism controllability limited high computational demand restricting use mobile device paper introduces first mobileoptimized video diffusion model starting spatiotemporal unet stable video diffusion svd reduce memory computational cost reducing frame resolution incorporating multiscale temporal representation introducing two novel pruning schema reduce number channel temporal block furthermore employ adversarial finetuning reduce denoising single step model coined mobilevd efficient v tflops slight quality drop fvd v generating latents px clip second pro result available httpsqualcommairesearchgithubiomobilevideodiffusion
audiodriven emotional talkinghead generation audiodriven video portrait synthesis crucial useful technology virtual human interaction filmmaking application recent advancement focused improving image fidelity lipsynchronization however generating accurate emotional expression important aspect realistic talkinghead generation remained underexplored previous work present novel system paper synthesizing highfidelity audiodriven video portrait accurate emotional expression specifically utilize variational autoencoder vaebased audiotomotion module generate facial landmark landmark concatenated emotional embeddings produce emotional landmark motiontoemotion module emotional landmark used render realistic emotional talkinghead video using neural radiance field nerfbased emotiontovideo module additionally propose pose sampling method generates natural idlestate nonspeaking video response silent audio input extensive experiment demonstrate method obtains accurate emotion generation higher fidelity
enhancing motion texttovideo generation decomposed encoding conditioning despite advancement texttovideo generation producing video realistic motion remains challenging current model often yield static minimally dynamic output failing capture complex motion described text issue stem internal bias text encoding overlook motion inadequate conditioning mechanism generation model address propose novel framework called decomposed motion demo enhances motion synthesis generation decomposing text encoding conditioning content motion component method includes content encoder static element motion encoder temporal dynamic alongside separate content motion conditioning mechanism crucially introduce textmotion videomotion supervision improve model understanding generation motion evaluation benchmark msrvtt evalcrafter vbench demonstrate demo superior ability produce video enhanced motion dynamic maintaining high visual quality approach significantly advance generation integrating comprehensive motion understanding directly textual description project page httpsprryangithubiodemoproject
survey generative ai llm video generation understanding streaming paper offer insightful examination currently toptrending ai technology ie generative artificial intelligence generative ai large language model llm reshaping field video technology including video generation understanding streaming highlight innovative use technology producing highly realistic video significant leap bridging gap realworld dynamic digital creation study also delf advanced capability llm video understanding demonstrating effectiveness extracting meaningful information visual content thereby enhancing interaction video realm video streaming paper discusses llm contribute efficient usercentric streaming experience adapting content delivery individual viewer preference comprehensive review navigates current achievement ongoing challenge future possibility applying generative ai llm videorelated task underscoring immense potential technology hold advancing field video technology related multimedia networking ai community
mmldm multimodal latent diffusion model sounding video generation sounding video generation svg audiovideo joint generation task challenged highdimensional signal space distinct data format different pattern content information address issue introduce novel multimodal latent diffusion model mmldm svg task first unify representation audio video data converting single couple image introduce hierarchical multimodal autoencoder construct lowlevel perceptual latent space modality shared highlevel semantic feature space former space perceptually equivalent raw signal space modality drastically reduces signal dimension latter space serf bridge information gap modality provides insightful crossmodal guidance proposed method achieves new stateoftheart result significant quality efficiency gain specifically method achieves comprehensive improvement evaluation metric faster training sampling speed landscape aist datasets moreover explore performance opendomain sounding video generation long sounding video generation audio continuation video continuation conditional singlemodal generation task comprehensive evaluation mmldm demonstrates exciting adaptability generalization ability
spectrum semantic processing emotioninformed videocaptioning retrieval understanding modality capturing video meaning critical concept analyzing subtle detail fundamental yet challenging task video captioning identifying dominant emotional tone video significantly enhances perception context despite strong emphasis video captioning existing model often need adequately address emotional theme resulting suboptimal captioning result address limitation paper proposes novel semantic processing emotioninformed videocaptioning retrieval understanding modality spectrum framework empower generation emotionally semantically credible caption leveraging pioneering structure spectrum discerns multimodal semantics emotional theme using visual text attribute investigation vtai determines orientation descriptive caption holistic conceptoriented theme hcot expressing emotionallyinformed fieldacquainted reference exploit videototext retrieval capability multifaceted nature video content estimate emotional probability candidate caption dominant theme video determined appropriately weighting embedded attribute vector applying coarse finegrained emotional concept define video contextual alignment furthermore using two loss function spectrum optimized integrate emotional information minimize prediction error extensive experiment emvidcap msvd msrvtt video captioning datasets demonstrate model significantly surpasses stateoftheart method quantitative qualitative evaluation highlight model ability accurately capture convey video emotion multimodal attribute
cgbench cluegrounded question answering benchmark long video understanding existing video understanding benchmark multimodal large language model mllms focus short video limited number benchmark long video understanding often rely solely multiplechoice question mcqs however inherent limitation mcqbased evaluation increasing reasoning ability mllms model give current answer purely combining short video understanding elimination without genuinely understanding video content address gap introduce cgbench novel benchmark designed cluegrounded question answering long video cgbench emphasizes model ability retrieve relevant clue question enhancing evaluation credibility feature manually curated video categorized granular system primary category secondary category tertiary category making largest benchmark long video analysis benchmark includes qa pair three major question type perception reasoning hallucination compensating drawback pure mcqbased evaluation design two novel cluebased evaluation method cluegrounded white box black box evaluation assess whether model generates answer based correct understanding video evaluate multiple closedsource opensource mllms cgbench result indicate current model significantly underperform understanding long video compared short one significant gap exists opensource commercial model hope cgbench advance development trustworthy capable mllms long video understanding annotation video data released httpscgbenchgithubioleaderboard
longvlm efficient long video understanding via large language model empowered large language model llm recent advancement videobased llm videollms driven progress various video understanding task model encode video representation pooling query aggregation vast number visual token making computational memory cost affordable despite successfully providing overall comprehension video content existing videollms still face challenge achieving detailed understanding due overlooking local information longterm video tackle challenge introduce longvlm simple yet powerful videollm long video understanding building upon observation long video often consist sequential key event complex action camera movement approach proposes decompose long video multiple shortterm segment encode local feature segment via hierarchical token merging module feature concatenated temporal order maintain storyline across sequential shortterm segment additionally propose integrate global semantics local feature enhance context understanding way encode video representation incorporate local global information enabling llm generate comprehensive response longterm video experimental result videochatgpt benchmark zeroshot video questionanswering datasets demonstrate superior capability model previous stateoftheart method qualitative example show model produce precise response long video understanding code available httpsgithubcomziplablongvlm
streaming long video understanding large language model paper present videostreaming advanced visionlanguage large model vllm video understanding capably understands arbitrarylength video constant number video token streamingly encoded adaptively selected challenge video understanding vision language area mainly lie significant computational burden caused great number token extracted long video previous work rely sparse sampling frame compression reduce token however approach either disregard temporal information long time span sacrifice spatial detail resulting flawed compression address limitation videostreaming two core design memorypropagated streaming encoding adaptive memory selection memorypropagated streaming encoding architecture segment long video short clip sequentially encodes clip propagated memory iteration utilize encoded result preceding clip historical memory integrated current clip distill condensed representation encapsulates video content current timestamp encoding process adaptive memory selection strategy selects constant number questionrelated memory historical memory feed llm generate informative response questionrelated selection reduces redundancy within memory enabling efficient precise video understanding meanwhile disentangled video extraction reasoning design allows llm answer different question video directly selecting corresponding memory without need encode whole video question model achieves superior performance higher efficiency long video benchmark showcasing precise temporal comprehension detailed question answering
groundedvideollm sharpening finegrained temporal grounding video large language model video large language model videollms demonstrated remarkable capability coarsegrained video understanding however struggle finegrained temporal grounding paper introduce groundedvideollm novel videollm adept perceiving reasoning specific video moment finegrained manner identify current videollms limitation finegrained video understanding since lack effective temporal modeling timestamp representation light sharpen model incorporating additional temporal stream encode relationship frame discrete temporal token enriched specific time knowledge represent timestamps optimize training groundedvideollm employ multistage training scheme beginning simple videocaptioning task progressively introducing video temporal grounding task increasing complexity enhance groundedvideollms temporal reasoning capability also curate grounded videoqa dataset automatic annotation pipeline extensive experiment demonstrate groundedvideollm excels finegrained grounding task temporal sentence grounding dense video captioning grounded videoqa also show great potential versatile video assistant general video understanding
dawn dynamic frame avatar nonautoregressive diffusion framework talking head video generation talking head generation intends produce vivid realistic talking head video single portrait speech audio clip although significant progress made diffusionbased talking head generation almost method rely autoregressive strategy suffer limited context utilization beyond current generation step error accumulation slower generation speed address challenge present dawn dynamic frame avatar nonautoregressive diffusion framework enables allatonce generation dynamiclength video sequence specifically consists two main component audiodriven holistic facial dynamic generation latent motion space audiodriven head pose blink generation extensive experiment demonstrate method generates authentic vivid video precise lip motion natural poseblink movement additionally high generation speed dawn possesses strong extrapolation capability ensuring stable production highquality long video result highlight considerable promise potential impact dawn field talking head video generation furthermore hope dawn spark exploration nonautoregressive approach diffusion model code publicly available httpsgithubcomhanbochengdawnpytorch
animatedifflightning crossmodel diffusion distillation present animatedifflightning lightningfast video generation model us progressive adversarial diffusion distillation achieve new stateoftheart fewstep video generation discus modification adapt video modality furthermore propose simultaneously distill probability flow multiple base diffusion model resulting single distilled motion module broader style compatibility pleased release distilled animatedifflightning model community use
deep video representation learning survey paper provides review representation learning video classify recent spatiotemporal feature learning method sequential visual data compare pro con general video analysis building effective feature video fundamental problem computer vision task involving video analysis understanding existing feature generally categorized spatial temporal feature effectiveness variation illumination occlusion view background discussed finally discus remaining challenge existing deep video representation learning study
video generation learned action prior stochastic video generation particularly challenging camera mounted moving platform camera motion interacts observed image pixel creating complex spatiotemporal dynamic making problem partially observable existing method typically address focusing raw pixellevel image reconstruction without explicitly modelling camera motion dynamic propose solution considering camera motion action part observed image state modelling image action within multimodal learning framework introduce three model video generation learning action prior vgleap treat imageaction pair augmented state generated single latent stochastic process us variational inference learn imageaction latent prior causalleap establishes causal relationship action observed image frame time learning action prior conditioned observed image state rafi integrates augmented imageaction state concept flow matching diffusion generative process demonstrating actionconditioned image generation concept extended diffusionbased model emphasize importance multimodal training partially observable video generation problem detailed empirical study new video action dataset roam
depth prediction autonomous driving using selfsupervised learning perception environment critical component enabling autonomous driving provides vehicle ability comprehend surroundings make informed decision depth prediction play pivotal role process help understanding geometry motion environment thesis focus challenge depth prediction using monocular selfsupervised learning technique problem approached broader perspective first exploring conditional generative adversarial network cgans potential technique achieve better generalization performed fundamental contribution conditional gans acontrario cgan proposed second contribution entail single imagetodepth selfsupervised method proposing solution rigidscene assumption using novel transformerbased method output pose dynamic object third significant aspect involves introduction videotodepth map forecasting approach method serf extension selfsupervised technique predict future depth involves creation novel transformer model capable predicting future depth given scene moreover various limitation aforementioned method addressed videotovideo depth map model proposed model leverage spatiotemporal consistency input output sequence predict accurate depth sequence output method significant application autonomous driving ad advanced driver assistance system ada
screenwriter automatic screenplay generation movie summarisation proliferation creative video content driven demand textual description summary allow user recall key plot point get overview without watching volume movie content speed turnover motivates automatic summarisation nevertheless challenging requiring identifying character intention longrange temporal dependency existing method attempting task rely heavily textual screenplay input greatly limiting applicability work propose task automatic screenplay generation method screenwriter operates video produce output includes dialogue speaker name scene break visual description screenwriter introduces novel algorithm segment video scene based sequence visual vector novel method challenging problem determining character name based database actor face demonstrate automatic screenplay used generate plot synopsis hierarchical summarisation method based scene break test quality final summary recent moviesum dataset augment video show superior number comparison model assume access goldstandard screenplay
knowledge nerf fewshot novel view synthesis dynamic articulated object present knowledge nerf synthesize novel view dynamic scene reconstructing dynamic scene sparse view rendering arbitrary perspective challenging problem application various domain previous dynamic nerf method learn deformation articulated object monocular video however quality reconstructed scene limited clearly reconstruct dynamic scene propose new framework considering two frame timewe pretrain nerf model articulated objectwhen articulated object move knowledge nerf learns generate novel view new state incorporating past knowledge pretrained nerf model minimal observation present state propose projection module adapt nerf dynamic scene learning correspondence pretrained knowledge base current state experimental result demonstrate effectiveness method reconstructing dynamic scene input image one state knowledge nerf new pipeline promising solution novel view synthesis dynamic articulated object data implementation publicly available
adaptive super resolution oneshot talkinghead generation oneshot talkinghead generation learns synthesize talkinghead video one source portrait image driving different identity video usually method require planebased pixel transformation via jacobin matrix facial image warp novel pose generation constraint using single image source pixel displacement often compromise clarity synthesized image method try improve quality synthesized video introducing additional superresolution module undoubtedly increase computational consumption destroy original data distribution work propose adaptive highquality talkinghead video generation method synthesizes highresolution video without additional pretrained module specifically inspired existing superresolution method downsample oneshot source image adaptively reconstruct highfrequency detail via encoderdecoder module resulting enhanced video clarity method consistently improves quality generated video straightforward yet effective strategy substantiated quantitative qualitative evaluation code demo video available urlhttpsgithubcomsongluchuanadasrtalkinghead
improving generative adversarial network video superresolution research explore different way improve generative adversarial network video superresolution task base single image superresolution gan model primary objective identify potential technique enhance model analyze technique yield significant improvement evaluate result using peak signaltonoise ratio psnr structural similarity index ssim finding indicate effective technique include temporal smoothing long shortterm memory lstm layer temporal loss function integration method result improvement psnr improvement ssim compared baseline video superresolution generative adversarial network gan model substantial improvement suggests potential application enhance current stateoftheart model
vistadream sampling multiview consistent image singleview scene reconstruction paper propose vistadream novel framework reconstruct scene singleview image recent diffusion model enable generating highquality novelview image singleview input image existing method concentrate building consistency input image generated image losing consistency generated image vistadream address problem twostage pipeline first stage vistadream begin building global coarse scaffold zooming little step inpainted boundary estimated depth map global scaffold use iterative diffusionbased rgbd inpainting generate novelview image inpaint hole scaffold second stage enhance consistency generated novelview image novel trainingfree multiview consistency sampling mc introduces multiview consistency constraint reverse sampling process diffusion model experimental result demonstrate without training finetuning existing diffusion model vistadream achieves consistent highquality novel view synthesis using singleview image outperforms baseline method large margin code video interactive demo available httpsvistadreamprojectpagegithubio
tuningfree noise rectification high fidelity imagetovideo generation imagetovideo generation task always suffer keeping high fidelity open domain traditional image animation technique primarily focus specific domain face human pose making difficult generalize open domain several recent framework based diffusion model generate dynamic content open domain image fail maintain fidelity found two main factor low fidelity loss image detail noise prediction bias denoising process end propose effective method applied mainstream video diffusion model method achieves high fidelity based supplementing precise image information noise rectification specifically given specified image method first add noise input image latent keep detail denoises noisy latent proper rectification alleviate noise prediction bias method tuningfree plugandplay experimental result demonstrate effectiveness approach improving fidelity generated video imagetovideo generated result please refer project website httpsnoiserectificationgithubio
videotetris towards compositional texttovideo generation diffusion model demonstrated great success texttovideo generation however existing method may face challenge handling complex long video generation scenario involve multiple object dynamic change object number address limitation propose videotetris novel framework enables compositional generation specifically propose spatiotemporal compositional diffusion precisely follow complex textual semantics manipulating composing attention map denoising network spatially temporally moreover propose enhanced video data preprocessing enhance training data regarding motion dynamic prompt understanding equipped new reference frame attention mechanism improve consistency autoregressive video generation extensive experiment demonstrate videotetris achieves impressive qualitative quantitative result compositional generation code available
boosting camera motion control video diffusion transformer recent advancement diffusion model significantly enhanced quality video generation however finegrained control camera pose remains challenge unetbased model shown promising result camera control transformerbased diffusion model ditthe preferred architecture largescale video generation suffer severe degradation camera motion accuracy paper investigate underlying cause issue propose solution tailored dit architecture study reveals camera control performance depends heavily choice conditioning method rather camera pose representation commonly believed address persistent motion degradation dit introduce camera motion guidance cmg based classifierfree guidance boost camera control additionally present sparse camera control pipeline significantly simplifying process specifying camera pose long video method universally applies unet dit model offering improved camera control video generation task
ctrladapter efficient versatile framework adapting diverse control diffusion model controlnets widely used adding spatial control texttoimage diffusion model different condition depth map scribblessketches human pose however come controllable video generation controlnets directly integrated new backbone due feature space mismatch training controlnets new backbone significant burden many user furthermore applying controlnets independently different frame effectively maintain object temporal consistency address challenge introduce ctrladapter efficient versatile framework add diverse control imagevideo diffusion model adaptation pretrained controlnets ctrladapter offer strong diverse capability including image video control sparseframe video control finegrained patchlevel multicondition control via moe router zeroshot adaptation unseen condition support variety downstream task beyond spatial control including video editing video style transfer textguided motion control six diverse unetditbased imagevideo diffusion model sdxl pixartalpha svd latte hotshotxl ctrladapter match performance pretrained controlnets coco achieves stateoftheart davis significantly lower computation gpu hour
evaluating safety texttovideo generative model recent development sora lead new era texttovideo generation along come rising concern security risk generated video may contain illegal unethical content lack comprehensive quantitative understanding safety posing challenge reliability practical deployment previous evaluation primarily focus quality video generation evaluation texttoimage model considered safety cover fewer aspect address unique temporal risk inherent video generation bridge research gap introduce new benchmark designed conducting safetycritical assessment texttovideo model define critical aspect video generation safety construct malicious prompt dataset including realworld prompt llmgenerated prompt jailbreak attackbased prompt based evaluation result draw several important finding including single model excels aspect different model showing various strength correlation assessment manual review generally high tradeoff usability safety texttovideo generative model indicates field video generation rapidly advance safety risk set surge highlighting urgency prioritizing video safety hope provide insight better understanding safety video generation era generative ai
showhowto generating sceneconditioned stepbystep visual instruction goal work generate stepbystep visual instruction form sequence image given input image provides scene context sequence textual instruction challenging problem requires generating multistep image sequence achieve complex goal grounded specific environment part challenge stem lack largescale training data problem contribution work thus threefold first introduce automatic approach collecting large stepbystep visual instruction training data instructional video apply approach one million video create largescale highquality dataset sequence imagetext pair second develop train showhowto video diffusion model capable generating stepbystep visual instruction consistent provided input image third evaluate generated image sequence across three dimension accuracy step scene task show model achieves stateoftheart result code dataset trained model publicly available
autoregressive video generation without vector quantization paper present novel approach enables autoregressive video generation high efficiency propose reformulate video generation problem nonquantized autoregressive modeling temporal framebyframe prediction spatial setbyset prediction unlike rasterscan prediction prior autoregressive model joint distribution modeling fixedlength token diffusion model approach maintains causal property gptstyle model flexible incontext capability leveraging bidirectional modeling within individual frame efficiency proposed approach train novel video autoregressive model without vector quantization termed nova result demonstrate nova surpasses prior autoregressive video model data efficiency inference speed visual fidelity video fluency even much smaller model capacity ie parameter nova also outperforms stateoftheart image diffusion model texttoimage generation task significantly lower training cost additionally nova generalizes well across extended video duration enables diverse zeroshot application one unified model code model publicly available httpsgithubcombaaivisionnova
generative human video compression multigranularity temporal trajectory factorization paper propose novel multigranularity temporal trajectory factorization framework generative human video compression hold great potential bandwidthconstrained humancentric video communication particular proposed motion factorization strategy facilitate implicitly characterize highdimensional visual signal compact motion vector representation compactness transform vector finegrained field motion expressibility coded bitstream entailed enough visual motion information lowest representation cost meanwhile resolutionexpandable generative module developed enhanced background stability proposed framework optimized towards higher reconstruction robustness flexible resolution adaptation experimental result show proposed method outperforms latest generative model stateoftheart video coding standard versatile video coding vvc talkingface video movingbody video term objective subjective quality project page found httpsgithubcomxyzyszextremehumanvideocompressionwithmttf
instancecap improving texttovideo generation via instanceaware structured caption texttovideo generation evolved rapidly recent year delivering remarkable result training typically relies videocaption paired data play crucial role enhancing generation performance however current video caption often suffer insufficient detail hallucination imprecise motion depiction affecting fidelity consistency generated video work propose novel instanceaware structured caption framework termed instancecap achieve instancelevel finegrained video caption first time based scheme design auxiliary model cluster convert original video instance enhance instance fidelity video instance used refine dense prompt structured phrase achieving concise yet precise description furthermore instancevid dataset curated training enhancement pipeline tailored instancecap structure proposed inference experimental result demonstrate proposed instancecap significantly outperform previous model ensuring high fidelity caption video reducing hallucination
polysmart trecvid medical video question answering video corpus visual answer localization vcval includes questionrelated video retrieval visual answer localization video specifically use texttotext retrieval find relevant video medical question based similarity video transcript answer generated visual answer localization start end timestamps answer predicted alignment visual content subtitle query queryfocused instructional step captioning qfisc task step caption generated specifically provide video caption generated llavanextvideo model video subtitle timestamps context ask generate step caption given medical query submit one run evaluation obtains fscore mean iou
occsora occupancy generation model world simulator autonomous driving understanding evolution scene important effective autonomous driving conventional method mode scene development motion individual instance world model emerge generative framework describe general scene dynamic however existing method adopt autoregressive framework perform nexttoken prediction suffer inefficiency modeling longterm temporal evolution address propose diffusionbased occupancy generation model occsora simulate development world autonomous driving employ scene tokenizer obtain compact discrete spatialtemporal representation occupancy input achieve highquality reconstruction longsequence occupancy video learn diffusion transformer spatialtemporal representation generate occupancy conditioned trajectory prompt conduct extensive experiment widely used nuscenes dataset occupancy annotation occsora generate authentic layout temporal consistency demonstrating ability understand spatial temporal distribution driving scene trajectoryaware generation occsora potential serve world simulator decisionmaking autonomous driving code available httpsgithubcomwzzhengoccsora
faker fullbody anonymization human keypoint extraction realtime video deidentification contemporary digital era protection personal information become paramount issue exponential growth medium industry heightened concern regarding anonymization individual captured video footage traditional method blurring pixelation commonly employed recent advancement introduced generative adversarial network gan redraw face video study propose novel approach employ significantly smaller model achieve realtime fullbody anonymization individual video unlike conventional technique often fail effectively remove personal identification information skin color clothing accessory body shape method successfully eradicates detail furthermore leveraging pose estimation algorithm approach accurately represents information regarding individual position movement posture algorithm seamlessly integrated cctv ip camera system installed various industrial setting functioning realtime thus facilitating widespread adoption fullbody anonymization technology
benchmarking multidimensional aigc video quality assessment dataset unified model recent year artificial intelligence aidriven video generation gained significant attention consequently growing need accurate video quality assessment vqa metric evaluate perceptual quality aigenerated content aigc video optimize video generation model however assessing quality aigc video remains significant challenge video often exhibit highly complex distortion unnatural action irrational object address challenge systematically investigate aigcvqa problem considering subjective objective quality assessment perspective subjective perspective construct largescale generated video quality assessment lgvq dataset consisting aigc video generated video generation model using carefully curated text prompt evaluate perceptual quality aigc video three critical dimension spatial quality temporal quality textvideo alignment objective perspective establish benchmark evaluating existing quality assessment metric lgvq dataset finding show current metric perform poorly dataset highlighting gap effective evaluation tool bridge gap propose unify generated video quality assessment ugvq model designed accurately evaluate multidimensional quality aigc video ugvq model integrates visual motion feature video textual feature corresponding prompt forming unified qualityaware feature representation tailored aigc video experimental result demonstrate ugvq achieves stateoftheart performance lgvq dataset across three quality dimension lgvq dataset ugvq model publicly available httpsgithubcomzczhangsjtuugvqgit
statespace decomposition model video prediction considering longterm motion trend stochastic video prediction enables consideration uncertainty future motion thereby providing better reflection dynamic nature environment stochastic video prediction method based image autoregressive recurrent model need feed prediction back latent space conversely statespace model decouple frame synthesis temporal prediction prof efficient however inferring longterm temporal information motion generalizing dynamic scenario nonstationary assumption remains unresolved challenge paper propose statespace decomposition stochastic video prediction model decomposes overall video frame generation deterministic appearance prediction stochastic motion prediction adaptive decomposition model generalization capability dynamic scenario enhanced context motion prediction obtaining prior longterm trend future motion crucial thus stochastic motion prediction branch infer longterm motion trend conditional frame guide generation future frame exhibit high consistency conditional frame experimental result demonstrate model outperforms baseline multiple datasets
avdit efficient audiovisual diffusion transformer joint audio video generation recent diffusion transformer dit shown impressive capability generating highquality singlemodality content including image video audio however still underexplored whether transformerbased diffuser efficiently denoise gaussian noise towards superb multimodal content creation bridge gap introduce avdit novel efficient audiovisual diffusion transformer designed generate highquality realistic video visual audio track minimize model complexity computational cost avdit utilizes shared dit backbone pretrained imageonly data lightweight newly inserted adapter trainable shared backbone facilitates audio video generation specifically video branch incorporates trainable temporal attention layer frozen pretrained dit block temporal consistency additionally small number trainable parameter adapt imagebased dit block audio generation extra shared dit block equipped lightweight parameter facilitates feature interaction audio visual modality ensuring alignment extensive experiment aist landscape datasets demonstrate avdit achieves stateoftheart performance joint audiovisual generation significantly fewer tunable parameter furthermore result highlight single shared image generative backbone modalityspecific adaptation sufficient constructing joint audiovideo generator source code pretrained model released
spagent adaptive task decomposition model selection general video generation editing opensource video generation editing model made significant progress individual model typically limited specific task failing meet diverse need user effectively coordinating model unlock wide range video generation editing capability however manual coordination complex timeconsuming requiring user deeply understand task requirement possess comprehensive knowledge model performance applicability limitation thereby increasing barrier entry address challenge propose novel video generation editing system powered semantic planning agent spagent spagent bridge gap diverse user intent effective utilization existing generative model enhancing adaptability efficiency overall quality video generation editing specifically spagent assembles tool library integrating stateoftheart opensource image video generation editing model tool finetuning manually annotated dataset spagent automatically coordinate tool video generation editing novelly designed threestep framework decoupled intent recognition principleguided route planning capabilitybased execution model selection additionally enhance spagents video quality evaluation capability enabling autonomously assess incorporate new video generation editing model tool library without human intervention experimental result demonstrate spagent effectively coordinate model generate edit video highlighting versatility adaptability across various video task
coherent video inpainting using optical flowguided efficient diffusion textguided video inpainting technique significantly improved performance content generation application recent family improvement us diffusion model become essential achieving highquality video inpainting result yet still face performance bottleneck temporal consistency computational efficiency motivates u propose new video inpainting framework using optical flowguided efficient diffusion floed higher video coherence specifically floed employ dualbranch architecture timeagnostic flow branch restores corrupted flow first multiscale flow adapter provide motion guidance main inpainting branch besides trainingfree latent interpolation method proposed accelerate multistep denoising process using flow warping flow attention cache mechanism floed efficiently reduces computational cost incorporating optical flow extensive experiment background restoration object removal task show floed outperforms stateoftheart diffusionbased method quality efficiency code model made publicly available
dynamic scene understanding objectcentric voxelization neural rendering learning objectcentric representation unsupervised video challenging unlike previous approach focus decomposing image present generative model named dynavols dynamic scene enables objectcentric learning within differentiable volume rendering framework key idea perform objectcentric voxelization capture nature scene infers perobject occupancy probability individual spatial location voxel feature evolve canonicalspace deformation function optimized inverse rendering pipeline compositional nerf additionally approach integrates semantic feature create semantic grid representing scene multiple disentangled voxel grid dynavols significantly outperforms existing model novel view synthesis unsupervised decomposition task dynamic scene jointly considering geometric structure semantic feature effectively address challenging realworld scenario involving complex object interaction furthermore trained explicitly meaningful voxel feature enable additional capability scene decomposition method achieve novel scene generation editing geometric shape manipulating motion trajectory object
efficient continuous video flow model video prediction multistep prediction model diffusion rectified flow model emerged stateoftheart solution generation task however model exhibit higher latency sampling new frame compared singlestep method latency issue becomes significant bottleneck adapting method video prediction task given typical video comprises approximately frame paper propose novel approach modeling multistep process aimed alleviating latency constraint facilitating adaptation process video prediction task approach reduces number sample step required predict next frame also minimizes computational demand reducing model size onethird original size evaluate method standard video prediction datasets including kth bair action robot demonstrating efficacy achieving stateoftheart performance benchmark
make video radicalizing identifying source influence qanon video recent year radicalization increasingly attempted videosharing platform previous study proposed identify online radicalization using generic social context analysis without taking account comprehensive viewer trait affect viewer perception radicalizing content address challenge examine qanon conspiracybased radicalizing group designed comprehensive questionnaire aiming understand viewer perception qanon video outline trait viewer qanon video appealing identify influential factor impact viewer perception video
poseguided finegrained sign language video generation sign language video important medium spreading learning sign language however existing human image synthesis method produce sign language image detail distorted blurred structurally incorrect also produce sign language video frame poor temporal consistency anomaly flickering abrupt detail change previous next frame address limitation propose novel poseguided motion model pgmm generating finegrained motionconsistent sign language video firstly propose new coarse motion module cmm completes deformation feature optical flow warping thus transfering motion coarsegrained structure without changing appearance secondly propose new pose fusion module pfm guide modal fusion rgb pose feature thus completing finegrained generation finally design new metric temporal consistency difference tcd quantitatively assess degree temporal consistency video comparing difference frame reconstructed video previous next frame target video extensive qualitative quantitative experiment show method outperforms stateoftheart method benchmark test visible improvement detail temporal consistency
video coding meet multimodal large language model unified paradigm video coding existing codecs designed eliminate intrinsic redundancy create compact representation compression however strong external prior multimodal large language model mllms explicitly explored video compression herein introduce unified paradigm crossmodality video coding cmvc pioneering approach explore multimodality representation video generative model video coding specifically encoder side disentangle video spatial content motion component subsequently transformed distinct modality achieve compact representation leveraging mllms decoding previously encoded component video generation model leveraged create multiple encodingdecoding mode optimize video reconstruction quality specific decoding requirement including texttexttovideo mode ensure highquality semantic information imagetexttovideo mode achieve superb perceptual consistency addition propose efficient frame interpolation model mode via lowrank adaption lora tuning guarantee perceptual quality allows generated motion cue behave smoothly experiment benchmark indicate achieves effective semantic reconstruction exhibit competitive perceptual consistency result highlight potential direction future research video coding
largescale video dataset improving consistency finegrained condition video content visual generation technology continue advance scale video datasets expanded rapidly quality datasets critical performance video generation model argue temporal splitting detailed caption video quality filtering three key factor determine dataset quality however existing datasets exhibit various limitation area address challenge introduce largescale highquality video dataset featuring accurate temporal splitting detailed caption superior video quality core approach lie improving consistency finegrained condition video content specifically employ linear classifier probability distribution enhance accuracy transition detection ensuring better temporal consistency provide structured caption splitted video average length word improve textvideo alignment additionally develop video training suitability score vtss integrates multiple submetrics allowing u filter highquality video original corpus finally incorporate several metric training process generation model refining finegrained condition experiment demonstrate effectiveness data processing pipeline quality proposed dataset dataset code released
seeing beyond view multiview driving scene video generation holistic attention generating multiview video autonomous driving training recently gained much attention challenge addressing crossview crossframe consistency existing method typically apply decoupled attention mechanism spatial temporal view dimension however approach often struggle maintain consistency across dimension particularly handling fastmoving object appear different time viewpoint paper present cogdriving novel network designed synthesizing highquality multiview driving video cogdriving leverage diffusion transformer architecture attention module enabling simultaneous association across spatial temporal viewpoint dimension also propose lightweight controller tailored cogdriving ie microcontroller us parameter standard controlnet enabling precise control birdseyeview layout enhance generation object instance crucial autonomous driving propose reweighted learning objective dynamically adjusting learning weight object instance training cogdriving demonstrates strong performance nuscenes validation set achieving fvd score highlighting ability generate realistic driving video project found httpsluhannangithubiocogdrivingpage
ditto motionspace diffusion controllable realtime talking head synthesis recent advance diffusion model revolutionized audiodriven talking head synthesis beyond precise lip synchronization diffusionbased method excel generating subtle expression natural head movement wellaligned audio signal however method confronted slow inference speed insufficient finegrained control facial motion occasional visual artifact largely due implicit latent space derived variational autoencoders vae prevent adoption realtime interaction application address issue introduce ditto diffusionbased framework enables controllable realtime talking head synthesis key innovation lie bridging motion generation photorealistic neural rendering explicit identityagnostic motion space replacing conventional vae representation design substantially reduces complexity diffusion learning enabling precise control synthesized talking head propose inference strategy jointly optimizes three key component audio feature extraction motion generation video synthesis optimization enables streaming processing realtime inference low firstframe delay functionality crucial interactive application ai assistant extensive experimental result demonstrate ditto generates compelling talking head video substantially outperforms existing method motion control realtime performance
direct preference optimization video large multimodal model language model reward preference modeling technique direct preference optimization dpo shown effective enhancing generalization ability large language model llm however task involving video instructionfollowing providing informative feedback especially detecting hallucination generated response remains significant challenge previous study explored using large large multimodal model lmms reward model guide preference modeling ability accurately assess factuality generated response compared corresponding video conclusively established paper introduces novel framework utilizes detailed video caption proxy video content enabling language model incorporate information supporting evidence scoring video question answering qa prediction approach demonstrates robust alignment openai model reward mechanism directly take video frame input furthermore show applying tailored reward dpo significantly improves performance video lmms video qa task
svastin sparse video adversarial attack via spatiotemporal invertible neural network robust imperceptible adversarial video attack challenging due spatial temporal characteristic video existing video adversarial attack method mainly take gradientbased approach generate adversarial video noticeable perturbation paper propose novel sparse adversarial video attack via spatiotemporal invertible neural network svastin generate adversarial video spatiotemporal feature space information exchanging consists guided target video learning gtvl module balance perturbation budget optimization speed spatiotemporal invertible neural network stin module perform spatiotemporal feature space information exchanging source video target feature tensor learned gtvl module extensive experiment demonstrate proposed svastin generate adversarial example higher imperceptibility stateoftheart method higher fooling rate code available hrefhttpsgithubcombrittanychensvastinhttpsgithubcombrittanychensvastin
interdyn controllable interactive dynamic video diffusion model predicting dynamic interacting object essential human intelligent system however existing approach limited simplified toy setting lack generalizability complex realworld environment recent advance generative model enabled prediction state transition based intervention focus generating single future state neglect continuous dynamic resulting interaction address gap propose interdyn novel framework generates video interactive dynamic given initial frame control signal encoding motion driving object actor key insight large video generation model act neural renderers implicit physic simulator learned interactive dynamic largescale video data effectively harness capability introduce interactive control mechanism condition video generation process motion driving entity qualitative result demonstrate interdyn generates plausible temporally consistent video complex object interaction generalizing unseen object quantitative evaluation show interdyn outperforms baseline focus static state transition work highlight potential leveraging video generative model implicit physic engine code trained model released httpsinterdynistuempgde
enhancing gans contrastive learningbased multistage progressive finetuning snn rlbased external optimization generative adversarial network gans forefront image synthesis especially medical field like histopathology help address challenge data scarcity patient privacy class imbalance however several inherent domainspecific issue remain gans training instability mode collapse insufficient feedback binary classification undermine performance challenge particularly pronounced highresolution histopathology image due complex feature representation high spatial detail response challenge work proposes novel framework integrating contrastive learningbased multistage progressive finetuning siamese neural network mftsnn reinforcement learningbased external optimizer rleo mftsnn improves feature similarity extraction histopathology data rleo act rewardbased guide balance gan training addressing mode collapse enhancing output quality proposed approach evaluated stateoftheart sota gan model demonstrates superior performance across multiple metric
revideo remake video motion content control despite significant advancement video generation editing using diffusion model achieving accurate localized video editing remains substantial challenge additionally existing video editing method primarily focus altering visual content limited research dedicated motion editing paper present novel attempt remake video revideo stand existing method allowing precise video editing specific area specification content motion content editing facilitated modifying first frame trajectorybased motion control offer intuitive user interaction experience revideo address new task involving coupling training imbalance content motion control tackle develop threestage training strategy progressively decouples two aspect coarse fine furthermore propose spatiotemporal adaptive fusion module integrate content motion control across various sampling step spatial location extensive experiment demonstrate revideo promising performance several accurate video editing application ie locally changing video content keeping motion constant keeping content unchanged customizing new motion trajectory modifying content motion trajectory method also seamlessly extend application multiarea editing without specific training demonstrating flexibility robustness
videollm know speak enhancing timesensitive video comprehension videotext duet interaction format recent research video large language model videollm predominantly focus model architecture training datasets leaving interaction format user model underexplored existing work user often interact videollms using entire video query input model generates response interaction format constrains application videollms scenario livestreaming comprehension video end response required realtime manner also result unsatisfactory performance timesensitive task requires localizing video segment paper focus videotext duet interaction format interaction format characterized continuous playback video user model insert text message position video playback text message end video continues play akin alternative two performer duet construct mmduetit videotext training dataset designed adapt videollms videotext duet interaction format also introduce multianswer grounded video question answering magqa task benchmark realtime response ability videollms trained mmduetit mmduet demonstrates adopting videotext duet interaction format enables model achieve significant improvement various timesensitive task cider dense video captioning map qvhighlights highlight detection charadessta temporal video grounding minimal training effort also enable videollms reply realtime manner video play code data demo available httpsgithubcomyellowbinarytreemmduet
taming multimodal joint training highquality videotoaudio synthesis propose synthesize highquality synchronized audio given video optional text condition using novel multimodal joint training framework mmaudio contrast singlemodality training conditioned limited video data mmaudio jointly trained largerscale readily available textaudio data learn generate semantically aligned highquality audio sample additionally improve audiovisual synchrony conditional synchronization module aligns video condition audio latents frame level trained flow matching objective mmaudio achieves new videotoaudio stateoftheart among public model term audio quality semantic alignment audiovisual synchronization low inference time generate clip parameter mmaudio also achieves surprisingly competitive performance texttoaudio generation showing joint training hinder singlemodality performance code demo available httpshkchengrexgithubiommaudio
human video generation novel scenario enables generalizable robot manipulation robot manipulation policy generalize novel task involving unseen object type new motion paper provide solution term predicting motion information web data human video generation conditioning robot policy generated video instead attempting scale robot data collection expensive show leverage video generation model trained easily available web data enabling generalization approach cast languageconditioned manipulation zeroshot human video generation followed execution single policy conditioned generated video train policy use order magnitude less robot interaction data compared video prediction model trained doesnt require finetuning video model directly use pretrained model generating human video result diverse realworld scenario show enables manipulating unseen object type performing novel motion task present robot data video
llmguided compositional scene generation recent advancement diffusion model content creation sparked surge interest generating content however scarcity scene datasets constrains current methodology primarily objectcentric generation overcome limitation present novel framework compositional generation unlike conventional method generate singular representation entire scene innovatively construct object within scene separately utilizing large language model llm framework begin decomposing input text prompt distinct entity map trajectory construct compositional scene accurately positioning object along designated path refine scene method employ compositional score distillation technique guided predefined trajectory utilizing pretrained diffusion model across texttoimage texttovideo domain extensive experiment demonstrate outstanding content creation capability compared prior art showcasing superior visual quality motion fidelity enhanced object interaction
fundus fluorescein angiography video generation retinal generative foundation model fundus fluorescein angiography ffa crucial diagnosing monitoring retinal vascular issue limited invasive nature restricted accessibility compared color fundus cf imaging existing method convert cf image ffa confined static image generation missing dynamic lesional change introduce autoregressive generative adversarial network gan model generates dynamic ffa video single cf image excels video generation achieving fvd psnr clinical expert validated fidelity generated video additionally model generator demonstrates remarkable downstream transferability across ten external public datasets including blood vessel segmentation retinal disease diagnosis systemic disease prediction multimodal retrieval showcasing impressive zeroshot fewshot capability finding position powerful noninvasive alternative ffa exam versatile retinal generative foundation model capture static temporal retinal feature enabling representation complex intermodality relationship
segment anything video systematic survey recent wave foundation model witnessed tremendous success computer vision cv beyond segment anything model sam sparked passion exploring taskagnostic visual foundation model empowered remarkable zeroshot generalization sam currently challenging numerous traditional paradigm cv delivering extraordinary performance various image segmentation multimodal segmentation eg texttomask task also video domain additionally latest released sam sparking research enthusiasm realm promptable visual segmentation image video however existing survey mainly focus sam various image processing task comprehensive indepth review video domain notably absent address gap work conduct systematic review sam video era foundation model first review progress sam video work focus application various task discussing recent advance innovation opportunity developing foundation model broad application begin brief introduction background sam videorelated research domain subsequently present systematic taxonomy categorizes existing method three key area video understanding video generation video editing analyzing summarizing advantage limitation furthermore comparative result sambased current stateoftheart method representative benchmark well insightful analysis offered finally discus challenge faced current research envision several future research direction field sam video beyond
ifmdm implicit face motion diffusion model highfidelity realtime talking head generation introduce novel approach highresolution talking head generation single image audio input prior method using explicit face model like morphable model facial landmark often fall short generating highfidelity video due lack appearanceaware motion representation generative approach video diffusion model achieve high video quality slow processing speed limit practical application proposed model implicit face motion diffusion model ifmdm employ implicit motion encode human face appearanceaware compressed facial latents enhancing video generation although implicit motion lack spatial disentanglement explicit model complicates alignment subtle lip movement introduce motion statistic help capture finegrained motion information additionally model provides motion controllability optimize tradeoff motion intensity visual quality inference ifmdm support realtime generation resolution video frame per second fps extensive evaluation demonstrate superior performance existing diffusion explicit face model code released publicly available alongside supplementary material video result found
worldgpt sorainspired video ai agent rich world model text image input several texttovideo diffusion model demonstrated commendable capability synthesizing highquality video content however remains formidable challenge pertaining maintaining temporal consistency ensuring action smoothness throughout generated sequence paper present innovative video generation ai agent harness power sorainspired multimodal learning build skilled world model framework based textual prompt accompanying image framework includes two part prompt enhancer full video translation first part employ capability chatgpt meticulously distill proactively construct precise prompt subsequent step thereby guaranteeing utmost accuracy prompt communication accurate execution following model operation second part employ compatible existing advanced diffusion technique expansively generate refine key frame conclusion video expertly harness power leading trailing key frame craft video enhanced temporal consistency action smoothness experimental result confirm method strong effectiveness novelty constructing world model text image input method
mimir improving video diffusion model precise text understanding text serf key control signal video generation due narrative nature render text description video clip current video diffusion model borrow feature text encoders yet struggle limited text comprehension recent success large language model llm showcase power decoderonly transformer offer three clear benefit texttovideo generation namely precise text understanding resulting superior scalability imagination beyond input text enabled next token prediction flexibility prioritize user interest instruction tuning nevertheless feature distribution gap emerging two different text modeling paradigm hinders direct use llm established model work address challenge mimir endtoend training framework featuring carefully tailored token fuser harmonize output text encoders llm design allows model fully leverage learned video prior capitalizing textrelated capability llm extensive quantitative qualitative result demonstrate effectiveness mimir generating highquality video excellent text comprehension especially processing short caption managing shifting motion project page httpslucariaacademygithubiomimir
rain realtime animation infinite video stream live animation gained immense popularity enhancing online engagement yet achieving highquality realtime stable animation diffusion model remains challenging especially consumergrade gpus existing method struggle generating long consistent video stream efficiently often limited latency issue degraded visual quality extended period paper introduce rain pipeline solution capable animating infinite video stream realtime low latency using single rtx gpu core idea rain efficiently compute frametoken attention across different noise level long timeintervals simultaneously denoising significantly larger number frametokens previous streambased method design allows rain generate video frame much shorter latency faster speed maintaining longrange attention extended video stream resulting enhanced continuity consistency consequently stable diffusion model finetuned rain epoch produce video stream realtime low latency without much compromise quality consistency infinite long despite advanced capability rain introduces additional attention block imposing minimal additional burden experiment benchmark datasets generating superlong video demonstrating rain animate character realtime much better quality accuracy consistency competitor costing less latency code model made publicly available
romo robust motion segmentation improves structure motion extensive progress reconstruction generation scene monocular casuallycaptured video task rely heavily known camera pose problem finding pose using structurefrommotion sfm often depends robustly separating static dynamic part video lack robust solution problem limit performance sfm cameracalibration pipeline propose novel approach videobased motion segmentation identify component scene moving wrt fixed world frame simple effective iterative method romo combine optical flow epipolar cue pretrained video segmentation model outperforms unsupervised baseline motion segmentation well supervised baseline trained synthetic data importantly combination offtheshelf sfm pipeline segmentation mask establishes new stateoftheart camera calibration scene dynamic content outperforming existing method substantial margin
languageguided selfsupervised video summarization using text semantic matching considering diversity video current video summarization method rely heavily supervised computer vision technique demand timeconsuming subjective manual annotation overcome limitation investigated selfsupervised video summarization inspired success large language model llm explored feasibility transforming video summarization task natural language processing nlp task leveraging advantage llm context understanding aim enhance effectiveness selfsupervised video summarization method begin generating caption individual video frame synthesized text summary llm subsequently measure semantic distance caption text summary notably propose novel loss function optimize model according diversity video finally summarized video generated selecting frame caption similar text summary method achieves stateoftheart performance summe dataset rank correlation coefficient addition method novel feature able achieve personalized summarization
largescale dataset texttovideo generation quality videotext pair fundamentally determines upper bound texttovideo model currently datasets used training model suffer significant shortcoming including low temporal consistency poorquality caption substandard video quality imbalanced data distribution prevailing video curation process depends image model tagging manual rulebased curation lead high computational load leaf behind unclean data result lack appropriate training datasets texttovideo model address problem present superior training dataset texttovideo model produced coarsetofine curation strategy dataset guarantee highquality video detailed caption excellent temporal consistency used train video generation model dataset led experimental result surpass obtained model
sdipaste synthetic dynamic instance copypaste video instance segmentation data augmentation method copypaste studied effective way expand training datasets incurring minimal cost method extensively implemented image level task found scalable implementation copypaste built specifically video task paper leverage recent growth video fidelity generative model explore effective way incorporating synthetically generated object existing video datasets artificially expand object instance pool first procure synthetic video sequence featuring object morph dynamically time carefully devised pipeline automatically segment copypastes dynamic instance across frame target background video sequence name video data augmentation pipeline synthetic dynamic instance copypaste test complex task video instance segmentation combine detection segmentation tracking object instance across video sequence extensive experiment popular youtubevis dataset using two separate popular network baseline achieve strong gain ap ap make code model publicly available
video repurposing user generated content largescale dataset benchmark demand producing shortform video sharing social medium platform experienced significant growth recent time despite notable advancement field video summarization highlight detection create partially usable short film raw video approach often domainspecific require indepth understanding realworld video content tackle predicament propose extensive dataset comprising video annotated clip aimed resolving video longtoshort task recognizing inherent constraint posed untrained human annotator result inaccurate annotation repurposed video propose twostage solution obtain annotation realworld usergenerated content furthermore offer baseline model address challenging task integrating audio visual caption aspect crossmodal fusion alignment framework aspire work ignite groundbreaking research lesserexplored realm video repurposing
disentangled motion modeling video frame interpolation video frame interpolation vfi aim synthesize intermediate frame existing frame enhance visual smoothness quality beyond conventional method based reconstruction loss recent work employed generative model improved perceptual quality however require complex training large computational cost pixel space modeling paper introduce disentangled motion modeling momo diffusionbased approach vfi enhances visual quality focusing intermediate motion modeling propose disentangled twostage training process initial stage frame synthesis flow model trained generate accurate frame flow optimal synthesis subsequent stage introduce motion diffusion model incorporates novel unet architecture specifically designed optical flow generate bidirectional flow frame learning simpler lowfrequency representation motion momo achieves superior perceptual quality reduced computational demand compared generative modeling method pixel space momo surpasses stateoftheart method perceptual metric across various benchmark demonstrating efficacy efficiency vfi
general method incorporate spatial information loss function ganbased superresolution model generative adversarial network gans shown great performance superresolution problem since generate visually realistic image video frame however model often introduce side effect output unexpected artifact noise reduce artifact enhance perceptual quality result paper propose general method effectively used ganbased superresolution sr model introducing essential spatial information training process extract spatial information input data incorporate training loss making corresponding loss spatially adaptive sa one utilize guide training process show proposed approach independent method used extract spatial information independent sr task model method consistently guide training process towards generating visually pleasing sr image video frame substantially mitigating artifact noise ultimately leading enhanced perceptual quality
fade dataset detecting falling object around building video falling object building cause severe injury pedestrian due great impact force exert although surveillance camera installed around building challenging human capture event surveillance video due small size fast motion falling object well complex background therefore necessary develop method automatically detect falling object around building surveillance video facilitate investigation falling object detection propose large diverse video dataset called fade falling object detection around building first time fade contains video scene featuring falling object category weather condition video resolution additionally develop new object detection method called fadenet effectively leverage motion information produce smallsized highquality proposal detecting falling object around building importantly method extensively evaluated analyzed comparing previous approach used generic object detection video object detection moving object detection fade dataset experimental result show proposed fadenet significantly outperforms method providing effective baseline future research dataset code publicly available httpsfadedatasetgithubiofadegithubio
retomeva recursive token merging video diffusionbased unrestricted adversarial attack recent diffusionbased unrestricted attack generate imperceptible adversarial example high transferability compared previous unrestricted attack restricted attack however existing work diffusionbased unrestricted attack mostly focused image yet seldom explored video paper propose recursive token merging video diffusionbased unrestricted adversarial attack retomeva first framework generate imperceptible adversarial video clip higher transferability specifically achieve spatial imperceptibility retomeva adopts timestepwise adversarial latent optimization talo strategy optimizes perturbation diffusion model latent space denoising step talo offer iterative accurate update generate powerful adversarial frame talo reduce memory consumption gradient computation moreover achieve temporal imperceptibility retomeva introduces recursive token merging retome mechanism matching merging token across video frame selfattention module resulting temporally consistent adversarial video retome concurrently facilitates interframe interaction attack process inducing diverse robust gradient thus leading better adversarial transferability extensive experiment demonstrate efficacy retomeva particularly surpassing stateoftheart attack adversarial transferability average
unimlvg unified framework multiview long video generation comprehensive control capability autonomous driving creation diverse realistic driving scenario become essential enhance perception planning capability autonomous driving system however generating longduration surroundview consistent driving video remains significant challenge address present unimlvg unified framework designed generate extended street multiperspective video precise control integrating single multiview driving video training data approach update ditbased diffusion model equipped crossframe crossview module across three stage multi training objective substantially boosting diversity quality generated visual content importantly propose innovative explicit viewpoint modeling approach multiview video generation effectively improve motion transition consistency capable handling various input reference format eg text image video unimlvg generates highquality multiview video according corresponding condition constraint bounding box framelevel text description compared best model similar capability framework achieves improvement fid fvd
fifodiffusion generating infinite video text without training propose novel inference technique based pretrained diffusion model textconditional video generation approach called fifodiffusion conceptually capable generating infinitely long video without additional training achieved iteratively performing diagonal denoising simultaneously process series consecutive frame increasing noise level queue method dequeues fully denoised frame head enqueuing new random noise frame tail however diagonal denoising doubleedged sword frame near tail take advantage cleaner frame forward reference strategy induces discrepancy training inference hence introduce latent partitioning reduce traininginference gap lookahead denoising leverage benefit forward referencing practically fifodiffusion consumes constant amount memory regardless target video length given baseline model wellsuited parallel inference multiple gpus demonstrated promising result effectiveness proposed method existing texttovideo generation baseline generated video example source code available project page
comprehensive survey synthetic infrared image synthesis synthetic infrared ir scene target generation important computer vision problem allows generation realistic ir image target training testing various application remote sensing surveillance target recognition also help reduce cost risk associated collecting realworld ir data survey paper aim provide comprehensive overview conventional mathematical modellingbased method deep learningbased method used generating synthetic ir scene target paper discusses importance synthetic ir scene target generation briefly cover mathematics blackbody grey body radiation well ir imagecapturing method potential use case synthetic ir scene target generation also described highlighting significance technique various field additionally paper explores possible new way developing new technique enhance efficiency effectiveness synthetic ir scene target generation highlighting need research advance field
controlnext powerful efficient control image video generation diffusion model demonstrated remarkable robust ability image video generation achieve greater control generated result researcher introduce additional architecture controlnet adapter referencenet integrate conditioning control however current controllable generation method often require substantial additional computational resource especially video generation face challenge training exhibit weak control paper propose controlnext powerful efficient method controllable image video generation first design straightforward efficient architecture replacing heavy additional branch minimal additional cost compared base model concise structure also allows method seamlessly integrate lora weight enabling style alteration without need additional training training reduce learnable parameter compared alternative furthermore propose another method called cross normalization cn replacement zeroconvolution achieve fast stable training convergence conducted various experiment different base model across image video demonstrating robustness method
cyclo cyclic graph transformer approach multiobject relationship modeling aerial video video scene graph generation vidsgg emerged transformative approach capturing interpreting intricate relationship among object temporal dynamic video sequence paper introduce new aeroeye dataset focus multiobject relationship modeling aerial video aeroeye dataset feature various drone scene includes visually comprehensive precise collection predicate capture intricate relationship spatial arrangement among object end propose novel cyclic graph transformer cyclo approach allows model capture direct longrange temporal dependency continuously updating history interaction circular manner proposed approach also allows one handle sequence inherent cyclical pattern process object relationship correct sequential order therefore effectively capture periodic overlapping relationship minimizing information loss extensive experiment aeroeye dataset demonstrate effectiveness proposed cyclo model demonstrating potential perform scene understanding drone video finally cyclo method consistently achieves stateoftheart sota result two inthewild scene graph generation benchmark ie pvsg aspire
meshbrush painting anatomical mesh neural stylization endoscopy style transfer promising approach close simtoreal gap medical endoscopy rendering synthetic endoscopic video traversing preoperative scan mri ct generate structurally accurate simulation well ground truth camera pose depth map although imagetoimage translation model cyclegan imitate realistic endoscopic image simulation unsuitable videotovideo synthesis due lack temporal consistency resulting artifact frame propose meshbrush neural mesh stylization method synthesize temporally consistent video differentiable rendering meshbrush us underlying geometry patient imaging data leveraging existing method learned pervertex texture stylized mesh guarantee consistency producing highfidelity output demonstrate mesh stylization promising approach creating realistic simulation downstream task training network preoperative planning although method tested designed ureteroscopy component transferable general endoscopic laparoscopic procedure code made public github
beyond deepfake image detecting aigenerated video recent advance generative ai led development technique generate visually realistic synthetic video number technique developed detect aigenerated synthetic image paper show synthetic image detector unable detect synthetic video demonstrate synthetic video generator introduce substantially different trace left image generator despite show synthetic video trace learned used perform reliable synthetic video detection generator source attribution even recompression furthermore demonstrate detecting video new generator zeroshot transferability challenging accurate detection video new generator achieved fewshot learning
generative ai replace immunofluorescent staining process comparison study synthetically generated cellpainting image brightfield cell imaging assay utilizing fluorescence stain essential observing subcellular organelle response perturbation immunofluorescent staining process routinely lab however recent innovation generative ai challenging idea staining required especially true availability cost specific fluorescence dye problem lab furthermore staining process take time lead interintra technician hinders downstream image data analysis reusability image data project recent study showed use generated synthetic immunofluorescence image brightfield bf image using generative ai algorithm literature therefore study benchmark compare five model three type generation backbone cnn gan diffusion model using publicly available dataset paper serf comparative study determine bestperforming model also proposes comprehensive analysis pipeline evaluating efficacy generator image synthesis highlighted potential deep learningbased generator image synthesis also discussed potential issue future research direction although generative ai show promise simplifying cell phenotyping using bf image staining research validation needed address key challenge model generalisability batch effect feature relevance computational cost
infinicube unbounded controllable dynamic driving scene generation worldguided video model present infinicube scalable method generating unbounded dynamic driving scene high fidelity controllability previous method scene generation either suffer limited scale lack geometric appearance consistency along generated sequence contrast leverage recent advancement scalable representation video model achieve large dynamic scene generation allows flexible control hd map vehicle bounding box text description first construct mapconditioned sparsevoxelbased generative model unleash power unbounded voxel world generation repurpose video model ground voxel world set carefully designed pixelaligned guidance buffer synthesizing consistent appearance finally propose fast feedforward approach employ voxel pixel branch lift dynamic video dynamic gaussians controllable object method generate controllable realistic driving scene extensive experiment validate effectiveness superiority model
make cheap scaling selfcascade diffusion model higherresolution adaptation diffusion model proven highly effective image video generation however encounter challenge correct composition object generating image varying size due singlescale training data adapting large pretrained diffusion model higher resolution demand substantial computational optimization resource yet achieving generation capability comparable lowresolution model remains challenging paper proposes novel selfcascade diffusion model leverage knowledge gained welltrained lowresolution imagevideo generation model enabling rapid adaptation higherresolution generation building employ pivot replacement strategy facilitate tuningfree version progressively leveraging reliable semantic guidance derived lowresolution model propose integrate sequence learnable multiscale upsampler module tuning version capable efficiently learning structural detail new scale small amount newly acquired highresolution training data compared full finetuning approach achieves training speedup requires tuning parameter extensive experiment demonstrate approach quickly adapt higherresolution image video synthesis finetuning step virtually additional inference time
vlogger multimodal diffusion embodied avatar synthesis propose vlogger method audiodriven human video generation single input image person build success recent generative diffusion model method consists stochastic diffusion model novel diffusionbased architecture augments texttoimage model spatial temporal control support generation high quality video variable length easily controllable highlevel representation human face body contrast previous work method require training person rely face detection cropping generates complete image face lip considers broad spectrum scenario eg visible torso diverse subject identity critical correctly synthesize human communicate also curate mentor new diverse dataset pose expression annotation one order magnitude larger previous one identity dynamic gesture train ablate main technical contribution vlogger outperforms stateoftheart method three public benchmark considering image quality identity preservation temporal consistency also generating upperbody gesture analyze performance vlogger respect multiple diversity metric showing architectural choice use mentor benefit training fair unbiased model scale finally show application video editing personalization
gaussian splatting modeling dynamic scene native primitive dynamic scene representation novel view synthesis captured video crucial enabling immersive experience required arvr metaverse application however task challenging due complexity unconstrained realworld scene temporal dynamic paper frame dynamic scene spatiotemporal volume learning problem offering native explicit reformulation minimal assumption motion serf versatile dynamic scene learning framework specifically represent target dynamic scene using collection gaussian primitive explicit geometry appearance feature dubbed gaussian splatting approach capture relevant information space time fitting underlying spatiotemporal volume modeling spacetime whole gaussians parameterized anisotropic ellipsis rotate arbitrarily space time model naturally learn viewdependent timeevolved appearance spherindrical harmonic notably model first solution support realtime rendering highresolution photorealistic novel view complex dynamic scene enhance efficiency derive several compact variant effectively reduce memory footprint mitigate risk overfitting extensive experiment validate superiority term visual quality efficiency across range dynamic scenerelated task eg novel view synthesis generation scene understanding scenario eg single object indoor scene driving environment synthetic real data
fast spatialtemporal consistent generation via video diffusion model availability largescale multimodal datasets advancement diffusion model significantly accelerated progress content generation prior approach rely multiple image video diffusion model utilizing score distillation sampling optimization generating pseudo novel view direct supervision however method hindered slow optimization speed multiview inconsistency issue spatial temporal consistency geometry extensively explored respectively diffusion model traditional monocular video diffusion model building foundation propose strategy migrate temporal consistency video diffusion model spatialtemporal consistency required generation specifically present novel framework efficient scalable content generation leveraging meticulously curated dynamic dataset develop video diffusion model capable synthesizing orbital view dynamic asset control dynamic strength asset introduce motion magnitude metric guidance additionally propose novel motion magnitude reconstruction loss classifierfree guidance refine learning generation motion dynamic obtaining orbital view asset perform explicit construction gaussian splatting coarsetofine manner synthesized multiview consistent image set enables u swiftly generate highfidelity diverse asset within several minute extensive experiment demonstrate method surpasses prior stateoftheart technique term generation efficiency geometry consistency across various prompt modality
oneclick upgrade sandwiched rgbd video compression stereoscopic teleconferencing stereoscopic video conferencing still challenging due need compress stereo rgbd video realtime though hardware implementation standard video codecs avc hevc widely available designed stereoscopic video suffer reduced quality performance specific multiview extension codecs complex lack efficient implementation paper propose new approach upgrade video codec support stereo rgbd video compression wrapping neural pre postprocessor pair neural network endtoend trained image codec proxy shown work sophisticated video codec also propose geometryaware loss function improve rendering quality train neural pre postprocessors synthetic people dataset evaluate synthetic realcaptured stereo rgbd video experimental result show neural network generalize well unseen data work outofbox various video codecs approach save bitrate compared conventional video coding scheme mvhevc level rendering quality novel view without need taskspecific hardware upgrade
vloggeraugmented graph neural network model microvideo recommendation existing microvideo recommendation model exploit interaction user microvideos andor multimodal information microvideos predict next microvideo user watch ignoring information related vloggers ie producer microvideos however microvideo scenario vloggers play significant role uservideo interaction since vloggers generally focus specific topic user tend follow vloggers interested therefore paper propose vloggeraugmented graph neural network model vagnn take effect vloggers consideration specifically construct tripartite graph user microvideos vloggers node capturing user preference different view ie videoview vloggerview moreover conduct crossview contrastive learning keep consistency node embeddings two different view besides predicting next uservideo interaction adaptively combine user preference video vlogger conduct extensive experiment two realworld datasets experimental result show vagnn outperforms multiple existing gnnbased recommendation model
dlkdd duallight knowledge distillation action recognition dark human action recognition dark video challenging task computer vision recent research focus applying dark enhancement method improve visibility video however video processing result loss critical information original unenhanced video conversely traditional twostream method capable learning information original processed video lead significant increase computational cost inference phase task video classification address challenge propose novel teacherstudent video classification framework named duallight knowledge distillation action recognition dark dlkdd framework enables model learn original enhanced video without introducing additional computational cost inference specifically dlkdd utilizes strategy knowledge distillation training teacher model trained enhanced video student model trained original video soft target generated teacher model teacherstudent framework allows student model predict action using original input video inference experiment proposed dlkdd framework outperforms stateoftheart method arid arid datasets achieve best performance dataset improvement using original video input thus avoiding use twostream framework enhancement module inference validate effectiveness distillation strategy ablative experiment result highlight advantage knowledge distillation framework dark human action recognition
videollmonline online video large language model streaming video recent large language model enhanced vision capability enabling comprehend image video interleaved visionlanguage content however learning method large multimodal model typically treat video predetermined clip making less effective efficient handling streaming video input paper propose novel learninginvideostream live framework enables temporally aligned longcontext realtime conversation within continuous video stream live framework comprises comprehensive approach achieve video streaming dialogue encompassing training objective designed perform language modeling continuous streaming input data generation scheme convert offline temporal annotation streaming dialogue format optimized inference pipeline speed model response realworld video stream live framework built videollmonline model upon demonstrate significant advantage processing streaming video instance average model support streaming dialogue video clip fps gpu moreover also showcase stateoftheart performance public offline video benchmark recognition captioning forecasting code model data demo made available httpsshowlabgithubiovideollmonline
clipvqavideo quality assessment via clip learning visionlanguage representation webscale data contrastive languageimage pretraining clip mechanism demonstrated remarkable performance many vision task however application widely studied video quality assessment vqa task still open issue paper propose efficient effective clipbased transformer method vqa problem clipvqa specifically first design effective video frame perception paradigm goal extracting rich spatiotemporal quality content information among video frame spatiotemporal quality feature adequately integrated together using selfattention mechanism yield videolevel quality representation utilize quality language description video supervision develop clipbased encoder language embedding fully aggregated generated content information via crossattention module producing videolanguage representation finally videolevel quality videolanguage representation fused together final video quality prediction vectorized regression loss employed efficient endtoend optimization comprehensive experiment conducted eight inthewild video datasets diverse resolution evaluate performance clipvqa experimental result show proposed clipvqa achieves new stateoftheart vqa performance better generalizability existing benchmark vqa method series ablation study also performed validate effectiveness module clipvqa
learning video context interleaved multimodal sequence narrative video movie pose significant challenge video understanding due rich context character dialogue storyline diverse demand identify relationship reason paper introduce movieseq multimodal language model developed address wide range challenge understanding video context core idea represent video interleaved multimodal sequence including image plot video subtitle either linking external knowledge database using offline model whisper subtitle instructiontuning approach empowers language model interact video using interleaved multimodal instruction example instead solely relying video input jointly provide character photo alongside name dialogue allowing model associate element generate comprehensive response demonstrate effectiveness validate movieseqs performance six datasets lvu mad movienet cmd tvc movieqa across five setting video classification audio description videotext retrieval video captioning video questionanswering code public httpsgithubcomshowlabmovieseq
et bench towards openended eventlevel videolanguage understanding recent advance video large language model videollms demonstrated great potential generalpurpose video understanding verify significance model number benchmark proposed diagnose capability different scenario however existing benchmark merely evaluate model videolevel questionanswering lacking finegrained eventlevel assessment task diversity fill gap introduce et bench eventlevel timesensitive video understanding benchmark largescale highquality benchmark openended eventlevel video understanding categorized within task taxonomy et bench encompasses sample task video total length domain providing comprehensive evaluation extensively evaluated imagellms videollms benchmark result reveal stateoftheart model coarselevel videolevel understanding struggle solve finegrained task eg grounding eventofinterests within video largely due short video context length improper time representation lack multievent training data focusing issue propose strong baseline model et chat together instructiontuning dataset et instruct tailored finegrained eventlevel understanding simple effective solution demonstrates superior performance multiple scenario
second hour reviewing multimodal large language model comprehensive long video understanding integration large language model llm visual encoders recently shown promising performance visual understanding task leveraging inherent capability comprehend generate humanlike text visual reasoning given diverse nature visual data multimodal large language model mmllms exhibit variation model designing training understanding image short video long video paper focus substantial difference unique challenge posed long video understanding compared static image short video understanding unlike static image short video encompass sequential frame spatial withinevent temporal information long video consist multiple event betweenevent longterm temporal information survey aim trace summarize advancement mmllms image understanding long video understanding review difference among various visual understanding task highlight challenge long video understanding including finegrained spatiotemporal detail dynamic event longterm dependency provide detailed summary advancement mmllms term model design training methodology understanding long video finally compare performance existing mmllms video understanding benchmark various length discus potential future direction mmllms long video understanding
video seal open efficient video watermarking proliferation aigenerated content sophisticated video editing tool made important challenging moderate digital platform video watermarking address challenge embedding imperceptible signal video allowing identification however rare open tool method often fall short efficiency robustness flexibility reduce gap paper introduces video seal comprehensive framework neural video watermarking competitive opensourced model approach jointly train embedder extractor ensuring watermark robustness applying transformation inbetween eg video codecs training multistage includes image pretraining hybrid posttraining extractor finetuning also introduce temporal watermark propagation technique convert image watermarking model efficient video watermarking model without need watermark every highresolution frame present experimental result demonstrating effectiveness approach term speed imperceptibility robustness video seal achieves higher robustness compared strong baseline especially challenging distortion combining geometric transformation video compression additionally provide new insight impact video compression training compare method operating different payload contribution work including codebase model public demo opensourced permissive license foster research development field
tinq temporal inconsistency guided blind video quality assessment blind video quality assessment bvqa actively researched usergenerated content ugc video recently superresolution sr technique widely applied ugc therefore effective bvqa method ugc sr scenario essential temporal inconsistency referring irregularity consecutive frame relevant video quality current bvqa approach typically model temporal relationship ugc video using statistic motion information inconsistency remain unexplored additionally different temporal inconsistency ugc video inconsistency sr video amplified due upscaling algorithm paper introduce temporal inconsistency guided blind video quality assessment tinq metric demonstrating exploring temporal inconsistency crucial effective bvqa since temporal inconsistency vary ugc sr video calculated different way based spatial module highlight inconsistent area across consecutive frame coarse fine granularity addition temporal module aggregate feature time two stage first stage employ visual memory capacity block adaptively segment time dimension based estimated complexity second stage focus selecting key feature stage work together consistencyaware fusion unit regress crosstimescale video quality extensive experiment ugc sr video quality datasets show method outperforms existing stateoftheart bvqa method code available httpsgithubcomlightingyxlitinq
ppllava varied video sequence understanding prompt guidance past year witnessed significant advancement videobased large language model however challenge developing unified model short long video understanding remains unresolved existing video llm handle hourlong video method custom long video tend ineffective shorter video image paper identify key issue redundant content video address propose novel pooling strategy simultaneously achieves token compression instructionaware visual feature aggregation model termed promptguided pooling llava ppllava short specifically ppllava consists three core component clipbased visualprompt alignment extract visual information relevant user instruction promptguided pooling compress visual sequence arbitrary scale using convolutionstyle pooling clip context extension designed lengthy prompt common visual dialogue moreover codebase also integrates advanced video direct preference optimization dpo visual interleave training extensive experiment validated performance model superior throughput visual context ppllava achieves better result image benchmark video llm achieving stateoftheart performance across various video benchmark excelling task ranging caption generation multiplechoice question handling video length second hour code available httpsgithubcomfarewellthreeppllava
selfguided trajectory control imagetovideo generation method imagetovideo generation achieved impressive photorealistic quality however adjusting specific element generated video object motion camera movement often tedious process trial error eg involving regenerating video different random seed recent technique address issue finetuning pretrained model follow conditioning signal bounding box point trajectory yet finetuning procedure computationally expensive requires datasets annotated object motion difficult procure work introduce framework controllable imagetovideo generation zeroshot control relying solely knowledge present pretrained imagetovideo diffusion model without need finetuning external knowledge zeroshot method outperforms unsupervised baseline significantly narrowing performance gap supervised model term visual quality motion fidelity additional detail video result available project page
vonet unsupervised video object learning parallel unet attention objectwise sequential vae unsupervised video object learning seek decompose video scene structural object representation without supervision depth optical flow segmentation present vonet innovative approach inspired monet utilizing unet architecture vonet employ efficient effective parallel attention inference process generating attention mask slot simultaneously additionally enhance temporal consistency mask across consecutive video frame vonet develops objectwise sequential vae framework integration innovative encoderside technique conjunction expressive transformerbased decoder establishes vonet leading unsupervised method object learning across five movi datasets encompassing video diverse complexity code available httpsgithubcomhnyuvonet
fast deep predictive coding network video feature extraction without label braininspired deep predictive coding network dpcns effectively model capture video feature bidirectional information flow even without label based overcomplete description video scene one bottleneck lack effective sparsification technique find discriminative robust dictionary fista best alternative paper proposes dpcn fast inference internal model variable state cause achieves high sparsity accuracy feature clustering proposed unsupervised learning procedure inspired adaptive dynamic programming majorizationminimization framework convergence rigorously analyzed experiment data set super mario bros video game validate approach outperforms previous version dpcns learning rate sparsity ratio feature clustering accuracy dcpns solid foundation explainability advance open door general application object recognition video without label
locomotion learning motionfocused videolanguage representation paper strives motionfocused videolanguage representation existing method learn videolanguage representation use spatialfocused data identifying object scene often enough distinguish relevant caption instead propose locomotion learn motionfocused caption describe movement temporal progression local object motion achieve adding synthetic motion video using parameter motion generate corresponding caption furthermore propose verbvariation paraphrasing increase caption variety learn link primitive motion highlevel verb able learn motionfocused videolanguage representation experiment demonstrate approach effective variety downstream task particularly limited data available finetuning code available httpshazeldoughtygithubiopaperslocomotion
simple effective temporal grounding pipeline basketball broadcast footage present reliable temporal grounding pipeline videotoanalytic alignment basketball broadcast footage given series frame input method quickly accurately extract timeremaining quarter value basketball broadcast scene work intends expedite development large multimodal video datasets train datahungry video model sport action recognition domain method aligns prelabeled corpus playbyplay annotation containing dense event annotation video frame enabling quick retrieval labeled video segment unlike previous method forgo need localize game clock finetuning outofthebox object detector find semantic text region directly endtoend approach improves generality work additionally interpolation parallelization technique prepare pipeline deployment large computing cluster code made publicly available
geometryaware videoinstruction tuning embodied navigation visionandlanguage navigation vln suffers limited diversity scale training data primarily constrained manual curation existing simulator address introduce videoinstruction dataset derived webbased room tour video capture realworld indoor space human walking demonstration unlike existing vln datasets leverage scale diversity online video generate openended human walking trajectory openworld navigable instruction compensate lack navigation data online video perform reconstruction obtain trajectory walking path augmented additional information room type object location shape surrounding scene dataset includes openended descriptionenriched trajectory instruction actionenriched trajectory room tour environment demonstrate experimentally enables significant improvement across multiple vln task including cvdn soon reverie moreover facilitates development trainable zeroshot vln agent showcasing potential challenge advancing towards openworld navigation
investigating memorization video diffusion model diffusion model widely used image video generation face significant limitation risk memorizing reproducing training data inference potentially generating unauthorized copyrighted content prior research focused image diffusion model idms video diffusion model vdms remain underexplored address gap first formally define two type memorization vdms content memorization motion memorization practical way focus privacy preservation applies generation type introduce new metric specifically designed separately assess content motion memorization vdms additionally curate dataset text prompt prone triggering memorization used conditioning vdms leveraging prompt generate diverse video various opensource vdms successfully extracting numerous training video tested model application proposed metric systematically analyze memorization across various pretrained vdms including textconditional unconditional model variety datasets comprehensive study reveals memorization widespread across tested vdms indicating vdms also memorize image training data addition video datasets finally propose efficient effective detection strategy content motion memorization offering foundational approach improving privacy vdms
viditq efficient accurate quantization diffusion transformer image video generation diffusion transformer demonstrated remarkable performance visual generation task generating realistic image video based textual instruction however larger model size multiframe processing video generation lead increased computational memory cost posing challenge practical deployment edge device posttraining quantization ptq effective method reducing memory cost computational complexity quantizing diffusion transformer find existing quantization method face challenge applied texttoimage video task address challenge begin systematically analyzing source quantization error conclude unique challenge posed dit quantization accordingly design improved quantization scheme viditq video image diffusion transformer quantization tailored specifically dit model validate effectiveness viditq across variety texttoimage video model achieving negligible degradation visual quality metric additionally implement efficient gpu kernel achieve practical memory saving endtoend latency speedup
good video lmm complex video reasoning robustness evaluation suite videolmms recent advancement large language model llm led development video large multimodal model videolmms handle wide range video understanding task model potential deployed realworld application robotics ai assistant medical surgery autonomous vehicle widespread adoption videolmms daily life underscore importance ensuring evaluating robust performance mirroring humanlike reasoning interaction capability complex realworld context however existing benchmark videolmms primarily focus general video comprehension ability neglect assessing reasoning capability complex video realworld context robustness model lens user prompt text query paper present complex video reasoning robustness evaluation suite cvrres novel benchmark comprehensively assesses performance videolmms across diverse realworld video dimension evaluate recent model including opensource closedsource variant find videolmms especially opensource one struggle robustness reasoning dealing complex video based analysis develop trainingfree dualstep contextual prompting dscp technique enhance performance existing videolmms finding provide valuable insight building next generation humancentric ai system advanced robustness reasoning capability dataset code publicly available httpsmbzuaioryxgithubiocvrrevaluationsuite
vript video worth thousand word advancement multimodal learning particularly video understanding generation require highquality videotext datasets improved model performance vript address issue meticulously annotated corpus highresolution video offering detailed dense scriptlike caption clip clip caption word longer videotext datasets unlike caption documenting static content previous datasets enhance video captioning video scripting documenting content also camera operation include shot type medium shot closeup etc camera movement panning tilting etc utilizing vript explore three training paradigm aligning text video modality rather clipcaption pair result vriptor topperforming video captioning model among opensource model comparable performance vriptor also powerful model capable endtoend generation dense detailed caption long video moreover introduce vripthard benchmark consisting three video understanding task challenging existing benchmark vripthal first benchmark evaluating action object hallucination video llm vriptrr combine reasoning retrieval resolving question ambiguity longvideo qas vriptero new task evaluate temporal understanding event long video rather action short video previous work code model datasets available httpsgithubcommutonixvript p included videotext datasets vript series
vidtldr training free token merging lightweight video transformer video transformer become prevalent solution various video downstream task superior expressive power flexibility however video transformer suffer heavy computational cost induced massive number token across entire video frame major barrier training model patch irrelevant main content eg background degrade generalization performance model tackle issue propose training free token merging lightweight video transformer vidtldr aim enhance efficiency video transformer merging background token without additional training vidtldr introduce novel approach capture salient region video attention map introduce saliencyaware token merging strategy dropping background token sharpening object score experiment show vidtldr significantly mitigates computational complexity video transformer achieving competitive performance compared base model without vidtldr code available httpsgithubcommlvlabvidtldr
understanding long video multimodal language model large language model llm allowed recent llmbased approach achieve excellent performance longvideo understanding benchmark investigate extensive world knowledge strong reasoning skill underlying llm influence strong performance surprisingly discover llmbased approach yield surprisingly good accuracy longvideo task limited video information sometimes even video specific information building explore injecting videospecific information llmbased framework utilize offtheshelf vision tool extract three objectcentric information modality video leverage natural language medium fusing information resulting multimodal video understanding mvu framework demonstrates stateoftheart performance across multiple video understanding benchmark strong performance also robotics domain task establish strong generality code httpsgithubcomkahnchanamvu
every shot count using exemplar repetition counting video video repetition counting infers number repetition recurring action motion within video propose exemplarbased approach discovers visual correspondence video exemplar across repetition within target video proposed every shot count escounts model attentionbased encoderdecoder encodes video varying length alongside exemplar different video training escounts regress location high correspondence exemplar within video tandem method learns latent encodes representation general repetitive motion use exemplarfree zeroshot inference extensive experiment commonly used datasets repcount countix ucfrep showcase escounts obtaining stateoftheart performance across three datasets detailed ablation demonstrate effectiveness method
auvmae knowledgeguide action unit detection via video masked autoencoder current facial action unit fau detection method generally encounter difficulty due scarcity labeled video training data limited number training face id render trained feature extractor insufficient coverage modeling large diversity interperson facial structure movement explicitly address challenge propose novel videolevel pretraining scheme fully exploring multilabel property faus video well temporal label consistency heart design pretrained video feature extractor based videomasked autoencoder together finetuning network jointly completes multilevel video faus analysis task emphie integrating videolevel framelevel fau detection thus dramatically expanding supervision set sparse faus annotation video frame including masked one moreover utilize interframe intraframe au pair state matrix prior knowledge guide network training instead traditional graph neural network better temporal supervision approach demonstrates substantial enhancement performance compared existing stateoftheart method used disfa faus datasets
delving deep engagement prediction short video understanding modeling popularity user generated content ugc short video social medium platform present critical challenge broad implication content creator recommendation system study delf deep intricacy predicting engagement newly published video limited user interaction surprisingly finding reveal mean opinion score previous video quality assessment datasets strongly correlate video engagement level address introduce substantial dataset comprising realworld ugc short video snapchat rather relying view count average watch time rate like propose two metric normalized average watch percentage nawp engagement continuation rate ecr describe engagement level short video comprehensive multimodal feature including visual content background music text data investigated enhance engagement prediction proposed dataset two key metric method demonstrates ability predict engagement short video purely video content
video instruction tuning synthetic data development video large multimodal model lmms hindered difficulty curating large amount highquality raw data web address propose alternative approach creating highquality synthetic dataset specifically video instructionfollowing namely dataset includes key task detailed captioning openended questionanswering qa multiplechoice qa training dataset combination existing visual instruction tuning data introduce llavavideo new video lmm experiment demonstrate llavavideo achieves strong performance across various video benchmark highlighting effectiveness dataset plan release dataset generation pipeline model checkpoint
whats video factorized autoregressive decoding online dense video captioning generating automatic dense caption video accurately describe content remains challenging area research current model require processing entire video instead propose efficient online approach output frequent detailed temporally aligned caption without access future frame model us novel autoregressive factorized decoding architecture model sequence visual feature time segment outputting localized description efficiently leverage context previous video segment allows model output frequent detailed caption comprehensively describe video according actual local content rather mimic training data second propose optimization efficient training inference enables scaling longer video approach show excellent performance compared offline online method us less compute annotation produced much comprehensive frequent utilized automatic video tagging largescale video data harvesting
subjectivealigned dataset metric texttovideo quality assessment rapid development generative model artificial intelligencegenerated content aigc exponentially increased daily life among texttovideo generation received widespread attention though many model released generating high perceptual quality video still lack method evaluate quality video quantitatively solve issue establish largestscale texttovideo quality assessment database date dataset composed video generated different model also conduct subjective study obtain video corresponding mean opinion score based propose novel transformerbased model subjectivealigned texttovideo quality assessment model extract feature textvideo alignment video fidelity perspective leverage ability large language model give prediction score experimental result show outperforms existing metric sota video quality assessment model quantitative analysis indicates capable giving subjectivealign prediction validating effectiveness dataset code released
directed domain finetuning tailoring separate modality specific training task large language model llm large visual language model lvlms forefront artificial intelligence field particularly task like text generation video captioning questionanswering typically applicable train model broader knowledge base datasets increase generalizability learn relationship topic recognize pattern instead propose provide instructional datasets specific task modality within distinct domain finetune parameter model using lora approach eliminate noise irrelevant given task also ensuring model generates enhanced precision work use videollava generate recipe given cooking video without transcript videollavas multimodal architecture allows u provide cooking image image encoder cooking video video encoder general cooking question text encoder thus aim remove noise unrelated cooking improving model capability generate specific ingredient list detailed instruction result approach finetuning videollava lead gain baseline videollava dataset may seem like marginal increase model train image instruction dataset size videollavas video instruction dataset videollavas
bridging gap analogue circuit discrete frame videotoevents simulator event camera operate fundamentally differently traditional active pixel sensor aps camera offering significant advantage recent research developed simulator convert video frame event addressing shortage real event datasets current simulator primarily focus logical behavior event camera however fundamental analogue property pixel circuit seldom considered simulator design gap analogue pixel circuit discrete video frame cause degeneration synthetic event particularly highcontrast scene paper propose novel method generating reliable event data based detailed analysis pixel circuitry event camera incorporate analogue property event camera pixel circuit simulator design analogue filtering signal light intensity event cutoff frequency independent video frame rate experimental result two relevant task including semantic segmentation image reconstruction validate reliability simulated event data even highcontrast scene demonstrates deep neural network exhibit strong generalization simulated real event data confirming synthetic event generated proposed method realistic wellsuited effective training
tivdiffusion towards objectcentric movement textdriven image video generation textdriven image video generation aim generate controllable video given first frame corresponding textual description primary challenge task lie two part identify target object ensure consistency movement trajectory textual description ii improve subjective quality generated video tackle challenge propose new diffusionbased framework termed tivdiffusion via objectcentric textualvisual alignment intending achieve precise control highquality video generation based textualdescribed motion different object concretely enable tivdiffuion model perceive textualdescribed object motion trajectory incorporating fused textual visual knowledge scaleoffset modulation moreover mitigate problem object disappearance misaligned object motion introduce objectcentric textualvisual alignment module reduces risk misaligned objectsmotion decoupling object reference image aligning textual feature object individually based innovation tivdiffusion achieves stateoftheart highquality video generation compared existing method
materialpicker multimodal material generation diffusion transformer highquality material generation key virtual environment authoring inverse rendering propose materialpicker multimodal material generator leveraging diffusion transformer dit architecture improving simplifying creation highquality material text prompt andor photograph method generate material based image crop material sample even captured surface distorted viewed angle partially occluded often case photograph natural scene allow user specify text prompt provide additional guidance generation finetune pretrained ditbased video generator material generator material map treated frame video sequence evaluate approach quantitatively qualitatively show enables diverse material generation better distortion correction previous work
comprehensive survey human video generation challenge method insight human video generation dynamic rapidly evolving task aim synthesize human body video sequence generative model given control condition text audio pose potential wideranging application film gaming virtual communication ability generate natural realistic human video critical recent advancement generative model laid solid foundation growing interest area despite significant progress task human video generation remains challenging due consistency character complexity human motion difficulty relationship environment survey provides comprehensive review current state human video generation marking best knowledge first extensive literature review domain start introduction fundamental human video generation evolution generative model facilitated field growth examine main method employed three key subtasks within human video generation textdriven audiodriven posedriven motion generation area explored concerning condition guide generation process furthermore offer collection commonly utilized datasets evaluation metric crucial assessing quality realism generated video survey concludes discussion current challenge field suggests possible direction future research goal survey offer research community clear holistic view advancement human video generation highlighting milestone achieved challenge lie ahead
asymrnr video diffusion transformer acceleration asymmetric reduction restoration diffusion transformer dit proven effective generating highquality video hindered high computational cost existing video dit sampling acceleration method often rely costly finetuning exhibit limited generalization capability propose asymmetric reduction restoration asymrnr trainingfree modelagnostic method accelerate video dit build observation redundancy feature token dit vary significantly across different model block denoising step feature type asymrnr asymmetrically reduces redundant token attention operation achieving acceleration negligible degradation output quality case even improving also tailored reduction schedule distribute reduction across component adaptively accelerate process introduce matching cache efficient reduction backed theoretical foundation extensive experimental validation asymrnr integrates stateoftheart video dit offer substantial speedup
advancing video quality assessment aigc recent year ai generative model made remarkable progress across various domain including text generation image generation video generation however assessing quality texttovideo generation still infancy existing evaluation framework fall short compared natural video current video quality assessment vqa method primarily focus evaluating overall quality natural video fail adequately account substantial quality discrepancy frame generated video address issue propose novel loss function combine mean absolute error crossentropy loss mitigate interframe quality inconsistency additionally introduce innovative technique retain critical content leveraging adversarial training enhance model generalization capability experimental result demonstrate method outperforms existing vqa technique aigc video dataset surpassing previous stateoftheart term plcc
human calibration pattern dynamic scene reconstruction unsynchronized uncalibrated video recent work dynamic neural field reconstruction assume input synchronized multiview video whose pose known input constraint often satisfied realworld setup making approach impractical show unsynchronized video unknown pose generate dynamic neural field long video capture human motion human one common dynamic subject captured video shape pose estimated using stateoftheart library noisy estimated human shape pose parameter provide decent initialization point start highly nonconvex underconstrained problem training consistent dynamic neural representation given shape pose parameter human individual frame formulate method calculate time offset video followed camera pose estimation analyze joint position train dynamic neural field employing multiresolution grid concurrently refine time offset camera pose setup still involves optimizing many parameter therefore introduce robust progressive learning strategy stabilize process experiment show approach achieves accurate spatiotemporal calibration highquality scene reconstruction challenging condition
match stereo video via bidirectional alignment video stereo matching task estimating consistent disparity map rectified stereo video considerable scope improvement datasets method within area recent learningbased method often focus optimizing performance independent stereo pair leading temporal inconsistency video existing video method typically employ sliding window operation time dimension result lowfrequency oscillation corresponding window size address challenge propose bidirectional alignment mechanism adjacent frame fundamental operation building introduce novel video processing framework bidastereo plugin stabilizer network bidastabilizer compatible general imagebased method regarding datasets current synthetic objectbased indoor datasets commonly used training benchmarking lack outdoor nature scenario bridge gap present realistic synthetic dataset benchmark focused natural scene along realworld dataset captured stereo camera diverse urban scene qualitative evaluation extensive experiment indomain outofdomain robustness evaluation demonstrate contribution method datasets showcasing improvement prediction quality achieving stateoftheart result various commonly used benchmark project page demo code datasets available urlhttpstomtomtommigithubiobidavideo
explainable controllable motion curve guided cardiac ultrasound video generation echocardiography video primary modality diagnosing heart disease limited data pose challenge clinical teaching machine learning training recently video generative model emerged promising strategy alleviate issue however previous method often relied holistic condition generation hindering flexible movement control specific cardiac structure context propose explainable controllable method echocardiography video generation taking initial frame motion curve guidance contribution threefold first extract motion information heart substructure construct motion curve enabling diffusion model synthesize customized echocardiography video modifying curve second propose structuretomotion alignment module map semantic feature onto motion curve across cardiac structure third positionaware attention mechanism designed enhance video consistency utilizing gaussian mask structural position information extensive experiment three echocardiography datasets show method outperforms others regarding fidelity consistency full code released
videotoaudio generation finegrained temporal semantics recent advance aigc video generation gained surge research interest academia industry eg sora however remains challenge produce temporally aligned audio synchronize generated video considering complicated semantic information included latter work inspired recent success texttoaudio tta generation first investigate videotoaudio vta generation framework based latent diffusion model ldm similar latest pioneering exploration vta preliminary result also show great potential ldm vta task still suffers suboptimal temporal alignment end propose enhance temporal alignment vta framelevel semantic information recently popular grounding segment anything model grounding sam extract finegrained semantics video frame enable vta produce betteraligned audio signal extensive experiment demonstrate effectiveness system objective subjective evaluation metric show better audio quality finegrained temporal alignment
sonique video background music generation using unpaired audiovisual data present sonique model generating background music tailored video content unlike traditional videotomusic generation approach rely heavily paired audiovisual datasets sonique leverage unpaired data combining royaltyfree music independent video source utilizing large language model llm video understanding converting visual description musical tag alongside unetbased conditional diffusion model sonique enables customizable music generation user control specific aspect music instrument genre tempo melody ensuring generated output fit creative vision sonique opensource demo available online
dragentity trajectory guided video generation using entity positional relationship recent year diffusion model achieved tremendous success field video generation controllable video generation receiving significant attention however existing control method still face two limitation firstly control condition depth map mesh difficult ordinary user obtain directly secondly challenging drive multiple object complex motion multiple trajectory simultaneously paper introduce dragentity video generation model utilizes entity representation controlling motion multiple object compared previous method dragentity offer two main advantage method userfriendly interaction allows user drag entity within image rather individual pixel use entity representation represent object image multiple object maintain relative spatial relationship therefore allow multiple trajectory control multiple object image different level complexity simultaneously experiment validate effectiveness dragentity demonstrating excellent performance finegrained control video generation
sora review background technology limitation opportunity large vision model sora texttovideo generative ai model released openai february model trained generate video realistic imaginative scene text instruction show potential simulating physical world based public technical report reverse engineering paper present comprehensive review model background related technology application remaining challenge future direction texttovideo ai model first trace soras development investigate underlying technology used build world simulator describe detail application potential impact sora multiple industry ranging filmmaking education marketing discus main challenge limitation need addressed widely deploy sora ensuring safe unbiased video generation lastly discus future development sora video generation model general advancement field could enable new way humanai interaction boosting productivity creativity video generation
unipaint unified spacetime video inpainting via mixtureofexperts paper present unipaint unified generative spacetime video inpainting framework enables spatialtemporal inpainting interpolation different existing method treat video inpainting video interpolation two distinct task leverage unified inpainting framework tackle observe two task mutually enhance synthesis performance specifically first introduce plugandplay spacetime video inpainting adapter employed various personalized model key insight propose mixture expert moe attention cover various task design spatialtemporal masking strategy training stage mutually enhance improve performance unipaint produce highquality aesthetically pleasing result achieving best quantitative result across various task scale setup code checkpoint available soon
image compression using novel view synthesis prior realtime visual feedback essential tetherless control remotely operated vehicle particularly inspection manipulation task though acoustic communication preferred choice mediumrange communication underwater limited bandwidth render impractical transmit image video realtime address propose modelbased image compression technique leverage prior mission information approach employ trained machinelearning based novel view synthesis model us gradient descent optimization refine latent representation help generate compressible difference camera image rendered image evaluate proposed compression technique using dataset artificial ocean basin demonstrating superior compression ratio image quality existing technique moreover method exhibit robustness introduction new object within scene highlighting potential advancing tetherless remotely operated vehicle operation
advancing diffusion model aliasfree resampling enhanced rotational equivariance recent advance image generation particularly via diffusion model led impressive improvement image synthesis quality despite diffusion model still challenged modelinduced artifact limited stability image fidelity work hypothesize primary cause issue improper resampling operation introduces aliasing diffusion model careful aliasfree resampling dictated image processing theory improve model performance image synthesis propose integration aliasfree resampling layer unet architecture diffusion model without adding extra trainable parameter thereby maintaining computational efficiency assess whether theorydriven modification enhance image quality rotational equivariance experimental result benchmark datasets including mnist mnistm reveal consistent gain image quality particularly term fid kid score furthermore propose modified diffusion process enables usercontrolled rotation generated image without requiring additional training finding highlight potential theorydriven enhancement aliasfree resampling generative model improve image quality maintaining model efficiency pioneer future research direction incorporate videogenerating diffusion model enabling deeper exploration application aliasfree resampling generative modeling
oneshot realistic talking portrait synthesis oneshot talking portrait generation aim reconstruct avatar unseen image animate reference video audio generate talking portrait video existing method fail simultaneously achieve goal accurate avatar reconstruction stable talking face animation besides existing work mainly focus synthesizing head part also vital generate natural torso background segment obtain realistic talking portrait video address limitation present framework improves oneshot reconstruction power large imagetoplane model distills prior knowledge face generative model facilitates accurate motionconditioned animation efficient motion adapter synthesizes realistic video natural torso movement switchable background using headtorsobackground superresolution model support oneshot audiodriven talking face generation generalizable audiotomotion model extensive experiment show generalizes well unseen identity generates realistic talking portrait video compared previous method video sample source code available
hoiswap swapping object video handobject interaction awareness study problem precisely swapping object video focus interacted hand given one userprovided reference object image despite great advancement diffusion model made video editing recently model often fall short handling intricacy handobject interaction hoi failing produce realistic edits especially object swapping result object shape functionality change bridge gap present hoiswap novel diffusionbased video editing framework trained selfsupervised manner designed two stage first stage focus object swapping single frame hoi awareness model learns adjust interaction pattern hand grasp based change object property second stage extends singleframe edit across entire sequence achieve controllable motion alignment original video warping new sequence stagei edited frame based sampled motion point conditioning video generation warped sequence comprehensive qualitative quantitative evaluation demonstrate hoiswap significantly outperforms existing method delivering highquality video edits realistic hois
trip temporal residual learning image noise prior imagetovideo diffusion model recent advance texttovideo generation demonstrated utility powerful diffusion model nevertheless problem trivial shaping diffusion model animate static image ie imagetovideo generation difficulty originates aspect diffusion process subsequent animated frame preserve faithful alignment given image also pursue temporal coherence among adjacent frame alleviate present trip new recipe imagetovideo diffusion paradigm pivot image noise prior derived static image jointly trigger interframe relational reasoning ease coherent temporal modeling via temporal residual learning technically image noise prior first attained onestep backward diffusion process based static image noised video latent code next trip executes residuallike dualpath scheme noise prediction shortcut path directly take image noise prior reference noise frame amplify alignment first frame subsequent frame residual path employ noised video static image latent code enable interframe relational reasoning thereby easing learning residual noise frame furthermore reference residual noise frame dynamically merged via attention mechanism final video generation extensive experiment dtdb msrvtt datasets demonstrate effectiveness trip imagetovideo generation please see project page
videolavit unified videolanguage pretraining decoupled visualmotional tokenization light recent advance multimodal large language model llm increasing attention scaling imagetext data informative realworld video compared static image video pose unique challenge effective largescale pretraining due modeling spatiotemporal dynamic paper address limitation videolanguage pretraining efficient video decomposition represents video keyframes temporal motion adapted llm using welldesigned tokenizers discretize visual temporal information token thus enabling unified generative pretraining video image text inference generated token llm carefully recovered original continuous pixel space create various video content proposed framework capable comprehending generating image video content demonstrated competitive performance across multimodal benchmark image video understanding generation code model available httpsvideolavitgithubio
tuningfree framework videotovideo editing task dynamic field digital content creation using generative model stateoftheart video editing model still offer level quality control user desire previous work video editing either extended imagebased generative model zeroshot manner necessitated extensive finetuning hinder production fluid video edits furthermore method frequently rely textual input editing guidance leading ambiguity limiting type edits perform recognizing challenge introduce novel tuningfree paradigm designed simplify video editing two primary step employing offtheshelf image editing model modify first frame utilizing existing imagetovideo generation model generate edited video temporal feature injection leverage existing image editing tool support extensive array video editing task including promptbased editing referencebased style transfer subjectdriven editing identity manipulation unattainable previous method also support video length evaluation show achieved clipscores comparable baseline method furthermore significantly outperformed baseline human evaluation demonstrating notable improvement visual consistency source video producing highquality edits across editing task
goldfish visionlanguage understanding arbitrarily long video current llmbased model video understanding process video within minute however struggle lengthy video due challenge noise redundancy well memory computation constraint paper present goldfish methodology tailored comprehending video arbitrary length also introduce tvqalong benchmark specifically designed evaluate model capability understanding long video question vision text content goldfish approach challenge efficient retrieval mechanism initially gather topk video clip relevant instruction proceeding provide desired response design retrieval mechanism enables goldfish efficiently process arbitrarily long video sequence facilitating application context movie television series facilitate retrieval process developed generates detailed description video clip addressing scarcity benchmark long video evaluation adapted tvqa short video benchmark extended content analysis aggregating question entire episode thereby shifting evaluation partial full episode comprehension attained accuracy rate tvqalong benchmark surpassing previous method also show exceptional performance short video comprehension exceeding existing stateoftheart method msvd msrvtt tgif tvqa short video benchmark respectively result indicate model significant improvement long shortvideo understanding model code made publicly available
see got learning creation posefree video scale recent generation model typically rely limitedscale goldlabels diffusion prior content creation however performance upperbounded constrained prior due lack scalable learning paradigm work present visualconditional multiview diffusion model trained largescale internet video openworld creation model aim get knowledge solely seeing visual content vast rapidly growing video data see got achieve first scale training data using proposed data curation pipeline automatically filter multiview inconsistency insufficient observation source video result highquality richly diverse largescale dataset multiview image termed containing frame video clip nevertheless learning generic prior video without explicit geometry camera pose annotation nontrivial annotating pose webscale video prohibitively expensive eliminate need pose condition introduce innovative visualcondition purely visual signal generated adding timedependent noise masked video data finally introduce novel visualconditional generation framework integrating warpingbased pipeline highfidelity generation numerical visual comparison single sparse reconstruction benchmark show trained costeffective scalable video data achieves notable zeroshot openworld generation capability markedly outperforming model trained costly constrained datasets please refer project page
demamba aigenerated video detection millionscale genvideo benchmark recently video generation technique advanced rapidly given popularity video content social medium platform model intensify concern spread fake information therefore growing demand detector capable distinguishing fake aigenerated video mitigating potential harm caused fake information however lack largescale datasets advanced video generator pose barrier development detector address gap introduce first aigenerated video detection dataset genvideo feature following characteristic large volume video including one million aigenerated real video collected rich diversity generated content methodology covering broad spectrum video category generation technique conducted extensive study dataset proposed two evaluation method tailored realworldlike scenario assess detector performance crossgenerator video classification task assesses generalizability trained detector generator degraded video classification task evaluates robustness detector handle video degraded quality dissemination moreover introduced plugandplay module named detail mamba demamba designed enhance detector identifying aigenerated video analysis inconsistency temporal spatial dimension extensive experiment demonstrate demambas superior generalizability robustness genvideo compared existing detector believe genvideo dataset demamba module significantly advance field aigenerated video detection code dataset aviliable urlhttpsgithubcomchenhaoxingdemamba
storydiffusion consistent selfattention longrange image video generation recent diffusionbased generative model maintaining consistent content across series generated image especially containing subject complex detail present significant challenge paper propose new way selfattention calculation termed consistent selfattention significantly boost consistency generated image augments prevalent pretrained diffusionbased texttoimage model zeroshot manner extend method longrange video generation introduce novel semantic space temporal motion prediction module named semantic motion predictor trained estimate motion condition two provided image semantic space module convert generated sequence image video smooth transition consistent subject significantly stable module based latent space especially context long video generation merging two novel component framework referred storydiffusion describe textbased story consistent image video encompassing rich variety content proposed storydiffusion encompasses pioneering exploration visual story generation presentation image video hope could inspire research aspect architectural modification code made publicly available httpsgithubcomhvisionnkustorydiffusion
seeing hearing opendomain visualaudio generation diffusion latent aligners video audio content creation serf core technique movie industry professional user recently existing diffusionbased method tackle video audio generation separately hinders technique transfer academia industry work aim filling gap carefully designed optimizationbased framework crossvisualaudio jointvisualaudio generation observe powerful generation ability offtheshelf video audio generation model thus instead training giant model scratch propose bridge existing strong model shared latent representation space specifically propose multimodality latent aligner pretrained imagebind model latent aligner share similar core classifier guidance guide diffusion denoising process inference time carefully designed optimization strategy loss function show superior performance method joint videoaudio generation visualsteered audio generation audiosteered visual generation task project website found
interactive generation laparoscopic video diffusion model generative ai general synthetic visual data generation specific hold much promise benefiting surgical training providing photorealism simulation environment current training method primarily rely reading material observing live surgery timeconsuming impractical work take significant step towards improving training process specifically use diffusion model combination zeroshot video diffusion method interactively generate realistic laparoscopic image video specifying surgical action text guiding generation tool position segmentation mask demonstrate performance approach using publicly available cholec dataset family evaluate fidelity factual correctness generated image using surgical action recognition model well pixelwise spatial control tool generation achieve fid
motion prompting controlling video generation motion trajectory motion control crucial generating expressive compelling video content however existing video generation model rely mainly text prompt control struggle capture nuance dynamic action temporal composition end train video generation model conditioned spatiotemporally sparse dense motion trajectory contrast prior motion conditioning work flexible representation encode number trajectory objectspecific global scene motion temporally sparse motion due flexibility refer conditioning motion prompt user may directly specify sparse trajectory also show translate highlevel user request detailed semidense motion prompt process term motion prompt expansion demonstrate versatility approach various application including camera object motion control interacting image motion transfer image editing result showcase emergent behavior realistic physic suggesting potential motion prompt probing video model interacting future generative world model finally evaluate quantitatively conduct human study demonstrate strong performance video result available webpage httpsmotionpromptinggithubio
mapping noise data enhanced diffusion model diffusion model established de facto primary paradigm visual generative modeling revolutionizing field remarkable success across various diverse application ranging highquality image synthesis temporal aware video generation despite advancement three fundamental limitation persist including discrepancy training inference process progressive information leakage throughout noise corruption procedure inherent constraint preventing effective integration modern optimization criterion like perceptual adversarial loss mitigate critical challenge paper present novel endtoend learning paradigm establishes direct optimization final generated sample initial noise proposed endtoend differentiable diffusion dubbed introduces several key improvement eliminates sequential trainingsampling mismatch intermediate information leakage via conceptualizing training direct transformation isotropic gaussian noise target data distribution additionally training framework enables seamless incorporation adversarial perceptual loss core optimization objective comprehensive evaluation across standard benchmark including reveals method achieves substantial performance gain term frechet inception distance fid clip score even fewer sampling step less finding highlight endtoend mechanism might pave way robust efficient solution emphie combining diffusion stability ganlike discriminative optimization endtoend manner
spectrum translation refinement image generation stig based contrastive learning spectral filter profile currently image generation synthesis remarkably progressed generative model despite photorealistic result intrinsic discrepancy still observed frequency domain spectral discrepancy appeared generative adversarial network diffusion model study propose framework effectively mitigate disparity frequency domain generated image improve generative performance gan diffusion model realized spectrum translation refinement image generation stig based contrastive learning adopt theoretical logic frequency component various generative network key idea refine spectrum generated image via concept imagetoimage translation contrastive learning term digital signal processing evaluate framework across eight fake image datasets various cuttingedge model demonstrate effectiveness stig framework outperforms cuttingedges showing significant decrease fid log frequency distance spectrum emphasize stig improves image quality decreasing spectral anomaly additionally validation result present frequencybased deepfake detector confuses case fake spectrum manipulated stig
qvd posttraining quantization video diffusion model recently video diffusion model vdms garnered significant attention due notable advancement generating coherent realistic video content however processing multiple frame feature concurrently coupled considerable model size result high latency extensive memory consumption hindering broader application posttraining quantization ptq effective technique reduce memory footprint improve computational efficiency unlike image diffusion observe temporal feature integrated frame feature exhibit pronounced skewness furthermore investigate significant interchannel disparity asymmetry activation video diffusion model resulting low coverage quantization level individual channel increasing challenge quantization address issue introduce first ptq strategy tailored video diffusion model dubbed qvd specifically propose high temporal discriminability quantization htdq method designed temporal feature retains high discriminability quantized feature providing precise temporal guidance video frame addition present scattered channel range integration scri method aim improve coverage quantization level across individual channel experimental validation across various model datasets bitwidth setting demonstrate effectiveness qvd term diverse metric particular achieve nearlossless performance degradation outperforming current method fvd
safesora towards safety alignment generation via human preference dataset mitigate risk harmful output large vision model lvms introduce safesora dataset promote research aligning texttovideo generation human value dataset encompasses human preference texttovideo generation task along two primary dimension helpfulness harmlessness capture indepth human preference facilitate structured reasoning crowdworkers subdivide helpfulness subdimensions harmlessness subcategories serving basis pilot annotation safesora dataset includes unique prompt unique video generated distinct lvms pair preference annotation labeled human demonstrate utility safesora dataset several application including training textvideo moderation model aligning lvms human preference finetuning prompt augmentation module diffusion model application highlight potential foundation texttovideo alignment research human preference modeling development validation alignment algorithm
echopulse ecg controlled echocardiogram video generation echocardiography echo essential cardiac assessment video quality interpretation heavily relies manual expertise leading inconsistent result clinical portable device echo video generation offer solution improving automated monitoring synthetic data generating highquality video routine health data however existing model often face high computational cost slow inference rely complex conditional prompt require expert annotation address challenge propose echopulse ecgconditioned echo video generation model echopulse introduces two key advancement accelerates echo video generation leveraging vqvae tokenization masked visual token modeling fast decoding condition readily accessible ecg signal highly coherent echo video bypassing complex conditional prompt best knowledge first work use timeseries prompt like ecg signal echo video generation echopulse enables controllable synthetic echo data generation also provides updated cardiac function information disease monitoring prediction beyond ecg alone evaluation three public private datasets demonstrate stateoftheart performance echo video generation across qualitative quantitative measure additionally echopulse easily generalized modality generation task cardiac mri fmri ct generation demo seen
decoupling degradation recurrent network video restoration underdisplay camera underdisplay camera udc system foundation fullscreen display device lens mount display pixel array lightemitting diode used display diffracts attenuates incident light causing various degradation light intensity change unlike general video restoration recovers video treating different degradation factor equally video restoration udc system challenging concern removing diverse degradation time preserving temporal consistency paper introduce novel video restoration network called specifically designed udc system employ set decoupling attention module dam effectively separate various video degradation factor specifically soft mask generation function proposed formulate frame flare haze based diffraction arising incident light different intensity followed proposed flare haze removal component leverage long shortterm feature learning handle respective degradation design offer targeted effective solution eliminating various type degradation udc system extend design multiscale overcome scalechanging degradation often occur longrange video demonstrate superiority propose largescale udc video benchmark gathering hdr video generating realistically degraded video using point spread function measured commercial udc system extensive quantitative qualitative evaluation demonstrate superiority compared stateoftheart video restoration udc image restoration method code available httpsgithubcomchengxuliuddrnetgit
omnivid generative framework universal video understanding core video understanding task recognition captioning tracking automatically detect object action video analyze temporal evolution despite sharing common goal different task often rely distinct model architecture annotation format contrast natural language processing benefit unified output space ie text sequence simplifies training powerful foundational language model extensive training corpus inspired seek unify output space video understanding task using language label additionally introducing time box token way variety video task could formulated videogrounded token generation enables u address various type video task including classification action recognition captioning covering clip captioning video question answering dense video captioning localization task visual object tracking within fully shared encoderdecoder architecture following generative framework comprehensive experiment demonstrate simple straightforward idea quite effective achieve stateoftheart competitive result seven video benchmark providing novel perspective universal video understanding code available
enhancing video summarization context awareness video summarization crucial research area aim efficiently browse retrieve relevant information vast amount video content available today exponential growth multimedia data ability extract meaningful representation video become essential video summarization technique automatically generate concise summary selecting keyframes shot segment capture video essence process improves efficiency accuracy various application including video surveillance education entertainment social medium despite importance video summarization lack diverse representative datasets hindering comprehensive evaluation benchmarking algorithm existing evaluation metric also fail fully capture complexity video summarization limiting accurate algorithm assessment hindering field progress overcome data scarcity challenge improve evaluation propose unsupervised approach leverage video data structure information generating informative summary moving away fixed annotation framework produce representative summary effectively moreover introduce innovative evaluation pipeline tailored specifically video summarization human participant involved evaluation comparing generated summary ground truth summary assessing informativeness humancentric approach provides valuable insight effectiveness proposed technique experimental result demonstrate trainingfree framework outperforms existing unsupervised approach achieves competitive result compared stateoftheart supervised method
videogigagan towards detailrich video superresolution video superresolution vsr approach shown impressive temporal consistency upsampled video however approach tend generate blurrier result image counterpart limited generative capability raise fundamental question extend success generative image upsampler vsr task preserving temporal consistency introduce videogigagan new generative vsr model produce video highfrequency detail temporal consistency videogigagan build upon largescale image upsampler gigagan simply inflating gigagan video model adding temporal module produce severe temporal flickering identify several key issue propose technique significantly improve temporal consistency upsampled video experiment show unlike previous vsr method videogigagan generates temporally consistent video finegrained appearance detail validate effectiveness videogigagan comparing stateoftheart vsr model public datasets showcasing video result superresolution
instruct editing scene scene using diffusion paper proposes instruct achieves awareness spatialtemporal consistency diffusion model generate highquality instructionguided dynamic scene editing result traditional application diffusion model dynamic scene editing often result inconsistency primarily due inherent framebyframe editing methodology addressing complexity extending instructionguided editing key insight treat scene scene decoupled two subproblems achieving temporal consistency video editing applying edits scene following first enhance model anchoraware attention module batch processing consistent editing additionally integrate optical flowguided appearance propagation sliding window fashion precise frametoframe editing incorporate depthbased projection manage extensive data scene followed iterative editing achieve convergence extensively evaluate approach various scene editing instruction demonstrate achieves spatially temporally consistent editing result significantly enhanced detail sharpness prior art notably instruct general applicable monocular challenging multicamera scene code result available
uniscene unified occupancycentric driving scene generation generating highfidelity controllable annotated training data critical autonomous driving existing method typically generate single data form directly coarse scene layout fails output rich data form required diverse downstream task also struggle model direct layouttodata distribution paper introduce uniscene first unified framework generating three key data form semantic occupancy video lidar driving scene uniscene employ progressive generation process decomposes complex task scene generation two hierarchical step first generating semantic occupancy customized scene layout meta scene representation rich semantic geometric information b conditioned occupancy generating video lidar data respectively two novel transfer strategy gaussianbased joint rendering priorguided sparse modeling occupancycentric approach reduces generation burden especially intricate scene providing detailed intermediate representation subsequent generation stage extensive experiment demonstrate uniscene outperforms previous sotas occupancy video lidar generation also indeed benefit downstream driving task project page
tora trajectoryoriented diffusion transformer video generation recent advancement diffusion transformer dit demonstrated remarkable proficiency producing highquality video content nonetheless potential transformerbased diffusion model effectively generating video controllable motion remains area limited exploration paper introduces tora first trajectoryoriented dit framework concurrently integrates textual visual trajectory condition thereby enabling scalable video generation effective motion guidance specifically tora consists trajectory extractor te spatialtemporal dit motionguidance fuser mgf te encodes arbitrary trajectory hierarchical spacetime motion patch motion compression network mgf integrates motion patch dit block generate consistent video accurately follow designated trajectory design aligns seamlessly dit scalability allowing precise control video content dynamic diverse duration aspect ratio resolution extensive experiment demonstrate tora excels achieving high motion fidelity compared foundational dit model also accurately simulating complex movement physical world code made available httpsgithubcomalibabatora
photorealistic object insertion diffusionguided inverse rendering correct insertion virtual object image realworld scene requires deep understanding scene lighting geometry material well image formation process recent largescale diffusion model shown strong generative inpainting capability find current model sufficiently understand scene shown single picture generate consistent lighting effect shadow bright reflection etc preserving identity detail composited object propose using personalized large diffusion model guidance physically based inverse rendering process method recovers scene lighting tonemapping parameter allowing photorealistic composition arbitrary virtual object single frame video indoor outdoor scene physically based pipeline enables automatic material tonemapping refinement
emodiffhead continuously emotional control talking head generation via diffusion task audiodriven portrait animation involves generating talking head video using identity image audio track speech many existing approach focus lip synchronization video quality tackle challenge generating emotiondriven talking head video ability control edit emotion essential producing expressive realistic animation response challenge propose emodiffhead novel method emotional talking head video generation enables finegrained control emotion category intensity also enables oneshot generation given flame model linearity expression modeling utilize deca method extract expression vector combined audio guide diffusion model generating video precise lip synchronization rich emotional expressiveness approach enables learning rich facial information emotionirrelevant data also facilitates generation emotional video effectively overcomes limitation emotional data lack diversity facial background information address absence emotional detail emotionirrelevant data extensive experiment user study demonstrate approach achieves stateoftheart performance compared emotion portrait animation method
dollar fewstep video generation via distillation latent reward optimization diffusion probabilistic model shown significant progress video generation however computational efficiency limited large number sampling step required reducing sampling step often compromise video quality generation diversity work introduce distillation method combine variational score distillation consistency distillation achieve fewstep video generation maintaining high quality diversity also propose latent reward model finetuning approach enhance video generation performance according specified reward metric approach reduces memory usage require reward differentiable method demonstrates stateoftheart performance fewstep generation video frame fps distilled student model achieves score vbench surpassing teacher model well baseline model kling onestep distillation accelerates teacher model diffusion sampling time enabling near realtime generation human evaluation validate superior performance student model compared teacher model using ddim sampling
cinepregen camera controllable video previsualization via enginepowered diffusion advancement video generative ai model eg sora creator increasingly using technique enhance video previsualization however face challenge incomplete mismatched ai workflow existing method mainly rely text description struggle camera placement key component previsualization address issue introduce cinepregen visual previsualization system enhanced enginepowered diffusion feature novel camera storyboard interface offer dynamic control global local camera adjustment combined userfriendly ai rendering workflow aim achieve consistent result multimasked ipadapter engine simulation guideline comprehensive evaluation study demonstrate system reduces development viscosity ie complexity challenge development process meet user need extensive control iteration design process outperforms ai video production workflow cinematic camera movement shown experiment withinsubjects user study intuitive camera control realistic rendering camera motion cinepregen show great potential improving video production individual creator industry professional
learning generalizable photorealistic video diffusion propose novel framework generating video organized grid video frame time viewpoint ax grid row contains frame sharing timestep column contains frame viewpoint propose novel twostream architecture one stream performs viewpoint update column stream performs temporal update row diffusion transformer layer synchronization layer exchange information two token stream propose two implementation synchronization layer using either hard soft synchronization feedforward architecture improves upon previous work three way higher inference speed enhanced visual quality measured fvd clip videoscore improved temporal viewpoint consistency measured videoscore
technical report competition solution modelscopesora report present approach adopted modelscopesora challenge focus finetuning data video generation model challenge evaluates participant ability analyze clean generate highquality datasets videobased texttovideo task specific computational constraint provided methodology involves data processing technique video description generation filtering acceleration report outline procedure tool utilized enhance quality training data ensuring improved performance texttovideo generation model
customcrafter customized video generation preserving motion concept composition ability customized video generation aim generate highquality video guided text prompt subject reference image however since trained static image finetuning process subject learning disrupts ability video diffusion model vdms combine concept generate motion restore ability method use additional video similar prompt finetune guide model requires frequent change guiding video even retuning model generating different motion inconvenient user paper propose customcrafter novel framework preserve model motion generation conceptual combination ability without additional video finetuning recovery preserving conceptual combination ability design plugandplay module update parameter vdms enhancing model ability capture appearance detail ability concept combination new subject motion generation observed vdms tend restore motion video early stage denoising focusing recovery subject detail later stage therefore propose dynamic weighted video sampling strategy using pluggability subject learning module reduce impact module motion generation early stage denoising preserving ability generate motion vdms later stage denoising restore module repair appearance detail specified subject thereby ensuring fidelity subject appearance experimental result show method significant improvement compared previous method code available httpsgithubcomwutaocscustomcrafter
ganfusion feedforward diffusion gan space train feedforward diffusion generator human character using singleview data supervision existing generative model yet match fidelity image video generative model stateoftheart generator either trained explicit supervision thus limited volume diversity existing data meanwhile generator trained data supervision typically produce coarser result textconditioned must revert testtime optimization observe gan diffusionbased generator complementary quality gans trained efficiently supervision produce highquality object hard condition text contrast denoising diffusion model conditioned efficiently tend hard train supervision introduce ganfusion start generating unconditional triplane feature data using gan architecture trained singleview data generate random sample gan caption train textconditioned diffusion model directly learns sample space good triplane feature decoded object
simgen simulatorconditioned driving scene generation controllable synthetic data generation substantially lower annotation cost training data prior work use diffusion model generate driving image conditioned object layout however model trained smallscale datasets like nuscenes lack appearance layout diversity moreover overfitting often happens trained model generate image based layout data validation set dataset work introduce simulatorconditioned scene generation framework called simgen learn generate diverse driving scene mixing data simulator real world us novel cascade diffusion pipeline address challenging simtoreal gap multicondition conflict driving video dataset diva collected enhance generative diversity simgen contains hour realworld driving video location worldwide simulated driving data metadrive simulator simgen achieves superior generation quality diversity preserving controllability based text prompt layout pulled simulator demonstrate improvement brought simgen synthetic data augmentation bev detection segmentation task showcase capability safetycritical data generation
scalable indoor novelview synthesis using dronecaptured imagery gaussian splatting scene reconstruction novelview synthesis large complex multistory indoor scene challenging timeconsuming task prior method utilized drone data capture radiance field scene reconstruction present certain challenge first order capture diverse viewpoint drone frontfacing camera approach fly drone unstable zigzag fashion hinders dronepiloting generates motion blur captured data secondly radiance field method easily scale arbitrarily large number image paper proposes efficient scalable pipeline indoor novelview synthesis dronecaptured video using gaussian splatting camera capture wide set viewpoint allowing comprehensive scene capture simple straightforward drone trajectory scale method large scene devise divideandconquer strategy automatically split scene smaller block reconstructed individually parallel also propose coarsetofine alignment strategy seamlessly match block together compose entire scene experiment demonstrate marked improvement reconstruction quality ie psnr ssim computation time compared prior approach
tunnel tryon excavating spatialtemporal tunnel highquality virtual tryon video video tryon challenging task well tackled previous work main obstacle lie preserving detail clothing modeling coherent motion simultaneously faced difficulty address video tryon proposing diffusionbased framework named tunnel tryon core idea excavating focus tunnel input video give closeup shot around clothing region zoom region tunnel better preserve fine detail clothing generate coherent motion first leverage kalman filter construct smooth crop focus tunnel inject position embedding tunnel attention layer improve continuity generated video addition develop environment encoder extract context information outside tunnel supplementary cue equipped technique tunnel tryon keep fine detail clothing synthesizes stable smooth video demonstrating significant advancement tunnel tryon could regarded first attempt toward commerciallevel application virtual tryon video
dawn video generation preliminary exploration soralike model highquality video generation encompassing texttovideo imagetovideo videotovideo generation hold considerable significance content creation benefit anyone express inherent creativity new way world simulation modeling understanding world model like sora advanced generating video higher resolution natural motion better visionlanguage alignment increased controllability particularly long video sequence improvement driven evolution model architecture shifting unet scalable parameterrich dit model along largescale data expansion refined training strategy however despite emergence ditbased closedsource opensource model comprehensive investigation capability limitation remains lacking furthermore rapid development made challenging recent benchmark fully cover soralike model recognize significant advancement additionally evaluation metric often fail align human preference
towards realistic landmarkguided facial video inpainting based gans facial video inpainting play crucial role wide range application including limited removal obstruction video conferencing telemedicine enhancement facial expression analysis privacy protection integration graphical overlay virtual makeup domain present serious challenge due intricate nature facial feature inherent human familiarity face heightening need accurate persuasive completion addressing challenge specifically related occlusion removal context focus progressive task generating complete image facial data covered mask ensuring spatial temporal coherence study introduces network designed expressionbased video inpainting employing generative adversarial network gans handle static moving occlusion across frame utilizing facial landmark occlusionfree reference image model maintains user identity consistently across frame enhance emotional preservation customized facial expression recognition fer loss function ensuring detailed inpainted output proposed framework exhibit proficiency eliminating occlusion facial video adaptive form whether appearing static dynamic frame providing realistic coherent result
simple strong baseline sounding video generation effective adaptation audio video diffusion model joint generation work build simple strong baseline sounding video generation given base diffusion model audio video integrate additional module single model train make model jointly generate audio video enhance alignment audiovideo pair introduce two novel mechanism model first one timestep adjustment provides different timestep information base model designed align sample generated along timesteps across modality second one new design additional module termed crossmodal conditioning positional encoding cmcpe cmcpe crossmodal information embedded represents temporal position information embeddings fed model like positional encoding compared popular crossattention mechanism cmcpe provides better inductive bias temporal alignment generated data experimental result validate effectiveness two newly introduced mechanism also demonstrate method outperforms existing method
vires video instance repainting via sketch text guided generation introduce vires video instance repainting method sketch text guidance enabling video instance repainting replacement generation removal existing approach struggle temporal consistency accurate alignment provided sketch sequence vires leverage generative prior texttovideo model maintain temporal consistency produce visually pleasing result propose sequential controlnet standardized selfscaling effectively extract structure layout adaptively capture highcontrast sketch detail augment diffusion transformer backbone sketch attention interpret inject finegrained sketch semantics sketchaware encoder ensures repainted result aligned provided sketch sequence additionally contribute vireset dataset detailed annotation tailored training evaluating video instance editing method experimental result demonstrate effectiveness vires outperforms stateoftheart method visual quality temporal consistency condition alignment human rating project pagehttpssuimucgithubiosuimugithubioprojectsvires
tarsier recipe training evaluating large video description model generating finegrained video description fundamental challenge video understanding work introduce tarsier family largescale videolanguage model designed generate highquality video description tarsier employ clipvit encode frame separately us llm model temporal relationship despite simple architecture demonstrate meticulously designed twostage training procedure tarsier model exhibit substantially stronger video description capability existing opensource model showing advantage human sidebyside evaluation strongest model additionally comparable stateoftheart proprietary model advantage disadvantage gemini pro upgraded building upon siglip improves significantly advantage besides video description tarsier prof versatile generalist model achieving new stateoftheart result across nine public benchmark including multichoice vqa openended vqa zeroshot video captioning second contribution introduction new benchmark httpstarsiervlmgithubio evaluating video description model consisting new challenging dataset featuring video diverse source varying complexity along automatic method specifically designed assess quality finegrained video description make model evaluation benchmark publicly available httpsgithubcombytedancetarsier
physgame uncovering physical commonsense violation gameplay video recent advancement videobased large language model video llm witnessed emergence diverse capability reason interpret dynamic visual content among gameplay video stand distinctive data source often containing glitch defy physic commonsense characteristic render effective benchmark assessing underexplored capability physical commonsense understanding video llm paper propose physgame pioneering benchmark evaluate physical commonsense violation gameplay video physgame comprises video associated glitch spanning four fundamental domain ie mechanic kinematics optic material property across distinct physical commonsense extensively evaluating various stateoftheart video llm finding reveal performance current opensource video llm significantly lag behind proprietary counterpart bridge gap curate instruction tuning dataset physinstruct questionanswering pair facilitate physical commonsense learning addition also propose preference optimization dataset physdpo training pair dispreferred response generated conditioned misleading title ie meta information hacking fewer frame ie temporal hacking lower spatial resolution ie spatial hacking based suite datasets propose physvlm physical knowledgeenhanced video llm extensive experiment physicaloriented benchmark physgame general video understanding benchmark demonstrate stateoftheart performance physvlm
diffusion attack leveraging stable diffusion naturalistic image attacking virtual reality vr adversarial attack remains significant security threat deep learningbased method physical digital adversarial attack focus enhancing attack performance crafting adversarial example contain large printable distortion easy human observer identify however attacker rarely impose limitation naturalness comfort appearance generated attack image resulting noticeable unnatural attack address challenge propose framework incorporate style transfer craft adversarial input natural style exhibit minimal detectability maximum natural appearance maintaining superior attack capability
simvs simulating world inconsistency robust view synthesis novelview synthesis technique achieve impressive result static scene struggle faced inconsistency inherent casual capture setting varying illumination scene motion unintended effect difficult model explicitly present approach leveraging generative video model simulate inconsistency world occur capture use process along existing multiview datasets create synthetic data training multiview harmonization network able reconcile inconsistent observation consistent scene demonstrate worldsimulation strategy significantly outperforms traditional augmentation method handling realworld scene variation thereby enabling highly accurate static reconstruction presence variety challenging inconsistency project page httpsalextrevithickgithubiosimvs
star benchmark situated reasoning realworld video reasoning real world divorced situation capture present knowledge surrounding situation perform reasoning accordingly crucial challenging machine intelligence paper introduces new benchmark evaluates situated reasoning ability via situation abstraction logicgrounded question answering realworld video called situated reasoning realworld video star benchmark benchmark built upon realworld video associated human action interaction naturally dynamic compositional logical dataset includes four type question including interaction sequence prediction feasibility represent situation realworld video hypergraphs connecting extracted atomic entity relation eg action person object relationship besides visual perception situated reasoning also requires structured situation comprehension logical reasoning question answer procedurally generated answering logic question represented functional program based situation hypergraph compare various existing video reasoning model find struggle challenging situated reasoning task propose diagnostic neurosymbolic model disentangle visual perception situation abstraction language understanding functional reasoning understand challenge benchmark
aigcbench comprehensive evaluation imagetovideo content generated ai burgeoning field artificial intelligence generated content aigc witnessing rapid advancement particularly video generation paper introduces aigcbench pioneering comprehensive scalable benchmark designed evaluate variety video generation task primary focus imagetovideo generation aigcbench tackle limitation existing benchmark suffer lack diverse datasets including varied opendomain imagetext dataset evaluates different stateoftheart algorithm equivalent condition employ novel text combiner create rich text prompt used generate image via advanced texttoimage model establish unified evaluation framework video generation task benchmark includes metric spanning four dimension assess algorithm performance dimension controlvideo alignment motion effect temporal consistency video quality metric reference videodependent videofree ensuring comprehensive evaluation strategy evaluation standard proposed correlate well human judgment providing insight strength weakness current algorithm finding extensive experiment aim stimulate research development field aigcbench represents significant step toward creating standardized benchmark broader aigc landscape proposing adaptable equitable framework future assessment video generation task opensourced dataset evaluation code project website httpswwwbenchcouncilorgaigcbench
distinguish fake video unleashing power largescale data motion feature development aigenerated content aigc empowered creation remarkably realistic aigenerated video involving sora however widespread adoption model raise concern regarding potential misuse including face video scam copyright dispute addressing concern requires development robust tool capable accurately determining video authenticity main challenge lie dataset neural classifier training current datasets lack varied comprehensive repository real generated content effective discrimination paper first introduce extensive video dataset designed specifically aigenerated video detection genviddet includes instance real generated video varying category frame per second resolution length comprehensiveness genviddet enables training generalizable video detector also present dualbranch transformer innovative effective method distinguishing real generated video enhanced incorporating motion information alongside visual appearance utilizes dualbranch architecture adaptively leverage fuse raw spatiotemporal data optical flow systematically explore critical factor affecting detection performance achieving optimal configuration trained genviddet distinguish real generated video content accuracy strong generalization capability even unseen type
vidmusician videotomusic generation semanticrhythmic alignment via hierarchical visual feature videotomusic generation present significant potential video production requiring generated music semantically rhythmically aligned video achieving alignment demand advanced music generation capability sophisticated video understanding efficient mechanism learn correspondence two modality paper propose vidmusician parameterefficient videotomusic generation framework built upon texttomusic model vidmusician leverage hierarchical visual feature ensure semantic rhythmic alignment video music specifically approach utilizes global visual feature semantic condition local visual feature rhythmic cue feature integrated generative backbone via crossattention inattention mechanism respectively twostage training process incrementally incorporate semantic rhythmic feature utilizing zero initialization identity initialization maintain inherent musicgenerative capability backbone additionally construct diverse videomusic dataset dvmset encompassing various scenario promo video commercial compilation experiment demonstrate vidmusician outperforms stateoftheart method across multiple evaluation metric exhibit robust performance aigenerated video sample available
motionclone trainingfree motion cloning controllable video generation motionbased controllable video generation offer potential creating captivating visual content existing method typically necessitate model training encode particular motion cue incorporate finetuning inject certain motion pattern resulting limited flexibility generalization work propose motionclone trainingfree framework enables motion cloning reference video versatile motioncontrolled video generation including texttovideo imagetovideo based observation dominant component temporalattention map drive motion synthesis rest mainly capture noisy subtle motion motionclone utilizes sparse temporal attention weight motion representation motion guidance facilitating diverse motion transfer across varying scenario meanwhile motionclone allows direct extraction motion representation single denoising step bypassing cumbersome inversion process thus promoting efficiency flexibility extensive experiment demonstrate motionclone exhibit proficiency global camera motion local object motion notable superiority term motion fidelity textual alignment temporal consistency
technical report soccernet dense video captioning task dense video captioning soccernet dataset propose generate video caption soccer action locate timestamp caption firstly apply blip video caption framework generate video caption locate timestamp using multisize sliding window temporal proposal generation proposal classification
cyberhost taming audiodriven avatar diffusion model region codebook attention diffusionbased video generation technology advanced significantly catalyzing proliferation research human animation however majority study confined samemodality driving setting crossmodality human body animation remaining relatively underexplored paper introduce endtoend audiodriven human animation framework ensures hand integrity identity consistency natural motion key design cyberhost region codebook attention mechanism improves generation quality facial hand animation integrating finegrained local feature learned motion pattern prior furthermore developed suite humanpriorguided training strategy including body movement map hand clarity score posealigned reference feature local enhancement supervision improve synthesis result knowledge cyberhost first endtoend audiodriven human diffusion model capable facilitating zeroshot video generation within scope human body extensive experiment demonstrate cyberhost surpasses previous work quantitative qualitative aspect
accelerating diffusion transformer tokenwise feature caching diffusion transformer shown significant effectiveness image video synthesis expense huge computation cost address problem feature caching method introduced accelerate diffusion transformer caching feature previous timesteps reusing following timesteps however previous caching method ignore different token exhibit different sensitivity feature caching feature caching token may lead destruction overall generation quality compared token paper introduce tokenwise feature caching allowing u adaptively select suitable token caching enable u apply different caching ratio neural layer different type depth extensive experiment pixartalpha opensora dit demonstrate effectiveness image video generation requirement training instance acceleration achieved opensora pixartalpha almost drop generation quality
smoothcache universal inference acceleration technique diffusion transformer diffusion transformer dit emerged powerful generative model various task including image video speech synthesis however inference process remains computationally expensive due repeated evaluation resourceintensive attention feedforward module address introduce smoothcache modelagnostic inference acceleration technique dit architecture smoothcache leverage observed high similarity layer output across adjacent diffusion timesteps analyzing layerwise representation error small calibration set smoothcache adaptively cache reuses key feature inference experiment demonstrate smoothcache achieves speed maintaining even improving generation quality across diverse modality showcase effectiveness ditxl image generation opensora texttovideo stable audio open texttoaudio highlighting potential enable realtime application broaden accessibility powerful dit model
importancebased token merging diffusion model diffusion model excel highquality image video generation however major drawback high latency simple yet powerful way speed merging similar token faster computation though result quality loss paper demonstrate preserving important token merging significantly improves sample quality notably importance token reliably determined using classifierfree guidance magnitude measure strongly correlated conditioning input corresponds output fidelity since classifierfree guidance incurs additional computational cost requires extra module method easily integrated diffusionbased framework experiment show approach significantly outperforms baseline across various application including texttoimage synthesis multiview image generation video generation
highquality video generation event camera via theoryinspired modelaided deep learning bioinspired event camera dynamic vision sensor capable asynchronously capturing perpixel brightness change called eventstreams high temporal resolution high dynamic range however nonstructural spatialtemporal eventstreams make challenging providing intuitive visualization rich semantic information human vision call eventstovideo solution take eventstreams input generate high quality video frame intuitive visualization however current solution predominantly datadriven without considering prior knowledge underlying statistic relating eventstreams video frame highly relies nonlinearity generalization capability deep neural network thus struggling reconstructing detailed texture scene complex work propose novel paradigm designed produce highquality video frame event approach leverage modelaided deep learning framework underpinned theoryinspired model meticulously derived fundamental imaging principle event camera deal issue statereset recurrent component also design temporal shift embedding module improve quality video frame comprehensive evaluation real world event camera datasets validate approach notably outperforming stateoftheart approach eg surpassing second best evaluation metric
domainadaptive video deblurring via testtime blurring dynamic scene video deblurring aim remove undesirable blurry artifact captured exposure process although previous video deblurring method achieved impressive result suffer significant performance drop due domain gap training testing video especially captured realworld scenario address issue propose domain adaptation scheme based blurring model achieve testtime finetuning deblurring model unseen domain since blurred sharp pair unavailable finetuning inference scheme generate domainadaptive training pair calibrate deblurring model target domain first relative sharpness detection module proposed identify relatively sharp region blurry input image regard pseudosharp image next utilize blurring model produce blurred image based pseudosharp image extracted testing synthesize blurred image compliance target data distribution propose domainadaptive blur condition generation module create domainspecific blur condition blurring model finally generated pseudosharp blurred pair used finetune deblurring model better performance extensive experimental result demonstrate approach significantly improve stateoftheart video deblurring method providing performance gain various realworld video deblurring datasets source code available httpsgithubcomjintinghedadeblur
irag advancing rag video incremental approach retrievalaugmented generation rag system combine strength language generation information retrieval power many realworld application like chatbots use rag understanding video appealing two critical limitation onetime upfront conversion content large corpus video text description entail high processing time also information rich video data typically captured text description since user query known apriori developing system video text conversion interactive querying video data challenging address limitation propose incremental rag system called irag augments rag novel incremental workflow enable interactive querying large corpus video unlike traditional rag irag quickly index large repository video incremental workflow us index opportunistically extract detail select portion video retrieve context relevant interactive user query incremental workflow avoids long video text conversion time overcomes information loss issue due conversion video text ondemand queryspecific extraction detail video data ensures high quality response interactive user query often known apriori best knowledge irag first system augment rag incremental workflow support efficient interactive querying large corpus video experimental result realworld datasets demonstrate faster video text ingestion ensuring latency quality response interactive user query comparable response traditional rag video data converted text upfront user querying
dance beat blending beat visuals dance video generation generating dance music crucial advancing automated choreography current method typically produce skeleton keypoint sequence instead dance video lack capability make specific individual dance reduces realworld applicability method also require precise keypoint annotation complicating data collection limiting use selfcollected video datasets overcome challenge introduce novel task generating dance video directly image individual guided music task enables dance generation specific individual without requiring keypoint annotation making versatile applicable various situation solution dance beat diffusion model dabfusion utilizes reference image music piece generate dance video featuring various dance type choreography music analyzed specially designed music encoder identifies essential feature including dance style movement rhythm dabfusion excels generating dance video individual training dataset also previously unseen person versatility stem approach generating latent optical flow contains necessary motion information animate person image evaluate dabfusions performance using aist dataset focusing video quality audiovideo synchronization motionmusic alignment propose motionmusic alignment score align build beat alignment score effectively evaluate motionmusic alignment new task experiment show dabfusion establishes solid baseline innovative task video result found project page httpsdabfusiongithubio
stiv scalable text image conditioned video generation field video generation made remarkable advancement yet remains pressing need clear systematic recipe guide development robust scalable model work present comprehensive study systematically explores interplay model architecture training recipe data curation strategy culminating simple scalable textimageconditioned video generation method named stiv framework integrates image condition diffusion transformer dit frame replacement incorporating text conditioning via joint imagetext conditional classifierfree guidance design enables stiv perform texttovideo textimagetovideo task simultaneously additionally stiv easily extended various application video prediction frame interpolation multiview generation long video generation etc comprehensive ablation study stiv demonstrate strong performance despite simple design model resolution achieves vbench surpassing leading open closedsource model like pika kling samesized model also achieves stateoftheart result vbench task resolution providing transparent extensible recipe building cuttingedge video generation model aim empower future research accelerate progress toward versatile reliable video generation solution
latentartifusion effective efficient histological artifact restoration framework histological artifact pose challenge pathologist computeraided diagnosis cad system leading error analysis current approach histological artifact restoration based generative adversarial network gans pixellevel diffusion model suffer performance limitation computational inefficiency paper propose novel framework latentartifusion leverage latent diffusion model ldm reconstruct histological artifact high performance computational efficiency unlike traditional pixellevel diffusion framework latentartifusion executes restoration process lowerdimensional latent space significantly improving computational efficiency moreover introduce novel regional artifact reconstruction algorithm latent space prevent mistransfer nonartifact region distinguishing approach ganbased method extensive experiment realworld histology datasets latentartifusion demonstrates remarkable speed outperforming stateoftheart pixellevel diffusion framework also consistently surpasses ganbased method least across multiple evaluation metric furthermore evaluate effectiveness proposed framework downstream tissue classification task showcasing practical utility code available httpsgithubcombugscreatorlatentartifusion
unveiling contextrelated anomaly knowledge graph empowered decoupling scene action humanrelated video anomaly detection detecting anomaly humanrelated video crucial surveillance application current method primarily include appearancebased actionbased technique appearancebased method rely lowlevel visual feature color texture shape learn large number pixel pattern feature related known scene training making effective detecting anomaly within familiar context however encountering new significantly changed scene ie unknown scene often fail existing sota method effectively capture relationship action surrounding scene resulting low generalization contrast actionbased method focus detecting anomaly human action usually less informative tend overlook relationship action scene leading incorrect detection instance normal event running beach abnormal event running street might considered normal due lack scene information short current method struggle integrate lowlevel visual highlevel action feature leading poor anomaly detection varied complex scene address challenge propose novel decouplingbased architecture humanrelated video anomaly detection decoad decoad significantly improves integration visual action feature decoupling interweaving scene action thereby enabling intuitive accurate understanding complex behavior scene decoad support fully supervised weakly supervised unsupervised setting
survey visual signal coding processing generative model technology standard optimization paper provides survey latest development visual signal coding processing generative model specifically focus presenting advancement generative model influence research domain visual signal coding processing survey study begin brief introduction wellestablished generative model including variational autoencoder vae model generative adversarial network gan model autoregressive ar model normalizing flow diffusion model subsequent section paper explores advancement visual signal coding based generative model well ongoing international standardization activity realm visual signal processing focus lie application development various generative model research visual signal restoration also present latest development generative visual signal synthesis editing along visual signal quality assessment using generative model quality assessment generative model practical implementation study closely linked investigation fast optimization paper additionally present latest advancement fast optimization visual signal coding processing generative model hope advance field providing researcher practitioner comprehensive literature review topic visual signal coding processing generative model
explorative inbetweening time space introduce bounded generation generalized task control video generation synthesize arbitrary camera subject motion based given start end frame objective fully leverage inherent generalization capability imagetovideo model without additional training finetuning original model achieved proposed new sampling strategy call time reversal fusion fuse temporally forward backward denoising path conditioned start end frame respectively fused path result video smoothly connects two frame generating inbetweening faithful subject motion novel view static scene seamless video looping two bounding frame identical curate diverse evaluation dataset image pair compare closest existing method find time reversal fusion outperforms related work subtasks exhibiting ability generate complex motion view guided bounded frame see project page httpstimereversalgithubio
texttoaudio generation synchronized video recent time focus texttoaudio tta generation intensified researcher strive synthesize audio textual description however existing method though leveraging latent diffusion model learn correlation audio text embeddings fall short come maintaining seamless synchronization produced audio video often result discernible audiovisual mismatch bridge gap introduce groundbreaking benchmark texttoaudio generation aligns video named benchmark distinguishes three novel metric dedicated evaluating visual alignment temporal consistency complement also present simple yet effective videoaligned tta generation model namely moving beyond traditional method refines latent diffusion approach integrating visualaligned text embeddings conditional foundation employ temporal multihead attention transformer extract understand temporal nuance video data feat amplified audiovisual controlnet adeptly merges temporal visual representation text embeddings enhancing integration weave contrastive learning objective designed ensure visualaligned text embeddings resonate closely audio feature extensive evaluation audiocaps demonstrate set new standard videoaligned tta generation ensuring visual alignment temporal consistency
cospeech gesture video generation via motiondecoupled diffusion model cospeech gesture presented lively form video achieve superior visual effect humanmachine interaction previous work mostly generate structural human skeleton resulting omission appearance information focus direct generation audiodriven cospeech gesture video work two main challenge suitable motion feature needed describe complex human movement crucial appearance information gesture speech exhibit inherent dependency temporally aligned even arbitrary length solve problem present novel motiondecoupled framework generate cospeech gesture video specifically first introduce welldesigned nonlinear tps transformation obtain latent motion feature preserving essential appearance information transformerbased diffusion model proposed learn temporal correlation gesture speech performs generation latent motion space followed optimal motion selection module produce longterm coherent consistent gesture video better visual perception design refinement network focusing missing detail certain area extensive experimental result show proposed framework significantly outperforms existing approach motion videorelated evaluation code demo resource available
controllable generation anyview rendering street scene controllable generative model image video achieved remarkable success highquality model scene particularly unbounded scenario like autonomous driving remain underdeveloped due high data acquisition cost paper introduce novel pipeline controllable street scene generation support multicondition control including bev map object text description unlike previous method reconstruct training generative model first train video generation model reconstructs generated data innovative approach enables easily controllable generation static scene acquisition resulting highquality scene reconstruction address minor error generated content propose deformable gaussian splatting monocular depth initialization appearance modeling manage exposure discrepancy across viewpoint validated nuscenes dataset generates diverse highquality driving scene support anyview rendering enhance downstream task like bev segmentation result demonstrate framework superior performance showcasing potential autonomous driving simulation beyond
sugar subjectdriven video customization zeroshot manner present sugar zeroshot method subjectdriven video customization given input image sugar capable generating video subject contained image aligning generation arbitrary visual attribute style motion specified userinput text unlike previous method require testtime finetuning fail generate textaligned video sugar achieves superior result without need extra cost testtime enable zeroshot capability introduce scalable pipeline construct synthetic dataset specifically designed subjectdriven customization leading million imagevideotext triplet additionally propose several method enhance model including special attention design improved training strategy refined sampling algorithm extensive experiment conducted compared previous method sugar achieves stateoftheart result identity preservation video dynamic videotext alignment subjectdriven video customization demonstrating effectiveness proposed method
one image reeditable dynamic model video generation one image editable dynamic model video generation novel direction change research area single image representation reconstruction image gaussian splatting demonstrated advantage implicit reconstruction compared original neural radiance field rapid development technology principle people tried used stable diffusion model generate targeted model text instruction however using normal implicit machine learning method hard gain precise motion action control difficult generate long content semantic continuous video address issue propose method theory used one single image generate editable model generate targeted semantic continuous timeunlimited video used normal basic gaussian splatting model generate model single image requires less volume video memory computer calculation ability subsequently designed automatic generation selfadaptive binding mechanism object armature combined reeditable motion action analyzing controlling algorithm proposed achieve better performance sota project area building model precise motion action control generating stable semantic continuous timeunlimited video input text instruction analyze detailed implementation method theory analysis relative comparison conclusion presented project code open source
trainingfree adaptive diffusion bounded difference approximation strategy diffusion model recently achieved great success synthesis highquality image video however existing denoising technique diffusion model commonly based stepbystep noise prediction suffers high computation cost resulting prohibitive latency interactive application paper propose adaptivediffusion relieve bottleneck adaptively reducing noise prediction step denoising process method considers potential skipping many noise prediction step possible keeping final denoised result identical original fullstep one specifically skipping strategy guided thirdorder latent difference indicates stability timesteps denoising process benefit reusing previous noise prediction result extensive experiment image video diffusion model demonstrate method significantly speed denoising process generating identical result original process achieving average speedup without quality degradation
neural field tracking anatomy surgical instrument monocular laparoscopic video clip laparoscopic video tracking primarily focus two target type surgical instrument anatomy former could used skill assessment latter necessary projection virtual overlay instrument anatomy tracking often considered two separate problem paper propose method joint tracking structure simultaneously based single monocular video clip train neural field represent continuous spatiotemporal scene used create track surface visible least one frame due small size instrument generally cover small part image resulting decreased tracking accuracy therefore propose enhanced class weighting improve instrument track evaluate tracking video clip laparoscopic cholecystectomy find mean tracking accuracy anatomical structure instrument additionally assess quality depth map obtained method scene reconstruction show pseudodepths comparable quality stateoftheart pretrained depth estimator laparoscopic video scared dataset method predicts depth mae mm relative error result show feasibility using neural field monocular reconstruction laparoscopic scene
selfsupervised monocular scene reconstruction egocentric video egocentric video provide valuable insight human interaction physical world sparked growing interest computer vision robotics community critical challenge fully understanding geometry dynamic egocentric video dense scene reconstruction however lack highquality labeled datasets field hindered effectiveness current supervised learning method work aim address issue exploring selfsupervised dynamic scene reconstruction approach introduce novel model unifies estimation multiple variable necessary egocentric monocular reconstruction including camera intrinsic camera pose video depth within fast feedforward framework starting pretrained singleframe depth intrinsic estimation model extend camera pose estimation align multiframe result largescale unlabeled egocentric video evaluate indomain zeroshot generalization setting achieving superior performance dense pointclouds sequence reconstruction compared baseline represents first attempt apply selfsupervised learning pointclouds sequence reconstruction labelscarce egocentric field enabling fast dense generalizable reconstruction interactable visualization code trained model released
videoshop localized semantic video editing noiseextrapolated diffusion inversion introduce videoshop trainingfree video editing algorithm localized semantic edits videoshop allows user use editing software including photoshop generative inpainting modify first frame automatically propagates change semantic spatial temporally consistent motion remaining frame unlike existing method enable edits imprecise textual instruction videoshop allows user add remove object semantically change object insert stock photo video etc finegrained control location appearance achieve imagebased video editing inverting latents noise extrapolation generate video conditioned edited image videoshop produce higher quality edits baseline editing benchmark using evaluation metric
genmm geometrically temporally consistent multimodal data generation video lidar multimodal synthetic data generation crucial domain autonomous driving robotics augmentedvirtual reality retail propose novel approach genmm jointly editing rgb video lidar scan inserting temporally geometrically consistent object method us reference image bounding box seamlessly insert blend new object target video inpaint region interest consistent box using diffusionbased video inpainting model compute semantic boundary object estimate surface depth using stateoftheart semantic segmentation monocular depth estimation technique subsequently employ geometrybased optimization algorithm recover shape object surface ensuring fit precisely within bounding box finally lidar ray intersecting new object surface updated reflect consistent depth geometry experiment demonstrate effectiveness genmm inserting various object across video lidar modality
ffa sora video generation fundus fluorescein angiography simulator fundus fluorescein angiography ffa critical diagnosing retinal vascular disease beginner often struggle image interpretation study develops ffa sora texttovideo model convert ffa report dynamic video via waveletflow variational autoencoder wfvae diffusion transformer dit trained anonymized dataset ffa sora accurately simulates disease feature input text confirmed objective metric frechet video distance fvd learned perceptual image patch similarity lpips visualquestionanswering score vqascore specific evaluation showed acceptable alignment generated video textual prompt bertscore additionally model demonstrated strong privacypreserving performance retrieval evaluation achieving average recallk human assessment indicated satisfactory visual quality average score best worst model address privacy concern associated sharing largescale ffa data enhances medical education
kvq kwai video quality assessment shortform video shortform ugc video platform like kwai tiktok emerging irreplaceable mainstream medium form thriving userfriendly engagement kaleidoscope creation etc however advancing contentgeneration mode eg special effect sophisticated processing workflow eg deartifacts introduced significant challenge recent ugc video quality assessment ambiguous content hinder identification qualitydetermined region ii diverse complicated hybrid distortion hard distinguish tackle challenge assist development shortform video establish first largescale kaleidoscope short video database quality assessment termed kvq comprises useruploaded short video processed video diverse practical processing workflow including preprocessing transcoding enhancement among absolute quality score video partial ranking score among indistinguishable sample provided team professional researcher specializing image processing based database propose first shortform video quality evaluator ie ksvqe enables quality evaluator identify qualitydetermined semantics content understanding large vision language model ie clip distinguish distortion distortion understanding module experimental result shown effectiveness ksvqe kvq database popular vqa database
videoprism foundational visual encoder video understanding introduce videoprism generalpurpose video encoder tackle diverse video understanding task single frozen model pretrain videoprism heterogeneous corpus containing highquality videocaption pair video clip noisy parallel text eg asr transcript pretraining approach improves upon masked autoencoding globallocal distillation semantic video embeddings token shuffling scheme enabling videoprism focus primarily video modality leveraging invaluable text associated video extensively test videoprism four broad group video understanding task web video question answering cv science achieving stateoftheart performance video understanding benchmark
improving video corpus moment retrieval partial relevance enhancement video corpus moment retrieval vcmr new video retrieval task aimed retrieving relevant moment large corpus untrimmed video using text query relevance video query partial mainly evident two untrimmed video contains many frame relevant query strong relevance typically observed within relevant relevance query varies different modality action description align visual element character conversation related textual informationexisting method often treat video content equally leading suboptimal moment retrieval argue effectively capturing partial relevance query video essential vcmr task end propose partial relevance enhanced modelprem improve vcmr vcmr involves two subtasks video retrieval moment localization align distinct objective implement specialized partial relevance enhancement strategy video retrieval introduce multimodal collaborative video retriever generating different query representation two modality modalityspecific pooling ensuring effective match moment localization propose focusthenfuse moment localizer utilizing modalityspecific gate capture essential content also introduce relevant contentenhanced training method retriever localizer enhance ability model capture relevant content experimental result tvr didemo datasets show proposed model outperforms baseline achieving new stateoftheart vcmr code available
short v regular video youtube comparative analysis user engagement content creation trend youtube introduced short video format allowing user upload short video prominently displayed website app despite large visual footprint study date looked impact short introduction production consumption content youtube paper present first comparative analysis youtube short versus regular video respect user engagement ie view like comment content creation frequency video category collected dataset containing information channel posted least one short analyzed metadata video short regular video uploaded january december spanning twoyear period including introduction short longitudinal analysis show content creator consistently increased frequency short production period especially newlycreated channel surpassed regular video also observe short target mostly entertainment category regular video cover wide variety category general short attract view like per view regular video attract less comment per view however short outperform regular video education political category much category study contributes understanding social medium dynamic quantifying spread shortform content motivating future research impact society
tempcompass video llm really understand video recently surge interest surrounding video large language model video llm however existing benchmark fail provide comprehensive feedback temporal perception ability video llm one hand unable distinguish different temporal aspect eg speed direction thus reflect nuanced performance specific aspect hand limited diversity task format eg multichoice qa hinders understanding temporal perception performance may vary across different type task motivated two problem propose textbftempcompass benchmark introduces diversity temporal aspect task format collect highquality test data devise two novel strategy video collection construct conflicting video share static content differ specific temporal aspect prevents video llm leveraging singleframe bias language prior collect task instruction propose paradigm human first annotate metainformation video llm generates instruction also design llmbased approach automatically accurately evaluate response video llm based tempcompass comprehensively evaluate stateoftheart sota video llm image llm reveal discerning fact model exhibit notably poor temporal perception ability data available
koala key frameconditioned long videollm long video question answering challenging task involves recognizing shortterm activity reasoning finegrained relationship stateoftheart video large language model vllms hold promise viable solution due demonstrated emergent capability new task however despite trained million short secondslong video vllms unable understand minuteslong video accurately answer question address limitation propose lightweight selfsupervised approach key frameconditioned long videollm koala introduces learnable spatiotemporal query adapt pretrained vllms generalizing longer video approach introduces two new tokenizers condition visual token computed sparse video key frame understanding short long video moment train proposed approach demonstrate effectiveness zeroshot long video understanding benchmark outperforms stateoftheart large model absolute accuracy across task surprisingly also empirically show approach help pretrained vllm understand long video also improves accuracy shortterm action recognition
objectattributerelation representation based video semantic communication rapid growth multimedia data volume increasing need efficient video transmission application virtual reality future video streaming service semantic communication emerging vital technique ensuring efficient reliable transmission lowbandwidth highnoise setting however current approach focus joint sourcechannel coding jscc depends endtoend training method often lack interpretable semantic representation struggle adaptability various downstream task paper introduce use objectattributerelation oar semantic framework video facilitate low bitrate coding enhance jscc process effective video transmission utilize oar sequence low bitrate representation generative video reconstruction additionally incorporate oar image jscc model prioritize communication resource area critical downstream task experiment traffic surveillance video datasets assess effectiveness approach term video transmission performance empirical finding demonstrate oarbased video coding method outperforms coding lower bitrates also synergizes jscc deliver robust efficient video transmission
standard compliant video coding using low complexity switchable neural wrapper proliferation high resolution video post great storage bandwidth pressure cloud video service driving development nextgeneration video codecs despite great progress made neural video coding existing approach still far economical deployment considering complexity ratedistortion performance tradeoff clear roadblock neural video coding paper propose new framework featuring standard compatibility high performance low decoding complexity employ set jointly optimized neural pre postprocessors wrapping standard video codec encode video different resolution ratedistorion optimal downsampling ratio signaled decoder persequence level target rate design low complexity neural postprocessor architecture handle different upsampling ratio change resolution exploit spatial redundancy highresolution video neural wrapper achieves ratedistortion performance improvement endtoend optimization codec proxy lightweight postprocessor architecture complexity mac pixel achieves bdrate reduction vvc uvg dataset aom ctc class approach potential advance performance latest video coding standard using neural processing minimal added complexity
video dataflywheel resolving impossible data trinity videolanguage understanding recently videolanguage understanding achieved great success largescale pretraining however data scarcity remains prevailing challenge study quantitatively reveals impossible trinity among data quantity diversity quality pretraining datasets recent effort seek refine largescale diverse asr datasets compromised low quality synthetic annotation method successfully leverage useful information multimodal video content frame tag asr transcript etc refine original annotation nevertheless struggle mitigate noise within synthetic annotation lack scalability dataset size expands address issue introduce video dataflywheel framework iteratively refines video annotation improved noise control method iterative refinement first leverage videolanguage model generate synthetic annotation resulting refined dataset pretrain finetune human refinement example stronger model process repeated continuous improvement noise control present adatailr novel noise control method requires weaker assumption noise distribution thereby proving effective large datasets theoretical guarantee combination iterative refinement adatailr achieve better scalability videolanguage understanding extensive experiment show framework outperforms existing data refinement baseline delivering performance boost improving dataset quality minimal diversity loss furthermore refined dataset facilitates significant improvement various videolanguage understanding task including video question answering textvideo retrieval
trace temporal grounding video llm via causal event modeling video temporal grounding vtg crucial capability video understanding model play vital role downstream task video browsing editing effectively handle various task simultaneously enable zeroshot prediction growing trend employing video llm vtg task however current video llmbased method rely exclusively natural language generation lacking ability model clear structure inherent video restricts effectiveness tackling vtg task address issue paper first formally introduces causal event modeling framework represents video llm output sequence event predict current event using previous event video input textural instruction event consists three component timestamps salient score textual caption propose novel taskinterleaved video llm called trace effectively implement causal event modeling framework practice trace process visual frame timestamps salient score text distinct task employing various encoders decoding head task token arranged interleaved sequence according causal event modeling framework formulation extensive experiment various vtg task datasets demonstrate superior performance trace compared stateoftheart video llm model code available httpsgithubcomgyxxygtrace
pvvtt privacycentric dataset missionspecific anomaly detection natural language interpretation video crime detection significant application computer vision artificial intelligence however existing datasets primarily focus detecting severe crime analyzing entire video clip often neglecting precursor activity ie privacy violation could potentially prevent crime address limitation present pvvtt privacy violation video text unique multimodal dataset aimed identifying privacy violation pvvtt provides detailed annotation video text scenario ensure privacy individual video provide video feature vector avoiding release raw video data privacyfocused approach allows researcher use dataset protecting participant confidentiality recognizing privacy violation often ambiguous contextdependent propose graph neural network gnnbased video description model model generates gnnbased prompt image large language model llm deliver costeffective highquality video description leveraging single video frame along relevant text method reduces number input token required maintaining descriptive quality optimizing llm apiusage extensive experiment validate effectiveness interpretability approach video description task flexibility pvvtt dataset
large motion video autoencoding crossmodal video vae learning robust video variational autoencoder vae essential reducing video redundancy facilitating efficient video generation directly applying image vaes individual frame isolation result temporal inconsistency suboptimal compression rate due lack temporal compression existing video vaes begun address temporal compression however often suffer inadequate reconstruction performance paper present novel powerful video autoencoder capable highfidelity video encoding first observe entangling spatial temporal compression merely extending image vae vae introduce motion blur detail distortion artifact thus propose temporalaware spatial compression better encode decode spatial information additionally integrate lightweight motion compression model temporal compression second propose leverage textual information inherent texttovideo datasets incorporate text guidance model significantly enhances reconstruction quality particularly term detail preservation temporal stability third improve versatility model joint training image video enhances reconstruction quality also enables model perform image video autoencoding extensive evaluation strong recent baseline demonstrate superior performance method project website found
megactor harness power raw video vivid portrait animation despite raw driving video contain richer information facial expression intermediate representation landmark field portrait animation seldom subject research due two challenge inherent portrait animation driven raw video significant identity leakage irrelevant background facial detail wrinkle degrade performance harness power raw video vivid portrait animation proposed pioneering conditional diffusion model named megactor first introduced synthetic data generation framework creating video consistent motion expression inconsistent id mitigate issue id leakage second segmented foreground background reference image employed clip encode background detail encoded information integrated network via text embedding module thereby ensuring stability background finally style transfer appearance reference image driving video eliminate influence facial detail driving video final model trained solely public datasets achieving result comparable commercial model hope help opensource communitythe code available httpsgithubcommegviiresearchmegfaceanimate
grounded video caption generation propose new task dataset model grounded video caption generation task unifies captioning object grounding video object caption grounded video via temporally consistent bounding box introduce following contribution first present task definition manually annotated test dataset task referred grounded video caption generation groc second introduce largescale automatic annotation method leveraging existing model grounded still image captioning together llm summarising framelevel caption temporally consistent caption video furthermore prompt llm track language classifying noun phrase framelevel caption noun phrase videolevel generated caption apply approach video dataset result new largescale training dataset called howtoground automatically annotated caption spatiotemporally consistent bounding box coherent natural language label third introduce new grounded video caption generation model called videoground train model new automatically annotated howtoground dataset finally result videoground model set state art new task grounded video caption generation perform extensive ablation demonstrate importance key technical contribution model
video new language realworld decision making text video data abundant internet support largescale selfsupervised learning next token frame prediction however equally leveraged language model significant realworld impact whereas video generation remained largely limited medium entertainment yet video data capture important information physical world difficult express language address gap discus underappreciated opportunity extend video generation solve task real world observe akin language video serve unified interface absorb internet knowledge represent diverse task moreover demonstrate like language model video generation serve planner agent compute engine environment simulator technique incontext learning planning reinforcement learning identify major impact opportunity domain robotics selfdriving science supported recent work demonstrates advanced capability video generation plausibly within reach lastly identify key challenge video generation mitigate progress addressing challenge enable video generation model demonstrate unique value alongside language model wider array ai application
survey aigenerated video evaluation growing capability ai generating video content brought forward significant challenge effectively evaluating video unlike static image text video content involves complex spatial temporal dynamic may require comprehensive systematic evaluation content aspect like video presentation quality semantic information delivery alignment human intention virtualreality consistency physical world survey identifies emerging field aigenerated video evaluation aigve highlighting importance assessing well aigenerated video align human perception meet specific instruction provide structured analysis existing methodology could potentially used evaluate aigenerated video outlining strength gap current approach advocate development robust nuanced evaluation framework handle complexity video content include conventional metricbased evaluation also current humaninvolved evaluation future modelcentered evaluation survey aim establish foundational knowledge base researcher academia practitioner industry facilitating future advancement evaluation method aigenerated video content
tsvg textdriven stereoscopic video generation advent stereoscopic video opened new horizon multimedia particularly extended reality xr virtual reality vr application immersive content captivates audience across various platform despite growing popularity producing stereoscopic video remains challenging due technical complexity involved generating stereo parallax refers positional difference object viewed two distinct perspective crucial creating depth perception complex process pose significant challenge creator aiming deliver convincing engaging presentation address challenge paper introduces textdriven stereoscopic video generation tsvg system innovative modelagnostic zeroshot approach streamlines video generation using text prompt create reference video video transformed point cloud sequence rendered two perspective subtle parallax difference achieving natural stereoscopic effect tsvg represents significant advancement stereoscopic content creation integrating stateoftheart trainingfree technique texttovideo generation depth estimation video inpainting flexible architecture ensures high efficiency userfriendliness allowing seamless update newer model without retraining simplifying production pipeline tsvg make stereoscopic video generation accessible broader audience demonstrating potential revolutionize field
textvideo retrieval via variational multimodal hypergraph network textvideo retrieval challenging task aim identify relevant video given textual query compared conventional textual retrieval main obstacle textvideo retrieval semantic gap textual nature query visual richness video content previous work primarily focus aligning query video finely aggregating wordframe matching signal inspired human cognitive process modularly judging relevance text video judgment need highorder matching signal due consecutive complex nature video content paper propose chunklevel textvideo matching query chunk extracted describe specific retrieval unit video chunk segmented distinct clip video formulate chunklevel matching nary correlation modeling word query frame video introduce multimodal hypergraph nary correlation modeling representing textual unit video frame node using hyperedges depict relationship multimodal hypergraph constructed way query video aligned highorder semantic space addition enhance model generalization ability extracted feature fed variational inference component computation obtaining variational representation gaussian distribution incorporation hypergraphs variational inference allows model capture complex nary interaction among textual visual content experimental result demonstrate proposed method achieves stateoftheart performance textvideo retrieval task
trusted video inpainting localization via deep attentive noise learning digital video inpainting technique substantially improved deep learning recent year although inpainting originally designed repair damaged area also used malicious manipulation remove important object creating false scene fact significant identify inpainted region blindly paper present trusted video inpainting localization network truvil excellent robustness generalization ability observing highfrequency noise effectively unveil inpainted region design deep attentive noise learning multiple stage capture inpainting trace firstly multiscale noise extraction module based high pas layer used create noise modality input rgb frame correlation two complementary modality explored crossmodality attentive fusion module facilitate mutual feature learning lastly spatial detail selectively enhanced attentive noise decoding module boost localization performance network prepare enough training sample also build framelevel video object segmentation dataset video pixellevel annotation frame extensive experimental result validate superiority truvil compared stateofthearts particular quantitative qualitative evaluation various inpainted video verify remarkable robustness generalization ability proposed truvil code dataset available httpsgithubcommultimediafortruvil
adult learner recall recognition performance affective feedback learning aigenerated synthetic video widespread use generative ai led multiple application aigenerated text medium potentially enhance learning outcome however limited number welldesigned experimental study investigating impact learning gain affective feedback aigenerated medium compared traditional medium eg text document human recording video current study recruited participant investigate adult learner recall recognition performance well affective feedback aigenerated synthetic video using mixedmethods approach preand posttest design specifically four learning condition aigenerated framing human instructorgenerated text aigenerated synthetic video human instructorgenerated text human instructorgenerated video human instructorgenerated text frame baseline considered result indicated statistically significant difference amongst condition recall recognition performance addition participant affective feedback statistically significantly different two video condition however adult learner preferred learn video format rather text material
multimodal emotion recognition visionlanguage prompting modality dropout paper present solution second multimodal emotion recognition challenge track enhance accuracy generalization performance emotion recognition propose several method multimodal emotion recognition firstly introduce emovclip model finetuned based clip using visionlanguage prompt learning designed videobased emotion recognition task leveraging prompt learning clip emovclip improves performance pretrained clip emotional video additionally address issue modality dependence multimodal fusion employ modality dropout robust information fusion furthermore aid baichuan better extracting emotional information suggest using prompt baichuan lastly utilize selftraining strategy leverage unlabeled video process use unlabeled video highconfidence pseudolabels generated model incorporate training set experimental result demonstrate model rank track achieving accuracy test set
rhythmic foley framework seamless audiovisual alignment videotoaudio synthesis research introduces innovative framework videotoaudio synthesis solves problem audiovideo desynchronization semantic loss audio incorporating semantic alignment adapter temporal synchronization adapter method significantly improves semantic integrity precision beat point synchronization particularly fastpaced action sequence utilizing contrastive audiovisual pretrained encoder model trained video highquality audio data improving quality generated audio dualadapter approach empowers user enhanced control audio semantics beat effect allowing adjustment controller achieve better result extensive experiment substantiate effectiveness framework achieving seamless audiovisual alignment
synthetic thermal rgb video automatic pain assessment utilizing visionmlp architecture pain assessment essential developing optimal pain management protocol alleviate suffering prevent functional decline patient consequently reliable accurate automatic pain assessment system essential continuous effective patient monitoring study present synthetic thermal video generated generative adversarial network integrated pain recognition pipeline evaluates efficacy framework consisting visionmlp transformerbased module utilized employing rgb synthetic thermal video unimodal multimodal setting experiment conducted facial video biovid database demonstrate effectiveness synthetic thermal video underline potential advantage
vimts unified video image text spotter enhancing crossdomain generalization text spotting task involving extraction textual information image video sequence face challenge crossdomain adaption imagetoimage imagetovideo generalization paper introduce new method termed vimts enhances generalization ability model achieving better synergy among different task typically propose prompt query generation module tasksaware adapter effectively convert original singletask model multitask model suitable image video scenario minimal additional parameter prompt query generation module facilitates explicit interaction different task tasksaware adapter help model dynamically learn suitable feature task additionally enable model learn temporal information lower cost propose synthetic video text dataset leveraging content deformation field codef algorithm notably method outperforms stateoftheart method average six crossdomain benchmark videolevel crossdomain adaption method even surpasses previous endtoend video spotting method video dstext average mota metric using imagelevel data demonstrate existing large multimodal model exhibit limitation generating crossdomain scene text spotting contrast vimts model requires significantly fewer parameter data code datasets made available httpsvimtextspottergithubio
human video generation diffusion transformer present novel approach generating highquality spatiotemporally coherent human video single image framework combine strength diffusion transformer capturing global correlation across viewpoint time cnns accurate condition injection core hierarchical transformer architecture factorizes selfattention across view time step spatial dimension enabling efficient modeling space precise conditioning achieved injecting human identity camera parameter temporal signal respective transformer train model collect multidimensional dataset spanning image video multiview data limited footage along tailored multidimensional training strategy approach overcomes limitation previous method based generative adversarial network vanilla diffusion model struggle complex motion viewpoint change generalization extensive experiment demonstrate method ability synthesize realistic coherent human motion video paving way advanced multimedia application area virtual reality animation
uniedit unified tuningfree framework video motion appearance editing recent advance textguided video editing showcased promising result appearance editing eg stylization however video motion editing temporal dimension eg eating waving distinguishes video editing image editing underexplored work present uniedit tuningfree framework support video motion appearance editing harnessing power pretrained texttovideo generator within inversionthengeneration framework realize motion editing preserving source video content based insight temporal spatial selfattention layer encode interframe intraframe dependency respectively introduce auxiliary motionreference reconstruction branch produce textguided motion source feature respectively obtained feature injected main editing path via temporal spatial selfattention layer extensive experiment demonstrate uniedit cover video motion editing various appearance editing scenario surpasses stateoftheart method code publicly available
intelligent director automatic framework dynamic visual composition using chatgpt rise short video platform represented tiktok trend user expressing creativity photo video increased dramatically however ordinary user lack professional skill produce highquality video using professional creation software meet demand intelligent userfriendly video creation tool propose dynamic visual composition dvc task interesting challenging task aim automatically integrate various medium element based user requirement create storytelling video propose intelligent director framework utilizing lens generate description image video frame combining chatgpt generate coherent caption recommending appropriate music name bestmatched music obtained music retrieval material caption image video music integrated seamlessly synthesize video finally apply style transfer construct personal album datasets verified effectiveness framework solving dvc qualitative quantitative comparison along user study demonstrating substantial potential
place solution mevis track cvpr pvuw workshop motion expression guided video segmentation referring video object segmentation rvos relies natural language expression segment target object video emphasizing modeling dense textvideo relation current rvos method typically use independently pretrained vision language model backbone resulting significant domain gap video text crossmodal feature interaction text feature used query initialization fully utilize important information text work propose using frozen pretrained visionlanguage model vlm backbone specific emphasis enhancing crossmodal feature interaction firstly use frozen convolutional clip backbone generate featurealigned vision text feature alleviating issue domain gap reducing training cost secondly add crossmodal feature fusion pipeline enhance utilization multimodal information furthermore propose novel video query initialization method generate higher quality video query without bell whistle method achieved jf mevis test set ranked place mevis track cvpr pvuw workshop motion expression guided video segmentation
sitar semisupervised image transformer action recognition recognizing action limited set labeled video remains challenge annotating visual data tedious also expensive due classified nature moreover handling spatiotemporal data using deep transformer introduce significant computational complexity paper objective address video action recognition semisupervised setting leveraging handful labeled video along collection unlabeled video compute efficient manner specifically rearrange multiple frame input video rowcolumn form construct super image subsequently capitalize vast pool unlabeled sample employ contrastive learning encoded super image proposed approach employ two pathway generate representation temporally augmented super image originating video specifically utilize imagetransformer generate representation apply contrastive loss function minimize similarity representation different video maximizing representation identical video method demonstrates superior performance compared existing stateoftheart approach semisupervised action recognition across various benchmark datasets significantly reducing computational cost
storynavi ondemand narrativedriven reconstruction video play generative ai manually navigating lengthy video seek information answer question tedious timeconsuming task user introduce storynavi novel system powered vllms generating customised video play experience retrieving material original video directly answer user query constructing nonlinear sequence identified relevant clip form cohesive narrative storynavi offer two mode playback constructed video play videocentric play original audio skip irrelevant segment narrativecentric narration guide experience original audio muted technical evaluation showed adequate retrieval performance compared human retrieval user evaluation show maintaining narrative coherence significantly enhances user engagement viewing disjointed video segment however factor like video genre content query may lead varying user preference playback mode
variational quantum circuit enhanced generative adversarial network generative adversarial network gan one widelyadopted machinelearning framework wide range application generating highquality image video audio content however training gan could become computationally expensive large neural network work propose hybrid quantumclassical architecture improving gan denoted qcgan performance examed numerically benchmarking classical gan using mindspore quantum task handwritten image generation generator qcgan consists quantum variational circuit together onelayer neural network discriminator consists traditional neural network leveraging entangling expressive power quantum circuit hybrid architecture achieved better performance frechet inception distance classical gan much fewer training parameter number iteration convergence also demonstrated superiority qcgan alternative quantum gan namely pathgan could hardly generate larger image work demonstrates value combining idea quantum computing machine learning area quantumforai aiforquantum
fast highresolution image synthesis latent adversarial diffusion distillation diffusion model main driver progress image video synthesis suffer slow inference speed distillation method like recently introduced adversarial diffusion distillation add aim shift model manyshot singlestep inference albeit cost expensive difficult optimization due reliance fixed pretrained discriminator introduce latent adversarial diffusion distillation ladd novel distillation approach overcoming limitation add contrast pixelbased add ladd utilizes generative feature pretrained latent diffusion model approach simplifies training enhances performance enabling highresolution multiaspect ratio image synthesis apply ladd stable diffusion obtain fast model match performance stateoftheart texttoimage generator using four unguided sampling step moreover systematically investigate scaling behavior demonstrate ladds effectiveness various application image editing inpainting
disenstudio customized multisubject texttovideo generation disentangled spatial control generating customized content video received increasing attention recently however existing work primarily focus customized texttovideo generation single subject suffering subjectmissing attributebinding problem video expected contain multiple subject furthermore existing model struggle assign desired action corresponding subject actionbinding problem failing achieve satisfactory multisubject generation performance tackle problem paper propose disenstudio novel framework generate textguided video customized multiple subject given image subject specifically disenstudio enhances pretrained diffusionbased texttovideo model proposed spatialdisentangled crossattention mechanism associate subject desired action model customized multiple subject proposed motionpreserved disentangled finetuning involves three tuning strategy multisubject cooccurrence tuning masked singlesubject tuning multisubject motionpreserved tuning first two strategy guarantee subject occurrence preserve visual attribute third strategy help model maintain temporal motiongeneration ability finetuning static image conduct extensive experiment demonstrate proposed disenstudio significantly outperforms existing method various metric additionally show disenstudio used powerful tool various controllable generation application
largescale visionlanguage sticker dataset multiframe animated sticker generation common form communication social mediastickers win user love internet scenario ability convey emotion vivid cute interesting way people prefer get appropriate sticker retrieval rather creation reason creating sticker timeconsuming relies rulebased creative tool limited capability nowadays advanced texttovideo algorithm spawned numerous general video generation system allow user customize highquality photorealistic video providing simple text prompt however creating customized animated sticker lower frame rate abstract semantics video greatly hindered difficulty data acquisition incomplete benchmark facilitate exploration researcher animated sticker generation asg field firstly construct currently largest visionlanguage sticker dataset named twomillion scale contains static animated sticker secondly improve performance traditional video generation method asg task discrete characteristic propose spatial temporal interaction sti layer utilizes semantic interaction detail preservation address issue insufficient information utilization moreover train baseline several video generation method eg transformerbased diffusionbased method conduct detailed analysis establish systemic supervision asg task best knowledge comprehensive largescale benchmark multiframe animated sticker generation hope work provide valuable inspiration scholar intelligent creation
arlon boosting diffusion transformer autoregressive model long video generation texttovideo model recently undergone rapid substantial advancement nevertheless due limitation data computational resource achieving efficient generation long video rich motion dynamic remains significant challenge generate highquality dynamic temporally consistent long video paper present arlon novel framework boost diffusion transformer autoregressive model long video generation integrating coarse spatial longrange temporal information provided ar model guide dit model specifically arlon incorporates several key innovation latent vector quantized variational autoencoder vqvae compress input latent space dit model compact visual token bridging ar dit model balancing learning complexity information density adaptive normbased semantic injection module integrates coarse discrete visual unit ar model dit model ensuring effective guidance video generation enhance tolerance capability noise introduced ar inference dit model trained coarser visual latent token incorporated uncertainty sampling module experimental result demonstrate arlon significantly outperforms baseline eight eleven metric selected vbench notable improvement dynamic degree aesthetic quality delivering competitive result remaining three simultaneously accelerating generation process addition arlon achieves stateoftheart performance long video generation detailed analysis improvement inference efficiency presented alongside practical application demonstrates generation long video using progressive text prompt see demo arlon httpakamsarlon
sparsecontrolled generation motion transfer recent advance generative model enable generation dynamic object singleview video existing approach utilize score distillation sampling form dynamic scene dynamic nerf dense gaussians however method struggle strike balance among reference view alignment spatiotemporal consistency motion fidelity singleview condition due implicit nature nerf intricate dense gaussian motion prediction address issue paper proposes efficient sparsecontrolled framework named decouples motion appearance achieve superior generation moreover introduce adaptive gaussian ag initialization gaussian alignment ga loss mitigate shape degeneration issue ensuring fidelity learned motion shape comprehensive experimental result demonstrate method surpasses existing method quality efficiency addition facilitated disentangled modeling motion appearance devise novel application seamlessly transfer learned motion onto diverse array entity according textual description
panacea panoramic controllable video generation autonomous driving field autonomous driving increasingly demand highquality annotated video training data paper propose panacea powerful universally applicable framework generating video data driving scene built upon foundation previous work panacea panacea adopts multiview appearance noise prior mechanism superresolution module enhanced consistency increased resolution extensive experiment show generated video sample panacea greatly benefit wide range task different datasets including object tracking object detection lane detection task nuscenes argoverse dataset result strongly prove panacea valuable data generation framework autonomous driving
dartcontrol diffusionbased autoregressive motion model realtime textdriven motion control textconditioned human motion generation allows user interaction natural language become increasingly popular existing method typically generate short isolated motion based single input sentence however human motion continuous extend long period carrying rich semantics creating long complex motion precisely respond stream text description particularly online realtime setting remains significant challenge furthermore incorporating spatial constraint textconditioned motion generation present additional challenge requires aligning motion semantics specified text description geometric information goal location scene geometry address limitation propose dartcontrol short dart diffusionbased autoregressive motion primitive model realtime textdriven motion control model effectively learns compact motion primitive space jointly conditioned motion history text input using latent diffusion model autoregressively generating motion primitive based preceding history current text input dart enables realtime sequential motion generation driven natural language description additionally learned motion primitive space allows precise spatial motion control formulate either latent noise optimization problem markov decision process addressed reinforcement learning present effective algorithm approach demonstrating model versatility superior performance various motion synthesis task experiment show method outperforms existing baseline motion realism efficiency controllability video result available project page
vbench comprehensive versatile benchmark suite video generative model video generation witnessed significant advancement yet evaluating model remains challenge comprehensive evaluation benchmark video generation indispensable two reason existing metric fully align human perception ideal evaluation system provide insight inform future development video generation end present vbench comprehensive benchmark suite dissects video generation quality specific hierarchical disentangled dimension tailored prompt evaluation method vbench several appealing property comprehensive dimension vbench comprises dimension video generation eg subject identity inconsistency motion smoothness temporal flickering spatial relationship etc evaluation metric finegrained level reveal individual model strength weakness human alignment also provide dataset human preference annotation validate benchmark alignment human perception evaluation dimension respectively valuable insight look current model ability across various evaluation dimension various content type also investigate gap video image generation model versatile benchmarking vbench support evaluating texttovideo imagetovideo introduce highquality image suite adaptive aspect ratio enable fair evaluation across different imagetovideo generation setting beyond assessing technical quality vbench evaluates trustworthiness video generative model providing holistic view model performance full opensourcing fully opensource vbench continually add new video generation model leaderboard drive forward field video generation
freedygs cameraposefree scene reconstruction based gaussian splatting dynamic surgical video reconstructing endoscopic video crucial highfidelity visualization efficiency surgical operation despite importance existing reconstruction method encounter several challenge including stringent demand accuracy imprecise camera positioning intricate dynamic scene necessity rapid reconstruction addressing issue paper present first cameraposefree scene reconstruction framework freedygs tailored dynamic surgical video leveraging gaussian splatting technology approach employ framebyframe reconstruction strategy delineated four distinct phase scene initialization joint learning scene expansion retrospective learning introduce generalizable gaussians parameterization module within scene initialization expansion phase proficiently generate gaussian attribute pixel rgbd frame joint learning phase crafted concurrently deduce scene deformation camera pose facilitated innovative flexible deformation module scene expansion stage gaussian point gradually grow camera move retrospective learning phase dedicated enhancing precision scene deformation reassessment prior frame efficacy proposed freedygs substantiated experiment two datasets stereomis hamlyn datasets experimental outcome underscore freedygs surpasses conventional baseline model rendering fidelity computational efficiency
movie gen cast medium foundation model present movie gen cast foundation model generates highquality hd video different aspect ratio synchronized audio also show additional capability precise instructionbased video editing generation personalized video based user image model set new stateoftheart multiple task texttovideo synthesis video personalization video editing videotoaudio generation texttoaudio generation largest video generation model parameter transformer trained maximum context length video token corresponding generated video second framespersecond show multiple technical innovation simplification architecture latent space training objective recipe data curation evaluation protocol parallelization technique inference optimization allow u reap benefit scaling pretraining data model size training compute training large scale medium generation model hope paper help research community accelerate progress innovation medium generation model video paper available httpsgofbmemoviegenresearchvideos
multiview video diffusion model generation current generation method achieved noteworthy efficacy aid advanced diffusion generative model however method lack multiview spatialtemporal modeling encounter challenge integrating diverse prior knowledge multiple diffusion model resulting inconsistent temporal appearance flicker paper propose novel generation pipeline namely aimed generating spatialtemporally consistent content monocular video first design unified diffusion model tailored multiview video generation incorporating learnable motion module frozen diffusion model capture multiview spatialtemporal correlation training curated dataset diffusion model acquires reasonable temporal consistency inherently preserve generalizability spatial consistency diffusion model subsequently propose score distillation sampling loss based multiview video diffusion model optimize representation parameterized dynamic nerf aim eliminate discrepancy arising multiple diffusion model allowing generating spatialtemporally consistent content moreover devise anchor loss enhance appearance detail facilitate learning dynamic nerf extensive qualitative quantitative experiment demonstrate method achieves superior performance compared previous method
miradata largescale video dataset long duration structured caption soras highmotion intensity long consistent video significantly impacted field video generation attracting unprecedented attention however existing publicly available datasets inadequate generating soralike video mainly contain short video low motion intensity brief caption address issue propose miradata highquality video dataset surpasses previous one video duration caption detail motion strength visual quality curate miradata diverse manually selected source meticulously process data obtain semantically consistent clip employed annotate structured caption providing detailed description four different perspective along summarized dense caption better assess temporal consistency motion intensity video generation introduce mirabench enhances existing benchmark adding consistency trackingbased motion strength metric mirabench includes evaluation prompt metric covering temporal consistency motion strength consistency visual quality textvideo alignment distribution similarity demonstrate utility effectiveness miradata conduct experiment using ditbased video generation model miradit experimental result mirabench demonstrate superiority miradata especially motion strength
generative video indexer efficient textvideo retrieval current textvideo retrieval method mainly rely crossmodal matching query video calculate similarity score sorted obtain retrieval result method considers matching candidate video query incurs significant time cost increase notably increase candidate generative model common natural language processing computer vision successfully applied document retrieval application multimodal retrieval remains unexplored enhance retrieval efficiency paper introduce modelbased video indexer named sequencetosequence generative model directly generating video identifier retrieving candidate video constant time complexity aim reduce retrieval time maintaining high accuracy achieve goal propose video identifier encoding queryidentifier augmentation approach represent video short sequence preserving semantic information method consistently enhances retrieval efficiency current stateoftheart model four standard datasets enables baseline original retrieval time achieve better retrieval performance msrvtt msvd activitynet didemo code available
millionscale real text image prompt dataset imagetovideo generation video generation model revolutionizing content creation imagetovideo model drawing increasing attention due enhanced controllability visual consistency practical application however despite popularity model rely userprovided text image prompt currently dedicated dataset studying prompt paper introduce first largescale dataset million unique userprovided text image prompt specifically imagetovideo generation additionally provide corresponding generated video five stateoftheart imagetovideo model begin outlining timeconsuming costly process curating largescale dataset next compare two popular prompt datasets vidprom texttovideo diffusiondb texttoimage highlighting difference basic semantic information dataset enables advancement imagetovideo research instance develop better model researcher use prompt analyze user preference evaluate multidimensional performance trained model enhance model safety may focus addressing misinformation issue caused imagetovideo model new research inspired difference existing datasets emphasize importance specialized imagetovideo prompt dataset project publicly available
efficient autoregressive video diffusion model causal generation cache sharing advance diffusion model today video generation achieved impressive quality extend generation length facilitate realworld application majority video diffusion model vdms generate video autoregressive manner ie generating subsequent clip conditioned last frame previous clip however existing autoregressive vdms highly inefficient redundant model must recompute conditional frame overlapped adjacent clip issue exacerbated conditional frame extended autoregressively provide model longterm context case computational demand increase significantly ie quadratic complexity wrt autoregression step paper propose efficient autoregressive vdm causal generation cache sharing causal generation introduces unidirectional feature computation ensures cache conditional frame precomputed previous autoregression step reused every subsequent step eliminating redundant computation cache sharing share cache across denoising step avoid huge cache storage cost extensive experiment demonstrated achieves stateoftheart quantitative qualitative video generation result significantly improves generation speed code available httpsgithubcomdawnlxcausalcachevdm
harnessing large language model trainingfree video anomaly detection video anomaly detection vad aim temporally locate abnormal event video existing work mostly rely training deep model learn distribution normality either videolevel supervision oneclass supervision unsupervised setting trainingbased method prone domainspecific thus costly practical deployment domain change involve data collection model training paper radically depart previous effort propose languagebased vad lavad method tackling vad novel trainingfree paradigm exploiting capability pretrained large language model llm existing visionlanguage model vlms leverage vlmbased captioning model generate textual description frame test video textual scene description devise prompting mechanism unlock capability llm term temporal aggregation anomaly score estimation turning llm effective video anomaly detector leverage modalityaligned vlms propose effective technique based crossmodal similarity cleaning noisy caption refining llmbased anomaly score evaluate lavad two large datasets featuring realworld surveillance scenario ucfcrime xdviolence showing outperforms unsupervised oneclass method without requiring training data collection
cinepile long video question answering dataset benchmark current datasets longform video understanding often fall short providing genuine longform comprehension challenge many task derived datasets successfully tackled analyzing one random frame video address issue present novel dataset benchmark cinepile specifically designed authentic longform video understanding paper detail innovative approach creating questionanswer dataset utilizing advanced llm humanintheloop building upon humangenerated raw data comprehensive dataset comprises multiplechoice question mcqs covering various visual multimodal aspect including temporal comprehension understanding humanobject interaction reasoning event action within scene additionally finetuned opensource videollms training split evaluated opensource proprietary videocentric llm test split dataset finding indicate although current model underperform compared human finetuning model lead significant improvement performance
learning semantic traversability egocentric video automated annotation strategy reliable autonomous robot navigation urban setting robot must ability identify semantically traversable terrain image based semantic understanding scene reasoning ability based semantic traversability frequently achieved using semantic segmentation model finetuned testing domain finetuning process often involves manual data collection target robot annotation human labelers prohibitively expensive unscalable work present effective methodology training semantic traversability estimator using egocentric video automated annotation process egocentric video collected camera mounted pedestrian chest dataset training semantic traversability estimator automatically generated extracting semantically traversable region video frame using recent foundation model image segmentation prompting technique extensive experiment video taken across several country city covering diverse urban scenario demonstrate high scalability generalizability proposed annotation method furthermore performance analysis realworld deployment autonomous robot navigation showcase trained semantic traversability estimator highly accurate able handle diverse camera viewpoint computationally light realworld applicable summary video available httpsyoutubeeuvohwala
ophnet largescale video benchmark ophthalmic surgical workflow understanding surgical scene perception via video critical advancing robotic surgery telesurgery aiassisted surgery particularly ophthalmology however scarcity diverse richly annotated video datasets hindered development intelligent system surgical workflow analysis existing datasets face challenge small scale lack diversity surgery phase category absence timelocalized annotation limitation impede action understanding model generalization validation complex diverse realworld surgical scenario address gap introduce ophnet largescale expertannotated video benchmark ophthalmic surgical workflow understanding ophnet feature diverse collection surgical video spanning type cataract glaucoma corneal surgery detailed annotation unique surgical phase finegrained operation sequential hierarchical annotation surgery phase operation enabling comprehensive understanding improved interpretability timelocalized annotation facilitating temporal localization prediction task within surgical workflow approximately hour surgical video ophnet time larger largest existing surgical workflow analysis benchmark code dataset available
fmital fewshot multiple instance temporal action localization probability distribution learning interval cluster refinement present fewshot temporal action localization model cant handle situation video contain multiple action instance purpose paper achieve manifold action instance localization lengthy untrimmed query video using limited trimmed support video address challenging problem effectively proposed novel solution involving spatialchannel relation transformer probability learning cluster refinement method accurately identify start end boundary action query video utilizing limited number labeled video proposed method adept capturing temporal spatial context effectively classify precisely locate action video enabling comprehensive utilization crucial detail selective cosine penalization algorithm designed suppress temporal boundary include action scene switch probability learning combined label generation algorithm alleviates problem action duration diversity enhances model ability handle fuzzy action boundary interval cluster help u get final result multiple instance situation fewshot temporal action localization model achieves competitive performance meticulous experimentation utilizing benchmark datasets code readily available httpsgithubcomycwfsfmital
authentication integrity smartphone video multimedia container structure analysis nowadays mobile device become natural substitute digital camera capture everyday situation easily quickly encouraging user express image video video shared across different platform exposing kind intentional manipulation criminal aware weakness forensic technique accuse innocent person exonerate guilty person judicial process commonly manufacturer comply specification standard creation video also video shared social network instant messaging application go filtering compression process reduce size facilitate transfer optimize storage platform omission specification result transformation carried platform embed feature pattern multimedia container video pattern make possible distinguish brand device generated video social network instant messaging application used transfer research recent year focused analysis avi container tiny video datasets work present novel technique detect possible attack mov format video affect integrity authenticity method based analysis structure video container generated mobile device behavior shared social network instant messaging application manipulated editing program objective proposal verify integrity video identify source acquisition distinguish original manipulated video
conditional brownian bridge diffusion model vhr sar optical image translation synthetic aperture radar sar imaging technology provides unique advantage able collect data regardless weather condition time however sar image exhibit complex backscatter pattern speckle noise necessitate expertise interpretation research translating sar image opticallike representation conducted aid interpretation sar data nevertheless existing study predominantly utilized lowresolution satellite imagery datasets largely based generative adversarial network gan known training instability low fidelity overcome limitation lowresolution data usage ganbased approach paper introduces conditional imagetoimage translation approach based brownian bridge diffusion model bbdm conducted comprehensive experiment msaw dataset paired sar optical image collection veryhighresolution vhr experimental result indicate method surpasses conditional diffusion model cdms ganbased model diverse perceptual quality metric
camvig camera aware imagetovideo generation multimodal transformer extend multimodal transformer include camera motion conditioning signal task video generation generative video model becoming increasingly powerful thus focusing research effort method controlling output model propose add virtual camera control generative video method conditioning generated video encoding threedimensional camera movement course generated video result demonstrate able successfully control camera video generation starting single frame camera signal demonstrate accuracy generated camera path using traditional computer vision method
vintage joint video text conditioning holistic audio generation recent advance audio generation focused texttoaudio videotoaudio task however method generate holistic sound onscreen offscreen generate sound aligning onscreen object generate semantically complete offscreen sound missing work address task holistic audio generation given video text prompt aim generate onscreen offscreen sound temporally synchronized video semantically aligned text video previous approach joint text videotoaudio generation often suffer modality bias favoring one modality overcome limitation introduce vintage flowbased transformer model jointly considers text video guide audio generation framework comprises two key component visualtext encoder joint vtsit model reduce modality bias improve generation quality employ pretrained unimodal texttoaudio videotoaudio generation model additional guidance due lack appropriate benchmark also introduce vintagebench dataset videotextaudio pair containing onscreen offscreen sound comprehensive experiment vintagebench demonstrate joint text visual interaction necessary holistic audio generation furthermore vintage achieves stateoftheart result vggsound benchmark source code pretrained model released demo available httpswwwyoutubecomwatchvqmqwhujpkji
learning watching review videobased learning approach robot manipulation robot learning manipulation skill hindered scarcity diverse unbiased datasets curated datasets help challenge remain generalizability realworld transfer meanwhile largescale inthewild video datasets driven progress computer vision selfsupervised technique translating robotics recent work explored learning manipulation skill passively watching abundant video sourced online showing promising result videobased learning paradigm provide scalable supervision reducing dataset bias survey review foundation video feature representation learning technique object affordance understanding handbody modeling largescale robot resource well emerging technique acquiring robot manipulation skill uncontrolled video demonstration discus learning observing largescale human video enhance generalization sample efficiency robotic manipulation survey summarizes videobased learning approach analysis benefit standard datasets survey metric benchmark discusses open challenge future direction nascent domain intersection computer vision natural language processing robot learning
place anything video controllable video editing demonstrated remarkable potential across diverse application particularly scenario capturing recapturing realworld video either impractical costly paper introduces novel efficient system named placeanything facilitates insertion object video solely based picture text description target object element system comprises three module generation video reconstruction target insertion integrated approach offer efficient effective solution producing editing highquality video seamlessly inserting realistic object user study demonstrate system effortlessly place object video using photograph object demo video found httpsyoutubeafxqgllrnte please also visit project page httpsplaceanythinggithubio get access
enhancing bandwidth efficiency video motion transfer application using deep learning based keypoint prediction propose deep learning based novel prediction framework enhanced bandwidth reduction motion transfer enabled video application video conferencing virtual reality gaming privacy preservation patient health monitoring model complex motion use first order motion model fomm represents dynamic object using learned keypoints along local affine transformation keypoints extracted selfsupervised keypoint detector organized time series corresponding video frame prediction keypoints enable transmission using lower frame per second source device performed using variational recurrent neural network vrnn predicted keypoints synthesized video frame using optical flow estimator generator network efficacy leveraging keypoint based representation conjunction vrnn based prediction video animation reconstruction demonstrated three diverse datasets realtime application result show effectiveness proposed architecture enabling additional bandwidth reduction existing keypoint based video motion transfer framework without significantly compromising video quality
enhancing traffic safety parallel dense video captioning endtoend event analysis paper introduces solution track ai city challenge task aim solve traffic safety description analysis dataset woven traffic safety wts realworld pedestriancentric traffic video dataset finegrained spatialtemporal understanding solution mainly focus following point solve dense video captioning leverage framework dense video captioning parallel decoding pdvc model visuallanguage sequence generate dense caption chapter video work leverage clip extract visual feature efficiently perform crossmodality training visual textual representation conduct domainspecific model adaptation mitigate domain shift problem pose recognition challenge video understanding moreover leverage captioned video conduct knowledge transfer better understanding wts video accurate captioning solution yielded test set achieving place competition open source code available
siavc semisupervised framework industrial accident video classification semisupervised learning suffers imbalance labeled unlabeled training data video surveillance scenario paper propose new semisupervised learning method called siavc industrial accident video classification specifically design video augmentation module called super augmentation block sab sab add gaussian noise randomly mask video frame according historical loss unlabeled data model optimization propose video crossset augmentation module vcam generate diverse pseudolabel sample highconfidence unlabeled sample alleviates mismatch sampling experience provides highquality training data additionally construct new industrial accident surveillance video dataset framelevel annotation namely evaluate proposed method compared stateoftheart semisupervised learning based method siavc demonstrates outstanding video classification performance achieving accuracy fire detection datasets respectively source code constructed dataset released urlhttpsgithubcomalchemyemperorsiavc
visionbased manipulation single human video openworld object graph present objectcentric approach empower robot learn visionbased manipulation skill human video investigate problem imitating robot manipulation single human video openworld setting robot must learn manipulate novel object one video demonstration introduce orion algorithm tackle problem extracting objectcentric manipulation plan single rgbd video deriving policy condition extracted plan method enables robot learn video captured daily mobile device ipad generalize policy deployment environment varying visual background camera angle spatial layout novel object instance systematically evaluate method shorthorizon longhorizon task demonstrating efficacy orion learning single human video open world video found project website httpsutaustinrplgithubioorionrelease
live video captioning dense video captioning task involves detection description event within video sequence traditional approach focus offline solution entire video analysis available captioning model work introduce paradigm shift towards live video captioning lvc lvc dense video captioning model must generate caption video stream online manner facing important constraint work partial observation video need temporal anticipation course ensuring ideally realtime response work formally introduce novel problem lvc propose new evaluation metric tailored online scenario demonstrating superiority traditional metric also propose lvc model integrating deformable transformer temporal filtering address lvc new challenge experimental evaluation activitynet caption dataset validate effectiveness approach highlighting performance lvc compared stateoftheart offline method result model well evaluation kit novel metric integrated made publicly available encourage research lvc
video incontext learning autoregressive transformer zeroshot video imitator people interact realworld largely dependent visual signal ubiquitous illustrate detailed demonstration paper explore utilizing visual signal new interface model interact environment specifically choose video representative visual signal training autoregressive transformer video datasets selfsupervised objective find model emerges zeroshot capability infer semantics demonstration video imitate semantics unseen scenario allows model perform unseen task watching demonstration video incontext manner without finetuning validate imitation capacity design various evaluation metric including objective subjective measure result show model generate highquality video clip accurately align semantic guidance provided demonstration video also show imitation capacity follows scaling law code model opensourced
slvideo sign language video moment retrieval framework slvideo video moment retrieval system sign language video incorporates facial expression addressing gap existing technology system extract embedding representation hand face sign video frame capture sign entirety enabling user search specific sign language video segment text query collection eight hour annotated portuguese sign language video used dataset clip model used generate embeddings initial result promising zeroshot setting addition slvideo incorporates thesaurus enables user search similar sign retrieved using video segment embeddings also support edition creation video sign language annotation project web page httpsnovasearchgithubioslvideo
aim challenge efficient video superresolution compressed content video superresolution vsr critical task enhancing lowbitrate lowresolution video particularly streaming application numerous solution developed often suffer high computational demand resulting low frame rate fps poor power efficiency especially mobile platform work compile different method address challenge solution endtoend realtime video superresolution framework optimized high performance low runtime also introduce new test set highquality video validate approach proposed solution tackle video upscaling two application general case tailored towards mobile device track solution reduced number parameter operation mac allow high fps improve vmaf psnr interpolation baseline report gauge efficient video superresolution method date
neural video representation redundancy reduction consistency preservation implicit neural representation inr embed various signal neural network gained attention recent year versatility handling diverse signal type context video inr achieves video compression embedding video signal directly network compressing conventional method either use index express time frame feature extracted individual frame network input latter method provides greater expressive capability input specific video however feature extracted frame often contain redundancy contradicts purpose video compression additionally redundancy make challenging accurately reconstruct highfrequency component frame address problem focus separating highfrequency lowfrequency component reconstructed frame propose video representation method generates highfrequency lowfrequency component frame using feature extracted highfrequency component temporal information respectively experimental result demonstrate method outperforms existing hnerv method achieving superior result percent video
ncst neuralbased color style transfer video retouching video color style transfer aim transform color style original video using reference style image existing method employ neural network come challenge like opaque transfer process limited user control outcome typically user finetune resulting image video tackle issue introduce method predicts specific parameter color style transfer using two image initially train neural network learn corresponding color adjustment parameter applying style transfer video finetune network key frame video chosen style image generating precise transformation parameter applied convert color style image video experimental result demonstrate algorithm surpasses current method color style transfer quality moreover parameter method specific interpretable meaning enabling user understand color style transfer process allowing perform manual finetuning desired
ikea manual work grounding assembly instruction internet video shape assembly ubiquitous task daily life integral constructing complex structure like ikea furniture significant progress made developing autonomous agent shape assembly existing datasets yet tackled grounding assembly instruction video essential holistic understanding assembly space time introduce ikea video manual dataset feature model furniture part instructional manual assembly video internet importantly annotation dense spatiotemporal alignment data modality demonstrate utility ikea video manual present five application essential shape assembly assembly plan generation partconditioned segmentation partconditioned pose estimation video object segmentation furniture assembly based instructional video manual application provide evaluation metric baseline method experiment annotated data highlight many challenge grounding assembly instruction video improve shape assembly including handling occlusion varying viewpoint extended assembly sequence
optimal transcoding preset selection live video streaming today digital landscape video content dominates internet traffic underscoring need efficient video processing support seamless live streaming experience platform like youtube live twitch facebook live paper introduces comprehensive framework designed optimize video transcoding parameter specific focus preset bitrate selection minimize distortion respecting constraint bitrate transcoding time framework comprises three main step feature extraction prediction optimization leverage extracted feature predict transcoding time ratedistortion employing supervised unsupervised method utilizing integer linear programming identifies optimal sequence presets bitrates video segment ensuring realtime application feasibility set constraint result demonstrate framework effectiveness enhancing video quality live streaming maintaining high standard video delivery managing computational resource efficiently optimization approach meet evolving demand video delivery offering solution realtime transcoding optimization evaluation using user generated content dataset showed average psnr improvement db default twitch configuration highlighting significant psnr gain additionally subsequent experiment demonstrated bdrate reduction reinforcing framework superior performance twitch default configuration
playable game generation recent year artificial intelligence generated content aigc advanced texttoimage generation texttovideo multimodal video synthesis however generating playable game present significant challenge due stringent requirement realtime interaction high visual quality accurate simulation game mechanic existing approach often fall short either lacking realtime capability failing accurately simulate interactive mechanic tackle playability issue propose novel method called emphplaygen encompasses game data generation autoregressive ditbased diffusion model comprehensive playabilitybased evaluation framework validated wellknown game playgen achieves realtime interaction ensures sufficient visual quality provides accurate interactive mechanic simulation notably result sustained even frame gameplay nvidia rtx gpu code publicly available playable demo generated ai
exocentric egocentric transfer action recognition short survey egocentric vision capture scene point view camera wearer exocentric vision capture overall scene context jointly modeling ego exo view crucial developing nextgeneration ai agent community regained interest field egocentric vision thirdperson view firstperson thoroughly investigated work aim study synchronously exocentric video contain many relevant signal transferrable egocentric video paper provide broad overview work combining egocentric exocentric vision
temporally consistent dynamic scene graph endtoend approach action tracklet generation understanding video content pivotal advancing realworld application like activity recognition autonomous system humancomputer interaction scene graph adept capturing spatial relationship object individual frame extending representation capture dynamic interaction across video sequence remains significant challenge address present tcdsg temporally consistent dynamic scene graph innovative endtoend framework detects track link subjectobject relationship across time generating action tracklets temporally consistent sequence entity interaction approach leverage novel bipartite matching mechanism enhanced adaptive decoder query feedback loop ensuring temporal coherence robust tracking extended sequence method establishes new benchmark achieving improvement temporal recallk action genome openpvsg meva datasets also pioneer augmentation meva persistent object id annotation comprehensive tracklet generation seamlessly integrating spatial temporal dynamic work set new standard multiframe video analysis opening new avenue highimpact application surveillance autonomous navigation beyond
motionaware contrastive learning temporal panoptic scene graph generation equip artificial intelligence comprehensive understanding towards temporal world video panoptic scene graph generation abstract visual data node represent entity edge capture temporal relation existing method encode entity mask tracked across temporal dimension mask tube predict relation temporal pooling operation fully utilize motion indicative entity relation overcome limitation introduce contrastive representation learning framework focus motion pattern temporal scene graph generation firstly framework encourages model learn close representation mask tube similar subjectrelationobject triplet secondly seek push apart mask tube temporally shuffled version moreover also learn distant representation mask tube belonging video different triplet extensive experiment show motionaware contrastive framework significantly improves stateoftheart method video datasets
spatiotemporal skip guidance enhanced video diffusion sampling diffusion model emerged powerful tool generating highquality image video content sampling guidance technique like cfg improve quality reduce diversity motion autoguidance mitigates issue demand extra weak model training limiting practicality largescale model work introduce spatiotemporal skip guidance stg simple trainingfree sampling guidance method enhancing transformerbased video diffusion model stg employ implicit weak model via selfperturbation avoiding need external model additional training selectively skipping spatiotemporal layer stg produce aligned degraded version original model boost sample quality without compromising diversity dynamic degree contribution include introducing stg efficient highperforming guidance technique video diffusion model eliminating need auxiliary model simulating weak model layer skipping ensuring qualityenhanced guidance without compromising sample diversity dynamic unlike cfg additional result visit httpsjunhahyunggithubiostguidance
recent advance digital image video forensics antiforensics counter antiforensics image video forensics recently gained increasing attention due proliferation manipulated image video especially social medium platform twitter instagram spread disinformation fake news survey explores image video identification forgery detection covering manipulated digital medium generative medium however medium forgery detection technique susceptible antiforensics hand antiforensics technique detected therefore cover antiforensics counter antiforensics technique image video finally conclude survey highlighting open problem domain
presenting sense effort vibration based force estimated inverse dynamic video present sense effort vibration help video viewer understand person video move body suppose sense effort related force generate vibration based force present sense effort vibration use perceived intensity make sense effort proportional vibration demonstration experience vibration watching video create vibration spot experience vibration made video taken spot
video representation learning jointembedding predictive architecture video representation learning increasingly important topic machine learning research present video jepa variancecovariance regularization vjvcr jointembedding predictive architecture selfsupervised video representation learning employ variance covariance regularization avoid representation collapse show hidden representation vjvcr contain abstract highlevel information input data specifically outperform representation obtained generative baseline downstream task require understanding underlying dynamic moving object video additionally explore different way incorporate latent variable vjvcr framework capture information uncertainty future nondeterministic setting
using physic informed generative adversarial network model porous medium microct scanning rock significantly enhances understanding porescale physic porous medium advancement porescale simulation method pore network model possible accurately simulate multiphase flow property including relative permeability ctscanned rock sample however limited number ctscanned sample challenge connecting porescale network fieldscale rock property often make difficult use porescale simulated property realistic fieldscale reservoir simulation deep learning approach create synthetic rock structure allow u simulate variation ct rock structure used compute representative rock property flow function however current deep learning method rock structure synthesis dont consider rock property derived well observation lacking direct link porescale structure fieldscale data present method construct rock structure constrained observed rock property using generative adversarial network gans conditioning accomplished gradual gaussian deformation process begin pretraining wasserstein gan reconstruct rock structure subsequently use pore network model simulator compute rock property latent vector image generation gan progressively altered using gaussian deformation approach produce rock structure constrained wellderived conditioning data gan gaussian deformation approach enables highresolution synthetic image generation reproduces userdefined rock property porosity permeability pore size distribution research provides novel way link gangenerated model fieldderived quantity
scenellm implicit language reasoning llm dynamic scene graph generation dynamic scene contain intricate spatiotemporal information crucial mobile robot uavs autonomous driving system make informed decision parsing scene semantic triplet subjectpredicateobject accurate scene graph generation sgg highly challenging due fluctuating spatiotemporal complexity inspired reasoning capability large language model llm propose scenellm novel framework leverage llm powerful scene analyzer dynamic sgg framework introduces videotolanguage mapping module transforms video frame linguistic signal scene token making input comprehensible llm better encode spatial information devise spatial information aggregation sia scheme inspired structure chinese character encodes spatial data token using optimal transport ot generate implicit language signal framelevel token sequence capture video spatiotemporal information improve llm ability process implicit linguistic input apply lowrank adaptation lora finetune model finally use transformerbased sgg predictor decode llm reasoning predict semantic triplet method achieves stateoftheart result action genome ag benchmark extensive experiment show effectiveness scenellm understanding generating accurate dynamic scene graph
subjective objective analysis indian social medium video quality conducted largescale subjective study perceptual quality usergenerated mobile video content set mobileoriginated video obtained indian social medium platform sharechat content viewed volunteer human subject controlled laboratory condition benefit culturally diversifying existing corpus usergenerated content ugc video quality datasets great need large diverse ugcvqa datasets given explosive global growth visual internet social medium platform particularly true regard video obtained smartphones especially rapidly emerging economy like india sharechat provides safe cultural community oriented space user generate share content preferred indian language dialect subjective quality study based data offer boost cultural visual language diversification video quality research community expect new data resource also allow development system predict perceived visual quality indian social medium video control scaling compression protocol streaming provide better user recommendation guide content analysis processing demonstrate value new data resource conducting study leading blind video quality model including new model called moeva deploys mixture expert predict video quality new livesharechat dataset sample source code moeva made freely available research community httpsgithubcomsandeepsmlivesc
saliency detection educational video analyzing performance current model identifying limitation advancement direction identifying region learning resource learner pay attention crucial assessing material impact improving design related support system saliency detection video address automatic recognition attentiondrawing region single frame educational setting recognition pertinent region video visual stream enhance content accessibility information retrieval task video segmentation navigation summarization advancement pave way development advanced aiassisted technology support learning greater efficacy however task becomes particularly challenging educational video due combination unique characteristic text voice illustration animation best knowledge currently study evaluates saliency detection approach educational video paper address gap evaluating four stateoftheart saliency detection approach educational video reproduce original study explore replication capability generalpurpose noneducational datasets investigate generalization capability model evaluate performance educational video conduct comprehensive analysis identify common failure scenario possible area improvement experimental result show educational video remain challenging context generic video saliency detection model
puppetmaster scaling interactive video generation motion prior partlevel dynamic present puppetmaster interactive video generative model serve motion prior partlevel dynamic test time given single image sparse set motion trajectory ie drag puppetmaster synthesize video depicting realistic partlevel motion faithful given drag interaction achieved finetuning largescale pretrained video diffusion model propose new conditioning architecture inject dragging control effectively importantly introduce alltofirst attention mechanism dropin replacement widely adopted spatial attention module significantly improves generation quality addressing appearance background issue existing model unlike motionconditioned video generator trained inthewild video mostly move entire object puppetmaster learned objaverseanimationhq new dataset curated partlevel motion clip propose strategy automatically filter suboptimal animation augment synthetic rendering meaningful motion trajectory puppetmaster generalizes well real image across various category outperforms existing method zeroshot manner realworld benchmark see project page result vggpuppetmastergithubio
pose guided human motion copy human motion copy intriguing yet challenging task artificial intelligence computer vision strives generate fake video target person performing motion source person problem inherently challenging due subtle humanbody texture detail generated temporal consistency considered existing approach typically adopt conventional gan loss produce target fake video intrinsically necessitates large number training sample challenging acquire meanwhile current method still difficulty attaining realistic image detail temporal consistency unfortunately easily perceived human observer motivated try tackle issue three aspect constrain posetoappearance generation perceptual loss theoretically motivated gromovwasserstein loss bridge gap pose appearance present episodic memory module posetoappearance generation propel continuous learning help model learn past poor generation also utilize geometrical cue face optimize facial detail refine key body part dedicated local gan advocate generating foreground sequencetosequence manner rather singleframe manner explicitly enforcing temporal inconsistency empirical result five datasets iper complexmotion solodance fish mouse datasets demonstrate method capable generating realistic target video precisely copying motion source video method significantly outperforms stateoftheart approach gain improvement psnr fid respectively
enhancing multimodal llm detailed accurate video captioning using multiround preference optimization video contain wealth information generating detailed accurate description natural language key aspect video understanding paper present videosalmonn advanced audiovisual large language model llm lowrank adaptation lora designed enhanced video paired audio captioning directed preference optimization dpo propose new metric evaluate completeness accuracy video description optimized using dpo improve training introduce novel multiround dpo mrdpo approach involves periodically updating dpo reference model merging reinitializing lora module proxy parameter update training round step incorporating guidance groundtruth video caption stabilize process address potential catastrophic forgetting noncaptioning ability due mrdpo propose rebirth tuning finetunes predpo llm using caption generated mrdpotrained model supervised label experiment show mrdpo significantly enhances videosalmonn captioning accuracy reducing global local error rate respectively decreasing repetition rate final videosalmonn model billion parameter surpasses leading model video captioning task maintaining competitive performance stateoftheart widely used video questionanswering benchmark among model similar size upon acceptance release code model checkpoint training test data demo available
mimicmotion highquality human motion video generation confidenceaware pose guidance recent year generative artificial intelligence achieved significant advancement field image generation spawning variety application however video generation still face considerable challenge various aspect controllability video length richness detail hinder application popularization technology work propose controllable video generation framework dubbed mimicmotion generate highquality video arbitrary length mimicking specific motion guidance compared previous method approach several highlight firstly introduce confidenceaware pose guidance ensures high frame quality temporal smoothness secondly introduce regional loss amplification based pose confidence significantly reduces image distortion lastly generating long smooth video propose progressive latent fusion strategy mean produce video arbitrary length acceptable resource consumption extensive experiment user study mimicmotion demonstrates significant improvement previous approach various aspect detailed result comparison available project page httpstencentgithubiomimicmotion
dragtraffic interactive controllable traffic scene generation autonomous driving evaluating training autonomous driving system require diverse scalable corner case however existing scene generation method lack controllability accuracy versatility resulting unsatisfactory generation result inspired draggan image generation propose dragtraffic generalized interactive controllable traffic scene generation framework based conditional diffusion dragtraffic enables nonexperts generate variety realistic driving scenario different type traffic agent adaptive mixture expert architecture employ regression model provide general initial solution refinement process based conditional diffusion model ensure diversity usercustomized context introduced crossattention ensure high controllability experiment realworld driving dataset show dragtraffic outperforms existing method term authenticity diversity freedom demo video code available httpschantsssgithubiodragtraffic
fleximo towards flexible texttohuman motion video generation current method generating human motion video rely extracting pose sequence reference video restricts flexibility control additionally due limitation pose detection technique extracted pose sequence sometimes inaccurate leading lowquality video output introduce novel task aimed generating human motion video solely reference image natural language approach offer greater flexibility ease use text accessible desired guidance video however training endtoend model task requires million highquality text human motion video pair challenging obtain address propose new framework called fleximo leverage largescale pretrained motion model approach straightforward textgenerated skeleton may consistently match scale reference image may lack detailed information overcome challenge introduce anchor point based rescale method design skeleton adapter fill missing detail bridge gap texttomotion motiontovideo generation also propose video refinement process enhance video quality large language model llm employed decompose natural language discrete motion sequence enabling generation motion video desired length assess performance fleximo introduce new benchmark called motionbench includes video across identity motion also propose new metric motionscore evaluate accuracy motion following qualitative quantitative result demonstrate method outperforms existing textconditioned imagetovideo generation method code model weight made publicly available
radiance field learner uav firstperson viewer firstpersonview fpv hold immense potential revolutionizing trajectory unmanned aerial vehicle uavs offering exhilarating avenue navigating complex building structure yet traditional neural radiance field nerf method face challenge sampling single point per iteration requiring extensive array view supervision uav video exacerbate issue limited viewpoint significant spatial scale variation resulting inadequate detail rendering across diverse scale response introduce fpvnerf addressing challenge three key facet temporal consistency leveraging spatiotemporal continuity ensures seamless coherence frame global structure incorporating various global feature point sampling preserve space integrity local granularity employing comprehensive framework multiresolution supervision multiscale scene feature representation tackle intricacy uav video spatial scale additionally due scarcity publicly available fpv video introduce innovative view synthesis method using nerf generate fpv perspective uav footage enhancing spatial perception drone novel dataset span diverse trajectory outdoor indoor environment uav domain differing significantly traditional nerf scenario extensive experiment encompassing interior exterior building structure fpvnerf demonstrates superior understanding uav flying space outperforming stateoftheart method curated uav dataset explore project page insight httpsfpvnerfgithubio
flip flowcentric generative planning generalpurpose manipulation world model aim develop modelbased planning framework world model scaled increasing model data budget generalpurpose manipulation task language vision input end present flowcentric generative planning flip modelbased planning algorithm visual space feature three key module multimodal flow generation model generalpurpose action proposal module flowconditioned video generation model dynamic module visionlanguage representation learning model value module given initial image language instruction goal flip progressively search longhorizon flow video plan maximize discounted return accomplish task flip able synthesize longhorizon plan across object robot task image flow general action representation dense flow information also provides rich guidance longhorizon video generation addition synthesized flow video plan guide training lowlevel control policy robot execution experiment diverse benchmark demonstrate flip improve success rate quality longhorizon video plan synthesis interactive world model property opening wider application future worksvideo demo website httpsnuslinslabgithubioflipweb
visatronic multimodal decoderonly model speech synthesis paper propose new task generating speech video people transcript vtts motivate new technique multimodal speech generation task generalizes task generating speech cropped lip video also complicated task generating generic audio clip eg dog barking video text multilingual version task could lead new technique crosslingual dubbing also present decoderonly multimodal model task call visatronic model embeds vision text speech directly common subspace transformer model us autoregressive loss learn generative model discretized melspectrograms conditioned speaker video transcript speech embedding modality common subspace visatronic achieve improved result model use text video input present much simpler approach multimodal speech generation compared prevailing approach rely lipdetectors complicated architecture fuse modality producing better result since model flexible enough accommodate different way ordering input sequence carefully explore different strategy better understand best way propagate information generative step facilitate research vtts release code ii clean transcription largescale dataset iii standardized evaluation protocol vtts incorporating objective subjective metric
movie gen swot analysis metas generative ai foundation model transforming medium generation advertising entertainment industry generative ai reshaping medium landscape enabling unprecedented capability video creation personalization scalability paper present comprehensive swot analysis metas movie gen cuttingedge generative ai foundation model designed produce hd video synchronized audio simple text prompt explore strength including highresolution video generation precise editing seamless audio integration make transformative tool across industry filmmaking advertising education however analysis also address limitation constraint video length potential bias generated content pose challenge broader adoption addition examine evolving regulatory ethical consideration surrounding generative ai focusing issue like content authenticity cultural representation responsible use comparative insight leading model like dalle google imagen paper highlight movie gen unique feature video personalization multimodal synthesis identifying opportunity innovation area requiring research finding provide actionable insight stakeholder emphasizing opportunity challenge deploying generative ai medium production work aim guide future advancement generative ai ensuring scalability quality ethical integrity rapidly evolving field
creating controllable portrait casual monocular video creating controllable human portrait casual smartphone video highly desirable due immense value arvr application recent development gaussian splatting shown improvement rendering quality training efficiency however still remains challenge accurately model disentangle head movement facial expression singleview capture achieve highquality rendering paper introduce address challenge represent entire scene including dynamic subject using set gaussians canonical space using set control signal head pose expression transform space learned deformation generate desired rendering key innovation carefully designed deformation method guided learnable prior derived morphable model approach highly efficient training effective controlling facial expression head position view synthesis across various capture demonstrate effectiveness learned deformation extensive quantitative qualitative experiment project page found
gan skip patch discriminator biological electron microscopy image generation generating realistic electron microscopy em image challenging problem due complex global local structure isola et al proposed conditional generative adversarial network gan general purpose imagetoimage translation fails generate realistic em image propose new architecture discriminator gan providing access multiple patch size using skip patch generating realistic em image
generalized grounded temporal reasoning robot instruction following combining large pretrained model consider scenario human clean table robot observing scene instructed task remove cloth using wiped table instruction following temporal reasoning requires robot identify relevant past object interaction ground object interest present scene execute task according human instruction directly grounding utterance referencing past interaction grounded object challenging due multihop nature reference past interaction large space object grounding video stream observing robot workspace key insight factor temporal reasoning task estimating video interval associated event reference ii performing spatial reasoning interaction frame infer intended object iii semantically track object location till current scene enable future robot interaction approach leverage existing large pretrained model possess inherent generalization capability combine appropriately temporal grounding task evaluation videolanguage corpus acquired robot manipulator displaying rich temporal interaction spatiallycomplex scene display average accuracy dataset code video available httpsreailiitdelhigithubiotemporalreasoninggithubio
taming rectified flow inversion editing rectifiedflowbased diffusion transformer like flux opensora demonstrated outstanding performance field image video generation despite robust generative capability model often struggle inversion inaccuracy could limit effectiveness downstream task image video editing address issue propose rfsolver novel trainingfree sampler effectively enhances inversion precision mitigating error odesolving process rectified flow specifically derive exact formulation rectified flow ode apply highorder taylor expansion estimate nonlinear component significantly enhancing precision ode solution timestep building upon rfsolver propose rfedit general featuresharingbased framework image video editing incorporating selfattention feature inversion process editing process rfedit effectively preserve structural information source image video achieving highquality editing result approach compatible pretrained rectifiedflowbased model image video task requiring additional training optimization extensive experiment across generation inversion editing task image video modality demonstrate superiority versatility method source code available
motion query identitymotion tradeoff texttovideo generation texttovideo diffusion model shown remarkable progress generating coherent video clip textual description however interplay motion structure identity representation model remains underexplored investigate selfattention query feature aka q feature simultaneously govern motion structure identity examine challenge arising representation interact analysis reveals q affect layout denoising q also strong effect subject identity making hard transfer motion without sideeffect transferring identity understanding dual role enabled u control query feature injection q injection demonstrate two application zeroshot motion transfer method time efficient existing approach trainingfree technique consistent multishot video generation character maintain identity across multiple video shot q injection enhances motion fidelity
lmagic language model assisted generation image coherence current era generative ai breakthrough generating panoramic scene single input image remains key challenge existing method use diffusionbased iterative simultaneous multiview inpainting however lack global scene layout prior lead subpar output duplicated object eg multiple bed bedroom requires timeconsuming human text input view propose lmagic novel method leveraging large language model guidance diffusing multiple coherent view degree panoramic scene lmagic harness pretrained diffusion language model without finetuning ensuring zeroshot performance output quality enhanced superresolution multiview fusion technique extensive experiment demonstrate resulting panoramic scene feature better scene layout perspective view rendering quality compared related work preference human evaluation combined conditional diffusion model lmagic accept various input modality including limited text depth map sketch colored script applying depth estimation enables point cloud generation dynamic scene exploration fluid camera motion code available httpsgithubcomintellabsmmpano video presentation available
facenhance facial expression enhancing recurrent ddpms facial expression vital nonverbal human communication found application various computer vision field like virtual reality gaming emotional ai assistant despite advancement many facial expression generation model encounter challenge low resolution eg pixel poor quality absence background detail paper introduce facenhance novel diffusionbased approach addressing constraint existing lowresolution facial expression generation model facenhance enhances lowresolution facial expression video pixel higher resolution pixel incorporating background detail improving overall quality leveraging conditional denoising within diffusion framework guided backgroundfree lowresolution video single neutral expression highresolution image facenhance generates video incorporating facial expression lowresolution video performed individual background neutral image complementing lightweight lowresolution model facenhance strike balance computational efficiency desirable image resolution quality extensive experiment mug facial expression database demonstrate efficacy facenhance enhancing lowresolution model output stateoftheart quality preserving content identity consistency facenhance represents significant progress towards resourceefficient highfidelity facial expression generation renewing outdated lowresolution method uptodate standard
pom efficient image video generation polynomial mixer diffusion model based multihead attention mha become ubiquitous generate high quality image video however encoding image video sequence patch result costly attention pattern requirement term memory compute grow quadratically alleviate problem propose dropin replacement mha called polynomial mixer pom benefit encoding entire sequence explicit state pom linear complexity respect number token explicit state also allows u generate frame sequential fashion minimizing memory compute requirement still able train parallel show polynomial mixer universal sequencetosequence approximator like regular mha adapt several diffusion transformer dit generating image video pom replacing mha obtain high quality sample using less computational resource code available httpsgithubcomdavidpicardhomm
cpa cameraposeawareness diffusion transformer video generation despite significant advancement made diffusion transformer ditbased method video generation remains notable gap controllable camera pose perspective existing work opensora adhere precisely anticipated trajectory physical interaction thereby limiting flexibility downstream application alleviate issue introduce cpa unified cameraposeawareness texttovideo generation approach elaborates camera movement integrates textual visual spatial condition specifically deploy sparse motion encoding sme module transform camera pose information spatialtemporal embedding activate temporal attention injection tai module inject motion patch stdit block plugin architecture accommodates original dit parameter facilitating diverse type camera pose flexible object movement extensive qualitative quantitative experiment demonstrate method outperforms ldmbased method long video generation achieving optimal performance trajectory consistency object consistency
thisthat languagegesture controlled video generation robot planning propose robot learning method communicating planning executing wide range task dubbed thisthat achieve robot planning general task leveraging power video generative model trained internetscale data containing rich physical semantic context work tackle three fundamental challenge videobased planning unambiguous task communication simple human instruction controllable video generation respect user intent translating visual planning robot action propose languagegesture conditioning generate video simpler clearer existing languageonly method especially complex uncertain environment suggest behavioral cloning design seamlessly incorporates video plan thisthat demonstrates stateoftheart effectiveness addressing three challenge justifies use video generation intermediate representation generalizable task planning execution project website
lightvqa video quality assessment model exposure correction visionlanguage guidance recently usergenerated content ugc video gained popularity daily life however ugc video often suffer poor exposure due limitation photographic equipment technique therefore video exposure correction vec algorithm proposed lowlight video enhancement llve overexposed video recovery oevr included equally important vec video quality assessment vqa unfortunately almost existing vqa model built generally measuring quality video comprehensive perspective result lightvqa trained llveqa proposed assessing llve extend work lightvqa expanding llveqa dataset video exposure correction quality assessment vecqa dataset overexposed video corresponding corrected version addition propose lightvqa vqa model specialized assessing vec lightvqa differs lightvqa mainly usage clip model visionlanguage guidance feature extraction followed new module referring human visual system hvs accurate assessment extensive experimental result show model achieves best performance current stateoftheart sota vqa model vecqa dataset public datasets
auroracap efficient performant video detailed captioning new benchmark video detailed captioning key task aim generate comprehensive coherent textual description video content benefiting video understanding generation paper propose auroracap video captioner based large multimodal model follow simplest architecture design without additional parameter temporal modeling address overhead caused lengthy video sequence implement token merging strategy reducing number input visual token surprisingly found strategy result little performance loss auroracap show superior performance various video image captioning benchmark example obtaining cider beating pro however existing video caption benchmark include simple description consisting dozen word limit research field therefore develop vdc video detailed captioning benchmark one thousand carefully annotated structured caption addition propose new llmassisted metric vdcscore bettering evaluation adopts divideandconquer strategy transform long caption evaluation multiple short questionanswer pair help human elo ranking experiment show benchmark better correlate human judgment video detailed captioning quality
safewatch efficient safetypolicy following video guardrail model transparent explanation rise generative ai rapid growth highquality video generation video guardrail become crucial ever ensure safety security across platform current video guardrail however either overly simplistic relying pure classification model trained simple policy limited unsafe category lack detailed explanation prompting multimodal large language model mllms long safety guideline inefficient impractical guardrailing realworld content bridge gap propose safewatch efficient mllmbased video guardrail model designed follow customized safety policy provide multilabel video guardrail output contentspecific explanation zeroshot manner particular unlike traditional mllmbased guardrail encode safety policy autoregressively causing inefficiency bias safewatch uniquely encodes policy chunk parallel eliminates position bias policy attended simultaneously equal importance addition improve efficiency accuracy safewatch incorporates policyaware visual token pruning algorithm adaptively selects relevant video token policy discarding noisy irrelevant information allows focused policycompliant guardrail significantly reduced computational overhead considering limitation existing video guardrail benchmark propose safewatchbench largescale video guardrail benchmark comprising video spanning six safety category cover task ensure comprehensive coverage potential safety scenario safewatch outperforms sota safewatchbench benchmark cut cost delivers toptier explanation validated llm human review
video anomaly detection motion appearance guided patch diffusion model recent endeavor one class video anomaly detection leverage diffusion model posit task generation problem diffusion model trained recover normal pattern exclusively thus reporting abnormal pattern outlier yet existing attempt neglect various formation anomaly predict normal sample feature level regardless abnormal object surveillance video often relatively small address novel patchbased diffusion model proposed specifically engineered capture finegrained local information observe anomaly video manifest deviation appearance motion therefore argue comprehensive solution must consider aspect simultaneously achieve accurate frame prediction address introduce innovative motion appearance condition seamlessly integrated patch diffusion model condition designed guide model generating coherent contextually appropriate prediction semantic content motion relation experimental result four challenging video anomaly detection datasets empirically substantiate efficacy proposed approach demonstrating consistently outperforms existing method detecting abnormal behavior
generation hybrid prior due fascinating generative performance texttoimage diffusion model growing generation work explore distilling generative prior using score distillation sampling sd loss bypass data scarcity problem existing method achieved promising result realism consistency generation still face challenge including lack realism insufficient dynamic motion paper propose novel method generation ensures dynamic amplitude authenticity direct supervision provided video prior specifically adopt texttovideo diffusion model generate reference video divide generation two stage static generation dynamic generation static generation achieved guidance input text first frame reference video dynamic generation stage introduce customized sd loss ensure multiview consistency videobased sd loss improve temporal consistency importantly direct prior reference video ensure quality geometry texture moreover design priorswitching training strategy avoid conflict different prior fully leverage benefit prior addition enrich generated motion introduce dynamic modeling representation composed deformation network topology network ensures dynamic continuity modeling topological change method support generation also enables generation monocular video comparison experiment demonstrate superiority method compared existing method
autovfx physically realistic video editing natural language instruction modern visual effect vfx software made possible skilled artist create imagery virtually anything however creation process remains laborious complex largely inaccessible everyday user work present autovfx framework automatically creates realistic dynamic vfx video single video natural language instruction carefully integrating neural scene modeling llmbased code generation physical simulation autovfx able provide physicallygrounded photorealistic editing effect controlled directly using natural language instruction conduct extensive experiment validate autovfxs efficacy across diverse spectrum video instruction quantitative qualitative result suggest autovfx outperforms competing method large margin generative quality instruction alignment editing versatility physical plausibility
storyagent customized storytelling video generation via multiagent collaboration advent aigenerated content aigc spurred research automated video generation streamline conventional process however automating storytelling video production particularly customized narrative remains challenging due complexity maintaining subject consistency across shot existing approach like mora aesopagent integrate multiple agent storytovideo generation fall short preserving protagonist consistency supporting customized storytelling video generation csvg address limitation propose storyagent multiagent framework designed csvg storyagent decomposes csvg distinct subtasks assigned specialized agent mirroring professional production process notably framework includes agent story design storyboard generation video creation agent coordination result evaluation leveraging strength different model storyagent enhances control generation process significantly improving character consistency specifically introduce customized imagetovideo method lorabe enhance intrashot temporal consistency novel storyboard generation pipeline proposed maintain subject consistency across shot extensive experiment demonstrate effectiveness approach synthesizing highly consistent storytelling video outperforming stateoftheart method contribution include introduction storyagent versatile framework video generation task novel technique preserving protagonist consistency
anisora exploring frontier animation video generation sora era animation gained significant interest recent film tv industry despite success advanced video generation model like sora kling cogvideox generating natural video lack effectiveness handling animation video evaluating animation video generation also great challenge due unique artist style violating law physic exaggerated motion paper present comprehensive system anisora designed animation video generation includes data processing pipeline controllable generation model evaluation dataset supported data processing pipeline highquality data generation model incorporates spatiotemporal mask module facilitate key animation production function imagetovideo generation frame interpolation localized imageguided animation also collect evaluation benchmark various animation video evaluation vbench human doubleblind test demonstrates consistency character motion achieving stateoftheart result animation video generation evaluation benchmark publicly available httpsgithubcombilibiliindexanisora
longduration highresolution audiodriven portrait image animation recent advance latent diffusionbased generative model portrait image animation hallo achieved impressive result shortduration video synthesis paper present update hallo introducing several design enhancement extend capability first extend method produce longduration video address substantial challenge appearance drift temporal artifact investigate augmentation strategy within image space conditional motion frame specifically introduce patchdrop technique augmented gaussian noise enhance visual consistency temporal coherence long duration second achieve resolution portrait video generation accomplish implement vector quantization latent code apply temporal alignment technique maintain coherence across temporal dimension integrating highquality decoder realize visual synthesis resolution third incorporate adjustable semantic textual label portrait expression conditional input extends beyond traditional audio cue improve controllability increase diversity generated content best knowledge proposed paper first method achieve resolution generate hourlong audiodriven portrait image animation enhanced textual prompt conducted extensive experiment evaluate method publicly available datasets including hdtf celebv introduced wild dataset experimental result demonstrate approach achieves stateoftheart performance longduration portrait video animation successfully generating rich controllable content resolution duration extending ten minute project page
unified framework intra interframe video compression video compression aim reconstruct seamless frame encoding motion residual information existing frame previous neural video compression method necessitate distinct codecs three type frame iframe pframe bframe hinders unified approach generalization across different video context intracodec technique lack advanced motion estimation motion compensation memc found intercodec leading fragmented framework lacking uniformity proposed intra interframe video compression framework employ single spatiotemporal codec guide feature compression rate according content importance unified codec transforms dependence across frame conditional coding scheme thus integrating intra interframe compression one cohesive strategy given absence explicit motion data achieving competent interframe compression conditional codec pose challenge resolve approach includes implicit interframe alignment mechanism pretrained diffusion denoising process utilization diffusioninverted reference feature rather random noise support initial compression state process allows selective denoising motionrich region based decoded feature facilitating accurate alignment without need memc experimental finding across various compression configuration ai ld ra frame type prove outperforms stateoftheart perceptual learned codecs impressively exhibit enhancement perceptual reconstruction performance benchmarked standard vtm official implementation found
dtsgan learning dynamic texture via spatiotemporal generative adversarial network dynamic texture synthesis aim generate sequence visually similar reference video texture exhibit specific stationary property time paper introduce spatiotemporal generative adversarial network dtsgan learn single dynamic texture capturing motion content distribution pipeline dtsgan new video sequence generated coarsest scale finest one avoid mode collapse propose novel strategy data update help improve diversity generated result qualitative quantitative experiment show model able generate high quality dynamic texture natural motion
accelerated imageaware generative diffusion modeling propose paper analytically new construct diffusion model whose drift diffusion parameter yield exponentially timedecaying signal noise ratio forward process reverse construct cleverly carry learning diffusion coefficient structure clean image using autoencoder proposed methodology significantly accelerates diffusion process reducing required diffusion time step around seen conventional model without compromising image quality reversetime diffusion departure conventional model typically use timeconsuming multiple run introduce parallel datadriven model generate reversetime diffusion trajectory single run model resulting collective blocksequential generative model eliminates need mcmcbased subsampling correction safeguarding improving image quality improve acceleration image generation collectively advancement yield generative model order magnitude faster conventional approach maintaining high fidelity diversity generated image hence promising widespread applicability rapid image synthesis task
literature review fetus brain motion correction mri paper provides comprehensive review latest advancement fetal motion correction mri delve various contemporary methodology technological advancement aimed overcoming challenge includes traditional fetal mri correction method like slice volume registration svr deep learningbased technique convolutional neural network cnns long shortterm memory lstm network transformer generative adversarial network gans recent advancement diffusion model insight derived literature review reflect thorough understanding technical intricacy practical implication fetal motion mri study offering reasoned perspective potential solution future improvement field
disrupting style mimicry attack video imagery generative ai model often used perform mimicry attack pretrained model finetuned small sample image learn mimic specific artist interest researcher introduced multiple antimimicry protection tool mist glaze antidreambooth recent evidence point growing trend mimicry model using video source training data paper present experience exploring technique disrupt style mimicry video imagery first validate mimicry attack succeed training individual frame extracted video show antimimicry tool offer protection applied individual frame approach vulnerable adaptive countermeasure remove protection exploiting randomness optimization result consecutive nearlyidentical frame develop new toolagnostic framework segment video short scene based framelevel similarity use perscene optimization baseline remove interframe randomization reducing computational cost show via image level metric endtoend user study resulting protection restores protection mimicry including countermeasure finally develop another adaptive countermeasure find fall short framework
global motion understanding largescale video object segmentation paper show transferring knowledge domain video understanding combined largescale learning improve robustness video object segmentation vos complex circumstance namely focus integrating scene global motion knowledge improve largescale semisupervised video object segmentation prior work vos mostly rely direct comparison semantic contextual feature perform dense matching current past frame passing actual motion structure hand optical flow estimation task aim approximate scene motion field exposing global motion pattern typically undiscoverable pair similarity search present warpformer architecture semisupervised video object segmentation exploit existing knowledge motion understanding conduct smoother propagation accurate matching framework employ generic pretrained optical flow estimation network whose prediction used warp past frame instance segmentation mask current frame domain consequently warped segmentation mask refined fused together aiming inpaint occluded region eliminate artifact caused flow field imperfect additionally employ novel largescale mose dataset train model various complex scenario method demonstrates strong performance davis validation davis testdev youtubevos validation competitive alternative stateoftheart method using much simpler memory mechanism instance understanding logic
comprehensive benchmark compositional texttovideo generation texttovideo generative model advanced significantly yet ability compose different object attribute action motion video remains unexplored previous texttovideo benchmark also neglect important ability evaluation work conduct first systematic study compositional texttovideo generation propose first benchmark tailored compositional texttovideo generation encompasses diverse aspect compositionality including consistent attribute binding dynamic attribute binding spatial relationship motion binding action binding object interaction generative numeracy carefully design evaluation metric multimodal large language model mllmbased detectionbased trackingbased metric better reflect compositional texttovideo generation quality seven proposed category text prompt effectiveness proposed metric verified correlation human evaluation also benchmark various texttovideo generative model conduct indepth analysis across different model various compositional category find compositional texttovideo generation highly challenging current model hope attempt could shed light future research direction
synthesizing audio silent video using sequence sequence modeling generating audio video visual context multiple practical application improving interact audiovisual medium example enhancing cctv footage analysis restoring historical video eg silent movie improving video generation model propose novel method generate audio video using sequencetosequence model improving prior work used cnns wavenet faced sound diversity generalization challenge approach employ vector quantized variational autoencoder vqvae capture video spatial temporal structure decoding custom audio decoder broader range sound trained dataset segment focusing specific domain model aim enhance application like cctv footage analysis silent movie restoration video generation model
method software tool generating artificial database biomedical image based deep neural network wide variety biomedical image data well method generating training image using basic deep neural network analyzed additionally platform creating image analyzed considering characteristic article develops method generating artificial biomedical image based gan gan architecture developed biomedical image synthesis data foundation module generating training image designed implemented software system comparison generated image database known database made
gvdiff grounded texttovideo generation diffusion model texttovideo generation significant attention directed toward development yet unifying discrete continuous grounding condition generation remains underexplored paper proposes grounded texttovideo generation framework termed gvdiff first inject grounding condition selfattention uncertaintybased representation explicitly guide focus network second introduce spatialtemporal grounding layer connects grounding condition target object enables model grounded generation capacity spatialtemporal domain third dynamic gate network adaptively skip redundant grounding process selectively extract grounding information semantics improving efficiency extensively evaluate grounded generation capacity gvdiff demonstrate versatility application including longrange video generation sequential prompt objectspecific editing
audiosynchronized visual animation current visual generation method produce high quality video guided text however effectively controlling object dynamic remains challenge work explores audio cue generate temporally synchronized image animation introduce audio synchronized visual animation asva task animating static image demonstrate motion dynamic temporally guided audio clip across multiple class end present dataset curated vggsound video featuring synchronized audio visual event across category also present diffusion model avsyncd capable generating dynamic animation guided audio extensive evaluation validate reliable benchmark synchronized generation demonstrate model superior performance explore avsyncds potential variety audio synchronized generation task generating full video without base image controlling object motion various sound hope established benchmark open new avenue controllable visual generation video project webpage httpslzhangbjgithubioprojectsasvaasvahtml
discrete continuous generating smooth transition pose sign language observation generating continuous sign language video discrete segment challenging due need smooth transition preserve natural flow meaning traditional approach simply concatenate isolated sign often result abrupt transition disrupting video coherence address propose novel framework employ conditional diffusion model synthesize contextually smooth transition frame enabling seamless construction continuous sign language sequence approach transforms unsupervised problem transition frame generation supervised training task simulating absence transition frame random masking segment longduration sign video model learns predict masked frame denoising gaussian noise conditioned surrounding sign observation allowing handle complex unstructured transition inference apply linearly interpolating padding strategy initializes missing frame interpolation boundary frame providing stable foundation iterative refinement diffusion model extensive experiment datasets demonstrate effectiveness method producing continuous natural sign language video
dual encoder gan inversion highfidelity head reconstruction single image gan inversion aim project single image latent space generative adversarial network gan thereby achieving geometry reconstruction exist encoders achieve good result gan inversion predominantly built specializes synthesizing nearfrontal view limiting synthesizing comprehensive scene diverse viewpoint contrast existing approach propose novel framework built panohead excels synthesizing image perspective achieve realistic modeling input image introduce dual encoder system tailored highfidelity reconstruction realistic generation different viewpoint accompanying propose stitching framework triplane domain get best prediction achieve seamless stitching encoders must output consistent result despite specialized different task reason carefully train encoders using specialized loss including adversarial loss based novel occlusionaware triplane discriminator experiment reveal approach surpasses existing encoder training method qualitatively quantitatively please visit project page
stair spatialtemporal reasoning auditable intermediate result video question answering recently witnessed rapid development video question answering model however model handle simple video term temporal reasoning performance tends drop answering temporalreasoning question long informative video tackle problem propose stair spatialtemporal reasoning model auditable intermediate result video question answering stair neural module network contains program generator decompose given question hierarchical combination several subtasks set lightweight neural module complete subtasks though neural module network already widely studied imagetext task applying video nontrivial task reasoning video requires different ability paper define set basic videotext subtasks video question answering design set lightweight module complete different prior work module stair return intermediate output specific intention instead always returning attention map make easier interpret collaborate pretrained model also introduce intermediate supervision make intermediate output accurate conduct extensive experiment several video question answering datasets various setting show stair performance explainability compatibility pretrained model applicability program annotation available code httpsgithubcomyellowbinarytreestair
nerv enhanced implicit neural video representation neural field also known implicit neural representation inr shown remarkable capability representing generating manipulating various data type allowing continuous data reconstruction low memory footprint though promising inr applied video compression still need improve ratedistortion performance large margin require huge number parameter long training iteration capture highfrequency detail limiting wider applicability resolving problem remains quite challenging task would make inr accessible compression task take step towards resolving shortcoming introducing neural representation video nerv enhanced implicit neural video representation straightforward yet effective enhancement original nerv decoder architecture featuring separable residual block scrbs sandwich upsampling block ub bilinear interpolation skip layer improved feature representation nerv allows video directly represented function approximated neural network significantly enhance representation capacity beyond current inrbased video codecs evaluate method uvg mcl jvc bunny datasets achieving competitive result video compression inr achievement narrow gap autoencoderbased video coding marking significant stride inrbased video compression research
vqnerv vector quantized neural representation video implicit neural representation inr excel encoding video within neural network showcasing promise computer vision task like video compression denoising inrbased approach reconstruct video frame contentagnostic embeddings hamper efficacy video frame regression restricts generalization ability video interpolation address deficiency hybrid neural representation video hnerv introduced contentadaptive embeddings nevertheless hnervs compression ratio remain relatively low attributable oversight leveraging network shallow feature interframe residual information work introduce advanced ushaped architecture vector quantizednerv vqnerv integrates novel componentthe vqnerv block block incorporates codebook mechanism discretize network shallow residual feature interframe residual information effectively approach prof particularly advantageous video compression result smaller size compared quantized feature furthermore introduce original codebook optimization technique termed shallow codebook optimization designed refine utility efficiency codebook experimental evaluation indicate vqnerv outperforms hnerv video regression task delivering superior reconstruction quality increase db peak signaltonoise ratio psnr better bit per pixel bpp efficiency improved video inpainting outcome
versatile deep visualaudio watermarking manipulation localization copyright protection aigenerated video revolutionized short video production filmmaking personalized medium making video local editing essential tool however progress also blur line reality fiction posing challenge multimedia forensics solve urgent issue proposed address limitation current video tampering forensics poor generalizability singular function single modality focus combining fragility videointovideo steganography deep robust watermarking method embed invisible visualaudio localization watermark copyright watermark original video frame audio enabling precise manipulation localization copyright protection also design temporal alignment fusion module degradation prompt learning enhance localization accuracy decoding robustness meanwhile introduce samplelevel audio localization method crossmodal copyright extraction mechanism couple information audio video frame effectiveness verified visualaudio tampering dataset emphasizing superiority localization precision copyright accuracy crucial sustainable development video editing aigc video era
mllm video narrator mitigating modality imbalance video moment retrieval video moment retrieval vmr aim localize specific temporal segment within untrimmed long video given natural language query existing method often suffer inadequate training annotation ie sentence typically match fraction prominent video content foreground limited wording diversity intrinsic modality imbalance leaf considerable portion visual information remaining unaligned text confines crossmodal alignment knowledge within scope limited text corpus thereby leading suboptimal visualtextual modeling poor generalizability leveraging visualtextual understanding capability multimodal large language model mllm work take mllm video narrator generate plausible textual description video thereby mitigating modality imbalance boosting temporal localization effectively maintain temporal sensibility localization design get text narrative certain video timestamp construct structured text paragraph time information temporally aligned visual content perform crossmodal feature merging temporalaware narrative corresponding video temporal feature produce semanticenhanced video representation sequence query localization subsequently introduce unimodal narrativequery matching mechanism encourages model extract complementary information contextual cohesive description improved retrieval extensive experiment two benchmark show effectiveness generalizability proposed method
cuboidnet multibranch convolutional neural network joint spacetime video super resolution demand highresolution video consistently rising across various domain propelled continuous advancement science technology societal nonetheless challenge arising limitation imaging equipment capability imaging condition well economic temporal factor often result obtaining lowresolution image particular situation spacetime video superresolution aim enhance spatial temporal resolution lowresolution lowframerate video currently available spacetime video superresolution method often fail fully exploit abundant information existing within spatiotemporal domain address problem tackle issue conceptualizing input lowresolution video cuboid structure drawing perspective introduce innovative methodology called cuboidnet incorporates multibranch convolutional neural network cuboidnet designed collectively enhance spatial temporal resolution video enabling extraction rich meaningful information across spatial temporal dimension specifically take input video cuboid generate different directional slice input different branch network proposed network contains four module ie multibranchbased hybrid feature extraction mbfe module multibranchbased reconstruction mbr module first stage quality enhancement qe module second stage cross frame quality enhancement cfqe module interpolated frame experimental result demonstrate proposed method effective spatial temporal superresolution video also spatial angular superresolution light field
videollamb longcontext video understanding recurrent memory bridge recent advancement largescale videolanguage model shown significant potential realtime planning detailed interaction however high computational demand scarcity annotated datasets limit practicality academic researcher work introduce videollamb novel framework utilizes temporal memory token within bridge layer allow encoding entire video sequence alongside historical visual data effectively preserving semantic continuity enhancing model performance across various task approach includes recurrent memory token scenetilling algorithm segment video independent semantic unit preserve semantic integrity empirically videollamb significantly outstrips existing videolanguage model demonstrating point improvement competitor across three videoqa benchmark point egocentric planning comprehensive result mvbench show achieves markedly better result previous model llm remarkably maintains robust performance pllava even video length increase time besides frame retrieval result specialized needle video haystack niavh benchmark validate videollambs prowess accurately identifying specific frame within lengthy video scenetilling algorithm also enables generation streaming video caption directly without necessitating additional training term efficiency videollamb trained frame support frame single nvidia gpu linear gpu memory scaling ensuring high performance costeffectiveness thereby setting new foundation longform videolanguage model academic practical application
lowcomputational video synopsis framework standard dataset video synopsis efficient method condensing surveillance video technique begin detection tracking object followed creation object tube tube consist sequence containing chronologically ordered bounding box unique object generate condensed video first step involves rearranging object tube maximize number nonoverlapping object frame tube stitched background image extracted source video lack standard dataset video synopsis task hinders comparison different video synopsis model paper address issue introducing standard dataset called synoclip designed specifically video synopsis task synoclip includes necessary feature needed evaluate various model directly effectively additionally work introduces video synopsis model called fgs low computational cost model includes emptyframe object detector identify frame empty object facilitating efficient utilization deep object detector moreover tube grouping algorithm proposed maintain relationship among tube synthesized video followed greedy tube rearrangement algorithm efficiently determines start time tube finally proposed model evaluated using proposed dataset source code finetuned object detection model tutorial available httpsgithubcomramtinmavideosynopsisfgs
deepfake detection video multiple face using geometricfakeness feature due development facial manipulation technique recent year deepfake detection video stream became important problem face biometrics brand monitoring online video conferencing solution case biometric authentication replace real datastream deepfake bypass liveness detection system using deepfake video conference penetrate private meeting deepfakes victim public figure also used fraudsters blackmailing extorsion financial fraud therefore task detecting deepfakes relevant ensuring privacy security existing approach deepfake detection performance deteriorates multiple face present video simultaneously object erroneously classified face research propose use geometricfakeness feature gff characterize dynamic degree face presence video perframe deepfake score analyze temporal inconsistency gffs frame train complex deep learning model output final deepfake prediction employ approach analyze video multiple face simultaneously present video video often occur practice eg online video conference case real face appearing frame together deepfake face significantly affect deepfake detection approach allows counter problem extensive experiment demonstrate approach outperforms current stateoftheart method popular benchmark datasets faceforensics dfdc celebdf wilddeepfake proposed approach remains accurate trained detect multiple different deepfake generation technique
free videollm promptguided visual perception efficient trainingfree video llm visionlanguage large model achieved remarkable success various multimodal task yet applying video understanding remains challenging due inherent complexity computational demand video data trainingbased videollms deliver high performance often require substantial resource training inference conversely trainingfree approach offer efficient alternative adapting pretrained imagellms model video task without additional training face inference efficiency bottleneck due large number visual token generated video frame work present novel promptguided visual perception framework abbreviated free videollm efficient inference trainingfree video llm proposed framework decouples spatialtemporal dimension performs temporal frame sampling spatial roi cropping respectively based taskspecific prompt method effectively reduces number visual token maintaining high performance across multiple video questionanswering benchmark extensive experiment demonstrate approach achieves competitive result significantly fewer token offering optimal tradeoff accuracy computational efficiency compared stateoftheart video llm code available httpsgithubcomcontrastivefreevideollm
videorag visuallyaligned retrievalaugmented long video comprehension existing large videolanguage model lvlms struggle comprehend long video correctly due limited context address problem finetuning longcontext lvlms employing gptbased agent emerged promising solution however finetuning lvlms would require extensive highquality data substantial gpu resource gptbased agent would rely proprietary model eg paper propose video retrievalaugmented generation videorag trainingfree costeffective pipeline employ visuallyaligned auxiliary text help facilitate crossmodality alignment providing additional information beyond visual content specifically leverage opensource external tool extract visuallyaligned information pure video data eg audio optical character object detection incorporate extracted information existing lvlm auxiliary text alongside video frame query plugandplay manner videorag offer several key advantage lightweight low computing overhead due singleturn retrieval ii easy implementation compatibility lvlm iii significant consistent performance gain across long video understanding benchmark including videomme mlvu longvideobench notably model demonstrates superior performance proprietary model like utilized model
neptune long orbit benchmarking long video understanding introduce neptune benchmark long video understanding requires reasoning long time horizon across different modality many existing video datasets model focused short clip long video datasets exist often solved powerful image model applied per frame often frame video usually manually annotated high cost order mitigate problem propose scalable dataset creation pipeline leverage large model vlms llm automatically generate dense timealigned video caption well tough question answer decoy set video segment minute length dataset neptune cover broad range long video reasoning ability consists subset emphasizes multimodal reasoning since existing metric openended question answering either rulebased may rely proprietary model provide new open source modelbased metric gem score openended response neptune benchmark evaluation reveal current opensource long video model perform poorly neptune particularly question testing temporal ordering counting state change neptune aim spur development advanced model capable understanding long video dataset available httpsgithubcomgoogledeepmindneptune
sampleefficient unsupervised policy cloning ensemble selfsupervised labeled video current advanced policy learning methodology demonstrated ability develop expertlevel strategy provided enough information however requirement including taskspecific reward expertlabeled trajectory huge environmental interaction expensive even unavailable many scenario contrast human efficiently acquire skill within trial error imitating easily accessible internet video absence supervision paper try let machine replicate efficient watchingandlearning process unsupervised policy ensemble selfsupervised labeled video upesv novel framework efficiently learn policy video without expert supervision upesv train video labeling model infer expert action expert video several organically combined selfsupervised task task performs duty together enable model make full use expert video rewardfree interaction advanced dynamic understanding robust prediction simultaneously upesv clone policy labeled expert video turn collecting environmental interaction selfsupervised task sampleefficient unsupervised ie rewardfree training process advanced videoimitated policy obtained extensive experiment sixteen challenging procedurallygenerated environment demonstrate proposed upesv achieves stateoftheart fewshot policy learning outperforming five current advanced baseline task without exposure supervision except video detailed analysis also provided verifying necessity selfsupervised task employed upesv
onthefly training gaussians efficient streaming photorealistic freeviewpoint video constructing photorealistic freeviewpoint video fvvs dynamic scene multiview video remains challenging endeavor despite remarkable advancement achieved current neural rendering technique method generally require complete video sequence offline training capable realtime rendering address constraint introduce method designed efficient fvv streaming realworld dynamic scene method achieves fast onthefly perframe reconstruction within second realtime rendering fps specifically utilize gaussians represent scene instead naive approach directly optimizing perframe employ compact neural transformation cache ntc model translation rotation markedly reducing training time storage required fvv frame furthermore propose adaptive addition strategy handle emerging object dynamic scene experiment demonstrate achieves competitive performance term rendering speed image quality training time model storage compared stateoftheart method
videogenofthought stepbystep generating multishot video minimal manual intervention current video generation model excel short clip fail produce cohesive multishot narrative due disjointed visual dynamic fractured storyline existing solution either rely extensive manual scriptingediting prioritize singleshot fidelity crossscene continuity limiting practicality movielike content introduce videogenofthought vgot stepbystep framework automates multishot video synthesis single sentence systematically addressing three core challenge narrative fragmentation existing method lack structured storytelling propose dynamic storyline modeling first convert user prompt concise shot description elaborates detailed cinematic specification across five domain character dynamic background continuity relationship evolution camera movement hdr lighting ensuring logical narrative progression selfvalidation visual inconsistency existing approach struggle maintaining visual consistency across shot identityaware crossshot propagation generates identitypreserving portrait ipp token maintain character fidelity allowing trait variation expression aging dictated storyline transition artifact abrupt shot change disrupt immersion adjacent latent transition mechanism implement boundaryaware reset strategy process adjacent shot feature transition point enabling seamless visual flow preserving narrative continuity vgot generates multishot video outperform stateoftheart baseline withinshot face consistency style consistency achieving better crossshot consistency fewer manual adjustment alternative
motionbank largescale video motion benchmark disentangled rulebased annotation paper tackle problem build benchmark large motion model lmm ultimate goal lmm serve foundation model versatile motionrelated task eg human motion generation interpretability generalizability though advanced recent lmmrelated work still limited smallscale motion data costly text description besides previous motion benchmark primarily focus pure body movement neglecting ubiquitous motion context ie human interacting human object scene address limitation consolidate largescale video action datasets knowledge bank build motionbank comprises video action datasets motion sequence frame natural diverse human motion different laboratorycaptured motion inthewild humancentric video contain abundant motion context facilitate better motion text alignment also meticulously devise motion caption generation algorithm automatically produce rulebased unbiased disentangled text description via kinematic characteristic motion extensive experiment show motionbank beneficial general motionrelated task human motion generation motion incontext generation motion understanding video motion together rulebased text annotation could serve efficient alternative larger lmms dataset code benchmark publicly available httpsgithubcomliangxuymotionbank
pemfvto pointenhanced video virtual tryon via maskfree paradigm video virtual tryon aim seamlessly transfer reference garment onto target person video preserving visual fidelity temporal coherence existing method typically rely inpainting mask define tryon area enabling accurate garment transfer simple scene eg inshop video however maskbased approach struggle complex realworld scenario overly large inconsistent mask often destroy spatialtemporal information leading distorted result maskfree method alleviate issue face challenge accurately determining tryon area especially video dynamic body movement address limitation propose pemfvto novel pointenhanced maskfree video virtual tryon framework leverage sparse point alignment explicitly guide garment transfer key innovation introduction pointenhanced guidance provides flexible reliable control spatiallevel garment transfer temporallevel video coherence specifically design pointenhanced transformer pet two core component pointenhanced spatial attention psa us framecloth point alignment precisely guide garment transfer pointenhanced temporal attention pta leverage frameframe point correspondence enhance temporal coherence ensure smooth transition across frame extensive experiment demonstrate pemfvto outperforms stateoftheart method generating natural coherent visually appealing tryon video particularly challenging inthewild scenario link paper homepage httpspemfvtogithubio
contextualized diffusion model textguided image video generation conditional diffusion model exhibited superior performance highfidelity textguided visual generation editing nevertheless prevailing textguided visual diffusion model primarily focus incorporating textvisual relationship exclusively reverse process often disregarding relevance forward process inconsistency forward reverse process may limit precise conveyance textual semantics visual synthesis result address issue propose novel general contextualized diffusion model contextdiff incorporating crossmodal context encompassing interaction alignment text condition visual sample forward reverse process propagate context timesteps two process adapt trajectory thereby facilitating crossmodal conditional modeling generalize contextualized diffusion ddpms ddims theoretical derivation demonstrate effectiveness model evaluation two challenging task texttoimage generation texttovideo editing task contextdiff achieves new stateoftheart performance significantly enhancing semantic alignment text condition generated sample evidenced quantitative qualitative evaluation code available
fast physicsdriven content generation single image content generation focus creating dynamic object change time existing method primarily rely pretrained video diffusion model utilizing sampling process reference video however approach face significant challenge firstly generated content often fails adhere realworld physic since video diffusion model incorporate physical prior secondly extensive sampling process large number parameter diffusion model result exceedingly timeconsuming generation process address issue introduce novel fast physicsdriven method controllable content generation single image integrates physical simulation directly generation process ensuring resulting content adheres natural physical law also eliminates use diffusion model dynamic generation phase significantly speeding process allows control dynamic including movement speed direction manipulating external force extensive experiment demonstrate generates highfidelity content significantly reduced inference time achieving stateoftheart performance code generated content available provided link
dynamic content generation via score composition video multiview diffusion model recent advancement generation predominantly propelled improvement image diffusion model model pretrained internetscale image data finetuned massive data offering capability producing highly consistent multiview image however due scarcity synchronized multiview video data remains challenging adapt paradigm generation directly despite available video data adequate training video multiview diffusion model separately provide satisfactory dynamic geometric prior respectively take advantage paper present novel framework dynamic content creation reconciles knowledge geometric consistency temporal smoothness model directly sample dense multiview multiframe image employed optimize continuous representation specifically design simple yet effective denoising strategy via score composition pretrained video multiview diffusion model based probability structure target image array alleviate potential conflict two heterogeneous score introduce variancereducing sampling via interpolated step facilitating smooth stable generation owing high parallelism proposed image generation process efficiency modern reconstruction pipeline framework generate content within minute notably method circumvents reliance expensive hardtoscale data thereby potential benefit scaling foundation video multiview diffusion model extensive experiment demonstrate efficacy proposed framework generating highly seamless consistent asset various type condition
spatialtemporal anchored generative gaussians recent progress pretrained diffusion model generation spurred interest content creation however achieving highfidelity generation spatialtemporal consistency remains challenge work propose novel framework combine pretrained diffusion model dynamic gaussian splatting highfidelity generation drawing inspiration generation technique utilize multiview diffusion model initialize multiview image anchoring input video frame video either realworld captured generated video diffusion model ensure temporal consistency multiview sequence initialization introduce simple yet effective fusion strategy leverage first frame temporal anchor selfattention computation almost consistent multiview sequence apply score distillation sampling optimize gaussian point cloud gaussian spatting specially crafted generation task adaptive densification strategy proposed mitigate unstable gaussian gradient robust optimization notably proposed pipeline require pretraining finetuning diffusion network offering accessible practical solution generation task extensive experiment demonstrate method outperforms prior generation work rendering quality spatialtemporal consistency generation robustness setting new stateoftheart generation diverse input including text image video
stylemaster stylize video artistic generation translation style control popular video generation model existing method often generate video far given style cause content leakage struggle transfer one video desired style first observation style extraction stage matter whereas existing method emphasize global style ignore local texture order bring texture feature preventing content leakage filter contentrelated patch retaining style one based promptpatch similarity global style extraction generate paired style dataset model illusion facilitate contrastive learning greatly enhances absolute style consistency moreover fill imagetovideo gap train lightweight motion adapter still video implicitly enhances stylization extent enables imagetrained model seamlessly applied video benefited effort approach stylemaster achieves significant improvement style resemblance temporal coherence also easily generalize video style transfer gray tile controlnet extensive experiment visualization demonstrate stylemaster significantly outperforms competitor effectively generating highquality stylized video align textual content closely resemble style reference image project page httpszixuanyegithubiostylemaster
spherical worldlocking audiovisual localization egocentric video egocentric video provide comprehensive context user scene understanding spanning multisensory perception behavioral interaction propose spherical worldlocking swl general framework egocentric scene representation implicitly transforms multisensory stream respect measurement head orientation compared conventional headlocked egocentric representation planar fieldofview swl effectively offset challenge posed selfmotion allowing improved spatial synchronization input modality using set multisensory embeddings worldlocked sphere design unified encoderdecoder transformer architecture preserve spherical structure scene representation without requiring expensive projection image world coordinate system evaluate effectiveness proposed framework multiple benchmark task egocentric video understanding including audiovisual active speaker localization auditory spherical source localization behavior anticipation everyday activity
learning generate diverse pedestrian movement web video noisy label understanding modeling pedestrian movement real world crucial application like motion forecasting scene simulation many factor influence pedestrian movement scene context individual characteristic goal often ignored existing human generation method web video contain natural pedestrian behavior rich motion context annotating pretrained predictor lead noisy label work propose learning diverse pedestrian movement web video first curate largescale dataset called citywalkers capture diverse realworld pedestrian movement urban scene based citywalkers propose generative model called pedgen diverse pedestrian movement generation pedgen introduces automatic label filtering remove lowquality label mask embedding train partial label also contains novel context encoder lift scene context incorporate various context factor generating realistic pedestrian movement urban scene experiment show pedgen outperforms existing baseline method pedestrian movement generation learning noisy label incorporating context factor addition pedgen achieves zeroshot generalization realworld simulated environment code model data made publicly available httpsgenforcegithubiopedgen
motion inversion video customization work present novel approach motion customization video generation addressing widespread gap exploration motion representation within video generative model recognizing unique challenge posed spatiotemporal nature video method introduces motion embeddings set explicit temporally coherent embeddings derived given video embeddings designed integrate seamlessly temporal transformer module video diffusion model modulating selfattention computation across frame without compromising spatial integrity approach provides compact efficient solution motion representation utilizing two type embeddings motion querykey embedding modulate temporal attention map motion value embedding modulate attention value additionally introduce inference strategy excludes spatial dimension motion querykey embedding applies differential operation motion value embedding designed debias appearance ensure embeddings focus solely motion contribution include introduction tailored motion embedding customization task demonstration practical advantage effectiveness method extensive experiment
videodirector precise video editing via texttovideo model despite typical inversionthenediting paradigm using texttoimage model demonstrated promising result directly extending texttovideo model still suffers severe artifact color flickering content distortion consequently current video editing method primarily rely model inherently lack temporalcoherence generative ability often resulting inferior editing result paper attribute failure typical editing paradigm tightly spatialtemporal coupling vanilla pivotalbased inversion strategy struggle disentangle spatialtemporal information video diffusion model complicated spatialtemporal layout vanilla crossattention control deficient preserving unedited content address limitation propose spatialtemporal decoupled guidance stdg multiframe nulltext optimization strategy provide pivotal temporal cue precise pivotal inversion furthermore introduce selfattention control strategy maintain higher fidelity precise partial content editing experimental result demonstrate method termed videodirector effectively harness powerful temporal generation capability model producing edited video stateoftheart performance accuracy motion smoothness realism fidelity unedited content
enhancing medical imaging gans synthesizing realistic image limited data research introduce innovative method synthesizing medical image using generative adversarial network gans proposed gans method demonstrates capability produce realistic synthetic image even trained limited quantity real medical image data showcasing commendable generalization prowess achieve devised generator discriminator network architecture founded deep convolutional neural network cnns leveraging adversarial training paradigm model optimization extensive experimentation across diverse medical image datasets method exhibit robust performance consistently generating synthetic image closely emulate structural textural attribute authentic medical image
diffusion based multidomain neuroimaging harmonization method preservation anatomical detail multicenter neuroimaging study face technical variability due batch difference across site potentially hinders data aggregation impact study reliabilityrecent effort neuroimaging harmonization aimed minimize technical gap reduce technical variability across batch generative adversarial network gan prominent method addressing image harmonization task ganharmonized image suffer artifact anatomical distortion given advancement denoising diffusion probabilistic model produce highfidelity image assessed efficacy diffusion model neuroimaging harmonization demonstrated diffusion model superior capability harmonizing image multiple domain ganbased method limited harmonizing image two domain per model experiment highlight learned domain invariant anatomical condition reinforces model accurately preserve anatomical detail differentiating batch difference diffusion step proposed method tested two public neuroimaging dataset abide ii yielding harmonization result consistent anatomy preservation superior fid score compared ganbased method conducted multiple analysis including extensive quantitative qualitative evaluation baseline model ablation study showcasing benefit learned condition improvement consistency perivascular space pvs segmentation harmonization
casgan contrastfree angiography synthesis iodinated contrast agent widely utilized numerous interventional procedure yet posing substantial health risk patient paper present casgan novel gan framework serf virtual contrast agent synthesize xray angiography via disentanglement representation learning vessel semantic guidance thereby reducing reliance iodinated contrast agent interventional procedure specifically approach disentangles xray angiography background vessel component leveraging medical prior knowledge specialized predictor learns map interrelationship component additionally vessel semanticguided generator corresponding loss function introduced enhance visual fidelity generated image experimental result xcad dataset demonstrate stateoftheart performance casgan achieving fid mmd promising result highlight casgans potential clinical application
phydiff physicsguided hourglass diffusion model diffusion mri synthesis diffusion mri dmri important neuroimaging technique high acquisition cost deep learning approach used enhance dmri predict diffusion biomarkers undersampled dmri generate comprehensive raw dmri generative adversarial network based method proposed include bvalues bvectors condition limited unstable training less desirable diversity emerging diffusion model dm promise improve generative performance however remains challenging include essential information conditioning dm relevant generation ie physical principle dmri white matter tract structure study propose physicsguided diffusion model generate highquality dmri model introduces physical principle dmri noise evolution diffusion process introduce querybased conditional mapping within difussion model addition enhance anatomical fine detials generation introduce xtract atlas prior white matter tract adopting adapter technique experiment result show method outperforms stateoftheart method potential advance dmri enhancement
rolling diffusion model diffusion model recently increasingly applied temporal data video fluid mechanic simulation climate data method generally treat subsequent frame equally regarding amount noise diffusion process paper explores rolling diffusion new approach us sliding window denoising process ensures diffusion process progressively corrupts time assigning noise frame appear later sequence reflecting greater uncertainty future generation process unfolds empirically show temporal dynamic complex rolling diffusion superior standard diffusion particular result demonstrated video prediction task using video dataset chaotic fluid dynamic forecasting experiment
diffvps video polyp segmentation via multitask diffusion network adversarial temporal reasoning diffusion probabilistic model recently attracted significant attention community computer vision due outstanding performance however substantial amount diffusionbased research focused generative task work introduces diffusion model advance result polyp segmentation video frequently challenged polyp high camouflage redundant temporal cuesin paper present novel diffusionbased network video polyp segmentation task dubbed diffvps incorporate multitask supervision diffusion model promote discrimination diffusion model pixelbypixel segmentation integrates contextual highlevel information achieved joint classification detection task explore temporal dependency temporal reasoning module trm devised via reasoning reconstructing target frame previous frame equip trm generative adversarial selfsupervised strategy produce realistic frame thus capture better dynamic cue extensive experiment conducted sunseg result indicate proposed diffvps significantly achieves stateoftheart performance code available httpsgithubcomlydiaylludiffvps
videoscore building automatic metric simulate finegrained human feedback video generation recent year witnessed great advance video generation however development automatic video metric lagging significantly behind none existing metric able provide reliable score generated video main barrier lack largescale humanannotated dataset paper release videofeedback first largescale dataset containing humanprovided multiaspect score synthesized video existing video generative model train videoscore initialized mantis based videofeedback enable automatic video quality assessment experiment show spearman correlation videoscore human reach videofeedbacktest beating prior best metric point result heldout evalcrafter genaibench vbench show videoscore consistently much higher correlation human judge metric due result believe videoscore serve great proxy human raters rate different video model track progress simulate finegrained human feedback reinforcement learning human feedback rlhf improve current video generation model
text prompting multiconcept video customization autoregressive generation present method multiconcept customization pretrained texttovideo model intuitively multiconcept customized video derived nonlinear intersection video manifold individual concept straightforward find hypothesize sequential controlled walking towards intersection video manifold directed text prompting lead solution generate various concept corresponding interaction sequentially autoregressive manner method generate video multiple custom concept subject action background teddy bear running towards brown teapot dog playing violin teddy bear swimming ocean quantitatively evaluate method using videoclip dino score addition human evaluation video result presented paper found
expertaf expert actionable feedback video feedback essential learning new skill improving one current skilllevel however current method skillassessment video provide score compare demonstration leaving burden knowing differently user introduce novel method generate actionable feedback video person physical activity basketball soccer method take video demonstration accompanying body pose generates freeform expert commentary describing person well could improve visual expert demonstration incorporates required correction show leverage video skilled activity expert commentary together strong language model create weaklysupervised training dataset task devise multimodal videolanguage model infer coaching feedback method able reason across multimodal input combination output fullspectrum actionable coaching expert commentary expert video retrieval expert pose generation outperforming strong visionlanguage model established metric human preference study code data publicly released
hotvcom generating buzzworthy comment video era social medium video platform popular hotcomments play crucial role attracting user impression shortform video making vital marketing branding purpose however existing research predominantly focus generating descriptive comment danmaku english offering immediate reaction specific video moment addressing gap study introduces textschotvcom largest chinese video hotcomment dataset comprising diverse video million comment also present textttcomheat framework synergistically integrates visual auditory textual data generate influential hotcomments chinese video dataset empirical evaluation highlight effectiveness framework demonstrating excellence newly constructed existing datasets
exploring temporal event cue dense video captioning cyclic colearning dense video captioning aim detect describe event untrimmed video paper present dense video captioning network called multiconcept cyclic learning mccl aim detect multiple concept frame level using concept enhance video feature provide temporal event cue design cyclic colearning generator localizer within captioning network promote semantic perception event localization specifically perform weakly supervised concept detection frame detected concept embeddings integrated video feature provide event cue additionally videolevel concept contrastive learning introduced obtain discriminative concept embeddings captioning network establish cyclic colearning strategy generator guide localizer event localization semantic matching localizer enhances generator event semantic perception location matching making semantic perception event localization mutually beneficial mccl achieves stateoftheart performance activitynet caption datasets extensive experiment demonstrate effectiveness interpretability
vidtwin video vae decoupled structure dynamic recent advancement video autoencoders video aes significantly improved quality efficiency video generation paper propose novel compact video autoencoder vidtwin decouples video two distinct latent space structure latent vector capture overall content global movement dynamic latent vector represent finegrained detail rapid movement specifically approach leverage encoderdecoder backbone augmented two submodules extracting latent space respectively first submodule employ qformer extract lowfrequency motion trend followed downsampling block remove redundant content detail second average latent vector along spatial dimension capture rapid motion extensive experiment show vidtwin achieves high compression rate high reconstruction quality psnr mcljcv dataset performs efficiently effectively downstream generative task moreover model demonstrates explainability scalability paving way future research video latent representation generation code released httpsgithubcommicrosoftvidtoktreemainvidtwin
content bias fréchet video distance frechet video distance fvd prominent metric evaluating video generation model known conflict human perception occasionally paper aim explore extent fvds bias toward perframe quality temporal realism identify source first quantify fvds sensitivity temporal axis decoupling frame motion quality find fvd increase slightly large temporal corruption analyze generated video show via careful sampling large set generated video contain motion one drastically decrease fvd without improving temporal quality study suggest fvds bias towards quality individual frame observe bias attributed feature extracted supervised video classifier trained contentbiased dataset show fvd feature extracted recent largescale selfsupervised video model less biased toward image quality finally revisit realworld example validate hypothesis
scaling diffusion mamba bidirectional ssms efficient image video generation recent development mamba architecture known selective state space approach shown potential efficient modeling long sequence however application image generation remains underexplored traditional diffusion transformer dit utilize selfattention block effective computational complexity scale quadratically input length limiting use highresolution image address challenge introduce novel diffusion architecture diffusion mamba dim foregoes traditional attention mechanism favor scalable alternative harnessing inherent efficiency mamba architecture dim achieves rapid inference time reduced computational load maintaining linear complexity respect sequence length architecture scale effectively also outperforms existing diffusion transformer image video generation task result affirm scalability efficiency dim establishing new benchmark image video generation technique work advance field generative model pave way application scalable architecture
video emotion openvocabulary recognition based multimodal large language model multimodal emotion recognition task great concern however traditional data set based fixed label resulting model often focus main emotion ignore detailed emotional change complex scene report introduces solution using mllms technology generate openvocabulary emotion label video solution includes use framework data generation processing training method result generation multimodel cojudgment merov openword emotion recognition challenge method achieved significant advantage leading superior capability complex emotion computation
enhancing film grain coding vvc improving encoding quality efficiency paper present indepth analysis film grain handling opensource implementation versatile video coding vvc standard focus two key component film grain analysis fga module implemented vvenc film grain synthesis fgs module implemented vvdec describe methodology used implement module discus generation supplementary enhancement information sei parameter signal film grain characteristic encoded video sequence additionally conduct subjective objective evaluation across full hd video assess effectiveness film grain handling result demonstrate capability fga fgs technique accurately analyze synthesize film grain thereby improving visual quality encoded video content overall study contributes advancing understanding implementation film grain handling technique vvc opensource implementation implication enhancing viewing experience multimedia application
timerewind rewinding time imageandevents video diffusion paper address novel challenge rewinding time single captured image recover fleeting moment missed shutter button pressed problem pose significant challenge computer vision computational photography requires predicting plausible precapture motion single static frame inherently illposed task due high degree freedom potential pixel movement overcome challenge leveraging emerging technology neuromorphic event camera capture motion information high temporal resolution integrating data advanced imagetovideo diffusion model proposed framework introduces event motion adaptor conditioned event camera data guiding diffusion model generate video visually coherent physically grounded captured event extensive experimentation demonstrate capability approach synthesize highquality video effectively rewind time showcasing potential combining event camera technology generative model work open new avenue research intersection computer vision computational photography generative modeling offering forwardthinking solution capturing missed moment enhancing future consumer camera smartphones please see project page httpstimerewindgithubio video result code release
yingsound videoguided sound effect generation multimodal chainofthought control generating sound effect productlevel video small amount labeled data available diverse scene requires production highquality sound fewshot setting tackle challenge limited labeled data realworld scene introduce yingsound foundation model designed videoguided sound generation support highquality audio generation fewshot setting specifically yingsound consists two major module first module us conditional flow matching transformer achieve effective semantic alignment sound generation across audio visual modality module aim build learnable audiovisual aggregator ava integrates highresolution visual feature corresponding audio feature multiple stage second module developed proposed multimodal visualaudio chainofthought cot approach generate finer sound effect fewshot setting finally industrystandard videotoaudio dataset encompasses various realworld scenario presented show yingsound effectively generates highquality synchronized sound across diverse conditional input automated evaluation human study project page urlhttpsgiantailabgithubioyingsound
twostage omnidirectional image synthesis geometric distortion correction omnidirectional image increasingly used various application including virtual reality sn social networking service however availability comparatively limited contrast normal field view nfov image since specialized camera required take omnidirectional image consequently several method proposed based generative adversarial network gan synthesize omnidirectional image approach shown difficulty training model due instability andor significant time consumption training address problem paper proposes novel omnidirectional image synthesis method twostage omnidirectional image synthesis generated highquality omnidirectional image drastically reduced training time realized utilizing vqgan vector quantized gan model pretrained largescale nfov image database imagenet without finetuning since pretrained model represent distortion omnidirectional image equirectangular projection erp applied directly omnidirectional image synthesis erp therefore twostage structure adopted first create global coarse image erp refine image integrating multiple local nfov image higher resolution compensate distortion erp based pretrained vqgan model result proposed method achieved reduction training time day omnidreamer four day higher image quality
videosavi selfaligned video language model without human supervision recent advance visionlanguage model vlms significantly enhanced video understanding task instruction tuning ie finetuning model datasets instruction paired desired output key improving model performance however creating diverse instructiontuning datasets challenging due high annotation cost complexity capturing temporal information video existing approach often rely large language model generate instructionoutput pair limit diversity lead response lack grounding video content address propose videosavi selfaligned video language model novel selftraining pipeline enables vlms generate training data without extensive manual annotation process involves three stage generating diverse videospecific question producing multiple candidate answer evaluating response alignment video content selfgenerated data used direct preference optimization dpo allowing model refine highquality output improve alignment video content experiment demonstrate even smaller model parameter effectively use selftraining approach outperforming previous method achieving result comparable trained proprietary preference data videosavi show significant improvement across multiple benchmark multichoice qa zeroshot openended qa temporal reasoning benchmark result demonstrate effectiveness selftraining approach enhancing video understanding reducing dependence proprietary model
geometryguided gan face animation animating human face image aim synthesize desired source identity naturallooking way mimicking driving video facial movement context generative adversarial network demonstrated remarkable potential realtime face reenactment using single source image yet constrained limited geometry consistency compared graphicbased approach paper introduce geometryguided gan face animation tackle limitation novel approach empowers face animation model incorporate information using image improving image generation capability talking head synthesis model integrate inverse rendering technique extract facial geometry property improving feedback loop generator weighted average ensemble discriminator face reenactment model leverage motion warping capture motion dynamic along orthogonal ray sampling volume rendering technique produce ultimate visual output evaluate performance conducted comprehensive experiment using various evaluation protocol talkinghead benchmark demonstrate effectiveness proposed framework compared stateoftheart realtime face animation method
worldsimbench towards video generation model world simulator recent advancement predictive model demonstrated exceptional capability predicting future state object scene however lack categorization based inherent characteristic continues hinder progress predictive model development additionally existing benchmark unable effectively evaluate highercapability highly embodied predictive model embodied perspective work classify functionality predictive model hierarchy take first step evaluating world simulator proposing dual evaluation framework called worldsimbench worldsimbench includes explicit perceptual evaluation implicit manipulative evaluation encompassing human preference assessment visual perspective actionlevel evaluation embodied task covering three representative embodied scenario openended embodied environment autonomous driving robot manipulation explicit perceptual evaluation introduce hfembodied dataset video assessment dataset based finegrained human feedback use train human preference evaluator aligns human perception explicitly assesses visual fidelity world simulator implicit manipulative evaluation assess videoaction consistency world simulator evaluating whether generated situationaware video accurately translated correct control signal dynamic environment comprehensive evaluation offer key insight drive innovation video generation model positioning world simulator pivotal advancement toward embodied artificial intelligence
realtime video generation pyramid attention broadcast present pyramid attention broadcast pab realtime high quality trainingfree approach ditbased video generation method founded observation attention difference diffusion process exhibit ushaped pattern indicating significant redundancy mitigate broadcasting attention output subsequent step pyramid style applies different broadcast strategy attention based variance best efficiency introduce broadcast sequence parallel efficient distributed inference pab demonstrates speedup across three model compared baseline achieving realtime generation video anticipate simple yet effective method serve robust baseline facilitate future research application video generation
short rise assessing effect youtube short longform video content short form content permeated video creator space past year led industry leading product tiktok youtube short instagram reel youtube particular previously synonymous main hub long form video content consumption monetization long form video easier allowed multiple advertisement placement course video model also facilitated thematic brand partnership however since introduction short form content creator found difficult generate revenue advertisement placement decreased lead unique situation people spending time watching shorter video yet generate less revenue creator paper perform study creator significant audience see introduction short form content affected view count engagement long form content finding reveal noteworthy trend since advent shortform content significant decrease view count engagement longform video channel
moviellm enhancing long video understanding aigenerated movie development multimodal model marked significant step forward machine understand video model shown promise analyzing short video clip however come longer format like movie often fall short main hurdle lack highquality diverse video data intensive work required collect annotate data face challenge propose moviellm novel framework designed synthesize consistent highquality video data instruction tuning pipeline carefully designed control style video improving textual inversion technique powerful text generation capability first framework thing approach stand flexibility scalability empowering user create customized movie one description make superior alternative traditional data collection method extensive experiment validate data produced moviellm significantly improves performance multimodal model understanding complex video narrative overcoming limitation existing datasets regarding scarcity bias
videodriven animation neural head avatar present new approach videodriven animation highquality neural head model addressing challenge personindependent animation video input typically highquality generative model learned specific individual multiview video footage resulting personspecific latent representation drive generation process order achieve personindependent animation video input introduce lstmbased animation network capable translating personindependent expression feature personalized animation parameter personspecific head model approach combine advantage personalized head model high quality realism convenience videodriven animation employing multiperson facial performance capture demonstrate effectiveness approach synthesized animation high quality based different source video well ablation study
aqua automated questionanswering software tutorial video visual anchor tutorial video popular help source learning featurerich software however getting quick answer question tutorial video difficult present automated approach responding tutorial question analyzing question found video comment identified different question type observed user frequently described part video question asked participant watch tutorial video ask question annotating video relevant visual anchor visual anchor referred ui element application workspace based insight built aqua pipeline generates useful answer question visual anchor demonstrate fusion showing recognize ui element visual anchor generate answer using augmented visual information software documentation evaluation study demonstrates approach provides better answer baseline method
unveiling invisible captioning video metaphor metaphor common communication tool used daytoday life detection generation metaphor textual form studied extensively metaphor form underexplored recent study shown visionlanguage vl model understand visual metaphor meme advert probing study done involve complex language phenomenon like metaphor video hence introduce new vl task describing metaphor present video work facilitate novel task construct release manually created dataset video humanwritten caption along new metric called average concept distance acd automatically evaluate creativity metaphor generated also propose novel lowresource video metaphor captioning system gitllava obtains comparable performance sota video language model proposed task perform comprehensive analysis existing video language model task publish dataset model benchmark result enable research
uvis unsupervised video instance segmentation video instance segmentation requires classifying segmenting tracking every object across video frame unlike existing approach rely mask box category label propose uvis novel unsupervised video instance segmentation uvis framework perform video instance segmentation without video annotation dense labelbased pretraining key insight come leveraging dense shape prior selfsupervised vision foundation model dino openset recognition ability imagecaption supervised visionlanguage model clip uvis framework consists three essential step framelevel pseudolabel generation transformerbased vi model training querybased tracking improve quality vi prediction unsupervised setup introduce dualmemory design design includes semantic memory bank generating accurate pseudolabels tracking memory bank maintaining temporal consistency object track evaluate approach three standard vi benchmark namely occluded vi uvis achieves ap without video annotation dense pretraining demonstrating potential unsupervised vi framework
generative outpainting enhance memorability shortform video expanding use shortform video format advertising social medium entertainment education need medium captivate remembered video memorability indicates u likely video remembered viewer emotional personal connection content paper present result using generative outpainting expand screen size shortform video view improving memorability advance machine learning deep learning compared leveraged understand extending border video screensizes affect memorability viewer using quantitative evaluation determine bestperforming model outpainting impact outpainting based image saliency video memorability score
foundation model adaptive feature selection synergistic approach video question answering paper tackle intricate challenge video questionanswering videoqa despite notable progress current method fall short effectively integrating question video frame semantic objectlevel abstraction create questionaware video representation introduce localglobal question aware video embedding lgqave incorporates three major innovation integrate multimodal knowledge better emphasize semantic visual concept relevant specific question lgqave move beyond traditional adhoc frame sampling utilizing crossattention mechanism precisely identifies relevant frame concerning question capture dynamic object within frame using distinct graph grounding question semantics minigpt model graph processed questionaware dynamic graph transformer qdgt refines output develop nuanced global local video representation additional crossattention module integrates local global embeddings generate final video embeddings language model us generate answer extensive evaluation across multiple benchmark demonstrate lgqave significantly outperforms existing model delivering accurate multichoice openended answer
oneshot posedriving face animation platform objective face animation generate dynamic expressive talking head video single reference face utilizing driving condition derived either video audio input current approach often require finetuning specific identity frequently fail produce expressive video due limited effectiveness module facilitate generation oneshot consecutive talking head video refine existing model integrating face locator motion frame mechanism subsequently optimize model using extensive human face video datasets significantly enhancing ability produce highquality expressive talking head video additionally develop demo platform using gradio framework streamlines process enabling user quickly create customized talking head video
videomme firstever comprehensive evaluation benchmark multimodal llm video analysis quest artificial general intelligence multimodal large language model mllms emerged focal point recent advancement however predominant focus remains developing capability static image understanding potential mllms processing sequential visual data still insufficiently explored highlighting absence comprehensive highquality assessment performance paper introduce videomme firstever fullspectrum multimodal evaluation benchmark mllms video analysis work distinguishes existing benchmark four key feature diversity video type spanning primary visual domain subfields ensure broad scenario generalizability duration temporal dimension encompassing short medium longterm video ranging second hour robust contextual dynamic breadth data modality integrating multimodal input besides video frame including subtitle audio unveil allround capability mllms quality annotation utilizing rigorous manual labeling expert annotator facilitate precise reliable model assessment video total hour manually selected annotated repeatedly viewing video content resulting questionanswer pair videomme extensively evaluate various stateoftheart mllms including series gemini pro well opensource image model like video model like llavanextvideo experiment reveal gemini pro bestperforming commercial model significantly outperforming opensource model dataset along finding underscore need improvement handling longer sequence multimodal data project page httpsvideommegithubio
omegance single parameter various granularity diffusionbased synthesis work introduce single parameter omega effectively control granularity diffusionbased synthesis parameter incorporated denoising step diffusion model reverse process approach require model retraining architectural modification additional computational overhead inference yet enables precise control level detail generated output moreover spatial mask denoising schedule varying omega value applied achieve regionspecific timestepspecific granularity control prior knowledge image composition control signal reference image facilitates creation precise omega mask granularity control specific object highlight parameter role controlling subtle detail variation technique named omegance combining omega nuance method demonstrates impressive performance across various image video synthesis task adaptable advanced diffusion model code available
solving masked jigsaw puzzle diffusion vision transformer solving image video jigsaw puzzle pose challenging task rearranging image fragment video frame unordered sequence restore meaningful image video sequence existing approach often hinge discriminative model tasked predicting either absolute position puzzle element permutation action applied original data unfortunately method face limitation effectively solving puzzle large number element paper propose jpdvt innovative approach harness diffusion transformer address challenge specifically generate positional information image patch video frame conditioned underlying visual content information employed accurately assemble puzzle piece correct position even scenario involving missing piece method achieves stateoftheart performance several datasets
video motion transfer diffusion transformer propose ditflow method transferring motion reference video newly synthesized one designed specifically diffusion transformer dit first process reference video pretrained dit analyze crossframe attention map extract patchwise motion signal called attention motion flow amf guide latent denoising process optimizationbased trainingfree manner optimizing latents amf loss generate video reproducing motion reference one also apply optimization strategy transformer positional embeddings granting u boost zeroshot motion transfer capability evaluate ditflow recently published method outperforming across multiple metric human evaluation
single trajectory distillation accelerating image video style transfer diffusionbased stylization method typically denoise specific partial noise state imagetoimage videotovideo task multistep diffusion process computationally expensive hinders realworld application promising solution speed process obtain fewstep consistency model trajectory distillation however current consistency model force initialstep alignment probability flow ode pfode trajectory student imperfect teacher model training strategy ensure consistency whole trajectory address issue propose single trajectory distillation std starting specific partial noise state introduce trajectory bank store teacher model trajectory state mitigating time cost training besides use asymmetric adversarial loss enhance style quality generated image extensive experiment image video stylization demonstrate method surpasses existing acceleration model term style similarity aesthetic evaluation code result available project page httpssingletrajectorydistillationgithubio
vividpose advancing stable video diffusion realistic human image animation human image animation involves generating video static image following specified pose sequence current approach typically adopt multistage pipeline separately learns appearance motion often lead appearance degradation temporal inconsistency address issue propose vividpose innovative endtoend pipeline based stable video diffusion svd ensures superior temporal stability enhance retention human identity propose identityaware appearance controller integrates additional facial information without compromising appearance detail clothing texture background approach ensures generated video maintain high fidelity identity human subject preserving key facial feature across various pose accommodate diverse human body shape hand movement introduce geometryaware pose controller utilizes dense rendering map smplx sparse skeleton map enables accurate alignment pose shape generated video providing robust framework capable handling wide range body shape dynamic hand movement extensive qualitative quantitative experiment ubcfashion tiktok benchmark demonstrate method achieves stateoftheart performance furthermore vividpose exhibit superior generalization capability proposed inthewild dataset code model available
animated sticker bringing sticker life video diffusion introduce animated sticker video diffusion model generates animation conditioned text prompt static sticker image model built top stateoftheart emu texttoimage model addition temporal layer model motion due domain gap ie difference visual motion style model performed well generating natural video longer generate vivid video applied sticker bridge gap employ twostage finetuning pipeline first weakly indomain data followed humanintheloop hitl strategy term ensembleofteachers distills best quality multiple teacher smaller student model show strategy allows u specifically target improvement motion quality maintaining style static image inference optimization model able generate eightframe video highquality interesting relevant motion one second
hrinr continuous spacetime video superresolution via event camera continuous spacetime video superresolution cstvsr aim simultaneously enhance video resolution frame rate arbitrary scale recently implicit neural representation inr applied video restoration representing video implicit field decoded arbitrary scale however highly illposed nature cstvsr limit effectiveness current inrbased method assume linear motion frame use interpolation feature warping generate feature arbitrary spatiotemporal position two consecutive frame restrains cstvsr capturing rapid nonlinear motion longterm dependency involving two frame complex dynamic scene paper propose novel cstvsr framework called hrinr capture holistic dependency regional motion based inr assisted event camera novel sensor renowned high temporal resolution low latency fully utilize rich temporal information event design feature extraction consisting regional event feature extractor taking event input via proposed event temporal pyramid representation capture regional nonlinear motion holistic eventframe feature extractor longterm dependence continuity motion propose novel inrbased decoder spatiotemporal embeddings capture longterm dependency larger temporal perception field validate effectiveness generalization method four datasets simulated real data showing superiority method
global spatialtemporal informationbased residual convlstm video spacetime superresolution converting lowframerate lowresolution video highframerate highresolution one spacetime video superresolution technique enhance visual experience facilitate efficient information dissemination propose convolutional neural network cnn spacetime video superresolution namely girnet generate highly accurate feature thus improve performance proposed network integrates featurelevel temporal interpolation module deformable convolution global spatialtemporal informationbased residual convolutional long shortterm memory convlstm module featurelevel temporal interpolation module leverage deformable convolution adapts deformation scale variation object across different scene location present efficient solution conventional convolution extracting feature moving object network effectively us forward backward feature information determine interframe offset leading direct generation interpolated frame feature global spatialtemporal informationbased residual convlstm module first convlstm used derive global spatialtemporal information input feature second convlstm us previously computed global spatialtemporal information feature initial cell state second convlstm adopts residual connection preserve spatial information thereby enhancing output feature experiment dataset show proposed method outperforms stateoftheart technique peak signaltonoiseratio db db db starnet tmnet respectively structural similarity indexby starnet tmnet respectively visually
one shot one talk wholebody talking avatar single image building realistic animatable avatar still requires minute multiview monocular selfrotating video method lack precise control gesture expression push boundary address challenge constructing wholebody talking avatar single image propose novel pipeline tackle two critical issue complex dynamic modeling generalization novel gesture expression achieve seamless generalization leverage recent poseguided imagetovideo diffusion model generate imperfect video frame pseudolabels overcome dynamic modeling challenge posed inconsistent noisy pseudovideos introduce tightly coupled hybrid avatar representation apply several key regularization mitigate inconsistency caused imperfect label extensive experiment diverse subject demonstrate method enables creation photorealistic precisely animatable expressive wholebody talking avatar single image
landmarkguided diffusion model highfidelity temporally coherent talking head generation audiodriven talking head generation significant challenging task applicable various field virtual avatar film production online conference however existing ganbased model emphasize generating wellsynchronized lip shape overlook visual quality generated frame diffusionbased model prioritize generating highquality frame neglect lip shape matching resulting jittery mouth movement address aforementioned problem introduce twostage diffusionbased model first stage involves generating synchronized facial landmark based given speech second stage generated landmark serve condition denoising process aiming optimize mouth jitter issue generate highfidelity wellsynchronized temporally coherent talking head video extensive experiment demonstrate model yield best performance
biasconflict sample synthesis adversarial removal debias strategy temporal sentence grounding video temporal sentence grounding video tsgv troubled dataset bias issue caused uneven temporal distribution target moment sample similar semantic component input video query text existing method resort utilizing prior knowledge bias artificially break uneven distribution remove limited amount significant language bias work propose biasconflict sample synthesis adversarial removal debias strategy bssard dynamically generates biasconflict sample explicitly leveraging potentially spurious correlation singlemodality feature temporal position target moment adversarial training bias generator continuously introduce bias generate biasconflict sample deceive grounding model meanwhile grounding model continuously eliminates introduced bias requires model multimodality alignment information bssard cover kind coupling relationship disrupt language visual bias simultaneously extensive experiment charadescd activitynetcd demonstrate promising debiasing capability bssard source code available httpsgithubcomqzhbbssard
followyourmultipose tuningfree multicharacter texttovideo generation via pose guidance texteditable posecontrollable character video generation challenging prevailing topic practical application however existing approach mainly focus singleobject video generation pose guidance ignoring realistic situation multicharacter appear concurrently scenario tackle propose novel multicharacter video generation framework tuningfree manner based separated text pose guidance specifically first extract character mask pose sequence identify spatial position generating character single prompt character obtained llm precise text guidance moreover spatialaligned cross attention multibranch control module proposed generate fine grained controllable multicharacter video visualized result generating video demonstrate precise controllability method multicharacter generation also verify generality method applying various personalized model moreover quantitative result show approach achieves superior performance compared previous work
modular blind video quality assessment blind video quality assessment bvqa play pivotal role evaluating improving viewing experience endusers across wide range videobased platform service contemporary deep learningbased model primarily analyze video content aggressively subsampled format blind impact actual spatial resolution frame rate video quality paper propose modular bvqa model method training improve modularity model comprises base quality predictor spatial rectifier temporal rectifier responding visual content distortion spatial resolution frame rate change video quality respectively training spatial temporal rectifier dropped probability render base quality predictor standalone bvqa model work better rectifier extensive experiment professionallygenerated content usergenerated content video database show quality model achieves superior comparable performance current method additionally modularity model offer opportunity analyze existing video quality database term spatial temporal complexity
exploring explainability video action recognition image classification video action recognition perhaps two foundational task computer vision consequently explaining inner working trained deep neural network prime importance numerous effort focus explaining decision trained deep neural network image classification exploration domain temporal version video action recognition scant work take deeper look problem begin revisiting gradcam one popular feature attribution method image classification extension video action recognition task examine method limitation address introduce videotcav building tcav image classification task aim quantify importance specific concept decisionmaking process video action recognition model scalable generation concept still open problem propose machineassisted approach generate spatial spatiotemporal concept relevant video action recognition testing videotcav establish importance temporallyvarying concept demonstrating superiority dynamic spatiotemporal concept trivial spatial concept conclusion introduce framework investigating hypothesis action recognition quantitatively testing thus advancing research explainability deep neural network used video action recognition
guide guidelineguided dataset instructional video comprehension substantial instructional video internet provide u tutorial completing various task existing instructional video datasets focus specific step video level lacking experiential guideline task level lead beginner struggling learn new task due lack relevant experience moreover specific step without guideline trivial unsystematic making difficult provide clear tutorial address problem present guide guidelineguided dataset contains video instructional task domain related daily life specifically annotate instructional task guideline representing common pattern shared taskrelated video basis annotate systematic specific step including associated guideline step specific step description timestamps proposed benchmark consists three subtasks evaluate comprehension ability model step captioning model generate caption specific step video guideline summarization model mine common pattern taskrelated video summarize guideline guidelineguided captioning model generate caption specific step guide guideline evaluate plenty foundation model guide perform indepth analysis given diversity practicality guide believe used better benchmark instructional video comprehension
multimodal language model domainspecific procedural video summarization video serve powerful medium convey idea tell story provide detailed instruction especially longformat tutorial tutorial valuable learning new skill one pace yet overwhelming due length dense content viewer often seek specific information like precise measurement stepbystep execution detail making essential extract summarize key segment efficiently intelligent timesensitive video assistant capable summarizing detecting highlight long video highly sought recent advancement multimodal large language model offer promising solution develop assistant research explores use multimodal model enhance video summarization stepbystep instruction generation within specific domain model need understand temporal event relationship among action across video frame approach focus finetuning timechat improve performance specific domain cooking medical procedure training model domainspecific datasets like tasty cooking medvidqa medical procedure aim enhance ability generate concise accurate summary instructional video curate restructure datasets create highquality videocentric instruction data finding indicate finetuned domainspecific procedural data timechat significantly improve extraction summarization key instructional step longformat video research demonstrates potential specialized multimodal model assist practical task providing personalized stepbystep guidance tailored unique aspect domain
intertrack tracking human object interaction without object template tracking human object interaction video important understand human behavior rapidly growing stream video data previous videobased method require predefined object template singleimagebased method templatefree lack temporal consistency paper present method track human object interaction without object shape template decompose tracking problem perframe pose tracking canonical shape optimization first apply singleview reconstruction method obtain temporallyinconsistent perframe interaction reconstruction human propose efficient autoencoder predict smpl vertex directly perframe reconstruction introducing temporally consistent correspondence object introduce pose estimator leverage temporal information predict smooth object rotation occlusion train model propose method generate synthetic interaction video synthesize total hour video sequence full ground truth experiment behave intercap show method significantly outperforms previous templatebased video tracking singleframe reconstruction method proposed synthetic video dataset also allows training videobased method generalize realworld video code dataset publicly released
videoglamm large multimodal model pixellevel visual grounding video finegrained alignment video text challenging due complex spatial temporal dynamic video existing videobased large multimodal model lmms handle basic conversation struggle precise pixellevel grounding video address introduce videoglamm lmm designed finegrained pixellevel grounding video based userprovided textual input design seamlessly connects three key component large language model dual vision encoder emphasizes spatial temporal detail spatiotemporal decoder accurate mask generation connection facilitated via tunable vl lv adapter enable close visionlanguage vl alignment architecture trained synchronize spatial temporal element video content textual instruction enable finegrained grounding curate multimodal dataset featuring detailed visuallygrounded conversation using semiautomatic annotation pipeline resulting diverse set videoqa triplet along object mask evaluate videoglamm three challenging task grounded conversation generation visual grounding referring video segmentation experimental result show model consistently outperforms existing approach across three task
storyteller improving long video description global audiovisual character identification existing large visionlanguage model lvlms largely limited processing short secondslong video struggle generating coherent description extended video spanning minute long video description introduces new challenge consistent character identification plotlevel description incorporating visual audio information address figure audiovisual character identification matching character name dialogue key factor propose storyteller system generating dense description long video incorporating lowlevel visual concept highlevel plot information storyteller us multimodal large language model integrates visual audio text modality perform audiovisual character identification minutelong video clip result fed lvlm enhance consistency video description validate approach movie description task introduce dataset dense description threeminute movie clip evaluate long video description create storyqa large set multiplechoice question test set assess description inputting answer question using accuracy automatic evaluation metric experiment show storyteller outperforms open closedsource baseline storyqa achieving higher accuracy strongest baseline demonstrating advantage human sidebyside evaluation additionally incorporating audiovisual character identification storyteller improves performance video description model showing relative improvement respectively accuracy storyqa
grounding video model action goal conditioned exploration large video model pretrained massive amount internet video provide rich source physical knowledge dynamic motion object task however video model grounded embodiment agent describe actuate world reach visual state depicted video tackle problem current method use separate visionbased inverse dynamic model trained embodimentspecific data map image state action gathering data train model often expensive challenging model limited visual setting similar one data available paper investigate directly ground video model continuous action selfexploration embodied environment using generated video state visual goal exploration propose framework us trajectory level action generation combination video guidance enable agent solve complex task without external supervision eg reward action label segmentation mask validate proposed approach task libero task metaworld task calvin task ithor visual navigation show approach par even surpasses multiple behavior cloning baseline trained expert demonstration without requiring action annotation
videoicl confidencebased iterative incontext learning outofdistribution video understanding recent advancement video large multimodal model lmms significantly improved video understanding reasoning capability however performance drop outofdistribution ood task underrepresented training data traditional method like finetuning ood datasets impractical due high computational cost incontext learning icl demonstration example shown promising generalization performance language task imagelanguage task without finetuning applying icl videolanguage task face challenge due limited context length video lmms video require longer token length address issue propose videoicl novel video incontext learning framework ood task introduces similaritybased relevant example selection strategy confidencebased iterative inference approach allows select relevant example rank based similarity used inference generated response low confidence framework selects new example performs inference iteratively refining result highconfidence response obtained approach improves ood video understanding performance extending effective context length without incurring high cost experimental result multiple benchmark demonstrate significant performance gain especially domainspecific scenario laying groundwork broader video comprehension application code released
tuning large multimodal model video using reinforcement learning ai feedback recent advancement large language model influenced development video large multimodal model vlmms previous approach vlmms involved supervised finetuning sft instructiontuned datasets integrating llm visual encoders adding additional learnable module video text multimodal alignment remains challenging primarily due deficient volume quality multimodal instructiontune data compared textonly data present novel alignment strategy employ multimodal ai system oversee called reinforcement learning ai feedback rlaif providing selfpreference feedback refine facilitating alignment video text modality specific propose contextaware reward modeling providing detailed video description context generation preference feedback order enrich understanding video content demonstrating enhanced performance across diverse video benchmark multimodal rlaif approach vlmrlaif outperforms existing approach including sft model commit opensourcing code model datasets foster research area
momentor advancing video large language model finegrained temporal reasoning large language model llm demonstrate remarkable proficiency comprehending handling textbased task many effort made transfer attribute video modality termed videollms however existing videollms capture coarsegrained semantics unable effectively handle task related comprehension localization specific video segment light challenge propose momentor videollm capable accomplishing finegrained temporal understanding task support training momentor design automatic data generation engine construct largescale video instruction dataset segmentlevel instruction data train momentor enabling perform segmentlevel reasoning localization zeroshot evaluation several task demonstrate momentor excels finegrained temporally grounded comprehension localization
fmmattack flowbased multimodal adversarial attack videobased llm despite remarkable performance videobased large language model llm adversarial threat remains unexplored fill gap propose first adversarial attack tailored videobased llm crafting flowbased multimodal adversarial perturbation small fraction frame within video dubbed fmmattack extensive experiment show attack effectively induce videobased llm generate incorrect answer video added imperceptible adversarial perturbation intriguingly fmmattack also induce garbling model output prompting videobased llm hallucinate overall observation inspire understanding multimodal robustness safetyrelated feature alignment across different modality great importance various large multimodal model code available httpsgithubcomthukingminfmmattack
gazeguided graph neural network action anticipation conditioned intention human utilize gaze concentrate essential information perceiving interpreting intention video incorporating human gaze computational algorithm significantly enhance model performance video understanding task work address challenging innovative task video understanding predicting action agent video based partial video introduce gazeguided action anticipation algorithm establishes visualsemantic graph video input method utilizes graph neural network recognize agent intention predict action sequence fulfill intention assess efficiency approach collect dataset containing household activity generated virtualhome environment accompanied human gaze data viewing video method outperforms stateoftheart technique achieving improvement accuracy intention recognition highlight efficiency method learning important feature human gaze data
place solution pvuw challenge video panoptic segmentation video panoptic segmentation vps challenging task extends image panoptic segmentationvps aim simultaneously classify track segment object video including thing stuff due wide application many downstream task video understanding video editing autonomous driving order deal task video panoptic segmentation wild propose robust integrated video panoptic segmentation solution use dvis framework baseline generate initial mask thenwe add additional image semantic segmentation model improve performance semantic classesfinally method achieves stateoftheart performance vpq score development test phase respectively ultimately ranked vps track pvuw challenge
trainingfree robust interactive video object segmentation interactive video object segmentation crucial video task various application video editing data annotating however current approach struggle accurately segment object across diverse domain recently segment anything model sam introduces interactive visual prompt demonstrates impressive performance across different domain paper propose trainingfree prompt tracking framework interactive video object segmentation ipt leveraging powerful generalization sam although point tracking efficiently capture pixelwise information object video point tend unstable tracked long period resulting incorrect segmentation towards fast robust interaction jointly adopt sparse point box tracking filtering unstable point capturing objectwise information better integrate reference information multiple interaction introduce crossround spacetime module crstm adaptively aggregate mask feature previous round frame enhancing segmentation stability framework demonstrated robust zeroshot video segmentation result popular vos datasets interaction type including davis youtubevos mose maintaining good tradeoff performance interaction time
vcllm video codecs secretly tensor codecs parameter size large language model llm continues expand need large memory footprint high communication bandwidth become significant bottleneck training inference llm mitigate bottleneck various tensor compression technique proposed reduce data size thereby alleviating memory requirement communication pressure research found video codecs despite originally designed compressing video show excellent efficiency compressing various type tensor demonstrate video codecs versatile generalpurpose tensor codecs achieving stateoftheart compression efficiency various task make use hardware video encoding decoding module available gpus create framework capable inference training video codecs repurposed tensor codecs greatly reduces requirement memory capacity communication bandwidth enabling training inference large model consumergrade gpus
videocot video chainofthought dataset active annotation tool multimodal large language model mllms flourishing mainly focus image less attention video especially subfields prompt engineering video chainofthought cot instruction tuning video therefore try explore collection cot datasets video lead video openqa improve reasoning ability mllms unfortunately making video cot datasets easy task given human annotation cumbersome expensive machinegenerated reliable due hallucination issue develop automatic annotation tool combine machine human expert active learning paradigm active learning interactive strategy model human expert way workload human labeling reduced quality dataset guaranteed help automatic annotation tool strive contribute three datasets namely videocot topicqa topiccot furthermore propose simple effective benchmark based collected datasets exploit cot maximize complex reasoning capability mllms extensive experiment demonstrate effectiveness solution
bridging information asymmetry textvideo retrieval datacentric approach online video content rapidly grows task textvideo retrieval tvr becomes increasingly important key challenge tvr information asymmetry video text video inherently richer information textual description often capture fragment complexity paper introduces novel datacentric framework bridge gap enriching textual representation better match richness video content training video segmented eventlevel clip captioned ensure comprehensive coverage retrieval large language model llm generates semantically diverse query capture broader range possible match enhance retrieval efficiency propose query selection mechanism identifies relevant diverse query reducing computational cost improving accuracy method achieves stateoftheart result across multiple benchmark demonstrating power datacentric approach addressing information asymmetry tvr work pave way new research focused leveraging data improve crossmodal retrieval
flying bird object detection dataset surveillance video flying bird dataset surveillance video introduced tailored development performance evaluation flying bird detection algorithm surveillance video dataset comprises video clip amounting frame total among frame contain instance flying bird proposed dataset flying bird surveillance video collected realistic surveillance scenario bird exhibit characteristic inconspicuous feature single frame instance generally small size shape variability flight attribute pose challenge need addressed developing flying bird detection method surveillance video finally advanced video object detection algorithm selected experimentation proposed dataset result demonstrated dataset remains challenging algorithm publicly available please visit dataset download link related processing script
airletters open video dataset character drawn air introduce airletters new video dataset consisting realworld video humangenerated articulated motion specifically dataset requires vision model predict letter human draw air unlike existing video datasets accurate classification prediction airletters rely critically discerning motion pattern integrating longrange information video time extensive evaluation stateoftheart image video understanding model airletters show method perform poorly fall far behind human baseline work show despite recent progress endtoend video understanding accurate representation complex articulated motion task trivial human remains open problem endtoend learning
video summarization technique comprehensive review rapid expansion video content across variety industry including social medium education entertainment surveillance made video summarization essential field study current work survey explores various approach method created video summarizing emphasizing abstractive extractive strategy process extractive summarization involves identification key frame segment source video utilizing method shot boundary recognition clustering hand abstractive summarization creates new content getting essential content video using machine learning model like deep neural network natural language processing reinforcement learning attention mechanism generative adversarial network multimodal learning also include approach incorporate two methodology along discussing us difficulty encountered realworld implementation paper also cover datasets used benchmark technique review attempt provide stateoftheart thorough knowledge current state future direction video summarization research
chatvtg video temporal grounding via chat video dialogue large language model video temporal grounding vtg aim ground specific segment within untrimmed video corresponding given natural language query existing vtg method largely depend supervised learning extensive annotated data laborintensive prone human bias address challenge present chatvtg novel approach utilizes video dialogue large language model llm zeroshot video temporal grounding chatvtg leverage video dialogue llm generate multigranularity segment caption match caption given query coarse temporal grounding circumventing need paired annotation data furthermore obtain precise temporal grounding result employ moment refinement finegrained caption proposal extensive experiment three mainstream vtg datasets including charadessta activitynetcaptions taco demonstrate effectiveness chatvtg chatvtg surpasses performance current zeroshot method
stablizing shape consistency videotovideo editing recent advancement generative ai significantly promoted content creation editing prevailing study extend exciting progress video editing study mainly transfer inherent motion pattern source video edited one result inferior consistency user prompt often observed due lack particular alignment delivered motion edited content address limitation present shapeconsistent video editing method namely paper method decomposes entire editing pipeline several sequential procedure edits first video frame establishes alignment delivered motion user prompt eventually propagates edited content frame based alignment furthermore curate testing benchmark namely davisedit comprehensive evaluation video editing considering various type prompt difficulty experimental result analysis illustrate outperforming performance visual consistency inference efficiency method compared existing stateoftheart study
motion free bframe coding neural video compression typical deep neural video compression network usually follow hybrid approach classical video coding contains two separate module motion coding residual coding addition symmetric autoencoder often used normal architecture motion residual coding paper propose novel approach handle drawback two typical abovementioned architecture call kernelbased motionfree video coding advantage motionfree approach twofold improves coding efficiency network significantly reduces computational complexity thanks eliminating motion estimation motion compensation motion coding timeconsuming engine addition kernelbased autoencoder alleviates blur artifact usually occur conventional symmetric autoencoder consequently improves visual quality reconstructed frame experimental result show proposed framework outperforms sota deep neural video compression network hevcclass b dataset competitive uvg mcljcv datasets addition generates highquality reconstructed frame comparison conventional motion codingbased symmetric autoencoder meanwhile model size much smaller motionbased network around three four time
mcucoder adaptive bitrate learned video compression iot device rapid growth camerabased iot device demand need efficient video compression particularly edge application device face hardware constraint often mb ram unstable internet connection traditional deep video compression method designed highend hardware exceeding capability constrained device consequently video compression scenario often limited mjpeg due high hardware efficiency low complexity paper introduces opensource adaptive bitrate video compression model tailored resourcelimited iot setting mcucoder feature ultralightweight encoder parameter minimal memory footprint making wellsuited edge device mcus mcucoder us similar amount energy mjpeg reduces bitrate mcljcv dataset uvg dataset measured msssim moreover mcucoder support adaptive bitrate streaming generating latent representation sorted importance allowing transmission based available bandwidth ensures smooth realtime video transmission even fluctuating network condition lowresource device source code available httpsgithubcomdskielmcucoder
progressaware video frame captioning image captioning provides isolated description individual image video captioning offer one single narrative entire video clip work explores important middle ground progressaware video captioning frame level novel task aim generate temporally finegrained caption accurately describe frame also capture subtle progression action throughout video sequence despite strong capability existing leading vision language model often struggle discern nuance framewise difference address propose progresscaptioner captioning model designed capture finegrained temporal dynamic within action sequence alongside develop framecap dataset support training framecapeval benchmark assess caption quality result demonstrate progresscaptioner significantly surpasses leading captioning model producing precise caption accurately capture action progression set new standard temporal precision video captioning finally showcase practical application approach specifically aiding keyframe selection advancing video understanding highlighting broad utility
towards openvocabulary video semantic segmentation semantic segmentation video focal point recent research however existing model encounter challenge faced unfamiliar category address introduce open vocabulary video semantic segmentation ovvss task designed accurately segment every pixel across wide range openvocabulary category including novel previously unexplored enhance ovvss performance propose robust baseline integrates spatialtemporal fusion module allowing model utilize temporal relationship across consecutive frame additionally incorporate random frame enhancement module broadening model understanding semantic context throughout entire video sequence approach also includes video text encoding strengthens model capability interpret textual information within video context comprehensive evaluation benchmark datasets vspw cityscape highlight ovvsss zeroshot generalization capability especially handling novel category result validate effectiveness demonstrating improved performance semantic segmentation task across diverse video datasets
vidtok versatile opensource video tokenizer encoding video content compact latent token become fundamental step video generation understanding driven need address inherent redundancy pixellevel representation consequently growing demand highperformance opensource video tokenizers videocentric research gain prominence introduce vidtok versatile video tokenizer delivers stateoftheart performance continuous discrete tokenizations vidtok incorporates several key advancement existing approach model architecture convolutional layer updownsampling module address training instability codebook collapse commonly associated conventional vector quantization vq integrate finite scalar quantization fsq discrete video tokenization improved training strategy including twostage training process use reduced frame rate integrating advancement vidtok achieves substantial improvement existing method demonstrating superior performance across multiple metric including psnr ssim lpips fvd standardized evaluation setting
ghop generative handobject prior interaction reconstruction grasp synthesis propose ghop denoising diffusion based generative prior handobject interaction allows modeling object human hand conditioned object category learn spatial diffusion model capture joint distribution represent human hand via skeletal distance field obtain representation aligned latent signed distance field object show handobject prior serve generic guidance facilitate task like reconstruction interaction clip human grasp synthesis believe model trained aggregating seven diverse realworld interaction datasets spanning across category represents first approach allows jointly generating hand object empirical evaluation demonstrate benefit joint prior videobased reconstruction human grasp synthesis outperforming current taskspecific baseline project website httpsjudyyegithubioghopwww
audioagent leveraging llm audio generation editing composition introduce audioagent multimodal framework audio generation editing composition based text video input conventional approach texttoaudio tta task often make singlepass inference text description straightforward design struggle produce highquality audio given complex text condition method utilize pretrained tta diffusion network audio generation agent work tandem decomposes text condition atomic specific instruction call agent audio generation audioagent generate highquality audio closely aligned provided text video exhibiting complex multiple event supporting variablelength variablevolume generation videotoaudio vta task existing method require training timestamp detector synchronize video event generated audio process tedious timeconsuming instead propose simpler approach finetuning pretrained large language model llm eg obtain semantic temporal condition bridge video audio modality consequently framework contributes comprehensive solution tta vta task without substantial computational overhead training
faster generation closer look clip image embeddings impact spatiotemporal crossattentions paper investigates role clip image embeddings within stable video diffusion svd framework focusing impact video generation quality computational efficiency finding indicate clip embeddings crucial aesthetic quality significantly contribute towards subject background consistency video output moreover computationally expensive crossattention mechanism effectively replaced simpler linear layer layer computed first diffusion inference step output cached reused throughout inference process thereby enhancing efficiency maintaining highquality output building insight introduce vcut trainingfree approach optimized efficiency within svd architecture vcut eliminates temporal crossattention replaces spatial crossattention onetime computed linear layer significantly reducing computational load implementation vcut lead reduction multipleaccumulate operation mac per video decrease model parameter achieving reduction latency compared baseline approach demonstrates conditioning semantic binding stage sufficient eliminating need continuous computation across inference step setting new standard efficient video generation
domain translation framework adversarial denoising diffusion model generate synthetic datasets echocardiography image currently medical image domain translation operation show high demand researcher clinician amongst capability task allows generation new medical image sufficiently high image quality making clinically relevant deep learning dl architecture specifically deep generative model widely used generate translate image one domain another proposed framework relies adversarial denoising diffusion model ddm synthesize echocardiography image perform domain translation contrary generative adversarial network gans ddms able generate high quality image sample large diversity ddm combined gan ability generate new data completed even faster sampling time work trained adversarial ddm combined gan learn reverse denoising process relying guide image making sure relevant anatomical structure echocardiography image kept represented generated image sample several domain translation operation result verified generative model able synthesize high quality image sample mse psnr db ssim proposed method showed high generalization ability introducing framework create echocardiography image suitable used clinical research purpose
vidman exploiting implicit dynamic video diffusion model effective robot manipulation recent advancement utilizing largescale video data learning video generation model demonstrate significant potential understanding complex physical dynamic suggests feasibility leveraging diverse robot trajectory data develop unified dynamicsaware model enhance robot manipulation however given relatively small amount available robot data directly fitting data without considering relationship visual observation action could lead suboptimal data utilization end propose vidman video diffusion robot manipulation novel framework employ twostage training mechanism inspired dualprocess theory neuroscience enhance stability improve data utilization efficiency specifically first stage vidman pretrained open xembodiment dataset oxe predicting future visual trajectory video denoising diffusion manner enabling model develop long horizontal awareness environment dynamic second stage flexible yet effective layerwise selfattention adapter introduced transform vidman efficient inverse dynamic model predicts action modulated implicit dynamic knowledge via parameter sharing vidman framework outperforms stateoftheart baseline model calvin benchmark achieving relative improvement demonstrates precision gain oxe smallscale dataset result provide compelling evidence world model significantly enhance precision robot action prediction code model public
deformationrecovery diffusion model drdm instance deformation image manipulation synthesis medical imaging diffusion model shown great potential synthetic image generation task however model often struggle interpretable connection generated existing image could create illusion address challenge research proposes novel diffusionbased generative model based deformation diffusion recovery model named deformationrecovery diffusion model drdm diverges traditional scoreintensity latent featurebased approach emphasizing morphological change deformation field rather direct image synthesis achieved introducing topologicalpreserving deformation field generation method randomly sample integrates set multiscale deformation vector field dvf drdm trained learn recover unreasonable deformation component thereby restoring randomly deformed image realistic distribution innovation facilitate generation diverse anatomically plausible deformation enhancing data augmentation synthesis analysis downstream task fewshot learning image registration experimental result cardiac mri pulmonary ct show drdm capable creating diverse large image size deformation scale highquality negative rate jacobian matrix determinant lower deformation field experimental result downstream task image segmentation image registration indicate significant improvement resulting drdm showcasing potential model advance image manipulation synthesis medical imaging beyond project page
videophy evaluating physical commonsense video generation recent advance internetscale video data pretraining led development texttovideo generative model create highquality video across broad range visual concept synthesize realistic motion render complex object hence generative model potential become generalpurpose simulator physical world however unclear far goal existing texttovideo generative model end present videophy benchmark designed assess whether generated video follow physical commonsense realworld activity eg marble roll placed slanted surface specifically curate diverse prompt involve interaction various material type physical world eg solidsolid solidfluid fluidfluid generate video conditioned caption diverse stateoftheart texttovideo generative model including open model eg cogvideox closed model eg lumiere dream machine human evaluation reveals existing model severely lack ability generate video adhering given text prompt also lack physical commonsense specifically best performing model generates video adhere caption physical law instance videophy thus highlight video generative model far accurately simulating physical world finally propose autoevaluator videoconphysics assess performance reliably newly released model
tiled diffusion image tiling seamless connection disparate image create coherent visual field crucial application texture creation video game asset development digital art traditionally tile constructed manually method pose significant limitation scalability flexibility recent research attempted automate process using generative model however current approach primarily focus tiling texture manipulating model singleimage generation without inherently supporting creation multiple interconnected tile across diverse domain paper present tiled diffusion novel approach extends capability diffusion model accommodate generation cohesive tiling pattern across various domain image synthesis require tiling method support wide range tiling scenario selftiling complex manytomany connection enabling seamless integration multiple image tiled diffusion automates tiling process eliminating need manual intervention enhancing creative possibility various application seamlessly tiling existing image tiled texture creation synthesis
baking gaussian splatting diffusion denoiser fast scalable singlestage generation reconstruction existing feedforward method mainly rely multiview diffusion model guarantee consistency method easily collapse changing prompt view direction mainly handle objectcentric case paper propose novel singlestage diffusion model diffusiongs object generation scene reconstruction single view diffusiongs directly output gaussian point cloud timestep enforce view consistency allow model generate robustly given prompt view direction beyond objectcentric input plus improve capability generality diffusiongs scale training data developing sceneobject mixed training strategy experiment show diffusiongs yield improvement psnrfid object scene stateoftheart method without depth estimator plus method enjoys faster speed gpu project page show video interactive result
sangria surgical video scene graph optimization surgical workflow prediction graphbased holistic scene representation facilitate surgical workflow understanding recently demonstrated significant success however task often hindered limited availability densely annotated surgical scene data work introduce endtoend framework generation optimization surgical scene graph downstream task approach leverage flexibility graphbased spectral clustering generalization capability foundation model generate unsupervised scene graph learnable property reinforce initial spatial graph sparse temporal connection using local match consecutive frame predict temporally consistent cluster across temporal neighborhood jointly optimizing spatiotemporal relation node feature dynamic scene graph downstream task phase segmentation address costly annotationburdensome task semantic scene comprehension scene graph generation surgical video using weak surgical phase label incorporating effective intermediate scene representation disentanglement step within pipeline solution outperforms sota cataract dataset accuracy score surgical workflow recognition
wolf dense video captioning world summarization framework propose wolf world summarization framework accurate video captioning wolf automated captioning framework adopts mixtureofexperts approach leveraging complementary strength vision language model vlms utilizing image video model framework capture different level information summarizes efficiently approach applied enhance video understanding autolabeling captioning evaluate caption quality introduce capscore llmbased metric assess similarity quality generated caption compared ground truth caption build four humanannotated datasets three domain autonomous driving general scene robotics facilitate comprehensive comparison show wolf achieves superior captioning performance compared stateoftheart approach research community cogagent commercial solution instance comparison wolf improves capscore qualitywise similaritywise challenging driving video finally establish benchmark video captioning introduce leaderboard aiming accelerate advancement video understanding captioning data alignment webpage
snapcap efficient snapshot compressive video captioning video captioning vc challenging multimodal task since requires describing scene language understanding various complex video machine traditional vc follows imagingcompressiondecodingandthencaptioning pipeline compression pivot storage transmission however pipeline potential shortcoming inevitable ie information redundancy resulting low efficiency information loss sampling process captioning address problem paper propose novel vc pipeline generate caption directly compressed measurement captured snapshot compressive sensing camera dub model snapcap specific benefiting signal simulation access obtain abundant measurementvideoannotation data pair model besides better extract languagerelated visual representation compressed measurement propose distill knowledge video via pretrained clip plentiful languagevision association guide learning snapcap demonstrate effectiveness snapcap conduct experiment two widelyused vc datasets qualitative quantitative result verify superiority pipeline conventional vc pipeline particular compared captionafterreconstruction method snapcap run least faster achieve better caption result
eventhdr event highspeed hdr video beyond event camera innovative neuromorphic sensor asynchronously capture scene dynamic due eventtriggering mechanism camera record event stream much shorter response latency higher intensity sensitivity compared conventional camera basis feature previous work attempted reconstruct high dynamic range hdr video event either suffered unrealistic artifact failed provide sufficiently high frame rate paper present recurrent convolutional neural network reconstruct highspeed hdr video event sequence key frame guidance prevent potential error accumulation caused sparse event data additionally address problem severely limited real dataset develop new optical system collect realworld dataset paired highspeed hdr video event stream facilitating future research field dataset provides first real paired dataset eventtohdr reconstruction avoiding potential inaccuracy simulation strategy experimental result demonstrate method generate highquality highspeed hdr video explore potential work crosscamera reconstruction downstream computer vision task including object detection panoramic segmentation optical flow estimation monocular depth estimation hdr scenario
learning camera movement control realworld drone video study seek automate camera movement control filming existing subject attractive video contrasting creation nonexistent content directly generating pixel select drone video test case due rich challenging motion pattern distinctive viewing angle precise control existing ai videography method struggle limited appearance diversity simulation training high cost recording expert operation difficulty designing heuristicbased goal cover scenario avoid issue propose scalable method involves collecting realworld training data improve diversity extracting camera trajectory automatically minimize annotation cost training effective architecture rely heuristic specifically collect highquality trajectory running reconstruction online video connecting camera pose consecutive frame formulate camera path using kalman filter identify remove lowquality data moreover introduce dvgformer autoregressive transformer leverage camera path image past frame predict camera movement next frame evaluate system across synthetic natural scene real city scan show system effectively learns perform challenging camera movement navigating obstacle maintaining low altitude increase perceived speed orbiting tower building useful recording highquality video data code available dvgformergithubio
pandora towards general world model natural language action video state world model simulate future state world response different action facilitate interactive content creation provides foundation grounded longhorizon reasoning current foundation model fully meet capability general world model large language model llm constrained reliance language modality limited understanding physical world video model lack interactive action control world simulation paper make step towards building general world model introducing pandora hybrid autoregressivediffusion model simulates world state generating video allows realtime control freetext action pandora achieves domain generality video consistency controllability largescale pretraining instruction tuning crucially pandora bypass cost trainingfromscratch integrating pretrained llm pretrained video model requiring additional lightweight finetuning illustrate extensive output pandora across diverse domain indooroutdoor naturalurban humanrobot etc result indicate great potential building stronger general world model largerscale training
unsupervised video summarization via reinforcement learning trained evaluator paper present novel approach unsupervised video summarization using reinforcement learning aim address existing limitation current unsupervised method including unstable training adversarial generatordiscriminator architecture reliance handcrafted reward function quality evaluation proposed method based concept concise informative summary result reconstructed video closely resembles original summarizer model assigns importance score frame generates video summary proposed scheme reinforcement learning coupled unique reward generation pipeline employed train summarizer model reward generation pipeline train summarizer create summary lead improved reconstruction comprises generator model capable reconstructing masked frame partially masked video along reward mechanism compare reconstructed video summary original video generator trained selfsupervised manner reconstruct randomly masked frame enhancing ability generate accurate summary training pipeline result summarizer model better mimic humangenerated video summary compared method relying handcrafted reward training process consists two stable isolated training step unlike adversarial architecture experimental result demonstrate promising performance fscores tvsum summe datasets respectively additionally inference stage time faster previously reported stateoftheart method
world simulator good story presenter consecutive eventsbased benchmark future long video generation current stateoftheart video generative model produce commercialgrade video highly realistic detail however still struggle coherently present multiple sequential event story specified prompt foreseeable essential capability future long video generation scenario example top generative model still fail generate video short simple story put elephant refrigerator existing detailoriented benchmark primarily focus finegrained metric like aesthetic quality spatialtemporal consistency fall short evaluating model ability handle eventlevel story presentation address gap introduce storyeval storyoriented benchmark specifically designed assess texttovideo model storycompletion capability storyeval feature prompt spanning class representing short story composed consecutive event employ advanced visionlanguage model verify completion event generated video applying unanimous voting method enhance reliability method ensure high alignment human evaluation evaluation model reveals challenge none exceeding average storycompletion rate storyeval provides new benchmark advancing model highlight challenge opportunity developing nextgeneration solution coherent storydriven video generation
identitypreserving texttovideo generation frequency decomposition identitypreserving texttovideo generation aim create highfidelity video consistent human identity important task video generation remains open problem generative model paper push technical frontier two direction resolved literature tuningfree pipeline without tedious casebycase finetuning frequencyaware heuristic identitypreserving ditbased control scheme propose consisid tuningfree ditbased controllable model keep human identity consistent generated video inspired prior finding frequency analysis diffusion transformer employ identitycontrol signal frequency domain facial feature decomposed lowfrequency global feature highfrequency intrinsic feature first lowfrequency perspective introduce global facial extractor encodes reference image facial key point latent space generating feature enriched lowfrequency information feature integrated shallow layer network alleviate training challenge associated dit second highfrequency perspective design local facial extractor capture highfrequency detail inject transformer block enhancing model ability preserve finegrained feature propose hierarchical training strategy leverage frequency information identity preservation transforming vanilla pretrained video generation model model extensive experiment demonstrate frequencyaware heuristic scheme provides optimal control solution ditbased model thanks scheme consisid generates highquality identitypreserving video making stride towards effective code httpsgithubcompkuyuangroupconsisid
unified static dynamic network efficient temporal filtering video grounding inspired activitysilent persistent activity mechanism human visual perception biology design unified static dynamic network unisdnet learn semantic association video textaudio query crossmodal environment efficient video grounding static modeling devise novel residual structure resmlp boost global comprehensive interaction video segment query achieving effective semantic enhancementsupplement dynamic modeling effectively exploit three characteristic persistent activity mechanism network design better video context comprehension specifically construct diffusely connected video clip graph basis sparse temporal masking reflect shortterm effect relationship innovatively consider temporal distance relevance joint auxiliary evidence clue design multikernel temporal gaussian filter expand context clue highdimensional space simulating complex visual perception conduct element level filtering convolution operation neighbour clip node message passing stage finally generating ranking candidate proposal unisdnet applicable natural language video grounding nlvg spoken language video grounding slvg task unisdnet achieves sota performance three widely used datasets nlvg well three datasets slvg eg reporting new record activitynet caption taco facilitate field collect two new datasets charadessta speech taco speech slvg task meanwhile inference speed unisdnet faster strong multiquery benchmark code available httpsgithubcomxianshunisdnet
turning text imagery captivating visual video ability visualize structure multiple perspective crucial comprehensive planning presentation paper introduces advanced application generative model akin stable video diffusion tailored architectural visualization explore potential model create consistent multiperspective video building single image generate design video directly textual description proposed method enhances design process offering rapid prototyping cost time efficiency enriched creative space architect designer harnessing power ai approach accelerates visualization architectural concept also enables interactive immersive experience client stakeholder advancement architectural visualization represents significant leap forward allowing deeper exploration design possibility effective communication complex architectural idea
refereverything towards segmenting everything speak video present rem framework segmenting wide range concept video described natural language method capitalizes visuallanguage representation learned video diffusion model internetscale datasets key insight approach preserving much generative model original representation possible finetuning narrowdomain referral object segmentation datasets result framework accurately segment track rare unseen object despite trained object mask limited set category additionally generalize nonobject dynamic concept wave crashing ocean demonstrated newly introduced benchmark referral video process segmentation refvps experiment show rem performs par stateoftheart approach indomain datasets like refdavis outperforming twelve point term region similarity outofdomain data leveraging power internetscale pretraining
pathway image manifold image editing via video generation recent advance image editing driven image diffusion model shown remarkable progress however significant challenge remain model often struggle follow complex edit instruction accurately frequently compromise fidelity altering key element original image simultaneously video generation made remarkable stride model effectively function consistent continuous world simulator paper propose merging two field utilizing imagetovideo model image editing reformulate image editing temporal process using pretrained video model create smooth transition original image desired edit approach traverse image manifold continuously ensuring consistent edits preserving original image key aspect approach achieves stateoftheart result textbased image editing demonstrating significant improvement edit accuracy image preservation visit project page
anatomical featureprioritized loss enhanced mr ct translation medical image synthesis precision localized structural detail crucial particularly addressing specific clinical requirement identification measurement fine structure traditional method image translation synthesis generally optimized global image reconstruction often fall short providing finesse required detailed local analysis study represents step toward addressing challenge introducing novel anatomical featureprioritized afp loss function synthesis process method enhances reconstruction focusing clinically significant structure utilizing feature pretrained model designed specific downstream task segmentation particular anatomical region afp loss function replace complement global reconstruction method ensuring balanced emphasis global image fidelity local structural detail various implementation loss function explored including integration different synthesis network ganbased cnnbased model approach applied evaluated two context lung mr ct translation focusing highquality reconstruction bronchial structure using private dataset pelvis mr ct synthesis targeting accurate representation organ muscle utilizing public dataset challenge leverage embeddings pretrained segmentation model specific anatomical region demonstrate capability afp loss prioritize accurately reconstruct essential feature tailored approach show promising potential enhancing specificity practicality medical image synthesis clinical application
leveraging usergenerated metadata online video cover song identification youtube rich source cover song since platform organized term video rather song retrieval cover trivial field cover song identification address problem provides approach usually rely audio content however including usergenerated video metadata available youtube promise improved identification result paper propose multimodal approach cover song identification online video platform combine entity resolution model audiobased approach using ranking model finding implicate leveraging usergenerated metadata stabilize cover song identification performance youtube
spatial decomposition temporal fusion based inter prediction learned video compression video compression performance closely related accuracy inter prediction tends difficult obtain accurate inter prediction local video region inconsistent motion occlusion traditional video coding standard propose various technology handle motion inconsistency occlusion recursive partition geometric partition longterm reference however existing learned video compression scheme focus obtaining overall minimized prediction error averaged region ignoring motion inconsistency occlusion local region paper propose spatial decomposition temporal fusion based inter prediction learned video compression handle motion inconsistency propose decompose video structure detail sdd component first perform sddbased motion estimation sddbased temporal context mining structure detail component generate shortterm temporal context handle occlusion propose propagate longterm temporal context recurrently accumulating temporal information historical reference feature fuse shortterm temporal context sddbased motion model long shortterm temporal context fusion proposed learned video codec obtain accurate inter prediction comprehensive experimental result demonstrate codec outperforms reference software common test datasets psnr msssim
analysis neural video compression network video coding increasing effort bringing highquality virtual reality technology market efficient video compression gain importance stateoftheart video coding standard integrates dedicated tool video considerable effort put designing projection format improved compression efficiency fastevolving field neural video compression network nvcs effect different projection format overall compression performance yet investigated thus unclear whether resampling conventional equirectangular projection erp projection format yield similar gain nvcs hybrid video codecs format perform best paper analyze several generation nvcs extensive set projection format respect compression performance video based analysis find projection format resampling yield significant improvement compression performance also nvcs adjusted cubemap projection acp equatorial cylindrical projection ecp show perform best achieve rate saving compared erp based wspsnr recent nvc remarkably observed rate saving higher emphasizing importance projection format resampling nvcs
universal representation learning video editing component paper focus understanding predominant video creation pipeline ie compositional video editing six main type editing component including video effect animation transition filter sticker text contrast existing visual representation learning visual material ie imagesvideos aim learn visual representation editing actionscomponents generally applied raw material start proposing first largescale dataset editing component video creation cover editing component video video dataset rendered various imagevideo material single editing component support atomic visual understanding different editing component also benefit several downstream task eg editing component recommendation editing component recognitionretrieval etc existing visual representation method perform poorly difficult disentangle visual appearance editing component raw material end benchmark popular alternative solution propose novel method learns attend appearance editing component regardless raw material method achieves favorable result editing component retrievalrecognition compared alternative solution user study also conducted show representation cluster visually similar editing component better alternative furthermore learned representation used transition recommendation task achieve stateoftheart result autotransition dataset code dataset available
elysium exploring objectlevel perception video via mllm multimodal large language model mllms demonstrated ability perceive object still image application videorelated task object tracking remains understudied lack exploration primarily due two key challenge firstly extensive pretraining largescale video datasets required equip mllms capability perceive object across multiple frame understand interframe relationship secondly processing large number frame within context window large language model llm impose significant computational burden address first challenge introduce largescale video dataset supported three task single object tracking sot referring single object tracking rsot video referring expression generation videoreg contains million annotated video frame corresponding object box description leveraging dataset conduct training mllms propose tokencompression model tselector tackle second challenge proposed approach elysium exploring objectlevel perception video via mllm endtoend trainable mllm attempt conduct objectlevel task video without requiring additional plugin expert model code datasets available httpsgithubcomhonwongelysium
text mass modeling stochastic embedding textvideo retrieval increasing prevalence video clip sparked growing interest textvideo retrieval recent advance focus establishing joint embedding space text video relying consistent embedding representation compute similarity however text content existing datasets generally short concise making hard fully describe redundant semantics video correspondingly single text embedding may less expressive capture video embedding empower retrieval study propose new stochastic text modeling method tmass ie text modeled stochastic embedding enrich text embedding flexible resilient semantic range yielding text mass specific introduce similarityaware radius module adapt scale text mass upon given textvideo pair plus design develop support text regularization control text mass training inference pipeline also tailored fully exploit text mass accurate retrieval empirical evidence suggests tmass effectively attracts relevant textvideo pair distancing irrelevant one also enables determination precise text embeddings relevant pair experimental result show substantial improvement tmass baseline also tmass achieves stateoftheart performance five benchmark datasets including msrvtt lsmdc didemo vatex charade
stllm large language model effective temporal learner large language model llm showcased impressive capability text comprehension generation prompting research effort towards video llm facilitate humanai interaction video level however effectively encode understand video videobased dialogue system remains solved paper investigate straightforward yet unexplored question feed spatialtemporal token llm thus delegating task video sequence modeling llm surprisingly simple approach yield significant improvement video understanding based upon propose stllm effective videollm baseline spatialtemporal sequence modeling inside llm furthermore address overhead stability issue introduced uncompressed video token within llm develop dynamic masking strategy tailormade training objective particularly long video also designed globallocal input module balance efficiency effectiveness consequently harness llm proficient spatialtemporal modeling upholding efficiency stability extensive experimental result attest effectiveness method concise model training pipeline stllm establishes new stateoftheart result videochatgptbench mvbench code available httpsgithubcomtencentarcstllm
parameterefficient instanceadaptive neural video compression learningbased neural video codecs nvcs emerged compelling alternative standard video codecs demonstrating promising performance simple easily maintainable pipeline however nvcs often fall short compression performance occasionally exhibit poor generalization capability due inferenceonly compression scheme dependence training data instanceadaptive video compression technique recently suggested viable solution finetuning encoder decoder network particular test instance video however finetuning model parameter incurs high computational cost increase bitrates often lead unstable training work propose parameterefficient instanceadaptive video compression framework inspired remarkable success parameterefficient finetuning largescale neural network model propose use lightweight adapter module easily attached pretrained nvcs finetuned test video sequence resulting algorithm significantly improves compression performance reduces encoding time compared existing instantadaptive video compression algorithm furthermore suggested finetuning method enhances robustness training process allowing proposed method widely used many practical setting conducted extensive experiment various standard benchmark datasets including uvg mcljvc hevc sequence experimental result shown significant improvement ratedistortion rd curve db psnr bd rate compared baseline nvc code available httpsgithubcomohsngjunpevc
rmtbvqa recurrent memory transformerbased blind video quality assessment enhanced video content recent advance deep learning numerous algorithm developed enhance video quality reduce visual artifact improve perceptual quality however little research reported quality assessment enhanced content evaluation enhancement method often based quality metric designed compression application paper propose novel blind deep video quality assessment vqa method specifically enhanced video content employ new recurrent memory transformer rmt based network architecture obtain video quality representation optimized novel contentqualityaware contrastive learning strategy based new database containing training patch enhanced content extracted quality representation combined linear regression generate videolevel quality index proposed method rmtbvqa evaluated vdpve vqa dataset perceptual video enhancement database fivefold cross validation result show superior correlation performance compared ten existing noreference quality metric
ubiss unified framework bimodal semantic summarization video surge amount video data video summarization technique including visualmodalvm textualmodaltm summarization attracting attention however unimodal summarization inevitably loses rich semantics video paper focus comprehensive video summarization task named bimodal semantic summarization video bissv specifically first construct largescale dataset bid video vmsummary tmsummary triplet format unlike traditional processing method construction procedure contains vmsummary extraction algorithm aiming preserve salient content within long video based bid propose unified framework ubiss bissv task model saliency information video generates tmsummary vmsummary simultaneously optimize model listwise rankingbased objective improve capacity capture highlight lastly propose metric provide joint evaluation bimodal summary experiment show unified framework achieves better performance multistage summarization pipeline code data available httpsgithubcommeiyutinggubiss
mama metaoptimized angular margin contrastive framework videolanguage representation learning data quality stand forefront deciding effectiveness videolanguage representation learning however videotext pair previous data typically align perfectly might lead videolanguage representation accurately reflect crossmodal semantics moreover previous data also possess uneven distribution concept thereby hampering downstream performance across unpopular subject address problem propose mama new approach learning videolanguage representation utilizing contrastive objective subtractive angular margin regularize crossmodal representation effort reach perfect similarity furthermore adapt nonuniform concept distribution mama utilizes multilayer perceptron mlpparameterized weighting function map loss value sample weight enable dynamic adjustment model focus throughout training training guided small amount unbiased metadata augmented videotext data generated large visionlanguage model mama improves videolanguage representation achieve superior performance commonly used video question answering textvideo retrieval datasets code model data made available httpsnguyentthonggithubiomama
visa reasoning video object segmentation via large language model existing video object segmentation vos relies explicit user instruction category mask short phrase restricting ability perform complex video segmentation requiring reasoning world knowledge paper introduce new task reasoning video object segmentation reasonvos task aim generate sequence segmentation mask response implicit text query require complex reasoning ability based world knowledge video context crucial structured environment understanding objectcentric interaction pivotal development embodied ai tackle reasonvos introduce visa videobased large language instructed segmentation assistant leverage world knowledge reasoning capability multimodal llm possessing ability segment track object video mask decoder moreover establish comprehensive benchmark consisting instructionmask sequence pair diverse video incorporates complex world knowledge reasoning segmentation task instructiontuning evaluation purpose reasonvos model experiment conducted datasets demonstrate effectiveness visa tackling complex reasoning segmentation vanilla referring segmentation video image domain code dataset available httpsgithubcomcilinyanvisa
disentangling spatiotemporal knowledge weakly supervised object detection segmentation surgical video weakly supervised video object segmentation wsvos enables identification segmentation map without requiring extensive training dataset object mask relying instead coarse video label indicating object presence current stateoftheart method either require multiple independent stage processing employ motion cue case endtoend trainable network lack segmentation accuracy part due difficulty learning segmentation map video transient object presence limit application wsvos semantic annotation surgical video multiple surgical tool frequently move field view problem difficult typically encountered wsvos paper introduces video spatiotemporal disentanglement network vdstnet framework disentangle spatiotemporal information using semidecoupled knowledge distillation predict highquality class activation map cam teacher network designed resolve temporal conflict specific object location timing video provided work student network integrates information time leveraging temporal dependency demonstrate efficacy framework public reference dataset challenging surgical video dataset object average present less annotated frame method outperforms stateoftheart technique generates superior segmentation mask videolevel weak supervision
unlocking exocentric videolanguage data egocentric video representation learning present embed egocentric model built exocentric data method designed transform exocentric videolanguage data egocentric video representation learning largescale exocentric data cover diverse activity significant potential egocentric learning inherent disparity egocentric exocentric data pose challenge utilizing one view seamlessly egocentric video predominantly feature closeup handobject interaction whereas exocentric video offer broader perspective human activity additionally narrative egocentric datasets typically actioncentric closely linked visual content contrast narrative style found exocentric datasets address challenge employ data transformation framework adapt exocentric data egocentric training focusing identifying specific video clip emphasize handobject interaction transforming narration style align egocentric perspective applying vision language style transfer framework creates new egocentric dataset derived exocentric videolanguage data extensive evaluation demonstrate effectiveness embed achieving stateoftheart result across various egocentric downstream task including absolute improvement multiinstance retrieval egtea classification benchmark zeroshot setting furthermore embed enables egocentric videolanguage model perform competitively exocentric task finally showcase embeds application across various exocentric datasets exhibiting strong generalization capability applied different exocentric datasets
temporal divideandconquer anomaly action localization semisupervised video hierarchical transformer anomaly action detection localization play essential role security advanced surveillance system however due tremendous amount surveillance video available data task unlabeled semilabeled video class known location anomaly event unknown work target anomaly localization semisupervised video mainstream direction addressing task focused segmentlevel multiinstance learning generation pseudo label aim explore promising yet unfulfilled direction solve problem learning temporal relation within video order locate anomaly event end propose hierarchical transformer model designed evaluate significance observed action anomalous video divideandconquer strategy along temporal axis approach segment parent video hierarchically multiple temporal child instance measure influence child node classifying abnormality parent video evaluating model two wellknown anomaly detection datasets ucfcrime shanghaitech prof ability interpret observed action within video localize anomalous one proposed approach outperforms previous work relying segmentlevel multipleinstance learning approach reaching promising performance compared recent pseudolabelingbased approach
prompting videolanguage foundation model domainspecific finegrained heuristic video question answering video question answering videoqa represents crucial intersection video understanding language processing requiring discriminative unimodal comprehension sophisticated crossmodal interaction accurate inference despite advancement multimodal pretrained model videolanguage foundation model system often struggle domainspecific videoqa due generalized pretraining objective addressing gap necessitates bridging divide broad crossmodal knowledge specific inference demand videoqa task end introduce heurvidqa framework leverage domainspecific entityaction heuristic refine pretrained videolanguage foundation model approach treat model implicit knowledge engine employing domainspecific entityaction prompter direct model focus toward precise cue enhance reasoning delivering finegrained heuristic improve model ability identify interpret key entity action thereby enhancing reasoning capability extensive evaluation across multiple videoqa datasets demonstrate method significantly outperforms existing model underscoring importance integrating domainspecific knowledge videolanguage model accurate contextaware videoqa
pseudolabeling keyword refining fewsupervised video captioning video captioning generate sentence describes video content existing method always require number caption eg per video train model quite costly work explore possibility using one groundtruth sentence introduce new task named fewsupervised video captioning specifically propose fewsupervised video captioning framework consists lexically constrained pseudolabeling module keywordrefined captioning module unlike random sampling natural language processing may cause invalid modification ie edit word former module guide model edit word using action eg copy replace insert delete pretrained tokenlevel classifier finetunes candidate sentence pretrained language model meanwhile former employ repetition penalized sampling encourage model yield concise pseudolabeled sentence less repetition selects relevant sentence upon pretrained videotext model moreover keep semantic consistency pseudolabeled sentence video content develop transformerbased keyword refiner videokeyword gated fusion strategy emphasize relevant word extensive experiment several benchmark demonstrate advantage proposed approach fewsupervised fullysupervised scenario code implementation available
vidmorp video moment retrieval pretraining unlabeled video wild given natural language query video moment retrieval aim localize described temporal moment untrimmed video major challenge task heavy dependence laborintensive annotation training unlike existing work directly train model manually curated data propose novel paradigm reduce annotation cost pretraining model unlabeled realworld video support introduce video moment retrieval pretraining vidmorp largescale dataset collected minimal human intervention consisting video captured wild pseudo annotation direct pretraining imperfect pseudo annotation however present significant challenge including mismatched sentencevideo pair imprecise temporal boundary address issue propose recorrect algorithm comprises two main phase semanticsguided refinement memoryconsensus correction semanticsguided refinement enhances pseudo label leveraging semantic similarity video frame clean unpaired data make initial adjustment temporal boundary following memoryconsensus correction phase memory bank track model prediction progressively correcting temporal boundary based consensus within memory comprehensive experiment demonstrate recorrects strong generalization ability across multiple downstream setting zeroshot recorrect achieves best fullysupervised performance two benchmark unsupervised recorrect reach code dataset pretrained model available httpsgithubcombaopjvidmorp
semisupervised contrastive learning controllable videotomusic retrieval content creator often use music enhance video soundtrack movie background music video blog social medium content however identifying best music video difficult timeconsuming task address challenge propose novel framework automatically retrieving matching music clip given video vice versa approach leverage annotated music label well inherent artistic correspondence visual music element distinct previous crossmodal music retrieval work method combine selfsupervised supervised training objective use selfsupervised labelsupervised contrastive learning train joint embedding space music video show effectiveness approach using music genre label supervised training component framework generalized music annotation eg emotion instrument etc furthermore method enables finegrained control much retrieval process focus selfsupervised v label information inference time evaluate learned embeddings variety videotomusic musictovideo retrieval task experiment show proposed approach successfully combine selfsupervised supervised objective effective controllable musicvideo retrieval
stitch contrast human action segmentation model using trimmed skeleton video existing skeletonbased human action classification model rely welltrimmed actionspecific skeleton video training testing precluding scalability realworld application untrimmed video exhibiting concatenated action predominant overcome limitation recently introduced skeleton action segmentation model involve untrimmed skeleton video endtoend training model optimized provide framewise prediction length testing video simultaneously realizing action localization classification yet achieving improvement imposes framewise annotated skeleton video remains timeconsuming practice paper feature novel framework skeletonbased action segmentation trained short trimmed skeleton video run longer untrimmed video approach implemented three step stitch contrast segment first stitch proposes temporal skeleton stitching scheme treat trimmed skeleton video elementary human motion compose semantic space sampled generate multiaction stitched sequence contrast learns contrastive representation stitched sequence novel discrimination pretext task enables skeleton encoder learn meaningful actiontemporal context improve action segmentation finally segment relates proposed method action segmentation learning segmentation layer handling particular data availability experiment involve trimmed source dataset untrimmed target dataset adaptation formulation realworld skeletonbased human action segmentation evaluate effectiveness proposed method
friendsqa new largescale deep video understanding dataset finegrained topic categorization story video video question answering videoqa aim answer natural language question according given video although existing model perform well factoid videoqa task still face challenge deep video understanding dvu task focus story video compared factoid video significant feature story video storyline composed complex interaction longrange evolvement core story topic including character action location understanding topic requires model possess dvu capability however existing dvu datasets rarely organize question according story topic making difficult comprehensively assess videoqa model dvu capability complex storyline additionally question quantity video length dataset limited high labor cost handcrafted dataset building method paper devise large language model based multiagent collaboration framework storymind automatically generate new largescale dvu dataset dataset friendsqa derived renowned sitcom friend average episode length second contains question evenly distributed across finegrained topic finally conduct comprehensive experiment stateoftheart videoqa model using friendsqa dataset
diffusionact controllable diffusion autoencoder oneshot face reenactment videodriven neural face reenactment aim synthesize realistic facial image successfully preserve identity appearance source face transferring target head pose facial expression existing ganbased method suffer either distortion visual artifact poor reconstruction quality ie background several important appearance detail hair stylecolor glass accessory faithfully reconstructed recent advance diffusion probabilistic model dpms enable generation highquality realistic image end paper present diffusionact novel method leverage photorealistic image generation diffusion model perform neural face reenactment specifically propose control semantic space diffusion autoencoder diffae order edit facial pose input image defined head pose orientation facial expression method allows oneshot self crosssubject reenactment without requiring subjectspecific finetuning compare stateoftheart gan diffusionbased method showing better onpar reenactment performance
towards generalizable tumor synthesis tumor synthesis enables creation artificial tumor medical image facilitating training ai model tumor detection segmentation however success tumor synthesis hinge creating visually realistic tumor generalizable across multiple organ furthermore resulting ai model capable detecting real tumor image sourced different domain eg hospital paper made progressive stride toward generalizable tumor synthesis leveraging critical observation earlystage tumor tend similar imaging characteristic computed tomography ct whether originate liver pancreas kidney ascertained generative ai model eg diffusion model create realistic tumor generalized range organ even trained limited number tumor example one organ moreover shown ai model trained synthetic tumor generalized detect segment real tumor ct volume encompassing broad spectrum patient demographic imaging protocol healthcare facility
generalizing deepfake video detection plugandplay videolevel blending spatiotemporal adapter tuning three key challenge hinder development current deepfake video detection temporal feature complex diverse identify general temporal artifact enhance model generalization spatiotemporal model often lean heavily one type artifact ignore ensure balanced learning video naturally resourceintensive tackle efficiency without compromising accuracy paper attempt tackle three challenge jointly first inspired notable generality using imagelevel blending data image forgery detection investigate whether videolevel blending effective video perform thorough analysis identify previously underexplored temporal forgery artifact facial feature drift ffd commonly exists across different forgery reproduce ffd propose novel videolevel blending data vb vb implemented blending original image warped version framebyframe serving hard negative sample mine general artifact second carefully design lightweight spatiotemporal adapter sta equip pretrained image model vits cnns ability capture spatial temporal feature jointly efficiently sta designed twostream varying kernel size allowing process spatial temporal feature separately extensive experiment validate effectiveness proposed method show approach generalize well previously unseen forgery video even latest generation method
bytheway boost texttovideo generation model higher quality trainingfree way texttovideo generation model offering convenient visual creation recently garnered increasing attention despite substantial potential generated video may present artifact including structural implausibility temporal inconsistency lack motion often resulting nearstatic video work identified correlation disparity temporal attention map across different block occurrence temporal inconsistency additionally observed energy contained within temporal attention map directly related magnitude motion amplitude generated video based observation present bytheway trainingfree method improve quality texttovideo generation without introducing additional parameter augmenting memory sampling time specifically bytheway composed two principal component temporal selfguidance improves structural plausibility temporal consistency generated video reducing disparity temporal attention map across various decoder block fourierbased motion enhancement enhances magnitude richness motion amplifying energy map extensive experiment demonstrate bytheway significantly improves quality texttovideo generation negligible additional cost
pyramidal flow matching efficient video generative modeling video generation requires modeling vast spatiotemporal space demand significant computational resource data usage reduce complexity prevailing approach employ cascaded architecture avoid direct training full resolution latent despite reducing computational demand separate optimization substage hinders knowledge sharing sacrifice flexibility work introduces unified pyramidal flow matching algorithm reinterprets original denoising trajectory series pyramid stage final stage operates full resolution thereby enabling efficient video generative modeling sophisticated design flow different pyramid stage interlinked maintain continuity moreover craft autoregressive video generation temporal pyramid compress fullresolution history entire framework optimized endtoend manner single unified diffusion transformer dit extensive experiment demonstrate method support generating highquality video resolution fps within gpu training hour code model opensourced httpspyramidflowgithubio
anchorcrafter animate cyberanchors saling product via humanobject interacting video generation automatic generation anchorstyle product promotion video present promising opportunity online commerce advertising consumer engagement however remains challenging task despite significant advancement poseguided human video generation addressing challenge identify integration humanobject interaction hoi poseguided human video generation core issue end introduce anchorcrafter novel diffusionbased system designed generate video featuring target human customized object achieving high visual fidelity controllable interaction specifically propose two key innovation hoiappearance perception enhances object appearance recognition arbitrary multiview perspective disentangles object human appearance hoimotion injection enables complex humanobject interaction overcoming challenge object trajectory conditioning interocclusion management additionally introduce hoiregion reweighting loss training objective enhances learning object detail extensive experiment demonstrate proposed system outperforms existing method preserving object appearance shape awareness simultaneously maintaining consistency human appearance motion project page httpscangczgithubioanchorcrafter
dispose disentangling pose guidance controllable human image animation controllable human image animation aim generate video reference image using driving video due limited control signal provided sparse guidance eg skeleton pose recent work attempted introduce additional dense condition eg depth map ensure motion alignment however strict dense guidance impairs quality generated video body shape reference character differs significantly driving video paper present dispose mine generalizable effective control signal without additional dense input disentangles sparse skeleton pose human image animation motion field guidance keypoint correspondence specifically generate dense motion field sparse motion field reference image provides regionlevel dense guidance maintaining generalization sparse pose control also extract diffusion feature corresponding pose keypoints reference image point feature transferred target pose provide distinct identity information seamlessly integrate existing model propose plugandplay hybrid controlnet improves quality consistency generated video freezing existing model parameter extensive qualitative quantitative experiment demonstrate superiority dispose compared current method project page hrefhttpsgithubcomlihxxxdisposehttpsgithubcomlihxxxdispose
spatiotemporal style transfer algorithm dynamic visual stimulus generation understanding visual information encoded biological artificial system often requires vision scientist generate appropriate stimulus test specific hypothesis although deep neural network model revolutionized field image generation method image style transfer available method video generation scarce introduce spatiotemporal style transfer stst algorithm dynamic visual stimulus generation framework allows powerful manipulation synthesis video stimulus vision research based twostream deep neural network model factorizes spatial temporal feature generate dynamic visual stimulus whose model layer activation matched input video example show algorithm enables generation model metamers dynamic stimulus whose layer activation within twostream model matched natural video show generated stimulus match lowlevel spatiotemporal feature natural counterpart lack highlevel semantic feature making powerful paradigm study object recognition late layer activation deep vision model exhibited lower similarity natural metameric stimulus compared early layer confirming lack highlevel information generated stimulus finally use generated stimulus probe representational capability predictive coding deep network result showcase potential application algorithm versatile tool dynamic stimulus generation vision science
exploiting style latent flow generalizing deepfake video detection paper present new approach detection fake video based analysis style latent vector abnormal behavior temporal change generated video discovered generated facial video suffer temporal distinctiveness temporal change style latent vector inevitable generation temporally stable video various facial expression geometric transformation framework utilizes stylegru module trained contrastive learning represent dynamic property style latent vector additionally introduce style attention module integrates stylegrugenerated feature contentbased feature enabling detection visual temporal artifact demonstrate approach across various benchmark scenario deepfake detection showing superiority crossdataset crossmanipulation scenario analysis also validate importance using temporal change style latent vector improve generality deepfake video detection
survey long video generation challenge method prospect video generation rapidly advancing research area garnering significant attention due broad range application one critical aspect field generation longduration video present unique challenge opportunity paper present first survey recent advancement long video generation summarises two key paradigm divide conquer temporal autoregressive delve common model employed paradigm including aspect network design conditioning technique furthermore offer comprehensive overview classification datasets evaluation metric crucial advancing long video generation research concluding summary existing study also discus emerging challenge future direction dynamic field hope survey serve essential reference researcher practitioner realm long video generation
animating past reconstruct trilobite via video generation paleontology study past life fundamentally relies fossil reconstruct ancient ecosystem understand evolutionary dynamic trilobite important group extinct marine arthropod offer valuable insight paleozoic environment wellpreserved fossil record reconstructing trilobite behaviour static fossil set new standard dynamic reconstruction scientific research education despite potential current computational method purpose like texttovideo face significant challenge maintaining visual realism consistency hinder application science context overcome obstacle introduce automatic prompt learning method within framework prompt finetuned video generation model generated large language model trained using reward quantify visual realism smoothness generated video finetuning video generation model along reward calculation make use collected dataset eoredlichia intermedia fossil image provides common representative visual detail class trilobite qualitative quantitative experiment show method generate trilobite video significantly higher visual realism compared powerful baseline promising boost scientific understanding public engagement
retta retrievalenhanced testtime adaptation zeroshot video captioning despite significant progress fullysupervised video captioning zeroshot method remain much less explored paper propose novel zeroshot video captioning framework named retrievalenhanced testtime adaptation retta take advantage existing pretrained largescale vision language model directly generate caption testtime adaptation specifically bridge video text using four key model general videotext retrieval model xclip general imagetext matching model clip text alignment model angle text generation model due sourcecode availability main challenge enable text generation model sufficiently aware content given video generate corresponding caption address problem propose using learnable token communication medium among four frozen model xclip clip angle different conventional way train token training data propose learn token soft target inference data several carefully crafted loss function enable token absorb video information catered procedure efficiently done iteration use iteration experiment require ground truth data extensive experimental result three widely used datasets msrvtt msvd vatex show absolute improvement term main metric cider compared several stateoftheart zeroshot video captioning method
llavasurg towards multimodal surgical assistant via structured surgical video learning multimodal large language model llm achieved notable success across various domain research medical field largely focused unimodal image meanwhile current generaldomain multimodal model video still lack capability understand engage conversation surgical video one major contributing factor absence datasets surgical field paper create new dataset surgqa consisting surgical videoinstruction pair largest kind far build dataset propose novel twostage questionanswer generation pipeline llm learn surgical knowledge structured manner publicly available surgical lecture video pipeline break generation process two stage significantly reduce task complexity allowing u use affordable locally deployed opensource llm premium paid llm service also mitigates risk llm hallucination questionanswer generation thereby enhancing overall quality generated data train llavasurg novel visionlanguage conversational assistant capable answering openended question surgical video surgqa dataset conduct comprehensive evaluation zeroshot surgical video questionanswering task show llavasurg significantly outperforms previous generaldomain model demonstrating exceptional multimodal conversational skill answering openended question surgical video release code model instructiontuning dataset
ecisvqg generation entitycentric informationseeking question video previous study question generation video mostly focused generating question common object attribute hence entitycentric work focus generation entitycentric informationseeking question video system could useful videobased learning recommending people also ask question videobased chatbots factchecking work address three key challenge identifying questionworthy information linking entity effectively utilizing multimodal signal best knowledge exist largescale dataset task video question generation datasets tv show movie human activity lack entitycentric informationseeking question hence contribute diverse dataset youtube video videoquestions consisting video manually annotated question propose model architecture combining transformer rich context signal title transcript caption embeddings combination crossentropy contrastive loss function encourage entitycentric question generation best method yield bleu rouge cider meteor score respectively demonstrating practical usability make code dataset publicly available httpsgithubcomthephukanecisvqg
personalvideo high idfidelity video customization without dynamic semantic degradation current texttovideo generation made significant progress synthesizing realistic general video still underexplored identityspecific human video generation customized id image key challenge lie maintaining high id fidelity consistently preserving original motion dynamic semantic following identity injection current video identity customization method mainly rely reconstructing given identity image texttoimage model divergent distribution model process introduces tuninginference gap leading dynamic semantic degradation tackle problem propose novel framework dubbed textbfpersonalvideo applies mixture reward supervision synthesized video instead simple reconstruction objective image specifically first incorporate identity consistency reward effectively inject reference identity without tuninginference gap propose novel semantic consistency reward align semantic distribution generated video original model preserve dynamic semantic following capability identity injection nonreconstructive reward training employ simulated prompt augmentation reduce overfitting supervising generated result semantic scenario gaining good robustness even single reference image extensive experiment demonstrate method superiority delivering high identity faithfulness preserving inherent video generation quality original model outshining prior method
virtual avatar generation model world navigator introduce sabrclimb novel video model simulating human movement rock climbing environment using virtual avatar diffusion transformer predicts sample instead noise diffusion step ingests entire video output complete motion sequence leveraging large proprietary dataset substantial computational resource showcase proof concept system train generalpurpose virtual avatar complex task robotics sport healthcare
chronomagicbench benchmark metamorphic evaluation texttotimelapse video generation propose novel texttovideo generation benchmark chronomagicbench evaluate temporal metamorphic capability model eg sora lumiere timelapse video generation contrast existing benchmark focus visual quality textual relevance generated video chronomagicbench focus model ability generate timelapse video significant metamorphic amplitude temporal coherence benchmark probe model physic biology chemistry capability freeform text query purpose chronomagicbench introduces prompt realworld video reference categorized four major type timelapse video biological humancreated meteorological physical phenomenon divided subcategories categorization comprehensively evaluates model capacity handle diverse complex transformation accurately align human preference benchmark introduce two new automatic metric mtscore chscore evaluate video metamorphic attribute temporal coherence mtscore measure metamorphic amplitude reflecting degree change time chscore assesses temporal coherence ensuring generated video maintain logical progression continuity based chronomagicbench conduct comprehensive manual evaluation ten representative model revealing strength weakness across different category prompt providing thorough evaluation framework address current gap video generation research moreover create largescale chronomagicpro dataset containing highquality pair timelapse video detailed caption ensuring high physical pertinence large metamorphic amplitude homepagehttpspkuyuangroupgithubiochronomagicbench
towards scene graph anticipation spatiotemporal scene graph represent interaction video decomposing scene individual object pairwise temporal relationship longterm anticipation finegrained pairwise relationship object challenging problem end introduce task scene graph anticipation sga adapt stateoftheart scene graph generation method baseline anticipate future pairwise relationship object propose novel approach scenesayer scenesayer leverage objectcentric representation relationship reason observed video frame model evolution relationship object take continuous time perspective model latent dynamic evolution object interaction using concept neuralode neuralsde respectively infer representation future relationship solving ordinary differential equation stochastic differential equation respectively extensive experimentation action genome dataset validates efficacy proposed method
adversarial diffusion compression realworld image superresolution realworld image superresolution realisr aim reconstruct highresolution image lowresolution input degraded complex unknown process many stable diffusion sdbased realisr method achieved remarkable success slow multistep inference hinders practical deployment recent sdbased onestep network like osediff alleviate issue still incur high computational cost due reliance large pretrained sd model paper proposes novel realisr method adcsr distilling onestep diffusion network osediff streamlined diffusiongan model adversarial diffusion compression adc framework meticulously examine module osediff categorizing two type removable vae encoder prompt extractor text encoder etc prunable denoising unet vae decoder since direct removal pruning degrade model generation capability pretrain pruned vae decoder restore ability decode image employ adversarial distillation compensate performance loss adcbased diffusiongan hybrid design effectively reduces complexity inference time computation parameter preserving model generation capability experiment manifest proposed adcsr achieves competitive recovery quality synthetic realworld datasets offering speedup previous onestep diffusionbased method code model available
breaking quality bottleneck video consistency model mixed reward feedback diffusionbased texttovideo model achieved significant success continue hampered slow sampling speed iterative sampling process address challenge consistency model proposed facilitate fast inference albeit cost sample quality work aim break quality bottleneck video consistency model vcm achieve textbfboth fast highquality video generation introduce integrates feedback mixture differentiable reward model consistency distillation cd process pretrained model notably directly optimize reward associated singlestep generation arise naturally computing cd loss effectively bypassing memory constraint imposed backpropagating gradient iterative sampling process remarkably generation achieve highest total score vbench even surpassing pika conduct human evaluation corroborate result validating generation preferred ddim sample teacher model representing tenfold acceleration improving video generation quality
lefusion controllable pathology synthesis via lesionfocused diffusion model patient data realworld clinical practice often suffers data scarcity longtail imbalance leading biased outcome algorithmic unfairness study address challenge generating lesioncontaining imagesegmentation pair lesionfree image previous effort medical imaging synthesis struggled separating lesion information background resulting lowquality background limited control synthetic output inspired diffusionbased image inpainting propose lefusion lesionfocused diffusion model redesigning diffusion learning objective focus lesion area simplify learning process improve control output preserving highfidelity background integrating forwarddiffused background context reverse diffusion process additionally tackle two major challenge lesion texture synthesis multipeak multiclass lesion introduce two effective strategy histogrambased texture control multichannel decomposition enabling controlled generation highquality lesion difficult scenario furthermore incorporate lesion mask diffusion allowing control lesion size location boundary thus increasing lesion diversity validated cardiac lesion mri lung nodule ct datasets lefusiongenerated data significantly improves performance stateoftheart segmentation model including nnunet swinunetr code model available
synchronous synthesis cospeech affective face body expression affordable input present multimodal learningbased method simultaneously synthesize cospeech facial expression upperbody gesture digital character using rgb video data captured using commodity camera approach learns sparse face landmark upperbody joint estimated directly video data generate plausible emotive character motion given speech audio waveform token sequence speaker face landmark motion bodyjoint motion computed video method synthesizes motion sequence speaker face landmark body joint match content affect speech design generator consisting set encoders transform input multimodal embedding space capturing correlation followed pair decoder synthesize desired face pose motion enhance plausibility synthesis use adversarial discriminator learns differentiate face pose motion computed original video synthesized motion based affective expression evaluate approach extend ted gesture dataset include viewnormalized cospeech face landmark addition body gesture demonstrate performance method thorough quantitative qualitative experiment multiple evaluation metric via user study observe method result low reconstruction error produce synthesized sample diverse facial expression body gesture digital character
mmmrs multimodal multigsd multiscene remote sensing dataset benchmark texttoimage generation recently diffusionbased generative paradigm achieved impressive general image generation capability text prompt due accurate distribution modeling stable training process however generating diverse remote sensing r image tremendously different general image term scale perspective remains formidable challenge due lack comprehensive remote sensing image generation dataset various modality ground sample distance gsd scene paper propose multimodal multigsd multiscene remote sensing mmmrs dataset benchmark texttoimage generation diverse remote sensing scenario specifically first collect nine publicly available r datasets conduct standardization sample bridge r image textual semantic information utilize largescale pretrained visionlanguage model automatically output text prompt perform handcrafted rectification resulting informationrich textimage pair including multimodal image particular design method obtain image different gsd various environment eg lowlight foggy single sample extensive manual screening refining annotation ultimately obtain mmmrs dataset comprises approximately million textimage pair extensive experimental result verify proposed mmmrs dataset allows offtheshelf diffusion model generate diverse r image across various modality scene weather condition gsd dataset available
learning temporally consistent video depth video diffusion prior work address challenge streamed video depth estimation expects perframe accuracy importantly crossframe consistency argue sharing contextual information frame clip pivotal fostering temporal consistency thus instead directly developing depth estimator scratch reformulate predictive task conditional generation problem provide contextual information within clip across clip specifically propose consistent contextaware training inference strategy arbitrarily long video provide crossclip context sample independent noise level frame within clip training using sliding window strategy initializing overlapping frame previously predicted frame without adding noise moreover design effective training strategy provide context within clip extensive experimental result validate design choice demonstrate superiority approach dubbed chronodepth project page httpsxdimlabgithubiochronodepth
sora generates video stunning geometrical consistency recently developed sora model exhibited remarkable capability video generation sparking intense discussion regarding ability simulate realworld phenomenon despite growing popularity lack established metric evaluate fidelity realworld physic quantitatively paper introduce new benchmark assesses quality generated video based adherence realworld physic principle employ method transforms generated video model leveraging premise accuracy reconstruction heavily contingent video quality perspective reconstruction use fidelity geometric constraint satisfied constructed model proxy gauge extent generated video conform realworld physic rule project page httpssorageometricalconsistencygithubio
drivescape towards highresolution controllable multiview driving video generation recent advancement generative model provided promising solution synthesizing realistic driving video crucial training autonomous driving perception model however existing approach often struggle multiview video generation due challenge integrating information maintaining spatialtemporal consistency effectively learning unified model propose drivescape endtoend framework multiview conditionguided video generation capable producing x highresolution video unlike method limited due box annotation frame rate drivescape overcomes ability operate sparse condition bidirectional modulated transformer bimot ensures precise alignment structural information maintaining spatialtemporal consistency drivescape excels video generation performance achieving stateoftheart result nuscenes dataset fid score fvd score project homepage
stillmoving customized video generation without customized video data customizing texttoimage model seen tremendous progress recently particularly area personalization stylization conditional generation however expanding progress video generation still infancy primarily due lack customized video data work introduce stillmoving novel generic framework customizing texttovideo model without requiring customized video data framework applies prominent design video model built texttoimage model eg via inflation assume access customized version model trained still image data eg using dreambooth styledrop naively plugging weight customized model model often lead significant artifact insufficient adherence customization data overcome issue train lightweight textitspatial adapter adjust feature produced injected layer importantly adapter trained textitfrozen video ie repeated image constructed image sample generated customized model training facilitated novel textitmotion adapter module allows u train static video preserving motion prior video model test time remove motion adapter module leave trained spatial adapter restores motion prior model adhering spatial prior customized model demonstrate effectiveness approach diverse task including personalized stylized conditional generation evaluated scenario method seamlessly integrates spatial prior customized model motion prior supplied model
explainable deepfake video detection using convolutional neural network capsulenet deepfake technology derived deep learning seamlessly insert individual digital medium irrespective actual participation foundation lie machine learning artificial intelligence ai initially deepfakes served research industry entertainment concept existed decade recent advancement render deepfakes nearly indistinguishable reality accessibility soared empowering even novice create convincing deepfakes however accessibility raise security concernsthe primary deepfake creation algorithm gan generative adversarial network employ machine learning craft realistic image video objective utilize cnn convolutional neural network capsulenet lstm differentiate deepfakegenerated frame original furthermore aim elucidate model decisionmaking process explainable ai fostering transparent humanai relationship offering practical example reallife scenario
unsupervised sign language translation generation motivated success unsupervised neural machine translation unmt introduce unsupervised sign language translation generation network uslnet learns abundant singlemodality text video data without parallel sign language data uslnet comprises two main component singlemodality reconstruction module text video rebuild input noisy version modality crossmodality backtranslation module textvideotext videotextvideo reconstruct input noisy version different modality using backtranslation procedureunlike singlemodality backtranslation procedure textbased unmt uslnet face crossmodality discrepancy feature representation length feature dimension mismatch text video sequence propose sliding window method address issue aligning variablelength text video sequence knowledge uslnet first unsupervised sign language translation generation model capable generating natural language text sign language video unified manner experimental result bbcoxford sign language dataset bobsl opendomain american sign language dataset openasl reveal uslnet achieves competitive result compared supervised baseline model indicating effectiveness sign language translation generation
vurf generalpurpose reasoning selfrefinement framework video understanding recent study demonstrated effectiveness large language model llm reasoning module deconstruct complex task manageable subtasks particularly applied visual reasoning task image contrast paper introduces video understanding reasoning framework vurf based reasoning power llm novel approach extend utility llm context video task leveraging capacity generalize minimal input output demonstration within contextual framework harness contextual learning capability presenting llm pair instruction corresponding highlevel program generate executable visual program video understanding enhance program accuracy robustness implement two important strategy emphfirstly employ feedbackgeneration approach powered rectify error program utilizing unsupported function emphsecondly taking motivation recent work selfrefinement llm output introduce iterative procedure improving quality incontext example aligning initial output output would generated llm bound structure incontext example result several videospecific task including visual qa video anticipation pose estimation multivideo qa illustrate enhancement efficacy improving performance visual programming approach video task
configurable embodied data generation classagnostic rgbd video segmentation paper present method generating largescale datasets improve classagnostic video segmentation across robot different form factor specifically consider question whether video segmentation model trained generic segmentation data could effective particular robot platform robot embodiment factored data generation process answer question pipeline formulated using reconstruction eg generate segmented video configurable based robot embodiment eg sensor type sensor placement illumination source resulting massive rgbd video panoptic segmentation dataset mvpd introduced extensive benchmarking foundation video segmentation model well support embodimentfocused research video segmentation experimental finding demonstrate using mvpd finetuning lead performance improvement transferring foundation model certain robot embodiment specific camera placement experiment also show using modality depth image camera pose lead improvement video segmentation accuracy consistency project webpage available httpstopiparicomprojectsmvpd
videoguided foley sound generation multimodal control generating sound effect video often requires creating artistic sound effect diverge significantly reallife source flexible control sound design address problem introduce multifoley model designed videoguided sound generation support multimodal conditioning text audio video given silent video text prompt multifoley allows user create clean sound eg skateboard wheel spinning without wind noise whimsical sound eg making lion roar sound like cat meow multifoley also allows user choose reference audio sound effect sfx library partial video conditioning key novelty model lie joint training internet video datasets lowquality audio professional sfx recording enabling highquality fullbandwidth audio generation automated evaluation human study demonstrate multifoley successfully generates synchronized highquality sound across varied conditional input outperforms existing method please see project page video result httpsificlgithubiomultifoley
eva zeroshot accurate attribute multiobject video editing current diffusionbased video editing primarily focus local editing textiteg objectbackground editing global style editing utilizing various dense correspondence however method often fail accurately edit foreground background simultaneously preserving original layout find crux issue stem imprecise distribution attention weight across designated region including inaccurate texttoattribute control attention leakage tackle issue introduce eva textbfzeroshot textbfmultiattribute video editing framework tailored humancentric video complex motion incorporate spatialtemporal layoutguided attention mechanism leverage intrinsic positive negative correspondence crossframe diffusion feature avoid attention leakage utilize correspondence boost attention score token within attribute across video frame limiting interaction token different attribute selfattention layer precise texttoattribute manipulation use discrete text embeddings focused specific layout area within crossattention layer benefiting precise attention weight distribution eva easily generalized multiobject editing scenario achieves accurate identity mapping extensive experiment demonstrate eva achieves stateoftheart result realworld scenario full result provided httpsknightyxpgithubioeva
hiding face plain sight defending deepfakes disrupting face detection paper investigates feasibility proactive deepfake defense framework em faceposion prevent individual becoming victim deepfake video sabotaging face detection motivation stem reliance deepfake method face detector automatically extract victim face video training synthesis testing face detector malfunction extracted face distorted incorrect subsequently disrupting training synthesis deepfake model achieve adapt various adversarial attack dedicated design purpose thoroughly analyze feasibility based facepoison introduce em videofacepoison strategy propagates facepoison across video frame rather applying individually frame strategy largely reduce computational overhead retaining favorable attack performance method validated five face detector extensive experiment eleven different deepfake model demonstrate effectiveness disrupting face detector hinder deepfake generation
ildiff generate transparent animated sticker implicit layout distillation highquality animated sticker usually contain transparent channel often ignored current video generation model generate finegrained animated transparency channel existing method roughly divided video matting algorithm diffusionbased algorithm method based video matting poor performance dealing semiopen area sticker diffusionbased method often used model single image lead local flicker modeling animated sticker paper firstly propose ildiff method generate animated transparent channel implicit layout distillation solves problem semiopen area collapse consideration temporal information existing method secondly create transparent animated sticker dataset tasd contains highquality sample transparent channel provide data support related field extensive experiment demonstrate ildiff produce finer smoother transparent channel compared method matting anything layer diffusion code dataset released link
motionaware latent diffusion model video frame interpolation advancement aigc video frame interpolation vfi become crucial component existing video generation framework attracting widespread research interest vfi task motion estimation neighboring frame play crucial role avoiding motion ambiguity however existing vfi method always struggle accurately predict motion information consecutive frame imprecise estimation lead blurred visually incoherent interpolated frame paper propose novel diffusion framework motionaware latent diffusion model madiff specifically designed vfi task incorporating motion prior conditional neighboring frame target interpolated frame predicted throughout diffusion sampling procedure madiff progressively refines intermediate outcome culminating generating visually smooth realistic result extensive experiment conducted benchmark datasets demonstrate method achieves stateoftheart performance significantly outperforming existing approach especially challenging scenario involving dynamic texture complex motion
style transfer stylealigned multiview image propose simple yet effective pipeline stylizing scene harnessing power image diffusion model given nerf model reconstructed set multiview image perform style transfer refining source nerf model using stylized image generated stylealigned imagetoimage diffusion model given target style prompt first generate perceptually similar multiview image leveraging depthconditioned diffusion model attentionsharing mechanism next based stylized multiview image propose guide style transfer process sliced wasserstein loss based feature map extracted pretrained cnn model pipeline consists decoupled step allowing user test various prompt idea preview stylized result proceeding nerf finetuning stage demonstrate method transfer diverse artistic style realworld scene competitive quality result video also available project page
music consistency model consistency model exhibited remarkable capability facilitating efficient imagevideo generation enabling synthesis minimal sampling step proven advantageous mitigating computational burden associated diffusion model nevertheless application consistency model music generation remains largely unexplored address gap present music consistency model textttmusiccm leverage concept consistency model efficiently synthesize melspectrogram music clip maintaining high quality minimizing number sampling step building upon existing texttomusic diffusion model textttmusiccm model incorporates consistency distillation adversarial discriminator training moreover find beneficial generate extended coherent music incorporating multiple diffusion process shared constraint experimental result reveal effectiveness model term computational efficiency fidelity naturalness notable textttmusiccm achieves seamless music synthesis mere four sampling step eg one second per minute music clip showcasing potential realtime application
comprehensive taxonomy analysis talking head synthesis technique portrait generation driving mechanism editing talking head synthesis advanced method generating portrait video still image driven specific content garnered widespread attention virtual reality augmented reality game production recently significant breakthrough made introduction novel model transformer diffusion model current method generate new content also edit generated material survey systematically review technology categorizing three pivotal domain portrait generation driven mechanism editing technique summarize milestone study critically analyze innovation shortcoming within domain additionally organize extensive collection datasets provide thorough performance analysis current methodology based various evaluation metric aiming furnish clear framework robust data support future research finally explore application scenario talking head synthesis illustrate specific case examine potential future direction
diffusion implicit policy unpaired sceneaware motion synthesis human motion generation longstanding problem sceneaware motion synthesis widely researched recently due numerous application prevailing method rely heavily paired motionscene data whose quantity limited meanwhile difficult generalize diverse scene trained specific one thus propose unified framework termed diffusion implicit policy dip sceneaware motion synthesis paired motionscene data longer necessary framework disentangle humanscene interaction motion synthesis training introduce interactionbased implicit policy motion diffusion inference synthesized motion derived iterative diffusion denoising implicit policy optimization thus motion naturalness interaction plausibility maintained simultaneously proposed implicit policy optimizes intermediate noised motion gan inversion manner maintain motion continuity control keyframe pose though controlnet branch motion inpainting longterm motion synthesis introduce motion blending stable transition multiple subtasks motion fused rotation power space translation linear space proposed method evaluated synthesized scene shapenet furniture real scene prox replica result show framework present better motion naturalness interaction plausibility cuttingedge method also indicates feasibility utilizing dip motion synthesis general task versatile scene httpsjingyugonggithubiodiffusionimplicitpolicy
animatezoo zeroshot video generation crossspecies animation via subject alignment recent video editing advancement rely accurate pose sequence animate subject however effort suitable crossspecies animation due pose misalignment specie example pose cat differs greatly pig due difference body structure paper present animatezoo zeroshot diffusionbased video generator address challenging crossspecies animation issue aiming accurately produce animal animation preserving background key technique used animatezoo subject alignment includes two step first improve appearance feature extraction integrating laplacian detail booster prompttuning identity extractor component specifically designed capture essential appearance information including identity fine detail second align shape feature address conflict differing subject introducing scaleinformation remover ensures accurate crossspecies animation moreover introduce two highquality animal video datasets featuring wide variety specie trained extensive datasets model capable generating video characterized accurate movement consistent appearance highfidelity frame without need preinference finetuning prior art required extensive experiment showcase outstanding performance method crossspecies action following task demonstrating exceptional shape adaptation capability project page available
baboonland dataset tracking primate wild automating behaviour recognition drone video using drone track multiple individual simultaneously natural environment powerful approach better understanding group primate behavior previous study demonstrated possible automate classification primate behavior video data study carried captivity groundbased camera understand group behavior selforganization collective whole troop need seen scale behavior seen relation natural environment ecological decision made study present novel dataset drone video baboon detection tracking behavior recognition baboon detection dataset created manually annotating baboon drone video bounding box tiling method subsequently applied create pyramid image various scale original resolution image resulting approximately image used baboon detection tracking dataset derived detection dataset bounding box assigned id throughout video process resulted half hour dense tracking data behavior recognition dataset generated converting track miniscenes video subregion centered animal miniscene manually annotated distinct behavior type resulting hour data benchmark result show mean average precision map detection model multiple object tracking precision mota botsort tracking algorithm micro accuracy behavior recognition model using deep learning classify wildlife behavior drone footage facilitates noninvasive insight collective behavior entire group
histodiffusion diffusion superresolution method digital pathology comprehensive quality assessment digital pathology advanced significantly last decade whole slide image wsis encompassing vast amount data essential accurate disease diagnosis highresolution wsis essential precise diagnosis technical limitation scanning equipment variablity slide preparation hinder obtaining image superresolution technique enhance lowresolution image generative adversarial network gans effective natural image superresolution task often struggle histopathology due overfitting mode collapse traditional evaluation metric fall short assessing complex characteristic histopathology image necessitating robust histologyspecific evaluation method introduce histodiffusion novel diffusionbased method specially designed generating evaluating superresolution image digital pathology includes restoration module histopathology prior controllable diffusion module generating highquality image curated two histopathology datasets proposed comprehensive evaluation strategy incorporates fullreference noreference metric thoroughly assess quality digital pathology image comparative analysis multiple datasets stateoftheart method reveal histodiffusion outperforms gans method offer versatile solution histopathology image superresolution capable handling multiresolution generation varied input size providing valuable support diagnostic process
gettok genaienriched multimodal tiktok dataset documenting attempted coup peru tiktok one largest fastestgrowing social medium site world tiktok feature however voice transcript often missing important feature ocr video description exist introduce generative ai enriched tiktok gettok data pipeline collecting tiktok video enriched data augmenting tiktok research api generative ai model case study collect video attempted coup peru initiated former president pedro castillo accompanying protest data includes information video published november march day generative ai augments collected data via transcript tiktok video text description shown video text displayed within video stance expressed video overall pipeline contribute better understanding online discussion multimodal setting application generative ai especially outlining utility pipeline nonenglishlanguage social medium code used produce pipeline public github repository httpsgithubcomgabbypintogettokperu
empirical study general video transformer adaptation remote physiological measurement remote physiological measurement rpm essential tool healthcare monitoring enables measurement physiological sign eg heart rate remote setting via physical wearable recently facial video seen rapid advancement videobased rpm however adopting facial video rpm clinical setting largely depends accuracy robustness work across patient population fortunately capability stateoftheart transformer architecture general natural video understanding resulted marked improvement translated facial understanding including rpm however existing rpm method usually need rpmspecific module eg temporal difference convolution handcrafted feature map although customized module increase accuracy demonstrated robustness across datasets due customization transformer architecture use advancement made general video transformer gvt study interrogate gvt architecture empirically analyze training design ie data preprocessing network configuration affect model performance applied rpm based structure video transformer propose configure spatiotemporal hierarchy align dense temporal information needed rpm signal feature extraction define several practical guideline gradually adapt gvts rpm without introducing rpmspecific module experiment demonstrate favorable result existing rpmspecific module counterpart conducted extensive experiment five datasets using intradataset crossdataset setting highlight proposed guideline generalized video transformer robust various datasets
vcome verbal video composition multimodal editing effect verbal video featuring voiceovers text overlay provide valuable content present significant challenge composition especially incorporating editing effect enhance clarity visual appeal paper introduce novel task verbal video composition editing effect task aim generate coherent visually appealing verbal video integrating multimodal editing effect across textual visual audio category achieve curate largescale dataset video effect composition publicly available source formulate task generative problem involving identification appropriate position verbal content recommendation editing effect position address task propose vcome general framework employ large multimodal model generate editing effect video composition specifically vcome take multimodal video context autoregressively output apply effect within verbal content effect appropriate position vcome also support promptbased control composition density style providing substantial flexibility diverse application extensive quantitative qualitative evaluation clearly demonstrate effectiveness vcome comprehensive user study show method produce video professional quality efficient professional editor
percept chat adapt multimodal knowledge transfer foundation model openworld video recognition openworld video recognition challenging since traditional network generalized well complex environment variation alternatively foundation model rich knowledge recently shown generalization power however apply knowledge fully explored openworld video recognition end propose generic knowledge transfer pipeline progressively exploit integrates external multimodal knowledge foundation model boost openworld video recognition name pca based three stage percept chat adapt first perform percept process reduce video domain gap obtain external visual knowledge second generate rich linguistic semantics external textual knowledge chat stage finally blend external multimodal knowledge adapt stage inserting multimodal knowledge adaptation module network conduct extensive experiment three challenging openworld video benchmark ie tinyvirat arid qvpipe approach achieves stateoftheart performance three datasets
general surgery vision transformer video pretrained foundation model general surgery absence openly accessible data specialized foundation model major barrier computational research surgery toward opensource largest dataset general surgery video todate consisting hour surgical video including data robotic laparoscopic technique across procedure ii propose technique video pretraining general surgery vision transformer gsvit surgical video based forward video prediction run realtime surgical application toward opensource code weight gsvit iii also release code weight procedurespecific finetuned version gsvit across procedure iv demonstrate performance gsvit phase annotation task displaying improved performance stateoftheart single frame predictor
aigvassessor benchmarking evaluating perceptual quality texttovideo generation lmm rapid advancement large multimodal model lmms led rapid expansion artificial intelligence generated video aigvs highlight pressing need effective video quality assessment vqa model designed specifically aigvs current vqa model generally fall short accurately assessing perceptual quality aigvs due presence unique distortion unrealistic object unnatural movement inconsistent visual element address challenge first present aigvqadb largescale dataset comprising aigvs generated advanced texttovideo model using diverse prompt aigvs systematic annotation pipeline including scoring ranking process devised collect expert rating date based aigvqadb introduce aigvassessor novel vqa model leverage spatiotemporal feature lmm framework capture intricate quality attribute aigvs thereby accurately predicting precise video quality score video pair preference comprehensive experiment aigvqadb existing aigv database aigvassessor demonstrates stateoftheart performance significantly surpassing existing scoring evaluation method term multiple perceptual quality dimension
unveiling deep shadow survey benchmark image video shadow detection removal generation deep learning era shadow created light encounter obstacle resulting region reduced illumination computer vision detecting removing generating shadow critical task improving scene understanding enhancing image quality ensuring visual consistency video editing optimizing virtual environment paper offer comprehensive survey evaluation benchmark shadow detection removal generation image video focusing deep learning approach past decade cover key aspect task deep model datasets evaluation metric comparative result consistent experimental setting main contribution include thorough survey shadow analysis standardization experimental comparison exploration relationship model size speed performance crossdataset generalization study identification open challenge future research direction provision publicly available resource support research field
generalized deepfake attribution landscape fake medium creation changed introduction generative adversarial network gan fake medium creation rise rapid advance generation technology leading new challenge detecting fake medium fundamental characteristic gan sensitivity parameter initialization known seed distinct seed utilized training lead creation unique model instance resulting divergent image output despite employing architecture mean even one gan architecture produce countless variation gan model depending seed used existing method attributing deepfakes work well seen specific gan model training gan architecture retrained different seed method struggle attribute fake seed dependency issue made difficult attribute deepfakes existing method proposed generalized deepfake attribution network gdan et attribute fake image respective gan architecture even generated retrained version gan architecture different seed crossseed finetuned version existing gan model extensive experiment crossseed finetuned data gan model show method highly effective compared existing method provided source code validate result
kandinsky texttoimage synthesis multifunctional generative framework texttoimage diffusion model popular introducing image manipulation method editing image fusion inpainting etc time imagetovideo texttovideo model also built top model present kandinsky novel model based latent diffusion achieving high level quality photorealism key feature new architecture simplicity efficiency adaptation many type generation task extend base model various application create multifunctional generation system includes textguided inpaintingoutpainting image fusion textimage fusion image variation generation generation also present distilled version model evaluating inference step reverse process without reducing image quality time faster base model deployed userfriendly demo system feature tested public domain additionally released source code checkpoint kandinsky extended model human evaluation show kandinsky demonstrates one highest quality score among open source generation system
mitigating analytical variability fmri result style transfer propose novel approach improve reproducibility neuroimaging result converting statistic map across different functional mri pipeline make assumption pipeline used compute fmri statistic map considered style component propose use different generative model among generative adversarial network gan diffusion model dm convert statistic map across different pipeline explore performance multiple gan framework design new dm framework unsupervised multidomain styletransfer constrain generation fmri statistic map using latent space auxiliary classifier distinguishes statistic map different pipeline extend traditional sampling technique used dm improve transition performance experiment demonstrate proposed method aresuccessful pipeline indeed transferred style component providing animportant source data augmentation future medical study
emag egomotion aware generalizable hand forecasting egocentric video predicting future human behavior egocentric video challenging critical task human intention understanding existing method forecasting hand position rely visual representation mainly focus handobject interaction paper investigate hand forecasting task tackle two significant issue persist existing method hand position future frame severely affected egomotions egocentric video prediction based visual information tends overfit background scene texture posing challenge generalization novel scene human behavior solve aforementioned problem propose emag egomotionaware generalizable hand forecasting method response first problem propose method considers egomotion represented sequence homography matrix two consecutive frame leverage modality optical flow trajectory hand interacting object egomotions thereby alleviating second issue extensive experiment two largescale egocentric video datasets epickitchens verify effectiveness proposed method particular model outperforms prior method intra crossdataset evaluation respectively project page httpsmasashihatanogithubioemag
gui action narrator action take place advent multimodal llm significantly enhanced image ocr recognition capability making gui automation viable reality increasing efficiency digital task one fundamental aspect developing gui automation system understanding primitive gui action comprehension crucial enables agent learn user demonstration essential element automation rigorously evaluate capability developed video captioning benchmark gui action comprising diverse video captioning sample task present unique challenge compared natural scene video captioning gui screenshots typically contain denser information natural scene event within gui subtler occur rapidly requiring precise attention appropriate time span spatial region accurate understanding address challenge introduce gui action dataset well simple yet effective framework textbfgui narrator gui video captioning utilizes cursor visual prompt enhance interpretation highresolution screenshots specifically cursor detector trained dataset multimodal llm model mechanism selecting keyframes key region generates caption experimental result indicate even today advanced multimodal model task remains highly challenging additionally evaluation show strategy effectively enhances model performance whether integrated finetuning opensource model employed prompting strategy closedsource model
dyblurf dynamic neural radiance field blurry monocular video recent advancement dynamic neural radiance field method yielded remarkable outcome however approach rely assumption sharp input image faced motion blur existing dynamic nerf method often struggle generate highquality novel view paper propose dyblurf dynamic radiance field approach synthesizes sharp novel view monocular video affected motion blur account motion blur input image simultaneously capture camera trajectory object discrete cosine transform dct trajectory within scene additionally employ global crosstime rendering approach ensure consistent temporal coherence across entire scene curate dataset comprising diverse dynamic scene specifically tailored task experimental result dataset demonstrate method outperforms existing approach generating sharp novel view motionblurred input maintaining spatialtemporal consistency scene
gaspct gaussian splatting novel ct projection view synthesis present gaspct novel view synthesis scene representation method used generate novel projection view computer tomography ct scan adapt gaussian splatting framework enable novel view synthesis ct based limited set image projection without need structure motion sfm methodology therefore reduce total scanning duration amount radiation dose patient receives scan adapted loss function usecase encouraging stronger background foreground distinction using two sparsity promoting regularizers beta loss total variation tv loss finally initialize gaussian location across space using uniform prior distribution brain positioning would expected within field view evaluate performance model using brain ct scan parkinson progression marker initiative ppmi dataset demonstrate rendered novel view closely match original projection view simulated scan better performance implicit scene representation methodology furthermore empirically observe reduced training time compared neural network based image synthesis sparseview ct image reconstruction finally memory requirement gaussian splatting representation reduced compared equivalent voxel grid image representation
tfoley controllable waveformdomain diffusion model temporaleventguided foley sound synthesis foley sound audio content inserted synchronously video play critical role user experience multimedia content recently active research foley sound synthesis leveraging advancement deep generative model however work mainly focus replicating single sound class textual sound description neglecting temporal information crucial practical application foley sound present tfoley temporaleventguided waveform generation model foley sound synthesis tfoley generates highquality audio using two condition sound class temporal event feature temporal conditioning devise temporal event feature novel conditioning technique named blockfilm tfoley achieves superior performance objective subjective evaluation metric generates foley sound wellsynchronized temporal event additionally showcase tfoleys practical application particularly scenario involving vocal mimicry temporal event control show demo companion website
motion dreamer boundary conditional motion reasoning physically coherent video generation recent advance video generation shown promise generating future scenario critical planning control autonomous driving embodied intelligence however realworld application demand visually plausible prediction require reasoning object motion based explicitly defined boundary condition initial scene image partial object motion term capability boundary conditional motion reasoning current approach either neglect explicit userdefined motion constraint producing physically inconsistent motion conversely demand complete motion input rarely available practice introduce motion dreamer twostage framework explicitly separate motion reasoning visual synthesis addressing limitation approach introduces instance flow sparsetodense motion representation enabling effective integration partial userdefined motion motion inpainting strategy robustly enable reasoning motion object extensive experiment demonstrate motion dreamer significantly outperforms existing method achieving superior motion plausibility visual realism thus bridging gap towards practical boundary conditional motion reasoning webpage available httpsenvisionresearchgithubiomotiondreamer
lifelong learning using dynamically growing tree subnetworks domain generalization video object segmentation current stateoftheart video object segmentation model achieved great success using supervised learning massive labeled training datasets however model trained using single source domain evaluated using video sampled source domain model evaluated using video sampled different target domain performance degrades significantly due poor domain generalization ie inability learn multidomain source simultaneously using traditional supervised learning paper propose dynamically growing tree subnetworks dgt learn effectively multidomain source dgt us novel lifelong learning technique allows model continuously effectively learn new domain without forgetting previously learned domain hence model generalize outofdomain video proposed work evaluated using singlesource indomain traditional video object segmentation multisource indomain multisource outofdomain video object segmentation result dgt show single source indomain performance gain datasets respectively however dgt evaluated using indomain multisources result show superior performance compared stateoftheart video object segmentation lifelong learning technique average performance increase fscore minimal catastrophic forgetting finally outofdomain experiment performance dgt better stateoftheart respectively
evcmf endtoend video captioning network multiscale feature conventional approach video captioning leverage variety offlineextracted feature generate caption despite availability various offlinefeatureextractors offer diverse information different perspective several limitation due fixed parameter concretely extractor solely pretrained imagevideo comprehension task making less adaptable video caption datasets additionally extractor capture feature prior classifier pretraining task ignoring significant amount valuable shallow information furthermore employing multiple offlinefeatures may introduce redundant information address issue propose endtoend encoderdecoderbased network evcmf video captioning efficiently utilizes multiscale visual textual feature generate video description specifically evcmf consists three module firstly instead relying multiple feature extractor directly feed video frame transformerbased network obtain multiscale visual feature update feature extractor parameter secondly fuse multiscale feature input masked encoder reduce redundancy encourage learning useful feature finally utilize enhanced transformerbased decoder efficiently leverage shallow textual information generate video description evaluate proposed model conduct extensive experiment benchmark datasets result demonstrate evcmf yield competitive performance compared stateoftheart method
vibe texttovideo benchmark evaluating hallucination large multimodal model recent advance large multimodal model lmms expanded capability video understanding texttovideo model excelling generating video textual prompt however still frequently produce hallucinated content revealing aigenerated inconsistency introduce vibe largescale dataset hallucinated video opensource model identify five major hallucination type vanishing subject omission error numeric variability subject dysmorphia visual incongruity using ten model generated manually annotated video diverse m coco caption proposed benchmark includes dataset hallucinated video classification framework using video embeddings vibe serf critical resource evaluating reliability advancing hallucination detection establish classification baseline timesformer cnn ensemble achieving best performance accuracy score initial baseline proposed achieve modest accuracy highlight difficulty automated hallucination detection need improved method research aim drive development robust model evaluate output based user preference
vidmuse simple videotomusic generation framework longshortterm modeling work systematically study music generation conditioned solely video first present largescale dataset comprising videomusic pair including various genre movie trailer advertisement documentary furthermore propose vidmuse simple framework generating music aligned video input vidmuse stand producing highfidelity music acoustically semantically aligned video incorporating local global visual cue vidmuse enables creation musically coherent audio track consistently match video content longshortterm modeling extensive experiment vidmuse outperforms existing model term audio quality diversity audiovisual alignment code datasets available httpsgithubcomzeyuetvidmuse
synthetic human memory aiedited image video implant false memory distort recollection ai increasingly used enhance image video intentionally unintentionally ai editing tool become integrated smartphones user modify animate photo realistic video study examines impact aialtered visuals false memoriesrecollections event didnt occur deviate reality preregistered study participant divided four condition participant viewed original image completed filler task saw stimulus corresponding assigned condition unedited image aiedited image aigenerated video aigenerated video aiedited image aiedited visuals significantly increased false recollection aigenerated video aiedited image strongest effect compared control confidence false memory also highest condition compared control discus potential application hci therapeutic memory reframing challenge ethical legal political societal domain
unified framework temporal video understanding task development video understanding proliferation task cliplevel temporal video analysis including temporal action detection tad temporal action segmentation ta generic event boundary detection gebd taskspecific video understanding model exhibited outstanding performance task remains dearth unified framework capable simultaneously addressing multiple task promising direction next generation ai end paper propose single unified framework coined formulate output temporal video understanding task sequence discrete token unified token representation train generalist model within single architecture different video understanding task absence multitask learning mtl benchmark compile comprehensive cotraining dataset borrowing datasets tad ta gebd task evaluate generalist model corresponding test set three task demonstrating produce reasonable result various task achieve advantage compared singletask training framework also investigate generalization performance generalist model new datasets different task yield superior performance specific model
animateanything consistent controllable animation video generation present unified controllable video generation approach animateanything facilitates precise consistent video manipulation across various condition including camera trajectory text prompt user motion annotation specifically carefully design multiscale control feature fusion network construct common motion representation different condition explicitly convert control information framebyframe optical flow incorporate optical flow motion prior guide final video generation addition reduce flickering issue caused largescale motion propose frequencybased stabilization module enhance temporal coherence ensuring video frequency domain consistency experiment demonstrate method outperforms stateoftheart approach detail video please refer webpage
posttraining quantization efficient text conditional audio diffusion model denoising diffusion model emerged stateoftheart generative task across image audio video domain producing highquality diverse contextually relevant data however broader adoption limited high computational cost large memory footprint posttraining quantization ptq offer promising approach mitigate challenge reducing model complexity lowbandwidth parameter yet direct application ptq diffusion model degrade synthesis quality due accumulated quantization noise across multiple denoising step particularly conditional task like texttoaudio synthesis work introduces novel framework quantizing audio diffusion modelsadms key contribution include coveragedriven prompt augmentation method activationaware calibration set generation algorithm textconditional adms technique ensure comprehensive coverage audio aspect modality preserving synthesis fidelity validate approach tango makeanaudio audioldm model textconditional audio generation extensive experiment demonstrate capability reduce model size achieving synthesis quality metric comparable fullprecision increase fd score show specific layer backbone network quantized weight activation without significant quality loss work pave way efficient deployment adms resourceconstrained environment
controllable talking face generation implicit facial keypoints editing audiodriven talking face generation garnered significant interest within domain digital human research existing method encumbered intricate model architecture intricately dependent complicating process reediting image video input work present controltalk talking face generation method control face expression deformation based driven audio construct head pose facial expression including lip motion single image sequential video input unified manner utilizing pretrained video synthesis renderer proposing lightweight adaptation controltalk achieves precise naturalistic lip synchronization enabling quantitative control mouth opening shape experiment show method superior stateoftheart performance widely used benchmark including hdtf mead parameterized adaptation demonstrates remarkable generalization capability effectively handling expression deformation across sameid crossid scenario extending utility outofdomain portrait regardless language code available httpsgithubcomneteasemediacontroltalk
stylepreserving lip sync via audioaware style reference audiodriven lip sync recently drawn significant attention due widespread application multimedia domain individual exhibit distinct lip shape speaking utterance attributed unique speaking style individual posing notable challenge audiodriven lip sync earlier method task often bypassed modeling personalized speaking style resulting suboptimal lip sync conforming general style recent lip sync technique attempt guide lip sync arbitrary audio aggregating information style reference video yet preserve speaking style well due inaccuracy style aggregation work proposes innovative audioaware style reference scheme effectively leverage relationship input audio reference audio style reference video address stylepreserving audiodriven lip sync specifically first develop advanced transformerbased model adept predicting lip motion corresponding input audio augmented style information aggregated crossattention layer style reference video afterwards better render lip motion realistic talking face video devise conditional latent diffusion model integrating lip motion modulated convolutional layer fusing reference facial image via spatial crossattention layer extensive experiment validate efficacy proposed approach achieving precise lip sync preserving speaking style generating highfidelity realistic talking face video
directavideo customized video generation userdirected camera movement object motion recent texttovideo diffusion model achieved impressive progress practice user often desire ability control object motion camera movement independently customized video creation however current method lack focus separately controlling object motion camera movement decoupled manner limit controllability flexibility texttovideo model paper introduce directavideo system allows user independently specify motion multiple object well camera pan zoom movement directing video propose simple yet effective strategy decoupled control object motion camera movement object motion controlled spatial crossattention modulation using model inherent prior requiring additional optimization camera movement introduce new temporal crossattention layer interpret quantitative camera movement parameter employ augmentationbased approach train layer selfsupervised manner smallscale dataset eliminating need explicit motion annotation component operate independently allowing individual combined control generalize opendomain scenario extensive experiment demonstrate superiority effectiveness method project page code available httpsdirectavideogithubio
zeroshot subjectdriven video customization precise motion control recent advance customized video generation enabled user create video tailored specific subject motion trajectory however existing method often require complicated testtime finetuning struggle balancing subject learning motion control limiting realworld application paper present zeroshot video customization framework capable generating video specific subject motion trajectory guided single image bounding box sequence respectively without need testtime finetuning specifically introduce reference attention leverage model inherent capability subject learning devise maskguided motion module achieve precise motion control fully utilizing robust motion signal box mask derived bounding box two component achieve intended function empirically observe motion control tends dominate subject learning address propose two key design masked reference attention integrates blended latent mask modeling scheme reference attention enhance subject representation desired position reweighted diffusion loss differentiates contribution region inside outside bounding box ensure balance subject motion control extensive experimental result newly curated dataset demonstrate outperforms stateoftheart method subject customization motion control dataset code model made publicly available
mri synthesis slicebased latent diffusion model improving tumor segmentation task datascarce regime despite increasing use deep learning medical image segmentation limited availability annotated training data remains major challenge due timeconsuming data acquisition privacy regulation context segmentation task providing medical image corresponding target mask essential however conventional data augmentation approach mainly focus image synthesis study propose novel slicebased latent diffusion architecture designed address complexity volumetric data generation slicebyslice fashion approach extends joint distribution modeling medical image associated mask allowing simultaneous generation datascarce regime approach mitigates computational complexity memory expensiveness typically associated diffusion model furthermore architecture conditioned tumor characteristic including size shape relative position thereby providing diverse range tumor variation experiment segmentation task using confirm effectiveness synthesized volume mask data augmentation
diffsr learning radar reflectivity synthesis via diffusion model satellite observation weather radar data synthesis fill data area ground observation missing existing method often employ reconstructionbased approach mse loss reconstruct radar data satellite observation however method lead oversmoothing hinders generation highfrequency detail highvalue observation area associated convective weather address issue propose twostage diffusionbased method called diffsr first pretrain reconstruction model globalscale data obtain radar estimation synthesize radar reflectivity combining radar estimation result satellite data condition diffusion model extensive experiment show method achieves stateoftheart sota result demonstrating ability generate highfrequency detail highvalue area
multitask sar image processing via ganbased unsupervised manipulation generative adversarial network gans shown tremendous potential synthesizing large number realistic sar image learning pattern data distribution gans achieve image editing introducing latent code demonstrating significant promise sar image processing compared traditional sar image processing method editing based gan latent space control entirely unsupervised allowing image processing conducted without labeled data additionally information extracted data interpretable paper proposes novel sar image processing framework called ganbased unsupervised editing gue aiming address following two issue disentangling semantic direction gan latent space finding meaningful direction establishing comprehensive sar image processing framework achieving multiple image processing function implementation gue decompose entangled semantic direction gan latent space training carefully designed network moreover accomplish multiple sar image processing task including despeckling localization auxiliary identification rotation editing single training process without form supervision extensive experiment validate effectiveness proposed method
gtautoact automatic datasets generation framework based game engine redevelopment action recognition current datasets action recognition task face limitation stemming traditional collection generation method including constrained range action class absence multiviewpoint recording limited diversity poor video quality laborintensive manually collection address challenge introduce gtautoact innovative dataset generation framework leveraging game engine technology facilitate advancement action recognition gtautoact excels automatically creating largescale wellannotated datasets extensive action class superior video quality framework distinctive contribution encompass innovatively transforms readily available coordinatebased human motion rotationorientated representation enhanced suitability multiple viewpoint employ dynamic segmentation interpolation rotation sequence create smooth realistic animation action offer extensively customizable animation scene implement autonomous video capture processing pipeline featuring randomly navigating camera autotrimming labeling functionality experimental result underscore framework robustness highlight potential significantly improve action recognition model training
tcpdm temporally consistent patch diffusion model infraredtovisible video translation infrared imaging offer resilience changing lighting condition capturing object temperature yet scenario lack visual detail compared daytime visible image pose significant challenge human machine interpretation paper proposes novel diffusion method dubbed temporally consistent patch diffusion model tcdpm infraredtovisible video translation method extending patch diffusion model consists two key component firstly propose semanticguided denoising leveraging strong representation foundational model method faithfully preserve semantic structure generated visible image secondly propose novel temporal blending module guide denoising trajectory ensuring temporal consistency consecutive frame experiment show tcpdm outperforms stateoftheart method fvd infraredtovisible video translation daytonight object detection code publicly available
diffsign aiassisted generation customizable sign language video enhanced realism proliferation several streaming service recent year made possible diverse audience across world view medium content movie tv show translation dubbing service added make content accessible local audience support making content accessible people different ability deaf hard hearing dhh community still lagging goal make medium content accessible dhh community generating sign language video synthetic signer realistic expressive using signer given medium content viewed globally may limited appeal hence approach combine parametric modeling generative modeling generate realisticlooking synthetic signer customize appearance based user preference first retarget human sign language pose sign language avatar optimizing parametric model highfidelity pose rendered avatar used condition pose synthetic signer generated using diffusionbased generative model appearance synthetic signer controlled image prompt supplied visual adapter result show sign language video generated using approach better temporal consistency realism signing video generated diffusion model conditioned text prompt also support multimodal prompt allow user customize appearance signer accommodate diversity eg skin tone gender approach also useful signer anonymization
utilizing generative adversarial network image data augmentation classification semiconductor wafer dicing induced defect semiconductor manufacturing wafer dicing process central yet vulnerable defect significantly impair yield proportion defectfree chip deep neural network current state art semiautomated visual inspection however notoriously known require particularly large amount data model training address challenge explore application generative adversarial network gan image data augmentation classification semiconductor wafer dicing induced defect enhance variety balance training data visual inspection system approach synthetic yet realistic image generated mimic realworld dicing defect employ three different gan variant highresolution image synthesis deep convolutional gan dcgan cyclegan workinprogress result demonstrate improved classification accuracy obtained showing average improvement baseline experiment dcgan experiment balanced accuracy may enable yield optimization production
mvinpainter learning multiview consistent inpainting bridge editing novel view synthesis nv generation recently achieved prominent improvement however work mainly focus confined category synthetic asset discouraged generalizing challenging inthewild scene fail employed synthesis directly moreover method heavily depended camera pose limiting realworld application overcome issue propose mvinpainter reformulating editing multiview inpainting task specifically mvinpainter partially inpaints multiview image reference guidance rather intractably generating entirely novel view scratch largely simplifies difficulty inthewild nv leverage unmasked clue instead explicit pose condition ensure crossview consistency mvinpainter enhanced video prior motion component appearance guidance concatenated reference keyvalue attention furthermore mvinpainter incorporates slot attention aggregate highlevel optical flow feature unmasked region control camera movement posefree training inference sufficient scenelevel experiment objectcentric forwardfacing datasets verify effectiveness mvinpainter including diverse task multiview object removal synthesis insertion replacement project page httpsewrfcasgithubiomvinpainter
keypoint guided deformable image manipulation using diffusion model paper introduce keypointguided diffusion probabilistic model kdm gain precise control image manipulating object keypoint propose twostage generative model incorporating optical flow map intermediate output dense pixelwise understanding semantic relation image sparse key point configured leading realistic image generation additionally integration optical flow help regulate interframe variance sequential image demonstrating authentic sequential image generation kdm evaluated diverse keypoint conditioned image synthesis task including facial image generation human pose synthesis echocardiography video prediction demonstrating kdm proving consistency enhanced photorealistic image compared stateoftheart model
identifying solving conditional image leakage imagetovideo diffusion model diffusion model obtained substantial progress imagetovideo generation however paper find model tend generate video less motion expected attribute issue called conditional image leakage imagetovideo diffusion model tend overrely conditional image large time step address challenge inference training aspect first propose start generation process earlier time step avoid unreliable largetime step well initial noise distribution optimal analytic expression analyticinit minimizing kl divergence actual marginal distribution bridge traininginference gap second design timedependent noise distribution timenoise conditional image training applying higher noise level larger time step disrupt reduce model dependency validate general strategy various collected opendomain image benchmark dataset extensive result show method outperform baseline producing higher motion score lower error maintaining image alignment temporal consistency thereby yielding superior overall performance enabling accurate motion control project page urlhttpscondimageleakgithubio
svp styleenhanced vivid portrait talking head diffusion model talking head generation thg typically driven audio important challenging task broad application prospect various field digital human film production virtual reality diffusion modelbased thg method present high quality stable content generation often overlook intrinsic style encompasses personalized feature speaking habit facial expression video consequence generated video content lack diversity vividness thus limited real life scenario address issue propose novel framework named styleenhanced vivid portrait svp fully leverage stylerelated information thg specifically first introduce novel probabilistic style prior learning model intrinsic style gaussian distribution using facial expression audio embedding distribution learned bespoked contrastive objective effectively capturing dynamic style information video finetune pretrained stable diffusion sd model inject learned intrinsic style controlling signal via cross attention experiment show model generates diverse vivid highquality video flexible control intrinsic style outperforming existing stateoftheart method
towards precise scaling law video diffusion transformer achieving optimal performance video diffusion transformer within given data compute budget crucial due high training cost necessitates precisely determining optimal model size training hyperparameters largescale training scaling law employed language model predict performance existence accurate derivation visual generation model remain underexplored paper systematically analyze scaling law video diffusion transformer confirm presence moreover discover unlike language model video diffusion model sensitive learning rate batch size two hyperparameters often precisely modeled address propose new scaling law predicts optimal hyperparameters model size compute budget optimal setting achieve comparable performance reduce inference cost compared conventional scaling method within compute budget tflops furthermore establish generalized precise relationship among validation loss model size compute budget enables performance prediction nonoptimal model size may also appealed practical inference cost constraint achieving better tradeoff
instructionguided editing control image multimedia survey llm era rapid advancement large language model llm multimodal learning transformed digital content creation manipulation traditional visual editing tool require significant expertise limiting accessibility recent stride instructionbased editing enabled intuitive interaction visual content using natural language bridge user intent complex editing operation survey provides overview technique focusing llm multimodal model empower user achieve precise visual modification without deep technical knowledge synthesizing publication explore method generative adversarial network diffusion model examining multimodal integration finegrained content control discus practical application across domain fashion scene manipulation video synthesis highlighting increased accessibility alignment human intuition survey compare existing literature emphasizing llmempowered editing identifies key challenge stimulate research aim democratize powerful visual editing across various industry entertainment education interested reader encouraged access repository httpsgithubcomtamlhpawesomeinstructionediting
dgl dynamic globallocal prompt tuning textvideo retrieval textvideo retrieval critical multimodal task find relevant video text query although pretrained model like clip demonstrated impressive potential area rising cost fully finetuning model due increasing model size continues pose problem address challenge prompt tuning emerged alternative however existing work still face two problem adapting pretrained imagetext model downstream videotext task visual encoder could encode framelevel feature failed extract globallevel general video information equipping visual text encoder separated prompt failed mitigate visualtext modality gap end propose dgl crossmodal dynamic prompt tuning method globallocal video attention contrast previous prompt tuning method employ shared latent space generate locallevel text frame prompt encourage intermodal interaction furthermore propose modeling video globallocal attention mechanism capture global video information perspective prompt tuning extensive experiment reveal parameter tuned crossmodal prompt tuning strategy dgl outperforms comparable fully finetuning method msrvtt vatex lsmdc activitynet datasets code available httpsgithubcomknightyxpdgl
vlnvideo utilizing driving video outdoor visionandlanguage navigation outdoor visionandlanguage navigation vln requires agent navigate realistic outdoor environment based natural language instruction performance existing vln method limited insufficient diversity navigation environment limited training data address issue propose vlnvideo utilizes diverse outdoor environment present driving video multiple city u augmented automatically generated navigation instruction action improve outdoor vln performance vlnvideo combine best intuitive classical approach modern deep learning technique using template infilling generate grounded navigation instruction combined image rotation similaritybased navigation action predictor obtain vln style data driving video pretraining deep learning vln model pretrain model touchdown dataset videoaugmented dataset created driving video three proxy task masked language modeling instruction trajectory matching next action prediction learn temporallyaware visuallyaligned instruction representation learned instruction representation adapted stateoftheart navigator finetuning touchdown dataset empirical result demonstrate vlnvideo significantly outperforms previous stateoftheart model task completion rate achieving new stateoftheart touchdown dataset
orientationconditioned facial texture mapping videobased facial remote photoplethysmography estimation camerabased remote photoplethysmography rppg enables contactless measurement important physiological signal pulse rate pr however dynamic unconstrained subject motion introduces significant variability facial appearance video confounding ability videobased method accurately extract rppg signal study leverage facial surface construct novel orientationconditioned facial texture video representation improves motion robustness existing videobased facial rppg estimation method proposed method achieves significant performance improvement crossdataset testing mmpd baseline using physnet model trained pure highlighting efficacy generalization benefit designed video representation demonstrate significant performance improvement tested motion scenario crossdataset testing mmpd even presence dynamic unconstrained subject motion emphasizing benefit disentangling motion modeling facial surface motion robust facial rppg estimation validate efficacy design decision impact different video processing step ablation study finding illustrate potential strength exploiting facial surface general strategy addressing dynamic unconstrained subject motion video code available httpssamcantrillgithubioorientationuvrppg
contrast imitate adapt learning robotic skill raw human video learning robotic skill raw human video remains nontrivial challenge previous work tackled problem leveraging behavior cloning learning reward function video despite remarkable performance may introduce several issue necessity robot action requirement consistent viewpoint similar layout human robot video well low sample efficiency end key insight learn task prior contrasting video learn action prior imitating trajectory video utilize task prior guide trajectory adapt novel scenario propose threestage skill learning framework denoted contrastimitateadapt cia interactionaware alignment transformer proposed learn task prior temporally aligning video pair trajectory generation model used learn action prior adapt novel scenario different human video inversioninteraction method designed initialize coarse trajectory refine limited interaction addition cia introduces optimization method based semantic direction trajectory interaction security sample efficiency alignment distance computed iaaformer used reward evaluate cia six realworld everyday task empirically demonstrate cia significantly outperforms previous stateoftheart work term task success rate generalization diverse novel scenario layout object instance
one token seg language instructed reasoning segmentation video introduce videolisa videobased multimodal large language model designed tackle problem languageinstructed reasoning segmentation video leveraging reasoning capability world knowledge large language model augmented segment anything model videolisa generates temporally consistent segmentation mask video based language instruction existing imagebased method lisa struggle video task due additional temporal dimension requires temporal dynamic understanding consistent segmentation across frame videolisa address challenge integrating sparse dense sampling strategy videollm balance temporal context spatial detail within computational constraint additionally propose onetokensegall approach using specially designed trk token enabling model segment track object across multiple frame extensive evaluation diverse benchmark including newly introduced reasonvos benchmark demonstrate videolisas superior performance video object segmentation task involving complex reasoning temporal understanding object tracking optimized video videolisa also show promising generalization image segmentation revealing potential unified foundation model languageinstructed object segmentation code model available httpsgithubcomshowlabvideolisa
grounding need dual temporal grounding video dialog realm video dialog response generation understanding video content temporal nuance conversation history paramount segment current research lean heavily largescale pretrained visuallanguage model often overlook temporal dynamic another delf deep spatialtemporal relationship within video demand intricate object trajectory preextractions sideline dialog temporal dynamic paper introduces dual temporal groundingenhanced video dialog model dtgvd strategically designed merge strength dominant approach emphasizes dual temporal relationship predicting dialog turnspecific temporal region filtering video content accordingly grounding response video dialog context one standout feature dtgvd heightened attention chronological interplay recognizing acting upon dependency different dialog turn capture nuanced conversational dynamic bolster alignment video dialog temporal dynamic weve implemented listwise contrastive learning strategy within framework accurately grounded turnclip pairing designated positive sample less precise pairing categorized negative refined classification funneled holistic endtoend response generation mechanism evaluation using datasets underscore superiority methodology
multipair temporal sentence grounding via multithread knowledge transfer network given videoquery pair untrimmed video sentence query temporal sentence grounding tsg aim locate queryrelevant segment video although previous respectable tsg method achieved remarkable success train videoquery pair separately ignore relationship different pair observe similar videoquery content help tsg model better understand generalize crossmodal representation also assist model locating complex videoquery pair previous method follow singlethread framework cotrain different pair usually spends much time reobtaining redundant knowledge limiting realworld application end paper pose brandnew setting multipair tsg aim cotrain pair particular propose novel videoquery cotraining approach multithread knowledge transfer network locate variety videoquery pair effectively efficiently firstly mine spatial temporal semantics across different query cooperate learn intra intermodal representation simultaneously design crossmodal contrast module explore semantic consistency selfsupervised strategy fully align visual textual representation different pair design prototype alignment strategy match object prototype phrase prototype spatial alignment align activity prototype sentence prototype temporal alignment finally develop adaptive negative selection module adaptively generate threshold crossmodal matching extensive experiment show effectiveness efficiency proposed method
hierarchical banzhaf interaction general videolanguage representation learning multimodal representation learning contrastive learning play important role artificial intelligence domain important subfield videolanguage representation learning focus learning representation using global semantic interaction predefined videotext pair however enhance refine coarsegrained global interaction detailed interaction necessary finegrained multimodal learning study introduce new approach model videotext game player using multivariate cooperative game theory handle uncertainty finegrained semantic interaction diverse granularity flexible combination vague intensity specifically design hierarchical banzhaf interaction simulate finegrained correspondence video clip textual word hierarchical perspective furthermore mitigate bias calculation within banzhaf interaction propose reconstructing representation fusion singlemodal crossmodal component reconstructed representation ensures fine granularity comparable singlemodal representation also preserving adaptive encoding characteristic crossmodal representation additionally extend original structure flexible encoderdecoder framework enabling model adapt various downstream task extensive experiment commonly used textvideo retrieval videoquestion answering video captioning benchmark superior performance validate effectiveness generalization method
explicit generation object without score distillation recent year increasing demand dynamic asset design gaming application given rise powerful generative pipeline capable synthesizing highquality object previous method generally rely score distillation sampling sd algorithm infer unseen view motion object thus leading unsatisfactory result defect like oversaturation janus problem therefore inspired recent progress video diffusion model propose optimize representation explicitly generating multiview video one input image however far trivial handle practical challenge faced pipeline including dramatic temporal inconsistency interframe geometry texture diversity semantic defect brought video generation result address issue propose novel multistage framework generates highquality consistent asset without score distillation specifically collaborative technique solution developed including attention injection strategy synthesize temporalconsistent multiview video robust efficient dynamic reconstruction method based gaussian splatting refinement stage diffusion prior semantic restoration qualitative result user preference study demonstrate framework outperforms baseline generation quality considerable margin code released
improving deep learning segmentation biophysically motivated cell synthesis biomedical research increasingly relies cell culture model aibased analysis potentially facilitate detailed accurate feature extraction singlecell level however requires precise segmentation cell datasets turn demand highquality ground truth training manual annotation gold standard ground truth data timeconsuming thus feasible generation large training datasets address present novel framework generating training data integrates biophysical modeling realistic cell shape alignment approach allows silico generation coherent membrane nucleus signal enable training segmentation model utilizing channel improved performance furthermore present new gan training scheme generates image data also matching label quantitative evaluation show superior performance biophysical motivated synthetic training data even outperforming manual annotation pretrained model underscore potential incorporating biophysical modeling enhancing synthetic training data quality
perceptual eventstovideo reconstruction using diffusion prior event camera mimicking human retina capture brightness change unparalleled temporal resolution dynamic range integrating event intensity pose highly illposed challenge marred initial condition ambiguity traditional regressionbased deep learning method fall short perceptual quality offering deterministic often unrealistic reconstruction paper introduce diffusion model eventstovideo reconstruction achieving colorful realistic perceptually superior video generation achromatic event powered image generation ability knowledge pretrained diffusion model proposed method achieve better tradeoff perception distortion reconstructed frame compared previous solution extensive experiment benchmark datasets demonstrate approach produce diverse realistic frame faithfulness given event
dip diffusion learning inconsistency pattern general deepfake detection advancement deepfake generation technique importance deepfake detection protecting multimedia content integrity become increasingly obvious recently temporal inconsistency clue explored improve generalizability deepfake video detection according observation temporal artifact forged video term motion information usually exhibit quite distinct inconsistency pattern along horizontal vertical direction could leveraged improve generalizability detector paper transformerbased framework diffusion learning inconsistency pattern dip proposed exploit directional inconsistency deepfake video detection specifically dip begin spatiotemporal encoder represent spatiotemporal information directional inconsistency decoder adopted accordingly directionaware attention inconsistency diffusion incorporated explore potential inconsistency pattern jointly learn inherent relationship addition spatiotemporal invariant loss sti loss introduced contrast spatiotemporally augmented sample pair prevent model overfitting nonessential forgery artifact extensive experiment several public datasets demonstrate method could effectively identify directional forgery clue achieve stateoftheart performance
physicsinformed latent diffusion multimodal brain mri synthesis recent advance generative model medical imaging shown promise representing multiple modality however variability modality availability across datasets limit general applicability synthetic data produce address present novel physicsinformed generative model capable synthesizing variable number brain mri modality including present original dataset approach utilizes latent diffusion model twostep generative process first unobserved physical tissue property map synthesized using latent diffusion model map combined physical signal model generate final mri scan experiment demonstrate efficacy approach generating unseen mr contrast preserving physical plausibility furthermore validate distribution generated tissue property comparing measured real brain tissue
trainingfree video temporal grounding using largescale pretrained model video temporal grounding aim identify video segment within untrimmed video relevant given natural language query existing video temporal localization model rely specific datasets training high data collection cost exhibit poor generalization capability acrossdataset outofdistribution ood setting paper propose trainingfree video temporal grounding tfvtg approach leverage ability pretrained large model naive baseline enumerate proposal video use pretrained visual language model vlms select best proposal according visionlanguage alignment however existing vlms trained imagetext pair trimmed video cliptext pair making struggle grasp relationship distinguish temporal boundary multiple event within video comprehend sensitive dynamic transition event transition one event another video address issue propose leveraging large language model llm analyze multiple subevents contained query text analyze temporal order relationship event secondly split subevent dynamic transition static status part propose dynamic static scoring function using vlms better evaluate relevance event description finally subevent description use vlms locate topk proposal leverage order relationship subevents provided llm filter integrate proposal method achieves best performance zeroshot video temporal grounding charadessta activitynet caption datasets without training demonstrates better generalization capability crossdataset ood setting
olvit multimodal state tracking via attentionbased embeddings videogrounded dialog present object language video transformer olvit novel model video dialog operating multimodal attentionbased dialog state tracker existing video dialog model struggle question requiring spatial temporal localization within video longterm temporal reasoning accurate object tracking across multiple dialog turn olvit address challenge maintaining global dialog state based output object state tracker ost language state tracker lst ost attends important object within video lst keep track important linguistic coreference previous dialog turn stark contrast previous work approach generic nature therefore capable learning continuous multimodal dialog state representation relevant object round result seamlessly integrated large language model llm offer high flexibility dealing different datasets task evaluation challenging dvd response classification simmc response generation datasets show olvit achieves new stateoftheart performance across datasets
cococo improving textguided video inpainting better consistency controllability compatibility recent advancement video generation remarkable yet many existing method struggle issue consistency poor textvideo alignment moreover field lack effective technique textguided video inpainting stark contrast wellexplored domain textguided image inpainting end paper proposes novel textguided video inpainting model achieves better consistency controllability compatibility specifically introduce simple efficient motion capture module preserve motion consistency design instanceaware region selection instead random region selection obtain better textual controllability utilize novel strategy inject personalized model cococo model thus obtain better model compatibility extensive experiment show model generate highquality video clip meanwhile model show better motion consistency textual controllability model compatibility detail shown cococozibojiagithubiocococozibojiagithubio
learned scanpaths aid blind panoramic video quality assessment panoramic video advantage providing immersive interactive viewing experience nevertheless spherical nature give rise various uncertain user viewing behavior pose significant challenge panoramic video quality assessment pvqa work propose endtoend optimized blind pvqa method explicit modeling user viewing pattern visual scanpaths method consists two module scanpath generator quality assessor scanpath generator initially trained predict future scanpaths minimizing expected code length jointly optimized quality assessor quality prediction blind pvqa method enables direct quality assessment panoramic image treating video composed identical frame experiment three public panoramic image video quality datasets encompassing synthetic authentic distortion validate superiority blind pvqa model existing method
dibs enhancing dense video captioning unlabeled video via pseudo boundary enrichment online refinement present dive boundary dibs novel pretraining framework dense video captioning dvc elaborates improving quality generated event caption associated pseudo event boundary unlabeled video leveraging capability diverse large language model llm generate rich dvcoriented caption candidate optimize corresponding pseudo boundary several meticulously designed objective considering diversity eventcentricity temporal ordering coherence moreover introduce novel online boundary refinement strategy iteratively improves quality pseudo boundary training comprehensive experiment conducted examine effectiveness proposed technique component leveraging substantial amount unlabeled video data achieve remarkable advancement standard dvc datasets like activitynet outperform previous stateoftheart across majority metric achieving unlabeled video data used pretraining
video enriched retrieval augmented generation using aligned video caption work propose use aligned visual caption mechanism integrating information contained within video retrieval augmented generation rag based chat assistant system caption able describe visual audio content video large corpus advantage textual format easy reason incorporate large language model llm prompt also typically require less multimedia content inserted multimodal llm context window typical configuration aggressively fill context window sampling video frame source video furthermore visual caption adapted specific use case prompting original foundational model captioner particular visual detail fine tuning hope helping advancing progress area curate dataset describe automatic evaluation procedure common rag task
youtube sfvhdr quality dataset popularity short form video sfv grown dramatically past year become phenomenal video category billion viewer meanwhile high dynamic range hdr advanced feature also becomes popular video sharing platform hot topic huge impact sfv hdr bring new question video quality research sfvhdr quality assessment significantly different traditional user generated content ugc quality assessment objective quality metric designed traditional ugc still work well sfvhdr answer question created first large scale sfvhdr dataset reliable subjective quality score covering popular content category also introduce general sampling framework maximize representativeness dataset provided comprehensive analysis subjective quality score short form sdr hdr video discus reliability stateoftheart ugc quality metric potential improvement
contextaware temporal embedding object video data video analysis understanding temporal context crucial recognizing object interaction event pattern contextual change time proposed model leverage adjacency semantic similarity object neighboring video frame construct contextaware temporal object embeddings unlike traditional method rely solely visual appearance temporal embedding model considers contextual relationship object creating meaningful embedding space temporally connected object vector positioned proximity empirical study demonstrate contextaware temporal embeddings used conjunction conventional visual embeddings enhance effectiveness downstream application moreover embeddings used narrate video using large language model llm paper describes intricate detail proposed objective function generate contextaware temporal object embeddings video data showcase potential application generated embeddings video analysis object classification task
portrait video editing empowered multimodal generative prior introduce portraitgen powerful portrait video editing method achieves consistent expressive stylization multimodal prompt traditional portrait video editing method often struggle temporal consistency typically lack rendering quality efficiency address issue lift portrait video frame unified dynamic gaussian field ensures structural temporal coherence across frame furthermore design novel neural gaussian texture mechanism enables sophisticated style editing also achieves rendering speed approach incorporates multimodal input knowledge distilled largescale generative model system also incorporates expression similarity guidance faceaware portrait editing module effectively mitigating degradation issue associated iterative dataset update extensive experiment demonstrate temporal consistency editing efficiency superior rendering quality method broad applicability proposed approach demonstrated various application including textdriven editing imagedriven editing relighting highlighting great potential advance field video editing demo video released code provided project page
bridging vision language modeling causality temporality video narrative video captioning critical task field multimodal machine learning aiming generate descriptive coherent textual narrative video content large visionlanguage model lvlms shown significant progress often struggle capture causal temporal dynamic inherent complex video sequence address limitation propose enhanced framework integrates causaltemporal reasoning module ctrm stateoftheart lvlms ctrm comprises two key component causal dynamic encoder cde temporal relational learner trl collectively encode causal dependency temporal consistency video frame design multistage learning strategy optimize model combining pretraining largescale videotext datasets finetuning causally annotated data contrastive alignment better embedding coherence experimental result standard benchmark msvd msrvtt demonstrate method outperforms existing approach automatic metric cider rougel human evaluation achieving fluent coherent relevant caption result validate effectiveness approach generating caption enriched causaltemporal narrative
teasergen generating teaser long documentary teaser effective tool promoting content entertainment commercial educational field however creating effective teaser long video challenging requires longrange multimodal modeling input video necessitating maintaining audiovisual alignment managing scene change preserving factual accuracy output teaser due lack publiclyavailable dataset progress along research direction hindered work present documentarynet collection documentary paired teaser featuring multimodal data stream video speech music sound effect narration documentarynet propose new twostage system generating teaser long documentary proposed teasergen system first generates teaser narration transcribed narration documentary using pretrained large language model selects relevant visual content accompany generated narration languagevision model narrationvideo matching explore two approach pretrainingbased model using pretrained contrastive languagevision model deep sequential model learns mapping narration visuals experimental result show pretrainingbased approach effective identifying relevant visual content directly trained deep autoregressive model
fast dynamic object generation singleview video generating dynamic object singleview video challenging due lack labeled data intuitive approach extend previous pipeline transferring offtheshelf image generation model score distillation samplinghowever approach would slow expensive scale due need backpropagating informationlimited supervision signal large pretrained model address propose efficient object generation framework called generates highquality spacetimeconsistent image different camera view us labeled data directly reconstruct content gaussian splatting model importantly method achieve realtime rendering continuous camera trajectory enable robust reconstruction sparse view introduce inconsistencyaware confidenceweighted loss design along lightly weighted score distillation loss extensive experiment synthetic real video show offer remarkable increase speed compared prior art alternative preserving quality novel view synthesis example take minute model dynamic object v minute previous art model
eprecon efficient framework realtime panoptic reconstruction monocular video panoptic reconstruction monocular video fundamental perceptual task robotic scene understanding however existing effort suffer inefficiency term inference speed accuracy limiting practical applicability present eprecon efficient realtime panoptic reconstruction framework current volumetricbased reconstruction method usually utilize multiview depth map fusion obtain scene depth prior timeconsuming pose challenge realtime scene reconstruction address issue propose lightweight module directly estimate scene depth prior volume reconstruction quality improvement generating occupancy probability voxels addition compared existing panoptic segmentation method eprecon extract panoptic feature voxel feature corresponding image feature obtaining detailed comprehensive instancelevel semantic information achieving accurate segmentation result experimental result dataset demonstrate superiority eprecon current stateoftheart method term panoptic reconstruction quality realtime inference code available
simple early exiting framework accelerated sampling diffusion model diffusion model shown remarkable performance generation problem various domain including image video text audio practical bottleneck diffusion model sampling speed due repeated evaluation score estimation network inference work propose novel framework capable adaptively allocating compute required score estimation thereby reducing overall sampling time diffusion model observe amount computation required score estimation may vary along time step score estimated based observation propose earlyexiting scheme skip subset parameter score estimation network inference based timedependent exit schedule using diffusion model image synthesis show method could significantly improve sampling throughput diffusion model without compromising image quality furthermore also demonstrate method seamlessly integrates various type solver faster sampling capitalizing compatibility enhance overall efficiency source code experiment available urlhttpsgithubcomtaehongmooneediffusion
imageconditioned diffusion model qspace upsampling dwi data propose imageconditioned diffusion model estimate high angular resolution diffusion weighted imaging dwi low angular resolution acquisition model call take input set low angular resolution dwi data us information estimate dwi data associated target gradient direction leverage unet architecture crossattention preserve positional information reference image guiding target image generation train evaluate singleshell dwi sample curated human connectome project hcp dataset specifically subsample hcp gradient direction produce low angular resolution dwi data train reconstruct missing high angular resolution sample compare two stateoftheart gan model result demonstrate achieves higherquality generated image consistently outperforms gan model downstream tensor estimation across multiple metric taken together study highlight potential diffusion model particular qspace upsampling thus offering promising toolkit clinical research application
garfield reinforced gaussian radiance field largescale scene reconstruction paper proposes novel framework largescale scene reconstruction based gaussian splatting aim address scalability accuracy challenge faced existing method tackling scalability issue split large scene multiple cell candidate pointcloud camera view cell correlated visibilitybased camera selection progressive pointcloud extension reinforce rendering quality three highlighted improvement made comparison vanilla strategy raygaussian intersection novel gaussians density control learning efficiency appearance decoupling module based convkan network solve uneven lighting condition largescale scene refined final loss color loss depth distortion loss normal consistency loss finally seamless stitching procedure executed merge individual gaussian radiance field novel view synthesis across different cell evaluation matrixcity datasets show method consistently generates highfidelity rendering result stateoftheart method largescale scene reconstruction validate generalizability proposed approach rendering selfcollected video clip recorded commercial drone
instructionbased image manipulation watching thing move paper introduces novel dataset construction pipeline sample pair frame video us multimodal large language model mllms generate editing instruction training instructionbased image manipulation model video frame inherently preserve identity subject scene ensuring consistent content preservation editing additionally video data capture diverse natural dynamicssuch nonrigid subject motion complex camera movementsthat difficult model otherwise making ideal source scalable dataset construction using approach create new dataset train instructmove model capable instructionbased complex manipulation difficult achieve synthetically generated datasets model demonstrates stateoftheart performance task adjusting subject pose rearranging element altering camera perspective
video prediction policy generalist robot policy predictive visual representation recent advancement robotics focused developing generalist policy capable performing multiple task typically policy utilize pretrained vision encoders capture crucial information current observation however previous vision encoders trained twoimage contrastive learning singleimage reconstruction perfectly capture sequential information essential embodied task recently video diffusion model vdms demonstrated capability accurately predict future image sequence exhibiting good understanding physical dynamic motivated strong visual prediction capability vdms hypothesize inherently possess visual representation reflect evolution physical world term predictive visual representation building hypothesis propose video prediction policy vpp generalist robotic policy conditioned predictive visual representation vdms enhance representation incorporate diverse human robotic manipulation datasets employing unified videogeneration training objective vpp consistently outperforms existing method across two simulated two realworld benchmark notably achieves relative improvement calvin abcd benchmark compared previous stateoftheart delivers increase success rate complex realworld dexterous manipulation task
generative inbetweening framewise conditionsdriven video generation generative inbetweening aim generate intermediate frame sequence utilizing two key frame input although remarkable progress made video generation model generative inbetweening still face challenge maintaining temporal stability due ambiguous interpolation path two key frame issue becomes particularly severe large motion gap input frame paper propose straightforward yet highly effective framewise conditionsdriven video generation fcvg method significantly enhances temporal stability interpolated video frame specifically fcvg provides explicit condition frame making much easier identify interpolation path two input frame thus ensuring temporally stable production visually plausible video frame achieve suggest extracting matched line two input frame easily interpolated frame frame serving framewise condition seamlessly integrated existing video generation model extensive evaluation covering diverse scenario natural landscape complex human pose camera movement animation existing method often exhibit incoherent transition across frame contrast fcvg demonstrates capability generate temporally stable video using linear nonlinear interpolation curve project page code available urlhttpsfcvginbetweengithubio
mitsgan safeguarding medical imaging tampering generative adversarial network progress generative model particularly generative adversarial network gans opened new possibility image generation raised concern potential malicious us especially sensitive area like medical imaging study introduces mitsgan novel approach prevent tampering medical image specific focus ct scan approach disrupts output attacker ctgan architecture introducing finely tuned perturbation imperceptible human eye specifically proposed approach involves introduction appropriate gaussian noise input protective measure various attack method aim enhance tamper resistance comparing favorably existing technique experimental result ct scan demonstrate mitsgans superior performance emphasizing ability generate tamperresistant image negligible artifact image tampering medical domain pose lifethreatening risk proactive approach contributes responsible ethical use generative model work provides foundation future research countering cyber threat medical imaging model code publicly available
multiscale conditional generative modeling microscopic image restoration advance diffusionbased generative model recent year revolutionized stateoftheart sota technique wide variety image analysis synthesis task whereas adaptation image restoration particularly within computational microscopy remains theoretically empirically underexplored research introduce multiscale generative model enhances conditional image restoration novel exploitation brownian bridge process within wavelet domain initiating brownian bridge diffusion process specifically lowestfrequency subband applying generative adversarial network subsequent multiscale highfrequency subbands wavelet domain method provides significant acceleration training sampling sustaining high image generation quality diversity par sota diffusion model experimental result various computational microscopy imaging task confirm method robust performance considerable reduction sampling step time pioneering technique offer efficient image restoration framework harmonizes efficiency quality signifying major stride incorporating cuttingedge generative model computational microscopy workflow
scaling law diffusion transformer diffusion transformer dit already achieved appealing synthesis scaling property content recreation eg image video generation however scaling law dit less explored usually offer precise prediction regarding optimal model size data requirement given specific compute budget therefore experiment across broad range compute budget flop conducted confirm existence scaling law dit first time concretely loss pretraining dit also follows powerlaw relationship involved compute based scaling law determine optimal model size required data also accurately predict texttoimage generation loss given model parameter compute budget flop additionally also demonstrate trend pretraining loss match generation performance eg fid even across various datasets complement mapping compute synthesis quality thus provides predictable benchmark assesses model performance data quality reduced cost
accelerating diffusion sartooptical image translation via adversarial consistency distillation synthetic aperture radar sar provides allweather highresolution imaging capability unique imaging mechanism often requires expert interpretation limiting widespread applicability translating sar image easily recognizable optical image using diffusion model help address challenge however diffusion model suffer high latency due numerous iterative inference generative adversarial network gans achieve image translation single iteration often cost image quality overcome issue propose new training framework sartooptical image translation combine strength approach method employ consistency distillation reduce iterative inference step integrates adversarial learning ensure image clarity minimize color shift additionally approach allows tradeoff quality speed providing flexibility based application requirement conducted experiment datasets performing quantitative evaluation using peak signaltonoise ratio psnr structural similarity index ssim frechet inception distance fid well calculating inference latency result demonstrate approach significantly improves inference speed time maintaining visual quality generated image thus offering robust efficient solution sartooptical image translation
loopgaussian creating cinemagraph multiview image via eulerian motion field cinemagraph unique form visual medium combine element still photography subtle motion create captivating experience however majority video generated recent work lack depth information confined constraint image space paper inspired significant progress field novel view synthesis nv achieved gaussian splatting propose loopgaussian elevate cinemagraph image space space using gaussian modeling achieve first employ method reconstruct gaussian point cloud multiview image static scenesincorporating shape regularization term prevent blurring artifact caused object deformation adopt autoencoder tailored gaussian project feature space maintain local continuity scene devise supergaussian clustering based acquired feature calculating similarity cluster employing twostage estimation method derive eulerian motion field describe velocity across entire scene gaussian point move within estimated eulerian motion field bidirectional animation technique ultimately generate cinemagraph exhibit natural seamlessly loopable dynamic experiment result validate effectiveness approach demonstrating highquality visually appealing scene generation project available httpspokerlishaogithubioloopgaussian
advanced multimodal liver tumor segmentation via diffusionbased image synthesis alignment multimodal learning demonstrated enhance performance across various clinical task owing diverse perspective offered different modality data however existing multimodal segmentation method rely wellregistered multimodal data unrealistic realworld clinical image particularly indistinct diffuse region liver tumor paper introduce fourstage multimodal liver tumor segmentation pipeline preregistration target organ multimodal ct dilation annotated modality mask followed use inpainting obtain multimodal normal ct without tumor synthesis strictly aligned multimodal ct tumor using latent diffusion model based multimodal ct feature randomly generated tumor mask finally training segmentation model thus eliminating need strictly aligned multimodal data extensive experiment public internal datasets demonstrate superiority stateoftheart multimodal segmentation method
towards accurate liptospeech synthesis inthewild paper introduce novel approach address task synthesizing speech silent video inthewild speaker solely based lip movement traditional approach directly generating speech lip video face challenge able learn robust language model speech alone resulting unsatisfactory outcome overcome issue propose incorporating noisy text supervision using stateoftheart liptotext network instills language information model noisy text generated using pretrained liptotext model enabling approach work without text annotation inference design visual texttospeech network utilizes visual stream generate accurate speech insync silent input video perform extensive experiment ablation study demonstrating approach superiority current stateoftheart method various benchmark datasets demonstrate essential practical application method assistive technology generating speech al patient lost voice make mouth movement demo video code additional detail found
highfidelity lipsynced talking face synthesis via landmarkbased diffusion model audiodriven talking face video generation attracted increasing attention due huge industrial potential previous method focus learning direct mapping audio visual content despite progress often struggle ambiguity mapping process leading flawed result alternative strategy involves facial structural representation eg facial landmark intermediary multistage approach better preserve appearance detail suffers error accumulation due independent optimization different stage moreover previous method rely generative adversarial network prone training instability mode collapse address challenge study proposes novel landmarkbased diffusion model talking face generation leverage facial landmark intermediate representation enabling endtoend optimization specifically first establish less ambiguous mapping audio landmark motion lip jaw introduce innovative conditioning module called talkformer align synthesized motion motion represented landmark via differentiable crossattention enables endtoend optimization improved lip synchronization besides talkformer employ implicit feature warping align reference image feature target motion preserving appearance detail extensive experiment demonstrate approach synthesize highfidelity lipsynced talking face video preserving subject appearance detail reference image
timesuite improving mllms long video understanding via grounded tuning multimodal large language model mllms demonstrated impressive performance short video understanding however understanding longform video still remains challenging mllms paper proposes timesuite collection new design adapt existing shortform video mllms long video understanding including simple yet efficient framework process long video sequence highquality video dataset grounded tuning mllms carefullydesigned instruction tuning task explicitly incorporate grounding supervision traditional qa format specifically based videochat propose longvideo mllm coined videochatt implementing token shuffling compress long video token introducing temporal adaptive position encoding tape enhance temporal awareness visual representation meanwhile introduce timepro comprehensive groundingcentric instruction tuning dataset composed task highquality grounded annotation notably design new instruction tuning task type called temporal grounded caption peform detailed video description corresponding time stamp prediction explicit temporal location prediction guide mllm correctly attend visual content generating description thus reduce hallucination risk caused llm experimental result demonstrate timesuite provides successful solution enhance long video understanding capability shortform mllm achieving improvement benchmark egoschema videomme respectively addition videochatt exhibit robust zeroshot temporal grounding capability significantly outperforming existing stateoftheart mllms finetuning performs par traditional supervised expert model
knowledge guided entityaware video captioning basketball benchmark despite recent emergence video captioning model generate text description specific entity name finegrained action far solved however great application basketball live text broadcast paper new multimodal knowledge graph supported basketball benchmark video captioning proposed specifically construct multimodal basketball game knowledge graph provide additional knowledge beyond video multimodal basketball game video captioning dataset contains type finegrained shooting event player knowledge ie image name constructed based develop knowledge guided entityaware video captioning network keanet based candidate player list encoderdecoder form basketball live text broadcast temporal contextual information video encoded introducing bidirectional gru bigru module entityaware module designed model relationship among player highlight key player extensive experiment multiple sport benchmark demonstrate keanet effectively leverage extera knowledge outperforms advanced video captioning model proposed dataset corresponding code publicly available soon
leveraging irregular repetition prior improving video action counting video action counting vac crucial analyzing sport fitness everyday activity quantifying repetitive action video however traditional vac method overlooked complexity action repetition interruption variability cycle duration research address shortfall introducing novel approach vac called irregular video action counting ivac ivac prioritizes modeling irregular repetition pattern video define two primary aspect intercycle consistency cycleinterval inconsistency intercycle consistency ensures homogeneity spatialtemporal representation cycle segment signifying action uniformity within cycle cycleinterval inconsistency highlight importance distinguishing cycle segment interval based inherent content difference encapsulate principle propose new methodology includes consistency inconsistency module supported unique pullpush loss mechanism model applies pull loss promote coherence among cycle segment feature push loss clearly distinguish feature cycle segment interval segment empirical evaluation conducted repcount dataset demonstrate model set new benchmark vac task performance furthermore model demonstrates exceptional adaptability generalization across various video content outperforming existing model two additional datasets ucfrep countix without need datasetspecific optimization result confirm efficacy approach addressing irregular repetition video pave way advancement video analysis understanding
fast encoderbased casual video via point track processing paper address longstanding challenge reconstructing structure video dynamic content current approach problem designed operate casual video recorded standard camera require long optimization time aiming significantly improve efficiency previous approach present learningbased approach enables inferring structure camera position dynamic content originating casual video using single efficient feedforward pas achieve propose operating directly point track input designing architecture tailored processing point track proposed architecture designed two key principle mind take account inherent symmetry present input point track data assumes movement pattern effectively represented using lowrank approximation trained unsupervised way dataset casual video utilizing point track extracted video without supervision experiment show reconstruct temporal point cloud camera position underlying video accuracy comparable stateoftheart method drastically reducing runtime show generalizes well unseen video unseen semantic category inference time
beyond alignment blind video face restoration via parsingguided temporalcoherent transformer multiple complex degradation coupled lowquality video face real world therefore blind video face restoration highly challenging illposed problem requiring hallucinating highfidelity detail also enhancing temporal coherence across diverse pose variation restoring frame independently naive manner inevitably introduces temporal incoherence artifact pose change keypoint localization error address propose first blind video face restoration approach novel parsingguided temporalcoherent transformer pgtformer without prealignment pgtformer leverage semantic parsing guidance select optimal face prior generating temporally coherent artifactfree result specifically pretrain temporalspatial vector quantized autoencoder highquality video face datasets extract expressive contextrich prior temporal parseguided codebook predictor tpcp restores face different pose based face parsing context cue without performing face prealignment strategy reduces artifact mitigates jitter caused cumulative error face prealignment finally temporal fidelity regulator tfr enhances fidelity temporal feature interaction improves video temporal consistency extensive experiment face video show method outperforms previous face restoration baseline code released hrefhttpsgithubcomkepengxupgtformerhttpsgithubcomkepengxupgtformer
unveiling potential harnessing deep metric learning circumvent video streaming encryption encryption internet shift http important step improve privacy internet user however increasing body work extracting information encrypted internet traffic without decrypt attack bypass security guarantee assumed given http thus need understood prior work showed variable bitrates video stream sufficient identify video someone watching work generally make tradeoff aspect accuracy scalability robustness etc tradeoff complicate practical use attack end propose deep metric learning framework based triplet loss method framework achieve robust generalisable scalable transferable encrypted video stream detection first triplet loss better able deal video stream seen training second approach accurately classify video seen training third show method scale well dataset video finally show model trained video stream chrome also classify stream firefox result suggest sidechannel attack broadly applicable originally thought provide code alongside diverse uptodate dataset future research
videosalmonn speechenhanced audiovisual large language model speech understanding element generic video understanding using audiovisual large language model avllms crucial yet understudied aspect paper proposes videosalmonn single endtoend avllm video processing understand visual frame sequence audio event music speech well obtain finegrained temporal information required speech understanding keeping efficient video element paper proposes novel multiresolution causal qformer mrc qformer structure connect pretrained audiovisual encoders backbone large language model moreover dedicated training approach including diversity loss unpaired audiovisual mixed training scheme proposed avoid frame modality dominance introduced speechaudiovisual evaluation benchmark videosalmonn achieves absolute accuracy improvement videoqa task absolute accuracy improvement audiovisual qa task human speech addition videosalmonn demonstrates remarkable video comprehension reasoning ability task unprecedented avllms training code model checkpoint available texttturlhttpsgithubcombytedancesalmonn
hypergraph multimodal large language model exploiting eeg eyetracking modality evaluate heterogeneous response video understanding understanding video creativity content often varies among individual difference focal point cognitive level across different age experience gender currently lack research area existing benchmark suffer several drawback limited number modality answer restrictive length content scenario within video excessively monotonous transmitting allegory emotion overly simplistic bridge gap realworld application introduce largescale subjective response indicator advertisement video dataset namely sriadv specifically collected real change electroencephalographic eeg eyetracking region different demographic viewed identical video content utilizing multimodal dataset developed task protocol analyze evaluate extent cognitive understanding video content among different user along dataset designed hypergraph multimodal large language model hmllm explore association among different demographic video element eeg eyetracking indicator hmllm could bridge semantic gap across rich modality integrate information beyond different modality perform logical reasoning extensive experimental evaluation sriadv additional videobased generative performance benchmark demonstrate effectiveness method code dataset released httpsgithubcommininglampmllmhmllm
sigma sinkhornguided masked video modeling videobased pretraining offer immense potential learning strong visual representation unprecedented scale recently masked video modeling method shown promising scalability yet fall short capturing higherlevel semantics due reconstructing predefined lowlevel target pixel tackle present sinkhornguided masked video modelling sigma novel video pretraining method jointly learns video model addition target feature space using projection network however simple modification mean regular reconstruction loss lead trivial solution network jointly optimized solution distribute feature spacetime tube evenly across limited number learnable cluster posing optimal transport problem enforce high entropy generated feature across batch infusing semantic temporal meaning feature space resulting cluster assignment used target symmetric prediction task video model predicts cluster assignment projection network vice versa experimental result ten datasets across three benchmark validate effectiveness sigma learning performant temporallyaware robust video representation improving upon stateoftheart method project website code available httpsquvalabgithubiosigma
explore crosscodec qualityrate convex hull relation adaptive streaming ongoing advancement video technology emergence new video platform supplier video content striving ensure video quality meet desire consumer accessing limited amount channel bandwidth often looking novel approach decrease use data thus required energy cost study evaluates quality rate performance codecs across resolution optimize video quality minimizing bitrate crucial energy cost efficiency approach original video native resolution encoded decoded rescaled using ffmpeg resolution encoding decoding performed various quantization level quality rate qr curve generated using psnr vmaf metric bitrate convex hull curve derived mathematically modelled resolution procedure systematically applied codecs result indicate increasing crf value reduce bitrate psnr vmaf psnr ranging db logarithmic polynomial modelling convex hull demonstrated high accuracy low rmse high rsquared value finding suggest convex hull one codec predict performance others aiding future contentdriven prediction methodology enhancing adaptive streaming efficiency keywords video codecs adaptive streaming compression bitrate psnr vmaf
evaluation large pretrained model gesture recognition using synthetic video work explore possibility using synthetically generated data videobased gesture recognition large pretrained model consider whether model sufficiently robust expressive representation space enable trainingfree classification specifically utilize various stateoftheart video encoders extract feature use knearest neighbor classification training data point derived synthetic video compare result another trainingfree approach zeroshot classification using text description gesture experiment dataset find using synthetic training video yield significantly lower classification accuracy real test video compared using relatively small number real training video also observe video backbone finetuned classification task serve superior feature extractor choice finetuning data substantial impact knearest neighbor performance lastly find zeroshot textbased classification performs poorly gesture recognition task gesture easily described natural language
data playwright authoring data video annotated narration creating data video effectively narrate story animated visuals requires substantial effort expertise promising research trend leveraging easytouse natural language nl interaction automatically synthesize data video component narrative content like text narration nl command specify userrequired design nevertheless previous research overlooked integration narrative content specific design authoring command leading generated result lack customization fail seamlessly fit narrative context address issue introduce novel paradigm creating data video seamlessly integrates user authoring narrative intent unified format called annotated narration allowing user incorporate nl command design authoring inline annotation within narration text informed formative study user preference annotated narration develop prototype system named data playwright embodies paradigm effective creation data video within data playwright user write annotated narration based uploaded visualization system interpreter automatically understands user input synthesizes data video narrationanimation interplay powered large language model finally user preview finetune video user study demonstrated participant effectively create data video data playwright effortlessly articulating desired outcome annotated narration
video quality assessment comprehensive survey video quality assessment vqa important processing task aiming predicting quality video manner highly consistent human judgment perceived quality traditional vqa model based natural image andor video statistic inspired model projected image real world dual model human visual system deliver limited prediction performance realworld usergenerated content ugc exemplified recent largescale vqa database containing large number diverse video content crawled web fortunately recent advance deep neural network large multimodality model lmms enabled significant progress solving problem yielding better result prior handcrafted model numerous deep learningbased vqa model developed progress direction driven creation contentdiverse largescale humanlabeled database supply ground truth psychometric video quality data present comprehensive survey recent progress development vqa algorithm benchmarking study database make possible also analyze open research direction study design vqa algorithm architecture github link httpsgithubcomtacogroupvideoqualityassessmentacomprehensivesurvey
bvicr multiview human dataset volumetric video compression advance immersive technology reconstruction enabled creation digital replica realworld object environment fine detail process generate vast amount data requiring efficient compression method satisfy memory bandwidth constraint associated data storage transmission however development validation efficient data compression method constrained lack comprehensive highquality volumetric video datasets typically require much effort acquire consume increased resource compared image video database bridge gap present open multiview volumetric human dataset denoted bvicr contains multiview rgbd capture corresponding textured polygonal mesh depicting range diverse human action video sequence contains view resolution duration second using bvicr benchmarked three conventional neural coordinatebased multiview video compression method following mpeg miv common test condition reported rate quality performance based various quality metric result show great potential neural representation based method volumetric video compression compared conventional video coding method average coding gain psnr dataset provides development validation platform variety task including volumetric reconstruction compression quality assessment database shared publicly urlhttpsgithubcomfanaaronzhangbvicr
lift leveraging human feedback texttovideo model alignment recent advance texttovideo generative model shown impressive capability however model still inadequate aligning synthesized video human preference eg accurately reflecting text description particularly difficult address human preference subjective challenging formalize objective function existing study train video quality assessment model rely humanannotated rating video evaluation overlook reasoning behind evaluation limiting ability capture nuanced human criterion moreover aligning model using videobased human feedback remains unexplored therefore paper proposes lift first method designed leverage human feedback model alignment specifically first construct human rating annotation dataset lifthra consisting approximately human annotation including score corresponding rationale based train reward model liftcritic learn reward function effectively serf proxy human judgment measuring alignment given video human expectation lastly leverage learned reward function align model maximizing rewardweighted likelihood case study apply pipeline showing finetuned model outperforms across metric highlighting potential human feedback improving alignment quality synthesized video
deblurgs gaussian splatting camera motion blur although significant progress made reconstructing sharp scene motionblurred image transition realworld application remains challenging primary obstacle stem severe blur lead inaccuracy acquisition initial camera pose structurefrommotion critical aspect often overlooked previous approach address challenge propose deblurgs method optimize sharp gaussian splatting motionblurred image even noisy camera pose initialization restore finegrained sharp scene leveraging remarkable reconstruction capability gaussian splatting approach estimate camera motion blurry observation synthesizes corresponding blurry rendering optimization process furthermore propose gaussian densification annealing strategy prevent generation inaccurate gaussians erroneous location early training stage camera motion still imprecise comprehensive experiment demonstrate deblurgs achieves stateoftheart performance deblurring novel view synthesis realworld synthetic benchmark datasets well fieldcaptured blurry smartphone video
pose estimation camera image underwater inspection highprecision localization pivotal underwater reinspection mission traditional localization method like inertial navigation system doppler velocity logger acoustic positioning face significant challenge costeffective application visual localization costeffective alternative case leveraging camera already equipped inspection vehicle estimate pose image surrounding scene amongst machine learningbased pose estimation image show promise underwater environment performing efficient relocalization using model trained based previously mapped scene explore efficacy learningbased pose estimator clear turbid water inspection mission assessing impact image format model architecture training data diversity innovate employing novel view synthesis model generate augmented training data significantly enhancing pose estimation unexplored region moreover enhance localization accuracy integrating pose estimator output sensor data via extended kalman filter demonstrating improved trajectory smoothness accuracy
videoguide improving video diffusion model without training teacher guide texttoimage diffusion model revolutionized visual content creation extending capability texttovideo generation remains challenge particularly preserving temporal consistency existing method aim improve consistency often cause tradeoff reduced imaging quality impractical computational time address issue introduce videoguide novel framework enhances temporal consistency pretrained model without need additional training finetuning instead videoguide leverage pretrained video diffusion model vdm guide early stage inference improving temporal quality interpolating guiding model denoised sample sampling model denoising process proposed method brings significant improvement temporal consistency image fidelity providing costeffective practical solution synergizes strength various video diffusion model furthermore demonstrate prior distillation revealing base model achieve enhanced text coherence utilizing superior data prior guiding model proposed method project page
ctnerf crosstime transformer dynamic neural radiance field monocular video goal work generate highquality novel view monocular video complex dynamic scene prior method dynamicnerf shown impressive performance leveraging timevarying dynamic radiation field however method limitation come accurately modeling motion complex object lead inaccurate blurry rendering detail address limitation propose novel approach build upon recent generalization nerf aggregate nearby view onto new viewpoint however method typically effective static scene overcome challenge introduce module operates time frequency domain aggregate feature object motion allows u learn relationship frame generate higherquality image experiment demonstrate significant improvement stateoftheart method dynamic scene datasets specifically approach outperforms existing method term accuracy visual quality synthesized view code available
medical video generation disease progression simulation modeling disease progression crucial improving quality efficacy clinical diagnosis prognosis often hindered lack longitudinal medical image monitoring individual patient address challenge propose first medical video generation mvg framework enables controlled manipulation diseaserelated image video feature allowing precise realistic personalized simulation disease progression approach begin leveraging large language model llm recaption prompt disease trajectory next controllable multiround diffusion model simulates disease progression state patient creating realistic intermediate disease state sequence finally diffusionbased video transition generation model interpolates disease progression state validate framework across three medical imaging domain chest xray fundus photography skin image result demonstrate mvg significantly outperforms baseline model generating coherent clinically plausible disease trajectory two user study veteran physician provide validation insight clinical utility generated sequence mvg potential assist healthcare provider modeling disease trajectory interpolating missing medical image data enhancing medical education realistic dynamic visualization disease progression
liveportrait efficient portrait animation stitching retargeting control portrait animation aim synthesize lifelike video single source image using appearance reference motion ie facial expression head pose derived driving video audio text generation instead following mainstream diffusionbased method explore extend potential implicitkeypointbased framework effectively balance computational efficiency controllability building upon develop videodriven portrait animation framework named liveportrait focus better generalization controllability efficiency practical usage enhance generation quality generalization ability scale training data million highquality frame adopt mixed imagevideo training strategy upgrade network architecture design better motion transformation optimization objective additionally discover compact implicit keypoints effectively represent kind blendshapes meticulously propose stitching two retargeting module utilize small mlp negligible computational overhead enhance controllability experimental result demonstrate efficacy framework even compared diffusionbased method generation speed remarkably reach rtx gpu pytorch inference code model available httpsgithubcomkwaivgiliveportrait
portraittalk towards customizable oneshot audiototalking face generation audiodriven talking face generation challenging task digital communication despite significant progress area existing method concentrate audiolip synchronization often overlooking aspect visual quality customization generalization crucial producing realistic talking face address limitation introduce novel customizable oneshot audiodriven talking face generation framework named portraittalk proposed method utilizes latent diffusion framework consisting two main component identitynet animatenet identitynet designed preserve identity feature consistently across generated video frame animatenet aim enhance temporal coherence motion consistency framework also integrates audio input reference image thereby reducing reliance referencestyle video prevalent existing approach key innovation portraittalk incorporation text prompt decoupled crossattention mechanism significantly expands creative control generated video extensive experiment including newly developed evaluation metric model demonstrates superior performance stateoftheart method setting new standard generation customizable realistic talking face suitable realworld application
neuralnetworkenhanced metalens camera highdefinition dynamic imaging longwave infrared spectrum provide lightweight costeffective solution longwave infrared imaging using singlet develop camera integrating highfrequencyenhancing cyclegan neural network metalens imaging system highfrequencyenhancing cyclegan improves quality original metalens image addressing inherent frequency loss introduced metalens addition bidirectional cyclic generative adversarial network incorporates highfrequency adversarial learning module module utilizes wavelet transform extract highfrequency component establishes highfrequency feedback loop enables generator enhance camera output integrating adversarial feedback highfrequency discriminator ensures generator adheres constraint imposed highfrequency adversarial loss thereby effectively recovering camera frequency loss recovery guarantee highfidelity image output camera facilitating smooth video production camera capable achieving dynamic imaging frame per second end point error value also achieve frechet inception distance peak signal noise ratio structural similarity recorded video
pointvos pointing video object segmentation current stateoftheart video object segmentation vos method rely dense perobject mask annotation training testing requires timeconsuming costly video annotation mechanism propose novel pointvos task spatiotemporally sparse pointwise annotation scheme substantially reduces annotation effort apply annotation scheme two largescale video datasets text description annotate point across object video based annotation propose new pointvos benchmark corresponding pointbased training mechanism use establish strong baseline result show existing vos method easily adapted leverage point annotation training achieve result close fullysupervised performance trained pseudomasks generated point addition show data used improve model connect vision language evaluating video narrative grounding vng task make code annotation available httpspointvosgithubio
dont judge look towards motion coherent video representation current training pipeline object recognition neglect hue jittering data augmentation brings appearance change detrimental classification also implementation inefficient practice study investigate effect hue variance context video understanding find variance beneficial since static appearance less important video contain motion information based observation propose data augmentation method video understanding named motion coherent augmentation mca introduces appearance variation video implicitly encourages model prioritize motion pattern rather static appearance concretely propose operation swapmix efficiently modify appearance video sample introduce variation alignment va resolve distribution shift caused swapmix enforcing model learn appearance invariant representation comprehensive empirical evaluation across various architecture different datasets solidly validates effectiveness generalization ability mca application va augmentation method code available httpsgithubcombespontaneousmcapytorch
videoagent memoryaugmented multimodal agent video understanding explore reconciling several foundation model large language model visionlanguage model novel unified memory mechanism could tackle challenging video understanding problem especially capturing longterm temporal relation lengthy video particular proposed multimodal agent videoagent construct structured memory store generic temporal event description objectcentric tracking state video given input task query employ tool including video segment localization object memory querying along visual foundation model interactively solve task utilizing zeroshot tooluse ability llm videoagent demonstrates impressive performance several longhorizon video understanding benchmark average increase nextqa egoschema baseline closing gap opensourced model private counterpart including gemini pro
adaptive cooperative streaming holographic video wireless network proximal policy optimization solution adapting holographic video streaming fluctuating wireless channel essential maintain consistent satisfactory quality experience qoe user however challenging task due dynamic uncertain characteristic wireless network address issue propose holographic video cooperative streaming framework designed generic wireless network multiple access point cooperatively transmit video different bitrates multiple user additionally model novel qoe metric tailored specifically holographic video streaming effectively encapsulate nuance holographic video quality quality fluctuation rebuffering occurrence simultaneously furthermore formulate formidable qoe maximization problem nonconvex mixed integer nonlinear programming problem using proximal policy optimization ppo new class reinforcement learning algorithm devise joint beamforming bitrate control scheme wisely adapted fluctuation wireless channel numerical result demonstrate superiority proposed scheme representative baseline
microemo timesensitive multimodal emotion recognition microexpression dynamic video dialogue multimodal large language model mllms demonstrated remarkable multimodal emotion recognition capability integrating multimodal cue visual acoustic linguistic context video recognize human emotional state however existing method ignore capturing local facial feature temporal dynamic microexpressions leverage contextual dependency utteranceaware temporal segment video thereby limiting expected effectiveness certain extent work propose microemo timesensitive mllm aimed directing attention local facial microexpression dynamic contextual dependency utteranceaware video clip model incorporates two key architectural contribution globallocal attention visual encoder integrates global framelevel timestampbound image feature local facial feature temporal dynamic microexpressions utteranceaware video qformer capture multiscale contextual dependency generating visual token sequence utterance segment entire video combining preliminary qualitative experiment demonstrate new explainable multimodal emotion recognition emer task exploit multimodal multifaceted clue predict emotion openvocabulary ov manner microemo demonstrates effectiveness compared latest method
kalmaninspired feature propagation video face superresolution despite promising progress face image superresolution video face superresolution remains relatively underexplored existing approach either adapt general video superresolution network face datasets apply established face image superresolution model independently individual video frame paradigm encounter challenge either reconstructing facial detail maintaining temporal consistency address issue introduce novel framework called kalmaninspired feature propagation keep designed maintain stable face prior time kalman filtering principle offer method recurrent ability use information previously restored frame guide regulate restoration process current frame extensive experiment demonstrate effectiveness method capturing facial detail consistently across video frame code video demo available httpsjnjabygithubioprojectskeep
momentcross nextgeneration realtime crossdomain ctr prediction livestreaming recommendation kuaishou kuaishou one largest shortvideo livestreaming platform compared shortvideo recommendation livestreaming recommendation complex temporarilyalive distribution user may watch long time feedback delay content unpredictable change time actually even user interested livestreaming author still may negative watching eg shortview since realtime content attractive enough therefore livestreaming recommendation exists challenging task recommend livestreaming right moment user additionally platform major exposure content short shortvideo amount exposed shortvideo exposed livestreaming thus user leave behavior shortvideos lead serious data imbalance problem making livestreaming data could fully reflect user interest case raise another challenging task utilize user shortvideo behavior make livestreaming recommendation better
omniclip adapting clip video recognition spatialtemporal omniscale feature learning recent visionlanguage model vlms textiteg clip made great progress video recognition despite improvement brought strong visual backbone extracting spatial feature clip still fall short capturing integrating spatialtemporal feature essential video recognition paper propose omniclip framework adapts clip video recognition focusing learning comprehensive feature encompassing spatial temporal dynamic spatialtemporal scale refer omniscale feature achieved design spatialtemporal block include parallel temporal adapter pta enabling efficient temporal modeling additionally introduce selfprompt generator spg module capture dynamic object spatial feature synergy pta spg allows omniclip discern varying spatial information across frame assess object scale time conducted extensive experiment supervised video recognition fewshot video recognition zeroshot recognition task result demonstrate effectiveness method especially omniclip achieving accuracy setting surpassing recent motionprompt approach even full training data code available urlhttpsgithubcomxiaobulomniclip
pite pixeltemporal alignment large videolanguage model fueled large language model llm wave large visuallanguage model lvlms emerged pivotal advancement bridging gap image text however video making challenging lvlms perform adequately due complexity relationship language spatialtemporal data structure recent large videolanguage model lvidlms align feature static visual data like image latent space language feature general multimodal task leverage ability llm sufficiently paper explore finegrained alignment approach via object trajectory different modality across spatial temporal dimension simultaneously thus propose novel lvidlm trajectoryguided pixeltemporal alignment dubbed pite exhibit promising applicable model property achieve finegrained videolanguage alignment curate multimodal pretraining dataset dataset provision moving trajectory pixel level individual object appear mention video caption automatic annotation pipeline meanwhile pite demonstrates astounding capability myriad videorelated multimodal task beat stateoftheart method large margin
dgns deformable gaussian splatting dynamic neural surface monocular dynamic reconstruction dynamic scene reconstruction monocular video critical realworld application paper tackle dual challenge dynamic novelview synthesis geometry reconstruction introducing hybrid framework deformable gaussian splatting dynamic neural surface dgns module leverage task training depth map generated deformable gaussian splatting module guide ray sampling faster processing provide depth supervision within dynamic neural surface module improve geometry reconstruction simultaneously dynamic neural surface directs distribution gaussian primitive around surface enhancing rendering quality refine depth supervision introduce depthfiltering process depth map derived gaussian rasterization extensive experiment public datasets demonstrate dgns achieves stateoftheart performance novelview synthesis reconstruction
instantdrag improving interactivity dragbased image editing dragbased image editing recently gained popularity interactivity precision however despite ability texttoimage model generate sample within second drag editing still lag behind due challenge accurately reflecting user interaction maintaining image content existing approach rely computationally intensive perimage optimization intricate guidancebased method requiring additional input mask movable region text prompt thereby compromising interactivity editing process introduce instantdrag optimizationfree pipeline enhances interactivity speed requiring image drag instruction input instantdrag consists two carefully designed network dragconditioned optical flow generator flowgen optical flowconditioned diffusion model flowdiffusion instantdrag learns motion dynamic dragbased image editing realworld video datasets decomposing task motion generation motionconditioned image generation demonstrate instantdrags capability perform fast photorealistic edits without mask text prompt experiment facial video datasets general scene result highlight efficiency approach handling dragbased image editing making promising solution interactive realtime application
generative object insertion gaussian splatting multiview diffusion model generating inserting new object content compelling approach achieving versatile scene recreation existing method rely sd optimization singleview inpainting often struggle produce highquality result address propose novel method object insertion content represented gaussian splatting approach introduces multiview diffusion model dubbed mvinpainter built upon pretrained stable video diffusion model facilitate viewconsistent object inpainting within mvinpainter incorporate controlnetbased conditional injection module enable controlled predictable multiview generation generating multiview inpainted result propose maskaware reconstruction technique refine gaussian splatting reconstruction sparse inpainted view leveraging fabricate technique approach yield diverse result ensures viewconsistent harmonious insertion produce better object quality extensive experiment demonstrate approach outperforms existing method
highfidelity talking portrait synthesis via personalized generative prior recent method audiodriven talking head synthesis often optimize neural radiance field nerf monocular talking portrait video leveraging capability render highfidelity novelview frame however often struggle reconstruct complete face geometry due absence comprehensive information input monocular video paper introduce novel audiodriven talking head synthesis framework called faithfully reconstruct plausible facial geometry effectively adopting pretrained generative prior given personalized generative model present novel audioguided attention unet architecture predicts dynamic face variation nerf space driven audio furthermore model modulated audiounrelated conditioning token effectively disentangle variation unrelated audio feature compared existing method method excels generating realistic facial geometry even extreme head pose also conduct extensive experiment showing approach surpasses stateoftheart benchmark term quantitative qualitative evaluation
direct mesh supervision neural radiance field representation generation present approach derive groundtruth radiance field textured mesh generation task many generative approach represent scene radiance field training groundtruth radiance field usually fitted multiview rendering largescale synthetic dataset often result artifact due occlusion underfitting issue propose analytic solution directly obtain groundtruth radiance field mesh characterizing density field occupancy function featuring defined surface thickness determining viewdependent color reflection function considering mesh environment lighting extract accurate radiance field provides direct supervision training generative nerfs single scene representation validate effectiveness across various task achieving noteworthy improvement psnr view synthesis single scene representation abo dataset psnr enhancement singleview conditional generation shapenet car notably improved mesh extraction nerf unconditional generation objaverse mug
panoramic generation resolution blooming virtual reality augmented reality vrar technology driven increasing demand creation highquality immersive dynamic environment however existing generative technique either focus solely dynamic object perform outpainting single perspective image failing meet requirement vrar application need freeviewpoint virtual view user move direction work tackle challenging task elevating single panorama immersive experience first time demonstrate capability generate omnidirectional dynamic scene view time resolution thereby providing immersive user experience method introduces pipeline facilitates natural scene animation optimizes set dynamic gaussians using efficient splatting technique realtime exploration overcome lack scenescale annotated data model especially panoramic format propose novel textbfpanoramic denoiser adapts generic diffusion prior animate consistently image transforming panoramic video dynamic scene targeted region subsequently propose textbfdynamic panoramic lifting elevate panoramic video immersive environment preserving spatial temporal consistency transferring prior knowledge model perspective domain panoramic domain lifting spatial appearance geometry regularization achieve highquality generation resolution first time
exploiting topological prior boosting point cloud generation paper present innovative enhancement sphere prior generative adversarial network spgan model stateoftheart gan designed point cloud generation novel method introduced point cloud generation elevates structural integrity overall quality generated point cloud incorporating topological prior training process generator specifically work utilizes kmeans algorithm segment point cloud repository cluster extract centroid used prior generation process spgan furthermore discriminator component spgan utilizes identical point cloud contributed centroid ensuring coherent consistent learning environment strategic use centroid intuitive guide boost efficiency global feature learning also substantially improves structural coherence fidelity generated point cloud applying kmeans algorithm generate centroid prior work intuitively experimentally demonstrates prior enhances quality generated point cloud
motionadaptive inference flexible learned bframe compression performance recent learned intra sequential video compression model exceed respective traditional codecs performance learned bframe compression model generally lag behind traditional bframe coding performance gap bigger complex scene large motion related fact distance past future reference vary hierarchical bframe compression depending level hierarchy cause motion range vary inability single bframe compression model adapt various motion range cause loss performance remedy propose controlling motion range flow prediction inference approximately match range motion training data downsampling video frame adaptively according amount motion level hierarchy order compress bframes using single flexiblerate model present stateoftheart bd rate result demonstrate superiority proposed singlemodel motionadaptive inference approach existing learned bframe compression model
fastcad realtime cad retrieval alignment scan video digitising world clean cad modelbased representation important application augmented reality robotics current stateoftheart method computationally intensive individually encode detected object optimise cad alignment second stage work propose fastcad realtime method simultaneously retrieves aligns cad model object given scene contrast previous work directly predict alignment parameter shape embeddings achieve highquality shape retrieval learning cad embeddings contrastive learning framework distilling fastcad singlestage method accelerates inference time factor compared method operating rgbd scan outperforming challenging alignment benchmark approach collaborates seamlessly online reconstruction technique enables realtime generation precise cad modelbased reconstruction video fps significantly improve alignment accuracy video setting reconstruction accuracy
deep understanding soccer match video soccer one popular sport worldwide live broadcast frequently available major match however extracting detailed framebyframe information player action video remains challenge utilizing stateoftheart computer vision technology system detect key object soccer ball player referee also track movement player ball recognizes player number classifies scene identifies highlight goal kick analyzing live tv stream soccer match system generate highlight gifs tactical illustration diverse summary graph ongoing game visual recognition technique deliver comprehensive understanding soccer game video enriching viewer experience detailed insightful analysis
umotion learned point cloud video compression ustructured temporal context generation point cloud video pcv versatile representation dynamic scene emerging application paper introduces umotion learningbased compression scheme pcv geometry attribute propose ustructured interframe prediction framework uinter performs explicit motion estimation compensation memc different scale varying level detail integrates topdown finetocoarse motion propagation bottomup motion predictive coding multiscale group motion compensation enable accurate motion estimation efficient motion compression scale addition design multiscale spatialtemporal predictive coding module capture crossscale spatial redundancy remaining uinter prediction conduct experiment following mpeg common test condition dense dynamic point cloud demonstrate umotion achieve significant gain mpeg gpccgestm recently published learningbased method geometry attribute compression
relocate simple trainingfree baseline visual query localization using regionbased representation present relocate simple trainingfree baseline designed perform challenging task visual query localization long video eliminate need taskspecific training efficiently handle long video relocate leverage regionbased representation derived pretrained vision model high level follows classic object localization approach identify object video frame compare object given query select similar one perform bidirectional tracking get spatiotemporal response however propose key enhancement handle small object cluttered scene partial visibility varying appearance notably refine selected object accurate localization generate additional visual query capture visual variation evaluate relocate challenging visual query localization dataset establishing new baseline outperforms prior taskspecific method relative improvement spatiotemporal average precision
learning selfsupervised audiovisual representation sound recommendation propose novel selfsupervised approach learning audio visual representation unlabeled video based correspondence approach us attention mechanism learn relative importance convolutional feature extracted different resolution audio visual stream us attention feature encode audio visual input based correspondence evaluated representation learned model classify audiovisual correlation well recommend sound effect visual scene result show representation generated attention model improves correlation accuracy compared baseline recommendation accuracy vggsound public video dataset additionally audiovisual representation learned training attention model crossmodal contrastive learning improves recommendation performance based evaluation using vggsound challenging dataset consisting gameplay video recording
stylerfvolvis style transfer neural radiance field expressive volume visualization volume visualization visualization synthesis attracted much attention due ability generate novel visualization without following conventional rendering pipeline however existing solution based generative adversarial network often require many training image take significant training time still issue low quality consistency flexibility persist paper introduces stylerfvolvis innovative style transfer framework expressive volume visualization volvis via neural radiance field nerf expressiveness stylerfvolvis upheld ability accurately separate underlying scene geometry ie content color appearance ie style conveniently modify color opacity lighting original rendering maintaining visual content consistency across view effectively transfer arbitrary style reference image reconstructed scene achieve design base nerf model scene geometry extraction palette color network classify region radiance field photorealistic editing unrestricted color network lift color palette constraint via knowledge distillation nonphotorealistic editing demonstrate superior quality consistency flexibility stylerfvolvis experimenting various volume rendering scene reference image comparing stylerfvolvis imagebased adain videobased rerevst nerfbased arf snerf style rendering solution
float generative motion latent flow matching audiodriven talking portrait rapid advancement diffusionbased generative model portrait image animation achieved remarkable result however still face challenge temporally consistent video generation fast sampling due iterative sampling nature paper present float audiodriven talking portrait video generation method based flow matching generative model shift generative modeling pixelbased latent space learned motion latent space enabling efficient design temporally consistent motion achieve introduce transformerbased vector field predictor simple yet effective framewise conditioning mechanism additionally method support speechdriven emotion enhancement enabling natural incorporation expressive motion extensive experiment demonstrate method outperforms stateoftheart audiodriven talking portrait method term visual quality motion fidelity efficiency
gim learning generalizable image matcher internet video image matching fundamental computer vision problem learningbased method achieve stateoftheart performance existing benchmark generalize poorly inthewild image method typically need train separate model different scene type impractical scene type unknown advance one underlying problem limited scalability existing data construction pipeline limit diversity standard image matching datasets address problem propose gim selftraining framework learning single generalizable model based image matching architecture using internet video abundant diverse data source given architecture gim first train standard domainspecific datasets combine complementary matching method create dense label nearby frame novel video label filtered robust fitting enhanced propagating distant frame final model trained propagated data strong augmentation also propose zeb first zeroshot evaluation benchmark image matching mixing data diverse domain zeb thoroughly assess crossdomain generalization performance different method applying gim consistently improves zeroshot performance stateoftheart image matching architecture hour youtube video relative zeroshot performance improves gim also enables generalization extreme crossdomain data bird eye view bev image projected point cloud fig importantly single zeroshot model consistently outperforms domainspecific baseline evaluated downstream task inherent respective domain video presentation available
animdirector large multimodal model powered agent controllable animation video generation traditional animation generation method depend training generative model humanlabelled data entailing sophisticated multistage pipeline demand substantial human effort incurs high training cost due limited prompting plan method typically produce brief informationpoor contextincoherent animation overcome limitation automate animation process pioneer introduction large multimodal model lmms core processor build autonomous animationmaking agent named animdirector agent mainly harness advanced understanding reasoning capability lmms generative ai tool create animated video concise narrative simple instruction specifically operates three main stage firstly animdirector generates coherent storyline user input followed detailed director script encompasses setting character profile interiorexterior description contextcoherent scene description include appearing character interior exterior scene event secondly employ lmms image generation tool produce visual image setting scene image designed maintain visual consistency across different scene using visuallanguage prompting method combine scene description image appearing character setting thirdly scene image serve foundation producing animated video lmms generating prompt guide process whole process notably autonomous without manual intervention lmms interact seamlessly generative tool generate prompt evaluate visual quality select best one optimize final output
selfsupervised video desmoking laparoscopic surgery due difficulty collecting real paired data existing desmoking method train model synthesizing smoke generalizing poorly real surgical scenario although work explored singleimage realworld desmoking unpaired learning manner still encounter challenge handling dense smoke work address issue together introducing selfsupervised surgery video desmoking selfsvd one hand observe frame captured activation highenergy device generally clear named presmoke frame p frame thus serve supervision smoky frame making realworld selfsupervised video desmoking practically feasible hand order enhance desmoking performance feed valuable information p frame model masking strategy regularization term presented avoid trivial solution addition construct real surgery video dataset desmoking cover variety smoky scene extensive experiment dataset show selfsvd remove smoke effectively efficiently recovering photorealistic detail stateoftheart method dataset code pretrained model available urlhttpsgithubcomzcsrenlongzselfsvd
vrdone onestage video visual relation detection video visual relation detection vidvrd focus understanding entity interact time space video key step gaining deeper insight video scene beyond basic visual task traditional method vidvrd challenged complexity typically split task two part one identifying relation category present another determining temporal boundary split overlook inherent connection element addressing need recognize entity pair spatiotemporal interaction across range duration propose vrdone streamlined yet efficacious onestage model vrdone combine feature subject object turning predicate detection instance segmentation combined representation setup allows relation category identification binary mask generation one go eliminating need extra step like proposal generation postprocessing vrdone facilitates interaction feature across various frame adeptly capturing shortlived enduring relation additionally introduce subjectobject synergy so module enhancing subject object perceive combining vrdone achieves stateoftheart performance vidor benchmark imagenetvidvrd showcasing superior capability discerning relation across different temporal scale code available
precise video camera control adjustable motion strength video generation technology developing rapidly broad potential application among technology camera control crucial generating professionalquality video accurately meet user expectation however existing camera control method still suffer several limitation including control precision neglect control subject motion dynamic work propose novel camera control method significantly enhances controllability providing adjustability strength subject motion improve control precision employ point trajectory camera coordinate system instead extrinsic matrix information control signal accurately control adjust strength subject motion explicitly model higherorder component video trajectory expansion merely linear term design operator effectively represents motion strength use adapter architecture independent base model structure experiment static dynamic scene show framework outperformances previous method quantitatively qualitatively project page
jointmotion mutual learning pose estimation video human pose estimation video long compelling yet challenging task within realm computer vision nevertheless task remains difficult complex video scene video defocus selfocclusion recent method strive integrate multiframe visual feature generated backbone network pose estimation however often ignore useful joint information encoded initial heatmap byproduct backbone generation comparatively method attempt refine initial heatmap fail consider spatiotemporal motion feature result performance existing method pose estimation fall short due lack ability leverage local joint heatmap information global motion feature dynamic address problem propose novel jointmotion mutual learning framework pose estimation effectively concentrate local joint dependency global pixellevel motion dynamic specifically introduce contextaware joint learner adaptively leverage initial heatmaps motion flow retrieve robust local joint feature given local joint feature global motion flow complementary propose progressive jointmotion mutual learning synergistically exchange information interactively learns joint feature motion flow improve capability model importantly capture diverse joint motion cue theoretically analyze propose information orthogonality objective avoid learning redundant information multicues empirical experiment show method outperforms prior art three challenging benchmark
comprehensive generative replay taskincremental segmentation concurrent appearance semantic forgetting generalist segmentation model increasingly favored diverse task involving various object different image source taskincremental learning til offer privacypreserving training paradigm using task arriving sequentially instead gathering due strict data sharing policy however task evolution span wide scope involves shift image appearance segmentation semantics intricate correlation causing concurrent appearance semantic forgetting solve issue propose comprehensive generative replay cgr framework restores appearance semantic knowledge synthesizing imagemask pair mimic past task data focus two aspect modeling imagemask correspondence promoting scalability diverse task specifically introduce novel bayesian joint diffusion bjd model highquality synthesis imagemask pair correspondence explicitly preserved conditional denoising furthermore develop taskoriented adapter toa recalibrates prompt embeddings modulate diffusion model making data synthesis compatible different task experiment incremental task cardiac fundus prostate segmentation show clear advantage alleviating concurrent appearance semantic forgetting code available httpsgithubcomjingyzhangcgr
lyric musicdriven dance synthesis dance typically involves professional choreography complex movement follow musical rhythm also influenced lyrical content integration lyric addition auditory dimension enriches foundational tone make motion generation amenable semantic meaning however existing dance synthesis method tend model motion conditioned audio signal work make two contribution bridge gap first propose novel probabilistic architecture incorporates multimodal diffusion model consistency distillation designed create dance conditioned music lyric one diffusion generation step second introduce first dancemotion dataset encompasses music lyric obtained pose estimation technology evaluate model musiconly baseline model objective metric human evaluation including dancer choreographer result demonstrate able produce realistic diverse dance matching lyric music video summary accessed
align step optimizing sampling schedule diffusion model diffusion model dm established stateoftheart generative modeling approach visual domain beyond crucial drawback dm slow sampling speed relying many sequential function evaluation large neural network sampling dm seen solving differential equation discretized set noise level known sampling schedule past work primarily focused deriving efficient solver little attention given finding optimal sampling schedule entire literature relies handcrafted heuristic work first time propose general principled approach optimizing sampling schedule dm highquality output called textitalign step leverage method stochastic calculus find optimal schedule specific different solver trained dm datasets evaluate novel approach several image video well toy data synthesis benchmark using variety different sampler observe optimized schedule outperform previous handcrafted schedule almost experiment method demonstrates untapped potential sampling schedule optimization especially fewstep synthesis regime
lmgaussian boost sparseview gaussian splatting large model prior aim address sparseview reconstruction scene leveraging prior largescale vision model recent advancement gaussian splatting demonstrated remarkable success reconstruction method typically necessitate hundred input image densely capture underlying scene making timeconsuming impractical realworld application however sparseview reconstruction inherently illposed underconstrained often resulting inferior incomplete outcome due issue failed initialization overfitting input image lack detail mitigate challenge introduce lmgaussian method capable generating highquality reconstruction limited number image specifically propose robust initialization module leverage stereo prior aid recovery camera pose reliable point cloud additionally diffusionbased refinement iteratively applied incorporate image diffusion prior gaussian optimization process preserve intricate scene detail finally utilize video diffusion prior enhance rendered image realistic visual effect overall approach significantly reduces data acquisition requirement compared previous method validate effectiveness framework experiment various public datasets demonstrating potential highquality scene reconstruction visual result website
incontext ensemble learning pseudo label improves videolanguage model lowlevel workflow understanding standard operating procedure sop defines lowlevel stepbystep written guide business software workflow sop generation crucial step towards automating endtoend software workflow manually creating sop timeconsuming recent advancement large videolanguage model offer potential automating sop generation analyzing recording human demonstration however current large videolanguage model face challenge zeroshot sop generation work first explore incontext learning videolanguage model sop generation propose explorationfocused strategy called incontext ensemble learning aggregate pseudo label multiple possible path sop proposed incontext ensemble learning well enables model learn beyond context window limit implicit consistency regularisation report incontext learning help videolanguage model generate temporally accurate sop proposed incontext ensemble learning consistently enhance capability videolanguage model sop generation
drivingworld constructing world model autonomous driving via video gpt recent success autoregressive ar generation model gpt series natural language processing motivated effort replicate success visual task work attempt extend approach autonomous driving building videobased world model capable generating realistic future video sequence predicting ego state however prior work tend produce unsatisfactory result classic gpt framework designed handle contextual information text lack inherent ability model spatial temporal dynamic essential video generation paper present drivingworld gptstyle world model autonomous driving featuring several spatialtemporal fusion mechanism design enables effective modeling spatial temporal dynamic facilitating highfidelity longduration video generation specifically propose nextstate prediction strategy model temporal coherence consecutive frame apply nexttoken prediction strategy capture spatial information within frame enhance generalization ability propose novel masking strategy reweighting strategy token prediction mitigate longterm drifting issue enable precise control work demonstrates ability produce highfidelity consistent video clip second duration time longer stateoftheart driving world model experiment show contrast prior work method achieves superior visual quality significantly accurate controllable future video generation code available httpsgithubcomyvanyindrivingworld
new lightweight hybrid graph convolutional neural network cnn scheme scene classification using object detection inference scene understanding play important role several highlevel computer vision application autonomous vehicle intelligent video surveillance robotics however solution proposed indooroutdoor scene classification ensure scene context adaptability computer vision framework propose first lightweight hybrid graph convolutional neural network lhgcnncnn framework addon object detection model proposed approach us output cnn object detection model predict observed scene type generating coherent gcnn representing semantic geometric content observed scene new method applied natural scene achieves efficiency scene classification cocoderived dataset containing large number different scene requiring fewer parameter traditional cnn method benefit scientific community make source code publicly available httpsgithubcomaymanbeghhybridgcnncnn
pancreatic tumor segmentation anomaly detection ct image using denoising diffusion model despite advance medicine cancer remained formidable challenge particularly case pancreatic tumor characterized diversity late diagnosis early detection pose significant challenge crucial effective treatment advancement deep learning technique particularly supervised algorithm significantly propelled pancreatic tumor detection medical field however supervised deep learning approach necessitate extensive labeled medical image training yet acquiring annotation limited costly conversely weakly supervised anomaly detection method requiring imagelevel annotation garnered interest existing methodology predominantly hinge generative adversarial network gans autoencoder model pose complexity training model may face difficulty accurately preserving fine image detail research present novel approach pancreatic tumor detection employing weak supervision anomaly detection denoising diffusion algorithm incorporating deterministic iterative process adding removing noise along classifier guidance method enables seamless translation image diseased healthy subject resulting detailed anomaly map without requiring complex training protocol segmentation mask study explores denoising diffusion model recent advancement traditional generative model like gans contributing field pancreatic tumor detection recognizing low survival rate pancreatic cancer study emphasizes need continued research leverage diffusion model efficiency medical segmentation task
general framework jersey number recognition sport video jersey number recognition important task sport video analysis partly due importance longterm player tracking viewed variant scene text recognition however lack published attempt apply scene text recognition model jersey number data introduce novel public jersey number recognition dataset hockey study scene text recognition method adapted problem address issue occlusion assess degree training one sport hockey generalized another soccer latter also consider jersey number recognition singleimage level aggregated across frame yield trackletlevel jersey number label demonstrate high performance image trackletlevel task achieving accuracy hockey image soccer tracklets code model data available httpsgithubcommkoshkinajerseynumberpipeline
learning thing move internet stereo video learning understand dynamic scene imagery crucial application ranging robotics scene reconstruction yet unlike problem largescale supervised training enabled rapid progress directly supervising method recovering motion remains challenging due fundamental difficulty obtaining ground truth annotation present system mining highquality reconstruction internet stereoscopic wideangle video system fuse filter output camera pose estimation stereo depth estimation temporal tracking method highquality dynamic reconstruction use method generate largescale data form worldconsistent pseudometric point cloud longterm motion trajectory demonstrate utility data training variant predict structure motion realworld image pair showing training reconstructed data enables generalization diverse realworld scene project page
nonadversarial learning vectorquantized common latent space multisequence mri adversarial learning help generative model translate mri source target sequence lacking paired sample however implementing mri synthesis adversarial learning clinical setting challenging due training instability mode collapse address issue leverage intermediate sequence estimate common latent space among multisequence mri enabling reconstruction distinct sequence common latent space propose generative model compress discrete representation sequence estimate gaussian distribution vectorquantized common vqc latent space multiple sequence moreover improve latent space consistency contrastive learning increase model stability domain augmentation experiment using dataset show nonadversarial model outperforms ganbased method vqc latent space aid model achieve antiinterference ability eliminate effect noise bias field artifact solid semantic representation ability potential oneshot segmentation code publicly available
cwdm conditional wavelet diffusion model crossmodality medical image synthesis paper contributes brat brain mr image synthesis challenge present conditional wavelet diffusion model cwdm directly solving paired imagetoimage translation task highresolution volume deep learningbased brain tumor segmentation model demonstrated clear clinical utility typically require mr scan various modality flair input however due time constraint imaging artifact modality may missing hindering application wellperforming segmentation algorithm clinical routine address issue propose method synthesizes one missing modality image conditioned three available image enabling application downstream segmentation model treat paired imagetoimage translation task conditional generation problem solve combining wavelet diffusion model highresolution image synthesis simple conditioning strategy approach allows u directly apply model fullresolution volume avoiding artifact caused slice patchwise data processing work focus specific application presented method applied kind paired imagetoimage translation problem ct leftrightarrow mr mr leftrightarrow pet translation maskconditioned anatomically guided image generation
geometric generative model based morphological equivariant pdes gans content image generation consist creating generating data noisy information extracting specific feature texture edge thin image structure interested generative model two main problem addressed firstly improvement specific feature extraction accounting multiscale level intrinsic geometric feature secondly equivariance network reduce complexity provide geometric interpretability proceed propose geometric generative model based equivariant partial differential equation pde group convolution neural network gcnns called pdegcnns built morphology operator generative adversarial network gans equivariant morphological pde layer composed multiscale dilation erosion formulated riemannian manifold group symmetry defined lie group take advantage lie group structure properly integrate equivariance layer able use riemannian metric solve multiscale morphological operation point lie group associated unique point manifold help u derive metric riemannian manifold tensor field invariant lie group induced metric symmetry proposed geometric morphological gan gmgan obtained using proposed morphological equivariant convolution pdegcnns bring nonlinearity classical cnns gmgan evaluated mnist data compared gans preliminary result show gmgan model outperforms classical gan
garmentdreamer guided garment synthesis diverse geometry texture detail traditional garment creation laborintensive involving sketching modeling uv mapping texturing timeconsuming costly recent advance diffusionbased generative model enabled new possibility garment generation text prompt image video however existing method either suffer inconsistency among multiview image require additional process separate cloth underlying human model paper propose garmentdreamer novel method leverage gaussian splatting g guidance generate wearable simulationready garment mesh text prompt contrast using multiview image directly predicted generative model guidance guidance ensures consistent optimization garment deformation texture synthesis method introduces novel garment augmentation module guided normal rgba information employ implicit neural texture field netf combined score distillation sampling sd generate diverse geometric texture detail validate effectiveness approach comprehensive qualitative quantitative experiment showcasing superior performance garmentdreamer stateoftheart alternative project page available httpsxuanligithubiogarmentdreamerdemo
smoothfoley creating continuous sound videotoaudio generation semantic guidance videotoaudio generation task drawn attention field multimedia due practicality producing foley sound semantic temporal condition fed generation model indicate sound event temporal occurrence recent study synthesizing immersive synchronized audio faced challenge video moving visual presence temporal condition accurate enough lowresolution semantic condition exacerbates problem tackle challenge propose smoothfoley generative model taking semantic guidance textual label across generation enhance semantic temporal alignment audio two adapter trained leverage pretrained texttoaudio generation model frame adapter integrates highresolution framewise video feature temporal adapter integrates temporal condition obtained similarity visual frame textual label incorporation semantic guidance textual label achieves precise audiovideo alignment conduct extensive quantitative qualitative experiment result show smoothfoley performs better existing model continuous sound scenario general scenario semantic guidance audio generated smoothfoley exhibit higher quality better adherence physical law
make actor talk generalizable highfidelity lip sync motion appearance disentanglement aim edit lip movement talking video according given speech preserving personal identity visual detail task decomposed two subproblems speechdriven lip motion generation visual appearance synthesis current solution handle two subproblems within single generative model resulting challenging tradeoff lipsync quality visual detail preservation instead propose disentangle motion appearance generate one one speechtomotion diffusion model motionconditioned appearance generation model however still remain challenge stage motionaware identity preservation visual detail preservation therefore preserve personal identity adopt landmark represent motion employ landmarkbased identity loss capture motionagnostic visual detail use separate encoders encode lip nonlip appearance motion integrate learned fusion module train mytalk largescale diverse dataset experiment show method generalizes well unknown even outofdomain person term lip sync visual detail preservation encourage reader watch video project page
motionguided diffusion gif generation present motionguided diffusion model imagetogif video generation tackle problem differently formulating task image translation problem steered text motion magnitude prompt shown teaser fig ensure model adheres motion guidance propose new motionguided warping module spatially transform feature source image conditioned two type prompt furthermore introduce perceptual loss ensure transformed feature map remains within space target image ensuring content consistency coherence preparation model training meticulously curated data extracting coherent image frame tgif videocaption dataset provides rich information temporal change subject pretraining apply model zeroshot manner number video datasets extensive qualitative quantitative experiment demonstrate effectiveness model capture semantic prompt text also spatial one motion guidance train model using single node gpus code dataset model made public
legopet hierarchical feature guided conditional diffusion pet image reconstruction positron emission tomography pet widely utilized cancer detection due ability visualize functional biological process vivo pet image usually reconstructed histogrammed raw data sinograms using traditional iterative technique eg osem mlem recently deep learning dl method shown promise directly mapping raw sinogram data pet image however dl approach regressionbased ganbased often produce overly smoothed image introduce various artifact respectively imageconditioned diffusion probabilistic model cdpms another class likelihoodbased dl technique capable generating highly realistic controllable image cdpms notable strength still face challenge maintain correspondence consistency input output image different domain eg sinogram v image domain well slow convergence rate address limitation introduce legopet hierarchical feature guided conditional diffusion model highperceptual quality pet image reconstruction sinograms conducted several experiment demonstrating legopet improves performance cdpms also surpasses recent dlbased pet image reconstruction technique term visual quality pixellevel psnrssim metric code available httpsgithubcomyransunlegopet
generation detection sign language deepfakes linguistic visual analysis research explores positive application deepfake technology upper body generation specifically sign language deaf hard hearing dhoh community given complexity sign language scarcity expert generated video vetted sign language expert accuracy construct reliable deepfake dataset evaluating technical visual credibility using computer vision natural language processing model dataset consisting video featuring seen unseen individual also used detect deepfake video targeting vulnerable individual expert annotation confirm generated video comparable real sign language content linguistic analysis using textual similarity score interpreter evaluation show interpretation generated video least similar authentic sign language visual analysis demonstrates convincingly realistic deepfakes produced even new subject using posestyle transfer model pay close attention detail ensuring hand movement accurate align driving video also apply machine learning algorithm establish baseline deepfake detection dataset contributing detection fraudulent sign language video
text prompt normality guidance weakly supervised video anomaly detection weakly supervised video anomaly detection wsvad challenging task generating finegrained pseudolabels based weaklabel selftraining classifier currently promising solution however since existing method use rgb visual modality utilization category text information neglected thus limiting generation accurate pseudolabels affecting performance selftraining inspired manual labeling process based event description paper propose novel pseudolabel generation selftraining framework based text prompt normality guidance tpwng wsvad idea transfer rich languagevisual knowledge contrastive languageimage pretraining clip model aligning video event description text corresponding video frame generate pseudolabels specifically first finetune clip domain adaptation designing two ranking loss distributional inconsistency loss propose learnable text prompt mechanism assist normality visual prompt improve matching accuracy video event description text video frame design pseudolabel generation module based normality guidance infer reliable framelevel pseudolabels finally introduce temporal context selfadaptive learning module learn temporal dependency different video event flexibly accurately extensive experiment show method achieves stateoftheart performance two benchmark datasets ucfcrime xdviole
narrativebridge enhancing video captioning causaltemporal narrative existing video captioning benchmark model lack causaltemporal narrative sequence event linked cause effect unfolding time driven character agent lack narrative restricts model ability generate text description capture causal temporal dynamic inherent video content address gap propose narrativebridge approach comprising novel causaltemporal narrative ctn caption benchmark generated using large language model fewshot prompting explicitly encoding causeeffect temporal relationship video description causeeffect network cen separate encoders capturing cause effect dynamic enabling effective learning generation caption causaltemporal narrative extensive experiment demonstrate cen significantly outperforms stateoftheart model articulating causal temporal aspect video content cider msvdctn msrvttctn datasets respectively crossdataset evaluation showcase cens strong generalization capability proposed framework understands generates nuanced text description intricate causaltemporal narrative structure present video addressing critical limitation video captioning project detail visit httpsnarrativebridgegithubio
implicit locationcaption alignment via complementary masking weaklysupervised dense video captioning weaklysupervised dense video captioning wsdvc aim localize describe event interest video without requiring annotation event boundary setting pose great challenge accurately locating temporal location event relevant supervision unavailable existing method rely explicit alignment constraint event location caption involve complex event proposal procedure training inference tackle problem propose novel implicit locationcaption alignment paradigm complementary masking simplifies complex event proposal localization process maintaining effectiveness specifically model comprises two component dualmode video captioning module mask generation module dualmode video captioning module capture global event information generates descriptive caption mask generation module generates differentiable positive negative mask localizing event mask enable implicit alignment event location caption ensuring caption generated positively negatively masked video complementary thereby forming complete video description way even weak supervision event location event caption aligned implicitly extensive experiment public datasets demonstrate method outperforms existing weaklysupervised method achieves competitive result compared fullysupervised method
semantics guided disentangled gan chest xray image rib segmentation label annotation chest xray image rib segmentation time consuming laborious labeling quality heavily relies medical knowledge annotator reduce dependency annotated data existing work often utilize generative adversarial network gan generate training data however ganbased method overlook nuanced information specific individual organ degrades generation quality chest xray image hence propose novel semantics guided disentangled gan sdgan generate highquality training data fully utilizing semantic information different organ chest xray image rib segmentation particular use three branch disentangle feature different organ use decoder combine feature generate corresponding image ensure generated image correspond input organ label semantics tag employ semantics guidance module perform semantic guidance generated image evaluate efficacy sdgan generating highquality sample introduce modified transunetmtunet specialized segmentation network designed multiscale contextual information extracting multibranch decoding effectively tackling challenge organ overlap also propose new chest xray image dataset cxrs includes sample various medical institution lung clavicle rib simultaneously annotated chest xray image visualization quantitative result demonstrate efficacy sdgan generating highquality chest xray imagemask pair using generated data trained mtunet overcomes limitation data scale outperforms segmentation network
instructavatar textguided emotion motion control avatar generation recent talking avatar generation model made stride achieving realistic accurate lip synchronization audio often fall short controlling conveying detailed expression emotion avatar making generated video less vivid controllable paper propose novel textguided approach generating emotionally expressive avatar offering finegrained control improved interactivity generalizability resulting video framework named instructavatar leverage natural language interface control emotion well facial motion avatar technically design automatic annotation pipeline construct instructionvideo paired training dataset equipped novel twobranch diffusionbased generator predict avatar audio text instruction time experimental result demonstrate instructavatar produce result align well condition outperforms existing method finegrained emotion control lipsync quality naturalness project page
stable video portrait rapid advance field generative ai texttoimage method particular transformed way interact perceive computergenerated imagery today parallel much progress made face reconstruction using morphable model paper present svp novel hybrid generation method output photorealistic video talking face leveraging large pretrained texttoimage prior controlled via specifically introduce personspecific finetuning general stable diffusion model lift video model providing temporal sequence conditioning introducing temporal denoising procedure output model generates temporally smooth imagery person control ie personspecific avatar facial appearance personspecific avatar edited morphed textdefined celebrity without finetuning test time method analyzed quantitatively qualitatively show method outperforms stateoftheart monocular head avatar method
emotivetalk expressive talking head generation audio information decoupling emotional video diffusion diffusion model revolutionized field talking head generation yet still face challenge expressiveness controllability stability longtime generation research propose emotivetalk framework address issue firstly realize better control generation lip movement facial expression visionguided audio information decoupling vaid approach designed generate audiobased decoupled representation aligned lip movement expression specifically achieve alignment audio facial expression representation space present diffusionbased cospeech temporal expansion dicte module within vaid generate expressionrelated representation multisource emotion condition constraint propose welldesigned emotional talking head diffusion ethd backbone efficiently generate highly expressive talking head video contains expression decoupling injection edi module automatically decouple expression reference portrait integrating target expression information achieving expressive generation performance experimental result show emotivetalk generate expressive talking head video ensuring promised controllability emotion stability longtime generation yielding stateoftheart performance compared existing method
diffdef diffusiongenerated deformation field conditional atlas anatomical atlas widely used population analysis conditional atlas target particular subpopulation defined via certain condition eg demographic pathology allow investigation finegrained anatomical difference morphological change correlated age existing approach use either registrationbased method unable handle large anatomical variation generative model suffer training instability hallucination overcome limitation use latent diffusion model generate deformation field transform general population atlas one representing specific subpopulation generating deformation field registering conditional atlas neighbourhood image ensure structural plausibility avoid hallucination occur direct image synthesis compare method several stateoftheart atlas generation method experiment using brain well wholebody mr image uk biobank method generates highly realistic atlas smooth transformation high anatomical fidelity outperforming baseline
sd see sorted quadruped skill synthesis single video demonstration paper present sd see sorted novel pipeline intuitive quadrupedal skill learning single demonstration video leveraging visual capability sd process input video novel chainofthought promoting technique sus generates executable reward function rf drive imitation locomotion skill learning proximal policy optimization ppobased reinforcement learning rl policy using environment information nvidia isaacgym simulator sd autonomously evaluates rf monitoring individual reward component supplying training footage fitness metric back prompted evolve rf achieve higher task fitness iteration validate method unitree robot demonstrating ability execute variable skill trotting bounding pacing hopping achieving high imitation fidelity locomotion stability sd show improvement sota method task adaptability reduced dependence domainspecific knowledge bypassing need laborintensive reward engineering largescale training datasets additional information opensourced code found httpsrplcsuclgithubiosdsweb
graphjigsaw conditioned diffusion model skeletonbased video anomaly detection skeletonbased video anomaly detection svad crucial task computer vision accurately identifying abnormal pattern event enables operator promptly detect suspicious activity thereby enhancing safety achieving demand comprehensive understanding human motion body region level also accounting wide variation performing single action however existing study fail simultaneously address crucial property paper introduces novel practical lightweight framework namely graphjigsaw conditioned diffusion model skeletonbased video anomaly detection gicisad overcome challenge associated svad gicisad consists three novel module graph attentionbased forecasting module capture spatiotemporal dependency inherent data graphlevel jigsaw puzzle maker module distinguish subtle regionlevel discrepancy normal abnormal motion graphbased conditional diffusion model generate wide spectrum human motion extensive experiment four widely used skeletonbased video datasets show gicisad outperforms existing method significantly fewer training parameter establishing new stateoftheart
muvi videotomusic generation semantic alignment rhythmic synchronization generating music aligns visual content video challenging task requires deep understanding visual semantics involves generating music whose melody rhythm dynamic harmonize visual narrative paper present muvi novel framework effectively address challenge enhance cohesion immersive experience audiovisual content muvi analyzes video content specially designed visual adaptor extract contextually temporally relevant feature feature used generate music match video mood theme also rhythm pacing also introduce contrastive musicvisual pretraining scheme ensure synchronization based periodicity nature music phrase addition demonstrate flowmatchingbased music generator incontext learning ability allowing u control style genre generated music experimental result show muvi demonstrates superior performance audio quality temporal synchronization generated music video sample available
chinese continuous sign language dataset based complex environment current bottleneck continuous sign language recognition cslr research lie fact publicly available datasets limited laboratory environment television program recording resulting single background environment uniform lighting significantly deviate diversity complexity found reallife scenario address challenge constructed new largescale dataset chinese continuous sign language csl based complex environment termed complex environment chinese sign language dataset cecsl dataset encompasses continuous csl video clip collected daily life scene featuring different complex background ensure representativeness generalization capability tackle impact complex background cslr performance propose timefrequency network tfnet model continuous sign language recognition model extract framelevel feature utilizes temporal spectral information separately derive sequence feature fusion aiming achieve efficient accurate cslr experimental result demonstrate approach achieves significant performance improvement cecsl validating effectiveness complex background condition additionally proposed method also yielded highly competitive result applied three publicly available csl datasets
daefuse adaptive discriminative autoencoder multimodality image fusion extreme scenario nighttime lowvisibility environment achieving reliable perception critical application like autonomous driving robotics surveillance multimodality image fusion particularly integrating infrared imaging offer robust solution combining complementary information different modality enhance scene understanding decisionmaking however current method face significant limitation ganbased approach often produce blurry image lack finegrained detail aebased method may introduce bias toward specific modality leading unnatural fusion result address challenge propose daefuse novel twophase discriminative autoencoder framework generates sharp natural fused image furthermore pioneer extension image fusion technique static image video domain preserving temporal consistency across frame thus advancing perceptual capability required autonomous navigation extensive experiment public datasets demonstrate daefuse achieves stateoftheart performance multiple benchmark superior generalizability task like medical image fusion
vmddpm vision mamba diffusion medical image synthesis realm smart healthcare researcher enhance scale diversity medical datasets medical image synthesis however existing method limited cnn local perception transformer quadratic complexity making difficult balance structural texture consistency end propose vision mamba ddpm vmddpm based state space model ssm fully combining cnn local perception ssm global modeling capability maintaining linear computational complexity specifically designed multilevel feature extraction module called multilevel state space block mssblock basic unit encoderdecoder structure called state space layer sslayer medical pathological image besides designed simple plugandplay zeroparameter sequence regeneration strategy crossscan module csm enabled module fully perceive spatial feature image stimulate generalization potential model best knowledge first medical image synthesis model based ssmcnn hybrid architecture experimental evaluation three datasets different scale ie acdc chestxray well qualitative evaluation radiologist demonstrate vmddpm achieves stateoftheart performance
inverse painting reconstructing painting process given input painting reconstruct timelapse video may painted formulate autoregressive image generation problem initially blank canvas iteratively updated model learns real artist training many painting video approach incorporates text region understanding define set painting instruction update canvas novel diffusionbased renderer method extrapolates beyond limited acrylic style painting trained showing plausible result wide range artistic style genre
infinigen indoors photorealistic indoor scene using procedural generation introduce infinigen indoors blenderbased procedural generator photorealistic indoor scene build upon existing infinigen system focus natural scene expands coverage indoor scene introducing diverse library procedural indoor asset including furniture architecture element appliance daytoday object also introduces constraintbased arrangement system consists domainspecific language expressing diverse constraint scene composition solver generates scene composition maximally satisfy constraint provide export tool allows generated object scene directly used training embodied agent realtime simulator omniverse unreal infinigen indoors opensourced bsd license please visit httpsinfinigenorg code video
diffusion model vision survey recent year vision become crucial field within computer vision powering wide range application autonomous driving robotics augmented reality ar medical imaging field relies accurate perception understanding reconstruction scene data source like image video diffusion model originally designed generative task offer potential flexible probabilistic approach better capture variability uncertainty present realworld data however traditional method often struggle efficiency scalability paper review stateoftheart approach leverage diffusion model visual task including limited object generation shape completion point cloud reconstruction scene understanding provide indepth discussion underlying mathematical principle diffusion model outlining forward reverse process well various architectural advancement enable model work datasets also discus key challenge applying diffusion model vision handling occlusion varying point density computational demand highdimensional data finally discus potential solution including improving computational efficiency enhancing multimodal fusion exploring use largescale pretraining better generalization across task paper serf foundation future exploration development rapidly evolving field
nickel diming gan dualmethod approach enhancing gan efficiency via knowledge distillation paper address challenge compressing generative adversarial network gans deployment resourceconstrained environment proposing two novel methodology distribution matching efficient compression dime network interactive compression via knowledge exchange learning nickel dime employ foundation model embedding kernel efficient distribution matching leveraging maximum mean discrepancy facilitate effective knowledge distillation simultaneously nickel employ interactive compression method enhances communication student generator discriminator achieving balanced stable compression process comprehensive evaluation architecture ffhq dataset show effectiveness approach nickel dime achieving fid score compression rate respectively remarkably method sustain generative quality even extreme compression rate surpassing previous stateoftheart performance large margin finding demonstrate methodology capacity significantly lower gans computational demand also pave way deploying highquality gan model setting limited resource code released soon
enhancing alzheimers disease prediction novel approach leveraging ganaugmented data improved cnn model accuracy alzheimers disease ad neurodegenerative disease affecting million individual across globe prevalence disease continues rise early diagnosis crucial improve clinical outcome neural network specifically convolutional neural network cnns promising tool diagnosing individual alzheimers however neural network anns cnns typically yield lower validation accuracy fed lower quantity data hence generative adversarial network gans utilized synthesize data augment existing mri datasets potentially yielding higher validation accuracy study use principle examining novel application ssmi metric selecting highquality synthetic data generated gan compare accuracy shuffled data generated gan observed incorporating gans ssmi metric returned highest accuracy compared traditional dataset
value aigenerated metadata ugc platform evidence largescale field experiment aigenerated content aigc advertisement copy product description social medium post becoming ubiquitous business practice however value aigenerated metadata title remains unclear usergenerated content ugc platform address gap conducted largescale field experiment leading shortvideo platform asia provide million user access aigenerated title uploaded video finding show provision aigenerated title significantly boosted content consumption increasing valid watch watch duration producer adopted title increase jumped respectively viewershipboost effect largely attributed use generative ai gai tool increasing likelihood video title effect pronounced group affected metadata sparsity mechanism analysis revealed aigenerated metadata improved uservideo matching accuracy platform recommender system interestingly video producer would posted title anyway adopting aigenerated title decreased viewership average implying aigenerated title may lower quality humangenerated one however producer chose cocreate gai significantly revised aigenerated title video outperformed counterpart either fully aigenerated humangenerated title showcasing benefit humanai cocreation study highlight value aigenerated metadata humanai metadata cocreation enhancing usercontent matching content consumption ugc platform
learning poseconditioned denoiser realistic gaussian avatar modeling advancement neural implicit representation differentiable rendering markedly improved ability learn animatable avatar sparse multiview rgb video however current method map observation space canonical space often face challenge capturing posedependent detail generalizing novel pose diffusion model demonstrated remarkable zeroshot capability image generation potential creating animatable avatar input remains underexplored work introduce novel approach featuring poseconditioned human modeling pipeline integrates iterative denoising rectifying step denoiser guided pose cue generates detailed multiview image provide rich feature set necessary highfidelity reconstruction pose rendering complementing gaussianbased rectifier render image enhanced consistency twostage projection strategy novel local coordinate representation additionally propose innovative sampling strategy ensure smooth temporal continuity across frame video synthesis method effectively address limitation traditional numerical solution handling illposed mapping producing realistic animatable human avatar experimental result demonstrate excels highfidelity avatar modeling robustly generalizes novel pose code available httpsgithubcomsilencetanggaussianactor
towards generalist robot learning internet video survey scaling deep learning massive diverse internet data yielded remarkably general capability visual natural language understanding generation however data remained scarce challenging collect robotics seeing robot learning struggle obtain similarly general capability promising learning video lfv method aim address robotics data bottleneck augmenting traditional robot data largescale internet video data video data offer broad foundational information regarding physical behaviour underlying physic world thus highly informative generalist robot survey present thorough overview emerging field lfv outline fundamental concept including benefit challenge lfv provide comprehensive review current method extracting knowledge largescale internet video addressing key challenge lfv boosting downstream robot reinforcement learning via use video data survey concludes critical discussion challenge opportunity lfv advocate scalable foundation model approach leverage full range available internet video improve learning robot policy dynamic model hope survey inform catalyse lfv research driving progress towards development generalpurpose robot
foleycrafter bring silent video life lifelike synchronized sound study neural foley automatic generation highquality sound effect synchronizing video enabling immersive audiovisual experience despite wide range application existing approach encounter limitation come simultaneously synthesizing highquality videoaligned ie semantic relevant temporal synchronized sound overcome limitation propose foleycrafter novel framework leverage pretrained texttoaudio model ensure highquality audio generation foleycrafter comprises two key component semantic adapter semantic alignment temporal controller precise audiovideo synchronization semantic adapter utilizes parallel crossattention layer condition audio generation video feature producing realistic sound effect semantically relevant visual content meanwhile temporal controller incorporates onset detector timestampbased adapter achieve precise audiovideo alignment one notable advantage foleycrafter compatibility text prompt enabling use text description achieve controllable diverse videotoaudio generation according user intent conduct extensive quantitative qualitative experiment standard benchmark verify effectiveness foleycrafter model code available httpsgithubcomopenmmlabfoleycrafter
standardizing generative face video compression using supplemental enhancement information paper proposes generative face video compression gfvc approach using supplemental enhancement information sei series compact spatial temporal representation face video signal ie keypoints facial semantics compact feature coded using sei message inserted coded video bitstream time writing proposed gfvc approach using sei message adopted official working draft versatile supplemental enhancement information vsei standard joint video expert team jvet isoiec jtc itut standardized new version itut isoiec best author knowledge jvet work proposed seibased gfvc approach first standardization activity generative video compression proposed sei approach advanced reconstruction quality earlyday modelbased coding mbc via stateoftheart generative technique also established new sei definition future gfvc application deployment experimental result illustrate proposed seibased gfvc approach achieve remarkable ratedistortion performance compared latest versatile video coding vvc standard whilst also potentially enabling wide variety functionality including userspecified animationfiltering metaverserelated application
motiongrounded video reasoning understanding perceiving motion pixel level paper introduce motiongrounded video reasoning new motion understanding task requires generating visual answer video segmentation mask according input question hence need implicit spatiotemporal reasoning grounding task extends existing spatiotemporal grounding work focusing explicit actionmotion grounding general format enabling implicit reasoning via question facilitate development new task collect largescale dataset called groundmore comprises video clip object mask deliberately designed question type causal sequential counterfactual descriptive benchmarking deep comprehensive motion reasoning ability groundmore uniquely requires model generate visual answer providing concrete visually interpretable response plain text evaluates model spatiotemporal grounding reasoning fostering address complex challenge motionrelated video reasoning temporal perception pixellevel understanding furthermore introduce novel baseline model named motiongrounded video reasoning assistant mora mora incorporates multimodal reasoning ability multimodal llm pixellevel perception capability grounding model sam temporal perception ability lightweight localization head mora achieves respectable performance groundmore outperforming best existing visual grounding baseline model average relatively hope novel challenging task pave way future advancement robust general motion understanding via video reasoning segmentation
textdriven traffic anomaly detection temporal highfrequency modeling driving video traffic anomaly detection tad driving video critical ensuring safety autonomous driving advanced driver assistance system previous singlestage tad method primarily rely frame prediction making vulnerable interference dynamic background induced rapid movement dashboard camera twostage tad method appear natural solution mitigate interference preextracting backgroundindependent feature bounding box optical flow using perceptual algorithm susceptible performance firststage perceptual algorithm may result error propagation paper introduce tthf novel singlestage method aligning video clip text prompt offering new perspective traffic anomaly detection unlike previous approach supervised signal method derived language rather orthogonal onehot vector providing comprehensive representation concerning visual representation propose model high frequency driving video temporal domain modeling capture dynamic change driving scene enhances perception driving behavior significantly improves detection traffic anomaly addition better perceive various type traffic anomaly carefully design attentive anomaly focusing mechanism visually linguistically guide model adaptively focus visual context interest thereby facilitating detection traffic anomaly shown proposed tthf achieves promising performance outperforming stateoftheart competitor auc dota dataset achieving high generalization dada dataset
matching anything segmenting anything robust association object across video frame complex scene crucial many application especially multiple object tracking mot current method predominantly rely labeled domainspecific video datasets limit crossdomain generalization learned similarity embeddings propose masa novel method robust instance association learning capable matching object within video across diverse domain without tracking label leveraging rich object segmentation segment anything model sam masa learns instancelevel correspondence exhaustive data transformation treat sam output dense object region proposal learn match region vast image collection design universal masa adapter work tandem foundational segmentation detection model enable track detected object combination present strong zeroshot tracking ability complex domain extensive test multiple challenging mot mot benchmark indicate proposed method using unlabeled static image achieves even better performance stateoftheart method trained fully annotated indomain video sequence zeroshot association project page httpsmatchinganythinggithubio
combining pre postdemosaicking noise removal raw video denoising one fundamental step processing pipeline convert data captured camera sensor displayready image video generally performed early pipeline usually demosaicking although study swapping order even conducting jointly proposed advent deep learning quality denoising algorithm steadily increased even modern neural network still hard time adapting new noise level scene indispensable realworld application mind propose selfsimilaritybased denoising scheme weight pre postdemosaicking denoiser bayerpatterned cfa video data show balance two lead better image quality empirically find higher noise level benefit higher influence predemosaicking also integrate temporal trajectory prefiltering step denoiser improve texture reconstruction proposed method requires estimation noise model sensor accurately adapts noise level competitive state art making suitable realworld videography
egomimic scaling imitation learning via egocentric video scale diversity demonstration data required imitation learning significant challenge present egomimic fullstack framework scale manipulation via human embodiment data specifically egocentric human video paired hand tracking egomimic achieves system capture human embodiment data using ergonomic project aria glass lowcost bimanual manipulator minimizes kinematic gap human data crossdomain data alignment technique imitation learning architecture cotrains human robot data compared prior work extract highlevel intent human video approach treat human robot data equally embodied demonstration data learns unified policy data source egomimic achieves significant improvement diverse set longhorizon singlearm bimanual manipulation task stateoftheart imitation learning method enables generalization entirely new scene finally show favorable scaling trend egomimic adding hour additional hand data significantly valuable hour additional robot data video additional information found httpsegomimicgithubio
video guided controllable dynamic physicsbased generation work introduce novel approach creating controllable dynamic gaussians using casually captured reference video method transfer motion object reference video variety generated gaussians across different category ensuring precise customizable motion transfer achieve employing blend skinningbased nonparametric shape reconstruction extract shape motion reference object process involves segmenting reference object motionrelated part based skinning weight establishing shape correspondence generated target shape address shape temporal inconsistency prevalent existing method integrate physical simulation driving target shape matched motion integration optimized displacement loss ensure reliable genuine dynamic approach support diverse reference input including human quadruped articulated object generate dynamic arbitrary length providing enhanced fidelity applicability unlike method heavily reliant diffusion video generation model technique offer specific highquality motion transfer maintaining shape integrity temporal consistency
frieren efficient videotoaudio generation network rectified flow matching videotoaudio generation aim synthesize contentmatching audio silent video remains challenging build model high generation quality efficiency visualaudio temporal synchrony propose frieren model based rectified flow matching frieren regress conditional transport vector field noise spectrogram latent straight path conduct sampling solving ode outperforming autoregressive scorebased model term audio quality employing nonautoregressive vector field estimator based feedforward transformer channellevel crossmodal feature fusion strong temporal alignment model generates audio highly synchronized input video furthermore reflow onestep distillation guided vector field model generate decent audio even one sampling step experiment indicate frieren achieves stateoftheart performance generation quality temporal alignment vggsound alignment accuracy reaching improvement inception score strong diffusionbased baseline audio sample available
synthetic brain image bridging gap brain mapping generative adversarial model magnetic resonance imaging mri vital modality gaining precise anatomical information play significant role medical imaging diagnosis therapy planning image synthesis problem seen revolution recent year due introduction deep learning technique specifically generative adversarial network gans work investigates use deep convolutional generative adversarial network dcgan producing highfidelity realistic mri image slice suggested approach us dataset variety brain mri scan train dcgan architecture discriminator network discerns created real slice generator network learns synthesise realistic mri image slice generator refines capacity generate slice closely mimic real mri data adversarial training approach outcome demonstrate dcgan promise range us medical imaging research since show effectively produce mri image slice train consequent number epoch work add expanding corpus research application deep learning technique medical image synthesis slice could produced possess capability enhance datasets provide data augmentation training deep learning model well number function made available make mri data cleaning easier three ready use clean dataset major anatomical plan
motion mode could happen next predicting diverse object motion single static image remains challenging current video generation model often entangle object movement camera motion scene change recent method predict specific motion motion arrow input rely synthetic data predefined motion limiting application complex scene introduce motion mode trainingfree approach explores pretrained imagetovideo generator latent distribution discover various distinct plausible motion focused selected object static image achieve employing flow generator guided energy function designed disentangle object camera motion additionally use energy inspired particle guidance diversify generated motion without requiring explicit training data experimental result demonstrate motion mode generates realistic varied object animation surpassing previous method even human prediction regarding plausibility diversity project webpage httpsmotionmodesgithubio
electrooptical image synthesis sar imagery using generative adversarial network utility synthetic aperture radar sar imagery remote sensing satellite image analysis well established offering robustness various weather lighting condition however sar image characterized unique structural texture characteristic often pose interpretability challenge analyst accustomed electrooptical eo imagery application compare stateoftheart generative adversarial network gans including cyclegan scyclegan novel dualgenerator gan utilizing partial convolution novel dualgenerator architecture utilizing transformer model designed progressively refine realism translated optical image thereby enhancing visual interpretability sar data demonstrate efficacy approach qualitative quantitative evaluation comparing synthesized eo image actual eo image term visual fidelity feature preservation result show significant improvement interpretability making sar data accessible analyst familiar eo imagery furthermore explore potential technology various application including environmental monitoring urban planning military reconnaissance rapid accurate interpretation sar data crucial research contributes field remote sensing bridging gap sar eo imagery offering novel tool enhanced data interpretation broader application sar technology various domain
selfsupervised learning deviation latent representation cospeech gesture video generation gesture pivotal enhancing cospeech communication recent work mostly focused pointlevel motion transformation fully supervised motion representation datadriven approach explore representation gesture cospeech focus selfsupervised representation pixellevel motion deviation utilizing diffusion model incorporates latent motion feature approach leverage selfsupervised deviation latent representation facilitate hand gesture generation crucial generating realistic gesture video result first experiment demonstrate method enhances quality generated video improvement fgd div fvd psnr ssim current stateoftheart method
atomovideo high fidelity imagetovideo generation recently video generation achieved significant rapid development based superior texttoimage generation technique work propose high fidelity framework imagetovideo generation named atomovideo based multigranularity image injection achieve higher fidelity generated video given image addition thanks high quality datasets training strategy achieve greater motion intensity maintaining superior temporal consistency stability architecture extends flexibly video frame prediction task enabling long sequence prediction iterative generation furthermore due design adapter training approach well combined existing personalized model controllable module quantitatively qualitatively evaluation atomovideo achieves superior result compared popular method example found project website httpsatomovideogithubio
semantically consistent videotoaudio generation using multimodal language large model existing work made stride video generation lack sound effect sfx background music bgm hinders complete immersive viewer experience introduce novel semantically consistent v ideotoaudio generation framework namely sva automatically generates audio semantically consistent given video content framework harness power multimodal large language model mllm understand video semantics key frame generate creative audio scheme utilized prompt texttoaudio model resulting videotoaudio generation natural language interface show satisfactory performance sva case study discus limitation along future research direction project page available
elevating flowguided video inpainting reference generation video inpainting vi challenging task requires effective propagation observable content across frame simultaneously generating new content present original video study propose robust practical vi framework leverage large generative model reference generation combination advanced pixel propagation algorithm powered strong generative model method significantly enhances framelevel quality object removal also synthesizes new content missing area based userprovided text prompt pixel propagation introduce oneshot pixel pulling method effectively avoids error accumulation repeated sampling maintaining subpixel precision evaluate various vi method realistic scenario also propose highquality vi benchmark hqvi comprising carefully generated video using alpha matte composition public benchmark hqvi dataset method demonstrates significantly higher visual quality metric score compared existing solution furthermore process highresolution video exceeding resolution ease underscoring superiority realworld application
rdpm solve diffusion probabilistic model via recurrent token prediction diffusion probabilistic model dpms emerged de facto approach highfidelity image synthesis operating diffusion process continuous vae latent significantly differ text generation method employed large language model llm paper introduce novel generative framework recurrent diffusion probabilistic model rdpm enhances diffusion process recurrent token prediction mechanism thereby pioneering field discrete diffusion progressively introducing gaussian noise latent representation image encoding vectorquantized token recurrent manner rdpm facilitates unique diffusion process discretevalue domain process iteratively predicts token code subsequent timesteps transforming initial standard gaussian noise source data distribution aligning gptstyle model term loss function rdpm demonstrates superior performance benefiting speed advantage requiring inference step model leverage diffusion process ensure highquality generation also convert continuous signal series highfidelity discrete token thereby maintaining unified optimization strategy discrete token text anticipate work contribute development unified model multimodal generation specifically integrating continuous signal domain image video audio text release code model weight opensource community
controlling rate distortion realism towards single comprehensive neural image compression model recent year neural networkdriven image compression nic gained significant attention work adopt deep generative model gans diffusion model enhance perceptual quality realism critical obstacle generative nic method model optimized single bit rate consequently multiple model required compress image different bit rate impractical realworld application tackle issue propose variablerate generative nic model specifically explore several discriminator design tailored variablerate approach introduce novel adversarial loss moreover incorporating newly proposed multirealism technique method allows user adjust bit rate distortion realism single model achieving ultracontrollability unlike existing variablerate generative nic model method match surpasses performance stateoftheart singlerate generative nic model covering wide range bit rate using one model code available httpsgithubcomiwashicrdr
vase objectcentric appearance shape manipulation real video recently several work tackled video editing task fostered success largescale texttoimage generative model however method holistically edit frame using text exploiting prior given foundation diffusion model focusing improving temporal consistency across frame work introduce framework objectcentric designed control object appearance notably execute precise explicit structural modification object build framework pretrained imageconditioned diffusion model integrate layer handle temporal dimension propose training strategy architectural modification enable shape control evaluate method imagedriven video editing task showing similar performance stateoftheart showcasing novel shapeediting capability detail code example available project page
stylecinegan landscape cinemagraph generation using pretrained stylegan propose method generate cinemagraphs automatically still landscape image using pretrained stylegan inspired success recent unconditional video generation leverage powerful pretrained image generator synthesize highquality cinemagraphs unlike previous approach mainly utilize latent space pretrained stylegan approach utilizes deep feature space gan inversion cinemagraph generation specifically propose multiscale deep feature warping msdfw warp intermediate feature pretrained stylegan different resolution using msdfw generated cinemagraphs high resolution exhibit plausible looping animation demonstrate superiority method user study quantitative comparison stateoftheart cinemagraph generation method video generation method us pretrained stylegan
lidardm generative lidar simulation generated world present lidardm novel lidar generative model capable producing realistic layoutaware physically plausible temporally coherent lidar video lidardm stand two unprecedented capability lidar generative modeling lidar generation guided driving scenario offering significant potential autonomous driving simulation ii lidar point cloud generation enabling creation realistic temporally coherent sequence heart model novel integrated world generation framework specifically employ latent diffusion model generate scene combine dynamic actor form underlying world subsequently produce realistic sensory observation within virtual environment experiment indicate approach outperforms competing algorithm realism temporal coherency layout consistency additionally show lidardm used generative world model simulator training testing perception model
crossmodal magnetic resonance imaging synthesis leveraging multiscale brain structure spanning multiple scalesfrom macroscopic anatomy intricate microscopic architecturethe human brain exemplifies complex system demand integrated approach fully understand complexity yet mapping nonlinear relationship scale remains challenging due technical limitation high cost multimodal magnetic resonance imaging mri acquisition introduce deep learning framework predicts brain microstructure macrostructure using generative adversarial network gan grounded scalefree selfsimilar nature brain organizationwhere microscale information inferred macroscale explicitly encodes multiscale brain representation distinct processing branch enhance image fidelity suppress artifact propose simple yet effective auxiliary discriminator learning objective result show faithfully translates mri corresponding fractional anisotropy fa image achieving improvement structural similarity index measure ssim compared previous method preserving individual neurobiological characteristic
learning longform video prior via generative pretraining concept involved longform video people object interaction viewed following implicit prior notably complex continue pose challenge comprehensively learned recent year generative pretraining gpt exhibited versatile capacity modeling kind text content even visual location manner work learning longform video prior instead operating pixel space efficient employ visual location like bounding box keypoints represent key information video simply discretized tokenized consumption gpt due scarcity suitable data create new dataset called movie serve representative includes synopsis shotbyshot keyframes finegrained annotation film set character consistent id bounding box whole body keypoints way longform video represented set token learned via generative pretraining experimental result validate approach great potential learning longform video prior code data released urlhttpsgithubcomshowlablongformvideoprior
bviugc video quality database usergenerated content transcoding recent year usergenerated content ugc become one major video type consumed via streaming network numerous research contribution focused assessing visual quality subjective test objective modeling case objective assessment based noreference scenario corresponding reference content assumed available however fullreference video quality assessment also important ugc delivery pipeline particularly associated video transcoding process context present new ugc video quality database bviugc usergenerated content transcoding contains nonpristine reference video test sequence work simulated creation nonpristine reference sequence wide range compression distortion typical content uploaded ugc platform transcoding comprehensive crowdsourced subjective study conducted involving human participant based collected subjective data benchmarked performance fullreference noreference quality metric result demonstrate poor performance srocc value lower metric predicting perceptual quality ugc two different scenario without reference
towards universal synthetic video detector face background manipulation fully aigenerated content existing deepfake detection technique primarily focus facial manipulation faceswapping lipsyncing however advancement texttovideo imagetovideo generative model allow fully aigenerated synthetic content seamless background alteration challenging facecentric detection method demanding versatile approach address introduce underlineuniversal underlinenetwork underlineidentifying underlinetampered synthunderlineetic video textttunite model unlike traditional detector capture fullframe manipulation textttunite extends detection capability scenario without face nonhuman subject complex background modification leverage transformerbased architecture process domainagnostic feature extracted video via foundation model given limited datasets encompassing facialbackground alteration content integrate taskirrelevant data alongside standard deepfake datasets training mitigate model tendency overfocus face incorporating attentiondiversity ad loss promotes diverse spatial attention across video frame combining ad loss crossentropy improves detection performance across varied context comparative evaluation demonstrate textttunite outperforms stateoftheart detector datasets crossdata setting featuring facebackground manipulation fully synthetic video showcasing adaptability generalizable detection capability
multimodal multigenre multipurpose audiovisual academic lecture dataset publishing opensource academic video recording emergent prevalent approach sharing knowledge online video carry rich multimodal information including speech facial body movement speaker well text picture slide possibly even paper although multiple academic video datasets constructed released support multimodal content recognition understanding task partially due lack highquality human annotation paper propose novel multimodal multigenre multipurpose audiovisual academic lecture dataset almost hour video five source covering computer science mathematics medical biology topic highquality human annotation slide text spoken word particular highvalued name entity dataset used multiple audiovisual recognition understanding task evaluation performed contextual speech recognition speech synthesis slide script generation task demonstrate diversity make challenging dataset
dintr tracking via diffusionbased interpolation object tracking fundamental task computer vision requiring localization object interest across video frame diffusion model shown remarkable capability visual generation making wellsuited addressing several requirement tracking problem work proposes novel diffusionbased methodology formulate tracking task firstly conditional process allows injecting indication target object generation process secondly diffusion mechanic developed inherently model temporal correspondence enabling reconstruction actual frame video however existing diffusion model rely extensive unnecessary mapping gaussian noise domain replaced efficient stable interpolation process proposed interpolation mechanism draw inspiration classic imageprocessing technique offering interpretable stable faster approach tailored specifically object tracking task leveraging strength diffusion model circumventing limitation diffusionbased interpolation tracker dintr present promising new paradigm achieves superior multiplicity seven benchmark across five indicator representation
aigenerated video detection via spatiotemporal anomaly learning advancement generation model led emergence highly realistic artificial intelligence aigenerated video malicious user easily create nonexistent video spread false information letter proposes effective aigenerated video detection aigvdet scheme capturing forensic trace twobranch spatiotemporal convolutional neural network cnn specifically two resnet subdetectors learned separately identifying anomaly spatical optical flow domain respectively result subdetectors fused enhance discrimination ability largescale generated video dataset gvd constructed benchmark model training evaluation extensive experimental result verify high generalization robustness aigvdet scheme code dataset available httpsgithubcommultimediaforaigvdet
consistent multiview diffusion enhancement despite advance neural rendering due scarcity highquality datasets inherent limitation multiview diffusion model view synthesis model generation restricted low resolution suboptimal multiview consistency study present novel enhancement pipeline dubbed employ multiview latent diffusion model enhance coarse input preserving multiview consistency method includes poseaware encoder diffusionbased denoiser refine lowquality multiview image along data augmentation multiview attention module epipolar aggregation maintain consistent highquality output across view unlike existing videobased approach model support seamless multiview enhancement improved coherence across diverse viewing angle extensive evaluation show significantly outperforms existing method boosting multiview enhancement perinstance optimization task
fast accurate cooperative radio map estimation enabled gan era realtime radio resource monitoring management urged support diverse wirelessempowered application call fast accurate estimation distribution radio resource usually represented spatial signal power strength geographical environment known radio map paper present cooperative radio map estimation crme approach enabled generative adversarial network gan called gancrme feature fast accurate radio map estimation without transmitter information radio map inferred exploiting interaction distributed received signal strength rss measurement mobile user geographical map using deep neural network estimator resulting low dataacquisition cost computational complexity moreover ganbased learning algorithm proposed boost inference capability deep neural network estimator exploiting power generative ai simulation result showcase proposed gancrme even capable coarse errorcorrection geographical map information inaccurate
joyvasa portrait animal image animation diffusionbased audiodriven facial dynamic head motion generation audiodriven portrait animation made significant advance diffusionbased model improving video quality lipsync accuracy however increasing complexity model led inefficiency training inference well constraint video length interframe continuity paper propose joyvasa diffusionbased method generating facial dynamic head motion audiodriven facial animation specifically first stage introduce decoupled facial representation framework separate dynamic facial expression static facial representation decoupling allows system generate longer video combining static facial representation dynamic motion sequence second stage diffusion transformer trained generate motion sequence directly audio cue independent character identity finally generator trained first stage us facial representation generated motion sequence input render highquality animation decoupled facial representation identityindependent motion generation process joyvasa extends beyond human portrait animate animal face seamlessly model trained hybrid dataset private chinese public english data enabling multilingual support experimental result validate effectiveness approach future work focus improving realtime performance refining expression control expanding application portrait animation code available httpsgithubcomjdhalgojoyvasa
hallo hierarchical audiodriven visual synthesis portrait image animation field portrait image animation driven speech audio input experienced significant advancement generation realistic dynamic portrait research delf complexity synchronizing facial movement creating visually appealing temporally consistent animation within framework diffusionbased methodology moving away traditional paradigm rely parametric model intermediate facial representation innovative approach embrace endtoend diffusion paradigm introduces hierarchical audiodriven visual synthesis module enhance precision alignment audio input visual output encompassing lip expression pose motion proposed network architecture seamlessly integrates diffusionbased generative model unetbased denoiser temporal alignment technique reference network proposed hierarchical audiodriven visual synthesis offer adaptive control expression pose diversity enabling effective personalization tailored different identity comprehensive evaluation incorporates qualitative quantitative analysis approach demonstrates obvious enhancement image video quality lip synchronization precision motion diversity visualization access source code found httpsfudangenerativevisiongithubiohallo
got ta hear sound source aware vision audio generation visiontoaudio synthesis broad application multimedia recent advancement method made possible generate relevant audio input video still image however immersiveness expressiveness generation limited one possible problem existing method solely rely global scene overlook detail local sounding object ie sound source address issue propose sound sourceaware generator able locally perceive multimodal sound source scene visual detection crossmodality translation contrastively learns crossmodal sound source cmss manifold semantically disambiguate source finally attentively mix cmss semantics rich audio representation pretrained audio generator output sound model cmss manifold curate novel singlesoundsource visualaudio dataset vggsound also design sound source matching score measure localized audio relevance addressing generation soundsource level surpasses stateoftheart method generation fidelity relevance evidenced extensive experiment demonstrate ability achieve intuitive control compositing vision text audio condition generation tried heard
llmguided iterative selfrefinement physicsgrounded texttovideo generation texttovideo generation recently enabled transformerbased diffusion model current model lack capability adhering realworld common knowledge physical rule due limited understanding physical realism deficiency temporal modeling existing solution either datadriven require extra model input generalizable outofdistribution domain paper present new dataindependent technique expands current model capability video generation outofdistribution domain enabling chainofthought stepback reasoning prompting experiment show improves existing model adherence realworld physical rule achieves improvement compared prompt enhancer source code available
quark realtime highresolution general neural view synthesis present novel neural algorithm performing highquality highresolution realtime novel view synthesis sparse set input rgb image video stream network reconstructs scene render novel view resolution nvidia feedforward network generalizes across wide variety datasets scene produce stateoftheart quality realtime method quality approach case surpasses quality top offline method order achieve result use novel combination several key concept tie together cohesive effective algorithm build previous work represent scene using semitransparent layer use iterative learned renderandrefine approach improve layer instead flat layer method reconstructs layered depth map ldms efficiently represent scene complex depth occlusion iterative update step embedded multiscale unetstyle architecture perform much compute possible reduced resolution within update step better aggregate information multiple input view use specialized transformerbased network component allows majority perinput image processing performed input image space opposed layer space increasing efficiency finally due realtime nature reconstruction rendering dynamically create discard internal geometry frame generating ldm view taken together produce novel effective algorithm view synthesis extensive evaluation demonstrate achieve stateoftheart quality realtime rate project page
lightit illumination modeling control diffusion model introduce lightit method explicit illumination control image generation recent generative method lack lighting control crucial numerous artistic aspect image generation setting overall mood cinematic appearance overcome limitation propose condition generation shading normal map model lighting single bounce shading includes cast shadow first train shading estimation module generate dataset realworld image shading pair train control network using estimated shading normal input method demonstrates highquality image generation lighting control numerous scene additionally use generated dataset train identitypreserving relighting model conditioned image target shading method first enables generation image controllable consistent lighting performs par specialized relighting stateoftheart method
shotit computeefficient imagetovideo search engine cloud rapid growth information technology user exposed massive amount data online including image music video led strong need provide effective corresponsive search service image music video search service operated based keywords namely using keywords find related image music video additionally imagetoimage search service enable user find similar image using one input image given video essentially composed image frame similar video searched one input image screenshot want target scenario provide efficient method implementation paper present shotit cloudnative imagetovideo search engine tailor search scenario computeefficient approach one main limitation faced scenario scale dataset typical imagetoimage search engine handle onetoone relationship colloquially one image corresponds another single image imagetovideo proliferates take length video example generate roughly image frame number video grows scale dataset explodes exponentially case computeefficient approach ought considered system design cater cloudnative trend choosing emerging technology vector database backbone shotit fit two metric performantly experiment two different datasets thousandscale blender open movie dataset millionscale proprietary tv genre dataset core ram intel xeon gold cloud machine object storage reveal effectiveness shotit demo regarding blender open movie dataset illustrated within paper
human action clip detecting aigenerated human motion fullblown aigenerated video generation continues journey uncanny valley produce content perceptually indistinguishable reality intermixed many exciting creative application malicious application harm individual organization democracy describe effective robust technique distinguishing real aigenerated human motion technique leverage multimodal semantic embedding making robust type laundering typically confound low midlevel approach method evaluated custombuilt dataset video clip human action generated seven texttovideo ai model matching real footage
fight scene detection movie highlight generation system paper research based project using bidirectional long shortterm memory bilstm network provide novel fight scene detection fsd model used movie highlight generation system mhgs based deep learning neural network movie usually fight scene keep audience amazed trailer generation application highlight generation tidious first identify scene manually compile generate highlight serving purpose proposed fsd system utilises temporal characteristic movie scene thus capable automatically identify fight scene thereby helping effective production captivating movie highlight observe proposed solution feature accuracy higher cnn hough forest accurate significantly higher cnn feature accuracy
deepfake generation detection benchmark survey deepfake technology dedicated creating highly realistic facial image video specific condition significant application potential field entertainment movie production digital human creation name advancement deep learning technique primarily represented variational autoencoders generative adversarial network achieved impressive generation result recently emergence diffusion model powerful generation capability sparked renewed wave research addition deepfake generation corresponding detection technology continuously evolve regulate potential misuse deepfakes privacy invasion phishing attack survey comprehensively review latest development deepfake generation detection summarizing analyzing current stateofthearts rapidly evolving field first unify task definition comprehensively introduce datasets metric discus developing technology discus development several related subfields focus researching four representative deepfake field face swapping face reenactment talking face generation facial attribute editing well forgery detection subsequently comprehensively benchmark representative method popular datasets field fully evaluating latest influential published work finally analyze challenge future research direction discussed field
instance brownian bridge text openvocabulary video instance segmentation temporally locating object arbitrary class text primary pursuit openvocabulary video instance segmentation vi insufficient vocabulary video data previous method leverage imagetext pretraining model recognizing object instance separately aligning frame class text ignoring correlation frame result separation break instance movement context video causing inferior alignment video text tackle issue propose link framelevel instance representation brownian bridge model instance dynamic align bridgelevel instance representation class text precisely openvocabulary vi brivis specifically build system upon frozen video segmentor generate framelevel instance query design temporal instance resampler tir generate query temporal context frame query mold instance query follow brownian bridge accomplish alignment class text design bridgetext alignment bta learn discriminative bridgelevel representation instance via contrastive objective setting minvis basic video segmentor brivis surpasses openvocabulary sota clear margin example challenging largevocabulary vi dataset burst brivis achieves map exhibit improvement compared map
spikemba multimodal spiking saliency mamba temporal video grounding temporal video grounding tvg critical task video content understanding requiring precise alignment video content natural language instruction despite significant advancement existing method face challenge managing confidence bias towards salient object capturing longterm dependency video sequence address issue introduce spikemba multimodal spiking saliency mamba temporal video grounding approach integrates spiking neural network snns state space model ssms leverage unique advantage handling different aspect task specifically use snns develop spiking saliency detector generates proposal set detector emits spike signal input signal exceeds predefined threshold resulting dynamic binary saliency proposal set enhance model capability retain infer contextual information introduce relevant slot learnable tensor encode prior knowledge slot work contextual moment reasoner maintain balance preserving contextual information exploring semantic relevance dynamically ssms facilitate selective information propagation addressing challenge longterm dependency video content combining snns proposal generation ssms effective contextual reasoning spikemba address confidence bias longterm dependency thereby significantly enhancing finegrained multimodal relationship capture experiment demonstrate effectiveness spikemba consistently outperforms stateoftheart method across mainstream benchmark
holmesvad towards unbiased explainable video anomaly detection via multimodal llm towards openended video anomaly detection vad existing method often exhibit biased detection faced challenging unseen event lack interpretability address drawback propose holmesvad novel framework leverage precise temporal supervision rich multimodal instruction enable accurate anomaly localization comprehensive explanation firstly towards unbiased explainable vad system construct first largescale multimodal vad instructiontuning benchmark ie dataset created using carefully designed semiautomatic labeling paradigm efficient singleframe annotation applied collected untrimmed video synthesized highquality analysis abnormal normal video clip using robust offtheshelf video captioner large language model llm building upon dataset develop customized solution interpretable video anomaly detection train lightweight temporal sampler select frame high anomaly response finetune multimodal large language model llm generate explanatory content extensive experimental result validate generality interpretability proposed holmesvad establishing novel interpretable technique realworld video anomaly analysis support community benchmark model publicly available httpsholmesvadgithubio
missiongnn hierarchical multimodal gnnbased weakly supervised video anomaly recognition missionspecific knowledge graph generation context escalating safety concern across various domain task video anomaly detection vad video anomaly recognition var emerged critically important application intelligent surveillance evidence investigation violence alerting etc task aimed identifying classifying deviation normal behavior video data face significant challenge due rarity anomaly lead extremely imbalanced data impracticality extensive framelevel data annotation supervised learning paper introduces novel hierarchical graph neural network gnn based model missiongnn address challenge leveraging stateoftheart large language model comprehensive knowledge graph efficient weakly supervised learning var approach circumvents limitation previous method avoiding heavy gradient computation large multimodal model enabling fully framelevel training without fixed video segmentation utilizing automated missionspecific knowledge graph generation model provides practical efficient solution realtime video analysis without constraint previous segmentationbased multimodal approach experimental validation benchmark datasets demonstrates model performance vad var highlighting potential redefine landscape anomaly detection recognition video surveillance system
videoeval comprehensive benchmark suite lowcost evaluation video foundation model growth highquality data advancement visual pretraining paradigm video foundation model vfms made significant progress recently demonstrating remarkable performance traditional video understanding benchmark however existing benchmark eg kinetics evaluation protocol often limited relatively poor diversity high evaluation cost saturated performance metric paper build comprehensive benchmark suite address issue namely videoeval specifically establish video task adaption benchmark vidtab video embedding benchmark videb two perspective evaluating task adaptability vfms fewshot condition assessing representation power directly applying downstream task videoeval conduct largescale study popular opensource vision foundation model study reveals insightful finding vfms overall current vfms exhibit weak generalization across diverse task increasing video data whether labeled weaklylabeled videotext pair necessarily improve task performance effectiveness pretraining paradigm may fully validated previous benchmark combining different pretraining paradigm help improve generalization capability believe study serf important complement current evaluation vfms offer valuable insight future research
learning natural consistency representation face forgery video detection face forgery video elicited critical social public concern various detector proposed however fullysupervised detector may lead easily overfitting specific forgery method video existing selfsupervised detector strict auxiliary task requiring audio multimodalities leading limited generalization robustness paper examine whether address issue leveraging visualonly real face video end propose learn natural consistency representation naco real face video selfsupervised manner inspired observation fake video struggle maintain natural spatiotemporal consistency even unknown forgery method different perturbation naco first extract spatial feature frame cnns integrates transformer learn longrange spatiotemporal representation leveraging advantage cnns transformer local spatial receptive field longterm memory respectively furthermore spatial predictive modulespm temporal contrastive moduletcm introduced enhance natural consistency representation learning spm aim predict random masked spatial feature spatiotemporal representation tcm regularizes latent distance spatiotemporal representation shuffling natural order disturb consistency could force naco sensitive natural spatiotemporal consistency representation learning stage mlp head finetuned perform usual forgery video classification task extensive experiment show method outperforms stateoftheart competitor impressive generalization robustness
queryefficient video adversarial attack stylized logo video classification system based deep neural network dnns demonstrated excellent performance accurately verifying video content however recent study shown dnns highly vulnerable adversarial example therefore deep understanding adversarial attack better respond emergency situation order improve attack performance many styletransferbased attack patchbased attack proposed however global perturbation former bring unnatural global color latter difficult achieve success targeted attack due limited perturbation space moreover compared plethora method targeting image classifier video adversarial attack still popular therefore generate adversarial example low budget provide higher verisimilitude propose novel blackbox video attack framework called stylized logo attack sla sla conducted three step first step involves building style reference set logo make generated example natural also carry target class feature targeted attack reinforcement learning rl employed determine style reference position parameter logo within video ensures stylized logo placed video optimal attribute finally perturbation optimization designed optimize perturbation improve fooling rate stepbystep manner sufficient experimental result indicate sla achieve better performance stateoftheart method still maintain good deception effect facing various defense method
personalized video summarization using textbased query conditional modeling proliferation video content platform like youtube vimeo present significant challenge efficiently locating relevant information automatic video summarization aim address extracting presenting key content condensed form thesis explores enhancing video summarization integrating textbased query conditional modeling tailor summary user need traditional method often produce fixed summary may align individual requirement overcome propose multimodal deep learning approach incorporates textual query visual information fusing different level model architecture evaluation metric accuracy assess quality generated summary thesis also investigates improving textbased query representation using contextualized word embeddings specialized attention network enhances semantic understanding query leading better video summary emulate humanlike summarization account visual coherence abstract factor like storyline consistency introduce conditional modeling approach method us multiple random variable joint distribution capture key summarization component resulting humanlike explainable summary addressing data scarcity fully supervised learning thesis proposes segmentlevel pseudolabeling approach selfsupervised method generates additional data improving model performance even limited humanlabeled datasets summary research aim enhance automatic video summarization incorporating textbased query improving query representation introducing conditional modeling addressing data scarcity thereby creating effective personalized video summary
realizing video summarization path languagebased semantic understanding recent development videobased large language model videollms significantly advanced video summarization aligning video feature case audio feature large language model llm videollms possesses unique strength weakness many recent method required extensive finetuning overcome limitation model resourceintensive work observe strength one videollm complement weakness another leveraging insight propose novel video summarization framework inspired mixture expert moe paradigm operates inferencetime algorithm without requiring form finetuning approach integrates multiple videollms generate comprehensive coherent textual summary effectively combine visual audio content provides detailed background description excels identifying keyframes enables semantically meaningful retrieval compared traditional computer vision approach rely solely visual information without need additional finetuning moreover resulting summary enhance performance downstream task summary video generation either keyframe selection combination texttoimage model languagedriven approach offer semantically rich alternative conventional method provides flexibility incorporate newer videollms enhancing adaptability performance video summarization task
verified video corpus moment retrieval benchmark finegrained video understanding existing video corpus moment retrieval vcmr limited coarsegrained understanding hinders precise video moment localization given finegrained query paper propose challenging finegrained vcmr benchmark requiring method localize bestmatched moment corpus partially matched candidate improve dataset construction efficiency guarantee highquality data annotation propose verified automatic underlinevidunderlineeotext annotation pipeline generate caption underlinerelunderlineiable underlinefinunderlineegrained static underlinedynamics specifically resort large language model llm large multimodal model lmm proposed static dynamic enhanced captioning module generate diverse finegrained caption video filter inaccurate annotation caused llm hallucination propose finegranularity aware noise evaluator finetune video foundation model disturbed hardnegatives augmented contrastive matching loss verified construct challenging finegrained vcmr benchmark containing charadesfig didemofig activitynetfig demonstrate high level annotation quality evaluate several stateoftheart vcmr model proposed dataset revealing still significant scope finegrained video understanding vcmr code datasets
dashcam video driving simulation stress testing automated vehicle rare event testing automated driving system ad simulation realistic driving scenario important verifying performance however converting realworld driving video simulation scenario significant challenge due complexity interpreting highdimensional video data timeconsuming nature precise manual scenario reconstruction work propose novel framework automates conversion realworld car crash video detailed simulation scenario ad testing approach leverage promptengineered video language modelsvlm transform dashcam footage scenic script define environment driving behavior carla simulator enabling generation realistic simulation scenario importantly rather solely aiming onetoone scenario reconstruction framework focus capturing essential driving behavior original video offering flexibility parameter weather road condition facilitate searchbased testing additionally introduce similarity metric help iteratively refine generated scenario feedback comparing key feature driving behavior real simulated video preliminary result demonstrate substantial time efficiency finishing realtosim conversion minute full automation human intervention maintaining high fidelity original driving event
overview trec medical video question answering medvidqa track one key goal artificial intelligence ai development multimodal system facilitates communication visual world image video using natural language query earlier work medical question answering primarily focused textual visual image modality may inefficient answering question requiring demonstration recent year significant progress achieved due introduction largescale languagevision datasets development efficient deep neural technique bridge gap language visual understanding improvement made numerous visionandlanguage task visual captioning visual question answering natural language video localization existing work language vision focused creating datasets developing solution opendomain application believe medical video may provide best possible answer many first aid medical emergency medical education question increasing interest ai support clinical decisionmaking improve patient engagement need explore challenge develop efficient algorithm medical languagevideo understanding generation toward introduced new task foster research toward designing system understand medical video provide visual answer natural language question equipped multimodal capability generate instruction step medical video task potential support development sophisticated downstream application benefit public medical professional
youtube datadriven analysis sentiment toxicity content recommendation study present datadriven analysis discourse youtube examining sentiment toxicity thematic pattern video content published january october analysis involved applying advanced natural language processing nlp technique sentiment analysis vader toxicity detection detoxify topic modeling using latent dirichlet allocation lda sentiment analysis revealed video description positive neutral negative indicating generally informative supportive tone pandemicrelated content toxicity analysis identified content toxic suggesting minimal exposure toxic content topic modeling revealed two main theme video covering general health information pandemicrelated impact focused news realtime update highlighting dual informational role youtube recommendation system also developed using tfidf vectorization cosine similarity refined sentiment toxicity topic filter ensure relevant contextaligned video recommendation system achieved aggregate coverage monthly coverage rate consistently demonstrating robust performance adaptability time evaluation across recommendation size showed coverage reaching five video recommendation ten video recommendation per video summary work present framework understanding discourse youtube recommendation system support user engagement promoting responsible relevant content related
xmic crossmodal instance conditioning egocentric action generalization lately growing interest adapting visionlanguage model vlms image thirdperson video classification due success zeroshot recognition however adaptation model egocentric video largely unexplored address gap propose simple yet effective crossmodal adaptation framework call xmic using video adapter pipeline learns align frozen text embeddings egocentric video directly shared embedding space novel adapter architecture retains improves generalization pretrained vlms disentangling learnable temporal modeling frozen visual encoder result enhanced alignment text embeddings egocentric video leading significant improvement crossdataset generalization evaluate approach epickitchens egtea datasets finegrained crossdataset action generalization demonstrating effectiveness method code available httpsgithubcomannushaxmic
evaluation texttovideo generation model dynamic perspective comprehensive constructive evaluation protocol play important role development sophisticated texttovideo generation model existing evaluation protocol primarily focus temporal consistency content continuity yet largely ignore dynamic video content dynamic essential dimension measuring visual vividness honesty video content text prompt study propose effective evaluation protocol termed devil center dynamic dimension evaluate model purpose establish new benchmark comprising text prompt fully reflect multiple dynamic grade define set dynamic score corresponding various temporal granularity comprehensively evaluate dynamic generated video based new benchmark dynamic score assess model design three metric dynamic range dynamic controllability dynamicsbased quality experiment show devil achieves pearson correlation exceeding human rating demonstrating potential advance generation model code available httpsgithubcommingxiangldevil
general taskoriented video segmentation present gvseg general video segmentation framework addressing four different video segmentation task ie instance semantic panoptic exemplarguided maintaining identical architectural design currently trend towards developing general video segmentation solution applied across multiple task streamlines research endeavor simplifies deployment however highly homogenized framework current design element maintains uniformity could overlook inherent diversity among different task lead suboptimal performance tackle gvseg provides holistic disentanglement modeling segment target thoroughly examining perspective appearance position shape basis ii reformulates query initialization matching sampling strategy alignment taskspecific requirement architectureagnostic innovation empower gvseg effectively address unique task accommodating specific property characterize extensive experiment seven goldstandard benchmark datasets demonstrate gvseg surpasses existing specializedgeneral solution significant margin four different video segmentation task
attentionbased generative adversarial network joint spacetime video superresolution many application including surveillance entertainment restoration need increase spatial resolution frame rate video sequence aim improve visual quality refine detail create realistic viewing experience existing spacetime video superresolution method effectively use spatiotemporal information address limitation propose generative adversarial network joint spacetime video superresolution generative network consists three operation shallow feature extraction deep feature extraction reconstruction us threedimensional convolution process temporal spatial information simultaneously includes novel attention mechanism extract important channel spatial information discriminative network us twobranch structure handle detail motion information making generated result accurate experimental result red datasets demonstrate effectiveness proposed method source code publicly available
dormant defending posedriven human image animation posedriven human image animation achieved tremendous progress enabling generation vivid realistic human video one single photo however conversely exacerbates risk image misuse attacker may use one available image create video involving politics violence illegal content counter threat propose dormant novel protection approach tailored defend posedriven human image animation technique dormant applies protective perturbation one human image preserving visual similarity original resulting poorquality video generation protective perturbation optimized induce misextraction appearance feature image create incoherence among generated video frame extensive evaluation across animation method datasets demonstrates superiority dormant baseline protection method leading misaligned identity visual distortion noticeable artifact inconsistent frame generated video moreover dormant show effectiveness realworld commercial service even fully blackbox access
simtube generating simulated video comment multimodal ai user persona audience feedback crucial refining video content yet typically come publication limiting creator ability make timely adjustment bridge gap introduce simtube generative ai system designed simulate audience feedback form video comment video release simtube feature computational pipeline integrates multimodal data videosuch visuals audio metadatawith user persona derived broad diverse corpus audience demographic generating varied contextually relevant feedback furthermore system ui allows creator explore customize simulated comment comprehensive evaluationcomprising quantitative analysis crowdsourced assessment qualitative user studieswe show simtubes generated comment relevant believable diverse often detailed informative actual audience comment highlighting potential help creator refine content release
arcon advancing autoregressive continuation driving video recent advancement autoregressive large language model llm led application video generation paper explores use large vision model lvms video continuation task essential building world model predicting future frame introduce arcon scheme alternate generating semantic rgb token allowing lvm explicitly learn highlevel structural video information find high consistency rgb image semantic map generated without special design moreover employ optical flowbased texture stitching method enhance visual quality experiment autonomous driving scenario show model consistently generate long video
manivideo generating handobject manipulation video dexterous generalizable grasping paper introduce manivideo novel method generating consistent temporally coherent bimanual handobject manipulation video given motion sequence hand object core idea manivideo construction multilayer occlusion mlo representation learns occlusion relationship occlusionfree normal map occlusion confidence map embedding mlo structure unet two form model enhances consistency dexterous handobject manipulation achieve generalizable grasping object integrate objaverse largescale object dataset address scarcity video data thereby facilitating learning extensive object consistency additionally propose innovative training strategy effectively integrates multiple datasets supporting downstream task humancentric handobject manipulation video generation extensive experiment demonstrate approach achieves video generation plausible handobject interaction generalizable object also outperforms existing sota method
gender bias texttovideo generation model case study sora advent texttovideo generation model revolutionized content creation produce highquality video textual prompt however concern regarding inherent bias model prompted scrutiny particularly regarding gender representation study investigates presence gender bias openais sora stateoftheart texttovideo generation model uncover significant evidence bias analyzing generated video diverse set genderneutral stereotypical prompt result indicate sora disproportionately associate specific gender stereotypical behavior profession reflects societal prejudice embedded training data
jpeglm llm image generator canonical codec representation recent work image video generation adopting autoregressive llm architecture due generality potentially easy integration multimodal system crux applying autoregressive training language generation visual generation discretization representing continuous data like image video discrete token common method discretizing image video include modeling raw pixel value prohibitively lengthy vector quantization requires convoluted prehoc training work propose directly model image video compressed file saved computer via canonical codecs eg jpeg using default llama architecture without visionspecific modification pretrain jpeglm scratch generate image avclm generate video proof concept directly outputting compressed file byte jpeg avc format evaluation image generation show simple straightforward approach effective pixelbased modeling sophisticated vector quantization baseline method yield reduction fid analysis show jpeglm especial advantage vector quantization model generating longtail visual element overall show using canonical codec representation help lower barrier language generation visual generation facilitating future research multimodal languageimagevideo llm
quality prediction ai generated image video emerging trend opportunity advent ai influenced many aspect human life selfdriving car intelligent chatbots textbased image video generation model capable creating realistic image video based user prompt texttoimage imagetoimage imagetovideo aibased method image video super resolution video frame interpolation denoising compression already gathered significant attention interest industry solution already implemented realworld product service however achieve widespread integration acceptance aigenerated enhanced content must visually accurate adhere intended use maintain high visual quality avoid degrading end user quality experience qoe one way monitor control visual quality aigenerated enhanced content deploying image quality assessment iqa video quality assessment vqa model however existing iqa vqa model measure visual fidelity term reconstruction quality pristine reference content designed assess quality generative artifact address newer metric model recently proposed performance evaluation overall efficacy limited datasets small otherwise lack representative content andor distortion capacity performance measure accurately report success iqavqa model genai paper examines current shortcoming possibility presented aigenerated enhanced image video content particular focus enduser perceived quality finally discus open question make recommendation future work genai quality assessment problem towards progressing interesting relevant field research
generative enhancement medical image limited availability medical image datasets due privacy concern high collection annotation cost pose significant challenge field medical imaging promising alternative use synthesized medical data solution realistic medical image synthesis due difficulty backbone design fewer training sample compared counterpart paper propose novel generative approach synthesis medical image enhancement existing datasets using conditional diffusion model method begin slice noted informed slice serve patient prior propagates generation process using segmentation mask decomposing medical image mask patient prior information offer flexible yet effective solution generating versatile image existing datasets enable dataset enhancement combining informed slice selection generation random position along editable mask volume introduce large variation diffusion sampling moreover informed slice contains patientwise information also facilitate counterfactual image synthesis datasetlevel deenhancement desired control experiment brain mri abdomen ct image demonstrate capable synthesizing highquality medical image volumetric consistency offering straightforward solution dataset enhancement inference code available
convofusion multimodal conversational diffusion cospeech gesture synthesis gesture play key role human communication recent method cospeech gesture generation managing generate beataligned motion struggle generating gesture semantically aligned utterance compared beat gesture align naturally audio signal semantically coherent gesture require modeling complex interaction language human motion controlled focusing certain word therefore present convofusion diffusionbased approach multimodal gesture synthesis generate gesture based multimodal speech input also facilitate controllability gesture synthesis method proposes two guidance objective allow user modulate impact different conditioning modality eg audio v text well choose certain word emphasized gesturing method versatile trained either generating monologue gesture even conversational gesture advance research multiparty interactive gesture dnd group gesture dataset released contains hour gesture data showing people interacting one another compare method several recent work demonstrate effectiveness method variety task urge reader watch supplementary video website
towards languageguided semanticaware eventtovideo reconstruction event camera harness advantage low latency high temporal resolution high dynamic range hdr compared standard camera due distinct imaging paradigm shift dominant line research focus eventtovideo reconstruction bridge eventbased standard computer vision however task remains challenging due inherently illposed nature event camera detect edge motion information locally consequently reconstructed video often plagued artifact regional blur primarily caused ambiguous semantics event data paper find language naturally conveys abundant semantic information rendering stunningly superior ensuring semantic consistency reconstruction accordingly propose novel framework called achieve semanticaware highquality reconstruction languageguided perspective buttressed textconditional diffusion model however due diffusion model inherent diversity randomness hardly possible directly apply achieve spatial temporal consistency reconstruction thus first propose eventguided spatiotemporal attention esa module condition event data denoising pipeline effectively introduce eventaware mask loss ensure temporal coherence noise initialization strategy enhance spatial consistency given absence eventtextvideo paired data aggregate existing datasets generate textual description using tagging model training evaluation extensive experiment three datasets covering diverse challenging scenario eg fast motion low light demonstrate superiority method
noiseenabled static scene recovery event camera event camera also known dynamic vision sensor emerging modality measuring fast dynamic asynchronously event camera capture change logintensity time stream event generally measure intensity hence used imaging dynamic scene however fluctuation due random photon arrival inevitably trigger noise event even static scene previous effort focused filtering undesirable noise event improve signal quality find photonnoise regime noise event correlated static scene intensity analyze noise event generation model relationship illuminance based understanding propose method called leverage illuminancedependent noise characteristic recover static part scene otherwise invisible event camera experimentally collect dataset noise event static scene train validate result provide novel approach capturing static scene event camera solely noise event without additional hardware
simtoreal toward general eventbased lowlight frame interpolation perscene optimization video frame interpolation vfi important video enhancement frame rate upconversion slowmotion generation introduction event camera capture perpixel brightness change asynchronously significantly enhanced vfi capability particularly highspeed nonlinear motion however eventbased method encounter challenge lowlight condition notably trailing artifact signal latency hinder direct applicability generalization addressing issue propose novel perscene optimization strategy tailored lowlight condition approach utilizes internal statistic sequence handle degraded event data lowlight condition improving generalizability different lighting camera setting evaluate robustness lowlight condition introduce evfill unique rgbevent dataset captured lowlight condition result demonstrate stateoftheart performance lowlight environment project page
degstalk decomposed perembedding gaussian field hairpreserving talking face synthesis accurately synthesizing talking face video capturing fine facial feature individual long hair present significant challenge tackle challenge existing method propose decomposed perembedding gaussian field degstalk gaussian splatting talking face synthesis method generating realistic talking face long hair degstalk employ deformable preembedding gaussian field dynamically adjust preembedding gaussian primitive using implicit expression coefficient enables precise capture dynamic facial region subtle expression additionally propose dynamic hairpreserving portrait rendering technique enhance realism long hair motion synthesized video result show degstalk achieves improved realism synthesis quality compared existing approach particularly handling complex facial dynamic hair preservation code publicly available httpsgithubcomcviszudegstalk
cascaded multipath shortcut diffusion model medical image translation imagetoimage translation vital component medical imaging processing many us wide range imaging modality clinical scenario previous method include generative adversarial network gans diffusion model dm offer realism suffer instability lack uncertainty estimation even though gan dm method individually exhibited capability medical image translation task potential combining gan dm improve translation performance enable uncertainty estimation remains largely unexplored work address challenge proposing cascade multipath shortcut diffusion model cmdm highquality medical image translation uncertainty estimation reduce required number iteration ensure robust performance method first obtains conditional gangenerated prior image used efficient reverse translation dm subsequent step additionally multipath shortcut diffusion strategy employed refine translation result estimate uncertainty cascaded pipeline enhances translation quality incorporating residual averaging cascade collected three different medical image datasets two subtasks dataset test generalizability approach experimental result found cmdm produce highquality translation comparable stateoftheart method providing reasonable uncertainty estimation correlate well translation error
conditional controllable image fusion image fusion aim integrate complementary information multiple input image acquired various source synthesize new fused image existing method usually employ distinct constraint design tailored specific scene forming fixed fusion paradigm however datadriven fusion approach challenging deploy varying scenario especially rapidly changing environment address issue propose conditional controllable fusion ccf framework general image fusion task without specific training due dynamic difference different sample ccf employ specific fusion constraint individual practice given powerful generative capability denoising diffusion model first inject specific constraint pretrained ddpm adaptive fusion condition appropriate condition dynamically selected ensure fusion process remains responsive specific requirement reverse diffusion stage thus ccf enables conditionally calibrating fused image step step extensive experiment validate effectiveness general fusion task across diverse scenario competing method without additional training
codecnerf toward fast encoding decoding compact highquality novelview synthesis neural radiance field nerf achieved huge success effectively capturing representing object scene however establish ubiquitous presence everyday medium format image video need fulfill three key objective fast encoding decoding time compact model size highquality rendering despite recent advancement comprehensive algorithm adequately address objective yet fully realized work present codecnerf neural codec nerf representation consisting encoder decoder architecture generate nerf representation single forward pas furthermore inspired recent parameterefficient finetuning approach propose finetuning method efficiently adapt generated nerf representation new test instance leading highquality image rendering compact code size proposed codecnerf newly suggested encodingdecodingfinetuning pipeline nerf achieved unprecedented compression performance remarkable reduction encoding time maintaining improving image quality widely used object datasets
rdsinger referencebased diffusion network singing voice synthesis singing voice synthesis svs aim produce highfidelity singing audio music score requiring detailed understanding note pitch duration unlike texttospeech task although diffusion model shown exceptional performance various generative task like image video creation application svs hindered time complexity challenge capturing acoustic feature particularly pitch transition network learn prior distribution use compressed latent state better start diffusion model denoising step doesnt consistently improve quality entire duration introduce rdsinger referencebased denoising diffusion network generates highquality audio svs task approach inspired animate anyone diffusion image network maintains intricate appearance feature reference image rdsinger utilizes melspectrogram reference mitigate denoising step artifact additionally existing model could influenced misleading information compressed latent state pitch transition address issue applying gaussian blur partial reference melspectrogram adjusting loss weight region extensive ablation study demonstrate efficiency method evaluation opencpop chinese singing dataset show rdsinger outperforms current stateoftheart svs method performance
gameir largescale synthesized groundtruth dataset image restoration gaming content image restoration method like superresolution image synthesis successfully used commercial cloud gaming product like nvidias dlss however restoration gaming content well studied general public discrepancy mainly caused lack groundtruth gaming training data match test case due unique characteristic gaming content common approach generating pseudo training data degrading original hr image result inferior restoration performance work develop gameir largescale highquality computersynthesized groundtruth dataset fill blank targeting two different application first superresolution deferred rendering support gaming solution rendering transferring lr image restoring hr image client side provide lrhr paired groundtruth frame coming video rendered task second novel view synthesis nv support multiview gaming solution rendering transferring part multiview frame generating remaining frame client side task hr frame video scene camera view addition rgb frame gbuffers deferred rendering stage also provided used help restoration furthermore evaluate several sota superresolution algorithm nerfbased nv algorithm dataset demonstrates effectiveness groundtruth gameir data improving restoration performance gaming content also test method incorporating gbuffers additional input information helping superresolution nv release dataset model general public facilitate research restoration method gaming content
network bending diffusion model audiovisual generation paper present first step towards creation tool enables artist create music visualization using pretrained generative machine learning model first investigate application network bending process applying transforms within layer generative network image generation diffusion model utilizing range pointwise tensorwise morphological operator identify number visual effect result various operator including easily recreated standard image editing tool find process allows continuous finegrain control image generation helpful creative application next generate musicreactive video using stable diffusion passing audio feature parameter network bending operator finally comment certain transforms radically shift image possibility learning latent space stable diffusion based transforms
disentangled multimodal brain mr image translation via transformerbased modality infuser multimodal magnetic resonance mr imaging play crucial role disease diagnosis due ability provide complementary information analyzing relationship multimodal image subject acquiring mr modality however expensive scanning session certain mr image may missed depending study protocol typical solution would synthesize missing modality acquired image using generative adversarial network gans yet gans constructed convolutional neural network cnns likely suffer lack global relationship mechanism condition desired modality address work propose transformerbased modality infuser designed synthesize multimodal brain mr image method extract modalityagnostic feature encoder transform modalityspecific feature using modality infuser furthermore modality infuser capture longrange relationship among brain structure leading generation realistic image carried experiment brat dataset translating four mr modality experimental result demonstrate superiority proposed method term synthesis quality addition conducted experiment brain tumor segmentation task different conditioning method
gaussianflow splatting gaussian dynamic content creation creating field gaussian splatting image video challenging task due underconstrained nature optimization draw photometric reference input video regulated generative model directly supervising gaussian motion remains underexplored paper introduce novel concept gaussian flow connects dynamic gaussians pixel velocity consecutive frame gaussian flow efficiently obtained splatting gaussian dynamic image space differentiable process enables direct dynamic supervision optical flow method significantly benefit dynamic content generation novel view synthesis gaussian splatting especially content rich motion hard handled existing method common color drifting issue happens generation also resolved improved guassian dynamic superior visual quality extensive experiment demonstrates method effectiveness quantitative qualitative evaluation show method achieves stateoftheart result task generation novel view synthesis project page httpszergovermindgithubiogaussianflowgithubio
novel hybrid integrated wgan model gradient penalty binary image denoising paper introduces novel approach image denoising leverage advantage generative adversarial network gans specifically propose model combine element model wasserstein gan wgan gradient penalty wgangp hybrid framework seek capitalize denoising capability conditional gans demonstrated model mitigating need exhaustive search optimal hyperparameters could potentially ruin stability learning process proposed method gans generator employed produce denoised image harnessing power conditional gan noise reduction simultaneously implementation lipschitz continuity constraint update featured wgangp aid reducing susceptibility mode collapse innovative design allows proposed model benefit strong point wgangp generating superior denoising result ensuring training stability drawing previous work imagetoimage translation gan stabilization technique proposed research highlight potential gans generalpurpose solution denoising paper detail development testing model showcasing effectiveness numerical experiment dataset created adding synthetic noise clean image numerical result based realworld dataset validation underscore efficacy approach imagedenoising task exhibiting significant enhancement traditional technique notably proposed model demonstrates strong generalization capability performing effectively even trained synthetic noise
advancing weaklysupervised audiovisual video parsing via segmentwise pseudo labeling audiovisual video parsing task aim identify temporally localize event occur either audio visual stream audible video often performs weaklysupervised manner video event label provided ie modality timestamps label unknown due lack densely annotated label recent work attempt leverage pseudo label enrich supervision commonly used strategy generate pseudo label categorizing known video event label modality however label still confined video level temporal boundary event remain unlabeled paper propose new pseudo label generation strategy explicitly assign label video segment utilizing prior knowledge learned open world specifically exploit largescale pretrained model namely clip clap estimate event video segment generate segmentlevel visual audio pseudo label respectively propose new loss function exploit pseudo label taking account categoryrichness segmentrichness label denoising strategy also adopted improve visual pseudo label flipping whenever abnormally large forward loss occur perform extensive experiment llp dataset demonstrate effectiveness proposed design achieve stateoftheart video parsing performance type event parsing ie audio event visual event audiovisual event also examine proposed pseudo label generation strategy relevant weaklysupervised audiovisual event localization task experimental result verify benefit generalization method
ganbased architecture lowdose computed tomography imaging denoising generative adversarial network gans surfaced revolutionary element within domain lowdose computed tomography ldct imaging providing advanced resolution enduring issue reconciling radiation exposure image quality comprehensive review synthesizes rapid advancement ganbased ldct denoising technique examining evolution foundational architecture stateoftheart model incorporating advanced feature anatomical prior perceptual loss function innovative regularization strategy critically analyze various gan architecture including conditional gans cgans cyclegans superresolution gans srgans elucidating unique strength limitation context ldct denoising evaluation provides qualitative quantitative result related improvement performance benchmark clinical datasets metric psnr ssim lpips highlighting positive result discus challenge preventing wider clinical use including interpretability image generated gans synthetic artifact need clinically relevant metric review concludes highlighting essential significance ganbased methodology progression precision medicine via tailored ldct denoising model underlining transformative possibility presented artificial intelligence within contemporary radiological practice
vdma video question answering dynamically generated multiagents technical report provides detailed description approach egoschema challenge egoschema challenge aim identify appropriate response question regarding given video clip paper propose video question answering dynamically generated multiagents vdma method complementary approach existing response generation system employing multiagent system dynamically generated expert agent method aim provide accurate contextually appropriate response report detail stage approach tool employed result experiment
embodimentagnostic action planning via objectpart scene flow observing key robotic action planning understand targetobject motion associated part manipulated end effector propose generate objectpart scene flow extract transformation solve action trajectory diverse embodiment advantage approach derives robot action explicitly object motion prediction yielding robust policy understanding object motion also beyond policy trained embodimentcentric data method embodimentagnostic generalizable across diverse embodiment able learn human demonstration method comprises three component objectpart predictor locate part end effector manipulate rgbd video generator predict future rgbd video trajectory planner extract embodimentagnostic transformation sequence solve trajectory diverse embodiment trained video even without trajectory data method still outperforms existing work significantly prevailing virtual environment metaworld frankakitchen respectively furthermore conducted realworld experiment showing policy trained human demonstration deployed various embodiment
enhancing video transformer action understanding vlmaided training owing ability extract relevant spatiotemporal video embeddings vision transformer vits currently best performing model video action understanding however generalization domain datasets somewhat limited contrast visual language model vlms demonstrated exceptional generalization performance currently unable process video consequently extract spatiotemporal pattern crucial action understanding paper propose fourtiered prompt ftp framework take advantage complementary strength vits vlms retain vits strong spatiotemporal representation ability improve visual encoding comprehensive general aligning vlm output ftp framework add four feature processor focus specific aspect human action video action category action component action description context information vlms employed training inference incurs minimal computation cost approach consistently yield stateoftheart performance instance achieve remarkable accuracy somethingsomething surpassing respectively
investigating eventbased camera video frame interpolation sport slowmotion replay provide thrilling perspective pivotal moment within sport game offering fresh captivating visual experience however capturing slowmotion footage typically demand hightech expensive camera infrastructure deep learning video frame interpolation vfi technique emerged promising avenue capable generating highspeed footage regular camera feed moreover utilization eventbased camera recently gathered attention provide valuable motion information frame enhancing vfi performance work present first investigation eventbased vfi model generating sport slowmotion video particularly design implement bicamera recording setup including rgb eventbased camera capture sport video temporally align spatially register camera experimental validation demonstrates timelens offtheshelf eventbased vfi model effectively generate slowmotion footage sport video first investigation underscore practical utility eventbased camera producing sport slowmotion content lay groundwork future research endeavor domain
headsetoff enabling photorealistic video conferencing economical vr headset virtual reality vr become increasingly popular remote collaboration video conferencing pose challenge user face covered headset existing solution limitation term accessibility paper propose headsetoff novel system achieves photorealistic video conferencing economical vr headset leveraging voicedriven face reconstruction headsetoff consists three main component multimodal predictor generator adaptive controller predictor effectively predicts user future behavior based different modality generator employ voice head motion eye blink animate human face adaptive controller dynamically selects appropriate generator model based tradeoff video quality delay experimental result demonstrate effectiveness headsetoff achieving highquality lowlatency video conferencing economical vr headset
selfenhancing video data management system compositional event large language model technical report complex video query answered decomposing modular subtasks however existing video data management system assume existence predefined module subtask introduce vocaludf novel selfenhancing system support compositional query video without need predefined module vocaludf automatically identifies construct missing module encapsulates userdefined function udfs thus expanding querying capability achieve formulate unified udf model leverage large language model llm aid new udf generation vocaludf handle wide range concept supporting programbased udfs ie python function generated llm distilledmodel udfs lightweight vision model distilled strong pretrained model resolve inherent ambiguity user intent vocaludf generates multiple candidate udfs us active learning efficiently select best one selfenhancing capability vocaludf significantly improves query performance across three video datasets
physical informed driving world model autonomous driving requires robust perception model trained highquality largescale multiview driving video task like object detection segmentation trajectory prediction world model provide costeffective solution generating realistic driving video challenge remain ensuring video adhere fundamental physical principle relative absolute motion spatial relationship like occlusion spatial consistency temporal consistency address propose drivephysica innovative model designed generate realistic multiview driving video accurately adhere essential physical principle three key advancement coordinate system aligner module integrates relative absolute motion feature enhance motion interpretation instance flow guidance module ensures precise temporal consistency via efficient flow extraction box coordinate guidance module improves spatial relationship understanding accurately resolve occlusion hierarchy grounded physical principle achieve stateoftheart performance driving video generation quality fid fvd nuscenes dataset downstream perception task project homepage
future research avenue artificial intelligence digital gaming exploratory report video game natural synergistic application domain artificial intelligence ai system offering potential enhance player experience immersion well providing valuable benchmark virtual environment advance ai technology general report present highlevel overview five promising research pathway applying stateoftheart ai method particularly deep learning digital gaming within context current research landscape objective work outline curated nonexhaustive list encouraging research direction intersection ai video game may serve inspire rigorous comprehensive research effort future discus investigating large language model core engine game agent modelling ii using neural cellular automaton procedural game content generation iii accelerating computationally expensive ingame simulation via deep surrogate modelling iv leveraging selfsupervised learning obtain useful video game state embeddings v training generative model interactive world using unlabelled video data also briefly address current technical challenge associated integration advanced deep learning system video game development indicate key area progress likely beneficial
paragan scalable distributed training framework generative adversarial network recent advance generative artificial intelligence fueled numerous application particularly involving generative adversarial network gans essential synthesizing realistic photo video however efficiently training gans remains critical challenge due computationally intensive numerically unstable nature existing method often require day even week training posing significant resource time constraint work introduce paragan scalable distributed gan training framework leverage asynchronous training asymmetric optimization policy accelerate gan training paragan employ congestionaware data pipeline hardwareaware layout transformation enhance accelerator utilization resulting improvement throughput paragan reduce training time biggan day hour achieving scaling efficiency additionally paragan enables unprecedented highresolution image generation using biggan
evaluating effectiveness attackagnostic feature morphing attack detection morphing attack diversified significantly past year new method based generative adversarial network gans diffusion model posing substantial threat face recognition system recent research demonstrated effectiveness feature extracted large vision model pretrained bonafide data attackagnostic feature detecting deep generative image building investigate potential image representation morphing attack detection mad develop supervised detector training simple binary linear svm extracted feature oneclass detector modeling distribution bonafide feature gaussian mixture model gmm method evaluated across comprehensive set attack various scenario including generalization unseen attack different source datasets printscan data result indicate attackagnostic feature effectively detect morphing attack outperforming traditional supervised oneclass detector literature scenario additionally provide insight strength limitation considered representation discus potential future research direction enhance robustness generalizability approach
dynamicavatars accurate dynamic facial avatar reconstruction precise editing diffusion model generating editing dynamic head avatar crucial task virtual reality film production however existing method often suffer facial distortion inaccurate head movement limited finegrained editing capability address challenge present dynamicavatars dynamic model generates photorealistic moving head avatar video clip parameter associated facial position expression approach enables precise editing novel promptbased editing model integrates userprovided prompt guiding parameter derived large language model llm achieve propose dualtracking framework based gaussian splatting introduce prompt preprocessing module enhance editing stability incorporating specialized gan algorithm connecting control module generates precise guiding parameter llm successfully address limitation existing method additionally develop dynamic editing strategy selectively utilizes specific training datasets improve efficiency adaptability model dynamic editing task
videotoaudio generation hidden alignment generating semantically temporally aligned audio content accordance video input become focal point researcher particularly following remarkable breakthrough texttovideo generation work aim offer insight videotoaudio generation paradigm focusing three crucial aspect vision encoders auxiliary embeddings data augmentation technique beginning foundational model built simple yet surprisingly effective intuition explore various vision encoders auxiliary embeddings ablation study employing comprehensive evaluation pipeline emphasizes generation quality videoaudio synchronization alignment demonstrate model exhibit stateoftheart videotoaudio generation capability furthermore provide critical insight impact different data augmentation method enhancing generation framework overall capacity showcase possibility advance challenge generating synchronized audio semantic temporal perspective hope insight serve stepping stone toward developing realistic accurate audiovisual generation model
nerf view synthesis subjective quality assessment objective metric evaluation neural radiance field nerf groundbreaking computer vision technology enables generation highquality immersive visual content multiple viewpoint capability significant advantage application virtualaugmented reality modelling content creation film entertainment industry however evaluation nerf method pose several challenge including lack comprehensive datasets reliable assessment methodology objective quality metric paper address problem nerf view synthesis nv quality assessment thoroughly conducting rigorous subjective quality assessment test considers several scene class recently proposed nv method additionally performance wide range stateoftheart conventional learningbased fullreference image video quality assessment metric evaluated subjective score subjective study study found error camera pose estimation result spatial misalignment synthesized reference image need corrected applying objective quality metric experimental result analyzed depth providing comparative evaluation several nv method objective quality metric across different class visual scene including real synthetic content frontface camera trajectory
choroidal thinning assessment facial video analysis different feature skin associated various medical condition provide opportunity evaluate monitor body health study created strategy assess choroidal thinning video analysis facial skin video capturing entire facial skin collected participant agerelated macular degeneration amd healthy individual facial video analyzed using videobased transangiosomes imaging photoplethysmography taippg generate facial imaging biomarkers correlated choroidal thickness ct measurement ct patient determined using sweptsource optical coherence tomography ssoct result revealed relationship relative blood pulsation amplitude bpa three typical facial angiosomes cheek sideforehead midforehead average macular ct r p r p r p considering diagnostic threshold newly developed facial video analysis tool effectively distinguished case choroidal thinning normal case yielding area curve finding shed light connection choroidal blood flow facial skin hemodynamics suggests potential predicting vascular disease widely accessible skin imaging data
slotvlm slowfast slot videolanguage modeling videolanguage model vlms powered advancement large language model llm charting new frontier video understanding pivotal challenge development efficient method encapsulate video content set representative token align llm work introduce slotvlm novel framework designed generate semantically decomposed video token term objectwise eventwise visual representation facilitate llm inference particularly design slowfast slot module ie sfslots adaptively aggregate dense video token clip vision encoder set representative slot order take account spatial object detail varied temporal dynamic sfslots built dualbranch structure slowslots branch focus extracting objectcentric slot feature high spatial resolution low slow frame sample rate emphasizing detailed object information conversely fastslots branch engineered learn eventcentric slot high temporal sample rate low spatial resolution feature complementary slot combined form vision context serving input llm efficient question answering experimental result demonstrate effectiveness slotvlm achieves stateoftheart performance video questionanswering
boosting neural representation video conditional decoder implicit neural representation inr emerged promising approach video storage processing showing remarkable versatility across various video task however existing method often fail fully leverage representation capability primarily due inadequate alignment intermediate feature target frame decoding paper introduces universal boosting framework current implicit video representation approach specifically utilize conditional decoder temporalaware affine transform module us frame index prior condition effectively align intermediate feature target frame besides introduce sinusoidal nervlike block generate diverse intermediate feature achieve balanced parameter distribution thereby enhancing model capacity highfrequency informationpreserving reconstruction loss approach successfully boost multiple baseline inr reconstruction quality convergence speed video regression exhibit superior inpainting interpolation result integrate consistent entropy minimization technique develop video codecs based boosted inr experiment uvg dataset confirm enhanced codecs significantly outperform baseline inr offer competitive ratedistortion performance compared traditional learningbased codecs code available httpsgithubcomxinjieqboostingnerv
lita language instructed temporallocalization assistant tremendous progress multimodal large language model llm recent work extended model video input promising instruction following capability however important missing piece temporal localization model accurately answer question identify three key aspect limit temporal localization capability time representation ii architecture iii data address shortcoming proposing language instructed temporallocalization assistant lita following feature introduce time token encode timestamps relative video length better represent time video introduce slowfast token architecture capture temporal information fine temporal resolution emphasize temporal localization data lita addition leveraging existing video datasets timestamps propose new task reasoning temporal localization rtl along dataset activitynetrtl learning evaluating task reasoning temporal localization requires reasoning temporal localization video llm lita demonstrates strong performance challenging task nearly doubling temporal mean intersectionoverunion miou baseline addition show emphasis temporal localization also substantially improves videobased text generation compared existing video llm including relative improvement temporal understanding code available httpsgithubcomnvlabslita
arena patchofinterest vit inference acceleration system edgeassisted video analytics advent edge computing made realtime intelligent video analytics feasible previous work based traditional model architecture eg cnn rnn etc employ various strategy filter nonregionofinterest content minimize bandwidth computation consumption show inferior performance adverse environment recently visual foundation model based transformer shown great performance adverse environment due amazing generalization capability however require large amount computation power limit application realtime intelligent video analytics paper find visual foundation model like vision transformer vit also dedicated acceleration mechanism video analytics end introduce arena endtoend edgeassisted video inference acceleration system based vit leverage capability vit accelerated token pruning offloading feeding patchesofinterest downstream model additionally design adaptive keyframe inference switching algorithm tailored different video capable adapting current video content jointly optimize accuracy bandwidth extensive experiment finding reveal arena boost inference speed average consuming bandwidth respectively high inference accuracy
groupaware parameterefficient updating contentadaptive neural video compression contentadaptive compression crucial enhancing adaptability pretrained neural codec various content although method practical neural image compression nic application neural video compression nvc still limited due two main aspect video compression relies heavily temporal redundancy therefore updating one frame lead significant error accumulating time nvc framework generally complex many large component easy update quickly encoding address previously mentioned challenge developed contentadaptive nvc technique called groupaware parameterefficient updating gpu initially minimize error accumulation adopt groupaware approach updating encoder parameter involves adopting patchbased group picture gop training strategy segment video patchbased gop updated facilitate globally optimized domaintransferable solution subsequently introduce parameterefficient deltatuning strategy achieved integrating several lightweight adapter coding component encoding process serial parallel configuration architectureagnostic module stimulate component large parameter thereby reducing update cost encoding time incorporate gpu latest nvc framework conduct comprehensive experiment whose result showcase outstanding video compression efficiency across four video benchmark adaptability one medical image benchmark
toxvidlm multimodal framework toxicity detection codemixed video era rapidly evolving internet technology surge multimodal content including video expanded horizon online communication however detection toxic content diverse landscape particularly lowresource codemixed language remains critical challenge substantial research addressed toxic content detection textual data realm video content especially nonenglish language relatively underexplored paper address research gap introducing benchmark dataset first kind consisting video codemixed hindienglish utterance collected youtube utterance within dataset meticulously annotated toxicity severity sentiment label developed advanced multimodal multitask framework built toxicity detection video content leveraging language model lm crafted primary objective along additional task conducting sentiment severity analysis toxvidlm incorporates three key module encoder module crossmodal synchronization module multitask module crafting generic multimodal lm customized intricate video classification task experiment reveal incorporating multiple modality video substantially enhances performance toxic content detection achieving accuracy weighted score respectively
shotluck holmes family efficient smallscale large language vision model video captioning summarization video increasingly prominent informationdense medium yet pose substantial challenge language model typical video consists sequence shorter segment shot collectively form coherent narrative shot analogous word sentence multiple data stream information visual auditory data must processed simultaneously comprehension entire video requires understanding visualaudio information shot also requires model link idea shot generate larger allencompassing story despite significant progress field current work often overlook video granular shotbyshot semantic information project propose family efficient large language vision model llvms boost video summarization captioning called shotluck holmes leveraging better pretraining data collection strategy extend ability existing small llvms able understand picture able understand sequence frame specifically show shotluck holmes achieves better performance stateoftheart result video captioning summary task significantly smaller computationally efficient model
thoracic surgery video analysis surgical phase recognition paper present approach surgical phase recognition using video data aiming provide comprehensive understanding surgical procedure automated workflow analysis advent robotic surgery digitized operating room generation vast amount data opened door application machine learning computer vision analysis surgical video among advancement surgical phase recognitionspr stand emerging technology potential recognize assess ongoing surgical scenario summarize surgery evaluate surgical skill offer surgical decision support facilitate medical training paper analyse evaluate framebased video clippingbased phase recognition thoracic surgery dataset consisting class phase specifically utilize imagenet vit imagebased classification videomae baseline model videobased classification show masked video distillationmvd exhibit superior performance achieving accuracy compared achieved imagenet vit finding underscore efficacy videobased classifier imagebased counterpart surgical phase recognition task
sstfb leveraging selfsupervised pretext learning temporal selfattention feature branching realtime video polyp segmentation polyp early cancer indicator assessing occurrence polyp removal critical observed colonoscopy screening procedure generates stream video frame segmenting polyp natural video screening procedure several challenge coexistence imaging artefact motion blur floating debris existing polyp segmentation algorithm developed curated still image datasets represent realworld colonoscopy performance often degrades video data propose video polyp segmentation method performs selfsupervised learning auxiliary task spatialtemporal selfattention mechanism improved representation learning endtoend configuration joint optimisation loss enable network learn discriminative contextual feature video experimental result demonstrate improvement respect several stateoftheart sota method ablation study also confirms choice proposed joint endtoend training improves network accuracy nearly dice similarity coefficient intersectionoverunion compared recently proposed method pns polyppvt respectively result previously unseen video data indicate proposed method generalises
alanavlm multimodal embodied ai foundation model egocentric video understanding ai personal assistant deployed via robot wearable require embodied understanding collaborate human effectively however current visionlanguage model vlms primarily focus thirdperson view video neglecting richness egocentric perceptual experience address gap propose three key contribution first introduce egocentric video understanding dataset evud training vlms video captioning question answering task specific egocentric video second present alanavlm parameter vlm trained using parameterefficient method evud finally evaluate alanavlms capability openeqa challenging benchmark embodied video question answering model achieves stateoftheart performance outperforming opensource model including strong socratic model using planner additionally outperform claude gemini pro vision showcase competitive result compared gemini pro even surpassing latter spatial reasoning research pave way building efficient vlms deployed robot wearable leveraging embodied video understanding collaborate seamlessly human everyday task contributing next generation embodied ai
harnessing llm automated video content analysis exploratory workflow short video depression despite growing interest leveraging large language model llm content analysis current study primarily focused textbased content present work explored potential llm assisting video content analysis conducting case study followed new workflow llmassisted multimodal content analysis workflow encompasses codebook design prompt engineering llm processing human evaluation strategically crafted annotation prompt get llm annotation structured form explanation prompt generate llm explanation better understanding llm reasoning transparency test llm video annotation capability analyzed keyframes extracted youtube short video depression compared llm annotation two human coder found llm higher accuracy object activity annotation emotion genre annotation moreover identified potential limitation llm capability annotating video based finding explore opportunity challenge future research improvement workflow also discus ethical concern surrounding future study based llmassisted video analysis
open vocabulary multilabel video classification pretrained visionlanguage model vlms enabled significant progress open vocabulary computer vision task image classification object detection image segmentation recent work focused extending vlms open vocabulary single label action classification video however previous method fall short holistic video understanding requires ability simultaneously recognize multiple action entity eg object video open vocabulary setting formulate problem open vocabulary multilabel video classification propose method adapt pretrained vlm clip solve task leverage large language model llm provide semantic guidance vlm class label improve open vocabulary performance two key contribution first propose endtoend trainable architecture learns prompt llm generate soft attribute clip textencoder enable recognize novel class second integrate temporal modeling module clip vision encoder effectively model spatiotemporal dynamic video concept well propose novel regularized finetuning technique ensure strong open vocabulary classification performance video domain extensive experimentation showcase efficacy approach multiple benchmark datasets
longvideobench benchmark longcontext interleaved videolanguage understanding large multimodal model lmms processing increasingly longer richer input albeit progress public benchmark available measure development mitigate gap introduce longvideobench questionanswering benchmark feature videolanguage interleaved input hour long benchmark includes varyinglength webcollected video subtitle across diverse theme designed comprehensively evaluate lmms longterm multimodal understanding achieve interpret primary challenge accurately retrieve reason detailed multimodal information long input formulate novel video questionanswering task termed referring reasoning specifically part question contains referring query reference related video context called referred context model required reason relevant video detail referred context following paradigm referring reasoning curate humanannotated multiplechoice question finegrained category establishing one comprehensive benchmark longform video understanding evaluation suggest longvideobench present significant challenge even advanced proprietary model eg opensource counterpart show even larger performance gap addition result indicate model performance benchmark improves capable processing frame positioning longvideobench valuable benchmark evaluating futuregeneration longcontext lmms
accelerating learned video compression via lowresolution representation learning recent year field learned video compression witnessed rapid advancement exemplified latest neural video codecs dcvcdc outperformed upcoming nextgeneration codec ecm term compression ratio despite learned video compression framework often exhibit low encoding decoding speed primarily due increased computational complexity unnecessary highresolution spatial operation hugely hinder application reality work introduce efficiencyoptimized framework learned video compression focus lowresolution representation learning aiming significantly enhance encoding decoding speed firstly diminish computational load reducing resolution interframe propagated feature obtained reused feature decoded frame including iframes implement joint training strategy iframe pframe model improving compression ratio secondly approach efficiently leverage multiframe prior parameter prediction minimizing computation decoding end thirdly revisit application online encoder update oeu strategy highresolution sequence achieving notable improvement compression ratio without compromising decoding efficiency efficiencyoptimized framework significantly improved balance compression ratio speed learned video compression comparison traditional codecs method achieves performance level par lowdecay p configuration reference software vtm furthermore contrasted dcvchem approach delivers comparable compression ratio boosting encoding decoding speed factor respectively rtx method decode frame
performance nonadversarial robustness segment anything model surgical video segmentation fully supervised deep learning dl model surgical video segmentation shown struggle nonadversarial realworld corruption image quality including smoke bleeding low illumination foundation model image segmentation segment anything model sam focus interactive promptbased segmentation move away semantic class thus trained larger diverse data offer outstanding zeroshot generalization appropriate user prompt recently building upon success proposed extend zeroshot interactive segmentation capability independent framebyframe video segmentation paper present first experimental study evaluating performance surgical video data leveraging segstrongc miccai endovis subchallenge dataset assess effectiveness uncorrupted endoscopic sequence evaluate nonadversarial robustness video corrupted image quality simulating smoke bleeding low brightness condition various prompt strategy experiment demonstrate zeroshot manner achieve competitive even superior performance compared fullysupervised deep learning model surgical video data including nonadversarial corruption image quality additionally consistently outperforms original sam medical variant across condition finally framesparse prompting consistently outperform framewise prompting suggesting allowing leverage temporal modeling capability lead coherent accurate segmentation compared frequent prompting
highefficiency neural video compression via hierarchical predictive learning enhanced deep hierarchical video compressiondhvc introduced singlemodel neural video codec operates across broad range bitrates delivering superior compression performance representative method also impressive complexity efficiency enabling realtime processing significantly smaller memory footprint standard gpus remarkable advancement stem use hierarchical predictive coding video frame uniformly transformed multiscale representation hierarchical variational autoencoders specific scale feature representation frame corresponding latent residual variable generated referencing lowerscale spatial feature frame conditionally entropyencoded using probabilistic model whose parameter predicted using samescale temporal reference previous frame lowerscale spatial reference current frame featurespace processing operates lowest highest scale frame completely eliminating need complexityintensive motion estimation compensation technique standard video codecs decade hierarchical approach facilitates parallel processing accelerating encoding decoding support transmissionfriendly progressive decoding making particularly advantageous networked video application presence packet loss source code made available
multivent massive multilingual benchmark eventcentric video retrieval efficiently retrieving synthesizing information largescale multimodal collection become critical challenge however existing video retrieval datasets suffer scope limitation primarily focusing matching descriptive vague query small collection professionally edited englishcentric video address gap introduce textbfmultivent largescale multilingual eventcentric video retrieval benchmark featuring collection news video query targeting specific world event query specifically target information found visual content audio embedded text text metadata video requiring system leverage source succeed task preliminary result show stateoftheart visionlanguage model struggle significantly task alternative approach show promise still insufficient adequately address problem finding underscore need robust multimodal retrieval system effective video retrieval crucial step towards multimodal content understanding generation
spatial meet temporal action recognition video action recognition made significant stride challenge remain effectively using spatial temporal information existing method often focus either spatial feature eg object appearance temporal dynamic eg motion rarely address need comprehensive integration capturing rich temporal evolution video frame preserving spatial detail crucial improving accuracy paper introduce temporal integration motion enhancement time layer novel preprocessing technique designed incorporate temporal information time layer generates new video frame rearranging original sequence preserving temporal order embedding temporally evolving frame single spatial grid size n time n transformation creates new frame balance spatial temporal information making compatible existing video model layer capture rich spatial detail similar existing method n increase temporal information becomes prominent spatial information decrease ensure compatibility model input demonstrate effectiveness time layer integrating popular action recognition model vision transformer video masked autoencoders rgb depth video data experiment show time layer enhances recognition accuracy offering valuable insight video processing task
ophclip hierarchical retrievalaugmented learning ophthalmic surgical videolanguage pretraining surgical practice involves complex visual interpretation procedural skill advanced medical knowledge making surgical visionlanguage pretraining vlp particularly challenging due complexity limited availability annotated data address gap propose ophclip hierarchical retrievalaugmented visionlanguage pretraining framework specifically designed ophthalmic surgical workflow understanding ophclip leverage ophvl dataset constructed largescale comprehensive collection hierarchically structured videotext pair ten thousand different combination attribute surgery phasesoperationsactions instrument medication well advanced aspect like cause eye disease surgical objective postoperative recovery recommendation etc hierarchical videotext correspondence enable ophclip learn finegrained longterm visual representation aligning short video clip detailed narrative description full video structured title capturing intricate surgical detail highlevel procedural insight respectively ophclip also design retrievalaugmented pretraining framework leverage underexplored largescale silent surgical procedure video automatically retrieving semantically relevant content enhance representation learning narrative video evaluation across datasets phase recognition multiinstrument identification show ophclips robust generalization superior performance
unleashing generalization endtoend autonomous driving controllable long video generation using generative model synthesize new data become defacto standard autonomous driving address data scarcity issue though existing approach able boost perception model discover approach fail improve performance planning endtoend autonomous driving model generated video usually less frame spatial temporal inconsistency negligible end propose delphi novel diffusionbased long video generation method shared noise modeling mechanism across multiviews increase spatial consistency featurealigned module achieves precise controllability temporal consistency method generate frame video without loss consistency time longer compared stateoftheart method instead randomly generating new data design sampling policy let delphi generate new data similar failure case improve sample efficiency achieved building failurecase driven framework help pretrained visual language model extensive experiment demonstrates delphi generates higher quality long video surpassing previous stateoftheart method consequentially generating training dataset size framework able go beyond perception prediction task first time best knowledge boost planning performance endtoend autonomous driving model margin
bidirectional m lesion filling synthesis using denoising diffusion implicit modelbased lesion repainting automatic magnetic resonance mr image processing pipeline widely used study people multiple sclerosis pwms encompassing task lesion segmentation brain parcellation however presence lesion often complicates analysis particularly brain parcellation lesion filling commonly used mitigate issue existing lesion filling algorithm often fall short accurately reconstructing realistic lesionfree image vital consistent downstream analysis additionally performance lesion segmentation algorithm often limited insufficient data lesion delineation training label paper propose novel approach leveraging denoising diffusion implicit model ddims m lesion filling synthesis based image inpainting modified ddim architecture trained enables m lesion filing synthesis specifically generate lesionfree flair image containing lesion add lesion flair image healthy subject former essential downstream analysis require lesionfree image latter valuable augmenting training datasets lesion segmentation task validate approach initial experiment paper demonstrate promising result lesion filling synthesis paving way future work
comparative study generative adversarial network image recognition algorithm based deep learning traditional method paper image recognition algorithm based combination deep learning generative adversarial network gan studied compared traditional image recognition method purpose study evaluate advantage application prospect deep learning technology especially gan field image recognition firstly paper review basic principle technique traditional image recognition method including classical algorithm based feature extraction sift hog combination support vector machine svm random forest classifier working principle network structure unique advantage gan image generation recognition introduced order verify effectiveness gan image recognition series experiment designed carried using multiple public image data set training testing experimental result show compared traditional method gan excellent performance processing complex image recognition accuracy antinoise ability specifically gans better able capture highdimensional feature detail image significantly improving recognition performance addition gans show unique advantage dealing image noise partial missing information generating highquality image
gaussian splatting physicsgrounded motion generation generation valuable technology virtual reality digital content creation recent work pushed boundary generation producing highfidelity object inefficient prompt simulating physicsgrounded motion accurately still remain unsolved challenge address challenge present innovative framework utilizes large language model llmrefined prompt diffusion priorsguided gaussian splatting g generating model accurate appearance geometric structure also incorporate continuum mechanicsbased deformation map color regularization synthesize vivid physicsgrounded motion generated gaussians adhering conservation mass momentum integrating generation physicsgrounded motion synthesis framework render photorealistic object exhibit physicsaware motion accurately reflecting behavior object various force constraint across different material extensive experiment demonstrate approach achieves highquality generation realistic physicsgrounded motion
beyond generation unlocking universal editing via selfsupervised finetuning recent advance video generation outpaced progress video editing remains constrained several limiting factor namely task dependency supervision severely limit generality b unnecessary artificial separation generation editing task c high computational cost training video model work propose ues unlocking universal editing via selfsupervision lightweight selfsupervised finetuning strategy transforms generation model unified generationediting system selfsupervised semantic alignment approach establishes dualconditioning mechanism original videotext pair jointly provide visual textual semantics enabling structured learning intrinsic spatiotemporal correspondence key advantage include universality supervisionfree adaptation diverse editing task ii unification generation editing applicable textimagetovideo model iii efficiency via lightweight finetune reduces tunable parameter enable systematic evaluation introduce comprehensive benchmark spanning video across humansanimals environment object comprising editing type scenario extensive experiment show ues enables model without inherent editing capability perform powerful universal editing preserving even enhancing original generation performance
bikvil keypointsbased visual imitation learning bimanual manipulation task visual imitation learning achieved impressive progress learning unimanual manipulation task small set visual observation thanks latest advance computer vision however learning bimanual coordination strategy complex object relation bimanual visual demonstration well generalizing categorical object novel cluttered scene remain unsolved challenge paper extend previous work keypointsbased visual imitation learning bimanual manipulation task proposed bikvil jointly extract socalled emphhybrid masterslave relationship hmsr among object hand bimanual coordination strategy subsymbolic task representation bimanual task representation objectcentric embodimentindependent viewpointinvariant thus generalizing well categorical object novel scene evaluate approach various realworld application showcasing ability learn finegrained bimanual manipulation task small number human demonstration video video source code available httpssitesgooglecomviewbikvil
lngen rectal lymph node generation via anatomical feature accurate segmentation rectal lymph node crucial staging treatment planning rectal cancer however complexity surrounding anatomical structure scarcity annotated data pose significant challenge study introduces novel lymph node synthesis technique aimed generating diverse realistic synthetic rectal lymph node sample mitigate reliance manual annotation unlike direct diffusion method often produce mask discontinuous suboptimal quality approach leverage implicit sdfbased method mask generation ensuring production continuous stable morphologically diverse mask experimental result demonstrate synthetic data significantly improves segmentation performance work highlight potential diffusion model accurately synthesizing structurally complex lesion lymph node rectal cancer alleviating challenge limited annotated data field aiding advancement rectal cancer diagnosis treatment
drdm disentangled representation diffusion model synthesizing realistic person image person image synthesis controllable body pose appearance essential task owing practical need context virtual tryon image editing video production however existing method face significant challenge detail missing limb distortion garment style deviation address issue propose disentangled representation diffusion model drdm generate photorealistic image source portrait specific desired pose appearance first pose encoder responsible encoding pose feature highdimensional space guide generation person image second bodypart subspace decoupling block bsdb disentangles feature different body part source figure feed various layer noise prediction block thereby supplying network rich disentangled feature generating realistic target image moreover inference develop parsing mapbased disentangled classifierfree guided sampling method amplifies conditional signal texture pose extensive experimental result deepfashion dataset demonstrate effectiveness approach achieving pose transfer appearance control
generating rich finegrained mmwave radar data video generalized gesture recognition millimeter wave radar gaining traction recently promising modality enabling pervasive privacypreserving gesture recognition however lack rich finegrained radar datasets hinders progress developing generalized deep learning model gesture recognition across various user posture eg standing sitting position scene remedy resort designing software pipeline exploit wealthy video generate realistic radar data need address challenge simulating diversified finegrained reflection property user gesture end design three key component gesture reflection point generator expands arm skeleton point form human reflection point ii signal simulation model simulates multipath reflection attenuation radar signal output human intensity map iii encoderdecoder model combine sampling module fitting module address difference number distribution point generated realworld radar data generating realistic radar data implement evaluate using video public data source selfcollected realworld radar data demonstrating superiority stateoftheart approach gesture recognition
editable scene simulation autonomous driving via collaborative llmagents scene simulation autonomous driving gained significant attention huge potential generating customized data however existing editable scene simulation approach face limitation term user interaction efficiency multicamera photorealistic rendering external digital asset integration address challenge paper introduces chatsim first system enables editable photorealistic driving scene simulation via natural language command external digital asset enable editing high command flexibilitychatsim leverage large language model llm agent collaboration framework generate photorealistic outcome chatsim employ novel multicamera neural radiance field method furthermore unleash potential extensive highquality digital asset chatsim employ novel multicamera lighting estimation method achieve sceneconsistent asset rendering experiment waymo open dataset demonstrate chatsim handle complex language command generate corresponding photorealistic scene video
enhancing video generation model posttraining data reward conditional guidance design paper focus enhancing diffusionbased texttovideo model posttraining phase distilling highly capable consistency model pretrained model proposed method introduces significant advancement integrating various supervision signal including highquality training data reward model feedback conditional guidance consistency distillation process comprehensive ablation study highlight crucial importance tailoring datasets specific learning objective effectiveness learning diverse reward model enhancing visual quality textvideo alignment additionally highlight vast design space conditional guidance strategy center designing effective energy function augment teacher ode solver demonstrate potential approach extracting motion guidance training datasets incorporating ode solver showcasing effectiveness improving motion quality generated video improved motionrelated metric vbench empirically establishes new stateoftheart result vbench total score surpassing proprietary system kling
automating video thumbnail selection generation multimodal multistage analysis thesis present innovative approach automate video thumbnail selection traditional broadcast content methodology establishes stringent criterion diverse representative aesthetically pleasing thumbnail considering factor like logo placement space incorporation vertical aspect ratio accurate recognition facial identity emotion introduce sophisticated multistage pipeline select candidate frame generate novel image blending video element using diffusion model pipeline incorporates stateoftheart model various task including downsampling redundancy reduction automated cropping face recognition closedeye emotion detection shot scale aesthetic prediction segmentation matting harmonization also leverage large language model visual transformer semantic consistency gui tool facilitates rapid navigation pipeline output evaluate method conducted comprehensive experiment study video proposed set included thumbnail chosen professional designer containing similar image survey participant showed preference method compared manually chosen thumbnail alternative method professional designer reported increase valid candidate compared alternative method confirming approach meet established criterion conclusion finding affirm proposed method accelerates thumbnail creation maintaining highquality standard fostering greater user engagement
textcraftor text encoder image quality controller diffusionbased texttoimage generative model eg stable diffusion revolutionized field content generation enabling significant advancement area like image editing video synthesis despite formidable capability model without limitation still challenging synthesize image aligns well input text multiple run carefully crafted prompt required achieve satisfactory result mitigate limitation numerous study endeavored finetune pretrained diffusion model ie unet utilizing various technology yet amidst effort pivotal question texttoimage diffusion model training remained largely unexplored possible feasible finetune text encoder improve performance texttoimage diffusion model finding reveal instead replacing clip text encoder used stable diffusion large language model enhance proposed finetuning approach textcraftor leading substantial improvement quantitative benchmark human assessment interestingly technique also empowers controllable image generation interpolation different text encoders finetuned various reward also demonstrate textcraftor orthogonal unet finetuning combined improve generative quality
optical generative model generative model cover various application area including image video music synthesis natural language processing molecular design among many others digital generative model become larger scalable inference fast energyefficient manner becomes challenge present optical generative model inspired diffusion model shallow fast digital encoder first map random noise phase pattern serve optical generative seed desired data distribution jointlytrained freespacebased reconfigurable decoder alloptically process generative seed create novel image never seen following target data distribution except illumination power random seed generation shallow encoder optical generative model consume computing power synthesis novel image report optical generation monochrome multicolor novel image handwritten digit fashion product butterfly human face following data distribution mnist fashion mnist celeba datasets respectively achieving overall performance comparable digital neural networkbased generative model experimentally demonstrate optical generative model used visible light generate snapshot novel image handwritten digit fashion product optical generative model might pave way energyefficient scalable rapid inference task exploiting potential optic photonics artificial intelligencegenerated content
nonrigid relative placement dense diffusion task relative placement predict placement one object relation another eg placing mug onto mug rack explicit objectcentric geometric reasoning recent method relative placement made tremendous progress towards dataefficient learning robot manipulation generalizing unseen task variation however yet represent deformable transformation despite ubiquity nonrigid body real world setting first step towards bridging gap propose crossdisplacement extension principle relative placement geometric relationship deformable object present novel visionbased method learn crossdisplacement dense diffusion end demonstrate method ability generalize unseen object instance outofdistribution scene configuration multimodal goal multiple highly deformable task simulation real world beyond scope prior work supplementary information video found
syntheocc synthesize geometriccontrolled street view image semantic mpis advancement autonomous driving increasingly reliant highquality annotated datasets especially task occupancy prediction occupancy label require dense annotation significant human effort paper propose syntheocc denotes diffusion model synthesize photorealistic geometriccontrolled image conditioning occupancy label driving scenario yield unlimited amount diverse annotated controllable datasets application like training perception model simulation syntheocc address critical challenge efficiently encode geometric information conditional input diffusion model approach innovatively incorporates semantic multiplane image mpis provide comprehensive spatially aligned scene description conditioning result syntheocc generate photorealistic multiview image video faithfully align given geometric label semantics voxel space extensive qualitative quantitative evaluation syntheocc nuscenes dataset prove effectiveness generating controllable occupancy datasets serve effective data augmentation perception model
head eye tracking dataset panoramic video rapid development widespread application vrar technology maximizing quality immersive panoramic video service match user personal preference habit become longstanding challenge understanding saliency region user focus based data collected hmds promote multimedia encoding transmission quality assessment time largescale datasets essential researcher developer explore shortlongterm user behavior pattern train ai model related panoramic video however existing panoramic video datasets often include lowfrequency user head eye movement data shortterm video lacking sufficient data analyzing user field view fov generating video saliency region driven practical factor paper present head eye tracking dataset involving user male female watching panoramic video dataset provides detail viewport gaze attention location user besides present statistic sample extracted dataset example deviation head eye movement challenge widely held assumption gaze attention decrease center fov following gaussian distribution analysis reveals consistent downward offset gaze fixation relative fov experimental setting involving multiple user video thats name dataset panonut saliency weighting shaped like donut finally also provide script generates saliency distribution based given head eye coordinate pregenerated saliency distribution map set video collected eye tracking data dataset available website
vtggpt tuningfree zeroshot video temporal grounding gpt video temporal grounding vtg aim locate specific temporal segment untrimmed video based linguistic query existing vtg model trained extensive annotated videotext pair process introduces human bias query also incurs significant computational cost tackle challenge propose vtggpt gptbased method zeroshot vtg without training finetuning reduce prejudice original query employ generate debiased query lessen redundant information video apply transform visual content precise caption finally devise proposal generator postprocessing produce accurate segment debiased query image caption extensive experiment demonstrate vtggpt significantly outperforms sota method zeroshot setting surpasses unsupervised approach notably achieves competitive performance comparable supervised method code available httpsgithubcomyoucanbabyvtggpt
patch spatiotemporal relation prediction video anomaly detection video anomaly detection vad aiming identify abnormality within specific context timeframe crucial intelligent video surveillance system recent deep learningbased vad model shown promising result generating highresolution frame often lack competence preserving detailed spatial temporal coherence video frame tackle issue propose selfsupervised learning approach vad interpatch relationship prediction task specifically introduce twobranch vision transformer network designed capture deep visual feature video frame addressing spatial temporal dimension responsible modeling appearance motion pattern respectively interpatch relationship dimension decoupled interpatch similarity order information patch mitigate memory consumption convert order information prediction task multilabel learning problem interpatch similarity prediction task distance matrix regression problem comprehensive experiment demonstrate effectiveness method surpassing pixelgenerationbased method significant margin across three public benchmark additionally approach outperforms selfsupervised learningbased method
owviscaptor abstractor openworld video instance segmentation captioning propose new task openworld video instance segmentation captioning requires detect segment track describe rich caption never seen object challenging task addressed developing abstractor connect vision model language foundation model concretely connect multiscale visual feature extractor large language model llm developing object abstractor objecttotext abstractor object abstractor consisting prompt encoder transformer block introduces spatiallydiverse openworld object query discover never seen object video interquery contrastive loss encourages diversity object query objecttotext abstractor augmented masked crossattention act bridge object query frozen llm generate rich descriptive objectcentric caption detected object generalized approach surpasses baseline jointly address task openworld video instance segmentation dense video object captioning never seen object objectcentric caption
thqa perceptual quality assessment database talking head realm medium technology digital human gained prominence due rapid advancement computer technology however manual modeling control required majority digital human pose significant obstacle efficient development speechdriven method offer novel avenue manipulating mouth shape expression digital human despite proliferation driving method quality many generated talking head th video remains concern impacting user visual experience tackle issue paper introduces talking head quality assessment thqa database featuring th video generated diverse speechdriven method extensive experiment affirm thqa database richness character speech feature subsequent subjective quality assessment experiment analyze correlation scoring result speechdriven method age gender addition experimental result show mainstream image video quality assessment method limitation thqa database underscoring imperative research enhance th video quality assessment thqa database publicly accessible
learning human motion monocular video via crossmodal manifold alignment learning human motion input fundamental task realm computer vision computer graphic many previous method grapple inherently ambiguous task introducing motion prior learning process however approach face difficulty defining complete configuration prior training robust model paper present videotomotion generator vtm leverage motion prior crossmodal latent feature space alignment human motion input namely video keypoints reduce complexity modeling motion prior model motion data separately upper lower body part additionally align motion data scaleinvariant virtual skeleton mitigate interference human skeleton variation motion prior evaluated aist vtm showcase stateoftheart performance reconstructing human motion monocular video notably vtm exhibit capability generalization unseen view angle inthewild video
enhancing inertial hand based har joint representation language pose synthetic imu due scarcity labeled sensor data har prior research turned video data synthesize inertial measurement unit imu data capitalizing rich activity annotation however generating imu data video present challenge har realworld setting attributed poor quality synthetic imu data limited efficacy subtle finegrained motion paper propose novel multimodal multitask contrastivebased framework approach address issue limited data pretraining procedure us video online repository aiming learn joint representation text pose imu simultaneously employing video data contrastive learning method seek enhance wearable har performance especially recognizing subtle activitiesour experimental finding validate effectiveness approach improving har performance imu data demonstrate model trained synthetic imu data generated video using method surpass existing approach recognizing finegrained activity
officialnv llmgenerated news video dataset multimodal fake news detection news medium especially video news medium penetrated every aspect daily life also brings risk fake news therefore multimodal fake news detection recently garnered increased attention however existing datasets comprised useruploaded video contain excess amount superfluous data introduces noise model training process address issue construct dataset named officialnv comprising officially published news video crawl officially published video augmented use llmsbased generation manual verification thereby expanding dataset also propose new baseline model called ofnvd capture key information multimodal feature glu attention mechanism performs feature enhancement modal aggregation via crossmodal transformer benchmarking dataset baseline demonstrates effectiveness model multimodal news detection
sportoonizer augmenting sport highlight narration visual impact via automatic manga broll generation sport highlight becoming increasingly popular videosharing platform yet crafting sport highlight video challenging requires producing engaging narrative different angle conforming different platform affordances constantly changing audience many content creator therefore create derivative work original sport video manga style enhance expressiveness manually creating inserting tailored mangastyle content still timeconsuming introduce sportoonizer system embedding pipeline automatic generation mangastyle animation highlight sport video insertion original video seamlessly merges dynamic manga sequence liveaction footage enriching visual tapestry deepening narrative scope leveraging genais sportoonizer craft compelling storyline encapsulating intensity sport moment athlete personal journey evaluation study demonstrates integrating manga brolls significantly enhances viewer engagement visual interest emotional connection towards athlete story viewing experience
gemvpc dual graphenhanced multimodal integration video paragraph captioning video paragraph captioning vpc aim generate paragraph caption summarises key event within video despite recent advancement challenge persist notably effectively utilising multimodal signal inherent video addressing longtail distribution word paper introduces novel multimodal integrated caption generation framework vpc leverage information various modality external knowledge base framework construct two graph videospecific temporal graph capturing major event interaction multimodal information commonsense knowledge theme graph representing correlation word specific theme graph serve input transformer network shared encoderdecoder architecture also introduce node selection module enhance decoding efficiency selecting relevant node graph result demonstrate superior performance across benchmark datasets
okami teaching humanoid robot manipulation skill single video imitation study problem teaching humanoid robot manipulation skill imitating single video demonstration introduce okami method generates manipulation plan single rgbd video derives policy execution heart approach objectaware retargeting enables humanoid robot mimic human motion rgbd video adjusting different object location deployment okami us openworld vision model identify taskrelevant object retarget body motion hand pose separately experiment show okami achieves strong generalization across varying visual spatial condition outperforming stateoftheart baseline openworld imitation observation furthermore okami rollout trajectory leveraged train closedloop visuomotor policy achieve average success rate without need laborintensive teleoperation video found website httpsutaustinrplgithubiookami
vidhal benchmarking temporal hallucination vision llm vision large language model vllms widely acknowledged prone hallucination existing research addressing problem primarily confined image input limited exploration videobased hallucination furthermore current evaluation method fail capture nuanced error generated response often exacerbated rich spatiotemporal dynamic video address introduce vidhal benchmark specially designed evaluate videobased hallucination vllms vidhal constructed bootstrapping video instance across wide range common temporal aspect defining feature benchmark lie careful creation caption represent varying level hallucination associated video enable finegrained evaluation propose novel caption ordering task requiring vllms rank caption hallucinatory extent conduct extensive experiment vidhal comprehensively evaluate broad selection model result uncover significant limitation existing vllms regarding hallucination generation benchmark aim inspire research holistic understanding vllm capability particularly regarding hallucination extensive development advanced vllms alleviate problem
temporal contrastive learning video temporal reasoning large visionlanguage model temporal reasoning critical challenge videolanguage understanding requires model align semantic concept consistently across time existing large visionlanguage model lvlms large language model llm excel static task struggle capture dynamic interaction temporal dependency video sequence work propose temporal semantic alignment via dynamic prompting tsadp novel framework enhances temporal reasoning capability dynamic taskspecific prompt temporal contrastive learning tsadp leverage dynamic prompt generator dpg encode finegrained temporal relationship temporal contrastive loss tcl align visual textual embeddings across time evaluate method vidsitu dataset augmented enriched temporal annotation demonstrate significant improvement stateoftheart model task intravideo entity association temporal relationship understanding chronology prediction human evaluation confirm tsadps ability generate coherent semantically accurate description analysis highlight robustness efficiency practical utility tsadp making step forward field videolanguage understanding
ensemble approach shortform video quality assessment using multimodal llm rise shortform video characterized diverse content editing style artifact pose substantial challenge learningbased blind video quality assessment bvqa model multimodal large language model mllms renowned superior generalization capability present promising solution paper focus effectively leveraging pretrained mllm shortform video quality assessment regarding impact preprocessing response variability insight combining mllm bvqa model first investigated frame preprocessing sampling technique influence mllms performance introduced lightweight learningbased ensemble method adaptively integrates prediction mllm stateoftheart bvqa model result demonstrated superior generalization performance proposed ensemble approach furthermore analysis contentaware ensemble weight highlighted video characteristic fully represented existing bvqa model revealing potential direction improve bvqa model
scaling rectified flow transformer highresolution image synthesis diffusion model create data noise inverting forward path data towards noise emerged powerful generative modeling technique highdimensional perceptual data image video rectified flow recent generative model formulation connects data noise straight line despite better theoretical property conceptual simplicity yet decisively established standard practice work improve existing noise sampling technique training rectified flow model biasing towards perceptually relevant scale largescale study demonstrate superior performance approach compared established diffusion formulation highresolution texttoimage synthesis additionally present novel transformerbased architecture texttoimage generation us separate weight two modality enables bidirectional flow information image text token improving text comprehension typography human preference rating demonstrate architecture follows predictable scaling trend correlate lower validation loss improved texttoimage synthesis measured various metric human evaluation largest model outperform stateoftheart model make experimental data code model weight publicly available
ambientaware generation action sound egocentric video generating realistic audio human action important many application creating sound effect film virtual reality game existing approach implicitly assume total correspondence video audio training yet many sound happen offscreen weak correspondence visuals resulting uncontrolled ambient sound hallucination test time propose novel ambientaware audio generation model avldm devise novel audioconditioning mechanism learn disentangle foreground action sound ambient background sound inthewild training video given novel silent video model us retrievalaugmented generation create audio match visual content semantically temporally train evaluate model two inthewild egocentric video datasets epickitchens introduce curated clip actionaudio correspondence model outperforms array existing method allows controllable generation ambient sound even show promise generalizing computer graphic game clip overall approach first focus videotoaudio generation faithfully observed visual content despite training uncurated clip natural background sound
synergizing motion appearance multiscale compensatory codebooks talking head video generation talking head video generation aim generate realistic talking head video preserve person identity source image motion driving video despite promising progress made field remains challenging critical problem generate video accurate pose finegrained facial detail simultaneously essentially facial motion often highly complex model precisely oneshot source face image provide sufficient appearance guidance generation due dynamic pose change tackle problem propose jointly learn motion appearance codebooks perform multiscale codebook compensation effectively refine facial motion condition appearance feature talking face image decoding specifically designed multiscale motion appearance codebooks learned simultaneously unified framework store representative global facial motion flow appearance pattern present novel multiscale motion appearance compensation module utilizes transformerbased codebook retrieval strategy query complementary information two codebooks joint motion appearance compensation entire process produce motion flow greater flexibility appearance feature fewer distortion across different scale resulting highquality talking head video generation framework extensive experiment various benchmark validate effectiveness approach demonstrate superior generation result qualitative quantitative perspective compared stateoftheart competitor
comprehensive review eegtooutput research decoding neural signal image video audio electroencephalography eeg invaluable tool neuroscience offering insight brain activity high temporal resolution recent advancement machine learning generative modeling catalyzed application eeg reconstructing perceptual experience including image video audio paper systematically review eegtooutput research focusing stateoftheart generative method evaluation metric data challenge using prisma guideline analyze study identify key trend challenge opportunity field finding emphasize potential advanced model generative adversarial network gans variational autoencoders vaes transformer highlighting pressing need standardized datasets crosssubject generalization roadmap future research proposed aim improve decoding accuracy broadening realworld application
inloop filtering via trained lookup table inloop filtering ilf key technology removing artifact imagevideo coding standard recently neural networkbased inloop filtering method achieve remarkable coding gain beyond capability advanced video coding standard becomes powerful coding tool candidate future video coding standard however utilization deep neural network brings heavy time computational complexity high demand highperformance hardware challenging apply general us coding scene address limitation inspired exploration image restoration propose efficient practical inloop filtering scheme adopting lookup table lut train dnn inloop filtering within fixed filtering reference range cache output value dnn lut via traversing possible input testing time coding process filtered pixel generated locating input pixel tobefiltered pixel reference pixel interpolating cached filtered pixel value enable large filtering reference range limited storage cost lut introduce enhanced indexing mechanism filtering process clippingfinetuning mechanism training proposed method implemented versatile video coding vvc reference software experimental result show ultrafast fast fast mode proposed method achieves average bdrate reduction intra ai random access ra configuration especially method friendly time computational complexity time increase kmacspixel kb storage cost single model solution may shed light journey practical neural networkbased coding tool evolution
temporal evolution knee osteoarthritis diffusionbased morphing model xray medical image synthesis knee osteoarthritis koa common musculoskeletal disorder significantly affect mobility older adult medical domain image containing temporal data frequently utilized study temporal dynamic statistically monitor disease progression deep learningbased generative model natural image widely researched comparatively method available synthesizing temporal knee xrays work introduce novel deeplearning model designed synthesize intermediate xray image specific patient healthy knee severe koa stage testing phase based healthy knee xray proposed model produce continuous effective sequence koa xray image varying degree severity specifically introduce diffusionbased morphing model modifying denoising diffusion probabilistic model approach integrates diffusion morphing module enabling model capture spatial morphing detail source target knee xray image synthesize intermediate frame along geodesic path hybrid loss consisting diffusion loss morphing loss supervision loss employed demonstrate proposed approach achieves highest temporal frame synthesis performance effectively augmenting data classification model simulating progression koa
predicting point track internet video enables generalizable robot manipulation seek learn generalizable goalconditioned policy enables zeroshot robot manipulation interacting unseen object novel scene without testtime adaptation typical approach rely large amount demonstration data generalization propose approach leverage web video predict plausible interaction plan learns taskagnostic transformation obtain robot action real world predicts track point image move future timesteps based goal trained diverse video web including human robot manipulating everyday object use track prediction infer sequence rigid transforms object manipulated obtain robot endeffector pose executed openloop manner refine openloop plan predicting residual action closed loop policy trained embodimentspecific demonstration show approach combining scalably learned track prediction residual policy requiring minimal indomain robotspecific data enables diverse generalizable robot manipulation present wide array realworld robot manipulation result across unseen task object scene
evaluation strategy efficient ratedistortion nerf streaming neural radiance field nerf revolutionized field visual representation enabling highly realistic detailed scene reconstruction sparse set image nerf us volumetric functional representation map point corresponding color opacity allowing photorealistic view synthesis arbitrary viewpoint despite advancement efficient streaming nerf content remains significant challenge due large amount data involved paper investigates ratedistortion performance two nerf streaming strategy pixelbased neural network nn parameterbased streaming former image coded transmitted throughout network latter respective nerf model parameter coded transmitted instead work also highlight tradeoff complexity performance demonstrating nn parameterbased strategy generally offer superior efficiency making suitable onetomany streaming scenario
coronetgan controlled pruning gans via hypernetworks generative adversarial network gans proven exhibit remarkable performance widely used across many generative computer vision application however unprecedented demand deployment gans resourceconstrained edge device still pose challenge due huge number parameter involved generation process led focused attention area compressing gans existing work use knowledge distillation overhead teacher dependency moreover ability control degree compression method hence propose coronetgan compressing gan using combined strength differentiable pruning method via hypernetworks proposed method provides advantage performing controllable compression training along reducing training time substantial factor experiment done various conditional gan architecture cyclegan signify effectiveness approach multiple benchmark datasets edgestoshoes horsetozebra summertowinter result obtained illustrate approach succeeds outperform baseline zebratohorse summertowinter achieving best fid score respectively yielding highfidelity image across datasets additionally approach also outperforms stateoftheart method achieving better inference time various smartphone chipsets datatypes making feasible solution deployment edge device
neurocine decoding vivid video sequence human brain activties pursuit understand intricacy human brain visual processing reconstructing dynamic visual experience brain activity emerges challenging yet fascinating endeavor recent advancement achieved success reconstructing static image noninvasive brain recording domain translating continuous brain activity video format remains underexplored work introduce neurocine novel dualphase framework targeting inherent challenge decoding fmri data noise spatial redundancy temporal lag framework proposes spatial masking temporal interpolationbased augmentation contrastive learning fmri representation diffusion model enhanced dependent prior noise video generation tested publicly available fmri dataset method show promising result outperforming previous stateoftheart model notable margin respectively decoding brain activity three subject fmri dataset measured ssim additionally attention analysis suggests model aligns existing brain structure function indicating biological plausibility interpretability
efficient digital twin data processing lowlatency multicast short video streaming paper propose novel efficient digital twin dt data processing scheme reduce service latency multicast short video streaming particularly dt constructed emulate analyze user status multicast group update swipe feature abstraction precise measurement model dt data processing developed characterize relationship among dt model size user dynamic user clustering accuracy service latency model consisting dt data processing delay video transcoding delay multicast transmission delay constructed incorporating impact user clustering accuracy finally joint optimization problem dt model size selection bandwidth allocation formulated minimize service latency efficiently solve problem diffusionbased resource management algorithm proposed utilizes denoising technique improve actiongeneration process deep reinforcement learning algorithm simulation result based realworld dataset demonstrate proposed dt data processing scheme outperforms benchmark scheme term service latency
hr human modeling human avatar triangular mesh highresolution texture video recently implicit neural representation widely used generate animatable human avatar however material geometry representation coupled neural network hard edit hinders application traditional graphic engine present framework acquiring human avatar attached highresolution physicallybased material texture triangular mesh monocular video method introduces novel information fusion strategy combine information monocular video synthesize virtual multiview image tackle sparsity input view reconstruct human deformable neural implicit surface extract triangle mesh wellbehaved pose initial mesh next stage addition introduce approach correct bias boundary size coarse mesh extracted finally adapt prior knowledge latent diffusion model superresolution multiview distill decomposed texture experiment show approach outperforms previous representation term high fidelity explicit result support deployment common renderers
frequencyguided diffusion model perturbation training skeletonbased video anomaly detection video anomaly detection essential yet challenging openset task computer vision often addressed leveraging reconstruction proxy task however existing reconstructionbased method encounter challenge two main aspect limited model robustness openset scenario overemphasis restricted capacity detailed motion reconstruction end propose novel frequencyguided diffusion model perturbation training enhances model robustness perturbation training emphasizes principal motion component guided motion frequency specifically first use trainable generator produce perturbative sample perturbation training diffusion model perturbation training phase model robustness enhanced domain reconstructed model broadened training generator subsequently perturbative sample introduced inference impact reconstruction normal abnormal motion differentially thereby enhancing separability considering motion detail originate highfrequency information propose masking method based discrete cosine transform separate highfrequency information lowfrequency information guided highfrequency information observed motion diffusion model focus generating lowfrequency information thus reconstructing motion accurately experimental result five video anomaly detection datasets including humanrelated openset benchmark demonstrate effectiveness proposed method code available httpsgithubcomxiaofengtanfgdmadcode
mededit counterfactual diffusionbased image editing brain mri denoising diffusion probabilistic model enable highfidelity image synthesis editing biomedicine model facilitate counterfactual image editing producing pair image one edited simulate hypothetical condition example model progression specific disease stroke lesion however current image editing technique often fail generate realistic biomedical counterfactuals either inadequately modeling indirect pathological effect like brain atrophy excessively altering scan disrupts correspondence original image propose mededit conditional diffusion model medical image editing mededit induces pathology specific area balancing modeling disease effect preserving integrity original scan evaluated mededit atlas stroke dataset using frechet inception distance dice score outperforming stateoftheart diffusionbased method palette sdedit additionally clinical evaluation boardcertified neuroradiologist confirmed mededit generated realistic stroke scan indistinguishable real one believe work enable counterfactual image editing research advance development realistic clinically useful imaging tool
tutorial diffusion model imaging vision astonishing growth generative tool recent year empowered many exciting application texttoimage generation texttovideo generation underlying principle behind generative tool concept diffusion particular sampling mechanism overcome shortcoming deemed difficult previous approach goal tutorial discus essential idea underlying diffusion model target audience tutorial includes undergraduate graduate student interested research diffusion model applying model solve problem
gaia rethinking action quality assessment aigenerated video assessing action quality imperative challenging due significant impact quality aigenerated video complicated inherently ambiguous nature action within aigenerated video aigv current action quality assessment aqa algorithm predominantly focus action real specific scenario pretrained normative action feature thus rendering inapplicable aigvs address problem construct gaia generic aigenerated action dataset conducting largescale subjective evaluation novel causal reasoningbased perspective resulting rating among videoaction pair based gaia evaluate suite popular texttovideo model ability generate visually rational action revealing pro con different category action also extend gaia testbed benchmark aqa capacity existing automatic evaluation method result show traditional aqa method actionrelated metric recent benchmark mainstream video quality method perform poorly average srcc respectively indicating sizable gap current model human action perception pattern aigvs finding underscore significance action quality unique perspective studying aigvs catalyze progress towards method enhanced capacity aqa aigvs
larp tokenizing video learned autoregressive generative prior present larp novel video tokenizer designed overcome limitation current video tokenization method autoregressive ar generative model unlike traditional patchwise tokenizers directly encode local visual patch discrete token larp introduces holistic tokenization scheme gather information visual content using set learned holistic query design allows larp capture global semantic representation rather limited local patchlevel information furthermore offer flexibility supporting arbitrary number discrete token enabling adaptive efficient tokenization based specific requirement task align discrete token space downstream ar generation task larp integrates lightweight ar transformer trainingtime prior model predicts next token discrete latent space incorporating prior model training larp learns latent space optimized video reconstruction also structured way conducive autoregressive generation moreover process defines sequential order discrete token progressively pushing toward optimal configuration training ensuring smoother accurate ar generation inference time comprehensive experiment demonstrate larps strong performance achieving stateoftheart fvd classconditional video generation benchmark larp enhances compatibility ar model video open potential build unified highfidelity multimodal large language model mllms
opensora plan opensource large video generation model introduce opensora plan opensource project aim contribute large generation model generating desired highresolution video long duration based various user input project comprises multiple component entire video generation process including waveletflow variational autoencoder joint imagevideo skiparse denoiser various condition controller moreover many assistant strategy efficient training inference designed multidimensional data curation pipeline proposed obtaining desired highquality data benefiting efficient thought opensora plan achieves impressive video generation result qualitative quantitative evaluation hope careful design practical experience inspire video generation research community code model weight publicly available urlhttpsgithubcompkuyuangroupopensoraplan
dsplats generation denoising splatsbased multiview diffusion model generating highquality content requires model capable learning robust distribution complex scene realworld object within recent gaussianbased reconstruction technique achieved impressive result recovering highfidelity asset sparse input image predicting gaussians feedforward manner however technique often lack extensive prior expressiveness offered diffusion model hand diffusion model successfully applied denoise multiview image show potential generating wide range photorealistic output still fall short explicit prior consistency work aim bridge two approach introducing dsplats novel method directly denoises multiview image using gaussian splatbased reconstructors produce diverse array realistic asset harness extensive prior diffusion model incorporate pretrained latent diffusion model reconstructor backbone predict set gaussians additionally explicit representation embedded denoising network provides strong inductive bias ensuring geometrically consistent novel view generation qualitative quantitative experiment demonstrate dsplats produce highquality spatially consistent output also set new standard singleimage reconstruction evaluated google scanned object dataset dsplats achieves psnr ssim lpips
temporalbench benchmarking finegrained temporal understanding multimodal video model understanding finegrained temporal dynamic crucial multimodal video comprehension generation due lack finegrained temporal annotation existing video benchmark mostly resemble static image benchmark incompetent evaluating model temporal understanding paper introduce temporalbench new benchmark dedicated evaluating finegrained temporal understanding video temporalbench consists video questionanswer pair derived highquality human annotation detailing temporal dynamic video clip result benchmark provides unique testbed evaluating various temporal understanding reasoning ability action frequency motion magnitude event order etc moreover enables evaluation various task like video question answering captioning short long video understanding well different model multimodal video embedding model text generation model result show stateoftheart model like achieve question answering accuracy temporalbench demonstrating significant gap human ai temporal understanding furthermore notice critical pitfall multichoice qa llm detect subtle change negative caption find centralized description cue prediction propose multiple binary accuracy mba correct bias hope temporalbench foster research improving model temporal reasoning capability dataset evaluation code made available
world model effective data machine driving scene representation closedloop simulation essential advancing endtoend autonomous driving system contemporary sensor simulation method nerf rely predominantly condition closely aligned training data distribution largely confined forwarddriving scenario consequently method face limitation rendering complex maneuver eg lane change acceleration deceleration recent advancement autonomousdriving world model demonstrated potential generate diverse driving video however approach remain constrained video generation inherently lacking spatiotemporal coherence required capture intricacy dynamic driving environment paper introduce enhances driving scene representation leveraging world model prior specifically utilize world model data machine synthesize novel trajectory video structured condition explicitly leveraged control spatialtemporal consistency traffic element besides cousin data training strategy proposed facilitate merging real synthetic data optimizing knowledge first utilize video generation model improving reconstruction driving scenario experimental result reveal significantly enhances generation quality novel trajectory view achieving relative improvement fid compared pvg deformablegs moreover markedly enhances spatiotemporal coherence driving agent verified comprehensive user study relative increase ntaiou metric
volumetric saliency guided image summarization rgbd indoor scene classification image summary abridged version original visual content used represent scene thus task scene classification identification indexing etc performed efficiently using unique summary saliency commonly used technique generating relevant image summary however definition saliency subjective nature depends upon application existing saliency detection method using rgbd data mainly focus color texture depth feature consequently generated summary contains either foreground object nonstationary object however application scene identification require stationary characteristic scene unlike stateoftheart method paper proposes novel volumetric saliencyguided framework indoor scene classification result highlight efficacy proposed method
towards unbiased robust spatiotemporal scene graph generation anticipation spatiotemporal scene graph stsgs provide concise expressive representation dynamic scene modeling object evolving relationship time however realworld visual relationship often exhibit longtailed distribution causing existing method task like video scene graph generation vidsgg scene graph anticipation sga produce biased scene graph end propose impartail novel training framework leverage loss masking curriculum learning mitigate bias generation anticipation spatiotemporal scene graph unlike prior method add extra architectural component learn unbiased estimator propose impartial training objective reduces dominance head class learning focus underrepresented tail relationship curriculumdriven mask generation strategy empowers model adaptively adjust bias mitigation strategy time enabling balanced robust estimation thoroughly assess performance various distribution shift also introduce two new task robust spatiotemporal scene graph generation robust scene graph anticipation offering challenging benchmark evaluating resilience stsg model extensive experiment action genome dataset demonstrate superior unbiased performance robustness method compared existing baseline
mechanism generative imagetoimage translation network generative adversarial network gans class neural network widely used field imagetoimage translation paper propose streamlined imagetoimage translation network simpler architecture compared existing model investigate relationship gans autoencoders provide explanation efficacy employing gan component task involving image translation show adversarial gan model yield result comparable existing method without additional complex loss penalty subsequently elucidate rationale behind phenomenon also incorporate experimental result demonstrate validity finding
dream improving videotext retrieval relevancebased augmentation using large foundation model recent progress videotext retrieval driven largely advancement model architecture training strategy however representation learning capability videotext retrieval model remain constrained lowquality limited training data annotation address issue present novel videotext retrieval paradigm relevancebased augmentation namely dream enhances video text data using large foundation model learn generalized feature specifically first adopt simple augmentation method generates selfsimilar data randomly duplicating dropping subwords frame addition inspired recent advancement visual language generative model propose robust augmentation method textual paraphrasing video stylization using large language model llm visual generative model vgms enrich video text information propose relevancebased augmentation method llm vgms generate integrate new relevant information original data leveraging enriched data extensive experiment several videotext retrieval benchmark demonstrate superiority dream existing method
learning online scale transformation talking head video generation oneshot talking head video generation us source image driving video create synthetic video source person facial movement imitate driving video however difference scale source driving image remain challenge face reenactment existing method attempt locate frame driving video aligns best source image imprecise alignment result suboptimal outcome end introduce scale transformation module automatically adjust scale driving image fit source image using information scale difference maintained detected keypoints source image driving frame furthermore keep perceiving scale information face generation process incorporate scale information learned scale transformation module layer generation process produce final result accurate scale method perform accurate motion transfer two image without anchor frame achieved contribution proposed online scale transformation facial reenactment network extensive experiment demonstrated proposed method adjusts scale driving face automatically according source face generates highquality face accurate scale crossidentity facial reenactment
towards open domain textdriven synthesis multiperson motion work aim generate natural diverse group motion multiple human textual description singleperson texttomotion generation extensively studied remains challenging synthesize motion one two subject inthewild prompt mainly due lack available datasets work curate human pose motion datasets estimating pose information largescale image video datasets model use transformerbased diffusion framework accommodates multiple datasets number subject frame experiment explore generation multiperson static pose generation multiperson motion sequence knowledge method first generate multisubject motion sequence high diversity fidelity large variety textual prompt
upsample guidance scale diffusion model without training diffusion model demonstrated superior performance across various generative task including image video audio however encounter difficulty directly generating highresolution sample previously proposed solution issue involve modifying architecture training partitioning sampling process multiple stage method limitation able directly utilize pretrained model asis requiring additional work paper introduce upsample guidance technique adapts pretrained diffusion model eg generate higherresolution image eg adding single term sampling process remarkably technique necessitate additional training relying external model demonstrate upsample guidance applied various model pixelspace latent space video diffusion model also observed proper selection guidance scale improve image quality fidelity prompt alignment
pseudomriguided pet image reconstruction method based diffusion probabilistic model anatomically guided pet reconstruction using mri information shown potential improve pet image quality however improvement limited pet scan paired mri information work employed diffusion probabilistic model dpm infer deepmri image fdgpet brain image use dpmgenerated guide pet reconstruction model trained brain fdg scan tested datasets containing multiple level count deepmri image appeared somewhat degraded acquired mri image regarding pet image quality volume interest analysis different brain region showed pet reconstructed image using acquired deepmri image improved image quality compared osem conclusion found analysing decimated datasets subjective evaluation performed two physician confirmed osem scored consistently worse mriguided pet image significant difference observed mriguided pet image proof concept show possible infer dpmbased mri imagery guide pet reconstruction enabling possibility changing reconstruction parameter strength prior anatomically guided pet reconstruction absence mri
reading frame multimodal depression detection video nonverbal cue depression prominent contributor global disability affect substantial portion population effort detect depression social medium text prevalent yet work explored depression detection usergenerated video content work address research gap proposing simple flexible multimodal temporal model capable discerning nonverbal depression cue diverse modality noisy realworld video show inthewild video using additional highlevel nonverbal cue crucial achieving good performance extracted processed audio speech embeddings face emotion embeddings face body hand landmark gaze blinking information extensive experiment show model achieves stateoftheart result three key benchmark datasets depression detection video substantial margin code publicly available github
network structural dense displacement based deformable mesh model optical flow study proposes network recognize displacement rc frame structure video monocular camera proposed network consists two module pofrnnet used generate dense optical flow well pofrnnet extract pose parameter h convert two video frame dense optical flow pofrnnet inputted dense optical flow output pose parameter h displacement point structure calculated parameter h fast fourier transform fft applied obtain frequency domain signal corresponding displacement signal furthermore comparison truth displacement first floor first video shown study finally predicted displacement four floor rc frame structure given three video exhibited last study
depthaware testtime training zeroshot video object segmentation zeroshot video object segmentation zsvos aim segmenting primary moving object without human annotation mainstream solution mainly focus learning single model largescale video datasets struggle generalize unseen video work introduce testtime training ttt strategy address problem key insight enforce model predict consistent depth ttt process detail first train single network perform segmentation depth prediction task effectively learned specifically designed depth modulation layer ttt process model updated predicting consistent depth map frame different data augmentation addition explore different ttt weight updating strategy empirical result suggest momentumbased weight initialization loopingbased training scheme lead stable improvement experiment show proposed method achieves clear improvement zsvos proposed video ttt strategy provides significant superiority stateoftheart ttt method code available httpsnifangbaagegithubiodattt
temporally consistent unbalanced optimal transport unsupervised action segmentation propose novel approach action segmentation task long untrimmed video based solving optimal transport problem encoding temporal consistency prior gromovwasserstein problem able decode temporally consistent segmentation noisy affinitymatching cost matrix video frame action class unlike previous approach method require knowing action order video attain temporal consistency furthermore resulting fused gromovwasserstein problem efficiently solved gpus using iteration projected mirror descent demonstrate effectiveness method unsupervised learning setting method used generate pseudolabels selftraining evaluate segmentation approach unsupervised learning pipeline breakfast youtube instruction desktop assembly datasets yielding stateoftheart result unsupervised video action segmentation task
transformerbased model prediction human gaze behavior video eyetracking application utilize human gaze video understanding task become increasingly important effectively automate process video analysis based eyetracking data important accurately replicate human gaze behavior however task present significant challenge due inherent complexity ambiguity human gaze pattern work introduce novel method simulating human gaze behavior approach us transformerbased reinforcement learning algorithm train agent act human observer primary role watching video simulating human gaze behavior employed eyetracking dataset gathered video generated virtualhome simulator primary focus activity recognition experimental result demonstrate effectiveness gaze prediction method highlighting capability replicate human gaze behavior applicability downstream task real humangaze used input
leveraging temporal contextualization video action recognition propose novel framework video understanding called temporally contextualized clip tcclip leverage essential temporal information global interaction spatiotemporal domain within video specific introduce temporal contextualization tc layerwise temporal information infusion mechanism video extract core information frame connects relevant information across frame summarization context token leverage context token feature encoding furthermore videoconditional prompting vp module process context token generate informative prompt text modality extensive experiment zeroshot fewshot basetonovel fullysupervised action recognition validate effectiveness model ablation study tc vp support design choice project page source code available httpsgithubcomnaveraitcclip
schvppnet spatial channel hybridattention video postprocessing network cnn transformer convolutional neural network cnn transformer attracted much attention recently video postprocessing vpp however interaction cnn transformer existing vpp method fully explored leading inefficient communication local global extracted feature paper explore interaction cnn transformer task vpp propose novel spatial channel hybridattention video postprocessing network schvppnet cooperatively exploit image prior spatial channel domain specifically spatial domain novel spatial attention fusion module designed two attention weight generated fuse local global representation collaboratively channel domain novel channel attention fusion module developed blend deep representation channel dimension dynamically extensive experiment show schvppnet notably boost video restoration quality average bitrate saving u v component ra configuration
csta cnnbased spatiotemporal attention video summarization video summarization aim generate concise representation video capturing essential content key moment reducing overall length although several method employ attention mechanism handle longterm dependency often fail capture visual significance inherent frame address limitation propose cnnbased spatiotemporal attention csta method stack feature frame single video form imagelike frame representation applies cnn frame feature methodology relies cnn comprehend inter intraframe relation find crucial attribute video exploiting ability learn absolute position within image contrast previous work compromising efficiency designing additional module focus spatial importance csta requires minimal computational overhead us cnn sliding window extensive experiment two benchmark datasets summe tvsum demonstrate proposed approach achieves stateoftheart performance fewer mac compared previous method code available
contextenhanced video moment retrieval large language model current method video moment retrieval vmr struggle align complex situation involving specific environmental detail character description action narrative tackle issue propose large language modelguided moment retrieval lmr approach employ extensive knowledge large language model llm improve video context representation well crossmodal alignment facilitating accurate localization target moment specifically lmr introduces context enhancement technique llm generate crucial targetrelated context semantics semantics integrated visual feature producing discriminative video representation finally languageconditioned transformer designed decode freeform language query fly using aligned video representation moment retrieval extensive experiment demonstrate lmr achieves stateoftheart result outperforming nearest competitor challenging qvhighlights charadessta benchmark respectively importantly performance gain significantly higher localization complex query
compressed video quality enhancement temporal group alignment fusion paper propose temporal group alignment fusion network enhance quality compressed video using longshort term correlation frame proposed model consists intragroup feature alignment intragfa module intergroup feature fusion intergff module feature enhancement fe module form group picture gop selecting frame video according temporal distance target enhanced frame grouping composed gop contain either long shortterm correlated information neighboring frame design intragfa module align feature frame gop eliminate motion existing frame construct intergff module fuse feature belonging different gop finally enhance fused feature fe module generate highquality video frame experimental result show proposed method achieves gain lower complexity compared stateoftheart method
computational thinking design pattern video game prior research explored potential application video game programming education elicit computational thinking skill however existing approach often either general taking account diversity genre mechanism video game narrow selecting tool specifically designed educational purpose paper propose fundamental approach defining beneficial connection individual design pattern present video game computational thinking skill argue video game capacity elicit skill even potentially train could effective method solidify conceptual base would make programming education effective
audiodriven highresolution seamless talking head video editing via stylegan existing method audiodriven talking head video editing limitation poor visual effect paper try tackle problem editing talking face image seamless different emotion based two module audiotolandmark module consisting crossreconstructed emotion disentanglement alignment network module bridge gap speech facial motion predicting corresponding emotional landmark speech landmarkbased editing module edits face video via stylegan aim generate seamless edited video consisting emotion content component input audio extensive experiment confirm compared stateofthearts method method provides highresolution video high visual quality
orthogonal hypercategory guided multiinterest elicitation microvideo matching watching microvideos becoming part public daily life usually user watching behavior thought rooted multiple different interest paper propose model named opal microvideo matching elicits user multiple heterogeneous interest disentangling multiple soft hard interest embeddings user interaction moreover opal employ twostage training strategy pretrain generate soft interest historical interaction guidance orthogonal hypercategories microvideos finetune reinforce degree disentanglement among interest learn temporal evolution interest user conduct extensive experiment two realworld datasets result show opal return diversified microvideos also outperforms six stateoftheart model term recall hit rate
mtfl multitimescale feature learning weaklysupervised anomaly detection surveillance video detection anomaly event relevant public safety requires combination finegrained motion information contextual event variable timescales end propose multitimescale feature learning mtfl method enhance representation anomaly feature short medium long temporal tubelets employed extract spatiotemporal video feature using video swin transformer experimental result demonstrate mtfl outperforms stateoftheart method ucfcrime dataset achieving anomaly detection performance auc moreover performs complementary sota auc shanghaitech ap xdviolence dataset furthermore generate extended dataset ucfcrime development evaluation wider range anomaly namely video anomaly detection dataset vadd involving video class extensive coverage realistic anomaly
elastictok adaptive tokenization image video efficient video tokenization remains key bottleneck learning general purpose vision model capable processing long video sequence prevailing approach restricted encoding video fixed number token token result overly lossy encoding many token result prohibitively long sequence length work introduce elastictok method condition prior frame adaptively encode frame variable number token enable computationally scalable way propose masking technique drop random number token end framess token encoding inference elastictok dynamically allocate token needed complex data leverage token simpler data need token empirical evaluation image video demonstrate effectiveness approach efficient token usage paving way future development powerful multimodal model world model agent
agenttosim learning interactive behavior model casual longitudinal video present agenttosim at framework learning interactive behavior model agent casual longitudinal video collection different prior work rely markerbased tracking multiview camera at learns natural behavior animal human agent noninvasively video observation recorded long timespan eg month single environment modeling behavior agent requires persistent tracking eg knowing point corresponds long time period obtain data develop coarsetofine registration method track agent camera time canonical space resulting complete persistent spacetime representation train generative model agent behavior using paired data perception motion agent queried reconstruction at enables realtosim transfer video recording agent interactive behavior simulator demonstrate result pet eg cat dog bunny human given monocular rgbd video captured smartphone
enhancing multimodal affective analysis learned live comment feature live comment also known danmaku usergenerated message synchronized video content comment overlay directly onto streaming video capturing viewer emotion reaction realtime prior work leveraged live comment affective analysis use limited due relative rarity live comment across different video platform address first construct live comment affective analysis lcaffect dataset contains live comment english chinese video spanning diverse genre elicit wide spectrum emotion using dataset use contrastive learning train video encoder produce synthetic live comment feature enhanced multimodal affective content analysis comprehensive experimentation wide range affective analysis task sentiment emotion recognition sarcasm detection english chinese demonstrate synthetic live comment feature significantly improve performance stateoftheart method
eventguided lowlight video semantic segmentation recent video semantic segmentation vss method demonstrated promising result welllit environment however performance significantly drop lowlight scenario due limited visibility reduced contextual detail addition unfavorable lowlight condition make harder incorporate temporal consistency across video frame thus lead video flickering effect compared conventional camera event camera capture motion dynamic filter temporalredundant information robust lighting condition end propose evsnet lightweight framework leverage event modality guide learning unified illuminationinvariant representation specifically leverage motion extraction module extract shortterm longterm temporal motion event modality motion fusion module integrate image feature motion feature adaptively furthermore use temporal decoder exploit video context generate segmentation prediction design evsnet result lightweight architecture achieving sota performance experimental result largescale datasets demonstrate proposed evsnet outperforms sota method higher parameter efficiency
referring video object segmentation via languagealigned track selection referring video object segmentation rvos requires tracking segmenting object throughout video according given natural language expression demanding complex motion understanding alignment visual representation language description given challenge recently proposed segment anything model emerges potential candidate due ability generate coherent segmentation mask track across video frame provide inherent spatiotemporal objectness object token representation paper introduce sola selection object language alignment novel framework leverage object token compact videolevel object representation aligned language feature lightweight track selection module effectively facilitate alignment propose ioubased pseudolabeling strategy bridge modality gap representation language feature extensive experiment show sola achieves stateoftheart performance mevis dataset demonstrate sola offer effective solution rvos project page available httpscvlabkaistgithubiosola
experimental study lowlatency video streaming oran setup generative ai video streaming service depend underlying communication infrastructure available network resource offer ultralow latency highquality content delivery open radio access network oran provides dynamic programmable flexible ran architecture configured support requirement timecritical application work considers setup constrained network resource supplemented glsgai glsmec technique order reach satisfactory video quality specifically implement novel semantic control channel enables glsmec support lowlatency application tight coupling among oran xapp glsmec control channel proposed concept experimentally verified actual oran setup support video streaming performance evaluation includes glspsnr metric endtoend latency finding reveal latency adjustment yield gain image glspsnr underscoring tradeoff potential optimized video quality resourcelimited environment
esvqa perceptual quality assessment egocentric spatial video rapid development extended reality xr egocentric spatial shooting display technology enhanced immersion engagement user assessing quality experience qoe egocentric spatial video crucial ensure highquality viewing experience however corresponding research still lacking paper use embodied experience highlight immersive experience study new problem ie embodied perceptual quality assessment egocentric spatial video specifically introduce first egocentric spatial video quality assessment database esvqad comprises egocentric spatial video mean opinion score moss furthermore propose novel multidimensional binocular feature fusion model termed esvqanet integrates binocular spatial motion semantic feature predict perceptual quality experimental result demonstrate esvqanet outperforms stateoftheart vqa model embodied perceptual quality assessment task exhibit strong generalization capability traditional vqa task database code released upon publication
genxd generating scene recent development visual generation remarkably successful however generation remain challenging realworld application due lack largescale data effective model design paper propose jointly investigate general generation leveraging camera object movement commonly observed daily life due lack realworld data community first propose data curation pipeline obtain camera pose object motion strength video based pipeline introduce largescale realworld scene dataset leveraging data develop framework genxd allows u produce scene propose multiviewtemporal module disentangle camera object movement seamlessly learn data additionally genxd employ masked latent condition support variety conditioning view genxd generate video follow camera trajectory well consistent view lifted representation perform extensive evaluation across various realworld synthetic datasets demonstrating genxds effectiveness versatility compared previous method generation
nonlinear inverse design mechanical multimaterial metamaterials enabled video denoising diffusion structure identifier metamaterials synthetic material customized property emerged promising field due advancement additive manufacturing material derive unique mechanical property internal lattice structure often composed multiple material repeat geometric pattern traditional inverse design approach shown potential struggle map nonlinear material behavior multiple possible structural configuration paper present novel framework leveraging video diffusion model type generative artificial intelligence ai inverse multimaterial design based nonlinear stressstrain response approach consists two key component field generator using video diffusion model create solution field based target nonlinear stressstrain response structure identifier employing two unet model determine corresponding multimaterial design incorporating multiple material plasticity large deformation innovative design method allows enhanced control highly nonlinear mechanical behavior metamaterials commonly seen realworld application offer promising solution generating nextgeneration metamaterials finely tuned mechanical characteristic
nexttoken prediction need nexttoken prediction considered promising path towards artificial general intelligence struggled excel multimodal task still dominated diffusion model eg stable diffusion compositional approach eg clip combined llm paper introduce new suite stateoftheart multimodal model trained solely nexttoken prediction tokenizing image text video discrete space train single transformer scratch mixture multimodal sequence outperforms several wellestablished taskspecific model generation perception task surpassing flagship model sdxl eliminating need diffusion compositional architecture also capable generating highfidelity video via predicting next token video sequence simplify complex multimodal model design converging singular focus token unlocking great potential scaling training inference result demonstrate nexttoken prediction promising path towards building general multimodal intelligence beyond language opensource key technique model support research direction
ct synthesis conditional diffusion model abdominal lymph node segmentation despite significant success achieved deep learning method medical image segmentation researcher still struggle computeraided diagnosis abdominal lymph node due complex abdominal environment small indistinguishable lesion limited annotated data address problem present pipeline integrates conditional diffusion model lymph node generation nnunet model lymph node segmentation improve segmentation performance abdominal lymph node synthesizing diversity realistic abdominal lymph node data propose lnddpm conditional denoising diffusion probabilistic model ddpm lymph node ln generation lnddpm utilizes lymph node mask anatomical structure mask model condition condition work two conditioning mechanism global structure conditioning local detail conditioning distinguish lymph node surroundings better capture lymph node characteristic obtained paired abdominal lymph node image mask used downstream segmentation task experimental result abdominal lymph node datasets demonstrate lnddpm outperforms generative method abdominal lymph node image synthesis better assist downstream abdominal lymph node segmentation task
pegasus personalized generative avatar composable attribute present pegasus method constructing personalized generative face avatar monocular video source generative avatar enables disentangled control selectively alter facial attribute eg hair nose preserving identity approach consists two stage synthetic database generation constructing personalized generative avatar generate synthetic video collection target identity varying facial attribute video synthesized borrowing attribute monocular video diverse identity build personspecific generative avatar modify attribute continuously preserving identity extensive experiment demonstrate method generating synthetic database creating generative avatar effective preserving identity achieving high realism subsequently introduce zeroshot approach achieve goal generative modeling efficiently leveraging previously constructed personalized generative model
refdrop controllable consistency image video generation via reference feature guidance rapidly growing interest controlling consistency across multiple generated image using diffusion model among various method recent work found simply manipulating attention module concatenating feature multiple reference image provides efficient approach enhancing consistency without finetuning despite popularity success study elucidated underlying mechanism contribute effectiveness work reveal popular approach linear interpolation image selfattention crossattention synthesized content reference feature constant coefficient motivated observation find coefficient necessary simplifies controllable generation mechanism resulting algorithm coin refdrop allows user control influence reference context direct precise manner besides enhancing consistency singlesubject image generation method also enables interesting application consistent generation multiple subject suppressing specific feature encourage diverse content highquality personalized video generation boosting temporal consistency even compared stateoftheart imagepromptbased generator ipadapter refdrop competitive term controllability quality avoiding need train separate image encoder feature injection reference image making versatile plugandplay solution image video diffusion model
improving textconditioned latent diffusion cancer pathology development generative model past decade allowed hyperrealistic data synthesis potentially beneficial synthetic data generation process relatively underexplored cancer histopathology one algorithm synthesising realistic image diffusion iteratively convert image noise learns recovery process noise wang vastola effective highly computationally expensive highresolution image rendering infeasible histopathology development variational autoencoders vaes allowed u learn representation complex highresolution image latent space vital byproduct ability compress highresolution image space recover lossless marriage diffusion vaes allows u carry diffusion latent space autoencoder enabling u leverage realistic generative capability diffusion maintaining reasonable computational requirement rombach et al yellapragada et al build foundational model task paving way generate realistic histopathology image paper discus pitfall current method namely yellapragada et al resolve critical error proposing improvement along way method achieve fid score beating sota counterpart yellapragada et al fid presenting traintime gpu memory usage reduction
aesopagent agentdriven evolutionary system storytovideo production agent aigc artificial intelligence generated content technology recently made significant progress propose aesopagent agentdriven evolutionary system storytovideo production aesopagent practical application agent technology multimodal content generation system integrates multiple generative capability within unified framework individual user leverage module easily innovative system would convert user story proposal script image audio integrate multimodal content video additionally animating unit eg sora could make video infectious aesopagent system could orchestrate task workflow video generation ensuring generated video rich content coherent system mainly contains two layer ie horizontal layer utility layer horizontal layer introduce novel ragbased evolutionary system optimizes whole video generation workflow step within workflow continuously evolves iteratively optimizes workflow accumulating expert experience professional knowledge including optimizing llm prompt utility usage utility layer provides multiple utility leading consistent image generation visually coherent term composition character style meanwhile provides audio special effect integrating expressive logically arranged video overall aesopagent achieves stateoftheart performance compared many previous work visual storytelling aesopagent designed convenient service individual user available following page httpsaesopaigithubio
ne videomusic database dataset symbolic video game music paired gameplay video neural model one popular approach music generation yet arent standard large datasets tailored learning music directly game data address research gap introduce novel dataset named nesvmdb containing gameplay video ne game paired original soundtrack symbolic format midi nesvmdb built upon nintendo entertainment system music database nesmdb encompassing music piece ne game approach involves collecting longplay video game original dataset slicing clip extracting audio clip subsequently apply audio fingerprinting algorithm similar shazam automatically identify corresponding piece nesmdb dataset additionally introduce baseline method based controllable music transformer generate ne music conditioned gameplay clip evaluated approach objective metric result showed conditional cmt improves musical structural quality compared unconditional counterpart moreover used neural classifier predict game genre generated piece result showed cmt generator learn correlation gameplay video game genre research conducted achieve humanlevel performance
aniclipart clipart animation texttovideo prior clipart premade art form offer convenient efficient way creating visual content however traditional workflow animating static clipart laborious timeconsuming involving step like rigging keyframing inbetweening recent advancement texttovideo generation hold great potential resolving challenge nevertheless direct application texttovideo model often struggle preserve visual identity clipart generate cartoonstyle motion resulting subpar animation outcome paper introduce aniclipart computational system convert static clipart highquality animation guided texttovideo prior generate natural smooth coherent motion first parameterize motion trajectory keypoints defined initial clipart image cubic bezier curve align motion trajectory given text prompt optimizing video score distillation sampling sd loss skeleton fidelity loss incorporating differentiable asrigidaspossible arap shape deformation differentiable rendering aniclipart endtoend optimized maintaining deformation rigidity extensive experimental result show proposed aniclipart consistently outperforms competing method term textvideo alignment visual identity preservation temporal consistency additionally showcase versatility aniclipart adapting generate layered animation allow topological change
sentimentoriented transformerbased variational autoencoder network live video commenting automatic live video commenting increasing attention due significance narration generation topic explanation etc however diverse sentiment consideration generated comment missing current method sentimental factor critical interactive commenting lack research far thus paper propose sentimentoriented transformerbased variational autoencoder sotvae network consists sentimentoriented diversity encoder module batch attention module achieve diverse video commenting multiple sentiment multiple semantics specifically sentimentoriented diversity encoder elegantly combine vae random mask mechanism achieve semantic diversity sentiment guidance fused crossmodal feature generate live video comment furthermore batch attention module also proposed paper alleviate problem missing sentimental sample caused data imbalance common live video popularity video varies extensive experiment livebot videoic datasets demonstrate proposed sotvae outperforms stateoftheart method term quality diversity generated comment related code available
fancyvideo towards dynamic consistent video generation via crossframe textual guidance synthesizing motionrich temporally consistent video remains challenge artificial intelligence especially dealing extended duration existing texttovideo model commonly employ spatial crossattention text control equivalently guiding different frame generation without framespecific textual guidance thus model capacity comprehend temporal logic conveyed prompt generate video coherent motion restricted tackle limitation introduce fancyvideo innovative video generator improves existing textcontrol mechanism welldesigned crossframe textual guidance module ctgm specifically ctgm incorporates temporal information injector tii temporal affinity refiner tar temporal feature booster tfb beginning middle end crossattention respectively achieve framespecific textual guidance firstly tii injects framespecific information latent feature text condition thereby obtaining crossframe textual condition tar refines correlation matrix crossframe textual condition latent feature along time dimension lastly tfb boost temporal consistency latent feature extensive experiment comprising quantitative qualitative evaluation demonstrate effectiveness fancyvideo video demo code model available
lip lying spotting temporal inconsistency audio visual lipsyncing deepfakes recent year deepfake technology achieved unprecedented success highquality video synthesis method also pose potential severe security threat humanity deepfake bifurcated entertainment application like face swapping illicit us lipsyncing fraud however lipforgery video neither change identity discernible visual artifact present formidable challenge existing deepfake detection method preliminary experiment shown effectiveness existing method often drastically decrease even fail tackling lipsyncing video paper first time propose novel approach dedicated lipforgery identification exploit inconsistency lip movement audio signal also mimic human natural cognition capturing subtle biological link lip head region boost accuracy better illustrate effectiveness advance proposed method create highquality lipsync dataset avlips employing stateoftheart lip generator hope highquality diverse dataset could well served research challenging interesting field experimental result show approach give average accuracy spotting lipsyncing video significantly outperforming baseline extensive experiment demonstrate capability tackle deepfakes robustness surviving diverse input transformation method achieves accuracy realworld scenario eg wechat video call show powerful capability real scenario deployment facilitate progress research community release resource httpsgithubcomaaroncomolipfd
sequential posterior sampling diffusion model diffusion model quickly risen popularity ability model complex distribution perform effective posterior sampling unfortunately iterative nature generative model make computationally expensive unsuitable realtime sequential inverse problem ultrasound imaging considering strong temporal structure across sequence frame propose novel approach model transition dynamic improve efficiency sequential diffusion posterior sampling conditional image synthesis modeling sequence data using video vision transformer vivit transition model based previous diffusion output initialize reverse diffusion trajectory lower noise scale greatly reducing number iteration required convergence demonstrate effectiveness approach realworld dataset high frame rate cardiac ultrasound image show achieves performance full diffusion trajectory accelerating inference enabling realtime posterior sampling furthermore show addition transition model improves psnr case severe motion method open new possibility realtime application diffusion model imaging domain requiring realtime inference
stochastic deep restoration prior imaging inverse problem deep neural network trained image denoisers widely used prior solving imaging inverse problem gaussian denoising thought sufficient learning image prior show prior deep model pretrained general restoration operator perform better introduce stochastic deep restoration prior sharp novel method leverage ensemble restoration model regularize inverse problem sharp improves upon method using gaussian denoiser prior better handling structured artifact enabling selfsupervised training even without fully sampled data prove sharp minimizes objective function involving regularizer derived score function minimum mean square error mmse restoration operator theoretically analyze convergence empirically sharp achieves stateoftheart performance task magnetic resonance imaging reconstruction singleimage superresolution surpassing denoiserand diffusionmodelbased method without requiring retraining
cyclic perceptual loss crossmodal image synthesis mri taupet alzheimers disease ad common form dementia characterised cognitive decline biomarkers tauproteins taupositron emission tomography taupet employ radiotracer selectively bind detect visualise tau protein aggregate within brain valuable early ad diagnosis less accessible due high cost limited availability invasive nature image synthesis neural network enables generation taupet image accessible magnetic resonance imaging mri image ensure highquality image synthesis propose cyclic perceptual loss combined mean squared error structural similarity index measure ssim loss cyclic perceptual loss sequentially calculates axial average perceptual loss specified number epoch followed coronal sagittal plane number epoch sequence cyclically performed interval reducing cycle repeat conduct supervised synthesis taupet image mri image using paired mri taupet image adni database collected data perform preprocessing including intensity standardisation taupet image manufacturer proposed loss applied generative unet variant outperformed perceptual loss ssim peak signaltonoise ratio psnr addition including cyclic perceptual loss original loss ganbased image synthesis model cyclegan improves ssim psnr least furthermore bymanufacturer pet standardisation help model synthesising highquality image minmax pet normalisation
accelerating vision diffusion transformer skip branch diffusion transformer dit emerging image video generation model architecture demonstrated great potential high generation quality scalability property despite impressive performance practical deployment constrained computational complexity redundancy sequential denoising process feature caching across timesteps proven effective accelerating diffusion model application dit limited fundamental architectural difference unetbased approach empirical analysis dit feature dynamic identify significant feature variation dit block present key challenge feature reusability address convert standard dit skipdit skip branch enhance feature smoothness introduce skipcache utilizes skip branch cache dit feature across timesteps inference time validated effectiveness proposal different dit backbone video image generation showcasing skip branch help preserve generation quality achieve higher speedup experimental result indicate skipdit achieves speedup almost free speedup minor reduction quantitative metric code available httpsgithubcomopensparsellmsskipditgit
physicsbased scene layout generation human motion creating scene captured motion achieve realistic humanscene interaction crucial animation movie video game character motion often captured bluescreened studio without real furniture object place may discrepancy planned motion captured one give rise need automatic scene layout generation relieve burden selecting positioning furniture object previous approach avoid artifact like penetration floating due lack physical constraint furthermore heavily rely specific data learn contact affordances restricting generalization ability different motion work present physicsbased approach simultaneously optimizes scene layout generator simulates moving human physic simulator attain plausible realistic interaction motion method explicitly introduces physical constraint automatically recover generate scene layout minimize motion tracking error identify object afford interaction use reinforcement learning perform dualoptimization character motion imitation controller scene layout generator facilitate optimization reshape tracking reward devise pose prior guidance obtained estimated pseudocontact label evaluate method using motion samp prox demonstrate physically plausible scene layout reconstruction compared previous kinematicsbased method
expressionaware video inpainting hmd removal xr application headmounted display hmds serve indispensable device observing extended reality xr environment virtual content however hmds present obstacle external recording technique block upper face user limitation significantly affect social xr application specifically teleconferencing facial feature eye gaze information play vital role creating immersive user experience study propose new network expressionaware video inpainting hmd removal evihrnet based generative adversarial network gans model effectively fill missing information regard facial landmark single occlusionfree reference image user framework component ensure preservation user identity across frame using reference frame improve level realism inpainted output introduce novel facial expression recognition fer loss function emotion preservation result demonstrate remarkable capability proposed framework remove hmds facial video maintaining subject facial expression identity moreover output exhibit temporal consistency along inpainted frame lightweight framework present practical approach hmd occlusion removal potential enhance various collaborative xr application without need additional hardware
functional imaging constrained diffusion brain pet synthesis structural mri magnetic resonance imaging mri positron emission tomography pet increasingly used multimodal analysis neurodegenerative disorder mri broadly utilized clinical setting pet less accessible many study attempted use deep generative model synthesize pet mri scan however often suffer unstable training inadequately preserve brain functional information conveyed pet end propose functional imaging constrained diffusion ficd framework brain pet image synthesis paired structural mri input condition new constrained diffusion model cdm ficd introduces noise pet progressively remove cdm ensuring high output fidelity throughout stable training phase cdm learns predict denoised pet functional imaging constraint introduced ensure voxelwise alignment denoised pet ground truth quantitative qualitative analysis conducted subject paired mri fdgpet scan suggest ficd achieves superior performance generating fdgpet data compared stateoftheart method validate effectiveness proposed ficd data total subject three downstream task experimental result suggesting utility generalizability
opflowtalker realistic natural talking face generation via optical flow guidance creating realistic natural lipreadable talking face video remains formidable challenge previous research primarily concentrated generating aligning singleframe image overlooking smoothness frametoframe transition temporal dependency often compromised visual quality effect practical setting particularly handling complex facial data audio content frequently led semantically incongruent visual illusion specifically synthesized video commonly featured disorganized lip movement making difficult understand recognize overcome limitation paper introduces application optical flow guide facial image generation enhancing interframe continuity semantic consistency propose opflowtalker novel approach utilizes predicted optical flow change audio input rather direct image prediction method smooth image transition aligns change semantic content moreover employ sequence fusion technique replace independent generation single frame thus preserving contextual information maintaining temporal coherence also developed optical flow synchronization module regulates fullface lip movement optimizing visual synthesis balancing regional dynamic furthermore introduce visual text consistency score vtcs accurately measure lipreadability synthesized video extensive empirical evidence validates effectiveness approach
mcgan enhancing gan training regressionbased generator loss generative adversarial network gans emerged powerful tool generating highfidelity data however main bottleneck existing approach lack supervision generator training often result undamped oscillation unsatisfactory performance address issue propose algorithm called monte carlo gan mcgan approach utilizing innovative generative loss function termly regression loss reformulates generator training regression task enables generator training minimizing mean squared error discriminator output real data expected discriminator fake data demonstrate desirable analytic property regression loss including discriminability optimality show method requires weaker condition discriminator effective generator training property justify strength approach improve training stability retaining optimality gan leveraging strong supervision regression loss extensive experiment diverse datasets including image data imagenet lsun bedroom time series data var stock data video data conducted demonstrate flexibility effectiveness proposed mcgan numerical result show proposed mcgan versatile enhancing variety backbone gan model achieves consistent significant improvement term quality accuracy training stability learned latent space
selfsupervised learning video representation child perspective child learn powerful internal model world around year egocentric visual experience internal model learned child visual experience highly generic learning algorithm require strong inductive bias recent advance collecting largescale longitudinal developmentally realistic video datasets generic selfsupervised learning ssl algorithm allowing u begin tackle nature v nurture question however existing work typically focus imagebased ssl algorithm visual capability learned static image eg object recognition thus ignoring temporal aspect world close gap train selfsupervised video model longitudinal egocentric headcam recording collected child two year period early development month resulting model highly effective facilitating learning action concept small number labeled example favorable data size scaling property display emergent video interpolation capability video model also learn accurate robust object representation imagebased model trained exact data result suggest important temporal aspect child internal model world may learnable visual experience using highly generic learning algorithm without strong inductive bias
sentimentenhanced graphbased sarcasm explanation dialogue sarcasm explanation dialogue sed new yet challenging task aim generate natural language explanation given sarcastic dialogue involves multiple modality ie utterance video audio although existing study achieved great success based generative pretrained language model bart overlook exploiting sentiment residing utterance video audio play important role reflecting sarcasm essentially involves subtle sentiment contrast nevertheless nontrivial incorporate sentiment boosting sed performance due three main challenge diverse effect utterance token sentiment gap videoaudio sentiment signal embedding space bart various relation among utterance utterance sentiment videoaudio sentiment tackle challenge propose novel sentimentenhanced graphbased multimodal sarcasm explanation framework named edge particular first propose lexiconguided utterance sentiment inference module heuristic utterance sentiment refinement strategy devised develop module named joint cross attentionbased sentiment inference jcasi extending multimodal sentiment analysis model jca derive joint sentiment label videoaudio clip thereafter devise contextsentiment graph comprehensively model semantic relation among utterance utterance sentiment videoaudio sentiment facilitate sarcasm explanation generation extensive experiment publicly released dataset wit verify superiority model cuttingedge method
picture worth thousand word exploring diagram videobased oop exercise counter llm overreliance much research highlighted impressive capability large language model llm like gpt bard solving introductory programming exercise recent work shown llm effectively solve range complex objectoriented programming oop exercise textbased specification raise concern academic integrity student might use model complete assignment unethically neglecting development important skill program design problemsolving computational thinking address propose innovative approach formulating oop task using diagram video way foster problemsolving deter student copyandprompt approach oop course introduce novel notation system specifying oop assignment encompassing structural behavioral requirement assess use classroom setting semester student perception approach explored survey generally student responded positively diagram video videobased project better received diagrambased exercise notation appears several benefit student investing effort understanding diagram feeling motivated engage videobased project furthermore student reported less inclined rely llmbased code generation tool diagram videobased exercise experiment bard vision ability revealed currently fall short interpreting diagram generate accurate code solution
rap retrievalaugmented planner adaptive procedure planning instructional video procedure planning instructional video entail generating sequence action step based visual observation initial target state despite rapid progress task remain several critical challenge solved adaptive procedure prior work hold unrealistic assumption number action step known fixed leading nongeneralizable model realworld scenario sequence length varies temporal relation understanding step temporal relation knowledge essential producing reasonable executable plan annotation cost annotating instructional video steplevel label ie timestamp sequencelevel label ie action category demanding laborintensive limiting generalizability largescale datasets work propose new practical setting called adaptive procedure planning instructional video procedure length fixed predetermined address challenge introduce retrievalaugmented planner rap model specifically adaptive procedure rap adaptively determines conclusion action using autoregressive model architecture temporal relation rap establishes external memory module explicitly retrieve relevant stateaction pair training video revise generated procedure tackle high annotation cost rap utilizes weaklysupervised learning manner expand training dataset taskrelevant unannotated video generating pseudo label action step experiment crosstask coin benchmark show superiority rap traditional fixedlength model establishing strong baseline solution adaptive procedure planning
video meeting change expression facial expression change speak video call given two unpaired set video people seek automatically find spatiotemporal pattern distinctive set existing method use discriminative approach perform posthoc explainability analysis method insufficient unable provide insight beyond obvious dataset bias explanation useful human good task instead tackle problem lens generative domain translation method generates detailed report learned inputdependent spatiotemporal feature extent vary domain demonstrate method discover behavioral difference conversing facetoface videocalls vcs also show applicability method discovering difference presidential communication style additionally able predict temporal changepoints video decouple expression unsupervised way increase interpretability usefulness model finally method generative used transform video call appear recorded setting experiment visualization show approach able discover range behavior taking step towards deeper understanding human behavior
popcat propagation particle complex annotation task novel dataset creation multiobject tracking crowdcounting industrialbased video arduous timeconsuming faced unique class densely populates video sequence propose time efficient method called popcat exploit multitarget temporal feature video data produce semisupervised pipeline segmentation boxbased video annotation method retains accuracy level associated human level annotation generating large volume semisupervised annotation greater generalization method capitalizes temporal feature use particle tracker expand domain humanprovided target point done use particle tracker reassociate initial point set image follow labeled frame yolo model trained generated data rapidly infers target video evaluation conducted animaltrack benchmark multitarget video trackingdetection set contain multiple similarlooking target camera movement feature would commonly seen wild situation specifically choose difficult datasets demonstrate efficacy pipeline comparison purpose method applied animaltrack visdrone show margin improvement best result value metric collected
adapting videolanguage model generalizable robotic reward via failure prompt generalpurpose robot operate reality executing broad range instruction across various environment imperative central reinforcement learning planning robotic agent generalizable reward function recent advance visionlanguage model clip shown remarkable performance domain deep learning paving way opendomain visual recognition however collecting data robot executing various language instruction across multiple environment remains challenge paper aim transfer videolanguage model robust generalization generalizable languageconditioned reward function utilizing robot video data minimal amount task singular environment unlike common robotic datasets used training reward function human videolanguage datasets rarely contain trivial failure video enhance model ability distinguish successful failed robot execution cluster failure video feature enable model identify pattern within cluster integrate newly trained failure prompt text encoder represent corresponding failure mode languageconditioned reward function show outstanding generalization new environment new instruction robot planning reinforcement learning
everything video unifying modality nextframe prediction multimodal learning involves integrating information various modality text image audio video pivotal numerous complex task like visual question answering crossmodal retrieval caption generation traditional approach rely modalityspecific encoders late fusion technique hinder scalability flexibility adapting new task modality address limitation introduce novel framework extends concept task reformulation beyond natural language processing nlp multimodal learning propose reformulate diverse multimodal task unified nextframe prediction problem allowing single model handle different modality without modalityspecific component method treat input output sequential frame video enabling seamless integration modality effective knowledge transfer across task approach evaluated range task including texttotext imagetotext videotovideo videototext audiototext demonstrating model ability generalize across modality minimal adaptation show task reformulation significantly simplify multimodal model design across various task laying groundwork generalized multimodal foundation model
bovila bootstrapping videolanguage alignment via llmbased selfquestioning answering development multimodal model rapidly advancing demonstrating remarkable capability however annotating videotext pair remains expensive insufficient take video question answering videoqa task example human annotated question answer often cover part video similar semantics also expressed different text form leading underutilization video address propose bovila selftraining framework augments question sample training llmbased selfquestioning answering help model exploit video information internal knowledge llm thoroughly improve modality alignment filter bad selfgenerated question introduce evidential deep learning edl estimate uncertainty assess quality selfgenerated question evaluating modality alignment within context best knowledge work first explore llmbased selftraining framework modality alignment evaluate bovila five strong videoqa benchmark outperforms several stateoftheart method demonstrate effectiveness generality additionally provide extensive analysis selftraining framework edlbased uncertainty filtering mechanism code made available httpsgithubcomdunknsabswbovila
followyourclick opendomain regional image animation via short prompt despite recent advance imagetovideo generation better controllability local animation less explored existing imagetovideo method locally aware tend move entire scene however human artist may need control movement different object region additionally current method require user describe target motion also provide redundant detailed description frame content two issue hinder practical utilization current tool paper propose practical framework named followyourclick achieve image animation simple user click specifying move short motion prompt specifying move technically propose firstframe masking strategy significantly improves video generation quality motionaugmented module equipped short motion prompt dataset improve short prompt following ability model control motion speed propose flowbased motion magnitude control control speed target movement precisely framework simpler yet precise user control better generation performance previous method extensive experiment compared baseline including commercial tool research method metric suggest superiority approach project page httpsfollowyourclickgithubio
dual conditioned motion diffusion posebased video anomaly detection video anomaly detection vad essential computer vision research existing vad method utilize either reconstructionbased predictionbased framework former excels detecting irregular pattern structure whereas latter capable spotting abnormal deviation trend address posebased video anomaly detection introduce novel framework called dual conditioned motion diffusion dcmd enjoys advantage approach dcmd integrates conditioned motion conditioned embedding comprehensively utilize pose characteristic latent semantics observed movement respectively reverse diffusion process motion transformer proposed capture potential correlation multilayered characteristic within spectrum space human motion enhance discriminability normal abnormal instance design novel united association discrepancy uad regularization primarily relies gaussian kernelbased time association selfattentionbased global association finally mask completion strategy introduced inference stage reverse diffusion process enhance utilization conditioned motion prediction branch anomaly detection extensive experiment four datasets demonstrate method dramatically outperforms stateoftheart method exhibit superior generalization performance
weaklysupervised pet anomaly detection using implicitlyguided attentionconditional counterfactual diffusion modeling multicenter multicancer multitracer study minimizing need pixellevel annotated data train pet lesion detection segmentation network highly desired transformative given time cost constraint associated expert annotation current unweaklysupervised anomaly detection method rely autoencoder generative adversarial network trained healthy data however ganbased network challenging train due issue simultaneous optimization two competing network mode collapse etc paper present weaklysupervised implicitly guided counterfactual diffusion model detecting anomaly pet image igcondapet solution developed validated using pet scan six retrospective cohort consisting total case containing local public datasets training conditioned image class label healthy v unhealthy via attention module employ implicit diffusion guidance perform counterfactual generation facilitates unhealthytohealthy domain translation generating synthetic healthy version unhealthy input image enabling detection anomaly calculated difference performance method compared several deep learning based weaklysupervised unsupervised method well traditional method like suvmax thresholding also highlight importance incorporating attention module network detection small anomaly code publicly available httpsgithubcomahxmedsigcondapetgit
interactivevideo usercentric controllable video generation synergistic multimodal instruction introduce textitinteractivevideo usercentric framework video generation different traditional generative approach operate based userprovided image text framework designed dynamic interaction allowing user instruct generative model various intuitive mechanism whole generation process eg text image prompt painting draganddrop etc propose synergistic multimodal instruction mechanism designed seamlessly integrate user multimodal instruction generative model thus facilitating cooperative responsive interaction user input generative process approach enables iterative finegrained refinement generation result precise effective user instruction textitinteractivevideo user given flexibility meticulously tailor key aspect video paint reference image edit semantics adjust video motion requirement fully met code model demo available
generating seamless virtual immunohistochemical whole slide image content color consistency immunohistochemical ihc stain play vital role pathologist analysis medical image providing crucial diagnostic information various disease virtual staining hematoxylin eosin hestained whole slide image wsis allows automatic production useful ihc stain without expensive physical staining process however current virtual wsi generation method based tilewise processing often suffer inconsistency content texture color tile boundary inconsistency lead artifact compromise image quality potentially hinder accurate clinical assessment diagnosis address limitation propose novel consistent wsi synthesis network ccwsinet extends gan model produce seamless synthetic whole slide image ccwsinet integrates content colorconsistency supervisor ensuring consistency across tile facilitating generation seamless synthetic wsis ensuring immunohistochemistry accuracy melanocyte detection validate method extensive imagequality analysis objective detection assessment subjective survey pathologist generating highquality synthetic wsis method open door advanced virtual staining technique broader application research clinical care
learning physical property gaussians via video diffusion recent year rapid development generation model opening new possibility application simulating dynamic movement object customizing behavior however current generative model tend focus surface feature color shape neglecting inherent physical property govern behavior object real world accurately simulate physicsaligned dynamic essential predict physical property material incorporate behavior prediction process nonetheless predicting diverse material realworld object still challenging due complex nature physical attribute paper propose novel method learning various physical property object video diffusion model approach involves designing highly generalizable physical simulation system based viscoelastic material model enables u simulate wide range material highfidelity capability moreover distill physical prior video diffusion model contains understanding realistic object material extensive experiment demonstrate effectiveness method elastic plastic material show great potential bridging gap physical world virtual neural space providing better integration application realistic physical principle virtual environment project page
magicmirror fast highquality avatar generation constrained search space introduce novel framework human avatar generation personalization leveraging text prompt enhance user engagement customization central approach key innovation aimed overcoming challenge photorealistic avatar synthesis firstly utilize conditional neural radiance field nerf model trained largescale unannotated multiview dataset create versatile initial solution space accelerates diversifies avatar generation secondly develop geometric prior leveraging capability texttoimage diffusion model ensure superior view invariance enable direct optimization avatar geometry foundational idea complemented optimization pipeline built variational score distillation vsd mitigates texture loss oversaturation issue supported extensive experiment strategy collectively enable creation custom avatar unparalleled visual quality better adherence input text prompt find result video website httpssyntecresearchgithubiomagicmirror
temporal spatial super resolution latent diffusion model medical mri image super resolution sr play critical role computer vision particularly medical imaging hardware acquisition time constraint often result low spatial temporal resolution diffusion model applied spatial temporal sr study explored use joint spatial temporal sr particularly medical imaging work address gap proposing use latent diffusion model ldm combined vector quantised gan vqganbased encoderdecoder architecture joint super resolution frame sr image denoising problem focusing improving spatial temporal resolution medical image using cardiac mri dataset data science bowl cardiac challenge consisting cine image spatial resolution slice per timestep demonstrate effectiveness approach ldm model achieves peak signal noise ratio psnr structural similarity index ssim learned perceptual image patch similarity lpips outperforming simple baseline method psnr ssim lpips ldm model generates image high fidelity perceptual quality diffusion step result suggest ldms hold promise advancing super resolution medical imaging potentially enhancing diagnostic accuracy patient outcome code link also shared
ditfastattn attention compression diffusion transformer model diffusion transformer dit excel image video generation face computational challenge due quadratic complexity selfattention operator propose ditfastattn posttraining compression method alleviate computational bottleneck dit identify three key redundancy attention computation dit inference spatial redundancy many attention head focus local information temporal redundancy high similarity attention output neighboring step conditional redundancy conditional unconditional inference exhibit significant similarity propose three technique reduce redundancy window attention residual sharing reduce spatial redundancy attention sharing across timesteps exploit similarity step attention sharing across cfg skip redundant computation conditional generation apply ditfastattn dit pixartsigma image generation task opensora video generation task result show image generation method reduces attention flop achieves endtoend speedup highresolution x generation
time weaver conditional time series generation model imagine generating city electricity demand pattern based weather presence electric vehicle location could used capacity planning winter freeze realworld time series often enriched paired heterogeneous contextual metadata weather location etc current approach time series generation often ignore paired metadata heterogeneity pose several practical challenge adapting existing conditional generation approach image audio video domain time series domain address gap introduce time weaver novel diffusionbased model leverage heterogeneous metadata form categorical continuous even timevariant variable significantly improve time series generation additionally show naive extension standard evaluation metric image time series domain insufficient metric penalize conditional generation approach poor specificity reproducing metadataspecific feature generated time series thus innovate novel evaluation metric accurately capture specificity conditional generation realism generated time series show time weaver outperforms stateoftheart benchmark generative adversarial network gans downstream classification task realworld energy medical air quality traffic data set
synthesising handwritten music gans comprehensive evaluation cyclewgan progan dcgan generation handwritten music sheet crucial step toward enhancing optical music recognition omr system rely large diverse datasets optimal performance however handwritten music sheet often found archive present challenge digitisation due fragility varied handwriting style image quality paper address data scarcity problem applying generative adversarial network gans synthesise realistic handwritten music sheet provide comprehensive evaluation three gan model dcgan progan cyclewgan comparing ability generate diverse highquality handwritten music image proposed cyclewgan model enhances style transfer training stability significantly outperforms dcgan progan qualitative quantitative evaluation cyclewgan achieves superior performance fid score kid making promising solution improving omr system
diffsal joint audio video learning diffusion saliency prediction audiovisual saliency prediction draw support diverse modality complement performance enhancement still challenged customized architecture well taskspecific loss function recent study denoising diffusion model shown promising unifying task framework owing inherent ability generalization following motivation novel diffusion architecture generalized audiovisual saliency prediction diffsal proposed work formulates prediction problem conditional generative task saliency map utilizing input audio video condition based spatiotemporal audiovisual feature extra network saliencyunet designed perform multimodal attention modulation progressive refinement groundtruth saliency map noisy map extensive experiment demonstrate proposed diffsal achieve excellent performance across six challenging audiovisual benchmark average relative improvement previous stateoftheart result six metric
blazebvd make scaletime equalization great blind video deflickering developing blind video deflickering bvd algorithm enhance video temporal consistency gaining importance amid flourish image processing video generation however intricate nature video data complicates training deep learning method leading high resource consumption instability notably severe lighting flicker underscore critical need compact representation beyond pixel value advance bvd research application inspired classic scaletime equalization ste work introduces histogramassisted solution called blazebvd highfidelity rapid bvd compared ste directly corrects pixel value temporally smoothing color histogram blazebvd leverage smoothed illumination histogram within ste filtering ease challenge learning temporal data using neural network technique blazebvd begin condensing pixel value illumination histogram precisely capture flickering local exposure variation histogram smoothed produce singular frame set filtered illumination map exposure map resorting deflickering prior blazebvd utilizes network restore faithful consistent texture impacted lighting change localized exposure issue blazebvd also incorporates lightweight network amend slight temporal inconsistency avoiding resource consumption issue comprehensive experiment synthetic realworld generated video showcase superior qualitative quantitative result blazebvd achieving inference speed faster stateofthearts
focusmae gallbladder cancer detection ultrasound video focused masked autoencoders recent year automated gallbladder cancer gbc detection gained attention researcher current stateoftheart sota methodology relying ultrasound sonography u image exhibit limited generalization emphasizing need transformative approach observe individual u frame may lack sufficient information capture disease manifestation study advocate paradigm shift towards videobased gbc detection leveraging inherent advantage spatiotemporal representation employing masked autoencoder mae representation learning address shortcoming conventional imagebased method propose novel design called focusmae systematically bias selection masking token highinformation region fostering refined representation malignancy additionally contribute extensive u video dataset gbc detection also note first study u videobased gbc detection validate proposed method curated dataset report new stateoftheart sota accuracy gbc detection problem accuracy current imagebased sota gbcnet radformer videobased sota adamae demonstrate generality proposed focusmae public ctbased covid detection dataset reporting improvement accuracy current baseline source code pretrained model available httpsgbciitdgithubiofocusmae
autotvg new visionlanguage pretraining paradigm temporal video grounding temporal video grounding tvg aim localize moment untrimmed video given language description since annotation tvg laborintensive tvg limited supervision accepted attention recent year great success visionlanguage pretraining guide tvg follow traditional pretraining finetuning paradigm however pretraining process would suffer lack temporal modeling finegrained alignment due difference data nature pretrain test besides large gap pretext downstream task make zeroshot testing impossible pretrained model avoid drawback traditional paradigm propose autotvg new visionlanguage pretraining paradigm tvg enables model learn semantic alignment boundary regression automatically annotated untrimmed video specific autotvg consists novel captioned moment generation cmg module generate captioned moment untrimmed video tvgnet regression head predict localization result experimental result charadessta activitynet caption show regarding zeroshot temporal video grounding autotvg achieves highly competitive performance indistribution method outofdistribution testing superior existing pretraining framework much less training data
secure video quality assessment resisting adversarial attack exponential surge video traffic intensified imperative video quality assessment vqa leveraging cuttingedge architecture current vqa model achieved humancomparable accuracy however recent study revealed vulnerability existing vqa model adversarial attack establish reliable practical assessment system secure vqa model capable resisting malicious attack urgently demanded unfortunately attempt made explore issue paper first attempt investigate general adversarial defense principle aiming endowing existing vqa model security specifically first introduce random spatial grid sampling video frame intraframe defense design pixelwise randomization guardian map globally neutralizing adversarial perturbation meanwhile extract temporal information video sequence compensation interframe defense building upon principle present novel vqa framework securityoriented perspective termed securevqa extensive experiment indicate securevqa set new benchmark security achieving competitive vqa performance compared stateoftheart model ablation study delve deeper analyzing principle securevqa demonstrating generalization contribution security leading vqa model
exploring efficient foundational multimodal model video summarization foundational model able generate text output given prompt instruction text audio image input recently model combined perform task video video summarization video foundation model perform pretraining aligning output modalityspecific model embedding space embeddings model used within language model finetuned desired instruction set aligning modality pretraining computationally expensive prevents rapid testing different base modality model finetuning evaluation carried within indomain video hard understand generalizability data efficiency method alleviate issue propose plugandplay video language model directly us text generated input modality language model avoiding pretraining alignment overhead instead finetuning leverage fewshot instruction adaptation strategy compare performance versus computational cost plugandplay style method baseline tuning method finally explore generalizability method domain shift present insight data useful training data limited analysis present practical insight leverage multimodal foundational model effective result given realistic compute data limitation
aligning neuronal coding dynamic visual scene foundation vision model brain represent everchanging environment neuron highly dynamic fashion temporal feature visual pixel dynamic natural scene entrapped neuronal response retina crucial establish intrinsic temporal relationship visual pixel neuronal response recent foundation vision model paved advanced way understanding image pixel yet neuronal coding brain largely lack deep understanding alignment pixel previous study employ static image artificial video derived static image emulating real complicated stimulus despite simple scenario effectively help separate key factor influencing visual coding complex temporal relationship receive consideration decompose temporal feature visual coding natural scene propose vist spatiotemporal convolutional neural network fed selfsupervised vision transformer vit prior aimed unraveling temporalbased encoding pattern retinal neuronal population model demonstrates robust predictive performance generalization test furthermore detailed ablation experiment demonstrate significance temporal module furthermore introduce visual coding evaluation metric designed integrate temporal consideration compare impact different number neuronal population complementary coding conclusion proposed vist demonstrates novel modeling framework neuronal coding dynamic visual scene brain effectively aligning brain representation video neuronal activity code available httpsgithubcomwuriningvist
sora see survey texttovideo generation impressive achievement made artificial intelligence path forward artificial general intelligence sora developed openai capable minutelevel worldsimulative ability considered milestone developmental path however despite notable success sora still encounter various obstacle need resolved survey embark perspective disassembling sora texttovideo generation conducting comprehensive review literature trying answer question textitfrom sora see specifically basic preliminary regarding general algorithm introduced literature categorized three mutually perpendicular dimension evolutionary generator excellent pursuit realistic panorama subsequently widely used datasets metric organized detail last importantly identify several challenge open problem domain propose potential future direction research development
augmenting efficient realtime surgical instrument segmentation video point tracking segment anything segment anything model sam powerful vision foundation model revolutionizing traditional paradigm segmentation despite reliance prompting frame large computational cost limit usage robotically assisted surgery application augmented reality guidance require little user intervention along efficient inference usable clinically study address limitation adopting lightweight sam variant meet efficiency requirement employing finetuning technique enhance generalization surgical scene recent advancement tracking point tap shown promising result accuracy efficiency particularly point occluded leave field view inspired progress present novel framework combine online point tracker lightweight sam model finetuned surgical instrument segmentation sparse point within region interest tracked used prompt sam throughout video sequence providing temporal consistency quantitative result surpass stateoftheart semisupervised video object segmentation method xmem endovis dataset iou dice method achieves promising performance comparable xmem transformerbased fully supervised segmentation method ex vivo ucl dvrk vivo datasets addition proposed method show promising zeroshot generalization ability labelfree stir dataset term efficiency tested method single geforce rtx gpu respectively achieving fps inference speed code available
learning multiple object state action via large language model recognizing state object video crucial understanding scene beyond action object instance egg raw cracked whisked cooking omelet state coexist simultaneously egg raw whisked however existing research assumes single object state change eg uncracked cracked overlooking coexisting nature multiple object state influence past state current state formulate object state recognition multilabel classification task explicitly handle multiple state propose learn multiple object state narrated video leveraging large language model llm generate pseudolabels transcribed narration capturing influence past state challenge narration mostly describe human action video rarely explain object state therefore use llm knowledge relationship action state derive missing object state accumulate derived object state consider past state context infer current object state pseudolabels newly collect dataset called multiple object state transition dataset includes manual multilabel annotation evaluation purpose covering object state across six object category experimental result show model trained llmgenerated pseudolabels significantly outperforms strong visionlanguage model demonstrating effectiveness pseudolabeling framework considers past context via llm
dreamwaltzg expressive gaussian avatar skeletonguided diffusion leveraging pretrained diffusion model score distillation sampling sd recent method shown promising result avatar generation however generating highquality avatar capable expressive animation remains challenging work present dreamwaltzg novel learning framework animatable avatar generation text core framework lie skeletonguided score distillation hybrid gaussian avatar representation specifically proposed skeletonguided score distillation integrates skeleton control human template diffusion model enhancing consistency sd supervision term view human pose facilitates generation highquality avatar mitigating issue multiple face extra limb blurring proposed hybrid gaussian avatar representation build efficient gaussians combining neural implicit field parameterized mesh enable realtime rendering stable sd optimization expressive animation extensive experiment demonstrate dreamwaltzg highly effective generating animating avatar outperforming existing method visual quality animation expressiveness framework support diverse application including human video reenactment multisubject scene composition
dreamstory opendomain story visualization llmguided multisubject consistent diffusion story visualization aim create visually compelling image video corresponding textual narrative despite recent advance diffusion model yielding promising result existing method still struggle create coherent sequence subjectconsistent frame based solely story end propose dreamstory automatic opendomain story visualization framework leveraging llm novel multisubject consistent diffusion model dreamstory consists llm acting story director innovative multisubject consistent diffusion model msd generating consistent multisubject across image first dreamstory employ llm generate descriptive prompt subject scene aligned story annotating scene subject subsequent subjectconsistent generation second dreamstory utilizes detailed subject description create portrait subject portrait corresponding textual information serving multimodal anchor guidance finally msd us multimodal anchor generate story scene consistent multisubject specifically msd includes masked mutual selfattention mmsa masked mutual crossattention mmca module mmsa mmca module ensure appearance semantic consistency reference image text respectively module employ masking mechanism prevent subject blending validate approach promote progress story visualization established benchmark assess overall performance story visualization framework subjectidentification accuracy consistency generation model extensive experiment validate effectiveness dreamstory subjective objective evaluation please visit project homepage httpsdreamxyzgithubiodreamstory
frame interpolation consecutive brownian bridge diffusion recent work video frame interpolation vfi try formulate vfi diffusionbased conditional image generation problem synthesizing intermediate frame given random noise neighboring frame due relatively high resolution video latent diffusion model ldms employed conditional generation model autoencoder compress image latent representation diffusion reconstructs image latent representation formulation pose crucial challenge vfi expects output deterministically equal ground truth intermediate frame ldms randomly generate diverse set different image model run multiple time reason diverse generation cumulative variance variance accumulated step generation generated latent representation ldms large make sampling trajectory random resulting diverse rather deterministic generation address problem propose unique solution frame interpolation consecutive brownian bridge diffusion specifically propose consecutive brownian bridge diffusion take deterministic initial value input resulting much smaller cumulative variance generated latent representation experiment suggest method improve together improvement autoencoder achieve stateoftheart performance vfi leaving strong potential enhancement
motiondreamer exploring semantic video diffusion feature zeroshot mesh animation animation technique bring digital world character life however manual animation tedious automated technique often specialized narrow shape class work propose technique automatic reanimation various shape based motion prior extracted video diffusion model unlike existing generation method focus solely motion leverage explicit meshbased representation compatible existing computergraphics pipeline furthermore utilization diffusion feature enhances accuracy motion fitting analyze efficacy feature animation fitting experimentally validate approach two different diffusion model four animation model finally demonstrate timeefficient zeroshot method achieves superior performance reanimating diverse set shape compared existing technique user study project website located httpslukasuzolascommotiondreamer
lmmvqa advancing video quality assessment large multimodal model explosive growth video streaming medium platform underscored urgent need effective video quality assessment vqa algorithm monitor perceptually optimize quality streaming video however vqa remains extremely challenging task due diverse video content complex spatial temporal distortion thus necessitating advanced method address issue nowadays large multimodal model lmms exhibited strong capability various visual understanding task motivating u leverage powerful multimodal representation ability lmms solve vqa task therefore propose first large multimodal video quality assessment lmmvqa model introduces novel spatiotemporal visual modeling strategy qualityaware feature extraction specifically first reformulate quality regression problem question answering qa task construct qa prompt vqa instruction tuning design spatiotemporal vision encoder extract spatial temporal feature represent quality characteristic video subsequently mapped language space spatiotemporal projector modality alignment finally aligned visual token qualityinquired text token aggregated input large language model llm generate quality score level extensive experiment demonstrate lmmvqa achieves stateoftheart performance across five vqa benchmark exhibiting average improvement generalization ability existing method furthermore due advanced design spatiotemporal encoder projector lmmvqa also performs exceptionally well general video understanding task validating effectiveness code released httpsgithubcomsueqklmmvqa
improving dynamic object interaction texttovideo generation ai feedback large texttovideo model hold immense potential wide range downstream application however model struggle accurately depict dynamic object interaction often resulting unrealistic movement frequent violation realworld physic one solution inspired large language model align generated output desired outcome using external feedback enables model refine response autonomously eliminating extensive manual data collection work investigate use feedback enhance object dynamic texttovideo model aim answer critical question type feedback paired specific selfimprovement algorithm effectively improve textvideo alignment realistic object interaction begin deriving unified probabilistic objective offline rl finetuning texttovideo model perspective highlight design element existing algorithm like kl regularization policy projection emerge specific choice within unified framework use derived method optimize set textvideo alignment metric eg clip score optical flow notice often fail align human perception generation quality address limitation propose leveraging visionlanguage model provide nuanced feedback specifically tailored object dynamic video experiment demonstrate method effectively optimize wide variety reward binary ai feedback driving significant improvement video quality dynamic interaction confirmed ai human evaluation notably observe substantial gain using reward signal derived ai feedback particularly scenario involving complex interaction multiple object realistic depiction object falling
memoryefficient highresolution oct volume synthesis cascaded amortized latent diffusion model optical coherence tomography oct image analysis play important role field ophthalmology current successful analysis model rely available large datasets challenging obtained certain task use deep generative model create realistic data emerges promising approach however due limitation hardware resource still difficulty synthesize highresolution oct volume paper introduce cascaded amortized latent diffusion model caldm synthesis highresolution oct volume memoryefficient way first propose nonholistic autoencoders efficiently build bidirectional mapping highresolution volume space lowresolution latent space tandem autoencoders propose cascaded diffusion process synthesize highresolution oct volume globaltolocal refinement process amortizing memory computational demand experiment public highresolution oct dataset show synthetic data realistic highresolution global feature surpassing capability existing method moreover performance gain two downstream finegrained segmentation task demonstrate benefit proposed method training deep learning model medical imaging task code public available
noninvasive invasive enhancing ffa synthesis cfp benchmark dataset novel network fundus imaging pivotal tool ophthalmology different imaging modality characterized specific advantage example fundus fluorescein angiography ffa uniquely provides detailed insight retinal vascular dynamic pathology surpassing color fundus photograph cfp detecting microvascular abnormality perfusion status however conventional invasive ffa involves discomfort risk due fluorescein dye injection meaningful challenging synthesize ffa image noninvasive cfp previous study primarily focused ffa synthesis single disease category work explore ffa synthesis multiple disease devising diffusionguided generative adversarial network introduces adaptive dynamic diffusion forward process discriminator add categoryaware representation enhancer moreover facilitate research collect first multidisease cfp ffa paired dataset named multidisease paired ocular synthesis mpos dataset four different fundus disease experimental result show ffa synthesis network generate better ffa image compared stateoftheart method furthermore introduce pairedmodal diagnostic network validate effectiveness synthetic ffa image diagnosis multiple fundus disease result show synthesized ffa image real cfp image higher diagnosis accuracy compared ffa synthesizing method research bridge gap noninvasive imaging ffa thereby offering promising prospect enhance ophthalmic diagnosis patient care focus reducing harm patient noninvasive procedure dataset code released support research field httpsgithubcomwhqxxhffasynthesis
diffusion foundation model bimanual manipulation bimanual manipulation essential robotics yet developing foundation model extremely challenging due inherent complexity coordinating two robot arm leading multimodal action distribution scarcity training data paper present robotics diffusion transformer rdt pioneering diffusion foundation model bimanual manipulation rdt build diffusion model effectively represent multimodality innovative design scalable transformer deal heterogeneity multimodal input capture nonlinearity high frequency robotic data address data scarcity introduce physically interpretable unified action space unify action representation various robot preserving physical meaning original action facilitating learning transferrable physical knowledge design managed pretrain rdt largest collection multirobot datasets date scaled parameter largest diffusionbased foundation model robotic manipulation finally finetuned rdt selfcreated multitask bimanual dataset episode refine manipulation capability experiment real robot demonstrate rdt significantly outperforms existing method exhibit zeroshot generalization unseen object scene understands follows language instruction learns new skill demonstration effectively handle complex dexterous task refer httpsrdtroboticsgithubiordtrobotics code video
reviewing fid sid metric generative adversarial network growth generative adversarial network gan model increased ability image processing provides numerous industry technology produce realistic image transformation however field recently established new evaluation metric research previous research shown frechet inception distance fid effective metric testing imagetoimage gans realworld application signed inception distance sid founded metric expands fid allowing unsigned distance paper us public datasets consist faccades cityscape map within cyclegan model training model evaluated inception distance metric measure generating performance trained model finding indicate usage metric sid incorporates efficient effective metric complement even exceed ability shown using fid imagetoimage gans
generative adversarial network motionblur image restoration everyday life photograph taken camera often suffer motion blur due hand vibration sudden movement phenomenon significantly detract quality image captured making interesting challenge develop deep learning model utilizes principle adversarial network restore clarity blurred pixel project focus leveraging generative adversarial network gans effectively deblur image affected motion blur ganbased tensorflow model defined training evaluating gopro dataset comprises paired street view image featuring clear blurred version adversarial training process discriminator generator help produce increasingly realistic image time peak signaltonoise ratio psnr structural similarity index measure ssim two evaluation metric used provide quantitative measure image quality allowing u evaluate effectiveness deblurring process mean psnr mean ssim average second deblurring time achieved project blurry pixel sharper output gan model show good image restoration effect real world application
echonetsynthetic privacypreserving video generation safe medical data sharing make medical datasets accessible without sharing sensitive patient information introduce novel endtoend approach generative deidentification dynamic medical imaging data generative method faced constraint term fidelity spatiotemporal coherence length generation failing capture complete detail dataset distribution present model designed produce highfidelity long complete data sample nearrealtime efficiency explore approach challenging task generating echocardiogram video develop generation method based diffusion model introduce protocol medical video dataset anonymization exemplar present echonetsynthetic fully synthetic privacycompliant echocardiogram dataset paired ejection fraction label part deidentification protocol evaluate quality generated dataset propose use clinical downstream task measurement top widely used potentially biased image quality metric experimental outcome demonstrate echonetsynthetic achieves comparable dataset fidelity actual dataset effectively supporting ejection fraction regression task code weight dataset available httpsgithubcomhreynaudechonetsynthetic
zerotohero enhancing zeroshot novel view synthesis via attention map filtering generating realistic image arbitrary view based single source image remains significant challenge computer vision broad application ranging ecommerce immersive virtual experience recent advancement diffusion model particularly model widely adopted generating plausible view video model however model still struggle inconsistency implausibility new view generation especially challenging change viewpoint work propose zerotohero novel testtime approach enhances view synthesis manipulating attention map denoising process drawing analogy denoising process stochastic gradient descent sgd implement filtering mechanism aggregate attention map enhancing generation reliability authenticity process improves geometric consistency without requiring retraining significant computational resource additionally modify selfattention mechanism integrate information source view reducing shape distortion process supported specialized sampling schedule experimental result demonstrate substantial improvement fidelity consistency validated diverse set outofdistribution object additionally demonstrate general applicability effectiveness zerotohero multiview image generation conditioned semantic map pose
hicast highly customized arbitrary style transfer adapter enhanced diffusion model goal arbitrary style transfer ast injecting artistic feature style reference given imagevideo existing method usually focus pursuing balance style content whereas ignoring significant demand flexible customized stylization result thereby limiting practical application address critical issue novel ast approach namely hicast proposed capable explicitly customizing stylization result according various source semantic clue specific model constructed based latent diffusion model ldm elaborately designed absorb content style instance condition ldm characterized introducing textitstyle adapter allows user flexibly manipulate output result aligning multilevel style information intrinsic knowledge ldm lastly extend model perform video ast novel learning objective leveraged video diffusion model training significantly improve crossframe temporal consistency premise maintaining stylization strength qualitative quantitative comparison well comprehensive user study demonstrate hicast outperforms existing sota method generating visually plausible stylization result
motion diffusionguided global hmr dynamic camera motion capture technology transformed numerous field film gaming industry sport science healthcare providing tool capture analyze human movement great detail holy grail topic monocular global human mesh motion reconstruction ghmr achieve accuracy par traditional multiview capture monocular video captured dynamic camera inthewild challenging task monocular input inherent depth ambiguity moving camera add additional complexity rendered human motion product human camera movement accounting confusion existing ghmr method often output motion unrealistic eg unaccounted root translation human cause foot sliding present diffopt novel global hmr method using diffusion optimization key insight recent advance human motion generation motion diffusion model mdm contain strong prior coherent human motion core method optimize initial motion reconstruction using mdm prior step lead globally coherent human motion optimization jointly optimizes motion prior loss reprojection loss correctly disentangle human camera motion validate diffopt video sequence electromagnetic database global human pose shape wild emdb egobody demonstrate superior global human motion recovery capability stateoftheart global hmr method prominently long video setting
transforming text modality resolution duration via flowbased large diffusion transformer sora unveils potential scaling diffusion transformer generating photorealistic image video arbitrary resolution aspect ratio duration yet still lack sufficient implementation detail technical report introduce family series flowbased large diffusion transformer flagdit equipped zeroinitialized attention unified framework designed transform noise image video multiview object audio clip conditioned text instruction tokenizing latent spatialtemporal space incorporating learnable placeholder nextline nextframe token seamlessly unifies representation different modality across various spatialtemporal resolution unified approach enables training within single framework different modality allows flexible generation multimodal data resolution aspect ratio length inference advanced technique like rope rmsnorm flow matching enhance stability flexibility scalability flagdit enabling model scale billion parameter extend context window token particularly beneficial creating ultrahighdefinition image model long video model remarkably powered flagdit requires training computational cost naive dit comprehensive analysis underscore preliminary capability resolution extrapolation highresolution editing generating consistent view synthesizing video seamless transition expect opensourcing foster creativity transparency diversity generative ai community
urcdm ultraresolution image synthesis histopathology diagnosing medical condition histopathology data requires thorough analysis across various resolution whole slide image wsi however existing generative method fail consistently represent hierarchical structure wsis due focus highfidelity patch tackle propose ultraresolution cascaded diffusion model urcdms capable synthesising entire histopathology image high resolution whilst authentically capturing detail underlying anatomy pathology magnification level evaluate method three separate datasets consisting brain breast kidney tissue surpass existing stateoftheart multiresolution model furthermore expert evaluation study conducted demonstrating urcdms consistently generate output across various resolution trained evaluator distinguish real image code additional example found github
conditional generative model contrastenhanced synthesis map brain mri contrast enhancement gadoliniumbased contrast agent gbcas vital tool tumor diagnosis neuroradiology based brain mri scan glioblastoma gadolinium administration address enhancement prediction neural network two new contribution firstly study potential generative model precisely conditional diffusion flow matching uncertainty quantification virtual enhancement secondly examine performance scan quantitive mri versus scan contrast scan scan advantage physically meaningful thereby comparable voxel range compare network prediction performance two modality incompatible grayvalue scale propose evaluate segmentation contrastenhanced region interest using dice jaccard score across model observe better segmentation scan scan
dialogue director bridging gap dialogue visualization multimodal storytelling recent advance aidriven storytelling enhanced video generation story visualization however translating dialoguecentric script coherent storyboards remains significant challenge due limited script detail inadequate physical context understanding complexity integrating cinematic principle address challenge propose dialogue visualization novel task transforms dialogue script dynamic multiview storyboards introduce dialogue director trainingfree multimodal framework comprising script director cinematographer storyboard maker framework leverage large multimodal model diffusionbased architecture employing technique chainofthought reasoning retrievalaugmented generation multiview synthesis improve script understanding physical context comprehension cinematic knowledge integration experimental result demonstrate dialogue director outperforms stateoftheart method script interpretation physical world understanding cinematic principle application significantly advancing quality controllability dialoguebased story visualization
multimodal latent language modeling nexttoken diffusion multimodal generative model require unified approach handle discrete data eg text code continuous data eg image audio video work propose latent language modeling latentlm seamlessly integrates continuous discrete data using causal transformer specifically employ variational autoencoder vae represent continuous data latent vector introduce nexttoken diffusion autoregressive generation vector additionally develop sigmavae address challenge variance collapse crucial autoregressive modeling extensive experiment demonstrate effectiveness latentlm across various modality image generation latentlm surpasses diffusion transformer performance scalability integrated multimodal large language model latentlm provides generalpurpose interface unifies multimodal generation understanding experimental result show latentlm achieves favorable performance compared transfusion vector quantized model setting scaling training token texttospeech synthesis latentlm outperforms stateoftheart valle model speaker similarity robustness requiring fewer decoding step result establish latentlm highly effective scalable approach advance large multimodal model
gomatching simple baseline video text spotting via long short term matching beyond text detection recognition task image text spotting video text spotting present augmented challenge inclusion tracking advanced endtoend trainable method shown commendable performance pursuit multitask optimization may pose risk producing suboptimal outcome individual task paper identify main bottleneck stateoftheart video text spotter limited recognition capability response issue propose efficiently turn offtheshelf querybased image text spotter specialist video present simple baseline termed gomatching focus training effort tracking maintaining strong recognition performance adapt image text spotter video datasets add rescoring head rescore detected instance confidence via efficient tuning leading better tracking candidate pool additionally design longshort term matching module termed lstmatcher enhance spotter tracking capability integrating long shortterm matching result via transformer based simple design gomatching delivers new record dstext bovtext proposed novel test arbitraryshaped text termed artvideo demonstrates gomatchings capability accommodate general dense small arbitraryshaped chinese english text scenario saving considerable training budget
uniav unified audiovisual perception multitask video event localization video localization task aim temporally locate specific instance video including temporal action localization tal sound event detection sed audiovisual event localization avel existing method overspecialize task overlooking fact instance often occur video form complete video content work present uniav unified audiovisual perception network achieve joint learning tal sed avel task first time uniav leverage diverse data available taskspecific datasets allowing model learn share mutually beneficial knowledge across task modality tackle challenge posed substantial variation datasets sizedomainduration distinct task characteristic propose uniformly encode visual audio modality video derive generic representation also designing taskspecific expert capture unique knowledge task besides develop unified languageaware classifier utilizing pretrained text encoder enabling model flexibly detect various type instance previously unseen one simply changing prompt inference uniav outperforms singletask counterpart large margin fewer parameter achieving onpar superior performance compared stateoftheart taskspecific method across activitynet desed benchmark
unmasking illusion understanding human perception audiovisual deepfakes emergence contemporary deepfakes attracted significant attention machine learning research artificial intelligence ai generated synthetic medium increase incidence misinterpretation difficult distinguish genuine content currently machine learning technique extensively studied automatically detecting deepfakes however human perception less explored malicious deepfakes could ultimately cause public social problem human correctly perceive authenticity content video watch answer obviously uncertain therefore paper aim evaluate human ability discern deepfake video subjective study present finding comparing human observer five stateoftheart audiovisual deepfake detection model end used gamification concept provide participant native english speaker nonnative english speaker webbased platform could access series video real fake determine authenticity participant performed experiment twice video different random order video manually selected fakeavceleb dataset found ai model performed better human evaluated video study also reveals deception impossible human tend overestimate detection capability experimental result may help benchmark human versus machine performance advance forensics analysis enable adaptive countermeasure
madrlbased rate adaptation video streaming multiviewpoint prediction last year video traffic network grown significantly key challenge video playback ensuring high quality experience qoe limited network bandwidth currently study focus tilebased adaptive bitrate abr streaming based single viewport prediction reduce bandwidth consumption however performance model singleviewpoint prediction severely limited inherent uncertainty head movement cope sudden movement user well paper first present multimodal spatialtemporal attention transformer generate multiple viewpoint trajectory probability given historical trajectory proposed method model viewpoint prediction classification problem us attention mechanism capture spatial temporal characteristic input video frame viewpoint trajectory multiviewpoint prediction multiagent deep reinforcement learning madrlbased abr algorithm utilizing multiviewpoint prediction video streaming proposed maximizing different qoe objective various network condition formulate abr problem decentralized partially observable markov decision process decpomdp problem present mappo algorithm based centralized training decentralized execution ctde framework solve problem experimental result show proposed method improves defined qoe metric compared existing abr method
oneshot training video object segmentation video object segmentation vos aim track object across frame video segment based initial annotated frame target object previous vos work typically rely fully annotated video training however acquiring fully annotated training video vos laborintensive timeconsuming meanwhile selfsupervised vos method attempted build vos system correspondence learning label propagation still absence mask prior harm robustness complex scenario label propagation paradigm make impractical term efficiency address issue propose first time general oneshot training framework vos requiring single labeled frame per training video applicable majority stateoftheart vos network specifically algorithm consists inferring object mask timeforward based initial labeled frame ii reconstructing initial object mask timebackward using mask step bidirectional training satisfactory vos network obtained notably approach extremely simple employed endtoend finally approach us single labeled frame youtubevos davis datasets achieve comparable result trained fully labeled datasets code released
many frame useful efficient strategy longform video qa longform video span across wide temporal interval highly information redundant contain multiple distinct event entity often loosely related therefore performing longform video question answering lvqa information necessary generate correct response often contained within small subset frame recent literature explore use large language model llm lvqa benchmark achieving exceptional performance relying vision language model vlms convert visual content within video natural language vlms often independently caption large number frame uniformly sampled long video efficient mostly redundant questioning decision choice explore optimal strategy keyframe selection significantly reduce redundancy namely hierarchical keyframe selector proposed framework lvnet achieves stateoftheart performance comparable caption scale across three benchmark lvqa datasets egoschema nextqa intentqa also demonstrating strong performance video hour long videomme code released publicly code found
dabit depth blur informed transformer video focal deblurring many realworld scenario recorded video suffer accidental focus blur video deblurring method exist specifically target motion blur spatialinvariant blur paper introduces framework optimized yet unattempted task video focal deblurring refocusing proposed method employ novel mapguided transformer addition image propagation effectively leverage continuous spatial variance focal blur restore footage also introduce flow refocusing module designed efficiently align relevant feature blurry sharp domain additionally propose novel technique generating synthetic focal blur data broadening model learning capability robustness include wider array content made new benchmark dataset davisblur available dataset modified extension popular davis video segmentation set provides realistic focal blur degradation well corresponding blur map comprehensive experiment demonstrate superiority approach achieve stateoftheart result average psnr performance greater comparable existing video restoration method source code developed database made available httpsgithubcomcrispianmdabit
data overfitting ondevice superresolution dynamic algorithm compiler codesign deep neural network dnns frequently employed variety computer vision application nowadays emerging trend current video distribution system take advantage dnns overfitting property perform video resolution upscaling splitting video chunk applying superresolution sr model overfit chunk scheme sr model plus video chunk able replace traditional video transmission enhance video quality transmission efficiency however many model chunk needed guarantee high performance lead tremendous overhead model switching memory footprint user end resolve problem propose dynamic deep neural network assisted contentaware data processing pipeline reduce model number one dydca help promote performance conserving computational resource additionally achieve real acceleration user end designed framework optimizes dynamic feature eg dynamic shape size control flow dydca enable series compilation optimization including fused code generation static execution planning etc employing technique method achieves better psnr realtime performance fps offtheshelf mobile phone meanwhile assisted compilation optimization achieve speedup saving memory consumption code available
relaxvqa residual fragment layer stack extraction enhancing video quality assessment rapid growth usergenerated content ugc exchanged user sharing platform need video quality assessment wild increasingly evident ugc typically acquired using consumer device undergoes multiple round compression transcoding reaching end user therefore traditional quality metric employ original content reference suitable paper propose relaxvqa novel noreference video quality assessment nrvqa model aim address challenge evaluating quality diverse video content without reference original uncompressed video relaxvqa us frame difference select spatiotemporal fragment intelligently together different expression spatial feature associated sampled frame used better capture spatial temporal variability quality neighbouring frame furthermore model enhances abstraction employing layerstacking technique deep neural network feature residual network vision transformer extensive testing across four ugc datasets demonstrates relaxvqa consistently outperforms existing nrvqa method achieving average srcc plcc opensource code trained model facilitate research application nrvqa found
vidlpro underlinevideounderlinelanguage underlinepretraining framework underlinerobotic laparoscopic surgery introduce vidlpro novel videolanguage vl pretraining framework designed specifically robotic laparoscopic surgery existing surgical vl model primarily rely contrastive learning propose comprehensive approach capture intricate temporal dynamic align video language vidlpro integrates videotext contrastive learning videotext matching masked language modeling objective learn rich vl representation support framework present gensurg carefully curated dataset derived gensurgery comprising surgical video clip paired caption generated using transcript extracted whisper model dataset address need largescale highquality vl data surgical domain extensive experiment benchmark datasets including autolaparo demonstrate efficacy approach vidlpro achieves stateoftheart performance zeroshot surgical phase recognition significantly outperforming existing surgical vl model surgvlp hecvl model demonstrates improvement accuracy score setting new benchmark field notably vidlpro exhibit robust performance even singleframe inference effectively scaling increased temporal context ablation study reveal impact frame sampling strategy model performance computational efficiency result underscore vidlpros potential foundation model surgical video understanding
vmid multimodal fusion llm framework detecting identifying misinformation short video short video platform become important channel news dissemination offering highly engaging immediate way user access current event share information however platform also emerged significant conduit rapid spread misinformation fake news rumor leverage visual appeal wide reach short video circulate extensively among audience existing fake news detection method mainly rely singlemodal information text image apply basic fusion technique limiting ability handle complex multilayered information inherent short video address limitation paper present novel fake news detection method based multimodal information designed identify misinformation multilevel analysis video content approach effectively utilizes different modal representation generate unified textual description fed large language model comprehensive evaluation proposed framework successfully integrates multimodal feature within video significantly enhancing accuracy reliability fake news detection experimental result demonstrate proposed approach outperforms existing model term accuracy robustness utilization multimodal information achieving accuracy significantly higher best baseline model svfend furthermore case study provide additional evidence effectiveness approach accurately distinguishing fake news debunking content real incident highlighting reliability robustness realworld application
videoespresso largescale chainofthought dataset finegrained video reasoning via core frame selection advancement large vision language model lvlms significantly improved multimodal understanding yet challenge remain video reasoning task due scarcity highquality largescale datasets existing video questionanswering videoqa datasets often rely costly manual annotation insufficient granularity automatic construction method redundant framebyframe analysis limiting scalability effectiveness complex reasoning address challenge introduce videoespresso novel dataset feature videoqa pair preserving essential spatial detail temporal coherence along multimodal annotation intermediate reasoning step construction pipeline employ semanticaware method reduce redundancy followed generating qa pair using develop video chainofthought cot annotation enrich reasoning process guiding extracting logical relationship qa pair video content exploit potential highquality videoqa pair propose hybrid lvlms collaboration framework featuring frame selector twostage instruction finetuned reasoning lvlm framework adaptively selects core frame performs cot reasoning using multimodal evidence evaluated proposed benchmark task popular lvlms method outperforms existing baseline task demonstrating superior video reasoning capability code dataset released httpsgithubcomhshjerryvideoespresso
realtime anomaly detection video stream thesis part cifre agreement company othello liasd laboratory objective develop artificial intelligence system detect realtime danger video stream achieve novel approach combining temporal spatial analysis proposed several avenue explored improve anomaly detection integrating object detection human pose detection motion analysis result interpretability technique commonly used image analysis activation saliency map extended video original method proposed proposed architecture performs binary multiclass classification depending whether alert cause need identified numerous neural networkmodels tested three selected look yolo used spatial analysis convolutional recurrent neuronal network crnn composed gated recurrent unit gru temporal analysis multilayer perceptron classification model handle different type data combined parallel series although parallel mode faster serial mode generally reliable training model supervised learning chosen two proprietary datasets created first dataset focus object may play potential role anomaly second consists video containing anomaly nonanomalies approach allows processing continuous video stream finite video providing greater flexibility detection
videolights feature refinement crosstask alignment transformer joint video highlight detection moment retrieval video highlight detection moment retrieval hdmr essential video analysis recent joint prediction transformer model often overlook crosstask dynamic videotext alignment refinement moreover model typically use limited unidirectional attention mechanism resulting weakly integrated representation suboptimal performance capturing interdependence video text modality although largelanguage visionlanguage model llmlvlms gained prominence across various domain application field remains relatively underexplored propose videolights novel hdmr framework addressing limitation convolutional projection feature refinement module alignment loss better videotext feature alignment ii bidirectional crossmodal fusion network strongly coupled queryaware clip representation iii unidirectional jointtask feedback mechanism enhancing task correlation addition iv introduce hard positivenegative loss adaptive error penalization improved learning v leverage lvlms like enhanced multimodal feature integration intelligent pretraining using synthetic data generated lvlms comprehensive experiment qvhighlights tvsum charadessta benchmark demonstrate stateoftheart performance code model available
sweettok semanticaware spatialtemporal tokenizer compact video discretization paper present textbfsemanticatextbfwartextbfe spatialttextbfemporal textbftokenizer sweettok novel video tokenizer overcome limitation current video tokenization method compacted yet effective discretization unlike previous approach process flattened local visual patch via direct discretization adaptive query tokenization sweettok proposes decoupling framework compressing visual input distinct spatial temporal query via textbfdecoupled textbfquery textbfautotextbfencoder dqae design allows sweettok efficiently compress video token count achieving superior fidelity capturing essential information across spatial temporal dimension furthermore design textbfmotionenhanced textbflanguage textbfcodebook mlc tailored spatial temporal compression address difference semantic representation appearance motion information sweettok significantly improves video reconstruction result wrt rfvd dataset better token compression strategy also boost downstream video generation result wrt gfvd additionally compressed decoupled token imbued semantic information enabling fewshot recognition capability powered llm downstream application
blendscape enabling enduser customization videoconferencing environment generative ai today videoconferencing tool support rich range professional social activity generic meeting environment dynamically adapted align distributed collaborator need enable enduser customization developed blendscape rendering composition system videoconferencing participant tailor environment meeting context leveraging ai image generation technique blendscape support flexible representation task space blending user physical digital background unified environment implement multimodal interaction technique steer generation exploratory study endusers investigated whether would find value using generative ai customize videoconferencing environment participant envisioned using system like blendscape facilitate collaborative activity future required control mitigate distracting unrealistic visual element implemented scenario demonstrate blendscapes expressiveness supporting environment design strategy prior work propose composition technique improve quality environment
actionconditioned video data improves predictability longterm video generation prediction remain challenging task computer vision particularly partially observable scenario camera mounted moving platform interaction observed image frame motion recording agent introduces additional complexity address issue introduce actionconditioned video generation acvg framework novel approach investigates relationship action generated image frame deep dual generatoractor architecture acvg generates video sequence conditioned action robot enabling exploration analysis vision action mutually influence one another dynamic environment evaluate framework effectiveness indoor robot motion dataset consists sequence image frame along sequence action taken robotic agent conducting comprehensive empirical study comparing acvg stateoftheart framework along detailed ablation study
motioncharacter identitypreserving motion controllable human video generation recent advancement personalized texttovideo generation highlight importance integrating characterspecific identity action however previous model struggle identity consistency controllable motion dynamic mainly due limited finegrained facial actionbased textual prompt datasets overlook key human attribute action address challenge propose motioncharacter efficient highfidelity human video generation framework designed identity preservation finegrained motion control introduce idpreserving module maintain identity fidelity allowing flexible attribute modification integrate idconsistency regionaware loss mechanism significantly enhancing identity consistency detail fidelity additionally approach incorporates motion control module prioritizes actionrelated text maintaining subject consistency along dataset humanmotion utilizes large language model generate detailed motion description simplify user control inference parameterize motion intensity single coefficient allowing easy adjustment extensive experiment highlight effectiveness motioncharacter demonstrating significant improvement idpreserving highquality video generation
unireal universal image generation editing via learning realworld dynamic introduce unireal unified framework designed address various image generation editing task existing solution often vary task yet share fundamental principle preserving consistency input output capturing visual variation inspired recent video generation model effectively balance consistency variation across frame propose unifying approach treat imagelevel task discontinuous video generation specifically treat varying number input output image frame enabling seamless support task image generation editing customization composition etc although designed imagelevel task leverage video scalable source universal supervision unireal learns world dynamic largescale video demonstrating advanced capability handling shadow reflection pose variation object interaction also exhibiting emergent capability novel application
xray sequential representation generation introduce xray novel sequential representation inspired penetrability xray scan xray transforms object series surface frame different layer making suitable generating model image method utilizes ray casting camera center capture geometric textured detail including depth normal color across intersected surface process efficiently condenses whole object multiframe video format motivating utilize network architecture similar video diffusion model design ensures efficient representation focusing solely surface information also propose twostage pipeline generate object xray diffusion model upsampler demonstrate practicality adaptability xray representation synthesizing complete visible hidden surface object single input image experimental result reveal stateoftheart superiority representation enhancing accuracy generation paving way new representation research practical application
multitask learning minimally invasive surgical vision review minimally invasive surgery mi revolutionized many procedure led reduced recovery time risk patient injury however mi pose additional complexity burden surgical team datadriven surgical vision algorithm thought key building block development future mi system improved autonomy recent advancement machine learning computer vision led successful application analyzing video obtained mi promise alleviating challenge mi video surgical scene action understanding encompasses multiple related task solved individually memoryintensive inefficient fail capture task relationship multitask learning mtl learning paradigm leverage information multiple related task improve performance aid generalization well suited finegrained highlevel understanding mi data review provides narrative overview current stateoftheart mtl system leverage video obtained mi beyond listing published approach discus benefit limitation mtl system moreover manuscript present analysis literature various application field mtl mi including large model highlighting notable trend new direction research development
bounding box probabilistic graphical model video anomaly detection simplified study formulate task video anomaly detection probabilistic analysis object bounding box hypothesize representation object via bounding box sufficient successfully identify anomalous event scene implied value approach increased object anonymization faster model training fewer computational resource particularly benefit application within video surveillance running edge device camera design model based human reasoning lends explaining model output humanunderstandable term meanwhile slowest model train within less second generation intel core processor approach constitutes drastic reduction problem feature space comparison prior art show result reduction performance result report highly competitive benchmark datasets cuhk avenue shanghaitech significantly exceed latest stateoftheart result streetscene far proven challenging vad dataset
segment video zeroshot promptable manner introduce preliminary exploration adapting segment anything model sam zeroshot promptable segmentation interprets data series multidirectional video leverage sam segmentation without training projection framework support various prompt type including point box mask generalize across diverse scenario object indoor scene outdoor environment raw sparse lidar demonstration multiple datasets eg objaverse scannet kitti highlight robust generalization capability best knowledge present faithful implementation sam may serve starting point future research promptable segmentation online demo code
actbench towards action controllable world model autonomous driving world model emerged promising neural simulator autonomous driving potential supplement scarce realworld data enable closedloop evaluation however current research primarily evaluates model based visual realism downstream task performance limited focus fidelity specific action instruction crucial property generating targeted simulation scene although study address action fidelity evaluation rely closedsource mechanism limiting reproducibility address gap develop openaccess evaluation framework actbench quantifying action fidelity along baseline world model terra benchmarking framework includes largescale dataset pairing short context video nuscenes corresponding future trajectory data provides conditional input generating future video frame enables evaluation action fidelity executed motion furthermore terra trained multiple largescale trajectoryannotated datasets enhance action fidelity leveraging framework demonstrate stateoftheart model fully adhere given instruction terra achieves improved action fidelity component benchmark framework made publicly available support future research
freestyle free lunch textguided style transfer using diffusion model rapid development generative diffusion model significantly advanced field style transfer however current style transfer method based diffusion model typically involve slow iterative optimization process eg model finetuning textual inversion style concept paper introduce freestyle innovative style transfer method built upon pretrained large diffusion model requiring optimization besides method enables style transfer text description desired style eliminating necessity style image specifically propose dualstream encoder singlestream decoder architecture replacing conventional unet diffusion model dualstream encoder two distinct branch take content image style text prompt input achieving content style decoupling decoder modulate feature dual stream based given content image corresponding style text prompt precise style transfer experimental result demonstrate highquality synthesis fidelity method across various content image style text prompt compared stateoftheart method require training freestyle approach notably reduces computational burden thousand iteration achieving comparable superior performance across multiple evaluation metric including clip aesthetic score clip score preference released code httpsgithubcomfreestylefreelunchfreestyle
rediffinet modeling discrepancy tumor segmentation using diffusion model identification tumor margin essential surgical decisionmaking glioblastoma patient provides reliable assistance neurosurgeon despite improvement deep learning architecture tumor segmentation year creating fully autonomous system suitable clinical floor remains formidable challenge model prediction yet reached desired level accuracy generalizability clinical application generative modeling technique seen significant improvement recent time specifically generative adversarial network gans denoisingdiffusionbased model ddpms used generate higherquality image fewer artifact finer attribute work introduce framework called rediffinet modeling discrepancy output segmentation model like unet ground truth using ddpms explicitly modeling discrepancy result show average improvement dice score crossvalidation compared stateoftheart unet segmentation model
tkplanes tiered kplanes high dimensional feature vector dynamic uavbased scene paper present new approach bridge domain gap synthetic realworld data unmanned aerial vehicle uavbased perception formulation designed dynamic scene consisting small moving object human action propose extension kplanes neural radiance field nerf wherein algorithm store set tiered feature vector tiered feature vector generated effectively model conceptual information scene well image decoder transforms output feature map rgb image technique leverage information amongst static dynamic object within scene able capture salient scene attribute high altitude video evaluate performance challenging datasets including okutama action observe considerable improvement accuracy state art neural rendering method
taskoriented hierarchical object decomposition visuomotor control good pretrained visual representation could enable robot learn visuomotor policy efficiently still existing representation take onesizefitsalltasks approach come two important drawback completely taskagnostic representation effectively ignore taskirrelevant information scene often lack representational capacity handle unconstrainedcomplex realworld scene instead propose train large combinatorial family representation organized scene entity object object part hierarchical object decomposition taskoriented representation hodor permit selectively assembling different representation specific task scaling representational capacity complexity scene task experiment find hodor outperforms prior pretrained representation scene vector representation objectcentric representation sampleefficient imitation learning across simulated realworld manipulation task find invariance captured hodor inherited downstream policy robustly generalize outofdistribution test condition permitting zeroshot skill chaining appendix code video
leveraging scene geometry depth information robust image deraining image deraining hold great potential enhancing vision autonomous vehicle rainy condition contributing safer driving previous work primarily focused employing single network architecture generate derained image however often fail fully exploit rich prior knowledge embedded scene particularly method overlook depth information provide valuable context scene geometry guide robust deraining work introduce novel learning framework integrates multiple network autoencoder deraining auxiliary network incorporate depth information two supervision network enforce feature consistency rainy clear scene multinetwork design enables model effectively capture underlying scene structure producing clearer accurately derained image leading improved object detection autonomous vehicle extensive experiment three widelyused datasets demonstrated effectiveness proposed method
xsvid extremely small video object detection dataset small video object detection svod crucial subfield modern computer vision essential early object discovery detection however existing svod datasets scarce suffer issue insufficiently small object limited object category lack scene diversity leading unitary application scenario corresponding method address gap develop xsvid dataset comprises aerial data various period scene annotates eight major object category evaluate existing method detecting extremely small object xsvid extensively collect three type object smaller pixel area extremely small textites relatively small textitrs generally small textitgs xsvid offer unprecedented breadth depth covering quantifying minuscule object significantly enriching scene object diversity dataset extensive validation xsvid publicly available dataset show existing method struggle small object detection significantly underperform compared general object detector leveraging strength previous method addressing weakness propose yoloft enhances local feature association integrates temporal motion feature significantly improving accuracy stability svod datasets benchmark available urlhttpsgjhhustgithubioxsvid
evaluating texttovisual generation imagetotext generation despite significant progress generative ai comprehensive evaluation remains challenging lack effective metric standardized benchmark instance widelyused clipscore measure alignment generated image text prompt fails produce reliable score complex prompt involving composition object attribute relation one reason text encoders clip notoriously act bag word conflating prompt horse eating grass grass eating horse address introduce vqascore us visualquestionanswering vqa model produce alignment score computing probability yes answer simple figure show text question though simpler prior art vqascore computed offtheshelf model produce stateoftheart result across many imagetext alignment benchmark also compute vqascore inhouse model follows best practice literature example use bidirectional imagequestion encoder allows image embeddings depend question asked vice versa inhouse model outperforms even strongest baseline make use proprietary interestingly although train image vqascore also align text video model vqascore allows researcher benchmark texttovisual generation using complex text capture compositional structure realworld prompt introduce genaibench challenging benchmark compositional text prompt require parsing scene object attribute relationship highorder reasoning like comparison logic genaibench also offer human rating leading image video generation model stable diffusion dalle
multiscale latent diffusion model enhanced feature extraction medical image various imaging modality used patient diagnosis offering unique advantage valuable insight anatomy pathology computed tomography ct crucial diagnostics providing highresolution image precise internal organ visualization ct ability detect subtle tissue variation vital diagnosing disease like lung cancer enabling early detection accurate tumor assessment however variation ct scanner model acquisition protocol introduce significant variability extracted radiomic feature even imaging patient variability pose considerable challenge downstream research clinical analysis depend consistent reliable feature extraction current method medical image feature extraction often based supervised learning approach including ganbased model face limitation generalizing across different imaging environment response challenge propose ltdiff multiscale latent diffusion model designed enhance feature extraction medical imaging model address variability standardizing nonuniform distribution latent space improving feature consistency ltdiff utilizes unet encoderdecoder architecture coupled conditional denoising diffusion probabilistic model ddpm latent bottleneck achieve robust feature extraction standardization extensive empirical evaluation patient phantom ct datasets demonstrate significant improvement image standardization higher concordance correlation coefficient ccc across multiple radiomic feature category advancement ltdiff represents promising solution overcoming inherent variability medical imaging data offering improved reliability accuracy feature extraction process
cameracontrolled imagetovideo diffusion model recent advancement integrated camera pose userfriendly physicsinformed condition video diffusion model enabling precise camera control paper identify one key challenge effectively modeling noisy crossframe interaction enhance geometry consistency camera controllability innovatively associate quality condition ability reduce uncertainty interpret noisy crossframe feature form noisy condition recognizing noisy condition provide deterministic information also introducing randomness potential misguidance due added noise propose applying epipolar attention aggregate feature along corresponding epipolar line thereby accessing optimal amount noisy condition additionally address scenario epipolar line disappear commonly caused rapid camera movement dynamic object occlusion ensuring robust performance diverse environment furthermore develop robust reproducible evaluation pipeline address inaccuracy instability existing camera control metric method achieves improvement camera controllability dataset without compromising dynamic generation quality demonstrates strong generalization outofdomain image training inference require memory respectively sequence resolution release checkpoint along training evaluation code dynamic video best viewed
estimating egobody pose doubly sparse egocentric video data study problem estimating body movement camera wearer egocentric video current method egobody pose estimation rely temporally dense sensor data imu measurement spatially sparse body part like head hand however propose even temporally sparse observation hand pose captured intermittently egocentric video natural periodic hand movement effectively constrain overall body motion naively applying diffusion model generate fullbody pose head pose sparse hand pose lead suboptimal result overcome develop twostage approach decomposes problem temporal completion spatial completion first method employ masked autoencoders impute hand trajectory leveraging spatiotemporal correlation head pose sequence intermittent hand pose providing uncertainty estimate subsequently employ conditional diffusion model generate plausible fullbody motion based temporally dense trajectory head hand guided uncertainty estimate imputation effectiveness method rigorously tested validated comprehensive experiment conducted various hmd setup amass datasets
frequencyaware guidance blind image restoration via diffusion model blind image restoration remains significant challenge lowlevel vision task recently denoising diffusion model shown remarkable performance image synthesis guided diffusion model leveraging potent generative prior pretrained model along differential guidance loss achieved promising result blind image restoration however model typically consider data consistency solely spatial domain often resulting distorted image content paper propose novel frequencyaware guidance loss integrated various diffusion model plugandplay manner proposed guidance loss based discrete wavelet transform simultaneously enforces content consistency spatial frequency domain experimental result demonstrate effectiveness method three blind restoration task blind image deblurring imaging turbulence blind restoration multiple degradation notably method achieves significant improvement psnr score remarkable enhancement image deblurring moreover method exhibit superior capability generating image rich detail reduced distortion leading best visual quality
guy want dance zeroshot compositional human dance generation multiple person human dance generation hdg aim synthesize realistic video image sequence driving pose despite great success existing method limited generating video single person specific background generalizability realworld scenario multiple person complex background remains unclear systematically measure generalizability hdg model introduce new task dataset evaluation protocol compositional human dance generation chdg evaluating stateoftheart method chdg empirically find fail generalize realworld scenario tackle issue propose novel zeroshot framework dubbed multidancezero synthesize video consistent arbitrary multiple person background precisely following driving pose specifically contrast straightforward ddim nulltext inversion first present poseaware inversion method obtain noisy latent code initialization text embeddings accurately reconstruct composed reference image since directly generating video lead severe appearance inconsistency propose compositional augmentation strategy generate augmented image utilize optimize set generalizable text embeddings addition consistencyguided sampling elaborated encourage background keypoints estimated clean image reverse step close reference image improving temporal consistency generated video extensive qualitative quantitative result demonstrate effectiveness superiority approach
vexpress conditional dropout progressive training portrait video generation field portrait video generation use single image generate portrait video become increasingly prevalent common approach involves leveraging generative model enhance adapter controlled generation however control signal eg text audio reference image pose depth map etc vary strength among weaker condition often struggle effective due interference stronger condition posing challenge balancing condition work portrait video generation identified audio signal particularly weak often overshadowed stronger signal facial pose reference image however direct training weak signal often lead difficulty convergence address propose vexpress simple method balance different control signal progressive training conditional dropout operation method gradually enables effective control weak condition thereby achieving generation capability simultaneously take account facial pose reference image audio experimental result demonstrate method effectively generate portrait video controlled audio furthermore potential solution provided simultaneous effective use condition varying strength
image inpainting corrupted image using semisuper resolution gan image inpainting valuable technique enhancing image corrupted primary challenge research revolves around extent corruption input image deep learning model must restore address challenge introduce generative adversarial network gan learning replicating missing pixel additionally developed distinct variant superresolution gan srgan refer semisrgan ssrgan furthermore leveraged three diverse datasets assess robustness accuracy proposed model training process involves varying level pixel corruption attain optimal accuracy generate highquality image
multiresolution guided gans medical image translation medical image translation process converting one imaging modality another order reduce need multiple image acquisition patient enhance efficiency treatment reducing time equipment labor needed paper introduce multiresolution guided generative adversarial network ganbased framework medical image translation framework us multiresolution denseattention unet generator multiresolution unet discriminator optimized unique combination loss function including voxelwise gan loss perception loss approach yield promising result volumetric image quality assessment iqa across variety imaging modality body region age group demonstrating robustness furthermore propose synthetictoreal applicability assessment additional evaluation assess effectiveness synthetic data downstream application segmentation comprehensive evaluation show method produce synthetic medical image highquality also potentially useful clinical application code available
sora incredible scary emerging governance challenge texttovideo generative ai model texttovideo generative ai model sora openai potential disrupt multiple industry paper report qualitative social medium analysis aiming uncover people perceived impact concern soras integration collected analyzed comment popular post soragenerated video comparison sora video midjourney image artist complaint copyright infringement generative ai found people concerned soras impact content creationrelated industry emerging governance challenge included forprofit nature openai blurred boundary real fake content human autonomy data privacy copyright issue environmental impact potential regulatory solution proposed people included lawenforced labeling ai content ai literacy education public based finding discus importance gauging people tech perception early propose policy recommendation regulate sora public release
improving unsupervised video object segmentation via fake flow generation unsupervised video object segmentation vos also known video salient object detection aim detect prominent object video pixel level recently twostream approach leverage rgb image optical flow map gained significant attention however limited amount training data remains substantial challenge study propose novel data generation method simulates fake optical flow single image thereby creating largescale training data stable network learning inspired observation optical flow map highly dependent depth map generate fake optical flow refining augmenting estimated depth map image incorporating simulated imageflow pair achieve new stateoftheart performance public benchmark datasets without relying complex module believe data generation method represents potential breakthrough future vos research
alignment need trainingfree augmentation strategy poseguided video generation character animation transformative field computer graphic vision enabling dynamic realistic video animation static image despite advancement maintaining appearance consistency animation remains challenge approach address introducing trainingfree framework ensures generated video sequence preserve reference image subtlety physique proportion dual alignment strategy decouple skeletal motion prior pose information enabling precise control animation generation method also improves pixellevel alignment conditional control reference character enhancing temporal consistency visual cohesion animation method significantly enhances quality video generation without need large datasets expensive computational resource
generative videolanguageaction model webscale knowledge robot manipulation present stateoftheart generalist robot agent versatile generalizable robot manipulation first pretrained vast number internet video capture dynamic world largescale pretraining involving million video clip billion token equips ability generalize across wide range robotic task environment subsequent policy learning following finetuned video generation action prediction using robot trajectory exhibit impressive multitask learning capability achieving average success rate across task moreover demonstrates exceptional generalization new previously unseen scenario including novel background environment object task notably scale effectively model size underscoring potential continued growth application project page
allegro open black box commerciallevel video generation model significant advancement made field video generation opensource community contributing wealth research paper tool training highquality model however despite effort available information resource remain insufficient achieving commerciallevel performance report open black box introduce textbfallegro advanced video generation model excels quality temporal consistency also highlight current limitation field present comprehensive methodology training highperformance commerciallevel video generation model addressing key aspect data model architecture training pipeline evaluation user study show allegro surpasses existing opensource model commercial model ranking behind hailuo kling code httpsgithubcomrhymesaiallegro model httpshuggingfacecorhymesaiallegro gallery
nerfad neural radiance field attentionbased disentanglement talking face synthesis talking face synthesis driven audio one current research hotspot field multidimensional signal processing multimedia neural radiance field nerf recently brought research field order enhance realism effect generated face however existing nerfbased method either burden nerf complex learning task lacking method supervised multimodal feature fusion precisely map audio facial region related speech movement reason ultimately result existing method generating inaccurate lip shape paper move portion nerf learning task ahead proposes talking face synthesis method via nerf attentionbased disentanglement nerfad particular attentionbased disentanglement module introduced disentangle face audioface identityface using speechrelated facial action unit au information precisely regulate audio affect talking face fuse audioface audio feature addition au information also utilized supervise fusion two modality extensive qualitative quantitative experiment demonstrate nerfad outperforms stateoftheart method generating realistic talking face video including image quality lip synchronization view video result please refer
highresolution talking head generation emotion style art style although automatically animating audiodriven talking head recently received growing interest previous effort mainly concentrated achieving lip synchronization audio neglecting two crucial element generating expressive video emotion style art style paper present innovative audiodriven talking face generation method called involves two stylized stage namely stylee stylea integrate textcontrolled emotion style picturecontrolled art style final output order prepare scarce emotional text description corresponding video propose laborfree paradigm employ largescale pretrained model automatically annotate emotional text label existing audiovisual datasets incorporating synthetic emotion text stylee stage utilizes largescale clip model extract emotion representation combined audio serving condition efficient latent diffusion model designed produce emotional motion coefficient model moving stylea stage develop coefficientdriven motion generator artspecific style path embedded wellknown stylegan allows u synthesize highresolution artistically stylized talking head video using generated emotional motion coefficient art style source picture moreover better preserve image detail avoid artifact provide stylegan multiscale content feature extracted identity image refine intermediate feature map designed content encoder refinement network respectively extensive experimental result demonstrate method outperforms existing stateoftheart method term audiolip synchronization performance emotion style art style
mustan multiscale temporal context attention robust video foreground segmentation video foreground segmentation vfs important computer vision task wherein one aim segment object motion background current method imagebased ie rely spatial cue ignoring motion cue therefore tend overfit training data dont generalize well outofdomain ood distribution solve problem prior work exploited several cue optical flow background subtraction mask etc however video data annotation like optical flow challenging task paper utilize temporal information spatial cue video data improve ood performance however challenge lie model temporal information given video data interpretable way creates noticeable difference therefore devise strategy integrates temporal context video development vfs approach give rise deep learning architecture namely based idea multiscale temporal context attention ie aid model learn better representation beneficial vfs introduce new video dataset namely indoor surveillance dataset isd vfs multiple annotation frame level foreground binary mask depth map instance semantic annotation therefore isd benefit computer vision task validate efficacy architecture compare performance baseline demonstrate proposed method significantly outperform benchmark method ood addition performance significantly improved certain video category ood data due isd
mvad multiple visual artifact detector video streaming visual artifact often introduced streamed video content due prevailing condition content production delivery since degrade quality user experience important automatically accurately detect order enable effective quality measurement enhancement existing detection method often focus single type artifact andor determine presence artifact thresholding objective quality index approach reported offer inconsistent prediction performance also impractical realworld application multiple artifact coexist interact paper propose multiple visual artifact detector mvad video streaming first time able detect multiple artifact using single framework reliant video quality assessment model approach employ new artifactaware dynamic feature extractor adfe obtain artifactrelevant spatial feature within frame multiple artifact type extracted feature processed recurrent memory vision transformer rmvit module capture shortterm longterm temporal information within input video proposed network architecture optimized endtoend manner based new large diverse training database generated simulating video streaming pipeline based adversarial data augmentation model evaluated two video artifact database maxwell bviartifact achieves consistent improved prediction result ten target visual artifact compared seven existing single multiple artifact detector source code training database available httpschenfengbristolgithubiomvad
scenecraft llm agent synthesizing scene blender code paper introduces scenecraft large language model llm agent converting text description blenderexecutable python script render complex scene hundred asset process requires complex spatial planning arrangement tackle challenge combination advanced abstraction strategic planning library learning scenecraft first model scene graph blueprint detailing spatial relationship among asset scene scenecraft writes python script based graph translating relationship numerical constraint asset layout next scenecraft leverage perceptual strength visionlanguage foundation model like gptv analyze rendered image iteratively refine scene top process scenecraft feature library learning mechanism compiles common script function reusable library facilitating continuous selfimprovement without expensive llm parameter tuning evaluation demonstrates scenecraft surpasses existing llmbased agent rendering complex scene shown adherence constraint favorable human assessment also showcase broader application potential scenecraft reconstructing detailed scene sintel movie guiding video generative model generated scene intermediary control signal
spatialfrequency dualdomain feature fusion network lowlight remote sensing image enhancement lowlight remote sensing image generally feature high resolution high spatial complexity continuously distributed surface feature space continuity scene lead extensive longrange correlation spatial domain within remote sensing image convolutional neural network rely local correlation longdistance modeling struggle establish longrange correlation image hand transformerbased method focus global information face high computational complexity processing highresolution remote sensing image another perspective fourier transform compute global information without introducing large number parameter enabling network efficiently capture overall image structure establish longrange correlation therefore propose dualdomain feature fusion network dffn lowlight remote sensing image enhancement specifically challenging task lowlight enhancement divided two manageable subtasks first phase learns amplitude information restore image brightness second phase learns phase information refine detail facilitate information exchange two phase designed information fusion affine block combine data different phase scale additionally constructed two dark light remote sensing datasets address current lack datasets dark light remote sensing image enhancement extensive evaluation show method outperforms existing stateoftheart method code available httpsgithubcomiijjlkdffn
phased consistency model consistency model cm made significant progress accelerating generation diffusion model however application highresolution textconditioned image generation latent space remains unsatisfactory paper identify three key flaw current design latent consistency model lcm investigate reason behind limitation propose phased consistency model pcms generalize design space address identified limitation evaluation demonstrate pcms outperform lcm across step generation setting pcms specifically designed multistep refinement achieve comparable generation result previously stateoftheart specifically designed method furthermore show methodology pcms versatile applicable video generation enabling u train stateoftheart fewstep texttovideo generator code available httpsgithubcomgunphasedconsistencymodel
harmonizing pixel melody maestroguided film score generation composition style transfer introduce film score generation framework harmonize visual pixel music melody utilizing latent diffusion model framework process film clip input generates music aligns general theme offering capability tailor output specific composition style model directly produce music video utilizing streamlined efficient tuning mechanism controlnet also integrates film encoder adept understanding film semantic depth emotional impact aesthetic appeal additionally introduce novel effective yet straightforward evaluation metric evaluate originality recognizability music within film score fill gap film score curate comprehensive dataset film video legendary original score injecting domainspecific knowledge datadriven generation model model outperforms existing methodology creating film score capable generating music reflects guidance maestro style thereby redefining benchmark automated film score laying robust groundwork future research domain code generated sample available
gait sequence upsampling using diffusion model single lidar sensor recently lidar emerged promising technique field gaitbased person identification serving alternative traditional rgb camera due robustness varying lighting condition ability capture geometric information however long capture distance use lowcost lidar sensor often result sparse human point cloud leading decline identification performance address challenge propose sparsetodense upsampling model pedestrian point cloud lidarbased gait recognition named lidargsu designed improve generalization capability existing identification model method utilizes diffusion probabilistic model dpms shown high fidelity generative task image completion work leverage dpms sparse sequential pedestrian point cloud conditional mask videotovideo translation approach applied inpainting manner conducted extensive experiment dataset evaluate generative quality recognition performance proposed method furthermore demonstrate applicability upsampling model using realworld dataset captured lowresolution sensor across varying measurement distance
soar selfoccluded avatar recovery single video wild selfocclusion common capturing people wild performer follow predefined motion script challenge existing monocular human reconstruction system assume full body visibility introduce selfoccluded avatar recovery soar method complete human reconstruction partial observation part body entirely unobserved soar leverage structural normal prior generative diffusion prior address illposed reconstruction problem structural normal prior model human reposable surfel model welldefined easily readable shape generative diffusion prior perform initial reconstruction refine using score distillation various benchmark show soar performs favorably stateoftheart reconstruction generation method onpar comparing concurrent work additional video result code available httpssoaravatargithubio
coherent temporal synthesis incremental action segmentation data replay successful incremental learning technique image prevents catastrophic forgetting keeping reservoir previous data original synthesized ensure model retains past knowledge adapting novel concept however application video domain rudimentary simply store frame exemplar action recognition paper present first exploration video data replay technique incremental action segmentation focusing action temporal modeling propose temporally coherent action tca model represents action using generative model instead storing individual frame integration conditioning variable capture temporal coherence allows model understand evolution action feature time therefore action segment generated tca replay diverse temporally coherent incremental setup breakfast dataset approach achieves significant increase accuracy compared baseline
inclusion global multimedia deepfake detection towards multidimensional facial forgery detection paper present global multimedia deepfake detection held concurrently inclusion multimedia deepfake detection aim detect automatic image audiovideo manipulation including limited editing synthesis generation photoshopetc challenge attracted team world valid result submission count invite top team present solution challenge top team awarded prize grand finale paper present solution top team two track boost research work field image audiovideo forgery detection methodology developed challenge contribute development nextgeneration deepfake detection system encourage participant open source method
vision audio beyond unified model audiovisual representation generation video encompasses visual auditory data creating perceptually rich experience two modality complement video valuable type medium investigation interplay audio visual element previous study audiovisual modality primarily focused either audiovisual representation learning generative modeling modality conditioned creating disconnect two branch unified framework learns representation generates modality developed yet work introduce novel framework called vision audio beyond vab bridge gap audiovisual representation learning visiontoaudio generation key approach vab rather working raw video frame audio data vab performs representation learning generative modeling within latent space particular vab us pretrained audio tokenizer image encoder obtain audio token visual feature respectively performs pretraining task visualconditioned masked audio token prediction training strategy enables model engage contextual learning simultaneous videotoaudio generation pretraining phase vab employ iterativedecoding approach rapidly generate audio token conditioned visual feature since vab unified model backbone finetuned various audiovisual downstream task experiment showcase efficiency vab producing highquality audio video capability acquire semantic audiovisual feature leading competitive result audiovisual retrieval classification
faster diffusion action segmentation temporal action segmentation ta essential task video analysis aiming segment classify continuous frame distinct action segment however ambiguous boundary action pose significant challenge highprecision segmentation recent advance diffusion model demonstrated substantial success ta task due stable training process highquality generation capability however heavy sampling step required diffusion model pose substantial computational burden limiting practicality realtime application additionally related work utilize transformerbased encoder architecture although architecture excel capturing longrange dependency incur high computational cost face featuresmoothing issue processing long video sequence address challenge propose effidiffact efficient highperformance ta algorithm specifically develop lightweight temporal feature encoder reduces computational overhead mitigates rank collapse phenomenon associated traditional selfattention mechanism furthermore introduce adaptive skip strategy allows dynamic adjustment timestep length based computed similarity metric inference thereby enhancing computational efficiency comprehensive experiment breakfast gtea datasets demonstrated effectiveness proposed algorithm
taqdit timeaware quantization diffusion transformer transformerbased diffusion model dubbed diffusion transformer dit achieved stateoftheart performance image video generation task however large model size slow inference speed limit practical application calling model compression method quantization unfortunately existing dit quantization method overlook impact reconstruction varying quantization sensitivity across different layer hinder achievable performance tackle issue propose innovative timeaware quantization dit taqdit specifically observe nonconvergence issue reconstructing weight activation separately quantization introduce joint reconstruction method resolve problem discover postgelu activation particularly sensitive quantization due significant variability across different denoising step well extreme asymmetry variation within step address propose timevarianceaware transformation facilitate effective quantization experimental result show quantizing dit weight activation method significantly surpasses previous quantization method
shapepreserving generation food image automatic dietary assessment traditional dietary assessment method heavily rely selfreporting timeconsuming prone bias recent advancement artificial intelligence ai revealed new possibility dietary assessment particularly analysis food image recognizing food estimating food volume image known key procedure automatic dietary assessment however procedure required large amount training image labeled food name volume currently unavailable alternatively recent study indicated training image artificially generated using generative adversarial network gans nonetheless convenient generation large amount food image known volume remain challenge existing technique work present simple ganbased neural network architecture conditional food image generation shape food container generated image closely resemble reference input image experiment demonstrate realism generated image shapepreserving capability proposed framework
cospeech facial animation generation multimodality guidance synthesis facial animation speech garnered considerable attention due scarcity highquality facial data wellannotated abundant multimodality label previous method often suffer limited realism lack lexible conditioning address challenge trilogy first introduce generalized neural parametric facial asset gnpfa efficient variational autoencoder mapping facial geometry image highly generalized expression latent space decoupling expression identity utilize gnpfa extract highquality expression accurate head pose large array video present dataset large diverse scanlevel cospeech facial animation dataset wellannotated emotional style label finally propose diffusion model gnpfa latent space cospeech facial animation generation accepting rich multimodality guidance audio text image extensive experiment demonstrate model achieves high fidelity facial animation synthesis also broadens scope expressiveness style adaptability facial animation
videomv consistent multiview generation based large video generative model generating multiview image based text singleimage prompt critical capability creation content two fundamental question topic data use training ensure multiview consistency paper introduces novel framework make fundamental contribution question unlike leveraging image diffusion model training propose dense consistent multiview generation model finetuned offtheshelf video generative model image video generative model suitable multiview generation underlying network architecture generates employ temporal module enforce frame consistency moreover video data set used train model abundant diverse leading reduced trainfinetuning domain gap enhance multiview consistency introduce denoising sampling first employ feedforward reconstruction module get explicit global model adopts sampling strategy effectively involves image rendered global model denoising sampling loop improve multiview consistency final image byproduct module also provides fast way create asset represented gaussians within second approach generate dense view converges much faster training stateoftheart approach gpu hour versus many thousand gpu hour comparable visual quality consistency finetuning approach outperforms existing stateoftheart method quantitative metric visual effect project page
humanactivity agv quality assessment benchmark dataset objective evaluation metric aidriven video generation technique made significant progress recent year however aigenerated video agvs involving human activity often exhibit substantial visual semantic distortion hindering practical application video generation technology realworld scenario address challenge conduct pioneering study human activity agv quality assessment focusing visual quality evaluation identification semantic distortion first construct aigenerated human activity video quality assessment humanagvqa dataset consisting agvs derived popular texttovideo model using text prompt describe diverse human activity conduct subjective study evaluate human appearance quality action continuity quality overall video quality agvs identify semantic issue human body part based humanagvqa benchmark performance model analyze strength weakness generating different category human activity second develop objective evaluation metric named aigenerated human activity video quality metric ghvq automatically analyze quality human activity agvs ghvq systematically extract humanfocused quality feature aigenerated contentaware quality feature temporal continuity feature making comprehensive explainable quality metric human activity agvs extensive experimental result show ghvq outperforms existing quality metric humanagvqa dataset large margin demonstrating efficacy assessing quality human activity agvs humanagvqa dataset ghvq metric released public httpsgithubcomzczhangsjtughvqgit
ddpm based xray image synthesizer access highquality datasets medical industry limit machine learning model performance address issue propose denoising diffusion probabilistic model ddpm combined unet architecture xray image synthesis focused pneumonia medical condition methodology employ pneumonia xray image obtained kaggle training result demonstrate effectiveness approach model successfully generated realistic image low mean squared error mse synthesized image showed distinct difference nonpneumonia image highlighting model ability capture key feature positive case beyond pneumonia application synthesizer extend various medical condition provided ample dataset available capability produce highquality image potentially enhance machine learning model performance aiding accurate efficient medical diagnosis innovative ddpmbased xray photo synthesizer present promising avenue addressing scarcity positive medical image datasets paving way improved medical image analysis diagnosis healthcare industry
aniportrait audiodriven synthesis photorealistic portrait animation study propose aniportrait novel framework generating highquality animation driven audio reference portrait image methodology divided two stage initially extract intermediate representation audio project sequence facial landmark subsequently employ robust diffusion model coupled motion module convert landmark sequence photorealistic temporally consistent portrait animation experimental result demonstrate superiority aniportrait term facial naturalness pose diversity visual quality thereby offering enhanced perceptual experience moreover methodology exhibit considerable potential term flexibility controllability effectively applied area facial motion editing face reenactment release code model weight httpsgithubcomscutzzjaniportrait
high quality human image animation using regional supervision motion blur condition recent advance video diffusion model enabled realistic controllable human image animation temporal coherence although generating reasonable result existing method often overlook need regional supervision crucial area face hand neglect explicit modeling motion blur leading unrealistic lowquality synthesis address limitation first leverage regional supervision detailed region enhance face hand faithfulness second model motion blur explicitly improve appearance quality third explore novel training strategy highresolution human animation improve overall fidelity experimental result demonstrate proposed method outperforms stateoftheart approach achieving significant improvement upon strongest baseline term reconstruction precision perceptual quality fvd humandance dataset code model made available
diffusion model diffusion model emerged powerful generative tool rivaling gans sample quality mirroring likelihood score autoregressive model subset model exemplified ddims exhibit inherent asymmetry trained step sample subset generation selective sampling approach though optimized speed inadvertently miss vital information unsampled step leading potential compromise sample quality address issue present new training method using innovative meticulously designed reintegrate information omitted selective sampling phase benefit approach manifold notably enhances sample quality exceptionally simple implement requires minimal code modification flexible enough compatible various sampling algorithm dataset model trained using algorithm showed improvement model trained traditional method across various sampling algorithm ddims pndms deis different number sampling step celeba dataset improvement ranged access code additional resource provided github
mobius high efficient spatialtemporal parallel training paradigm texttovideo generation task inspired success texttoimage generation task many researcher devoting texttovideo generation task framework usually inherit model add extratemporal layer training generate dynamic video viewed finetuning task however traditional serial mode temporal layer follow spatial layer result high gpu memory training time consumption according serial feature flow believe serial mode bring training cost large diffusion model massive datasets environmentally friendly suitable development therefore propose highly efficient spatialtemporal parallel training paradigm task named mobius temporal layer spatial layer parallel optimizes feature flow backpropagation mobius save gpu memory training time greatly improve finetuning task provide novel insight aigc community release code future
uwafagan ultrawideangle fluorescein angiography transformation via multiscale generation registration enhancement fundus photography combination ultrawideangle fundus uwf technique becomes indispensable diagnostic tool clinical setting offering comprehensive view retina nonetheless uwf fluorescein angiography uwffa necessitates administration fluorescent dye via injection patient hand elbow unlike uwf scanning laser ophthalmoscopy uwfslo mitigate potential adverse effect associated injection researcher proposed development crossmodality medical image generation algorithm capable converting uwfslo image uwffa counterpart current image generation technique applied fundus photography encounter difficulty producing highresolution retinal image particularly capturing minute vascular lesion address issue introduce novel conditional generative adversarial network uwafagan synthesize uwffa uwfslo approach employ multiscale generator attention transmit module efficiently extract global structure local lesion additionally counteract image blurriness issue arises training misaligned data registration module integrated within framework method performs nontrivially inception score detail generation clinical user study indicate uwffa image generated uwafagan clinically comparable authentic image term diagnostic reliability empirical evaluation proprietary uwf image datasets elucidate uwafagan outperforms extant methodology code accessible httpsgithubcomtinysquauwafagan
tvtrees multimodal entailment tree neurosymbolic video reasoning challenging model understand complex multimodal content television clip part videolanguage model often rely singlemodality reasoning lack interpretability combat issue propose tvtrees first multimodal entailment tree generator tvtrees serf approach video understanding promotes interpretable jointmodality reasoning searching tree entailment relationship simple textvideo evidence higherlevel conclusion prove questionanswer pair also introduce task multimodal entailment tree generation evaluate reasoning quality method performance challenging tvqa benchmark demonstrates interpretable stateoftheart zeroshot performance full clip illustrating multimodal entailment tree generation bestofbothworlds alternative blackbox system
adversarially masked video consistency unsupervised domain adaptation study problem unsupervised domain adaptation egocentric video propose transformerbased model learn classdiscriminative domaininvariant feature representation consists two novel design first module called generative adversarial domain alignment network aim learning domaininvariant representation simultaneously learns mask generator domaininvariant encoder adversarial way domaininvariant encoder trained minimize distance source target domain masking generator conversely aim producing challenging mask maximizing domain distance second masked consistency learning module learn classdiscriminative representation enforces prediction consistency masked target video full form better evaluate effectiveness domain adaptation method construct challenging benchmark egocentric video method achieves stateoftheart performance epickitchen proposed benchmark
joyhallo digital human model mandarin audiodriven video generation creating mandarin video present significant challenge collecting comprehensive mandarin datasets difficult complex lip movement mandarin complicate model training compared english study collected hour mandarin speech video jd health international inc employee resulting jdhhallo dataset dataset includes diverse range age speaking style encompassing conversational specialized medical topic adapt joyhallo model mandarin employed chinese model audio feature embedding semidecoupled structure proposed capture interfeature relationship among lip expression pose feature integration improves information utilization efficiency also accelerates inference speed notably joyhallo maintains strong ability generate english video demonstrating excellent crosslanguage generation capability code model available httpsjdhalgogithubiojoyhallo
beyond fvd enhanced evaluation metric video generation quality frechet video distance fvd widely adopted metric evaluating video generation distribution quality however effectiveness relies critical assumption analysis reveals three significant limitation nongaussianity inflated convnet feature space insensitivity feature temporal distortion impractical sample size required reliable estimation finding undermine fvds reliability show fvd fall short standalone metric video generation evaluation extensive analysis wide range metric backbone architecture propose jedi jepa embedding distance based feature derived joint embedding predictive architecture measured using maximum mean discrepancy polynomial kernel experiment multiple opensource datasets show clear evidence superior alternative widely used fvd metric requiring sample reach steady value increasing alignment human evaluation average
vlm see robot human demo video robot action plan via vision language model vision language model vlms recently adopted robotics capability common sense reasoning generalizability existing work applied vlms generate task motion planning natural language instruction simulate training data robot learning work explore using vlm interpret human demonstration video generate robot task planning method integrates keyframe selection visual perception vlm reasoning pipeline named seedo enables vlm see human demonstration explain corresponding plan robot validate approach collected set longhorizon human video demonstrating pickandplace task three diverse category designed set metric comprehensively benchmark seedo several baseline including stateoftheart videoinput vlms experiment demonstrate seedos superior performance deployed generated task plan simulation environment real robot arm
another day unique video captioning discriminative prompting long video contain many repeating action event shot repetition frequently given identical caption make difficult retrieve exact desired clip using text search paper formulate problem unique captioning given multiple clip caption generate new caption clip uniquely identifies propose captioning discriminative prompting cdp predicts property separate identically captioned clip use generate unique caption introduce two benchmark unique captioning based egocentric footage timeloop movie repeating action common demonstrate caption generated cdp improve texttovideo egocentric video timeloop movie
empowering deaf hard hearing community enhancing video caption using large language model today digital age video content prevalent serving primary source information education entertainment however deaf hard hearing dhh community often face significant challenge accessing video content due inadequacy automatic speech recognition asr system providing accurate reliable caption paper address urgent need improve video caption quality leveraging large language model llm present comprehensive study explores integration llm enhance accuracy contextawareness caption generated asr system methodology involves novel pipeline corrects asrgenerated caption using advanced llm explicitly focus model like due robust performance language comprehension generation task introduce dataset representative realworld challenge dhh community face evaluate proposed pipeline result indicate llmenhanced caption significantly improve accuracy evidenced notably lower word error rate wer achieved wer compared original asr caption wer show approximate improvement wer compared original asr caption
towards learning contrast kinetics multicondition latent diffusion model contrast agent dynamic contrast enhanced magnetic resonance imaging allow localize tumor observe contrast kinetics essential cancer characterization respective treatment decisionmaking however contrast agent administration associated adverse health risk also restricted patient pregnancy kidney malfunction adverse reaction contrast uptake key biomarker lesion malignancy cancer recurrence risk treatment response becomes pivotal reduce dependency intravenous contrast agent administration end propose multiconditional latent diffusion model capable acquisition timeconditioned image synthesis dcemri temporal sequence evaluate medical image synthesis additionally propose validate frechet radiomics distance image quality measure based biomarker variability synthetic real imaging data result demonstrate method ability generate realistic multisequence fatsaturated breast dcemri uncover emerging potential deep learning based contrast kinetics simulation publicly share accessible codebase httpsgithubcomrichardobiccnet provide userfriendly library frechet radiomics distance calculation httpspypiorgprojectfrdscore
prediction action visual policy learning via joint denoising process diffusion model demonstrated remarkable capability image generation task including image editing video creation representing good understanding physical world line diffusion model also shown promise robotic control task denoising action known diffusion policy although diffusion generative model diffusion policy exhibit distinct capabilitiesimage prediction robotic action respectivelythey technically follow similar denoising process robotic task ability predict future image generate action highly correlated since share underlying dynamic physical world building insight introduce pad novel visual policy learning framework unifies image prediction robot action within joint denoising process specifically pad utilizes diffusion transformer dit seamlessly integrate image robot state enabling simultaneous prediction future image robot action additionally pad support cotraining robotic demonstration largescale video datasets easily extended robotic modality depth image pad outperforms previous method achieving significant relative improvement full metaworld benchmark utilizing single textconditioned visual policy within dataefficient imitation learning setting furthermore pad demonstrates superior generalization unseen task realworld robot manipulation setting success rate increase compared strongest baseline project page httpssitesgooglecomviewpadpaper
navid videobased vlm plan next step visionandlanguage navigation visionandlanguage navigation vln stand key research problem embodied ai aiming enabling agent navigate unseen environment following linguistic instruction field generalization longstanding challenge either outofdistribution scene sim real paper propose navid videobased large vision language model vlm mitigate generalization gap navid make first endeavor showcase capability vlms achieve stateoftheart level navigation performance without map odometer depth input following human instruction navid requires onthefly video stream monocular rgb camera equipped robot output nextstep action formulation mimic human navigate naturally get rid problem introduced odometer noise gap map depth input moreover videobased approach effectively encode historical observation robot spatiotemporal context decision making instruction following train navid navigation sample collected continuous environment including actionplanning instructionreasoning sample along largescale web data extensive experiment show navid achieves stateoftheart performance simulation environment real world demonstrating superior crossdataset transfer thus believe proposed vlm approach plan next step navigation agent also research field
visionreward finegrained multidimensional human preference learning image video generation visual generative model achieved remarkable progress synthesizing photorealistic image video yet aligning output human preference across critical dimension remains persistent challenge though reinforcement learning human feedback offer promise preference alignment existing reward model visual generation face limitation including blackbox scoring without interpretability potentially resultant unexpected bias present visionreward general framework learning human visual preference image video generation specifically employ hierarchical visual assessment framework capture finegrained human preference leverage linear weighting enable interpretable preference learning furthermore propose multidimensional consistent strategy using visionreward reward model preference optimization visual generation experiment show visionreward significantly outperform existing image video reward model machine metric human evaluation notably visionreward surpasses videoscore preference prediction accuracy texttovideo model visionreward achieve higher pairwise win rate compared model using videoscore code datasets provided httpsgithubcomthudmvisionreward
articulated object manipulation using online axis estimation tracking articulated object manipulation requires precise object interaction object axis must carefully considered previous research employed interactive perception manipulating articulated object typically openloop approach often suffer overlooking interaction dynamic address limitation present closedloop pipeline integrating interactive perception online axis estimation segmented point cloud method leverage interactive perception technique foundation interactive perception inducing slight object movement generate point cloud frame evolving dynamic scene point cloud segmented using segment anything model moving part object masked accurate motion online axis estimation guiding subsequent robotic action approach significantly enhances precision efficiency manipulation task involving articulated object experiment simulated environment demonstrate method outperforms baseline approach especially task demand precise axisbased control project page httpshytidelgithubiovideotrackingforaxisestimation
learning correlation structure vision transformer introduce new attention mechanism dubbed structural selfattention structsa leverage rich correlation pattern naturally emerging keyquery interaction attention structsa generates attention map recognizing spacetime structure keyquery correlation via convolution us dynamically aggregate local context value feature effectively leverage rich structural pattern image video scene layout object motion interobject relation using structsa main building block develop structural vision transformer structvit evaluate effectiveness image video classification task achieving stateoftheart result somethingsomething finegym
mlstrack multilevel semantic interaction rmot new trend multiobject tracking task track object interest using natural language however scarcity paired promptinstance data hinders progress address challenge propose highquality yet lowcost data generation method base unreal engine construct brandnew benchmark dataset named referuecity primarily includes scene intersection surveillance video detailing appearance action people vehicle specifically provides video total expression comparable scale referkitti dataset additionally propose multilevel semanticguided multiobject framework called mlstrack interaction model text enhanced layer layer introduction semantic guidance module sgm semantic correlation branch scb extensive experiment referuecity referkitti datasets demonstrate effectiveness proposed framework achieves stateoftheart performance code datatsets available
grouped discrete representation guide objectcentric learning similar human perceiving visual scene object objectcentric learning ocl abstract dense image video sparse objectlevel feature transformerbased ocl handle complex texture well due decoding guidance discrete representation obtained discretizing noisy feature image video feature map using template feature codebook however treating feature minimal unit overlook composing attribute thus impeding model generalization indexing feature natural number loses attributelevel commonality characteristic thus diminishing heuristic model convergence propose textitgrouped discrete representation gdr address issue grouping feature attribute indexing tuple number extensive experiment across different query initialization dataset modality model architecture gdr consistently improves convergence generalizability visualization show method effectively capture attributelevel information feature source code available upon acceptance
hybrid localglobal context learning neural video compression neural video codecs current stateoftheart method typically adopt multiscale motion compensation handle diverse motion method estimate compress either optical flow deformable offset reduce interframe redundancy however flowbased method often suffer inaccurate motion estimation complicated scene deformable convolutionbased method robust higher bit cost motion coding paper propose hybrid context generation module combine advantage method optimal way achieves accurate compensation low bit cost specifically considering characteristic feature different scale adopt flowguided deformable compensation largestscale produce accurate alignment detailed region smallerscale feature perform flowbased warping save bit cost motion coding furthermore design localglobal context enhancement module fully explore localglobal information previous reconstructed signal experimental result demonstrate proposed hybrid localglobal context learning hlgc method significantly enhance stateoftheart method standard test datasets
egopoints advancing point tracking egocentric video introduce egopoints benchmark point tracking egocentric video annotate challenging track egocentric sequence compared popular tapviddavis evaluation benchmark include point go outofview point require reidentification reid returning view measure performance model challenging point introduce evaluation metric specifically monitor tracking performance point inview outofview point require reidentification propose pipeline create semireal sequence automatic ground truth generate sequence combining dynamic kubric object scene point epic field finetuning point tracking method sequence evaluating annotated egopoints sequence improve cotracker across metric including tracking accuracy percentage point accuracy reid sequence point also improve pip respectively
prism semisupervised multiview stereo monocular structure prior promise unsupervised multiviewstereo mv leverage large unlabeled datasets yet current method underperform training difficult data handheld smartphone video indoor scene meanwhile highquality synthetic datasets available mv network trained datasets fail generalize realworld example bridge gap propose semisupervised learning framework allows u train real rendered image jointly capturing structural prior synthetic data ensuring parity realworld domain central framework novel set loss leverage powerful existing monocular relativedepth estimator trained synthetic dataset transferring rich structure relative depth mv prediction unlabeled data inspired perceptual image metric compare mv monocular prediction via deep feature loss multiscale statistical loss full framework call prism achieves large quantitative qualitative improvement current unsupervised syntheticsupervised mv network bestcasescenario result opening door using unlabeled smartphone video photorealistic synthetic datasets training mv network
texttoimage gan pretrained representation generating desired image conditioned given text description received lot attention recently diffusion model autoregressive model demonstrated outstanding expressivity gradually replaced gan favored architecture texttoimage synthesis however still face obstacle slow inference speed expensive training cost achieve powerful faster texttoimage synthesis complex scene propose tiger texttoimage gan pretrained representation specific propose visionempowered discriminator highcapacity generator visionempowered discriminator absorbs complex scene understanding ability domain generalization ability pretrained vision model enhance model performance unlike previous work explore stacking multiple pretrained model discriminator collect multiple different representation ii highcapacity generator aim achieve effective textimage fusion increasing model capacity highcapacity generator consists multiple novel highcapacity fusion block hfblock hfblock contains several deep fusion module global fusion module play different role benefit model extensive experiment demonstrate outstanding performance proposed tiger standard zeroshot texttoimage synthesis task standard texttoimage synthesis task tiger achieves stateoftheart performance two challenging datasets obtain new fid coco cub zeroshot texttoimage synthesis task achieve comparable performance fewer model parameter smaller training data size faster inference speed additionally experiment analysis conducted supplementary material
enhancing humancentered dynamic scene understanding via multiple llm collaborated reasoning humancentered dynamic scene understanding play pivotal role enhancing capability robotic autonomous system videobased humanobject interaction vhoi detection crucial task semantic scene understanding aimed comprehensively understanding hoi relationship within video benefit behavioral decision mobile robot autonomous driving system although previous vhoi detection model made significant stride accurate detection specific datasets still lack general reasoning ability like human being effectively induce hoi relationship study propose vhoi multillms collaborated reasoning vhoi mlcr novel framework consisting series plugandplay module could facilitate performance current vhoi detection model leveraging strong reasoning ability different offtheshelf pretrained large language model llm design twostage collaboration system different llm vhoi task specifically first stage design crossagents reasoning scheme leverage llm conduct reasoning different aspect second stage perform multillms debate get final reasoning answer based different knowledge different llm additionally devise auxiliary training strategy utilizes clip large visionlanguage model enhance base vhoi model discriminative ability better cooperate llm validate superiority design demonstrating effectiveness improving prediction accuracy base vhoi model via reasoning multiple perspective
hierarchical openvocabulary scene graph languagegrounded robot navigation recent openvocabulary robot mapping method enrich dense geometric map pretrained visuallanguage feature map allow prediction pointwise saliency map queried certain language concept largescale environment abstract query beyond object level still pose considerable hurdle ultimately limiting languagegrounded robotic navigation work present hovsg hierarchical openvocabulary scene graph mapping approach languagegrounded robot navigation leveraging openvocabulary vision foundation model first obtain stateoftheart openvocabulary segmentlevel map subsequently construct scene graph hierarchy consisting floor room object concept enriched openvocabulary feature approach able represent multistory building allows robotic traversal using crossfloor voronoi graph hovsg evaluated three distinct datasets surpasses previous baseline openvocabulary semantic accuracy object room floor level producing reduction representation size compared dense openvocabulary map order prove efficacy generalization capability hovsg showcase successful longhorizon languageconditioned robot navigation within realworld multistorage environment provide code trial video data httphovsggithubio
eliteevgs learning eventbased gaussian splatting distilling eventtovideo prior event camera bioinspired sensor output asynchronous sparse event stream instead fixed frame benefiting distinct advantage high dynamic range high temporal resolution event camera applied address reconstruction important robotic mapping recently neural rendering technique gaussian splatting shown successful reconstruction however still remains underexplored develop effective eventbased pipeline particular typically depends highquality initialization dense multiview constraint potential problem appears optimization event given inherent sparse property end propose novel eventbased framework named eliteevgs key idea distill prior knowledge offtheshelf eventtovideo model effectively reconstruct scene event coarsetofine optimization manner specifically address complexity initialization event introduce novel warmup initialization strategy optimizes coarse frame generated model incorporates event refine detail propose progressive event supervision strategy employ windowslicing operation progressively reduce number event used supervision subtly relives temporal randomness event frame benefiting optimization local textural global structural detail experiment benchmark datasets demonstrate eliteevgs reconstruct scene better textural structural detail meanwhile method yield plausible performance captured realworld data including diverse challenging condition fast motion low light scene
posetalk textandaudiobased pose control motion refinement oneshot talking head generation previous audiodriven talking head generation thg method generate head pose driving audio generated pose lip match audio well editable study propose textbfposetalk thg system freely generate lipsynchronized talking head video free head pose conditioned text prompt audio core insight method using head pose connect visual linguistic audio signal first propose generate pose audio text prompt audio offer shortterm variation rhythm correspondence head movement text prompt describe longterm semantics head motion achieve goal devise pose latent diffusion pld model generate motion latent text prompt audio cue pose latent space second observe lossimbalance problem loss lip region contributes less total reconstruction loss caused pose lip making optimization lean towards head movement rather lip shape address issue propose refinementbased learning strategy synthesize natural talking video using two cascaded network ie coarsenet refinenet coarsenet estimate coarse motion produce animated image novel pose refinenet focus learning finer lip motion progressively estimating lip motion lowtohigh resolution yielding improved lipsynchronization performance experiment demonstrate pose prediction strategy achieves better pose diversity realness compared textonly audioonly video generator model outperforms stateoftheart method synthesizing talking video natural head motion project httpsjunleengithubioprojectsposetalk
mora enabling generalist video generation via multiagent framework texttovideo generation made significant stride replicating capability advanced system like openai sora remains challenging due closedsource nature existing opensource method struggle achieve comparable performance often hindered ineffective agent collaboration inadequate training data quality paper introduce mora novel multiagent framework leverage existing opensource module replicate sora functionality address fundamental limitation proposing three key technique multiagent finetuning selfmodulation factor enhance interagent coordination datafree training strategy us large model synthesize training data humanintheloop mechanism combined multimodal large language model data filtering ensure highquality training datasets comprehensive experiment six video generation task demonstrate mora achieves performance comparable sora vbench outperforming existing opensource method across various task specifically texttovideo generation task mora achieved video quality score surpassing sora outperforming baseline model across six key metric additionally imagetovideo generation task mora achieved perfect dynamic degree score demonstrating exceptional capability enhancing motion realism achieving higher imaging quality sora result highlight potential collaborative multiagent system humanintheloop mechanism advancing texttovideo generation code available urlhttpsgithubcomlichaosunmora
ganmut generating modifying facial expression realm emotion synthesis ability create authentic nuanced facial expression continues gain importance ganmut study discusses recently introduced advanced gan framework instead relying predefined label learns dynamic interpretable emotion space methodology map discrete emotion vector starting neutral state magnitude reflecting emotion intensity current project aim extend study framework benchmarking across various datasets image resolution facial detection methodology involve conducting series experiment using two emotional datasets affnet contains video captured uncontrolled environment include diverse camera angle head position lighting condition providing realworld challenge affnet offer image labelled emotion improving diversity emotional expression available training first two experiment focus training ganmut using dataset processed either retinaface mtcnn highperformance deep learning face detector setup help determine well ganmut learn synthesise emotion challenging condition assess comparative effectiveness face detection technology subsequent two experiment merge affnet datasets combining real world variability diverse emotional label affnet face detector retinaface mtcnn employed evaluate whether enhanced diversity combined datasets improves ganmuts performance compare impact face detection method hybrid setup
microplastic identification using aidriven image segmentation gangenerated ecological context current method microplastic identification water sample costly require expert analysis propose deep learning segmentation model automatically identify microplastics microscopic image labeled image microplastic moore institute plastic pollution research employ generative adversarial network gan supplement generate diverse training data verify validity generated data conducted reader study expert able discern generated microplastic real microplastic rate percent segmentation model trained combined data achieved diverse dataset compared model without generated data finding aim enhance ability expert citizen detect microplastic across diverse ecological context thereby improving cost accessibility microplastic analysis
crossmodality image synthesis tofmra cta using diffusionbased model cerebrovascular disease often requires multiple imaging modality accurate diagnosis treatment monitoring computed tomography angiography cta timeofflight magnetic resonance angiography tofmra two common noninvasive angiography technique distinct strength accessibility safety diagnostic accuracy cta widely used acute stroke due faster acquisition time higher diagnostic accuracy tofmra preferred safety avoids radiation exposure contrast agentrelated health risk despite predominant role cta clinical workflow scarcity opensource cta data limiting research development ai model task large vessel occlusion detection aneurysm segmentation study explores diffusionbased imagetoimage translation model generate synthetic cta image tofmra input demonstrate modality conversion tofmra cta show diffusion model outperform traditional unetbased approach work compare different stateoftheart diffusion architecture sampler offering recommendation optimal model performance crossmodality translation task
poco policy composition heterogeneous robot learning training general robotic policy heterogeneous data different task significant challenge existing robotic datasets vary different modality color depth tactile proprioceptive information collected different domain simulation real robot human video current method usually collect pool data one domain train single policy handle heterogeneity task domain prohibitively expensive difficult work present flexible approach dubbed policy composition combine information across diverse modality domain learning scenelevel tasklevel generalized manipulation skill composing different data distribution represented diffusion model method use tasklevel composition multitask manipulation composed analytic cost function adapt policy behavior inference time train method simulation human real robot data evaluate tooluse task composed policy achieves robust dexterous performance varying scene task outperforms baseline single data source simulation realworld experiment see httpsliruiwgithubiopolicycomp detail
one shot gans long tail problem skin lesion dataset using novel content space assessment metric long tail problem frequently arise medical field particularly due scarcity medical data rare condition scarcity often lead model overfitting limited sample consequently training model datasets heavily skewed class number sample varies significantly problem emerges training imbalanced datasets result selective detection model accurately identifies image belonging majority class disregard minority class cause model lack generalizability preventing use newer data pose significant challenge developing image detection diagnosis model medical image datasets address challenge one shot gans model employed augment tail class dataset generating additional sample furthermore enhance accuracy novel metric tailored suit one shot gans utilized
interactive scene authoring specialized generative primitive generating highquality digital asset often requires expert knowledge complex design tool introduce specialized generative primitive generative framework allows nonexpert user author highquality scene seamless lightweight controllable manner primitive efficient generative model capture distribution single exemplar real world framework user capture video environment turn highquality explicit appearance model thanks gaussian splatting user select region interest guided semanticallyaware feature create generative primitive adapt generative cellular automaton singleexemplar training controllable generation decouple generative task appearance model operating sparse voxels recover highquality output subsequent sparse patch consistency step primitive trained within minute used author new scene interactively fully compositional manner showcase interactive session various primitive extracted realworld scene controlled create asset scene minute also demonstrate additional capability primitive handling various representation control generation transferring appearance editing geometry
mgancrcm novel multiple generative adversarial network coarserefinement based cognizant method image inpainting image inpainting widely used technique computer vision reconstructing missing damaged pixel image recent advancement generative adversarial network gans demonstrated superior performance traditional method due deep learning capability adaptability across diverse image domain residual network resnet also gained prominence ability enhance feature representation compatibility architecture paper introduces novel architecture combining gan resnet model improve image inpainting outcome framework integrates three component transpose convolutionbased gan guided blind inpainting fast resnetconvolutional neural network frcnn object removal comodulation gan comod gan refinement model performance evaluated benchmark datasets achieving accuracy imagenet celeba comparative analysis demonstrate proposed architecture outperforms existing method highlighting effectiveness qualitative quantitative evaluation
instant policy incontext imitation learning via graph diffusion following impressive capability incontext learning large transformer incontext imitation learning icil promising opportunity robotics introduce instant policy learns new task instantly without training one two demonstration achieving icil two key component first introduce inductive bias graph representation model icil graph generation problem learned diffusion process enabling structured reasoning demonstration observation action second show model trained using pseudodemonstrations arbitrary trajectory generated simulation virtually infinite pool training data simulated real experiment show instant policy enables rapid learning various everyday robot task also show serve foundation crossembodiment zeroshot transfer languagedefined task code video available httpswwwrobotlearningukinstantpolicy
facelift single image head view generation gslrm present facelift feedforward approach rapid highquality head reconstruction single image pipeline begin employing multiview latent diffusion model generates consistent side back view head single facial input generated view serve input gslrm reconstructor produce comprehensive representation using gaussian splat train system develop dataset multiview rendering using synthetic human head asset diffusionbased multiview generator trained exclusively synthetic head image gslrm reconstructor undergoes initial training objaverse followed finetuning synthetic head data facelift excels preserving identity maintaining view consistency across view despite trained solely synthetic data facelift demonstrates remarkable generalization realworld image extensive qualitative quantitative evaluation show facelift outperforms stateoftheart method head reconstruction highlighting practical applicability robust performance realworld image addition single image reconstruction facelift support video input novel view synthesis seamlessly integrates reanimation technique enable facial animation project page httpsweijielyugithubiofacelift
smart scenemotionaware human action recognition framework mental disorder group patient mental disorder often exhibit risky abnormal action climbing wall hitting window necessitating intelligent video behavior monitoring smart healthcare rising internet thing iot technology however development visionbased human action recognition har action hindered lack specialized algorithm datasets paper innovatively propose build visionbased har dataset including abnormal action often occurring mental disorder group introduce novel scenemotionaware action recognition technology framework named smart consisting two technical module first propose scene perception module extract human motion trajectory humanscene interaction feature introduces additional scene information supplementary semantic representation action second multistage fusion module fuse skeleton motion motion trajectory humanscene interaction feature enhancing semantic association skeleton motion supplementary representation thus generating comprehensive representation human motion scene information effectiveness proposed method validated selfcollected har dataset mentalhad achieving accuracy unseen subject scene outperforming stateoftheart approach respectively demonstrated subject scene generalizability make possible smart migration practical deployment smart healthcare system mental disorder patient medical setting code dataset released publicly research httpsgithubcominowlzysmartgit
swaptalk audiodriven talking face generation oneshot customization latent space combining face swapping lip synchronization technology offer costeffective solution customized talking face generation however directly cascading existing model together tends introduce significant interference task reduce video clarity interaction space limited lowlevel semantic rgb space address issue propose innovative unified framework swaptalk accomplishes face swapping lip synchronization task latent space referring recent work face generation choose vqembedding space due excellent editability fidelity performance enhance framework generalization capability unseen identity incorporate identity loss training face swapping module additionally introduce expert discriminator supervision within latent space training lip synchronization module elevate synchronization quality evaluation phase previous study primarily focused selfreconstruction lip movement synchronous audiovisual video better approximate realworld application expand evaluation scope asynchronous audiovideo scenario furthermore introduce novel identity consistency metric comprehensively assess identity consistency time series generated facial video experimental result hdtf demonstrate method significantly surpasses existing technique video quality lip synchronization accuracy face swapping fidelity identity consistency demo available httpswaptalkcc
acdc autoregressive coherent multimodal generation using diffusion correction autoregressive model arm diffusion model dm represent two leading paradigm generative modeling excelling distinct area arm global context modeling longsequence generation dm generating highquality local context especially continuous data image short video however arm often suffer exponential error accumulation long sequence leading physically implausible result dm limited local context generation capability work introduce autoregressive coherent multimodal generation diffusion correction acdc zeroshot approach combine strength arm dm inference stage without need additional finetuning acdc leverage arm global context generation memoryconditioned dm local correction ensuring highquality output correcting artifact generated multimodal token particular propose memory module based large language model llm dynamically adjusts conditioning text dm preserving crucial global context information experiment multimodal task including coherent multiframe story generation autoregressive video generation demonstrate acdc effectively mitigates accumulation error significantly enhances quality generated output achieving superior performance remaining agnostic specific arm dm architecture project page
learning spatiotemporal inconsistency via thumbnail layout face deepfake detection deepfake threat society cybersecurity provoked significant public apprehension driving intensified effort within realm deepfake video detection current videolevel method mostly based cnns resulting high computational demand although achieved good performance paper introduces elegantly simple yet effective strategy named thumbnail layout tall transforms video clip predefined layout realize preservation spatial temporal dependency transformation process involves sequentially masking frame position within frame frame resized subframes reorganized predetermined layout forming thumbnail tall modelagnostic remarkable simplicity necessitating minimal code modification furthermore introduce graph reasoning block grb semantic consistency sc loss strengthen tall culminating tall grb enhances interaction different semantic region capture semanticlevel inconsistency clue semantic consistency loss imposes consistency constraint semantic feature improve model generalization ability extensive experiment intradataset crossdataset diffusiongenerated image detection deepfake generation method recognition show tall achieves result surpassing comparable stateoftheart method demonstrating effectiveness approach various deepfake detection problem code available
pgcs physical law embedded generative cloud synthesis remote sensing image data quantity quality critical information extraction analyzation remote sensing however current remote sensing datasets often fail meet two requirement cloud primary factor degrading data quantity quality limitation affect precision result remote sensing application particularly derived datadriven technique paper physical law embedded generative cloud synthesis method pgcs proposed generate diverse realistic cloud image enhance real data promote development algorithm subsequent task cloud correction cloud detection data augmentation classification recognition segmentation pgcs method involves two key phase spatial synthesis spectral synthesis spatial synthesis phase stylebased generative adversarial network utilized simulate spatial characteristic generating infinite number singlechannel cloud spectral synthesis phase atmospheric scattering law embedded local statistic global fitting method converting singlechannel cloud multispectral cloud experimental result demonstrate pgcs achieves high accuracy phase performs better three existing cloud synthesis method two cloud correction method developed pgcs exhibit superior performance compared stateoftheart method cloud correction task furthermore application pgcs data various sensor investigated successfully extended code provided httpsgithubcomliyingxupgcs
nonasymptotic bound forward process denoising diffusion ornsteinuhlenbeck hard beat denoising diffusion probabilistic model ddpms represent recent advance generative modelling delivered stateoftheart result across many domain application despite success rigorous theoretical understanding error within ddpms particularly nonasymptotic bound required comparison efficiency remain scarce making minimal assumption initial data distribution allowing example manifold hypothesis paper present explicit nonasymptotic bound forward diffusion error total variation tv expressed function terminal time parametrise multimodal data distribution term distance r furthest mode consider forward diffusion additive multiplicative noise analysis rigorously prof mild assumption canonical choice ornsteinuhlenbeck ou process significantly improved term reducing terminal time function r error tolerance motivated data distribution arising generative modelling also establish cutoff like phenomenon rtoinfty convergence invariant measure tv ou process initialized multimodal distribution maximal mode distance r
maggie masked guided gradual human instance matting human matting foundation task image video processing human foreground pixel extracted input prior work either improve accuracy additional guidance improve temporal consistency single instance across frame propose new framework maggie masked guided gradual human instance matting predicts alpha matte progressively human instance maintaining computational cost precision consistency method leverage modern architecture including transformer attention sparse convolution output instance matte simultaneously without exploding memory latency although keeping constant inference cost multipleinstance scenario framework achieves robust versatile performance proposed synthesized benchmark higher quality image video matting benchmark novel multiinstance synthesis approach publicly available source introduced increase generalization model realworld scenario
automatically generating mixed reality instruction augmenting extracted motion video paper introduces mixed reality system automatically generates sport exercise instruction video mixed reality instruction great potential physical training existing work require substantial time cost create experience overcomes limitation transforming arbitrary instructional video available online mr avatar aienabled motion capture deepmotion automatically enhances avatar motion following augmentation technique contrasting highlighting difference user avatar posture visualizing key trajectory movement specific body part manipulation time speed using body motion spatially repositioning avatar different perspective developed hololens azure kinect showcase various use case including yoga dancing soccer tennis physical exercise study result confirm provides engaging playful learning experience compared existing video instruction
exploring aibased anonymization industrial image video data context feature preservation rising technology protection privacysensitive information becoming increasingly important industry production facility image video recording beneficial documentation tracing production error coordinating workflow individual image video need anonymized however anonymized data reusable application work apply deep learningbased fullbody anonymization framework generates artificial identity industrial image video data compare performance conventional anonymization technique therefore consider quality identity generation temporal consistency applicability pose estimation action recognition
motionbooth motionaware customized texttovideo generation work present motionbooth innovative framework designed animating customized subject precise control object camera movement leveraging image specific object efficiently finetune texttovideo model capture object shape attribute accurately approach present subject region loss video preservation loss enhance subject learning performance along subject token crossattention loss integrate customized subject motion control signal additionally propose trainingfree technique managing subject camera motion inference particular utilize crossattention map manipulation govern subject motion introduce novel latent shift module camera movement control well motionbooth excels preserving appearance subject simultaneously controlling motion generated video extensive quantitative qualitative evaluation demonstrate superiority effectiveness method project page httpsjianzongwugithubioprojectsmotionbooth
rextime benchmark suite reasoningacrosstime video introduce rextime benchmark designed rigorously test ai model ability perform temporal reasoning within video event specifically rextime focus reasoning across time ie humanlike understanding question corresponding answer occur different video segment form reasoning requiring advanced understanding causeandeffect relationship across video segment pose significant challenge even frontier multimodal large language model facilitate evaluation develop automated pipeline generating temporal reasoning questionanswer pair significantly reducing need laborintensive manual annotation benchmark includes carefully vetted validation sample test sample manually curated accuracy relevance evaluation result show frontier large language model outperform academic model still lag behind human performance significant accuracy gap additionally pipeline creates training dataset machine generated sample without manual effort empirical study suggest enhance acrosstime reasoning via finetuning
unified embedding alignment openvocabulary video instance segmentation openvocabulary video instance segmentation vi attracting increasing attention due ability segment track arbitrary object however recent openvocabulary vi attempt obtained unsatisfactory result especially term generalization ability novel category discover domain gap vlm feature eg clip instance query underutilization temporal consistency two central cause mitigate issue design train novel openvocabulary vi baseline called ovformer ovformer utilizes lightweight module unified embedding alignment query embeddings clip image embeddings remedy domain gap unlike previous imagebased training method conduct videobased model training deploy semionline inference scheme fully mine temporal consistency video without bell whistle ovformer achieves map backbone lvvis exceeding previous stateoftheart performance extensive experiment closevocabulary vi datasets also demonstrate strong zeroshot generalization ability ovformer map youtubevis map ovis code available httpsgithubcomfanghaookovformer
ecvoice audio text extraction optimization video based idiom similarity replacement text extraction audio video play important role multimedia editing processing popular open source toolkit whisper performs fast human voice recognition however recognition performance dependent computing resource make low computing memory running whisper become difficult paper present available solution extract human voice video gain high quality text generation voice generated voice used video language translation translated voice simulation improve extraction transform quality human voice present ecvoice method using idiom similarity computation analysis improve quality audio text extraction relative experiment held verify ecvoice improve idiom grammar correction rate average method simple fast mean method cause less bad influence consuming computing resource improving voice recognition rate method solution significantly enhance whisper recognition low computing memory
real face video animation platform recent year facial video generation model gained popularity however model often lack expressive power dealing exaggerated animestyle face due absence highquality animestyle face training set propose facial animation platform enables realtime conversion real human face cartoonstyle face supporting multiple model built gradio framework platform ensures excellent interactivity userfriendliness user input real face video image select desired cartoon style system automatically analyze facial feature execute necessary preprocessing invoke appropriate model generate expressive animestyle face employ variety model within system process hdtf dataset thereby creating animated facial video dataset
oneshot video imitation via parameterized symbolic abstraction graph learning manipulate dynamic deformable object single demonstration video hold great promise term scalability previous approach predominantly focused either replaying object relationship actor trajectory former often struggle generalize across diverse task latter suffers data inefficiency moreover methodology encounter challenge capturing invisible physical attribute force paper propose interpret video demonstration parameterized symbolic abstraction graph psag node represent object edge denote relationship object ground geometric constraint simulation estimate nongeometric visually imperceptible attribute augmented psag applied real robot experiment approach validated across range task cutting avocado cutting vegetable pouring liquid rolling dough slicing pizza demonstrate successful generalization novel object distinct visual physical property
less concatenating video sign language translation small set sign limited amount labeled data training brazilian sign language libra portuguese translation model challenging problem due video collection annotation cost paper proposes generating sign language content concatenating short clip containing isolated signal training sign language translation model employ vlibrasil dataset composed sign video sign interpreted least three person create hundred thousand sentence respective libra translation feed model specifically propose several experiment varying vocabulary size sentence structure generating datasets approximately video result achieve meaningful score meteor respectively technique enables creation extension existing datasets much lower cost collection annotation thousand sentence providing clear direction future work
multicounter multiple action agnostic repetition counting untrimmed video multiinstance repetitive action counting mrac aim estimate number repetitive action performed multiple instance untrimmed video commonly found humancentric domain like sport exercise paper propose multicounter fully endtoend deep learning framework enables simultaneous detection tracking counting repetitive action multiple human instance specifically multicounter incorporates two novel module mixed spatiotemporal interaction efficient context correlation across consecutive frame taskspecific head accurate perception periodic boundary generalization actionagnostic human instance train multicounter synthetic dataset called multirep generated annotated realworld video experiment multirep dataset validate fundamental challenge mrac task showcase superiority proposed model compared bytetrackrepnet solution combine advanced tracker single repetition counter multicounter substantially improves periodmap reduces avgmae increase avgobo time set new benchmark field mrac moreover multicounter run realtime commodity gpu server insensitive number human instance video
languagebased video colorization creative consistent color automatic video colorization inherently illposed problem monochrome frame multiple optional color candidate previous exemplarbased video colorization method restrict user imagination due elaborate retrieval process alternatively conditional image colorization method combined postprocessing algorithm still struggle maintain temporal consistency address issue present languagebased video colorization creative consistent color guide colorization process using userprovided language description model built upon pretrained crossmodality generative model leveraging comprehensive language understanding robust color representation ability introduce crossmodality prefusion module generate instanceaware text embeddings enabling application creative color additionally propose temporally deformable attention prevent flickering color shift crossclip fusion maintain longterm color consistency extensive experimental result demonstrate outperforms relevant method achieving semantically accurate color unrestricted creative correspondence temporally robust consistency
designminds enhancing videobased design ideation visionlanguage model contextinjected large language model ideation critical component videobased design vbd video serve primary medium design exploration inspiration emergence generative ai offer considerable potential enhance process streamlining video analysis facilitating idea generation paper present designminds prototype integrates stateoftheart visionlanguage model vlm contextenhanced large language model llm support ideation vbd evaluate designminds conducted betweensubject study design practitioner comparing performance baseline condition result demonstrate designminds significantly enhances flexibility originality ideation also increasing task engagement importantly introduction technology negatively impact user experience technology acceptance usability
identitypreserving poseguided character animation via facial landmark transformation creating realistic poseguided imagetovideo character animation preserving facial identity remains challenging especially complex dynamic scenario dancing precise identity consistency crucial existing method frequently encounter difficulty maintaining facial coherence due misalignment facial landmark extracted driving video provide head pose expression cue facial geometry reference image address limitation introduce facial landmark transformation flt method leverage morphable model address limitation flt convert landmark face model adjusts face model align reference identity transforms back landmark guide imagetovideo generation process approach ensures accurate alignment reference facial geometry enhancing consistency generated video reference image experimental result demonstrate flt effectively preserve facial identity significantly improving poseguided character animation model
polysmart trecvid video captioning vtt paper present method result videototext vtt task trecvid exploring capability visionlanguage model vlms like llava llavanextvideo generating natural language description video content investigate impact finetuning vlms vtt datasets enhance description accuracy contextual relevance linguistic consistency analysis reveals finetuning substantially improves model ability produce detailed domainaligned text bridging gap generic vlm task specialized need vtt experimental result demonstrate finetuned model outperforms baseline vlms across various evaluation metric underscoring importance domainspecific tuning complex vtt task
multimodal generative ai multimodal llm diffusion beyond multimodal generative ai received increasing attention academia industry particularly two dominant family technique multimodal large language model mllm show impressive ability multimodal understanding ii diffusion model sora exhibit remarkable multimodal power especially respect visual generation one natural question arises possible unified model understanding generation answer question paper first provide detailed review mllm diffusion model including probabilistic modeling procedure multimodal architecture design advanced application imagevideo large language model well texttoimagevideo generation discus two important question unified model whether unified model adopt autoregressive diffusion probabilistic modeling ii whether model utilize dense architecture mixture expertsmoe architecture better support generation understanding two objective provide several possible strategy building unified model analyze potential advantage disadvantage also summarize existing largescale multimodal datasets better model pretraining future conclude paper present several challenging future direction believe contribute ongoing advancement multimodal generative ai
hybrid conditional latent diffusion high frequency enhancement cbcttoct synthesis background conebeam computed tomography cbct play crucial role imageguided radiotherapy artifact noise make unsuitable accurate dose calculation artificial intelligence method shown promise enhancing cbct quality produce synthetic ct sct image however existing method either produce image suboptimal quality incur excessive time cost failing satisfy clinical practice standard method material propose novel hybrid conditional latent diffusion model efficient accurate cbcttoct synthesis named employ unified feature encoder ufe compress image lowdimensional latent space thereby optimizing computational efficiency beyond use cbct image propose integrating highfrequency knowledge hybrid condition guide diffusion model generating sct image preserved structural detail highfrequency information captured using designed highfrequency extractor hfe inference utilize denoising diffusion implicit model facilitate rapid sampling construct new inhouse prostate dataset paired cbct ct validate effectiveness method result extensive experimental result demonstrate approach outperforms stateoftheart method term sct quality generation efficiency moreover medical physicist conduct dosimetric evaluation validate benefit method practical dose calculation achieving remarkable gamma passing rate criterion superior method conclusion proposed efficiently achieve highquality cbcttoct synthesis min per patient promising performance dose calculation show great potential enhancing realworld adaptive radiotherapy
diffusion model enhance resolution microscopy image tutorial diffusion model emerged prominent technique generative modeling neural network making mark task like texttoimage translation superresolution tutorial provide comprehensive guide build denoising diffusion probabilistic model ddpms scratch specific focus transforming lowresolution microscopy image corresponding highresolution version provide theoretical background mathematical derivation detailed python code implementation using pytorch along technique enhance model performance
radrotator rotation radiograph diffusion model transforming twodimensional image threedimensional volume wellknown yet challenging problem computer vision community medical domain previous study attempted convert two input radiograph computed tomography ct volume following effort introduce diffusion modelbased technology rotate anatomical content input radiograph space potentially enabling visualization entire anatomical content radiograph viewpoint similar previous study used ct volume create digitally reconstructed radiograph drrs training data model however addressed two significant limitation encountered previous study utilized conditional diffusion model classifierfree guidance instead generative adversarial network gans achieve higher mode coverage improved output image quality tradeoff slower inference time often less critical medical application demonstrated unreliable output style transfer deep learning dl model cyclegan transfer style actual radiograph drrs could replaced simple yet effective training transformation randomly change pixel intensity histogram input groundtruth imaging data training transformation make diffusion model agnostic distribution variation input data pixel intensity enabling reliable training dl model input drrs applying exact model conventional radiograph drrs inference
data augmentation pipeline generate synthetic labeled datasets echocardiography image using gan due privacy issue limited amount publicly available labeled datasets domain medical imaging propose image generation pipeline synthesize echocardiographic image corresponding ground truth label alleviate need data collection laborious errorprone human labeling image subsequent deep learning dl task proposed method utilizes detailed anatomical segmentation heart ground truth label source initial dataset combined second dataset made real echocardiographic image train generative adversarial network gan synthesize realistic cardiovascular ultrasound image paired ground truth label generate synthetic dataset trained gan us high resolution anatomical model computed tomography ct input qualitative analysis synthesized image showed main structure heart well delineated closely follow label obtained anatomical model assess usability synthetic image dl task segmentation algorithm trained delineate left ventricle left atrium myocardium quantitative analysis segmentation given model trained synthetic image indicated potential use gan approach generate synthetic data use data train dl model different clinical task therefore tackle problem scarcity labeled echocardiography datasets
early stopping criterion training generative adversarial network biomedical imaging generative adversarial network gans high computational cost train complex architecture throughout training process gans output analyzed qualitatively based loss synthetic image diversity quality based qualitative analysis training manually halted desired synthetic image generated utilizing early stopping criterion computational cost dependence manual oversight reduced yet impacted training problem mode collapse nonconvergence instability particularly prevalent biomedical imagery training problem degrade diversity quality synthetic image high computational cost associated training make complex architecture increasingly inaccessible work proposes novel early stopping criterion quantitatively detect training problem halt training reduce computational cost associated synthesizing biomedical image firstly range generator discriminator loss value investigated assess whether mode collapse nonconvergence instability occur sequentially concurrently interchangeably throughout training gans secondly utilizing occurrence conjunction mean structural similarity index msssim frechet inception distance fid score synthetic image form basis proposed early stopping criterion work help identify occurrence training problem gans using lowresource computational cost reduces training time generate diversified highquality synthetic image
glance focus memory prompting multievent video question answering video question answering videoqa emerged vital tool evaluate agent ability understand human daily behavior despite recent success large vision language model many multimodal task complex situation reasoning video involving multiple humanobject interaction event still remains challenging contrast human easily tackle using series episode memory anchor quickly locate questionrelated key moment reasoning mimic effective reasoning strategy propose glancefocus model one simple way apply action detection model predict set action key memory however action within closed set vocabulary hard generalize various video domain instead train encoderdecoder generate set dynamic event memory glancing stage apart using supervised bipartite matching obtain event memory design unsupervised memory generation method get rid dependence event annotation next focusing stage event memory act bridge establish correlation question highlevel event concept lowlevel lengthy video content given question model first focus generated key event memory focus relevant moment reasoning designed multilevel crossattention mechanism conduct extensive experiment four multievent videoqa benchmark including star egotaskqa agqa nextqa proposed model achieves stateoftheart result surpassing current large model various challenging reasoning task code model available
videototext pedestrian monitoring vtpm leveraging computer vision large language model privacypreserve pedestrian activity monitoring intersection computer vision advanced research methodology enhancing system service across various field core component traffic monitoring system improving road safety however monitoring system dont preserve privacy pedestrian appear video potentially revealing identity addressing issue paper introduces videototext pedestrian monitoring vtpm monitor pedestrian movement intersection generates realtime textual report including traffic signal weather information vtpm us computer vision model pedestrian detection tracking achieving latency second per video frame additionally detects crossing violation accuracy incorporating traffic signal data proposed framework equipped generate realtime textual report pedestrian activity stating safety concern like crossing violation conflict impact weather behavior latency second enhance comprehensive analysis generated textual report medium finetuned historical analysis generated textual report finetuning enables reliable analysis pedestrian safety intersection effectively detecting pattern safety critical event proposed vtpm offer efficient alternative video footage using textual report reducing memory usage saving million percent eliminating privacy issue enabling comprehensive interactive historical analysis
image synthesis graph conditioning clipguided diffusion model scene graph advancement generative model sparked significant interest generating image adhering specific structural guideline scene graph image generation one task generating image consistent given scene graph however complexity visual scene pose challenge accurately aligning object based specified relation within scene graph existing method approach task first predicting scene layout generating image layout using adversarial training work introduce novel approach generate image scene graph eliminates need predicting intermediate layout leverage pretrained texttoimage diffusion model clip guidance translate graph knowledge image towards first pretrain graph encoder align graph feature clip feature corresponding image using gan based training fuse graph feature clip embedding object label present given scene graph create graph consistent clip guided conditioning signal conditioning input object embeddings provide coarse structure image graph feature provide structural alignment based relationship among object finally fine tune pretrained diffusion model graph consistent conditioning signal reconstruction clip alignment loss elaborate experiment reveal method outperforms existing method standard benchmark cocostuff visual genome dataset
find finetuning initial noise distribution policy optimization diffusion model recent year largescale pretrained diffusion model demonstrated outstanding capability image video generation task however existing model tend produce visual object commonly found training dataset diverges user input prompt underlying reason behind inaccurate generated result lie model difficulty sampling specific interval initial noise distribution corresponding prompt moreover challenging directly optimize initial distribution given diffusion process involves multiple denoising step paper introduce finetuning initial noise distribution find framework policy optimization unleashes powerful potential pretrained diffusion network directly optimizing initial distribution align generated content userinput prompt end first reformulate diffusion denoising procedure onestep markov decision process employ policy optimization directly optimize initial distribution addition dynamic reward calibration module proposed ensure training stability optimization furthermore introduce ratio clipping algorithm utilize historical data network training prevent optimized distribution deviating far original policy restrain excessive optimization magnitude extensive experiment demonstrate effectiveness method texttoimage texttovideo task surpassing sota method achieving consistency prompt generated content method achieves time faster sota approach homepage available urlhttpsgithubcomvpxecnufindwebsite
additional look ganbased augmentation deep learning image classification availability training data one main limitation deep learning application medical imaging data augmentation popular approach overcome problem new approach machine learning based augmentation particular usage generative adversarial network gan case gans generate image similar original dataset overall training data amount bigger lead better performance trained network gan model consists two network generator discriminator interconnected feedback loop creates competitive environment work continuation previous research trained nvidia limited chest xray image dataset paper study dependence ganbased augmentation performance dataset size focus small sample two datasets considered one image per class image total second image per class image total train set validating quality generated image use trained gans one augmentation approach multiclass classification problem compare quality ganbased augmentation approach two different approach classical augmentation augmentation employing transfer learningbased classification chest xray image result quantified using different classification quality metric compared result literature ganbased augmentation approach found comparable classical augmentation case medium large datasets underperforms case smaller datasets correlation size original dataset quality classification visible independently augmentation approach
add augmenting nerfbased pseudolidar point cloud resolving classimbalance problem typical lidarbased object detection model trained supervised manner realworld data collection often imbalanced class longtailed deal augmenting minorityclass example sampling ground truth gt lidar point database pasting scene interest often used challenge still remain inflexibility locating gt sample limited sample diversity work propose leverage pseudolidar point cloud generated low cost video capturing surround view miniature realworld object minor class method called pseudo ground truth augmentation pgtaug consists three main step volumetric instance reconstruction using view synthesis model ii objectlevel domain alignment lidar intensity estimation iii hybrid contextaware placement method ground map information demonstrate superiority generality method performance improvement extensive experiment conducted three popular benchmark ie nuscenes kitti lyft especially datasets large domain gap captured different lidar configuration code data publicly available upon publication
pcqa strong baseline aigc quality assessment based prompt condition development large language model llm diffusion model brings boom artificial intelligence generated content aigc essential build effective quality assessment framework provide quantifiable evaluation different image video based aigc technology content generated aigc method driven crafted prompt therefore intuitive prompt also serve foundation aigc quality assessment study proposes effective aigc quality assessment qa framework first propose hybrid prompt encoding method based dualsource clip contrastive languageimage pretraining text encoder understand respond prompt condition second propose ensemblebased feature mixer module effectively blend adapted prompt vision feature empirical study practice two datasets aigenerated image quality assessment database texttovideo quality assessment database validates effectiveness proposed method prompt condition quality assessment pcqa proposed simple feasible framework may promote research development multimodal generation field
paindiffusion learning express pain accurate pain expression synthesis essential improving clinical training humanrobot interaction current robotic patient simulator rpss lack realistic pain facial expression limiting effectiveness medical training work introduce paindiffusion generative model synthesizes naturalistic facial pain expression unlike traditional heuristic autoregressive method paindiffusion operates continuous latent space ensuring smoother natural facial motion supporting indefinitelength generation via diffusion forcing approach incorporates intrinsic characteristic pain expressiveness emotion allowing personalized controllable pain expression synthesis train evaluate model using biovid heatpain database additionally integrate paindiffusion robotic system assess applicability realtime rehabilitation exercise qualitative study clinician reveal paindiffusion produce realistic pain expression std preference rate groundtruth recording result suggest paindiffusion serve viable alternative real patient clinical training simulation bridging gap synthetic naturalistic pain expression code video available
latent diffusion model histopathology pretrained embeddings unpaired frozen section ffpe translation frozen section f technique rapid efficient method taking minute prepare slide pathologist evaluation surgery enabling immediate decision surgical intervention however f process often introduces artifact distortion like fold icecrystal effect contrast artifact distortion absent higherquality formalinfixed paraffinembedded ffpe slide require day prepare generative adversarial network ganbased method used translate f ffpe image may leave morphological inaccuracy remaining f artifact introduce new artifact reducing quality translation clinical assessment study benchmark recent generative model focusing gans latent diffusion model ldms overcome limitation introduce novel approach combine ldms histopathology pretrained embeddings enhance restoration f image framework leverage ldms conditioned text pretrained embeddings learn meaningful feature f ffpe histopathology image diffusion denoising technique approach preserve essential diagnostic attribute like color staining tissue morphology also proposes embedding translation mechanism better predict targeted ffpe representation input f image result work achieves significant improvement classification performance area curve rising accompanied advantageous casefd work establishes new benchmark f ffpe image translation quality promising enhanced reliability accuracy histopathology f image analysis work available
magic fixup streamlining photo editing watching dynamic video propose generative model given coarsely edited image synthesizes photorealistic output follows prescribed layout method transfer fine detail original image preserve identity part yet adapts lighting context defined new layout key insight video powerful source supervision task object camera motion provide many observation world change viewpoint lighting physical interaction construct image dataset sample pair source target frame extracted video randomly chosen time interval warp source frame toward target using two motion model mimic expected testtime user edits supervise model translate warped image ground truth starting pretrained diffusion model model design explicitly enables fine detail transfer source frame generated image closely following userspecified layout show using simple segmentation coarse manipulation synthesize photorealistic edit faithful user input addressing secondorder effect like harmonizing lighting physical interaction edited object
humanvid demystifying training data cameracontrollable human image animation human image animation involves generating video character photo allowing user control unlocking potential video movie production recent approach yield impressive result using highquality training data inaccessibility datasets hamper fair transparent benchmarking moreover approach prioritize human motion overlook significance camera motion video leading limited control unstable video generation demystify training data present humanvid first largescale highquality dataset tailored human image animation combine crafted realworld synthetic data realworld data compile vast collection realworld video internet developed applied careful filtering rule ensure video quality resulting curated collection highresolution humancentric video human camera motion annotation accomplished using pose estimator slambased method expand synthetic dataset collected avatar asset leveraged existing asset body shape skin texture clothing notably introduce rulebased camera trajectory generation method enabling synthetic pipeline incorporate diverse precise camera motion annotation rarely found realworld data verify effectiveness humanvid establish baseline model named camanimate short cameracontrollable human animation considers human camera motion condition extensive experimentation demonstrate simple baseline training humanvid achieves stateoftheart performance controlling human pose camera motion setting new benchmark demo data code could found project website httpshumanvidgithubio
depth anything medical image comparative study monocular depth estimation mde critical component many medical tracking mapping algorithm particularly endoscopic laparoscopic video however ground truth depth map acquired real patient data supervised learning viable approach predict depth map medical scene although selfsupervised learning mde recently gained attention output difficult evaluate reliably mdes generalizability patient anatomy limited work evaluates zeroshot performance newly released depth anything model medical endoscopic laparoscopic scene compare accuracy inference speed depth anything mde model trained general scene well indomain model trained endoscopic data finding show although zeroshot capability depth anything quite impressive necessarily better model speed performance hope study spark research employing foundation model mde medical scene
dgd dynamic gaussians distillation tackle task learning dynamic semantic radiance field given single monocular video input learned semantic radiance field capture perpoint semantics well color geometric property dynamic scene enabling generation novel view corresponding semantics enables segmentation tracking diverse set semantic entity specified using simple intuitive interface includes user click text prompt end present dgd unified representation appearance semantics dynamic scene building upon recently proposed dynamic gaussians representation representation optimized time color semantic information key method joint optimization appearance semantic attribute jointly affect geometric property scene evaluate approach ability enable dense semantic object tracking demonstrate highquality result fast render diverse set scene project webpage available httpsisaaclabegithubiodgdwebsite
mllmsul multimodal large language model semantic scene understanding localization traffic scenario multimodal large language model mllms shown satisfactory effect many autonomous driving task paper mllms utilized solve joint semantic scene understanding risk localization task relying frontview image proposed mllmsul framework dualbranch visual encoder first designed extract feature two resolution rich visual information conducive language model describing risk object different size accurately language generation llama model finetuned predict scene description containing type driving scenario action risk object driving intention suggestion egovehicle ultimately transformerbased network incorporating regression token trained locate risk object extensive experiment existing dramarolisp dataset extended dramasris dataset demonstrate method efficient surpassing many stateoftheart imagebased videobased method specifically method achieves score cider score scene understanding task accuracy localization task code datasets available httpsgithubcomfjqtongjimllmsul
evolving singlemodal multimodal facial deepfake detection survey survey address critical challenge deepfake detection amidst rapid advancement artificial intelligence aigenerated medium including video audio text become realistic risk misuse spread misinformation commit identity fraud increase focused facecentric deepfakes work trace evolution traditional singlemodality method sophisticated multimodal approach handle audiovisual textvisual scenario provide comprehensive taxonomy detection technique discus evolution generative method autoencoders gans diffusion model categorize technology unique attribute knowledge first survey kind also explore challenge adapting detection method new generative model enhancing reliability robustness deepfake detector proposing direction future research survey offer detailed roadmap researcher supporting development technology counter deceptive use ai medium creation particularly facial forgery curated list related paper found
vasttrack vast category visual object tracking paper introduce novel benchmark dubbed vasttrack towards facilitating development general visual tracking via encompassing abundant class video vasttrack possesses several attractive property vast object category particular cover target object class largely surpassing object category existing popular benchmark eg class lasot category vast object class expect learn general object tracking larger scale compared current benchmark vasttrack offer sequence million frame make date largest benchmark regarding number video thus could benefit training even powerful visual tracker deep learning era rich annotation besides conventional bounding box annotation vasttrack also provides linguistic description video rich annotation vasttrack enables development visiononly visionlanguage tracking ensure precise annotation video manually labeled multiple round careful inspection refinement understand performance existing tracker provide baseline future comparison extensively assess representative tracker result surprisingly show significant drop compared current datasets due lack abundant category video diverse scenario training effort required improve general tracking vasttrack evaluation result made publicly available httpsgithubcomhenglanvasttrack
clipvis adapting clip openvocabulary video instance segmentation openvocabulary video instance segmentation strives segment track instance belonging open set category video visionlanguage model contrastive languageimage pretraining clip shown robust zeroshot classification ability imagelevel openvocabulary task paper propose simple encoderdecoder network called clipvis adapt clip openvocabulary video instance segmentation clipvis adopts frozen clip introduces three module including classagnostic mask generation temporal topkenhanced matching weighted openvocabulary classification given set initial query classagnostic mask generation introduces pixel decoder transformer decoder clip pretrained image encoder predict query mask corresponding object score mask iou score temporal topkenhanced matching performs query matching across frame using k mostly matched frame finally weighted openvocabulary classification first employ mask pooling generate query visual feature clip pretrained image encoder second performs weighted classification using object score mask iou score clipvis require annotation instance category identity experiment performed various video instance segmentation datasets demonstrate effectiveness proposed method especially novel category using convnextb backbone clipvis achieves ap apn score validation set lvvis dataset outperforms respectively release source code model
image video compression using generative sparse representation fidelity control propose framework learned image video compression using generative sparse visual representation svr guided fidelitypreserving control embedding input discrete latent space spanned learned visual codebooks svrbased compression transmits integer codeword index efficient crossplatform robust however highquality hq reconstruction decoder relies intermediate feature input encoder via direct connection due prohibitively high transmission cost previous svrbased compression method remove feature link resulting largely degraded reconstruction quality work treat intermediate feature fidelitypreserving control signal guide conditioned generative reconstruction decoder instead discarding directly transferring signal draw lowquality lq fidelitypreserving alternative input sent decoder low bitrate control signal provide complementary fidelity cue improve reconstruction quality determined compression rate lq alternative tuned trade bitrate fidelity perceptual quality framework conveniently used learned image compression lic learned video compression lvc since svr robust input perturbation large portion codeword index adjacent frame transferring different index svrbased lic lvc share similar processing pipeline experiment standard image video compression benchmark demonstrate effectiveness approach
improving interpretable embeddings adhoc video search generative caption multiword concept bank aligning user query video clip crossmodal latent space semantic concept two mainstream approach adhoc video search av however effectiveness existing approach bottlenecked small size available videotext datasets low quality concept bank result failure unseen query outofvocabulary problem paper address two problem constructing new dataset developing multiword concept bank specifically capitalizing generative model construct new dataset consisting million generated text video pair pretraining tackle outofvocabulary problem develop multiword concept bank based syntax analysis enhance capability stateoftheart interpretable av method modeling relationship query word also study impact current advanced feature method experimental result show integration aboveproposed element double performance av method msrvtt dataset improves xinfap trecvid av query set eight year margin average
dataset scaling cartoon research handdrawn cartoon animation employ sketch flatcolor segment create illusion motion recent advancement like clip svd sora show impressive result understanding generating natural video scaling large model extensive datasets effective cartoon empirical experiment argue ineffectiveness stem notable bias handdrawn cartoon diverges distribution natural video harness success scaling paradigm benefit cartoon research unfortunately sizable cartoon dataset available exploration research propose dataset first largescale cartoon animation dataset comprises million keyframes covering various artistic style region year comprehensive semantic annotation including videotext description pair anime tag content taxonomy etc pioneer benefit largescale cartoon dataset comprehension generation task finetuning contemporary foundation model like video clip video mamba svd achieving outstanding performance cartoonrelated task motivation introduce largescaling cartoon research foster generalization robustness future cartoon application dataset code pretrained model publicly available
styletalk unified framework controlling speaking style talking head individual unique facial expression head pose style reflect personalized speaking style existing oneshot talking head method capture personalized characteristic therefore fail produce diverse speaking style final video address challenge propose oneshot stylecontrollable talking face generation method obtain speaking style reference speaking video drive oneshot portrait speak reference speaking style another piece audio method aim synthesize stylecontrollable coefficient morphable model including facial expression head movement unified framework specifically proposed framework first leverage style encoder extract desired speaking style reference video transform style code framework us styleaware decoder synthesize coefficient audio input style code decoding framework adopts twobranch architecture generates stylized facial expression coefficient stylized head movement coefficient respectively obtaining coefficient image renderer render expression coefficient specific person talkinghead video extensive experiment demonstrate method generates visually authentic talking head video diverse speaking style one portrait image audio clip
sustechgan image generation object detection adverse condition autonomous driving autonomous driving significantly benefit datadriven deep neural network however data autonomous driving typically fit longtailed distribution critical driving data adverse condition hard collect although generative adversarial network gans applied augment data autonomous driving generating driving image adverse condition still challenging work propose novel framework sustechgan customized dual attention module multiscale generator novel loss function generate driving image improving object detection autonomous driving adverse condition test sustechgan wellknown gans generate driving image adverse condition rain night apply generated image retrain object detection network specifically add generated image training datasets retrain wellknown evaluate improvement retrained object detection adverse condition experimental result show generated driving image sustechgan significantly improved performance retrained rain night condition outperforms wellknown gans opensource code video description datasets available page facilitate image generation development autonomous driving adverse condition
place solution lsvos challenge referring video object segmentation recent transformerbased model dominated referring video object segmentation rvos task due superior performance prior work adopt unified detr framework generate segmentation mask querytoinstance manner work integrate strength leading rvos model build effective paradigm first obtain binary mask sequence rvos model improve consistency quality mask propose twostage multimodel fusion strategy stage rationally ensemble rvos model based framework design well training strategy leverage different video object segmentation vos model enhance mask coherence object propagation mechanism method achieves jf refyoutubevos validation set jf test set rank place largescale video object segmentation challenge iccv track code available
large model based sequential keyframe extraction video summarization keyframe extraction aim sum video semantics minimum number frame paper put forward large model based sequential keyframe extraction video summarization dubbed lmske contains three stage first use large model cut video consecutive shot employ large model generate frame visual feature within shot second develop adaptive clustering algorithm yield candidate keyframes shot candidate keyframe locating nearest cluster center third reduce candidate keyframes via redundancy elimination within shot finally concatenate accordance sequence shot final sequential keyframes evaluate lmske curate benchmark dataset conduct rich experiment whose result exhibit lmske performs much better quite sota competitor average average fidelity average compression ratio
enhancing surveillance camera fov quality via semantic line detection classification deep hough transform quality recorded video image significantly influenced camera field view fov critical application like surveillance system selfdriving car inadequate fov give rise severe safety security concern including car accident theft due failure detect individual object conventional method establishing correct fov heavily rely human judgment lack automated mechanism assess video image quality based fov paper introduce innovative approach harness semantic line detection classification alongside deep hough transform identify semantic line thus ensuring suitable fov understanding view parallel line approach yield effective score public egocart dataset coupled notably high median score line placement metric illustrate method offer straightforward mean assessing quality camera field view achieving classification accuracy metric serve proxy evaluating potential performance video image quality application
audio hallucination large audiovideo language model large audiovideo language model generate description video audio however sometimes ignore audio content producing audio description solely reliant visual information paper refers audio hallucination analyzes large audiovideo language model gather sentence inquiring audio information annotate whether contain hallucination sentence hallucinated also categorize type hallucination result reveal sentence hallucinated distinct trend observed noun verb hallucination type based tackle task audio hallucination classification using pretrained audiotext model zeroshot finetuning setting experimental result reveal zeroshot model achieve higher performance random finetuning model achieve outperforming zeroshot model
trainingfree action recognition goal inference dynamic frame selection introduce vidtfs trainingfree openvocabulary video goal action inference framework combine frozen vision foundational model vfm large language model llm novel dynamic frame selection module experiment demonstrate proposed frame selection module improves performance framework significantly validate performance proposed vidtfs four widely used video datasets including crosstask coin activitynet covering goal inference action recognition task openvocabulary setting without requiring training finetuning result show vidtfs outperforms pretrained instructiontuned multimodal language model directly stack llm vfm downstream video inference task vidtfs adaptability show future potential generalizing new trainingfree video inference task
world model millionlength video language blockwise ringattention enabling longcontext understanding remains key challenge scaling existing sequence model crucial component developing generally intelligent model process operate long temporal horizon potentially consist million token paper aim address challenge providing comprehensive exploration full development process producing context language model videolanguage model setting new benchmark language retrieval new capability long video understanding detail long context data curation process progressive context extension token present efficient opensource implementation scalable training long sequence additionally opensource family parameter model capable processing long text document video exceeding token
multimodality transrectal ultrasound video classification identification clinically significant prostate cancer prostate cancer common noncutaneous cancer world recently multimodality transrectal ultrasound trus increasingly become effective tool guidance prostate biopsy aim effectively identifying prostate cancer propose framework classification clinically significant prostate cancer cspca multimodality trus video framework utilizes two model extract feature bmode image shear wave elastography image respectively adaptive spatial fusion module introduced aggregate two modality feature orthogonal regularized loss used mitigate feature redundancy proposed framework evaluated inhouse dataset containing trus video achieves favorable performance identifying cspca area curve auc furthermore visualized class activation mapping cam image generated proposed framework may provide valuable guidance localization cspca thus facilitating trusguided targeted biopsy code publicly available
digital video manipulation detection technique based compression algorithm digital image video play important role everyday life nowadays people access affordable mobile device equipped advanced integrated camera powerful image processing application technological development facilitates generation multimedia content also intentional modification either recreational malicious purpose forensic technique detect manipulation image video become essential paper proposes forensic technique analysing compression algorithm used coding presence recompression us information macroblocks characteristic standard motion vector vector support machine used create model allows accurately detect video recompressed
hri align videobased hri study design realworld setting hri research using autonomous robot realworld setting produce result highest ecological validity study modality many difficulty limit study feasibility effectiveness propose hri research framework maximize realworld insight offered videobased study hri framework used design online study using firstperson video robot realworld encounter surrogate online study n distinguished withinsubjects effect four robot behavioral condition perceived social intelligence human willingness help robot enter exterior door realworld betweensubjects replication n using two condition confirmed validity online study finding sufficiency participant recruitment target based power analysis online study result hri framework offer hri researcher principled way take advantage efficiency videobased study modality generating directly transferable knowledge realworld hri code data study provided
efficient effective weaklysupervised action segmentation via actiontransitionaware boundary alignment weaklysupervised action segmentation task learning partition long video several action segment training video accompanied transcript ordered list action existing method need infer pseudo segmentation training serial alignment frame transcript timeconsuming hard parallelized training work aim escape inefficient alignment massive redundant frame instead directly localize action transition pseudo segmentation generation transition refers change action segment next adjacent one transcript true transition submerged noisy boundary due intrasegment visual variation propose novel actiontransitionaware boundary alignment atba framework efficiently effectively filter noisy boundary detect transition addition boost semantic learning case noise inevitably present pseudo segmentation also introduce videolevel loss utilize trusted videolevel supervision extensive experiment show effectiveness approach performance training speed
point supervision worth video instance segmentation video instance segmentation vi challenging vision task aim detect segment track object video conventional vi method rely denselyannotated object mask expensive reduce human annotation one point object video frame training obtain highquality mask prediction close fully supervised model proposed training method consists classagnostic proposal generation module provide rich negative sample spatiotemporal pointbased matcher match object query provided point annotation comprehensive experiment three vi benchmark demonstrate competitive performance proposed framework nearly matching fully supervised method
prota probabilistic token aggregation textvideo retrieval textvideo retrieval aim find relevant crossmodal sample given query recent method focus modeling whole spatialtemporal relation however since video clip contain diverse content caption model aligning asymmetric videotext pair high risk retrieving many false positive result paper propose probabilistic token aggregation prota handle crossmodal interaction content asymmetry specifically propose dual partialrelated aggregation disentangle reaggregate token representation lowdimension highdimension space propose tokenbased probabilistic alignment generate tokenlevel probabilistic representation maintain feature representation diversity addition adaptive contrastive loss proposed learn compact crossmodal distribution space based extensive experiment prota achieves significant improvement msrvtt lsmdc didemo
screwmimic bimanual imitation human video screw space projection bimanual manipulation longstanding challenge robotics due large number degree freedom strict spatial temporal synchronization required generate meaningful behavior human learn bimanual manipulation skill watching human refining ability play work aim enable robot learn bimanual manipulation behavior human video demonstration finetune interaction inspired seminal work psychology biomechanics propose modeling interaction two hand serial kinematic linkage screw motion particular use define new action space bimanual manipulation screw action introduce screwmimic framework leverage novel action representation facilitate learning human demonstration selfsupervised policy finetuning experiment demonstrate screwmimic able learn several complex bimanual behavior single human video demonstration outperforms baseline interpret demonstration finetune directly original space motion arm information video result httpsrobinlabcsutexaseduscrewmimic
spacetime reinforcement network video object segmentation recently video object segmentation vos network typically use memorybased method query frame mask predicted spacetime matching memory frame despite method superior performance suffer two issue challenging data destroy spacetime coherence adjacent video frame pixellevel matching lead undesired mismatching caused noise distractors address aforementioned issue first propose generate auxiliary frame adjacent frame serving implicit shorttemporal reference query one next learn prototype video object prototypelevel matching implemented query memory experiment demonstrated network outperforms stateoftheart method davis achieving jf score attains competitive result youtube vos addition network exhibit high inference speed fps
previously recap story summarization introduce multimodal story summarization leveraging tv episode recap short video sequence interweaving key story moment previous episode bring viewer speed propose plotsnap dataset featuring two crime thriller tv show rich recap long episode minute story summarization label unlocked matching recap shot corresponding substories episode propose hierarchical model talesumm process entire episode creating compact shot dialog representation predicts importance score video shot dialog utterance enabling interaction local story group unlike traditional summarization method extract multiple plot point long video present thorough evaluation story summarization including promising crossseries generalization talesumm also show good result classic video summarization benchmark
place solution mose track cvpr pvuw workshop complex video object segmentation complex video object segmentation serf fundamental task wide range downstream application video editing automatic data annotation present place solution mose track pvuw mitigate problem caused tiny object similar object fast movement mose use instance segmentation generate extra pretraining data valid test set mose segmented instance combined object extracted coco augment training data enhance semantic representation baseline model besides motion blur added training increase robustness image blur induced motion finally apply test time augmentation tta memory strategy inference stage method ranked mose track pvuw mathcalj mathcalf mathcaljmathcalf
multiscale temporal difference transformer videotext retrieval currently field videotext retrieval many transformerbased method usually stack frame feature regrade frame token use transformer video temporal modeling however commonly neglect inferior ability transformer modeling local temporal information tackle problem propose transformer variant named multiscale temporal difference transformer mstdt mstdt mainly address defect traditional transformer limited ability capture local temporal information besides order better model detailed dynamic information make use difference feature frame practically reflects dynamic movement video extract interframe difference feature integrate difference frame feature multiscale temporal transformer general proposed mstdt consists shortterm multiscale temporal difference transformer longterm temporal transformer former focus modeling local temporal information latter aim modeling global temporal information last propose new loss narrow distance similar sample extensive experiment show backbone clip mstdt attained new stateoftheart result
videohallucer evaluating intrinsic extrinsic hallucination large videolanguage model recent advancement multimodal large language model mllms extended capability video understanding yet model often plagued hallucination irrelevant nonsensical content generated deviating actual video context work introduces videohallucer first comprehensive benchmark hallucination detection large videolanguage model lvlms videohallucer categorizes hallucination two main type intrinsic extrinsic offering subcategories detailed analysis including objectrelation temporal semantic detail extrinsic factual extrinsic nonfactual hallucination adopt adversarial binary videoqa method comprehensive evaluation pair basic hallucinated question crafted strategically evaluating eleven lvlms videohallucer reveal majority current model exhibit significant issue hallucination ii scaling datasets parameter improves model ability detect basic visual cue counterfactuals provides limited benefit detecting extrinsic factual hallucination iii existing model adept detecting fact identifying hallucination byproduct analysis instruct development selfpep framework achieving average improvement hallucination resistance across model architecture
exposure completing temporally consistent neural high dynamic range video rendering high dynamic range hdr video rendering low dynamic range ldr video frame alternate exposure encounter significant challenge due exposure change absence time stamp exposure change absence make existing method generate flickering hdr result paper propose novel paradigm render hdr frame via completing absent exposure information hence exposure information complete consistent approach involves interpolating neighbor ldr frame time dimension reconstruct ldr frame absent exposure combining interpolated given ldr frame complete set exposure information available time stamp benefit fusing process hdr result reducing noise ghosting artifact therefore improving temporal consistency extensive experimental evaluation standard benchmark demonstrate method achieves stateoftheart performance highlighting importance absent exposure completing hdr video rendering code available
endtoend video question answering frame scoring mechanism adaptive sampling video question answering videoqa emerged challenging frontier field multimedia processing requiring intricate interaction visual textual modality simply uniformly sampling frame indiscriminately aggregating framelevel visual feature often fall short capturing nuanced relevant context video well perform videoqa mitigate issue propose novel videoqa framework equipped tailored frame selection strategy effective efficient videoqa propose three framescoring mechanism consider question relevance interframe similarity evaluate importance frame given question video furthermore design differentiable adaptive frame sampling mechanism facilitate endtoend training frame selector answer generator experimental result across three widely adopted benchmark demonstrate model consistently outperforms existing videoqa method establishing new sota across nextqa star tvqa furthermore quantitative qualitative analysis validate effectiveness design choice
data story towards automatic animated data video creation llmbased multiagent system creating data story raw data challenging due human limited attention span need specialized skill recent advancement large language model llm offer great opportunity develop system autonomous agent streamline data storytelling workflow though multiagent system benefit fully realizing llm potential decomposed task individual agent designing system also face challenge task decomposition performance optimization subtasks workflow design better understand issue develop data director llmbased multiagent system designed automate creation animated data video representative genre data story data director interprets raw data break task design agent role make informed decision automatically seamlessly integrates diverse component data video case study demonstrates data director effectiveness generating data video throughout development derived lesson learned addressing challenge guiding advancement autonomous agent data storytelling also shed light future direction global optimization humanintheloop design application advanced multimodal llm
continuous perception benchmark human continuously perceive process visual signal however current video model typically either sample key frame sparsely divide video chunk densely sample within chunk approach stem fact existing video benchmark addressed analyzing key frame aggregating information separate chunk anticipate next generation vision model emulate human perception processing visual input continuously holistically facilitate development model propose continuous perception benchmark video question answering task solved focusing solely frame captioning small chunk summarizing using language model extensive experiment demonstrate existing model whether commercial opensource struggle task indicating need new technical advancement direction
vdpi video deblurring pseudoinverse modeling video deblurring challenging task aim recover sharp sequence blur noisy observation imageformation model play crucial role traditional modelbased method constraining possible solution however case deep learningbased method despite deeplearning model achieving better result traditional modelbased method remain widely popular due flexibility increasing number scholar combine two achieve better deblurring performance paper proposes introducing knowledge imageformation model deep learning network using pseudoinverse blur use deep network fit blurring estimate pseudoinverse use estimation combined variational deeplearning network deblur video sequence notably experimental result demonstrate modification significantly improve performance deep learning model video deblurring furthermore experiment different datasets achieved notable performance improvement proving proposed method generalize different scenario camera
learning keypoints multiagent behavior analysis using selfsupervision study social interaction collective behavior multiagent video analysis crucial biology selfsupervised keypoint discovery emerged promising solution reduce need manual keypoint annotation existing method often struggle video containing multiple interacting agent especially specie color address introduce bkindmulti novel approach leverage pretrained video segmentation model guide keypoint discovery multiagent scenario eliminates need timeconsuming manual annotation new experimental setting organism extensive evaluation demonstrate improved keypoint regression downstream behavioral classification video fly mouse rat furthermore method generalizes well specie including ant bee human highlighting potential broad application automated keypoint annotation multiagent behavior analysis code available httpsdanielpkhalilgithubiobkindmulti
selfprompting polyp segmentation colonoscopy using hybrid yolosam model early diagnosis treatment polyp colonoscopy essential reducing incidence mortality colorectal cancer crc however variability polyp characteristic presence artifact colonoscopy image video pose significant challenge accurate efficient polyp detection segmentation paper present novel approach polyp segmentation integrating segment anything model sam model method leverage bounding box prediction autonomously generate input prompt sam thereby reducing need manual annotation conducted exhaustive test five benchmark colonoscopy image datasets two colonoscopy video datasets demonstrating method exceeds stateoftheart model image video segmentation task notably approach achieves high segmentation accuracy using bounding box annotation significantly reducing annotation time effort advancement hold promise enhancing efficiency scalability polyp detection clinical setting
shaking fake detecting deepfake video real time via active probe realtime deepfake type generative ai capable creating nonexisting content eg swapping one face another video unfortunately misused produce deepfake video web conference video call identity authentication malicious purpose including financial scam political misinformation deepfake detection countermeasure deepfake attracted considerable attention academic community yet existing work typically rely learning passive feature may perform poorly beyond seen datasets paper propose sfake new realtime deepfake detection method innovatively exploit deepfake model inability adapt physical interference specifically sfake actively sends probe trigger mechanical vibration smartphone resulting controllable feature footage consequently sfake determines whether face swapped deepfake based consistency facial area probe pattern implement sfake evaluate effectiveness selfbuilt dataset compare six detection method result show sfake outperforms detection method higher detection accuracy faster process speed lower memory consumption
dressrecon freeform human reconstruction monocular video present method reconstruct timeconsistent human body model monocular video focusing extremely loose clothing handheld object interaction prior work human reconstruction either limited tight clothing object interaction requires calibrated multiview capture personalized template scan costly collect scale key insight highquality yet flexible reconstruction careful combination generic human prior articulated body shape learned largescale training data videospecific articulated bagofbones deformation fit single video via testtime optimization accomplish learning neural implicit model disentangles body versus clothing deformation separate motion model layer capture subtle geometry clothing leverage imagebased prior human body pose surface normal optical flow optimization resulting neural field extracted timeconsistent mesh optimized explicit gaussians highfidelity interactive rendering datasets highly challenging clothing deformation object interaction dressrecon yield higherfidelity reconstruction prior art project page
tikguard deep learning transformerbased solution detecting unsuitable tiktok content kid rise shortform video platform like tiktok brought new challenge safeguarding young viewer inappropriate content traditional moderation method often fall short handling vast rapidly changing landscape usergenerated video increasing risk child encountering harmful material paper introduces tikguard transformerbased deep learning approach aimed detecting flagging content unsuitable child tiktok using specially curated dataset tikharm leveraging advanced video classification technique tikguard achieves accuracy showing notable improvement existing method similar context direct comparison limited uniqueness tikharm dataset tikguards performance highlight potential enhancing content moderation contributing safer online experience minor study underscore effectiveness transformer model video classification set foundation future research area
solution temporal action localisation task perception test challenge report present method temporal action localisation tal focus identifying classifying action within specific time interval throughout video sequence employ data augmentation technique expanding training dataset using overlapping label dataset enhancing model ability generalize across various action class feature extraction utilize stateoftheart model including umt video feature beat cavmae audio feature approach involves training multimodal video audio unimodal video model followed combining prediction using weighted box fusion wbf method fusion strategy ensures robust action localisation overall approach achieves score securing first place competition
simpler better point tracking pseudolabelling real video stateoftheart point tracker trained synthetic data due difficulty annotating real video task however result suboptimal performance due statistical gap synthetic real video order understand issue better introduce comprising new tracking model new semisupervised training recipe allows real video without annotation used training generating pseudolabels using offtheshelf teacher new model eliminates simplifies component previous tracker resulting simpler often smaller architecture training scheme much simpler prior work achieves better result using time less data study scaling behaviour understand impact using real unsupervised data point tracking model available online offline variant reliably track visible occluded point
pseudo dataset generation outofdomain multicamera view recommendation multicamera system indispensable movie tv show medium selecting appropriate camera every timestamp decisive impact production quality audience preference learningbased view recommendation framework assist professional decisionmaking however often struggle outside training domain scarcity labeled multicamera view recommendation datasets exacerbates issue based insight many video edited original multicamera video propose transforming regular video pseudolabeled multicamera view recommendation datasets promisingly training model pseudolabeled datasets stemming video target domain achieve relative improvement model accuracy target domain bridge accuracy gap indomain neverbeforeseen domain
zeroshot action recognition surveillance video growing demand surveillance public space present significant challenge due shortage human resource current aibased video surveillance system heavily rely core computer vision model require extensive finetuning particularly difficult surveillance setting due limited datasets difficult setting viewpoint low quality etc work propose leveraging large visionlanguage model lvlms known strong zero fewshot generalization tackle video understanding task surveillance specifically explore stateoftheart lvlm improved tokenlevel sampling method selfreflective sampling selfres experiment ucfcrime dataset show represents significant leap zeroshot performance boost baseline selfres additionally increase zeroshot action recognition performance result highlight potential lvlms paired improved sampling technique advancing surveillance video analysis diverse scenario
dycoke dynamic compression token fast video large language model video large language model vllms significantly advanced recently processing complex video content yet inference efficiency remains constrained high computational cost stemming thousand visual token generated video input empirically observe unlike single image input vllms typically attend visual token different frame different decoding iteration making oneshot pruning strategy prone removing important token mistake motivated present dycoke trainingfree token compression method optimize token representation accelerate vllms dycoke incorporates plugandplay temporal compression module minimize temporal redundancy merging redundant token across frame applies dynamic kv cache reduction prune spatially redundant token selectively ensures highquality inference dynamically retaining critical token decoding step extensive experimental result demonstrate dycoke outperform prior sota counterpart achieving inference speedup memory reduction baseline vllm still improving performance training
mufm mambaenhanced feedback model micro video popularity prediction surge microvideos transforming concept popularity researcher delve vast multimodal datasets growing interest understanding origin popularity force driving rapid expansion recent study suggest virality short video tied inherent multimodal content also heavily influenced strength platform recommendation driven audience feedback paper introduce framework capturing longterm dependency user feedback dynamic event interaction based mamba hawkes process experiment largescale opensource multimodal dataset show model significantly outperforms stateoftheart approach across various metric believe model capability map relationship within user feedback behavior sequence contribute evolution nextgeneration recommendation algorithm platform application also enhance understanding micro video dissemination broader societal impact
advanced learningbased inter prediction future video coding fourth generation audio video coding standard inter prediction filter interpf reduces discontinuity prediction adjacent reconstructed pixel inter prediction paper proposes low complexity learningbased inter prediction llip method replace traditional interpf llip enhances filtering process leveraging lightweight neural network model parameter exported efficient inference specifically extract pixel coordinate utilized traditional interpf form training dataset subsequently export weight bias trained neural network model implement inference process without thirdparty dependency enabling seamless integration video codec without relying libtorch thus achieving faster inference speed ultimately replace traditional handcraft filtering parameter interpf learned optimal filtering parameter practical solution make combination deep learning encoding tool traditional video encoding scheme efficient experimental result show approach achieves coding gain u v component random access ra configuration average
hyperseg towards universal visual segmentation large language model paper aim address universal segmentation image video perception strong reasoning ability empowered visual large language model vllms despite significant progress current unified segmentation method limitation adaptation image video scenario well complex reasoning segmentation make difficult handle various challenging instruction achieve accurate understanding finegrained visionlanguage correlation propose hyperseg first vllmbased universal segmentation model pixellevel image video perception encompassing generic segmentation task complex reasoning perception task requiring powerful reasoning ability world knowledge besides fully leverage recognition capability vllms finegrained visual information hyperseg incorporates hybrid entity recognition finegrained visual perceiver module various segmentation task combined temporal adapter hyperseg achieves comprehensive understanding temporal information experimental result validate effectiveness insight resolving universal image video segmentation task including complex reasoning perception task code available
towards universal soccer video understanding globally celebrated sport soccer attracted widespread interest fan world paper aim develop comprehensive multimodal framework soccer video understanding specifically make following contribution paper introduce largest multimodal soccer dataset date featuring video detailed annotation complete match automated annotation pipeline ii present advanced soccerspecific visual encoder matchvision leverage spatiotemporal information across soccer video excels various downstream task iii conduct extensive experiment ablation study event classification commentary generation multiview foul recognition matchvision demonstrates stateoftheart performance substantially outperforming existing model highlight superiority proposed data model believe work offer standard paradigm sport understanding research
lightweight stochastic video prediction via hybrid warping accurate video prediction deep neural network especially dynamic region challenging task computer vision critical application autonomous driving remote working telemedicine due inherent uncertainty existing prediction model often struggle complexity motion dynamic occlusion paper propose novel stochastic longterm video prediction model focus dynamic region employing hybrid warping strategy integrating frame generated forward backward warping approach effectively compensates weakness technique improving prediction accuracy realism moving region video also addressing uncertainty making stochastic prediction account various motion furthermore considering realtime prediction introduce mobilenetbased lightweight architecture model model called svphw achieves stateoftheart performance two benchmark datasets
dynamicvlm simple dynamic visual token compression videollm application large visionlanguage model lvlms analyzing image video exciting rapidly evolving field recent year weve seen significant growth highquality imagetext datasets finetuning image understanding still lack comparable datasets video additionally many videollms extension singleimage vlms may efficiently handle complexity longer video study introduce largescale synthetic dataset created proprietary model using carefully designed prompt tackle wide range question also explore dynamic visual token compression architecture strike balance computational efficiency performance proposed model achieves stateoftheart result across various video task show impressive generalization setting new baseline multiimage understanding notably model delivers absolute improvement llavaonevision videomme muirbench code available httpsgithubcomhonwongbytevideollm
color enhancement vpcc compressed point cloud via attribute map optimization videobased point cloud compression vpcc convert dynamic point cloud data video sequence using traditional video codecs efficient encoding however lossy compression scheme introduces artifact degrade color attribute data paper introduces framework designed enhance color quality vpcc compressed point cloud propose lightweight decompression unet ldcunet neural network optimize projection map generated vpcc encoding optimized map backprojected space enhance corresponding point cloud attribute additionally introduce transfer learning strategy develop customized natural image dataset initial training model finetuned using projection map compressed point cloud whole strategy effectively address scarcity point cloud training data experiment conducted public voxelized full body long sequence dataset demonstrate effectiveness proposed method improving color quality
hierarchical diffusion policy kinematicsaware multitask robotic manipulation paper introduces hierarchical diffusion policy hdp hierarchical agent multitask robotic manipulation hdp factorises manipulation policy hierarchical structure highlevel taskplanning agent predicts distant nextbest endeffector pose nbp lowlevel goalconditioned diffusion policy generates optimal motion trajectory factorised policy representation allows hdp tackle longhorizon task planning generating finegrained lowlevel action generate contextaware motion trajectory satisfying robot kinematics constraint present novel kinematicsaware goalconditioned control agent robot kinematics diffuser rkdiffuser specifically rkdiffuser learns generate endeffector pose joint position trajectory distill accurate kinematicsunaware endeffector pose diffuser kinematicsaware less accurate joint position diffuser via differentiable kinematics empirically show hdp achieves significantly higher success rate stateoftheart method simulation realworld
denoising diffusion probabilistic model six simple step denoising diffusion probabilistic model ddpms popular class deep generative model successfully applied diverse range problem including image video generation protein material synthesis weather forecasting neural surrogate partial differential equation despite ubiquity hard find introduction ddpms simple comprehensive clean clear compact explanation necessary research paper able elucidate different design step taken formulate ddpm rationale step presented often omitted save space moreover exposition typically presented variational lower bound perspective unnecessary arguably harmful obfuscates method working suggests generalisation perform well practice hand perspective take continuous timelimit beautiful general high barriertoentry require background knowledge stochastic differential equation probability flow note distill formulation ddpm six simple step come clear rationale assume reader familiar fundamental topic machine learning including basic probabilistic modelling gaussian distribution maximum likelihood estimation deep learning
differentially private medical image synthesis controllable latent diffusion model generally small size public medical imaging datasets coupled stringent privacy concern hamper advancement datahungry deep learning model medical imaging study address challenge cardiac mri image shortaxis view propose latent diffusion model generate synthetic image conditioned medical attribute ensuring patient privacy differentially private model training knowledge first work apply quantify differential privacy medical image generation pretrain model public data finetune differential privacy uk biobank dataset experiment reveal pretraining significantly improves model performance achieving frechet inception distance fid compared model without pretraining additionally explore tradeoff privacy constraint image quality investigating tighter privacy budget affect output controllability may lead degraded performance result demonstrate proper consideration training differential privacy substantially improve quality synthetic cardiac mri image still notable challenge achieving consistent medical realism
unconditional latent diffusion model memorize patient imaging data implication openly sharing synthetic data ai model present wide range application field medicine however achieving optimal performance requires access extensive healthcare data often readily available furthermore imperative preserve patient privacy restricts patient data sharing third party even within institute recently generative ai model gaining traction facilitating opendata sharing proposing synthetic data surrogate real patient data despite promise model susceptible patient data memorization model generate patient data copy instead novel synthetic sample considering importance problem surprisingly received relatively little attention medical imaging community end assess memorization unconditional latent diffusion model train latent diffusion model ct mr xray datasets synthetic data generation detect amount training data memorized utilizing novel selfsupervised copy detection approach investigate various factor influence memorization finding show surprisingly high degree patient data memorization across datasets comparison nondiffusion generative model autoencoders generative adversarial network indicates latent diffusion model susceptible memorization overall outperform nondiffusion model synthesis quality analysis reveal using augmentation strategy small architecture increasing dataset reduce memorization overtraining model enhance collectively result emphasize importance carefully training generative model private medical imaging datasets examining synthetic data ensure patient privacy sharing medical research application
tracking virtual meeting wild reidentification multiparticipant virtual meeting recent year workplace educational institute widely adopted virtual meeting platform led growing interest analyzing extracting insight meeting requires effective detection tracking unique individual practice standardization video meeting recording layout captured across different platform service turn creates challenge acquiring data stream analyzing uniform fashion approach provides solution general form video recording usually consisting grid participant creffigvideomeeting single video source metadata participant location using least amount constraint assumption data acquired conventional approach often use yolo model coupled tracking algorithm assuming linear motion trajectory akin observed cctv footage however assumption fall short virtual meeting participant video feed window abruptly change location across grid organic video meeting setting participant frequently join leave leading sudden nonlinear movement video grid disrupts optical flowbased tracking method depend linear motion consequently standard object detection tracking method might mistakenly assign multiple participant tracker paper introduce novel approach track reidentify participant remote video meeting utilizing spatiotemporal prior arising data domain turn increase tracking capability compared use general object tracking approach reduces error rate average compared yolobased tracking method baseline
physmotion physicsgrounded dynamic single image introduce physmotion novel framework leverage principled physicsbased simulation guide intermediate representation generated single image input condition eg applied force torque producing highquality physically plausible video generation utilizing continuum mechanicsbased simulation prior knowledge approach address limitation traditional datadriven generative model result consistent physically plausible motion framework begin reconstructing feedforward gaussian single image geometry optimization representation timestepped using differentiable material point method mpm continuum mechanicsbased elastoplasticity model provides strong foundation realistic dynamic albeit coarse level detail enhance geometry appearance ensure spatiotemporal consistency refine initial simulation using texttoimage diffusion model crossframe attention resulting physically plausible video retains intricate detail comparable input image conduct comprehensive qualitative quantitative evaluation validate efficacy method project page available
federated voxel scene graph intracranial hemorrhage intracranial hemorrhage potentially lethal condition whose manifestation vastly diverse shift across clinical center worldwide deeplearningbased solution starting model complex relation brain structure still struggle generalize gathering diverse data natural approach privacy regulation often limit sharing medical data propose first application federated scene graph generation show model leverage increased training data diversity scene graph generation recall clinically relevant relation across datasets compared model trained single centralized dataset learning structured data representation federated setting open way development new method leverage finer information regularize across client effectively
generating synthetic articulated posecontrollable cyclist data computer vision application autonomous driving ad perception cyclist considered safetycritical scene object commonly used publiclyavailable ad datasets typically contain large amount car vehicle object instance low number cyclist instance usually limited appearance pose diversity cyclist training data scarcity problem limit generalization deeplearning perception model cyclist semantic segmentation pose estimation cyclist crossing intention prediction also limit research new cyclistrelated task finegrained cyclist pose estimation spatiotemporal analysis complex interaction human articulated object address data scarcity problem paper propose framework generate synthetic dynamic cyclist data asset used generate training data different task framework designed methodology creating new partbased multiview articulated synthetic bicycle dataset call use train gaussian splatting reconstruction image rendering method propose parametric bicycle composition model assemble posecontrollable bicycle finally using dynamic information cyclist video build complete synthetic dynamic cyclist rider pedaling bicycle reposing selectable synthetic person automatically placing rider onto one new articulated bicycle using proposed keypoint optimizationbased inverse kinematics pose refinement present qualitative quantitative result compare generated cyclist recent stable diffusionbased method
empirical study excitation aggregation design adaption videotext retrieval model transferred clip defactor standard solve video clip retrieval task framelevel input triggering surge model videotext retrieval domain work rethink inherent limitation widelyused mean pooling operation frame feature aggregation investigate adaption excitation aggregation design discriminative video representation generation present novel excitationandaggregation design including excitation module available capturing nonmutuallyexclusive relationship among frame feature achieving framewise feature recalibration aggregation module applied learn exclusiveness used frame representation aggregation similarly employ cascade sequential module aggregation design generate discriminative video representation sequential type besides adopt excitation design tight type obtain representative frame feature multimodal interaction proposed module evaluated three benchmark datasets msrvtt activitynet didemo achieving msrvtt activitynet didemo outperform result relative absolute improvement demonstrating superiority proposed excitation aggregation design hope work serve alternative frame representation aggregation facilitate future research
video coding crosscomponent sample offset beyond exploration traditional spatial temporal subjective visual signal redundancy image video compression recent research focused leveraging crosscolor component redundancy enhance coding efficiency crosscomponent coding approach motivated statistical correlation among different color component ycbcr color space luma color component typically exhibit finer detail chroma cbcr color component inspired previous crosscomponent coding algorithm paper introduces novel inloop filtering approach named crosscomponent sample offset ccso ccso utilizes colocated neighboring luma sample generate correction signal luma chroma reconstructed sample multiplicationfree nonlinear mapping process implemented using lookuptable input mapping group reconstructed luma sample output offset value applied center luma colocated chroma sample experimental result demonstrate proposed ccso applied image video coding resulting improved coding efficiency visual quality method adopted experimental nextgeneration video codec beyond developed alliance open medium aomedia achieving significant objective coding gain psnr vmaf quality metric respectively random access configuration additionally ccso notably improves subjective visual quality
hallucination mitigation prompt longterm video understanding recently multimodal large language model made significant advancement video understanding task however ability understand unprocessed long video limited primarily due difficulty supporting enormous memory overhead although existing method achieve balance memory information aggregating frame inevitably introduce severe hallucination issue address issue paper construct comprehensive hallucination mitigation pipeline based existing mllms specifically use clip score guide frame sampling process question selecting key frame relevant question inject question information query image qformer obtain important visual feature finally answer generation stage utilize chainofthought incontext learning technique explicitly control generation answer worth mentioning breakpoint mode found image understanding model achieved better result video understanding model therefore aggregated answer type model using comparison mechanism ultimately achieved global breakpoint mode respectively moviechat dataset surpassing official baseline model moreover proposed method third place cvpr loveu longterm video question answering challenge code avaiable
graph unfolding sampling transitory video summarization via gershgorin disc alignment usergenerated video ugvs uploaded mobile phone social medium site like youtube tiktok short nonrepetitive summarize transitory ugv several keyframes linear time via fast graph sampling based gershgorin disc alignment gda specifically first model sequence n frame ugv mhop path graph mathcalgo n similarity two frame within time instant encoded positive edge based feature similarity towards efficient sampling unfold mathcalgo path graph mathcalg specified generalized graph laplacian matrix mathcall via one two graph unfolding procedure provable performance bound show maximizing smallest eigenvalue coefficient matrix mathbfb textitdiagleftmathbfhright mu mathcall mathbfh binary keyframe selection vector equivalent minimizing worstcase signal reconstruction error maximize instead gershgorin circle theorem gct lower bound choosing mathbfh via new fast graph sampling algorithm iteratively aligns leftends gershgorin disc graph node frame extensive experiment multiple short video datasets show algorithm achieves comparable better video summarization performance compared stateoftheart method substantially reduced complexity
videgothink assessing egocentric video understanding capability embodied ai recent advancement multimodal large language model mllms opened new avenue application embodied ai building previous work egothink introduce videgothink comprehensive benchmark evaluating egocentric video understanding capability bridge gap mllms lowlevel control embodied ai design four key interrelated task video questionanswering hierarchy planning visual grounding reward modeling minimize manual annotation cost develop automatic data generation pipeline based dataset leveraging prior knowledge multimodal capability three human annotator filter generated data ensure diversity quality resulting videgothink benchmark conduct extensive experiment three type model apibased mllms opensource imagebased mllms opensource videobased mllms experimental result indicate mllms including perform poorly across task related egocentric video understanding finding suggest foundation model still require significant advancement effectively applied firstperson scenario embodied ai conclusion videgothink reflects research trend towards employing mllms egocentric vision akin human capability enabling active observation interaction complex realworld environment
interpretable representation learning video using nonlinear prior learning interpretable representation visual data important challenge make machine decision understandable human improve generalisation outside training distribution end propose deep learning framework one specify nonlinear prior video eg newtonian physic allow model learn interpretable latent variable use generate video hypothetical scenario observed training time extending variational autoencoder vae prior simple isotropic gaussian arbitrary nonlinear temporal additive noise model anm describe large number process eg newtonian physic propose novel linearization method construct gaussian mixture model gmm approximating prior derive numerically stable monte carlo estimate kl divergence posterior prior gmms validate method different realworld physic video including pendulum mass spring falling object pulsar rotating neutron star specify physical prior experiment show correct variable learned model trained intervene change different physical variable oscillation amplitude adding air drag generate physically correct video hypothetical scenario observed previously
towards unified method network dynamic via adversarial weighted link prediction network dynamic eg traffic burst data center network channel fading cellular wifi network great impact performance communication network eg throughput capacity delay jitter article proposes unified predictionbased method handle dynamic various network system view graph deep learning generally formulate dynamic prediction network temporal link prediction task analyze possible challenge prediction weighted network link weight widevaluerange sparsity issue inspired highresolution video frame prediction generative adversarial network gan try adopt adversarial learning generate highquality predicted snapshot network dynamic expected support precise finegrained network control novel highquality temporal link prediction hqtlp model gan developed illustrate potential basic idea extensive experiment various application scenario demonstrate powerful capability hqtlp
stegogan leveraging steganography nonbijective imagetoimage translation imagetoimage translation model postulate unique correspondence exists semantic class source target domain however assumption always hold realworld scenario due divergent distribution different class set asymmetrical information representation conventional gans attempt generate image match distribution target domain may hallucinate spurious instance class absent source domain thereby diminishing usefulness reliability translated image cycleganbased method also known hide mismatched information generated image bypass cycle consistency objective process known steganography response challenge nonbijective image translation introduce stegogan novel model leverage steganography prevent spurious feature generated image approach enhances semantic consistency translated image without requiring additional postprocessing supervision experimental evaluation demonstrate stegogan outperforms existing ganbased model across various nonbijective imagetoimage translation task qualitatively quantitatively code pretrained model accessible httpsgithubcomsianwusidistegogan
endtoend inceptionunet based generative adversarial network snow rain removal superior performance introduced deep learning approach removing atmospheric particle snow rain single image favor usage classical one however deep learningbased approach still suffer challenge related particle appearance characteristic size type transparency furthermore due unique characteristic rain snow particle single network based deep learning approach struggle handling degradation scenario simultaneously paper global framework consists two generative adversarial network gans proposed handle removal particle individually architecture desnowing deraining gans introduce integration feature extraction phase classical unet generator network turn enhances removal performance presence severe variation size appearance furthermore realistic dataset contains pair snowy image next groundtruth image estimated using lowrank approximation approach presented experiment show proposed desnowing deraining approach achieve significant improvement comparison stateoftheart approach tested synthetic realistic datasets
drivingsphere building highfidelity world closedloop simulation autonomous driving evaluation requires simulation environment closely replicate actual road condition including realworld sensory data responsive feedback loop however many existing simulation need predict waypoints along fixed route public datasets synthetic photorealistic data ie openloop simulation usually lack ability assess dynamic decisionmaking recent effort closedloop simulation offer feedbackdriven environment process visual sensor input produce output differ realworld data address challenge propose drivingsphere realistic closedloop simulation framework core idea build world representation generate reallife controllable driving scenario specific framework includes dynamic environment composition module construct detailed driving world format occupancy equipping static background dynamic object visual scene synthesis module transforms data highfidelity multiview video output ensuring spatial temporal consistency providing dynamic realistic simulation environment drivingsphere enables comprehensive testing validation autonomous driving algorithm ultimately advancing development reliable autonomous car benchmark publicly released
deep generative data assimilation multimodal setting robust integration physical knowledge data key improve computational simulation earth system model data assimilation crucial achieving goal provides systematic framework calibrate model output observation include remote sensing imagery ground station measurement uncertainty quantification conventional method including kalman filter variational approach inherently rely simplifying linear gaussian assumption computationally expensive nevertheless rapid adoption datadriven method many area computational science see potential emulating traditional data assimilation deep learning especially generative model particular diffusionbased probabilistic framework large overlap data assimilation principle allows conditional generation sample bayesian inverse framework model shown remarkable success textconditioned image generation imagecontrolled video synthesis likewise one frame data assimilation observationconditioned state calibration work propose slam scorebased latent assimilation multimodal setting specifically assimilate insitu weather station data exsitu satellite imagery calibrate vertical temperature profile globally extensive ablation demonstrate slam robust even lowresolution noisy sparse data setting knowledge work first apply deep generative framework multimodal data assimilation using realworld datasets important step building robust computational simulator including nextgeneration earth system model code available httpsgithubcomyongquanquslams
csg contextsemantic guided diffusion approach de novo musculoskeletal ultrasound image generation use synthetic image medical imaging artificial intelligence ai solution shown beneficial addressing limited availability diverse unbiased representative data despite extensive use synthetic image generation method controlling semantics variability context detail remains challenging limiting effectiveness producing diverse representative medical image datasets work introduce scalable semantic contextconditioned generative model coined csg contextsemantic guidance dual conditioning approach allows comprehensive control structure appearance advancing synthesis realistic diverse ultrasound image demonstrate ability csg generate finding pathological anomaly musculoskeletal msk ultrasound image moreover test quality synthetic image using threefold validation protocol result show synthetic image generated csg improve performance semantic segmentation model exhibit enhanced similarity real image compared baseline method undistinguishable real image according turing test furthermore demonstrate extension csg allows enhancing variability space image synthetically generating augmentation anatomical geometry texture
videogui benchmark gui automation instructional video graphical user interface gui automation hold significant promise enhancing human productivity assisting computer task existing task formulation primarily focus simple task specified single languageonly instruction insert new slide work introduce videogui novel multimodal benchmark designed evaluate gui assistant visualcentric gui task sourced highquality web instructional video benchmark focus task involving professional novel software eg adobe photoshop stable diffusion webui complex activity eg video editing videogui evaluates gui assistant hierarchical process allowing identification specific level may fail highlevel planning reconstruct procedural subtasks visual condition without language description ii middlelevel planning generate sequence precise action narration based visual state ie screenshot goal iii atomic action execution perform specific action accurately clicking designated element level design evaluation metric across individual dimension provide clear signal individual performance clicking dragging typing scrolling atomic action execution evaluation videogui reveals even sota large multimodal model performs poorly visualcentric gui task especially highlevel planning
exploration improvement nerfbased scene editing technique nerfs highquality scene synthesis capability quickly accepted scholar year proposed significant progress made scene representation synthesis however high computational cost limit intuitive efficient editing scene making nerfs development scene editing field facing many challenge paper review preliminary exploration scholar nerf scene object editing field recent year mainly changing shape texture scene object new synthesized scene combination residual model gan transformer nerf generalization ability nerf scene editing expanded including realizing realtime new perspective editing feedback multimodal editing text synthesized scene synthesis performance indepth exploration light shadow editing initially achieving optimization indirect touch editing detail representation complex scene currently nerf editing method focus touch point material indirect point dealing complex larger scene difficult balance accuracy breadth efficiency quality overcoming challenge may become direction future nerf scene editing technology
generative detail compensation via gan diffusion oneshot generalizable neural radiance field paper focus oneshot novel view synthesis onvs task target synthesizing photorealistic novel view given one reference image per scene previous oneshot generalizable neural radiance field ognerf method solve task inferencetime finetuningfree manner yet suffer blurry issue due encoderonly architecture highly relies limited reference image hand recent diffusionbased method show vivid plausible result via distilling pretrained diffusion model representation yet require tedious perscene optimization targeting issue propose generative detail compensation framework via gan diffusion inferencetime finetuningfree vivid plausible detail detail following coarsetofine strategy mainly composed onestage parallel pipeline opp detail enhancer coarse stage opp first efficiently insert gan model existing ognerf pipeline primarily relieving blurry issue indistribution prior captured training dataset achieving good balance sharpness lpips fid fidelity psnr ssim fine stage leverage pretrained image diffusion model complement rich outdistribution detail maintaining decent consistency extensive experiment synthetic realworld datasets show noticeably improves detail without perscene finetuning
diffusion policy generalizable visuomotor policy learning via simple representation imitation learning provides efficient way teach robot dexterous skill however learning complex skill robustly generalizablely usually consumes large amount human demonstration tackle challenging problem present diffusion policy novel visual imitation learning approach incorporates power visual representation diffusion policy class conditional action generative model core design utilization compact visual representation extracted sparse point cloud efficient point encoder experiment involving simulation task successfully handle task demonstration surpasses baseline relative improvement real robot task demonstrates precise control high success rate given demonstration task show excellent generalization ability diverse aspect including space viewpoint appearance instance interestingly real robot experiment rarely violates safety requirement contrast baseline method frequently necessitating human intervention extensive evaluation highlight critical importance representation realworld robot learning video code data available
laplacianguided entropy model neural codec blurdissipated synthesis replacing gaussian decoder conditional diffusion model enhances perceptual quality reconstruction neural image compression lack inductive bias image data restricts ability achieve stateoftheart perceptual level address limitation adopt nonisotropic diffusion model decoder side model imposes inductive bias aimed distinguishing frequency content thereby facilitating generation highquality image moreover framework equipped novel entropy model accurately model probability distribution latent representation exploiting spatiochannel correlation latent space accelerating entropy decoding step channelwise entropy model leverage local global spatial context within channel chunk global spatial context built upon transformer specifically designed image compression task designed transformer employ laplacianshaped positional encoding learnable parameter adaptively adjusted channel cluster experiment demonstrate proposed framework yield better perceptual quality compared cuttingedge generativebased codecs proposed entropy model contributes notable bitrate saving
fetaldiffusion posecontrollable fetal mri synthesis conditional diffusion model quality fetal mri significantly affected unpredictable substantial fetal motion leading introduction artifact even fast acquisition sequence employed development realtime fetal pose estimation approach volumetric epi fetal mri open promising avenue fetal motion monitoring prediction challenge arise fetal pose estimation due limited number real scanned fetal mr training image hindering model generalization acquired fetal mri lack adequate pose study introduce fetaldiffusion novel approach utilizing conditional diffusion model generate synthetic fetal mri controllable pose additionally auxiliary poselevel loss adopted enhance model performance work demonstrates success proposed model producing highquality synthetic fetal mri image accurate recognizable fetal pose comparing favorably invivo real fetal mri furthermore show integration synthetic fetal mr image enhances fetal pose estimation model performance particularly number available real scanned data limited resulting increase pck reduced mean error experiment done single gpu method hold promise improving realtime tracking model thereby addressing fetal motion issue effectively
enhanced segmentation femoral bone metastasis ct scan patient using synthetic data generation diffusion model purpose bone metastasis major impact quality life patient diverse term size location making segmentation complex manual segmentation timeconsuming expert segmentation subject operator variability make obtaining accurate reproducible segmentation bone metastasis ctscans challenging yet important task achieve material method deep learning method tackle segmentation task efficiently require large datasets along expert manual segmentation generalize new image propose automated data synthesis pipeline using denoising diffusion probabilistic model ddpm enchance segmentation femoral metastasis ctscan volume patient used existing lesion along healthy femur create new realistic synthetic metastatic image trained ddpm improve diversity realism simulated volume also investigated operator variability manual segmentation result created new volume trained unet segmentation model real synthetic data compare segmentation performance evaluated performance model depending amount synthetic data used training conclusion result showed segmentation model trained synthetic data outperformed trained real volume model perform especially well considering operator variability
structureaware stylized image synthesis robust medical image segmentation accurate medical image segmentation essential effective diagnosis treatment planning often challenged domain shift caused variation imaging device acquisition condition patientspecific attribute traditional domain generalization method typically require inclusion part test domain within training set always feasible clinical setting limited diverse data additionally although diffusion model demonstrated strong capability image generation style transfer often fail preserve critical structural information necessary precise medical analysis address issue propose novel medical image segmentation method combine diffusion model structurepreserving network structureaware oneshot image stylization approach effectively mitigates domain shift transforming image various source consistent style maintaining location size shape lesion ensures robust accurate segmentation even target domain absent training data experimental evaluation colonoscopy polyp segmentation skin lesion segmentation datasets show method enhances robustness accuracy segmentation model achieving superior performance metric compared baseline model without style transfer structureaware stylization framework offer practical solution improving medical image segmentation across diverse domain facilitating reliable clinical diagnosis
adversarial augmentation training make action recognition model robust realistic video distribution shift despite recent advance video action recognition achieving strong performance existing benchmark model often lack robustness faced natural distribution shift training test data propose two novel evaluation method assess model resilience distribution disparity one method us two different datasets collected different source us one training validation testing precisely created dataset split training testing using subset class overlapping train test datasets proposed method extract feature mean class target evaluation datasets training data ie class prototype estimate test video prediction cosine similarity score sample class prototype target class procedure alter model weight using target dataset require aligning overlapping class two different datasets thus efficient method test model robustness distribution shift without prior knowledge target distribution address robustness problem adversarial augmentation training generating augmented view video hard classification model applying gradient ascent augmentation parameter well curriculum scheduling strength video augmentation experimentally demonstrate superior performance proposed adversarial augmentation approach baseline across three stateoftheart action recognition model tsm video swin transformer uniformer presented work provides critical insight model robustness distribution shift present effective technique enhance video action recognition performance realworld deployment
motion meet attention video motion prompt video contain rich spatiotemporal information traditional method extracting motion used task action recognition often rely visual content rather precise motion feature phenomenon referred blind motion extraction behavior prof inefficient capturing motion interest due lack motionguided cue recently attention mechanism enhanced many computer vision task effectively highlighting salient visual area inspired propose modified sigmoid function learnable slope shift parameter attention mechanism modulate motion signal frame differencing map approach generates sequence attention map enhance processing motionrelated video content ensure temporal continuity smoothness attention map apply pairwise temporal attention variation regularization remove unwanted motion eg noise preserving important one perform hadamard product pair attention map original video frame highlight evolving motion interest time highlighted motion termed video motion prompt subsequently used input model instead original video frame formalize process motion prompt layer incorporate regularization term loss function learn better motion prompt layer serf adapter model video data bridging gap traditional blind motion extraction extraction relevant motion interest show lightweight plugandplay motion prompt layer seamlessly integrates model like slowfast timesformer enhancing performance benchmark finegym mpii cooking
videoautoarena automated arena evaluating large multimodal model video analysis user simulation large multimodal model lmms advanced video analysis capability recently garnered significant attention however evaluation rely traditional method like multiplechoice question benchmark videomme longvideobench prone lack depth needed capture complex demand realworld user address limitationand due prohibitive cost slow pace human annotation video taskswe introduce videoautoarena arenastyle benchmark inspired lmsys chatbot arena framework designed automatically assess lmms video analysis ability videoautoarena utilizes user simulation generate openended adaptive question rigorously assess model performance video understanding benchmark feature automated scalable evaluation framework incorporating modified elo rating system fair continuous comparison across multiple lmms validate automated judging system construct gold standard using carefully curated subset human annotation demonstrating arena strongly aligns human judgment maintaining scalability additionally introduce faultdriven evolution strategy progressively increasing question complexity push model toward handling challenging video analysis scenario experimental result demonstrate videoautoarena effectively differentiates among stateoftheart lmms providing insight model strength area improvement streamline evaluation introduce videoautobench auxiliary benchmark human annotator label winner subset videoautoarena battle use judge compare response humanvalidated answer together videoautoarena videoautobench offer costeffective scalable framework evaluating lmms usercentric video analysis
hindi audiovideodeepfake havdf hindi languagebased audiovideo deepfake dataset deepfakes offer great potential innovation creativity also pose significant risk privacy trust security vast hindispeaking population india particularly vulnerable deepfakedriven misinformation campaign fake video speech hindi enormous impact rural semiurban community digital literacy tends lower people inclined trust video content development effective framework detection tool combat deepfake misuse requires highquality diverse extensive datasets existing popular datasets like ffdf faceforensics dfdc deepfake detection challenge based english language hence paper aim create first novel hindi deep fake dataset named hindi audiovideodeepfake havdf dataset generated using faceswap lipsyn voice cloning method multistep process allows u create rich varied dataset capture nuance hindi speech facial expression providing robust foundation training evaluating deepfake detection model hindi language context unique kind previous datasets contain either deepfake video synthesized audio type deepfake dataset used training detector deepfake video audio datasets notably newly introduced havdf dataset demonstrates lower detection accuracy across existing detection method like headpose etc compared wellknown datasets ffdf dfdc trend suggests havdf dataset present deeper challenge detect possibly due focus hindi language content diverse manipulation technique havdf dataset fill gap hindispecific deepfake datasets aiding multilingual deepfake detection development
instrugen automatic instruction generation visionandlanguage navigation via large multimodal model recent research visionandlanguage navigation vln indicates agent suffer poor generalization unseen environment due lack realistic training environment highquality pathinstruction pair existing method constructing realistic navigation scene high cost extension instruction mainly relies predefined template rule lacking adaptability alleviate issue propose instrugen vln pathinstruction pair generation paradigm specifically use youtube house tour video realistic navigation scene leverage powerful visual understanding generation ability large multimodal model lmms automatically generate diverse highquality vln pathinstruction pair method generates navigation instruction different granularity achieves finegrained alignment instruction visual observation difficult achieve previous method additionally design multistage verification mechanism reduce hallucination inconsistency lmms experimental result demonstrate agent trained pathinstruction pair generated instrugen achieves stateoftheart performance rxr benchmark particularly unseen environment code available
cross group attention groupwise rolling multimodal medical image synthesis multimodal mr image synthesis aim generate missing modality image fusing mapping available mri data existing approach typically adopt imagetoimage translation scheme however method often suffer suboptimal performance due spatial misalignment different modality typically treated input channel therefore paper propose adaptive groupwise interaction network aginet explores intermodality intramodality relationship multimodal mr image synthesis specifically group first predefined along channel dimension perform adaptive rolling standard convolutional kernel capture intermodality spatial correspondence time crossgroup attention module introduced fuse information across different channel group leading better feature representation evaluated effectiveness model publicly available ixi datasets aginet achieved stateoftheart performance multimodal mr image synthesis code released
modeling driver risk perception via attention improve driving assistance advanced driver assistance system ada alert driver safetycritical scenario often provide superfluous alert due lack consideration driver knowledge scene awareness modeling aspect together datadriven way challenging due scarcity critical scenario data incabin driver state world state recorded together explore benefit driver modeling context forward collision warning fcw system working realworld video dataset onroad fcw deployment collect observer subjective validity rating deployed alert also annotate participant gazetoobjects extract trajectory ego vehicle vehicle semiautomatically generate risk estimate scene driver perception two step process first model movement vehicle given scenario joint trajectory forecasting problem reason driver risk perception scene counterfactually modifying input forecasting model represent driver actual observation vehicle scene difference behaviour give u estimate driver behaviour account actual inattentive observation downstream effect overall scene risk compare learned scene representation well traditional worsecase deceleration model achieve future trajectory forecast experiment show using risk formulation generate fcw alert may lead improved false positive rate fcws improved fcw timing
paired conditional generative adversarial network highly accelerated liver mri purpose mri high spatiotemporal resolution desired imageguided liver radiotherapy acquiring densely sampling kspace data timeconsuming accelerated acquisition sparse sample desirable often cause degraded image quality long reconstruction time propose reconstruct paired conditional generative adversarial network recongan shorten mri reconstruction time maintaining reconstruction quality method patient underwent freebreathing liver mri included study fully retrospectively undersampled data time first reconstructed using nufft algorithm recongan trained input output pair three type network unet reconstruction swin transformer explored generator patchgan selected discriminator recongan processed data temporal slice total patient temporal slice split training patient slice test patient slice result recongan consistently achieved comparablebetter psnr ssim rmse score compared csunet model inference time recongan unet c gtv detection task showed recongan c compared unet better improved dice score recongan c unet unprocessed undersampled image conclusion generative network adversarial training proposed promising efficient reconstruction result demonstrated inhouse dataset rapid qualitative reconstruction liver mr potential facilitate online adaptive mrguided radiotherapy liver cancer
gghead fast generalizable gaussian head learning head prior large image collection important step towards highquality human modeling core requirement efficient architecture scale well largescale datasets large image resolution unfortunately existing gans struggle scale generate sample high resolution due relatively slow train render speed typically rely superresolution network expense global consistency address challenge propose generative gaussian head gghead adopts recent gaussian splatting representation within gan framework generate representation employ powerful cnn generator predict gaussian attribute uv space template head mesh way gghead exploit regularity template uv layout substantially facilitating challenging task predicting unstructured set gaussians improve geometric fidelity generated representation novel total variation loss rendered uv coordinate intuitively regularization encourages neighboring rendered pixel stem neighboring gaussians template uv space taken together pipeline efficiently generate head trained singleview image observation proposed framework match quality existing head gans ffhq substantially faster fully consistent result demonstrate realtime generation rendering highquality head resolution first time project website httpstobiaskirschsteingithubiogghead
roboabc affordance generalization beyond category via semantic correspondence robot manipulation enabling robotic manipulation generalizes outofdistribution scene crucial step toward openworld embodied intelligence human being ability rooted understanding semantic correspondence among object naturally transfer interaction experience familiar object novel one although robot lack reservoir interaction experience vast availability human video internet may serve valuable resource extract affordance memory including contact point inspired natural way human think propose roboabc confronted unfamiliar object require generalization robot acquire affordance retrieving object share visual semantic similarity affordance memory next step map contact point retrieved object new object establishing correspondence may present formidable challenge first glance recent research find naturally arises pretrained diffusion model enabling affordance mapping even across disparate object category roboabc framework robot may generalize manipulate outofcategory object zeroshot manner without manual annotation additional training part segmentation precoded knowledge viewpoint restriction quantitatively roboabc significantly enhances accuracy visual affordance retrieval large margin compared stateoftheart sota endtoend affordance model also conduct realworld experiment crosscategory objectgrasping task roboabc achieved success rate proving capacity realworld task
neural rendering hardware acceleration review neural rendering new image video generation method based deep learning combine deep learning model physical knowledge computer graphic obtain controllable realistic scene model realize control scene attribute lighting camera parameter posture one hand neural rendering make full use advantage deep learning accelerate traditional forward rendering process also provide new solution specific task inverse rendering reconstruction hand design innovative hardware structure adapt neural rendering pipeline break parallel computing power consumption bottleneck existing graphic processor expected provide important support future key area virtual augmented reality film television creation digital entertainment artificial intelligence metaverse paper review technical connotation main challenge research progress neural rendering basis analyze common requirement neural rendering pipeline hardware acceleration characteristic current hardware acceleration architecture discus design challenge neural rendering processor architecture finally future development trend neural rendering processor architecture prospected
selfdrsc selfsupervised learning dual reversed rolling shutter correction modern consumer camera commonly employ rolling shutter r imaging mechanism via image captured scanning scene rowbyrow resulting r distortion dynamic scene correct r distortion existing method adopt fully supervised learning manner requires high framerate global shutter g image groundtruth supervision paper propose enhanced selfsupervised learning framework dual reversed r distortion correction selfdrsc firstly introduce lightweight drsc network incorporates bidirectional correlation matching block refine joint optimization optical flow corrected r feature thereby improving correction performance reducing network parameter subsequently effectively train drsc network propose selfsupervised learning strategy ensures cycle consistency input reconstructed dual reversed r image r reconstruction selfdrsc interestingly formulated specialized instance video frame interpolation row reconstructed r image interpolated predicted g image utilizing r distortion time map achieving superior performance simplifying training process selfdrsc enables feasible onestage selfsupervised training additionally besides start end r scanning time selfdrsc allows supervision g image arbitrary intermediate scanning time thus enabling learned drsc network generate high framerate g video code trained model available
navigation world model navigation fundamental skill agent visualmotor capability introduce navigation world model nwm controllable video generation model predicts future visual observation based past observation navigation action capture complex environment dynamic nwm employ conditional diffusion transformer cdit trained diverse collection egocentric video human robotic agent scaled billion parameter familiar environment nwm plan navigation trajectory simulating evaluating whether achieve desired goal unlike supervised navigation policy fixed behavior nwm dynamically incorporate constraint planning experiment demonstrate effectiveness planning trajectory scratch ranking trajectory sampled external policy furthermore nwm leverage learned visual prior imagine trajectory unfamiliar environment single input image making flexible powerful tool nextgeneration navigation system
drivinggpt unifying driving world modeling planning multimodal autoregressive transformer world modelbased searching planning widely recognized promising path toward humanlevel physical intelligence however current driving world model primarily rely video diffusion model specialize visual generation lack flexibility incorporate modality like action contrast autoregressive transformer demonstrated exceptional capability modeling multimodal data work aim unify driving model simulation trajectory planning single sequence modeling problem introduce multimodal driving language based interleaved image action token develop drivinggpt learn joint world modeling planning standard nexttoken prediction drivinggpt demonstrates strong performance actionconditioned video generation endtoend planning outperforming strong baseline largescale nuplan navsim benchmark
generating printready personalized ai art product minimal user input present novel framework advance generative artificial intelligence ai application realm printed art product specifically addressing largeformat product require highresolution artwork framework consists pipeline address two major challenge domain high complexity generating effective prompt low native resolution image produced diffusion model integrating aienhanced prompt generation aipowered upscaling technique framework efficiently produce highquality diverse artistic image suitable many new commercial use case work represents significant step towards democratizing highquality ai art opening new avenue consumer artist designer business
multiscale texture loss ct denoising gans generative adversarial network gans proved powerful framework denoising application medical imaging however ganbased denoising algorithm still suffer limitation capturing complex relationship within image regard loss function play crucial role guiding image generation process encompassing much synthetic image differs real image grasp highly complex nonlinear textural relationship training process work present novel approach capture embed multiscale texture information loss function method introduces differentiable multiscale texture representation image dynamically aggregated selfattention layer thus exploiting endtoend gradientbased optimization validate approach carrying extensive experiment context lowdose ct denoising challenging application aim enhance quality noisy ct scan utilize three publicly available datasets including one simulated two real datasets result promising compared wellestablished loss function also consistent across three different gan architecture code available httpsgithubcomtrainlaboratorymultiscaletexturelossmstlf
face cartoon incremental superresolution using knowledge distillation facial superresolutionhallucination important area research seek enhance lowresolution facial image variety application generative adversarial network gans shown promise area ability adapt new unseen data remains challenge paper address problem proposing incremental superresolution using gans knowledge distillation isrkd face cartoon previous research area investigated incremental learning critical realworld application new data continually generated proposed isrkd aim develop novel unified framework facial superresolution handle different setting including different type face cartoon face various level detail achieve ganbased superresolution network pretrained celeba dataset incrementally trained icartoonface dataset using knowledge distillation retain performance celeba test set improving performance icartoonface test set experiment demonstrate effectiveness knowledge distillation incrementally adding capability model cartoon face superresolution retaining learned knowledge facial hallucination task gans
oed towards onestage endtoend dynamic scene graph generation dynamic scene graph generation dsgg focus identifying visual relationship within spatialtemporal domain video conventional approach often employ multistage pipeline typically consist object detection temporal association multirelation classification however method exhibit inherent limitation due separation multiple stage independent optimization subproblems may yield suboptimal solution remedy limitation propose onestage endtoend framework termed oed streamlines dsgg pipeline framework reformulates task set prediction problem leverage pairwise feature represent subjectobject pair within scene graph moreover another challenge dsgg capturing temporal dependency introduce progressively refined module prm aggregating temporal context without constraint additional tracker handcrafted trajectory enabling endtoend optimization network extensive experiment conducted action genome benchmark demonstrate effectiveness design code model available urlhttpsgithubcomguanwpkuoed
diffusion mri meet diffusion model novel deep generative model diffusion mri generation diffusion mri dmri advanced imaging technique characterizing tissue microstructure white matter structural connectivity human brain demand highquality dmri data growing driven need better resolution improved tissue contrast however acquiring highquality dmri data expensive timeconsuming context deep generative modeling emerges promising solution enhance image quality minimizing acquisition cost scanning time study propose novel generative approach perform dmri generation using deep diffusion model generate high dimension high resolution data preserving gradient information brain structure demonstrated method image mapping task aimed enhancing quality dmri image approach demonstrates highly enhanced performance generating dmri image compared current stateoftheart sota method achievement underscore substantial progression enhancing dmri quality highlighting potential novel generative approach revolutionize dmri imaging standard
openvocabulary spatiotemporal action detection spatiotemporal action detection stad important finegrained video understanding task current method require box label supervision action class advance however realworld application likely come across new action class seen training action category space large hard enumerate also cost data annotation model training new class extremely high traditional method need perform detailed box annotation retrain whole network scratch paper propose new challenging setting performing openvocabulary stad better mimic situation action detection open world openvocabulary spatiotemporal action detection ovstad requires training model limited set base class box label supervision expected yield good generalization performance novel action class ovstad build two benchmark based existing stad datasets propose simple effective method based pretrained videolanguage model vlm better adapt holistic vlm finegrained action detection task carefully finetune localized video regiontext pair customized finetuning endows vlm better motion understanding thus contributing accurate alignment video region text local region feature global video feature fusion alignment adopted improve action detection performance providing global context method achieves promising performance novel class
llavidal large language vision model daily activity living current large language vision model llvms trained web video perform well general video understanding struggle finegrained detail complex humanobject interaction hoi viewinvariant representation learning essential activity daily living adl limitation stem lack specialized adl video instructiontuning datasets insufficient modality integration capture discriminative action representation address propose semiautomated framework curating adl datasets creating adlx multiview multimodal rgbs instructiontuning dataset additionally introduce llavidal llvm integrating video skeleton hois model adls complex spatiotemporal relationship training llavidal simple joint alignment modality yield suboptimal result thus propose multimodal progressive mmpro training strategy incorporating modality stage following curriculum also establish adl mcq video description benchmark assess llvm performance adl task trained adlx llavidal achieves stateoftheart performance across adl benchmark code data made publicly available httpsadlxgithubio
hpc hierarchical progressive coding framework volumetric video volumetric video based neural radiance field nerf hold vast potential various application substantial data volume pose significant challenge compression transmission current nerf compression lack flexibility adjust video quality bitrate within single model various network device capacity address issue propose hpc novel hierarchical progressive volumetric video coding framework achieving variable bitrate using single model specifically hpc introduces hierarchical representation multiresolution residual radiance field reduce temporal redundancy longduration sequence simultaneously generating various level detail propose endtoend progressive learning approach multiratedistortion loss function jointly optimize hierarchical representation compression hpc trained realize multiple compression level current method need train multiple fixedbitrate model different ratedistortion rd tradeoff extensive experiment demonstrate hpc achieves flexible quality level variable bitrate single model exhibit competitive rd performance even outperforming fixedbitrate model across various datasets
zeroshot action localization via confidence large visionlanguage model precise action localization untrimmed video vital field professional sport minimally invasive surgery delineation particular motion recording dramatically enhance analysis many case large scale datasets videolabel pair localization unavailable limiting opportunity finetune videounderstanding model recent development large visionlanguage model lvlm address need impressive zeroshot capability variety video understanding task however adaptation lvlms powerful visual question answering capability zeroshot localization longform video still relatively unexplored end introduce true zeroshot action localization method zeal specifically leverage builtin action knowledge large language model llm inflate action detailed description archetypal start end action description serve query lvlm generating framelevel confidence score aggregated produce localization output simplicity flexibility method lends amenable capable lvlms developed demonstrate remarkable result zeroshot action localization challenging benchmark without training code publicly available hrefhttpsgithubcomjosaklilaizealgithubcomjosaklilaizeal
identitydriven multimedia forgery detection via reference assistance recent advancement deepfake technique paved way generating various medium forgery response potential hazard medium forgery many researcher engage exploring detection method increasing demand highquality medium forgery datasets despite existing datasets certain limitation firstly datasets focus manipulating visual modality usually lack diversity forgery approach considered secondly quality medium often inadequate clarity naturalness meanwhile size dataset also limited thirdly commonly observed realworld forgery motivated identity yet identity information individual portrayed forgery within existing datasets remains underexplored detection identity information could essential clue boost performance moreover official medium concerning relevant identity internet serve prior knowledge aiding audience forgery detector determining true identity therefore propose identitydriven multimedia forgery dataset idforge contains video shot sourced wild video celebrity collected internet fake video shot involve type manipulation across visual audio textual modality additionally idforge provides extra real video shot reference set celebrity correspondingly propose referenceassisted multimodal forgery detection network rmfdn aiming detection deepfake video extensive experiment proposed dataset demonstrate effectiveness rmfdn multimedia detection task
digital twinbased network management better qoe multicast short video streaming multicast short video streaming enhance bandwidth utilization enabling simultaneous video transmission multiple user shared wireless channel existing network management scheme mainly rely sequential buffering principle general quality experience qoe model may deteriorate qoe user swipe behavior exhibit distinct spatiotemporal variation paper propose digital twin dtbased network management scheme enhance qoe firstly user status emulated dt utilized estimate transmission capability watching probability distribution submulticast group smgs adaptive segment buffering smgs buffer aligned unique virtual buffer managed dt finegrained buffer update multicast qoe model consisting rebuffering time video quality quality variation developed considering mutual influence segment buffering among smgs finally joint optimization problem segment version selection slot division formulated maximize qoe efficiently solve problem datamodeldriven algorithm proposed integrating convex optimization method deep reinforcement learning algorithm simulation result based realworld dataset demonstrate proposed dtbased network management scheme outperforms benchmark scheme term qoe improvement
crema generalizable efficient videolanguage reasoning via multimodal modular fusion despite impressive advancement recent multimodal reasoning approach still limited flexibility efficiency model typically process fixed modality input require update numerous parameter paper tackle critical challenge proposes crema generalizable highly efficient modular modalityfusion framework incorporate new modality enhance video reasoning first augment multiple informative modality optical flow point cloud audio thermal heatmap touch map given video without extra human annotation leveraging sensor existing pretrained model next introduce query transformer multiple parameterefficient module associated accessible modality project diverse modality feature llm token embedding space allowing model integrate different data type response generation furthermore propose novel progressive multimodal fusion design supported lightweight fusion module modalitysequential training strategy help compress information across various assisting modality maintaining computational efficiency llm improving performance validate method videolanguage reasoning task assisted diverse modality including conventional videoqa qa achieve betterequivalent performance strong multimodal llm including onellm sevila reducing trainable parameter provide extensive analysis crema including impact modality reasoning domain design fusion module example visualization
lave llmpowered agent assistance language augmentation video editing video creation become increasingly popular yet expertise effort required editing often pose barrier beginner paper explore integration large language model llm video editing workflow reduce barrier design vision embodied lave novel system provides llmpowered agent assistance languageaugmented editing feature lave automatically generates language description user footage serving foundation enabling llm process video assist editing task user provides editing objective agent plan executes relevant action fulfill moreover lave allows user edit video either agent direct ui manipulation providing flexibility enabling manual refinement agent action user study included eight participant ranging novice proficient editor demonstrated laves effectiveness result also shed light user perception proposed llmassisted editing paradigm impact user creativity sense cocreation based finding propose design implication inform future development agentassisted content editing
learn suspected anomaly event prompt video anomaly detection model weakly supervised video anomaly detection wsvad rely multiple instance learning aiming distinguish normal abnormal snippet without specifying type anomaly however ambiguous nature anomaly definition across context may introduce inaccuracy discriminating abnormal normal event show model anomalous novel framework proposed guide learning suspected anomaly event prompt given textual prompt dictionary potential anomaly event caption generated anomaly video semantic anomaly similarity could calculated identify suspected event video snippet enables new multiprompt learning process constrain visualsemantic feature across video well provides new way label pseudo anomaly selftraining demonstrate effectiveness comprehensive experiment detailed ablation study conducted four datasets namely xdviolence ucfcrime tad shanghaitech proposed model outperforms stateoftheart method term ap auc furthermore show promising performance openset crossdataset case data code model found urlhttpsgithubcomshiwoazlap
hdrflow realtime hdr video reconstruction large motion reconstructing high dynamic range hdr video image sequence captured alternating exposure challenging especially presence large camera object motion existing method typically align low dynamic range sequence using optical flow attention mechanism deghosting however often struggle handle large complex motion computationally expensive address challenge propose robust efficient flow estimator tailored realtime hdr video reconstruction named hdrflow hdrflow three novel design hdrdomain alignment loss haloss efficient flow network multisize large kernel mlk new hdr flow training scheme haloss supervises flow network learn hdroriented flow accurate alignment saturated dark region mlk effectively model large motion negligible cost addition incorporate synthetic data sintel training dataset utilizing provided forward flow backward flow generated u supervise flow network enhancing performance large motion region extensive experiment demonstrate hdrflow outperforms previous method standard benchmark best knowledge hdrflow first realtime hdr video reconstruction method video sequence captured alternating exposure capable processing resolution input
unsupervised modalitytransferable video highlight detection representation activation sequence learning identifying highlight moment raw video material crucial improving efficiency editing video pervasive internet platform however extensive work manually labeling footage created obstacle applying supervised method video unseen category absence audio modality contains valuable cue highlight detection many video also make difficult use multimodal strategy paper propose novel model crossmodal perception unsupervised highlight detection proposed model learns representation visualaudio level semantics imageaudio pair data via selfreconstruction task achieve unsupervised highlight detection investigate latent representation network propose representation activation sequence learning rasl module kpoint contrastive learning learn significant representation activation connect visual modality audio modality use symmetric contrastive learning scl module learn paired visual audio representation furthermore auxiliary task masked feature vector sequence fvs reconstruction simultaneously conducted pretraining representation enhancement inference crossmodal pretrained model generate representation paired visualaudio semantics given visual modality rasl module used output highlight score experimental result show proposed framework achieves superior performance compared stateoftheart approach
towards multimodal video paragraph captioning model robust missing modality video paragraph captioning vpc involves generating detailed narrative long video utilizing supportive modality speech event boundary however existing model constrained assumption constant availability single auxiliary modality impractical given diversity unpredictable nature realworld scenario end propose missingresistant framework mrvpc effectively harness available auxiliary input maintains resilience even absence certain modality framework propose multimodal vpc mvpc architecture integrating video speech event boundary input unified manner process various auxiliary input moreover fortify model incomplete data introduce dropam data augmentation strategy randomly omits auxiliary input paired distillam regularization target distills knowledge teacher model trained modalitycomplete data enabling efficient learning modalitydeficient environment exhaustive experimentation activitynet caption mrvpc proven deliver superior performance modalitycomplete modalitymissing test data work highlight significance developing resilient vpc model pave way adaptive robust multimodal video understanding
zeroshot promptbased video encoder surgical gesture recognition purpose order produce surgical gesture recognition system support wide variety procedure either large annotated dataset must acquired fitted model must generalize new label called zeroshot capability paper investigate feasibility latter option method leveraging bridgeprompt framework prompttune pretrained visiontext model clip gesture recognition surgical video utilize extensive outside video data text also make use label metadata weakly supervised contrastive loss result experiment show promptbased video encoder outperforms standard encoders surgical gesture recognition task notably display strong performance zeroshot scenario gesturestasks provided encoder training phase included prediction phase additionally measure benefit inclusion text description feature extractor training schema conclusion bridgeprompt similar pretrainedprompttuned video encoder model present significant visual representation surgical robotics especially gesture recognition task given diverse range surgical task gesture ability model zeroshot transfer without need task gesture specific retraining make invaluable
simultaneous detection interaction reasoning objectcentric action recognition interaction human object important recognizing objectcentric action existing method usually adopt twostage pipeline object proposal first detected using pretrained detector fed action recognition model extracting video feature learning object relation action recognition however since action prior unknown object detection stage important object could easily overlooked leading inferior action recognition performance paper propose endtoend objectcentric action recognition framework simultaneously performs detection interaction reasoning one stage particularly extracting video feature base network create three module concurrent object detection interaction reasoning first patchbased object decoder generates proposal video patch token interactive object refining aggregation identifies important object action recognition adjusts proposal score based position appearance aggregate objectlevel info global video representation lastly object relation modeling module encodes object relation three module together video feature extractor trained jointly endtoend fashion thus avoiding heavy reliance offtheshelf object detector reducing multistage training burden conduct experiment two datasets somethingelse ikeaassembly evaluate performance proposed approach conventional compositional fewshot action recognition task indepth experimental analysis show crucial role interactive object learning action recognition outperform stateoftheart method datasets
sinc adaptive camerabased vitals unsupervised learning periodic signal subtle periodic signal blood volume pulse respiration extracted rgb video enabling noncontact health monitoring low cost advancement remote pulse estimation remote photoplethysmography rppg currently driven deep learning solution however modern approach trained evaluated benchmark datasets ground truth contactppg sensor present first noncontrastive unsupervised learning framework signal regression mitigate need labelled video data minimal assumption periodicity finite bandwidth approach discovers blood volume pulse directly unlabelled video find encouraging sparse power spectrum within normal physiological bandlimits variance batch power spectrum sufficient learning visual feature periodic signal perform first experiment utilizing unlabelled video data specifically created rppg train robust pulse rate estimator given limited inductive bias successfully applied approach camerabased respiration changing bandlimits target signal show approach general enough unsupervised learning bandlimited quasiperiodic signal different domain furthermore show framework effective finetuning model unlabelled video single subject allowing personalized adaptive signal regressors
onestage openvocabulary temporal action detection leveraging temporal multiscale action label feature openvocabulary temporal action detection openvocab tad advanced video analysis approach expands closedvocabulary temporal action detection closedvocab tad capability closedvocab tad typically confined localizing classifying action based predefined set category contrast openvocab tad go limited predefined category particularly useful realworld scenario variety action video vast always predictable prevalent method openvocab tad typically employ approach involves generating action proposal identifying action however error made first stage adversely affect subsequent action identification accuracy additionally existing study face challenge handling action different duration owing use fixed temporal processing method therefore propose approach consisting two primary module multiscale video analysis mva videotext alignment vta mva module capture action varying temporal resolution overcoming challenge detecting action diverse duration vta module leverage synergy visual textual modality precisely align video segment corresponding action label critical step accurate action identification openvocab scenario evaluation widely recognized datasets showed proposed method achieved superior result compared method openvocab closedvocab setting serf strong demonstration effectiveness proposed method tad task
anticipation perfect deepfake identityanchored artifactagnostic detection rebalanced deepfake detection protocol deep generative model advance anticipate deepfakes achieving perfectiongenerating discernible artifact noise however current deepfake detector intentionally inadvertently rely artifact detection exclusive deepfakes absent genuine example bridge gap introduce rebalanced deepfake detection protocol rddp stresstest detector balanced scenario genuine forged example bear similar artifact offer two rddp variant rddpwhitehat us whitehat deepfake algorithm create selfdeepfakes genuine portrait video resemblance underlying identity yet carry similar artifact deepfake video rddpsurrogate employ surrogate function eg gaussian noise process genuine forged example introducing equivalent noise thereby sidestepping need deepfake algorithm towards detecting perfect deepfake video aligns genuine one present idminer detector identifies puppeteer behind disguise focusing motion artifact appearance identitybased detector authenticates video comparing reference footage equipped artifactagnostic loss framelevel identityanchored loss videolevel idminer effectively single identity signal amidst distracting variation extensive experiment comparing idminer baseline detector conventional rddp evaluation two deepfake datasets along additional qualitative study affirm superiority method necessity detector designed counter perfect deepfakes
egocentric videolanguage model truly understand handobject interaction egocentric videolanguage pretraining crucial step advancing understanding handobject interaction firstperson scenario despite success existing testbeds find current egovlms easily misled simple modification changing verb noun interaction description model struggling distinguish change raise question egovlms truly understand handobject interaction address question introduce benchmark called egohoibench revealing performance limitation current egocentric model confronted challenge attribute performance gap insufficient finegrained supervision greater difficulty egovlms experience recognizing verb compared noun tackle issue propose novel asymmetric contrastive objective named egonce videototext objective enhance text supervision generating negative caption using large language model leveraging pretrained vocabulary hoirelated word substitution texttovideo objective focus preserving objectcentric feature space cluster video representation based shared noun extensive experiment demonstrate egonce significantly enhances egohoi understanding leading improved performance across various egovlms task multiinstance retrieval action recognition temporal understanding code available httpsgithubcomxuboshenegoncepp
ptmvqa efficient video quality assessment leveraging diverse pretrained model wild video quality assessment vqa challenging problem due numerous factor affect perceptual quality video eg content attractiveness distortion type motion pattern level however annotating mean opinion score mo video expensive timeconsuming limit scale vqa datasets pose significant obstacle deep learningbased method paper propose vqa method named ptmvqa leverage pretrained model transfer knowledge model pretrained various pretasks enabling benefit vqa different aspect specifically extract feature video different pretrained model frozen weight integrate generate representation since model possess various field knowledge often trained label irrelevant quality propose intraconsistency interdivisibility icid loss impose constraint feature extracted multiple pretrained model intraconsistency constraint ensures feature extracted different pretrained model unified qualityaware latent space interdivisibility introduces pseudo cluster based annotation sample try separate feature sample different cluster furthermore constantly growing number pretrained model crucial determine model use use address problem propose efficient scheme select suitable candidate model better clustering performance vqa datasets chosen candidate extensive experiment demonstrate effectiveness proposed method
differentiable task graph learning procedural activity representation online mistake detection egocentric video procedural activity sequence keysteps aimed achieving specific goal crucial build intelligent agent able assist user effectively context task graph emerged humanunderstandable representation procedural activity encoding partial ordering keysteps previous work generally relied handcrafted procedure extract task graph video paper propose approach based direct maximum likelihood optimization edge weight allows gradientbased learning task graph naturally plugged neural network architecture experiment dataset demonstrate ability approach predict accurate task graph observation action sequence improvement previous approach owing differentiability proposed framework also introduce featurebased approach aiming predict task graph keystep textual video embeddings observe emerging video understanding ability task graph learned approach also shown significantly enhance online mistake detection procedural egocentric video achieving notable gain epictento datasets code replicating experiment available httpsgithubcomfpviplabdifferentiabletaskgraphlearning
large gaussian reconstruction model present first large reconstruction model produce animated object singleview video input single feedforward pas take second key success novel dataset multiview video containing curated rendered animated object objaverse dataset depicts diverse object animation rendered viewpoint resulting video total frame keep simple scalability build directly top lgm pretrained large reconstruction model output gaussian ellipsoid multiview image input output perframe gaussian splatting representation video frame sampled low fps upsamples representation higher fps achieve temporal smoothness add temporal selfattention layer base lgm help learn consistency across time utilize pertimestep multiview rendering loss train model representation upsampled higher framerate training interpolation model produce intermediate gaussian representation showcase trained synthetic data generalizes extremely well inthewild video producing high quality animated asset
investigating video reasoning capability large language model trope movie large language model llm demonstrated effectiveness language task also video reasoning paper introduces novel dataset trope movie tim designed testbed exploring two critical yet previously overlooked video reasoning skill abstract perception understanding tokenizing abstract concept video longrange compositional reasoning planning integrating intermediate reasoning step understanding longrange video numerous frame utilizing trope movie storytelling tim evaluates reasoning capability stateoftheart llmbased approach experiment show current method including captionerreasoner large multimodal model instruction finetuning visual programming marginally outperform random baseline tackling challenge abstract perception longrange compositional reasoning address deficiency propose faceenhanced viper role interaction fevori context query reduction conquer enhance visual programming fostering role interaction awareness progressively refining movie context trope query reasoning process significantly improving performance point however performance still lag behind human level v additionally introduce new protocol evaluate necessity abstract perception longrange compositional reasoning task resolution done analyzing code generated visual programming using abstract syntax tree ast thereby confirming increased complexity tim dataset code available
pravic probabilistic adaptation framework realtime video classification video processing generally divided two main category processing entire video typically yield optimal classification outcome realtime processing objective make decision promptly possible latter often driven need identify rapidly potential critical dangerous situation could include machine failure traffic accident heart problem dangerous behavior although model dedicated processing entire video typically welldefined clearly presented literature case online processing plethora handdevised method exist address present novel unified theoreticallybased adaptation framework dealing online classification problem video data initial phase study establish robust mathematical foundation theory classification sequential data potential make decision early stage allows u construct natural function encourages model return outcome much faster subsequent phase demonstrate straightforward readily implementable method adapting offline model online recurrent operation finally comparing proposed approach nononline stateoftheart baseline demonstrated use encourages network make earlier classification decision without compromising accuracy
groprompt efficient grounded prompting adaptation referring video object segmentation referring video object segmentation rvos aim segment object referred query sentence throughout entire video existing method require endtoend training dense mask annotation could computationconsuming less scalable work aim efficiently adapt foundation segmentation model addressing rvos weak supervision proposed grounded prompting groprompt framework specifically propose textaware prompt contrastive learning tapcl enhance association position prompt referring sentence box supervision including textcontrastive prompt learning textcon modalitycontrastive prompt learning modalcon frame level video level respectively proposed tapcl groprompt framework generate temporalconsistent yet textaware position prompt describing location movement referred object video experimental result standard rvos benchmark refyoutubevos jhmdbsentences demonstrate competitive performance proposed groprompt framework given bounding box weak supervision
unleashing potential tracklets unsupervised video person reidentification rich temporalspatial information videobased person reidentification method shown broad prospect although tracklets easily obtained readymade tracking model annotating identity still expensive impractical therefore videobased method propose using identity annotation camera label facilitate feature learning also simply average frame feature tracklet overlooking unexpected variation inherent identity consistency within tracklets paper propose selfsupervised refined clustering ssrc framework without relying annotation auxiliary information promote unsupervised video person reidentification specifically first propose noisefiltered tracklet partition nftp module reduce feature bias tracklets caused noisy tracking result sequentially partition noisefiltered tracklets subtracklets cluster merge subtracklets using selfsupervised signal tracklet partition enhanced progressive strategy generate reliable pseudo label facilitating intraclass crosstracklet aggregation moreover propose class smoothing classification csc loss efficiently promote model learning extensive experiment mar dukemtmcvideoreid datasets demonstrate proposed ssrc unsupervised video person reidentification achieves stateoftheart result comparable advanced supervised method
towards timely video analytics service network edge realtime video analytics service aim provide user accurate recognition result timely however existing study usually fall dilemma reducing delay improving accuracy edge computing scenario imposes strict transmission computation resource constraint making balancing conflicting metric dynamic network condition difficult regard introduce age processed information aopi concept quantifies time elapsed since generation latest accurately recognized frame aopi depicts integrated impact recognition accuracy transmission computation efficiency derive closedform expression aopi preemptive nonpreemptive computation scheduling policy wrt transmissioncomputation rate recognition accuracy video frame investigate joint problem edge server selection video configuration adaptation bandwidthcomputation resource allocation minimize longterm average aopi camera propose online method ie lyapunovbased block coordinate descent lbcd solve problem decouples original problem two subproblems optimize video configurationresource allocation edge server selection strategy separately prove lbcd achieves asymptotically optimal performance according testbed experiment simulation result lbcd reduces average aopi compared stateoftheart baseline
long context transfer language vision video sequence offer valuable temporal information existing large multimodal model lmms fall short understanding extremely long video many work address reducing number visual token using visual resamplers alternatively paper approach problem perspective language model simply extrapolating context length language backbone enable lmms comprehend order magnitude visual token without video training call phenomenon long context transfer carefully ablate property effectively measure lmms ability generalize long context vision modality develop vniah visual needleinahaystack purely synthetic long vision benchmark inspired language model niah test proposed long video assistant longva process frame visual token without additional complexity extended context length longva achieves stateoftheart performance videomme among model densely sampling input frame work opensourced httpsgithubcomevolvinglmmslablongva
bviaom new training dataset deep video compression optimization deep learning playing important role enhancing performance conventional hybrid video codecs learningbased method typically require diverse representative training material optimization order achieve model generalization optimal coding performance however existing datasets either offer limited content variability come restricted licensing term constraining use research purpose address issue propose new training dataset named bviaom contains uncompressed sequence various resolution covering wide range content texture type dataset come flexible licensing term offer competitive performance used training set optimizing deep video coding tool experimental result demonstrate used training set optimize two popular network architecture two different coding tool proposed dataset lead additional bitrate saving percentage point term psnry vmaf respectively compared existing training dataset bvidvc widely used deep video coding bviaom dataset available httpsgithubcomfanaaronzhangbviaom
sam robotic surgery empirical evaluation robustness generalization surgical video segmentation recent segment anything model sam demonstrated remarkable foundational competence semantic segmentation memory mechanism mask decoder addressing challenge video tracking object occlusion thereby achieving superior result interactive segmentation image video building upon previous empirical study explore zeroshot segmentation performance sam robotassisted surgery based prompt alongside robustness realworld corruption static image employ two form prompt bounding box video sequence prompt applied initial frame extensive experimentation miccai endovis endovis benchmark sam utilizing bounding box prompt outperforms stateoftheart sota method comparative evaluation result point prompt also exhibit substantial enhancement sam capability nearing even surpassing existing unprompted sota methodology besides sam demonstrates improved inference speed less performance degradation various image corruption although slightly unsatisfactory result remain specific edge region sam robust adaptability prompt underscore potential downstream surgical task limited prompt requirement
breast tumor classification based selfsupervised contrastive learning ultrasound video background breast ultrasound prominently used diagnosing breast tumor present many automatic system based deep learning developed help radiologist diagnosis however training system remains challenging usually datahungry demand amount labeled data need professional knowledge expensive method adopted triplet network selfsupervised contrastive learning technique learn representation unlabeled breast ultrasound video clip designed new hard triplet loss learn representation particularly discriminate positive negative image pair hard recognize also constructed pretraining dataset breast ultrasound video video patient includes anchor sample dataset image positive sample dataset image negative sample dataset dynamically generated video clip constructed finetuning dataset including image patient transferred pretrained network downstream benignmalignant classification task compared performance stateoftheart model including three model pretrained imagenet previous contrastive learning model retrained datasets result conclusion experiment revealed model achieved area receiver operating characteristic curve auc significantly higher others assessed dependence pretrained model number labeled data revealed sample required achieve auc proposed framework greatly reduces demand labeled data hold potential use automatic breast ultrasound image diagnosis
visual language model image video understanding beginning visualglm cogvlm continuously exploring vlms pursuit enhanced visionlanguage fusion efficient higherresolution architecture broader modality application propose family new generation visual language model image video understanding including image understanding model inherits visual expert architecture improved training recipe pretraining posttraining stage supporting input resolution time pixel video understanding model integrates multiframe input timestamps proposes automated temporal grounding data construction notably family achieved stateoftheart result benchmark like mmbench mmvet textvqa mvbench vcgbench model opensourced contributing advancement field
ladtalk latent denoising synthesizing talking head video high frequency detail audiodriven talking head generation pivotal area within filmmaking virtual reality although existing method made significant stride following endtoend paradigm still encounter challenge producing video highfrequency detail due limited expressivity domain limitation prompted u explore effective postprocessing approach synthesize photorealistic talking head video specifically employ pretrained model foundation model leveraging robust audiolip alignment capability drawing theory lipschitz continuity theoretically established noise robustness vector quantised auto encoders vqaes experiment demonstrate highfrequency texture deficiency foundation model temporally consistently recovered spaceoptimised vector quantised auto encoder sovqae introduced thereby facilitating creation realistic talking head video conduct experiment conventional dataset highfrequency talking head hftk dataset curated result indicate method ladtalk achieves new stateoftheart video quality outofdomain lip synchronization performance
vimguard novel multimodal system video misinformation guarding rise social medium shortform video sfv facilitated breeding ground misinformation emergence large language model significant research gone curbing misinformation problem automatic false claim detection text unfortunately automatic detection misinformation sfv complex problem remains largely unstudied text sample monomodal containing word sfvs comprise three different modality word visuals nonlinguistic audio work introduce video masked autoencoders misinformation guarding vimguard first deeplearning architecture capable factchecking sfv analysis three constituent modality vimguard leverage dualcomponent system first video audio masked autoencoders analyze visual nonlinguistic audio element video discern intention specifically whether intends make informative claim deemed sfv informative intent passed second component retrieval augmented generation system validates factual accuracy spoken word evaluation vimguard outperformed three cuttingedge factcheckers thus setting new standard sfv factchecking marking significant stride toward trustworthy news social platform promote testing iteration vimguard deployed chrome extension code opensourced github
tomato assessing visual temporal reasoning capability multimodal foundation model existing benchmark often highlight remarkable performance achieved stateoftheart multimodal foundation model mfms leveraging temporal context video understanding however well model truly perform visual temporal reasoning study existing benchmark show capability mfms likely overestimated many question solved using single outoforder frame systematically examine current visual temporal reasoning task propose three principle corresponding metric multiframe gain frame order sensitivity frame information disparity following principle introduce tomato temporal reasoning multimodal evaluation novel benchmark crafted rigorously assess mfms temporal reasoning capability video understanding tomato comprises carefully curated humanannotated question spanning six task ie action count direction rotation shape trend velocity frequency visual cue applied video including selfrecorded generated video encompass humancentric realworld simulated scenario comprehensive evaluation reveals humanmodel performance gap bestperforming model moreover indepth analysis uncovers fundamental limitation beyond gap current mfms accurately recognize event isolated frame fail interpret frame continuous sequence believe tomato serve crucial testbed evaluating nextgeneration mfms call community develop ai system capable comprehending human world dynamic video modality
loyalty creator dilute loyalty promoted product examining heterogeneous effect livestreamed content video game usage social medium platform led online consumption community fandom involve complex network ancillary creator consumer focused core product intellectual property example video game community include network player content creator centered around specific video game network complex video game publisher often sponsor creator creator publisher may divergent incentive specifically creator potentially benefit content build following expense core game research investigates relationship consuming livestreamed content engagement specific video game examine causal effect viewing livestreamed content subsequent gameplay specific game using unexpected service interruption livestreaming platform time zone difference among user find livestreamed content significantly increase gameplay increase livestreamed viewing minute result increase gameplay minute also explore effect varies user loyalty different type streamer channel firmowned mega micro positive effect livestreamed content greatest microstreamers smallest megastreamers finding salient firm allocating sponsorship resource
multicast scheme live streaming course largescale geographically dense campus network video course become significant component modern education however increasing demand live streaming video course place considerable strain service capability campus network challenge associated live streaming course video campus network environment exhibit distinct spatial distribution characteristic audience specific video course may highly concentrated certain area leading large number user attempting access live stream simultaneously utilizing content delivery network cdn distribute video campus scenario creates substantial unicast pressure edge cdn server paper proposes twolayer dynamic partitioning recursive bit string rb virtual domain network layer multicast architecture specifically designed largescale geographically dense multicast scenario within campus network approach reduces redundant multicast message approximately compared twolayer fixed partitioning method additionally establishes multicast source authentication capability based source address validation improvement savi facilitates secure multicast group key exchange using concise exchange protocol within webrtc framework nextgeneration data plane programmable softwaredefined network rb stateless multicast technology integrated unique characteristic largescale geographically dense campus network scenario dynamically efficiently extend multicast coverage every dormitory
implementing optimized secured multimedia streaming protocol participatory sensing scenario multimedia streaming protocol becoming increasingly popular crowdsensing due ability deliver highquality video content internet realtime streaming multimedia content context live video streaming requires high bandwidth large storage capacity ensure sufficient throughput crowdsensing distribute information shared video content among multiple user network reducing storage capacity computational bandwidth requirement however crowdsensing introduces several security constraint must taken account ensure confidentiality integrity availability data specific case video streaming commonly named visual crowdsensing vcs within context data transmitted wireless network making vulnerable security breach susceptible eavesdropping interception attacker multimedia often contains sensitive user data may subject various privacy law including data protection law law related photography video recording based local gdpr general data protection regulation reason realization secure protocol optimized distributed data streaming realtime becomes increasingly important crowdsensing smartenviroment context article discus use symmetric aesctr encryption based protocol securing data streaming crowdsensed network
rewind understanding long video instructed learnable memory visionlanguage model vlms crucial application requiring integrated understanding textual visual information however existing vlms struggle long video due computational inefficiency memory limitation difficulty maintaining coherent understanding across extended sequence address challenge introduce rewind novel memorybased vlm designed efficient long video understanding preserving temporal fidelity rewind operates twostage framework first stage rewind maintains dynamic learnable memory module novel textbfreadperceivewrite cycle store update instructionrelevant visual information video unfolds module utilizes learnable query crossattentions memory content input stream ensuring low memory requirement scaling linearly number token second stage propose adaptive frame selection mechanism guided memory content identify instructionrelevant key moment enriches memory representation detailed spatial information selecting highresolution frame combined memory content fed large language model llm generate final answer empirically demonstrate rewinds superior performance visual question answering vqa temporal grounding task surpassing previous method long video benchmark notably rewind achieves score gain accuracy improvement vqa dataset miou increase charadessta temporal grounding
stable mean teacher semisupervised video action detection work focus semisupervised learning video action detection video action detection requires spatiotemporal localization addition classification limited amount label make model prone unreliable prediction present stable mean teacher simple endtoend teacherbased framework benefit improved temporally consistent pseudo label relies novel error recovery eor module learns student mistake labeled sample transfer knowledge teacher improve pseudo label unlabeled sample moreover existing spatiotemporal loss take temporal coherency account prone temporal inconsistency address present difference pixel dop simple novel constraint focused temporal consistency leading coherent temporal detection evaluate approach four different spatiotemporal detection benchmark ava youtubevos approach outperforms supervised baseline action detection average margin ava using merely data provides competitive performance compared supervised baseline trained annotation respectively evaluate effectiveness ava scaling largescale datasets youtubevos video object segmentation demonstrating generalization capability task video domain code model publicly available
multimodal sentiment analysis based video audio input despite abundance current research working sentiment analysis video audio finding best model give highest accuracy rate still considered challenge researcher field main objective paper prove usability emotion recognition model take video audio input datasets used train model cremad dataset audio ravdess dataset video finetuned model used audio video avarage probability emotion generated two previous model utilized decision making framework disparity result one model get much higher accuracy another test framework created method used weighted average method confidence level threshold method dynamic weighting based confidence method rulebased logic method limited approach give encouraging result make future research method viable
querycentric audiovisual cognition network moment retrieval segmentation stepcaptioning video emerged favored multimedia format internet better gain video content new topic hirest presented including video retrieval moment retrieval moment segmentation stepcaptioning pioneering work chooses pretrained clipbased model video retrieval leverage feature extractor three challenging task solved multitask learning paradigm nevertheless work struggle learn comprehensive cognition userpreferred content due disregarding hierarchy association relation across modality paper guided shallowtodeep principle propose querycentric audiovisual cognition quag network construct reliable multimodal representation moment retrieval segmentation stepcaptioning specifically first design modalitysynergistic perception obtain rich audiovisual content modeling global contrastive alignment local finegrained interaction visual audio modality devise querycentric cognition us deeplevel query perform temporalchannel filtration shallowlevel audiovisual representation cognize userpreferred content thus attain querycentric audiovisual representation three task extensive experiment show quag achieves sota result hirest test quag querybased video summarization task verify good generalization
cimgen controlled image manipulation finetuning pretrained generative model limited data content creation image editing benefit flexible user control common intermediate representation conditional image generation semantic map information object present image compared raw rgb pixel modification semantic map much easier one take semantic map easily modify map selectively insert remove replace object map method proposed paper take modified semantic map alter original image accordance modified map method leverage traditional pretrained imagetoimage translation gans cyclegan gan finetuned limited dataset reference image associated semantic map discus qualitative quantitative performance technique illustrate capacity possible application field image forgery image editing also demonstrate effectiveness proposed image forgery technique thwarting numerous deep learningbased image forensic technique highlighting urgent need develop robust generalizable image forensic tool fight spread fake medium
pcacgan sparsetensorbased generative adversarial network point cloud attribute compression learningbased method proven successful compressing geometric information point cloud attribute compression however still lag behind nonlearningbased method mpeg gpcc standard bridge gap propose novel deep learningbased point cloud attribute compression method us generative adversarial network gan sparse convolution layer method also includes module adaptively selects resolution voxels used voxelize input point cloud sparse vector used represent voxelized point cloud sparse convolution process sparse tensor ensuring computational efficiency best knowledge first application gans compress point cloud attribute experimental result show method outperforms existing learningbased technique rival latest gpcc test model term visual quality
lightweight ganbased image fusion algorithm visible infrared image paper present lightweight image fusion algorithm specifically designed merging visible light infrared image emphasis balancing performance efficiency proposed method enhances generator generative adversarial network gan integrating convolutional block attention module cbam improve feature focus utilizing depthwise separable convolution dsconv efficient computation innovation significantly reduce model computational cost including number parameter inference latency maintaining even enhancing quality fused image comparative experiment using dataset demonstrate proposed algorithm outperforms similar image fusion method term fusion quality also offer resourceefficient solution suitable deployment embedded device effectiveness lightweight design validated extensive ablation study confirming potential realtime application complex environment
enhancing image resolution evaluating advanced technique based convolutional generative neural network paper investigates enhancement spatial resolution band contain spectral information using advanced superresolution technique factor stateoftheart cnn model compared enhanced gan approach term quality feasibility therefore representative dataset comprising lowresolution image corresponding highresolution aerial orthophotos required literature study reveals feasible dataset land type interest forest reason adequate dataset generated addition accounting accurate alignment image source optimization result reveal cnnbased approach produce satisfactory outcome tend yield blurry image contrast ganbased model provide clear detailed image also demonstrate superior performance term quantitative assessment underlying potential framework beyond specific land type investigated
duoliftganreconstructing ct singleview biplanar xrays generative adversarial network computed tomography ct provides highly detailed threedimensional medical image costly timeconsuming often inaccessible intraoperative setting organization et al recent advancement explored reconstructing chest volume sparse xrays singleview orthogonal doubleview image however current model tend process image planar manner prioritizing visual realism structural accuracy work introduce duolift generative adversarial network duoliftgan novel architecture dual branch independently elevate image feature representation output merged unified feature map decoded complete chest volume enabling richer information capture also present masked loss function directs reconstruction towards critical anatomical region improving structural accuracy visual quality paper demonstrates duoliftgan significantly enhances reconstruction accuracy achieving superior visual realism compared existing method
leveraging motion data boost motion generation textdriven human motion synthesis capturing significant attention ability effortlessly generate intricate movement abstract text cue showcasing potential revolutionizing motion design film narrative also virtual reality experience computer game development existing method often rely motion capture data require special setup resulting higher cost data acquisition ultimately limiting diversity scope human motion contrast human video offer vast accessible source motion data covering wider range style activity paper explore leveraging human motion extracted video alternative data source improve textdriven motion generation approach introduces novel framework disentangles local joint motion global movement enabling efficient learning local motion prior data first train singleview local motion generator large dataset textmotion pair enhance model synthesize motion finetune generator data transforming multiview generator predicts viewconsistent local joint motion root dynamic experiment dataset novel text prompt demonstrate method efficiently utilizes data supporting realistic human motion generation broadening range motion type support code made publicly available
phocolens photorealistic consistent reconstruction lensless imaging lensless camera offer significant advantage size weight cost compared traditional lensbased system without focusing lens lensless camera rely computational algorithm recover scene multiplexed measurement however current algorithm struggle inaccurate forward imaging model insufficient prior reconstruct highquality image overcome limitation introduce novel twostage approach consistent photorealistic lensless image reconstruction first stage approach ensures data consistency focusing accurately reconstructing lowfrequency content spatially varying deconvolution method adjusts change point spread function psf across camera field view second stage enhances photorealism incorporating generative prior pretrained diffusion model conditioning lowfrequency content retrieved first stage diffusion model effectively reconstructs highfrequency detail typically lost lensless imaging process also maintaining image fidelity method achieves superior balance data fidelity visual quality compared existing method demonstrated two popular lensless system phlatcam diffusercam project website httpsphocolensgithubio
hunyuanvideo systematic framework large video generative model recent advancement video generation significantly impacted daily life individual industry however leading video generation model remain closedsource resulting notable performance gap industry capability available public report introduce hunyuanvideo innovative opensource video foundation model demonstrates performance video generation comparable even surpassing leading closedsource model hunyuanvideo encompasses comprehensive framework integrates several key element including data curation advanced architectural design progressive model scaling training efficient infrastructure tailored largescale model training inference result successfully trained video generative model billion parameter making largest among opensource model conducted extensive experiment implemented series targeted design ensure high visual quality motion dynamic textvideo alignment advanced filming technique according evaluation professional hunyuanvideo outperforms previous stateoftheart model including runway luma three topperforming chinese video generative model releasing code foundation model application aim bridge gap closedsource opensource community initiative empower individual within community experiment idea fostering dynamic vibrant video generation ecosystem code publicly available httpsgithubcomtencenthunyuanvideo
enhancing ratedistortionperception flexibility learned image codecs conditional diffusion decoder learned image compression codecs recently achieved impressive compression performance surpassing efficient image coding architecture however approach trained minimize rate distortion often lead unsatisfactory visual result low bitrates since perceptual metric taken account paper show conditional diffusion model lead promising result generative compression task used decoder given compressed representation allow creating new tradeoff point distortion perception decoder side based sampling method
fortifying fully convolutional generative adversarial network image superresolution using divergence measure superresolution sr timehallowed image processing problem aim improve quality lowresolution lr sample standard highresolution hr counterpart aim address introducing superresolution generator surge fullyconvolutional generative adversarial network ganbased architecture sr show distinct convolutional feature obtained increasing depth gan generator optimally combined set learnable convex weight improve quality generated sr sample process employ jensenshannon gromovwasserstein loss respectively srhr lrsr pair distribution aid generator surge better exploit available information attempt improve sr moreover train discriminator surge wasserstein loss gradient penalty primarily prevent mode collapse proposed surge endtoend gan workflow tailormade superresolution offer improved performance maintaining low inference time efficacy surge substantiated superior performance compared stateoftheart contender benchmark datasets
adversarial attack image classification based generative adversarial network adversarial attack image classification system always important problem field machine learning generative adversarial network gans popular model field image generation widely used various novel scenario due powerful generative capability however popularity generative adversarial network misuse fake image technology raised series security problem malicious tampering people photo video invasion personal privacy inspired generative adversarial network work proposes novel adversarial attack method aiming gain insight weakness image classification system improve antiattack ability specifically generative adversarial network used generate adversarial sample small perturbation enough affect decisionmaking classifier adversarial sample generated adversarial learning training generator classifier extensive experiment analysis evaluate effectiveness method classical image classification dataset result show model successfully deceives variety advanced classifier maintaining naturalness adversarial sample
dctdiff intriguing property image generative modeling dct space paper explores image modeling frequency space introduces dctdiff endtoend diffusion generative paradigm efficiently model image discrete cosine transform dct space investigate design space dctdiff reveal key design factor experiment different framework uvit dit generation task various diffusion sampler demonstrate dctdiff outperforms pixelbased diffusion model regarding generative quality training efficiency remarkably dctdiff seamlessly scale highresolution generation without using latent diffusion paradigm finally illustrate several intriguing property dct image modeling example provide theoretical proof image diffusion seen spectral autoregression bridging gap diffusion autoregressive model effectiveness dctdiff introduced property suggest promising direction image modeling frequency space code
karma augmenting embodied ai agent longandshort term memory system embodied ai agent responsible executing interconnected longsequence household task often face difficulty incontext memory leading inefficiency error task execution address issue introduce karma innovative memory system integrates longterm shortterm memory module enhancing large language model llm planning embodied agent memoryaugmented prompting karma distinguishes longterm shortterm memory longterm memory capturing comprehensive scene graph representation environment shortterm memory dynamically record change object position state dualmemory structure allows agent retrieve relevant past scene experience thereby improving accuracy efficiency task planning shortterm memory employ strategy effective adaptive memory replacement ensuring retention critical information discarding less pertinent data compared stateoftheart embodied agent enhanced memory memoryaugmented embodied ai agent improves success rate composite task complex task within simulator respectively enhances task execution efficiency furthermore demonstrate karma plugandplay capability allows seamless deployment realworld robotic system mobile manipulation platformsthrough plugandplay memory system karma significantly enhances ability embodied agent generate coherent contextually appropriate plan making execution complex household task efficient experimental video work found code available
interactive visual learning stable diffusion diffusionbased generative model impressive ability create convincing image garnered global attention however complex internal structure operation often pose challenge nonexperts grasp introduce diffusion explainer first interactive visualization tool designed elucidate stable diffusion transforms text prompt image tightly integrates visual overview stable diffusion complex component detailed explanation underlying operation integration enables user fluidly transition multiple level abstraction animation interactive element offering realtime handson experience diffusion explainer allows user adjust stable diffusion hyperparameters prompt without need installation specialized hardware accessible via user web browser diffusion explainer making significant stride democratizing ai education fostering broader public access user spanning country used opensourced tool httpspoloclubgithubiodiffusionexplainer video demo available httpsyoutubembkiadzjpna
genziqa generalized image quality assessment using promptguided latent diffusion model design noreference nr image quality assessment iqa algorithm extremely important benchmark calibrate user experience modern visual system major drawback stateoftheart nriqa method limited ability generalize across diverse iqa setting reasonable distribution shift recent texttoimage generative model latent diffusion model generate meaningful visual concept fine detail related text concept work leverage denoising process diffusion model generalized iqa understanding degree alignment learnable qualityaware text prompt image particular learn crossattention map intermediate layer denoiser latent diffusion model capture qualityaware representation image addition also introduce learnable qualityaware text prompt enable crossattention feature better qualityaware extensive cross database experiment across various usergenerated synthetic lowlight contentbased benchmarking database show latent diffusion model achieve superior generalization iqa compared method literature
lowresource data generation textual control natural language serf common straightforward signal human interact seamlessly machine recognizing importance interface machine learning community investing considerable effort generating data semantically coherent textual instruction stride made texttodata generation spanning image editing audio synthesis video creation beyond lowresource area characterized expensive annotation complex data structure molecule motion dynamic time series often lack textual label deficiency impedes supervised learning thereby constraining application advanced generative model texttodata task response challenge lowresource scenario propose novel approach utilizes unlabeled data understand underlying data distribution unsupervised diffusion model subsequently undergoes controllable finetuning via novel constraint optimizationbased learning objective ensures controllability effectively counteracts catastrophic forgetting comprehensive experiment demonstrate able achieve enhanced performance regarding controllability across various modality including molecule motion time series compared existing baseline
tpc testtime procrustes calibration diffusionbased human image animation human image animation aim generate human motion video input reference human image target motion video current diffusionbased image animation system exhibit high precision transferring human identity targeted motion yet still exhibit irregular quality output optimal precision achieved physical composition ie scale rotation human shape reference image target pose frame aligned absence alignment noticeable decline fidelity consistency especially realworld environment compositional misalignment commonly occurs posing significant challenge practical usage current system end propose testtime procrustes calibration tpc enhances robustness diffusionbased image animation system maintaining optimal performance even faced compositional misalignment effectively addressing realworld scenario tpc provides calibrated reference image diffusion model enhancing capability understand correspondence human shape reference target image method simple applied diffusionbased image animation system modelagnostic manner improving effectiveness test time without additional training
multirobot motion planning diffusion model diffusion model recently successfully applied wide range robotics application learning complex multimodal behavior data however prior work mostly confined singlerobot smallscale environment due high sample complexity learning multirobot diffusion model paper propose method generating collisionfree multirobot trajectory conform underlying data distribution using singlerobot data algorithm multirobot multimodel planning diffusion mmd combining learned diffusion model classical searchbased technique generating datadriven motion collision constraint scaling show compose multiple diffusion model plan large environment single diffusion model fails generalize well demonstrate effectiveness approach planning dozen robot variety simulated scenario motivated logistics environment view video demonstration supplementary material code httpsgithubcomyoraishmmd
mozart touch lightweight multimodal music generation framework based pretrained large model recent year aigenerated content aigc witnessed rapid advancement facilitating creation music image artistic form across wide range industry however current model image videotomusic synthesis struggle capture nuanced emotion atmosphere conveyed visual content fill gap propose mozart touch multimodal music generation framework capable generating music aligned crossmodal input image video text framework consists three key component multimodal captioning module large language model llm understanding bridging module music generation module unlike traditional endtoend method mozart touch us llm accurately interpret visual element without requiring training finetuning music generation model providing efficiency transparency clear interpretable prompt also introduce llmbridge method resolve heterogeneous representation challenge descriptive text different modality series objective subjective evaluation demonstrate mozart touch outperforms current stateoftheart model code example available httpsgithubcomtiffanyblewsmozartstouch
revisiting registrationbased synthesis focus unsupervised mr image synthesis deep learning dl led significant improvement medical image synthesis enabling advanced imagetoimage translation generate synthetic image however dl method face challenge domain shift high demand training data limiting generalizability applicability historically image synthesis also carried using deformable image registration dir method warp moving image desired modality match anatomy fixed image however concern speed accuracy led decline popularity recent advance dlbased dir revisit reinvigorate line research paper propose fast accurate synthesis method based dir use task synthesizing rare magnetic resonance mr sequence white matter nulled wmn image demonstrate potential approach training method learns dir model based widely available mprage sequence cerebrospinal fluid nulled csfn inversion recovery gradient echo pulse sequence testing trained dir model first applied estimate deformation moving fixed csfn image subsequently estimated deformation applied align paired wmn counterpart moving csfn image yielding synthetic wmn image fixed csfn image experiment demonstrate promising result unsupervised image synthesis using dir finding highlight potential technique context supervised synthesis method constrained limited training data
combo compositional world model embodied multiagent cooperation paper investigate problem embodied multiagent cooperation decentralized agent must cooperate given egocentric view world effectively plan setting contrast learning world dynamic singleagent scenario must simulate world dynamic conditioned arbitrary number agent action given partial egocentric visual observation world address issue partial observability first train generative model estimate overall world state given partial egocentric observation enable accurate simulation multiple set action world state propose learn compositional world model multiagent cooperation factorizing naturally composable joint action multiple agent compositionally generating video conditioned world state leveraging compositional world model combination vision language model infer action agent use tree search procedure integrate module facilitate online cooperative planning evaluate method three challenging benchmark agent result show compositional world model effective framework enables embodied agent cooperate efficiently different agent across various task arbitrary number agent showing promising future proposed method video found httpsembodiedagicsumasseducombo
monkey see monkey harnessing selfattention motion diffusion zeroshot motion transfer given remarkable result motion synthesis diffusion model natural question arises effectively leverage model motion editing existing diffusionbased motion editing method overlook profound potential prior embedded within weight pretrained model enables manipulating latent feature space hence primarily center handling motion space work explore attention mechanism pretrained motion diffusion model uncover role interaction attention element capturing representing intricate human motion pattern carefully integrate element transfer leader motion follower one maintaining nuanced characteristic follower resulting zeroshot motion transfer editing feature associated selected motion allows u confront challenge observed prior motion diffusion approach use general directive eg text music editing ultimately failing convey subtle nuance effectively work inspired monkey closely imitates see maintaining unique motion pattern hence call monkey see monkey dub momo employing technique enables accomplishing task synthesizing outofdistribution motion style transfer spatial editing furthermore diffusion inversion seldom employed motion result editing effort focus generated motion limiting editability real one momo harness motion inversion extending application real generated motion experimental result show advantage approach current art particular unlike method tailored specific application training approach applied inference time requiring training webpage httpsmonkeyseedocggithubio
learning dynamic tetrahedron highquality talking head synthesis recent work implicit representation neural radiance field nerf advanced generation realistic animatable head avatar video sequence implicit method still confronted visual artifact jitter since lack explicit geometric constraint pose fundamental challenge accurately modeling complex facial deformation paper introduce dynamic tetrahedron dyntet novel hybrid representation encodes explicit dynamic mesh neural network ensure geometric consistency across various motion viewpoint dyntet parameterized coordinatebased network learn signed distance deformation material texture anchoring training data predefined tetrahedron grid leveraging marching tetrahedron dyntet efficiently decodes textured mesh consistent topology enabling fast rendering differentiable rasterizer supervision via pixel loss enhance training efficiency incorporate classical morphable model facilitate geometry learning define canonical space simplifying texture learning advantage readily achievable owing effective geometric representation employed dyntet compared prior work dyntet demonstrates significant improvement fidelity lip synchronization realtime performance according various metric beyond producing stable visually appealing synthesis video method also output dynamic mesh promising enable many emerging application
data augmentation diffusion model colon polyp localization low data regime much real data enough scarcity data medical domain hinders performance deep learning model data augmentation technique alleviate problem usually rely functional transformation data guarantee preserve original task approximate distribution data using generative model way reducing problem also obtain new sample resemble original data denoising diffusion model promising deep learning technique learn good approximation different kind data like image time series tabular data automatic colonoscopy analysis specifically polyp localization colonoscopy video task assist clinical diagnosis treatment annotation video frame training deep learning model time consuming task usually small datasets obtained fine tuning application model using large dataset generated data could alternative improve performance conduct set experiment training different diffusion model generate jointly colonoscopy image localization annotation using combination existing open datasets generated data used various transfer learning experiment task polyp localization model based yolo low data regime
attack scene flow using point cloud deep neural network made significant advancement accurately estimating scene flow using point cloud vital many application like video analysis action recognition navigation robustness technique however remains concern particularly face adversarial attack proven deceive stateoftheart deep neural network many domain surprisingly robustness scene flow network attack thoroughly investigated address problem proposed approach aim bridge gap introducing adversarial whitebox attack specifically tailored scene flow network experimental result show generated adversarial example obtain relative degradation average endpoint error kitti datasets study also reveals significant impact attack targeting point cloud one dimension color channel average endpoint error analyzing success failure attack scene flow network optical flow network variant show higher vulnerability optical flow network code available httpsgithubcomaheldisattackonsceneflowusingpointcloudsgit
qdit accurate posttraining quantization diffusion transformer recent advancement diffusion model particularly architectural transformation unetbased model diffusion transformer dit significantly improve quality scalability image video generation however despite impressive capability substantial computational cost largescale model pose significant challenge realworld deployment posttraining quantization ptq emerges promising solution enabling model compression accelerated inference pretrained model without costly retraining however research dit quantization remains sparse existing ptq framework primarily designed traditional diffusion model tend suffer biased quantization leading notable performance degradation work identify dit typically exhibit significant spatial variance weight activation along temporal variance activation address issue propose qdit novel approach seamlessly integrates two key technique automatic quantization granularity allocation handle significant variance weight activation across input channel samplewise dynamic activation quantization adaptively capture activation change across timesteps sample extensive experiment conducted imagenet vbench demonstrate effectiveness proposed qdit specifically quantizing imagenet time qdit achieves remarkable reduction fid compared baseline challenging setting maintains high fidelity image video generation establishing new benchmark efficient highquality quantization dit code available hrefhttpsgithubcomjuanerxqdithttpsgithubcomjuanerxqdit
generalized event camera event camera capture world high time resolution minimal bandwidth requirement however event stream encode change brightness contain sufficient scene information support wide variety downstream task work design generalized event camera inherently preserve scene intensity bandwidthefficient manner generalize event camera term event generated information transmitted implement design turn singlephoton sensor provide digital access individual photon detection modality give u flexibility realize rich space generalized event camera singlephoton event camera capable highspeed highfidelity imaging low readout rate consequently event camera support plugandplay downstream inference without capturing new event datasets designing specialized eventvision model practical implication design involve lightweight nearsensorcompatible computation provide way use singlephoton sensor without exorbitant bandwidth cost
diffusionbased unsupervised audiovisual speech enhancement paper proposes new unsupervised audiovisual speech enhancement avse approach combine diffusionbased audiovisual speech generative model nonnegative matrix factorization nmf noise model first diffusion model pretrained clean speech conditioned corresponding video data simulate speech generative distribution pretrained model paired nmfbased noise model estimate clean speech iteratively specifically diffusionbased posterior sampling approach implemented within reverse diffusion process iteration speech estimate obtained used update noise parameter experimental result confirm proposed avse approach outperforms audioonly counterpart also generalizes better recent supervisedgenerative avse method additionally new inference algorithm offer better balance inference speed performance compared previous diffusionbased method code demo available
generating reward function video legged robot behavior learning learning behavior legged robot present significant challenge due inherent instability complex constraint recent research proposed use large language model llm generate reward function reinforcement learning thereby replacing need manually designed reward expert however approach relies textual description define learning objective fails achieve controllable precise behavior learning clear directionality paper introduce new method directly generates reward function video depicting behavior mimicked learned specifically first process video containing target behavior converting motion information individual video keypoint trajectory represented coordinate transforming module trajectory fed llm generate reward function turn used train policy enhance quality reward function develop videoassisted iterative reward refinement scheme visually assesses learned behavior provides textual feedback llm feedback guide llm continually refine reward function ultimately facilitating efficient behavior learning experimental result task involving bipedal quadrupedal robot motion control demonstrate method surpasses performance stateoftheart llmbased reward generation method term human normalized score importantly switching video input find method rapidly learn diverse motion behavior walking running
anidoc animation creation made easier production animation follows industrystandard workflow encompassing four essential stage character design keyframe animation inbetweening coloring research focus reducing labor cost process harnessing potential increasingly powerful generative ai using video diffusion model foundation anidoc emerges video line art colorization tool automatically convert sketch sequence colored animation following reference character specification model exploit correspondence matching explicit guidance yielding strong robustness variation eg posture reference character line art frame addition model could even automate inbetweening process user easily create temporally consistent animation simply providing character image well start end sketch code available
flow crossdomain manipulation interface present scalable learning framework enables robot acquire realworld manipulation skill without need realworld robot training data key idea behind use object flow manipulation interface bridging domain gap different embodiment ie human robot training environment ie realworld simulated comprises two component flow generation network flowconditioned policy flow generation network trained human demonstration video generates object flow initial scene image conditioned task description flowconditioned policy trained simulated robot play data map generated object flow robot action realize desired object movement using flow input policy directly deployed real world minimal simtoreal gap leveraging realworld human video simulated robot play data bypass challenge teleoperating physical robot real world resulting scalable system diverse task demonstrate capability variety realworld task including manipulation rigid articulated deformable object
pair script shooting script short drama generating highquality shooting script containing information scene shot language essential short drama script generation collect popular short drama episode internet average short episode total number short episode total duration hour totaling terabyte tb perform keyframe extraction annotation episode obtain shooting script perform script restoration extracted shooting script based selfdeveloped large short drama generation model skyreels lead dataset containing pair script shooting script short drama called compare existing dataset detail demonstrate deeper insight achieved based based researcher achieve several deeper farreaching script optimization goal may drive paradigm shift entire field texttovideo significantly advance field short drama video generation data code available
plamo plan move rich physical environment controlling humanoid complex physically simulated world longstanding challenge numerous application gaming simulation visual content creation setup given rich complex scene user provides list instruction composed target location locomotion type solve task present plamo sceneaware path planner robust physicsbased controller path planner produce sequence motion path considering various limitation scene imposes motion location height speed complementing planner control policy generates rich realistic physical motion adhering plan demonstrate combination module enables traversing complex landscape diverse form responding realtime change environment video
neural mp generalist neural motion planner current paradigm motion planning generates solution scratch every new problem consumes significant amount time computational resource complex cluttered scene motion planning approach often take minute produce solution human able accurately safely reach goal second leveraging prior experience seek applying datadriven learning scale problem motion planning approach build large number complex scene simulation collect expert data motion planner distills reactive generalist policy combine lightweight optimization obtain safe path real world deployment perform thorough evaluation method motion planning task across four diverse environment randomized pose scene obstacle real world demonstrating improvement motion planning success rate state art sampling optimization learning based planning method video result available mihdalalgithubioneuralmotionplanner
intrinsic singleimage hdr reconstruction low dynamic range ldr common camera fails capture rich contrast natural scene resulting loss color detail saturated pixel reconstructing high dynamic range hdr luminance present scene single ldr photograph important task many application computational photography realistic display image hdr reconstruction task aim infer lost detail using context present scene requiring neural network understand highlevel geometric illumination cue make challenging datadriven algorithm generate accurate highresolution result work introduce physicallyinspired remodeling hdr reconstruction problem intrinsic domain intrinsic model allows u train separate network extend dynamic range shading domain recover lost color detail albedo domain show dividing problem two simpler subtasks improves performance wide variety photograph
actionbased image editing guided human instruction textbased image editing typically approached static task involves operation inserting deleting modifying element input image based human instruction given static nature task paper aim make task dynamic incorporating action intend modify position posture object image depict different action maintaining visual property object implement challenging task propose new model sensitive action text instruction learning recognize contrastive action discrepancy model training done new datasets defined extracting frame video show visual scene action show substantial improvement image editing using actionbased text instruction high reasoning capability allow model use input image starting scene action generating new image show final scene action
creative portraiture exploring creative adversarial network conditional creative adversarial network convolutional neural network cnns combined generative adversarial network gans create deep convolutional generative adversarial network dcgans great success dcgans used generating image video creative domain fashion design painting common critique use dcgans creative application limited ability generate creative product generator simply learns copy training distribution explore extension dcgans creative adversarial network can using can generate novel creative portrait using wikiart dataset train network moreover introduce extension can conditional creative adversarial network ccans demonstrate potential generate creative portrait conditioned style label argue generating product conditioned inspired style label closely emulates real creative process human produce imaginative work still rooted previous style
mambamir arbitrarymasked mamba joint medical image reconstruction uncertainty estimation recent mamba model shown remarkable adaptability visual representation learning including medical imaging task study introduces mambamir mambabased model medical image reconstruction well generative adversarial networkbased variant mambamirgan proposed mambamir inherits several advantage linear complexity global receptive field dynamic weight original mamba model innovated arbitrarymask mechanism effectively adapt mamba image reconstruction task providing randomness subsequent monte carlobased uncertainty estimation experiment conducted various medical image reconstruction task including fast mri svct cover anatomical region knee chest abdomen demonstrated mambamir mambamirgan achieve comparable superior reconstruction result relative stateoftheart method additionally estimated uncertainty map offer insight reliability reconstruction quality code publicly available httpsgithubcomayanglabmambamir
onboard processing hyperspectral imagery deep learning advancement methodology challenge emerging trend recent advancement deep learning technique spurred considerable interest application hyperspectral imagery processing paper provides comprehensive review latest development field focusing methodology challenge emerging trend deep learning architecture convolutional neural network cnns autoencoders deep belief network dbns generative adversarial network gans recurrent neural network rnns examined suitability processing hyperspectral data key challenge including limited training data computational constraint identified along strategy data augmentation noise reduction using gans paper discusses efficacy different network architecture highlighting advantage lightweight cnn model cnns onboard processing moreover potential hardware accelerator particularly field programmable gate array fpgas enhancing processing efficiency explored review concludes insight ongoing research trend including integration deep learning technique earth observation mission chime mission emphasizes need exploration refinement deep learning methodology address evolving demand hyperspectral image processing
generalizable diffusion framework lowdose fewview cardiac spect myocardial perfusion imaging using spect widely utilized diagnose coronary artery disease image quality negatively affected lowdose fewview acquisition setting although various deep learning method introduced improve image quality lowdose fewview spect data previous approach often fail generalize across different acquisition setting limiting applicability reality work introduced diffusion framework cardiac spect imaging effectively adapts different acquisition setting without requiring network retraining finetuning using image projection data consistency strategy proposed ensure diffusion sampling step aligns lowdosefewview projection measurement image data scanner geometry thus enabling generalization different lowdosefewview setting incorporating anatomical spatial information ct total variation constraint proposed conditional strategy allow observe contextual information entire image volume addressing memory issue diffusion model extensively evaluated proposed method clinical tetrofosmin stressrest study patient study reconstructed different lowcount different fewview level model evaluation ranging view view respectively validated cardiac catheterization result diagnostic comment nuclear cardiologist presented result show potential achieve lowdose fewview spect imaging without compromising clinical performance additionally could directly applied fulldose spect image improve image quality especially lowdose stressfirst cardiac spect imaging protocol
physicsdriven framework controllable efficient content generation single image task content generation involves creating dynamic model evolve time response specific input condition image existing method rely heavily pretrained video diffusion model guide content dynamic approach often fail capture essential physical principle video diffusion model lack robust understanding realworld physic moreover model face challenge providing finegrained control dynamic exhibit high computational cost work propose novel highefficiency framework generates physicscompliant content single image enhanced control capability approach uniquely integrates physical simulation generation pipeline ensuring adherence fundamental physical law inspired human ability infer physical property visually introduce physical perception module ppm discerns material property structural component object input image facilitating accurate downstream simulation significantly accelerates generation process eliminating iterative optimization step dynamic modeling phase allows user intuitively control movement speed direction generated content adjusting external force achieving finely tunable physically plausible animation extensive evaluation show outperforms existing method inference speed physical realism producing highquality controllable content project page available link
freescale unleashing resolution diffusion model via tuningfree scale fusion visual diffusion model achieve remarkable progress yet typically trained limited resolution due lack highresolution data constrained computation resource hampering ability generate highfidelity image video higher resolution recent effort explored tuningfree strategy exhibit untapped potential higherresolution visual generation pretrained model however method still prone producing lowquality visual content repetitive pattern key obstacle lie inevitable increase highfrequency information model generates visual content exceeding training resolution leading undesirable repetitive pattern deriving accumulated error tackle challenge propose freescale tuningfree inference paradigm enable higherresolution visual generation via scale fusion specifically freescale process information different receptive scale fuse extracting desired frequency component extensive experiment validate superiority paradigm extending capability higherresolution visual generation image video model notably compared previous bestperforming method freescale unlocks generation image first time
belief scene graph expanding partial scene object computation expectation article propose novel concept belief scene graph utilitydriven extension partial scene graph enable efficient highlevel task planning partial information propose graphbased learning methodology computation belief also referred expectation given scene graph used strategically add new node referred blind node relevant robotic mission propose method computation expectation based correlation information ceci reasonably approximate real beliefexpectation learning histogram available training data novel graph convolutional neural network gcn model developed learn ceci repository scene graph database scene graph exists training novel ceci model present novel methodology generating scene graph dataset based semantically annotated reallife space generated dataset utilized train proposed ceci model extensive validation proposed method establish novel concept textitbelief scene graph bsg core component integrate expectation abstract representation new concept evolution classical scene graph concept aim enable highlevel reasoning task planning optimization variety robotics mission efficacy overall framework evaluated object search scenario also tested reallife experiment emulate human common sense unseenobjects video article showcasing experimental demonstration please refer following link
optimized twostage aibased neural decoding enhanced visual stimulus reconstruction fmri data aibased neural decoding reconstructs visual perception leveraging generative model map brain activity measured functional mri fmri latent hierarchical representation traditionally ridge linear model transform fmri latent space decoded using latent diffusion model ldm via pretrained variational autoencoder vae due complexity noisiness fmri data newer approach split reconstruction two sequential step first one providing rough visual approximation second improving stimulus prediction via ldm endowed clip embeddings work proposes nonlinear deep network improve fmri latent space representation optimizing dimensionality alike experiment natural scene dataset showed proposed architecture improved structural similarity reconstructed image respect stateoftheart model based ridge linear transform reconstructed image semantics improved measured perceptual similarity respect stateoftheart noise sensitivity analysis ldm showed role first stage fundamental predict stimulus featuring high structural similarity conversely providing large noise stimulus affected less semantics predicted stimulus structural similarity ground truth predicted stimulus poor finding underscore importance leveraging nonlinear relationship bold signal latent representation twostage generative ai optimizing fidelity reconstructed visual stimulus noisy fmri data
analyzing neural networkbased generative diffusion model convex optimization diffusion model gaining widespread use cuttingedge image video audio generation scorebased diffusion model stand among method necessitating estimation score function input data distribution study present theoretical framework analyze twolayer neural networkbased diffusion model reframing score matching denoising score matching convex optimization prove training shallow neural network score prediction done solving single convex program although analysis diffusion model operate asymptotic setting rely approximation characterize exact predicted score function establish convergence result neural networkbased diffusion model finite data result provide precise characterization neural networkbased diffusion model learn nonasymptotic setting
text diffusion reinforced conditioning diffusion model demonstrated exceptional capability generating highquality image video audio due adaptiveness iterative refinement provide strong potential achieving better nonautoregressive sequence generation however existing text diffusion model still fall short performance due challenge handling discreteness language paper thoroughly analyzes text diffusion model uncovers two significant limitation degradation selfconditioning training misalignment training sampling motivated finding propose novel text diffusion model called trec mitigates degradation reinforced conditioning misalignment timeaware variance scaling extensive experiment demonstrate competitiveness trec autoregressive nonautoregressive diffusion baseline moreover qualitative analysis show advanced ability fully utilize diffusion process refining sample
stale diffusion hyperrealistic movie generation using oldschool method two year ago stable diffusion achieved superhuman performance generating image superhuman number finger following steady decline technical novelty propose stale diffusion method solidifies ossifies stable diffusion maximumentropy state stable diffusion work analogously barn stable infinite set horse escaped diffusion horse long left barn proposal may seen antiquated irrelevant nevertheless vigorously defend claim novelty identifying early adopter slow science movement produce extremely important pearl wisdom future speed contribution also seen quasistatic implementation recent call pause ai experiment wholeheartedly support result careful archaeological expedition git commit history found naturallyaccumulating error produced novel entropymaximising stale diffusion method produce sleepinducing hyperrealistic video good one imagination
simple effective masked diffusion language model diffusion model excel generating highquality image prior work report significant performance gap diffusion autoregressive ar method language modeling work show simple masked discrete diffusion performant previously thought apply effective training recipe improves performance masked diffusion model derive simplified raoblackwellized objective result additional improvement objective simple form mixture classical masked language modeling loss used train encoderonly language model admit efficient sampler including one generate arbitrary length text semiautoregressively like traditional language model language modeling benchmark range masked diffusion model trained modern engineering practice achieves new stateoftheart among diffusion model approach ar perplexity provide code along blog post video tutorial project page httpsssahoocommdlm
ai challenge video quality assessment usergenerated content method result paper review ai video quality assessment vqa challenge focused usergenerated content ugc aim challenge gather deep learningbased method capable estimating perceptual quality ugc video usergenerated video youtube ugc dataset include diverse content sport game lyric anime etc quality resolution proposed method must process fhd frame second challenge total participant registered submitted code model performance submission reviewed provided survey diverse deep model efficient video quality assessment usergenerated content
high frequency matter uncertainty guided image compression wavelet diffusion diffusion probabilistic model recently achieved remarkable success generating highquality image however balancing high perceptual quality low distortion remains challenging image compression application address issue propose efficient uncertaintyguided image compression approach wavelet diffusion ugdiff approach focus high frequency compression via wavelet transform since high frequency component crucial reconstructing image detail introduce wavelet conditional diffusion model high frequency prediction followed residual codec compress transmits prediction residual decoder diffusion predictionthenresidual compression paradigm effectively address low fidelity issue common direct reconstruction existing diffusion model considering uncertainty random sampling diffusion model design uncertaintyweighted ratedistortion rd loss tailored residual compression providing rational tradeoff rate distortion comprehensive experiment two benchmark datasets validate effectiveness ugdiff surpassing stateoftheart image compression method rd performance perceptual quality subjective quality inference time code available
recasting generic pretrained vision transformer objectcentric scene encoders manipulation policy generic reusable pretrained image representation encoders become standard component method many computer vision task visual representation robot however utility limited leading recent wave effort pretrain roboticsspecific image encoders better suited robotic task generic counterpart propose scene object transformer abbreviated soft wrapper around pretrained vision transformer pvt model bridge gap without training rather construct representation final layer activation soft individuates locates objectlike entity pvt attention describes pvt activation producing objectcentric embedding across standard choice generic pretrained vision transformer pvt demonstrate case policy trained softpvt far outstrip standard pvt representation manipulation task simulated real setting approaching stateoftheart roboticsaware representation code appendix video httpssitesgooglecomviewrobotsoft
towards general textguided image synthesis customized multimodal brain mri generation multimodal brain magnetic resonance mr imaging indispensable neuroscience neurology however due accessibility mri scanner lengthy acquisition time multimodal mr image commonly available current mr image synthesis approach typically trained independent datasets specific task leading suboptimal performance applied novel datasets task present tumsyn textguided universal mr image synthesis generalist model flexibly generate brain mr image demanded imaging metadata routinely acquired scan guided text prompt ensure tumsyns image synthesis precision versatility generalizability first construct brain mr database comprising image mri modality center pretrain mrispecific text encoder using contrastive learning effectively control mr image synthesis based text prompt extensive experiment diverse datasets physician assessment indicate tumsyn generate clinically meaningful mr image specified imaging metadata supervised zeroshot scenario therefore tumsyn utilized along acquired mr scan facilitate largescale mribased screening diagnosis brain disease
texsensegan userguided system optimizing texturerelated vibrotactile feedback using generative adversarial network vibration rendering essential creating realistic tactile experience humanvirtual object interaction video game controller vr device dynamically adjusting vibration parameter based user action system convey spatial feature contribute texture representation however generating arbitrary vibration replicate realworld material texture challenging due large parameter space study proposes humanintheloop vibration generation model based user preference enable user easily control generation vibration sample large parameter space introduced optimization model based differential subspace search ds generative adversarial network gan ds user employ onedimensional slider easily modify highdimensional latent space ensure gan generate desired vibration trained generative model using open dataset tactile vibration data selected five type vibration target sample generation experiment extensive user experiment conducted using generated real sample result indicated system could generate distinguishable sample matched target characteristic moreover established correlation subject ability distinguish real sample ability distinguish generated sample
eventaided freetrajectory gaussian splatting scene reconstruction casually captured video wide application realworld scenario recent advancement differentiable rendering technique several method attempted simultaneously optimize scene representation nerf camera pose despite recent progress existing method relying traditional camera input tend fail highspeed equivalently lowframerate scenario event camera inspired biological vision record pixelwise intensity change asynchronously high temporal resolution providing valuable scene motion information blind interframe interval paper introduce event camera aid scene construction casually captured video first time propose eventaided freetrajectory called seamlessly integrates advantage event camera three key component first leverage event generation model egm fuse event frame supervising rendered view observed event stream second adopt contrast maximization cmax framework piecewise manner extract motion information maximizing contrast image warped event iwe thereby calibrating estimated pose besides based linear event generation model legm brightness information encoded iwe also utilized constrain gradient domain third mitigate absence color information event introduce photometric bundle adjustment pba ensure view consistency across event frame evaluate method public tank temple benchmark newly collected realworld dataset realevdavis project page
segtalker segmentationbased talking face generation maskguided local editing audiodriven talking face generation aim synthesize video lip movement synchronized input audio however current generative technique face challenge preserving intricate regional texture skin teeth address aforementioned challenge propose novel framework called segtalker decouple lip movement image texture introducing segmentation intermediate representation specifically given mask image employed parsing network first leverage speech drive mask generate talking segmentation disentangle semantic region image style code using maskguided encoder ultimately inject previously generated talking segmentation style code maskguided stylegan synthesize video frame way texture fully preserved moreover approach inherently achieve background separation facilitate maskguided facial local editing particular editing mask swapping region texture given reference image eg hair lip eyebrow approach enables facial editing seamlessly generating talking face video experiment demonstrate proposed approach effectively preserve texture detail generate temporally consistent video remaining competitive lip synchronization quantitative qualitative result hdtf mead datasets illustrate superior performance method existing method
realisdance equip controllable character animation realistic hand controllable character animation emerging task generates character video controlled pose sequence given character image although character consistency made significant progress via reference unet another crucial factor pose control well studied existing method yet resulting several issue generation may fail input pose sequence corrupted hand generated using dwpose sequence blurry unrealistic generated video shaky pose sequence smooth enough paper present realisdance handle issue realisdance adaptively leverage three type pose avoiding failed generation caused corrupted pose sequence among pose type hamer provides accurate depth information hand enabling realisdance generate realistic hand even complex gesture besides using temporal attention main unet realisdance also insert temporal attention pose guidance network smoothing video pose condition aspect moreover introduce pose shuffle augmentation training improve generation robustness video smoothness qualitative experiment demonstrate superiority realisdance existing method especially hand quality
panoptic scene graph generation living threedimensional space moving forward fourth dimension time allow artificial intelligence develop comprehensive understanding environment introduce panoptic scene graph new representation bridge raw visual data perceived dynamic world highlevel visual understanding specifically abstract rich sensory data node represent entity precise location status information edge capture temporal relation facilitate research new area build richly annotated dataset consisting rgbd video total frame labeled panoptic segmentation mask well finegrained dynamic scene graph solve propose transformerbased model predict panoptic segmentation mask track mask along time axis generate corresponding scene graph via relation component extensive experiment new dataset show method serve strong baseline future research end provide realworld application example demonstrate achieve dynamic scene understanding integrating large language model system
diffusion posterior proximal sampling image restoration diffusion model demonstrated remarkable efficacy generating highquality sample existing diffusionbased image restoration algorithm exploit pretrained diffusion model leverage data prior yet still preserve element inherited unconditional generation paradigm strategy initiate denoising process pure white noise incorporate random noise generative step leading oversmoothed result paper present refined paradigm diffusionbased image restoration specifically opt sample consistent measurement identity generative step exploiting sampling selection avenue output stability enhancement number candidate sample used selection adaptively determined based signaltonoise ratio timestep additionally start restoration process initialization combined measurement signal providing supplementary information better align generative process extensive experimental result analysis validate proposed method significantly enhances image restoration performance consuming negligible additional computational resource
texttomodel textconditioned neural network diffusion trainonceforall personalization generative artificial intelligence genai made significant progress understanding world knowledge generating content human language across various modality like texttotext large language model texttoimage stable diffusion texttovideo sora paper investigate capability genai texttomodel generation see whether genai comprehend hyperlevel knowledge embedded within ai parameter specifically study practical scenario termed trainonceforall personalization aiming generate personalized model diverse endusers task using text prompt inspired recent emergence neural network diffusion present tina textconditioned neural network diffusion trainonceforall personalization tina leverage diffusion transformer model conditioned task description embedded using clip model despite astronomical number potential personalized task eg design tina demonstrates remarkable indistribution outofdistribution generalization even trained small datasets sim verify whether tina understands world knowledge analyzing capability zeroshotfewshot image prompt different number personalized class prompt natural language description predicting unseen entity
using spatial diffusion optoacoustic tomography image reconstruction optoacoustic tomography image reconstruction problem interest recent year exploiting exceptional generative power recently proposed diffusion model consider scheme based conditional diffusion process using simple initial image reconstruction method delay sum consider specially designed autoencoder architecture generates latent representation used conditional information generative diffusion process numerical result show merit proposal term quality metric psnr ssim showing conditional information generated term initial reconstructed image able bias generative process diffusion model order enhance image correct artifact even recover finer detail initial reconstruction method able obtain
analysing diffusion segmentation medical image denoising diffusion probabilistic model become increasingly popular due ability offer probabilistic modeling generate diverse output versatility inspired adaptation image segmentation multiple prediction model produce segmentation result achieve high quality also capture uncertainty inherent model powerful architecture proposed improving diffusion segmentation performance however notable lack analysis discussion difference diffusion segmentation image generation thorough evaluation missing distinguish improvement architecture provide segmentation general benefit diffusion segmentation specifically work critically analyse discus diffusion segmentation medical image differs diffusion image generation particular focus training behavior furthermore conduct assessment proposed diffusion segmentation architecture perform trained directly segmentation lastly explore different medical segmentation task influence diffusion segmentation behavior diffusion process could adapted accordingly analysis aim provide indepth insight behavior diffusion segmentation allow better design evaluation diffusion segmentation method future
infinitydrive breaking time limit driving world model autonomous driving system struggle complex scenario due limited access diverse extensive outofdistribution driving data critical safe navigation world model offer promising solution challenge however current driving world model constrained short time window limited scenario diversity bridge gap introduce infinitydrive first driving world model exceptional generalization capability delivering stateoftheart performance high fidelity consistency diversity minutescale video generation infinitydrive introduces efficient spatiotemporal comodeling module paired extended temporal training strategy enabling highresolution video generation consistent spatial temporal coherence incorporating memory injection retention mechanism alongside adaptive memory curve loss minimize cumulative error achieving consistent video generation lasting frame minute comprehensive experiment multiple datasets validate infinitydrives ability generate complex varied scenario highlighting potential nextgeneration driving world model built evolving demand autonomous driving project homepage
general flow foundation affordance scalable robot learning address challenge acquiring realworld manipulation skill scalable framework hold belief identifying appropriate prediction target capable leveraging largescale datasets crucial achieving efficient universal learning therefore propose utilize flow represents future trajectory point object interest ideal prediction target exploit scalable data resource turn attention human video develop first time languageconditioned flow prediction model directly largescale rgbd human video datasets predicted flow offer actionable guidance thus facilitating zeroshot skill transfer realworld scenario deploy method policy based closedloop flow prediction remarkably without indomain finetuning method achieves impressive success rate zeroshot humantorobot skill transfer covering task scene framework feature following benefit scalability leveraging crossembodiment data resource wide application multiple object category including rigid articulated soft body stable skill transfer providing actionable guidance small inference domaingap code data supplementary material available httpsgeneralflowgithubio
diffdti fast diffusion tensor imaging using featureenhanced joint diffusion model magnetic resonance diffusion tensor imaging dti critical tool neural disease diagnosis however long scan time greatly hinders widespread clinical use dti accelerate image acquisition featureenhanced joint diffusion model diffdti proposed obtain accurate dti parameter map limited number diffusionweighted image dwis diffdti introduces joint diffusion model directly learns joint probability distribution dwis dti parametric map conditional generation additionally feature enhancement fusion mechanism fefm designed incorporated generative process diffdti preserve fine structure generated dti map comprehensive evaluation performance diffdti conducted human connectome project dataset result demonstrate diffdti outperforms existing stateoftheart fast dti imaging method term visual quality quantitative metric furthermore diffdti shown ability produce highfidelity dti map three dwis thus overcoming requirement minimum six dwis dti
seppo semipolicy preference optimization diffusion alignment reinforcement learning human feedback rlhf method emerging way finetune diffusion model dm visual generation however commonly used onpolicy strategy limited generalization capability reward model offpolicy approach require large amount difficulttoobtain paired humanannotated data particularly visual generation task address limitation offpolicy rlhf propose preference optimization method aligns dm preference without relying reward model paired humanannotated data specifically introduce semipolicy preference optimization seppo method seppo leverage previous checkpoint reference model using generate onpolicy reference sample replace losing image preference pair approach allows u optimize using offpolicy winning image furthermore design strategy reference model selection expands exploration policy space notably simply treat reference sample negative example learning instead design anchorbased criterion assess whether reference sample likely winning losing image allowing model selectively learn generated reference sample approach mitigates performance degradation caused uncertainty reference sample quality validate seppo across texttoimage texttovideo benchmark seppo surpasses previous approach texttoimage benchmark also demonstrates outstanding performance texttovideo benchmark code released httpsgithubcomdwanzhangaiseppo
evaluating mitigating ip infringement visual generative ai popularity visual generative ai model like dalle stable diffusion xl stable video diffusion sora increasing extensive evaluation discovered stateoftheart visual generative model generate content bear striking resemblance character protected intellectual property right held major entertainment company sony marvel nintendo raise potential legal concern happens input prompt contains character name even descriptive detail characteristic mitigate ip infringement problem also propose defense method detail develop revised generation paradigm identify potentially infringing generated content prevent ip infringement utilizing guidance technique diffusion process capability recognize generated content may infringing intellectual property right mitigate infringement employing guidance method throughout diffusion process without retrain finetune pretrained model experiment wellknown character ip like spiderman iron man superman demonstrate effectiveness proposed defense method data code found
dtvlt multimodal diverse text benchmark visual language tracking based llm visual language tracking vlt emerged cuttingedge research area harnessing linguistic data enhance algorithm multimodal input broadening scope traditional single object tracking sot encompass video understanding application despite vlt benchmark still depend succinct humanannotated text description video description often fall short capturing nuance video content dynamic lack stylistic variety language constrained uniform level detail fixed annotation frequency result algorithm tend default memorize answer strategy diverging core objective achieving deeper understanding video content fortunately emergence large language model llm enabled generation diverse text work utilizes llm generate varied semantic annotation term text length granularity representative sot benchmark thereby establishing novel multimodal benchmark specifically propose new visual language tracking benchmark diverse text named dtvlt based five prominent vlt sot benchmark including three subtasks shortterm tracking longterm tracking global instance tracking offer four granularity text benchmark considering extent density semantic information expect multigranular generation strategy foster favorable environment vlt video understanding research conduct comprehensive experimental analysis dtvlt evaluating impact diverse text tracking performance hope identified performance bottleneck existing algorithm support research vlt video understanding proposed benchmark experimental result toolkit released gradually httpvideocubeaitestunioncom
generic framework high fidelity talking face generation finegrained intramodal alignment despite numerous completed study achieving high fidelity talking face generation highly synchronized lip movement corresponding arbitrary audio remains significant challenge field shortcoming published study continue confuse many researcher paper introduces generic framework high fidelity talking face generation finegrained intramodal alignment reenact high fidelity original video producing highly synchronized lip movement regardless given audio tone volume key success use diagonal matrix enhance ordinary alignment audioimage intramodal feature significantly increase comparative learning positive negative sample additionally multiscaled supervision module introduced comprehensively reenact perceptional fidelity original video across facial region emphasizing synchronization lip movement input audio fusion network used fuse facial region rest experimental result demonstrate significant achievement reenactment original video quality well highly synchronized talking lip outperforming generic framework produce talking video competitively closer ground truth level current stateoftheart method
single generated video highfidelity reconstruction dynamic gaussian surfels video generative model receiving particular attention given ability generate realistic imaginative frame besides model also observed exhibit strong consistency significantly enhancing potential act world simulator work present novel reconstruction model excels accurately reconstructing ie sequential representation single generated video addressing challenge associated nonrigidity frame distortion capability pivotal creating highfidelity virtual content maintain spatial temporal coherence core proposed dynamic gaussian surfels dg technique dg optimizes timevarying warping function transform gaussian surfels surface element static state dynamically warped state transformation enables precise depiction motion deformation time preserve structural integrity surfacealigned gaussian surfels design warpedstate geometric regularization based continuous warping field estimating normal additionally learn refinement rotation scaling parameter gaussian surfels greatly alleviates texture flickering warping process enhances capture finegrained appearance detail also contains novel initialization state provides proper start warping field dg equipping existing video generative model overall framework demonstrates highfidelity generation appearance geometry
multigranularity contrastive crossmodal collaborative generation endtoend longterm video question answering longterm video question answering videoqa challenging visionandlanguage bridging task focusing semantic understanding untrimmed longterm video diverse freeform question simultaneously emphasizing comprehensive crossmodal reasoning yield precise answer canonical approach often rely offtheshelf feature extractor detour expensive computation overhead often result domainindependent modalityunrelated representation furthermore inherent gradient blocking unimodal comprehension crossmodal interaction hinders reliable answer generation contrast recent emerging successful videolanguage pretraining model enable costeffective endtoend modeling fall short domainspecific ratiocination exhibit disparity task formulation toward end present entirely endtoend solution longterm videoqa multigranularity contrastive crossmodal collaborative generation mcg model derive discriminative representation possessing high visual concept introduce joint unimodal modeling jum clipbone architecture leverage multigranularity contrastive learning mcl harness intrinsically explicitly exhibited semantic correspondence alleviate task formulation discrepancy problem propose crossmodal collaborative generation ccg module reformulate videoqa generative task instead conventional classification scheme empowering model capability crossmodal highsemantic fusion generation rationalize answer extensive experiment conducted six publicly available videoqa datasets underscore superiority proposed method
motif making text count image animation motion focal loss textimagetovideo generation aim generate video image following text description also referred textguided image animation existing method struggle generate video align well text prompt particularly motion specified overcome limitation introduce motif simple yet effective approach directs model learning region motion thereby improving text alignment motion generation use optical flow generate motion heatmap weight loss according intensity motion modified objective lead noticeable improvement complement existing method utilize motion prior model input additionally due lack diverse benchmark evaluating generation propose bench dataset consists imagetext pair robust evaluation present human evaluation protocol asks annotator select overall preference two video followed justification comprehensive evaluation bench motif outperforms nine opensourced model achieving average preference bench additional result released
dreamdance animating human image enriching geometry cue pose work present dreamdance novel method animating human image using skeleton pose sequence conditional input existing approach struggle generating coherent highquality content efficient userfriendly manner concretely baseline method relying pose guidance lack cue information leading suboptimal result method using representation guidance achieve higher quality involve cumbersome timeintensive process address limitation dreamdance enriches geometry cue pose introducing efficient diffusion model enabling highquality human image animation various guidance key insight human image naturally exhibit multiple level correlation progressing coarse skeleton pose finegrained geometry cue geometry cue explicit appearance detail capturing correlation could enrich guidance signal facilitating intraframe coherency interframe consistency specifically construct dataset comprising highquality dance video detailed frame annotation including human pose depth normal map next introduce mutually aligned geometry diffusion model generate finegrained depth normal map enriched guidance finally crossdomain controller incorporates multilevel guidance animate human image effectively video diffusion model extensive experiment demonstrate method achieves stateoftheart performance animating human image
approximately invertible neural network learned image compression learned image compression attracted considerable interest recent year typically comprises analysis transform synthesis transform quantization entropy coding model analysis transform synthesis transform used encode image latent feature decode quantized feature reconstruct image regarded coupled transforms however analysis transform synthesis transform designed independently existing method making unreliable highquality image compression inspired invertible neural network generative modeling invertible module used construct coupled analysis synthesis transforms considering noise introduced feature quantization invalidates invertible process paper proposes approximately invertible neural network ainn framework learned image compression formulates ratedistortion optimization lossy image compression using inn quantization differentiates using inn generative modelling generally speaking ainn used theoretical foundation inn based lossy compression method based formulation ainn progressive denoising module pdm developed effectively reduce quantization noise decoding moreover cascaded feature recovery module cfrm designed learn highdimensional feature recovery lowdimensional one reduce noise feature channel compression addition frequencyenhanced decomposition synthesis module fdsm developed explicitly enhancing highfrequency component image address loss highfrequency information inherent neural network based image compression extensive experiment demonstrate proposed ainn outperforms existing learned image compression method
textdriven tumor synthesis tumor synthesis generate example ai often miss overdetects improving ai performance training challenging case however existing synthesis method typically unconditional generating image random variable conditioned tumor shape lack controllability specific tumor characteristic texture heterogeneity boundary pathology type result generated tumor may overly similar duplicate existing training data failing effectively address ai weakness propose new textdriven tumor synthesis approach termed textomorph provides textual control tumor characteristic particularly beneficial example confuse ai early tumor detection increasing sensitivity tumor segmentation precise radiotherapy increasing dsc classification benign malignant tumor improving sensitivity incorporating text mined radiology report synthesis process increase variability controllability synthetic tumor target ai failure case precisely moreover textomorph us contrastive learning across different text ct scan significantly reducing dependence scarce imagereport pair pair used study leveraging large corpus radiology report finally developed rigorous test evaluate synthetic tumor including textdriven visual turing test radiomics pattern analysis showing synthetic tumor realistic diverse texture heterogeneity boundary pathology
finegrained dynamic network generic event boundary detection generic event boundary detection gebd aim pinpointing event boundary naturally perceived human playing crucial role understanding longform video given diverse nature generic boundary spanning different video appearance object action task remains challenging existing method usually detect various boundary protocol regardless distinctive characteristic detection difficulty resulting suboptimal performance intuitively intelligent reasonable way adaptively detect boundary considering special property light propose novel dynamic pipeline generic event boundary named dybdet introducing multiexit network architecture dybdet automatically learns subnet allocation different video snippet enabling finegrained detection various boundary besides multiorder difference detector also proposed ensure generic boundary effectively identified adaptively processed extensive experiment challenging kineticsgebd tapos datasets demonstrate adopting dynamic strategy significantly benefit gebd task leading obvious improvement performance efficiency compared current stateoftheart
large generative modelassisted talkingface semantic communication system rapid development generative artificial intelligence ai continually unveils potential semantic communication semcom however current talkingface semcom system still encounter challenge low bandwidth utilization semantic ambiguity diminished quality experience qoe study introduces large generative modelassisted talkingface semantic communication lgmtsc system tailored talkingface video communication firstly introduce generative semantic extractor gse transmitter based funasr model convert semantically sparse talkingface video text high information density secondly establish private knowledge base kb based large language model llm semantic disambiguation correction complemented joint knowledge basesemanticchannel coding scheme finally receiver propose generative semantic reconstructor gsr utilizes sadtalker model transform text back highqoe talkingface video matching user timbre simulation result demonstrate feasibility effectiveness proposed lgmtsc system
mumullama multimodal music understanding generation via large language model research large language model advanced significantly across text speech image video however multimodal music understanding generation remain underexplored due lack wellannotated datasets address introduce dataset hour multimodal data including text image video music annotation based dataset propose mumullama model leverage pretrained encoders music image video music generation integrate audioldm musicgen evaluation across four tasksmusic understanding texttomusic generation promptbased music editing multimodal music generationdemonstrates mumullama outperforms stateoftheart model showing potential multimodal music application
attention normalization impact cardinality generalization slot attention objectcentric scene decomposition important representation downstream task field computer vision robotics recently proposed slot attention module already leveraged several derivative work image segmentation object tracking video deep learning component performs unsupervised objectcentric scene decomposition input image based attention architecture latent slot vector hold compressed information object attend localized perceptual feature input image paper demonstrate design decision normalizing aggregated value attention architecture considerable impact capability slot attention generalize higher number slot object seen training propose investigate alternative original normalization scheme increase generalization capability slot attention varying slot object count resulting performance gain task unsupervised image segmentation newly proposed normalization represent minimal easy implement modification usual slot attention module changing value aggregation mechanism weighted mean operation scaled weighted sum operation
physdreamer physicsbased interaction object via video generation realistic object interaction crucial creating immersive virtual experience yet synthesizing realistic object dynamic response novel interaction remains significant challenge unlike unconditional textconditioned dynamic generation actionconditioned dynamic requires perceiving physical material property object grounding motion prediction property object stiffness however estimating physical material property open problem due lack material groundtruth data measuring property real object highly difficult present physdreamer physicsbased approach endows static object interactive dynamic leveraging object dynamic prior learned video generation model distilling prior physdreamer enables synthesis realistic object response novel interaction external force agent manipulation demonstrate approach diverse example elastic object evaluate realism synthesized interaction user study physdreamer take step towards engaging realistic virtual experience enabling static object dynamically respond interactive stimulus physically plausible manner see project page httpsphysdreamergithubio
highfidelity freeview synthesis emotional talking head present novel approach synthesizing talking head controllable emotion featuring enhanced lip synchronization rendering quality despite significant progress field prior method still suffer multiview consistency lack emotional expressiveness address issue collect dataset calibrated multiview video emotional annotation perframe geometry training dataset propose textitspeechtogeometrytoappearance mapping framework first predicts faithful geometry sequence audio feature appearance talking head represented gaussians synthesized predicted geometry appearance disentangled canonical dynamic gaussians learned multiview video fused render freeview talking head animation moreover model enables controllable emotion generated talking head rendered widerange view method exhibit improved rendering quality stability lip motion generation capturing dynamic facial detail wrinkle subtle expression experiment demonstrate effectiveness approach generating highfidelity emotioncontrollable talking head code dataset released
champ controllable consistent human image animation parametric guidance study introduce methodology human image animation leveraging human parametric model within latent diffusion framework enhance shape alignment motion guidance curernt human generative technique methodology utilizes smplskinned multiperson linear model human parametric model establish unified representation body shape pose facilitates accurate capture intricate human geometry motion characteristic source video specifically incorporate rendered depth image normal map semantic map obtained smpl sequence alongside skeletonbased motion guidance enrich condition latent diffusion model comprehensive shape detailed pose attribute multilayer motion fusion module integrating selfattention mechanism employed fuse shape motion latent representation spatial domain representing human parametric model motion guidance perform parametric shape alignment human body reference image source video motion experimental evaluation conducted benchmark datasets demonstrate methodology superior ability generate highquality human animation accurately capture pose shape variation furthermore approach also exhibit superior generalization capability proposed inthewild dataset project page httpsfudangenerativevisiongithubiochamp
contextual ad narration interleaved multimodal sequence audio description ad task aim generate description visual element visually impaired individual help access longform video content like movie video feature text character bank context information input generated ad able correspond character name provide reasonable contextual description help audience understand storyline movie achieve goal propose leverage pretrained foundation model simple unified framework generate ad interleaved multimodal sequence input termed uniad enhance alignment feature across various modality finer granularity introduce simple lightweight module map video feature textual feature space moreover also propose characterrefinement module provide precise information identifying main character play significant role video context unique design incorporate contextual information contrastive loss architecture generate smooth contextual ad experiment madeval dataset show uniad achieve stateoftheart performance ad generation demonstrates effectiveness approach code available httpsgithubcommcgnjuuniad
uniforensics face forgery detection via general facial representation previous deepfake detection method mostly depend lowlevel textural feature vulnerable perturbation fall short detecting unseen forgery method contrast highlevel semantic feature less susceptible perturbation limited forgeryspecific artifact thus stronger generalization motivated propose detection method utilizes highlevel semantic feature face identify inconsistency temporal domain introduce uniforensics novel deepfake detection framework leverage transformerbased video classification network initialized metafunctional face encoder enriched facial representation way take advantage powerful spatiotemporal model highlevel semantic information face furthermore leverage easily accessible real face data guide model focusing spatiotemporal feature design dynamic video selfblending dvsb method efficiently generate training sample diverse spatiotemporal forgery trace using real facial video based advance framework twostage training approach first stage employ novel selfsupervised contrastive learning encourage network focus forgery trace impelling video generated forgery process similar representation basis representation learned first stage second stage involves finetuning face forgery detection dataset build deepfake detector extensive experiment validates uniforensics outperforms existing face forgery method generalization ability robustness particular method achieves cross dataset auc challenging dfdc respectively
tkgdm trainingfree chroma key content generation diffusion model diffusion model enabled generation highquality image strong focus realism textual fidelity yet largescale texttoimage model stable diffusion struggle generate image foreground object placed chroma key background limiting ability separate foreground background element without finetuning address limitation present novel trainingfree chroma key content generation diffusion model tkgdm optimizes initial random noise produce image foreground object specifiable color background proposed method first explore manipulation color aspect initial noise controlled background generation enabling precise separation foreground background without finetuning extensive experiment demonstrate trainingfree method outperforms existing method qualitative quantitative evaluation matching surpassing finetuned model finally successfully extend task eg consistency model texttovideo highlighting transformative potential across various generative application independent control foreground background crucial
domain generalization pose estimation nerfbased image synthesis work introduces novel augmentation method increase diversity train set improve generalization ability pose estimation network purpose neural radiance field trained synthetic image exploited generate augmented set method enriches initial set enabling synthesis image unseen viewpoint ii rich illumination condition appearance extrapolation iii randomized texture validate augmentation method challenging usecase spacecraft pose estimation show significantly improves pose estimation generalization capability speed dataset method reduces error pose target domain
fusecaps investigating feature fusion based framework capsule endoscopy image classification order improve model accuracy generalization class imbalance issue work offer strong methodology classifying endoscopic image suggest hybrid feature extraction method combine convolutional neural network cnns multilayer perceptrons mlps radiomics rich multiscale feature extraction made possible combination capture deep handmade representation feature used classification head classify disease producing model higher generalization accuracy framework achieved validation accuracy capsule endoscopy video frame classification task
explicitnerfqa quality assessment database explicit nerf model compression recent year neural radiance field nerf demonstrated significant advantage representing synthesizing scene explicit nerf model facilitate practical nerf application faster rendering speed also attract considerable attention nerf compression due huge storage cost address challenge nerf compression study paper construct new dataset called explicitnerfqa use object diverse geometry texture material complexity train four typical explicit nerf model across five parameter level lossy compression introduced model generation pivoting selection key parameter hash table size instantngp voxel grid resolution plenoxels rendering nerf sample processed video sequence pvs large scale subjective experiment lab environment conducted collect subjective score viewer diversity content accuracy mean opinion score mo characteristic nerf distortion comprehensively presented establishing heterogeneity proposed dataset stateoftheart objective metric tested new dataset best person correlation around collected fullreference objective metric tested noreference metric report poor result correlation demonstrating need development robust noreference metric dataset including nerf sample source object multiview image nerf generation pvss mo made publicly available following location httpsgithubcomyukexingexplicitnerfqa
unirs unifying multitemporal remote sensing task vision language model domain gap remote sensing imagery natural image recently received widespread attention visionlanguage model vlms demonstrated excellent generalization performance remote sensing multimodal task however current research still limited exploring remote sensing vlms handle different type visual input bridge gap introduce textbfunirs first visionlanguage model textbfunifying multitemporal textbfremote textbfsensing task across various type visual input unirs support single image dualtime image pair video input enabling comprehensive remote sensing temporal analysis within unified framework adopt unified visual representation approach enabling model accept various visual input dualtime image pair task customize change extraction module enhance extraction spatiotemporal feature additionally design prompt augmentation mechanism tailored model reasoning process utilizing prior knowledge generalpurpose vlm provide clue unirs promote multitask knowledge sharing model jointly finetuned mixed dataset experimental result show unirs achieves stateoftheart performance across diverse task including visual question answering change captioning video scene classification highlighting versatility effectiveness unifying multitemporal remote sensing task code dataset released soon
arges spatiotemporal transformer ulcerative colitis severity assessment endoscopy video accurate assessment disease severity endoscopy video ulcerative colitis uc crucial evaluating drug efficacy clinical trial severity often measured mayo endoscopic subscore me ulcerative colitis endoscopic index severity uceis score however expert mesuceis annotation timeconsuming susceptible interrater variability factor addressable automation automation attempt framelevel label face challenge fullysupervised solution due prevalence videolevel label clinical trial cnnbased weaklysupervised model wsl endtoend training lack generalization new disease score ignore spatiotemporal information crucial accurate scoring address limitation propose arges deep learning framework utilizes transformer positional encoding incorporate spatiotemporal information frame feature estimate disease severity score endoscopy video extracted feature derived foundation model argesfm pretrained large diverse dataset multiple clinical trial frame video evaluate four uc disease severity score including me three uceis component score test set evaluation indicates significant improvement score increasing me three uceis component score compared stateoftheart method prospective validation previously unseen clinical trial data demonstrates model successful generalization
towards nationwide analytical healthcare infrastructure privacypreserving augmented knee rehabilitation case study purpose paper contribute towards nearfuture privacypreserving big data analytical healthcare platform capable processing streamed uploaded timeseries data video patient experimental work includes reallife knee rehabilitation video dataset capturing set exercise simple personalised general challenging movement aimed returning sport convert video mobile privacypreserving diagnostic timeseries data employed google mediapipe pose estimation developed proofofconcept algorithm augment knee exercise video overlaying patient stick figure element updating generated timeseries plot knee angle estimation streamed csv file format patient physiotherapist video sidetoside timeseries visually indicating potential issue excessive knee flexion unstable knee movement stick figure overlay error possible setting apriori kneeangle parameter address adherence rehabilitation programme quantify exercise set repetition adaptive algorithm correctly identify exercise side frontview video transparent algorithm design adaptive visual analysis various knee exercise pattern contributes towards interpretable ai inform nearfuture privacypreserving nonvendor locking opensource development enduser computing device onpremises nonproprietary cloud platform deployed within national healthcare system
tuningfree visual customization via view iterative selfattention control finetuning diffusion model enable wide range personalized generation editing application diverse visual modality lowrank adaptation lora accelerates finetuning process still requires multiple reference image timeconsuming training constrains scalability largescale realtime application paper propose textitview iterative selfattention control visctrl tackle challenge specifically visctrl trainingfree method injects appearance structure userspecified subject another subject target image unlike previous approach require finetuning model initially obtain initial noise reference target image ddim inversion denoising phase feature reference image injected target image via selfattention mechanism notably iteratively performing feature injection process ensure reference image feature gradually integrated target image approach result consistent harmonious editing one reference image denoising step moreover benefiting plugandplay architecture design proposed feature gradual sampling strategy multiview editing method easily extended edit complex visual domain extensive experiment show efficacy visctrl across spectrum task including personalized editing image video scene
binary noise binary task masked bernoulli diffusion unsupervised anomaly detection high performance denoising diffusion model image generation paved way application unsupervised medical anomaly detection diffusionbased method require lot gpu memory long sampling time present novel fast unsupervised anomaly detection approach based latent bernoulli diffusion model first apply autoencoder compress input image binary latent representation next diffusion model follows bernoulli noise schedule employed latent space trained restore binary latent representation perturbed one binary nature diffusion model allows u identify entry latent space high probability flipping binary code denoising process indicates outofdistribution data propose masking algorithm based probability improves anomaly detection score achieve stateoftheart performance compared diffusionbased unsupervised anomaly detection algorithm significantly reducing sampling time memory consumption code available
gaussian need unified framework solving inverse problem via diffusion posterior sampling diffusion model generate variety highquality image modeling complex data distribution trained diffusion model also effective image prior solving inverse problem existing diffusionbased method integrate data consistency step within diffusion reverse sampling process data consistency step rely approximate likelihood function paper show existing approximation either insufficient computationally inefficient address issue propose unified likelihood approximation method incorporates covariance correction term enhance performance avoids propagating gradient diffusion model correction term integrated reverse diffusion sampling process achieves better convergence towards true data posterior selected distribution improves performance realworld natural image datasets furthermore present efficient way factorize invert covariance matrix likelihood function several inverse problem present comprehensive experiment demonstrate effectiveness method several existing approach
stable diffusion segmentation biomedical image singlestep reverse process diffusion model demonstrated effectiveness across various generative task however applied medical image segmentation model encounter several challenge including significant resource time requirement also necessitate multistep reverse process multiple sample produce reliable prediction address challenge introduce first latent diffusion segmentation model named sdseg built upon stable diffusion sd sdseg incorporates straightforward latent estimation strategy facilitate singlestep reverse process utilizes latent fusion concatenation remove necessity multiple sample extensive experiment indicate sdseg surpasses existing stateoftheart method five benchmark datasets featuring diverse imaging modality remarkably sdseg capable generating stable prediction solitary reverse step sample epitomizing model stability implied name code available httpsgithubcomlintianyustablediffusionseg
splatmover multistage openvocabulary robotic manipulation via editable gaussian splatting present splatmover modular robotics stack openvocabulary robotic manipulation leverage editability gaussian splatting gsplat scene representation enable multistage manipulation task splatmover consists asksplat gsplat representation distills semantic grasp affordance feature scene asksplat enables geometric semantic affordance understanding scene critical many robotics task ii seesplat realtime sceneediting module using semantic masking infilling visualize motion object result robot interaction realworld seesplat creates digital twin evolving environment throughout manipulation task iii graspsplat grasp generation module us asksplat seesplat propose affordancealigned candidate grasp openworld object asksplat trained realtime rgb image brief scanning phase prior operation seesplat graspsplat run realtime operation demonstrate superior performance splatmover hardware experiment kinova robot compared two recent baseline four singlestage openvocabulary manipulation task four multistage manipulation task using edited scene reflect change due prior manipulation stage possible existing baseline video demonstration code project available httpssplatmovergithubio
crowdsourced nerf collecting data production vehicle street view reconstruction recently neural radiance field nerf achieved impressive result novel view synthesis blocknerf showed capability leveraging nerf build large cityscale model largescale modeling mass image data necessary collecting image specially designed datacollection vehicle support largescale application acquire massive highquality data remains opening problem noting automotive industry huge amount image data crowdsourcing convenient way largescale data collection paper present crowdsourced framework utilizes substantial data captured production vehicle reconstruct scene nerf model approach solves key problem largescale reconstruction data come use firstly crowdsourced massive data filtered remove redundancy keep balanced distribution term time space structurefrommotion module performed refine camera pose finally image well pose used train nerf model certain block highlight present comprehensive framework integrates multiple module including data selection sparse reconstruction sequence appearance embedding depth supervision ground surface occlusion completion complete system capable effectively processing reconstructing highquality scene crowdsourced data extensive quantitative qualitative experiment conducted validate performance system moreover proposed application named firstview navigation leveraged nerf model generate street view guide driver synthesized video
agentstudio toolkit building general virtual agent general virtual agent need handle multimodal observation master complex action space selfimprove dynamic opendomain environment however existing environment often domainspecific require complex setup limit agent development evaluation realworld setting result current evaluation lack indepth analysis decompose fundamental agent capability introduce agentstudio trinity environment tool benchmark address issue agentstudio provides lightweight interactive environment highly generic observation action space eg video observation guiapi action integrates tool creating online benchmark task annotating gui element labeling action video based environment tool curate online task suite benchmark gui interaction function calling efficient autoevaluation also reorganize existing datasets collect new one using tool establish three datasets groundui idmbench criticbench datasets evaluate fundamental agent ability including gui grounding learning video success detection pointing desideratum robust general openended virtual agent
ssnvc single stream neural video compression implicit temporal information recently neural video compression nvc technique achieved remarkable performance even surpassing best traditional lossy video codec however existing nvc method heavily rely transmitting motion vector mv generate accurate contextual feature following drawback compressing transmitting mv requires specialized mv encoder decoder make module redundant due existence mv encoderdecoder training strategy complex paper present noval single stream nvc framework ssnvc remove complex mv encoderdecoder structure us onestage training strategy ssnvc implicitly use temporal information adding previous entropy model feature current entropy model using previous two frame generate predicted motion information decoder side besides enhance frame generator generate higher quality reconstructed frame experiment demonstrate ssnvc achieve stateoftheart performance multiple benchmark greatly simplify compression process well training process
multitalk enhancing talking head generation across language multilingual video dataset recent study speechdriven talking head generation achieved convincing result verbal articulation however generating accurate lipsyncs degrades applied input speech language possibly due lack datasets covering broad spectrum facial movement across language work introduce novel task generate talking head speech diverse language collect new multilingual video dataset comprising hour talking video language proposed dataset present multilingually enhanced model incorporates languagespecific style embeddings enabling capture unique mouth movement associated language additionally present metric assessing lipsync accuracy multilingual setting demonstrate training talking head model proposed dataset significantly enhances multilingual performance code datasets available httpsmultitalkgithubio
robot absence video foundation model enhance intermittent supervision paper investigates application video foundation model vifms generating robot data summary enhance intermittent human supervision robot team propose novel framework produce generic querydriven summary longduration robot vision data three modality storyboards short video text user study involving participant evaluate efficacy summary method allowing operator accurately retrieve observation action occurred robot operating without supervision extended duration min finding reveal querydriven summary significantly improve retrieval accuracy compared generic summary raw data albeit increased task duration storyboards found effective presentation modality especially objectrelated query work represents knowledge first zeroshot application vifms generating multimodal robottohuman communication intermittent supervision context demonstrating promise limitation model humanrobot interaction hri scenario
passive deepfake detection across multimodalities comprehensive survey recent year deepfakes dfs utilized malicious purpose individual impersonation misinformation spreading artist style imitation raising question ethical security concern however existing survey focused accuracy performance passive df detection approach single modality image video audio comprehensive survey explores passive approach across multiple modality including image video audio multimodal domain extend discussion beyond detection accuracy including generalization robustness attribution interpretability additionally discus threat model passive approach including potential adversarial strategy different level adversary knowledge capability also highlight current challenge df detection including lack generalization across different generative model need comprehensive trustworthiness evaluation limitation existing multimodal approach finally propose future research direction address unexplored emerging issue field passive df detection adaptive learning dynamic benchmark holistic trustworthiness evaluation multimodal detector talkingface video generation
study data augmentation technique overcome data scarcity wound classification using deep learning chronic wound significant burden individual healthcare system affecting million people incurring high cost wound classification using deep learning technique promising approach faster diagnosis treatment initiation however lack high quality data train ml model major challenge realize potential ml wound care fact data limitation biggest challenge study using medical forensic imaging today study data augmentation technique used overcome data scarcity limitation unlock potential deep learning based solution study explore range data augmentation technique geometric transformation wound image advanced gans enrich expand datasets using kera tensorflow panda library implemented data augmentation technique generate realistic wound image show geometric data augmentation improve classification performance score top stateoftheart model across several key class wound experiment gan based augmentation prove viability using degans generate wound image richer variation study result show data augmentation valuable privacypreserving tool huge potential overcome data scarcity limitation believe part realworld mlbased wound care system
hybrid approach detection combining wasserstein gan transfer learning extremely contagious rapid growth drawn attention towards early diagnosis early diagnosis enables healthcare professional government authority break chain transition flatten epidemic curve number case accelerating across developed world induced viral pneumonia case big challenge overlapping case viral pneumonia lung infection limited dataset long training hour serious problem cater limited amount data often result overfitting model due reason model predict generalized result fill gap proposed ganbased approach synthesize image later fed deep learning model classify image normal viral pneumonia specifically customized wasserstein gan proposed generate chest xray image compare real image expanded dataset used train four proposed deep learning model googlenet mnast result showed expanded dataset utilized deep learning model deliver high classification accuracy particular achieved highest accuracy among four proposed scheme rest model like googlenet mnast delivered testing accuracy respectively later efficiency model compared state art model basis accuracy proposed model applied address issue scant datasets problem image analysis
generative iris prior embedded transformer iris restoration iris restoration complexly degraded iris image aiming improve iris recognition performance challenging problem due complex degradation directly training convolutional neural network cnn without prior yield satisfactory result work propose generative iris prior embedded transformer model gformer build hierarchical encoderdecoder network employing transformer block generative iris prior first tame transformer block model longrange dependency target image second pretrain iris generative adversarial network gan obtain rich iris prior incorporate iris restoration process iris feature modulator experiment demonstrate proposed gformer outperforms stateoftheart method besides iris recognition performance significantly improved applying gformer
hpix generating vector map satellite image vector map find widespread utility across diverse domain due capacity store also represent discrete data boundary building footprint disaster impact analysis digitization urban planning location point transport link although extensive research exists identifying building footprint road type satellite imagery generation vector map imagery remains area limited exploration furthermore conventional map generation technique rely laborintensive manual feature extraction rulebased approach impose inherent limitation surmount limitation propose novel method called hpix utilizes modified generative adversarial network gans generate vector tile map satellite image hpix incorporates two hierarchical framework one operating global level local level resulting comprehensive model empirical evaluation proposed approach showcase effectiveness producing highly accurate visually captivating vector tile map derived satellite image extend study application include mapping road intersection building footprint cluster based area
trimodal confluence temporal dynamic scene graph generation operating room comprehensive understanding surgical scene allows monitoring surgical process reducing occurrence accident enhancing efficiency medical professional semantic modeling within operating room scene graph generation sgg task challenging since involves consecutive recognition subtle surgical action prolonged period address challenge propose trimodal ie image point cloud language confluence temporal dynamic framework termed tritempor diverging previous approach integrated temporal information via memory graph method embrace two advantage directly exploit bimodal temporal information video streaming hierarchical feature interaction prior knowledge large language model llm embedded alleviate classimbalance problem operating theatre specifically model performs temporal interaction across frame point cloud including scaleadaptive multiview temporal interaction viewtemp geometrictemporal point aggregation pointtemp furthermore transfer knowledge biomedical llm llavamed deepen comprehension intraoperative relation proposed tritempor enables aggregation trimodal feature relationaware unification predict relation generate scene graph experimental result benchmark demonstrate superior performance model longterm streaming
videodistill languageaware vision distillation video question answering significant advancement video question answering videoqa made thanks thriving large imagelanguage pretraining framework although imagelanguage model efficiently represent video language branch typically employ goalfree vision perception process interact vision language well answer generation thus omitting crucial visual cue paper inspired human recognition learning pattern propose videodistill framework languageaware ie goaldriven behavior vision perception answer generation process videodistill generates answer questionrelated visual embeddings follows thinkingobservinganswering approach closely resembles human behavior distinguishing previous research specifically develop languageaware gating mechanism replace standard crossattention avoiding language direct fusion visual representation incorporate mechanism two key component entire framework first component differentiable sparse sampling module selects frame containing necessary dynamic semantics relevant question second component vision refinement module merges existing spatialtemporal attention layer ensure extraction multigrained visual semantics associated question conduct experimental evaluation various challenging video questionanswering benchmark videodistill achieves stateoftheart performance general longform videoqa datasets addition verify videodistill effectively alleviate utilization language shortcut solution egotaskqa dataset
tooncrafter generative cartoon interpolation introduce tooncrafter novel approach transcends traditional correspondencebased cartoon video interpolation paving way generative interpolation traditional method implicitly assume linear motion absence complicated phenomenon like disocclusion often struggle exaggerated nonlinear large motion occlusion commonly found cartoon resulting implausible even failed interpolation result overcome limitation explore potential adapting liveaction video prior better suit cartoon interpolation within generative framework tooncrafter effectively address challenge faced applying liveaction video motion prior generative cartoon interpolation first design toon rectification learning strategy seamlessly adapts liveaction video prior cartoon domain resolving domain gap content leakage issue next introduce dualreferencebased decoder compensate lost detail due highly compressed latent prior space ensuring preservation fine detail interpolation result finally design flexible sketch encoder empowers user interactive control interpolation result experimental result demonstrate proposed method produce visually convincing natural dynamic also effectively handle disocclusion comparative evaluation demonstrates notable superiority approach existing competitor
planllm video procedure planning refinable large language model video procedure planning ie planning sequence action step given video frame start goal state essential ability embodied ai recent work utilize large language model llm generate enriched action step description text guide action step decoding although llm introduced method decode action step closedset onehot vector limiting model capability generalizing new step task additionally fixed action step description based worldlevel commonsense may contain noise specific instance visual state paper propose planllm crossmodal joint learning framework llm video procedure planning propose llmenhanced planning module fully us generalization ability llm produce freeform planning output enhance action step decoding also propose mutual information maximization module connect worldlevel commonsense step description samplespecific information visual state enabling llm employ reasoning ability generate step sequence assistance llm method closedset open vocabulary procedure planning task planllm achieves superior performance three benchmark demonstrating effectiveness design
denoising jointembedding predictive architecture jointembedding predictive architecture jepas shown substantial promise selfsupervised representation learning yet application generative modeling remains underexplored conversely diffusion model demonstrated significant efficacy modeling arbitrary probability distribution paper introduce denoising jointembedding predictive architecture djepa pioneering integration jepa within generative modeling recognizing jepa form masked image modeling reinterpret generalized nexttoken prediction strategy facilitating data generation autoregressive manner furthermore incorporate diffusion loss model pertoken probability distribution enabling data generation continuous space also adapt flow matching loss alternative diffusion loss thereby enhancing flexibility djepa empirically increased gflops djepa consistently achieves lower fid score fewer training epoch indicating good scalability base large huge model outperform previous generative model across scale imagenet conditional generation benchmark beyond image generation djepa wellsuited continuous data modeling including video audio
diffusion assisted image reconstruction optoacoustic tomography paper consider problem acoustic inversion context optoacoustic tomography image reconstruction problem leveraging ability recently proposed diffusion model image generative task among others devise image reconstruction architecture based conditional diffusion process scheme make use initial image reconstruction preprocessed autoencoder generate adequate representation representation used conditional information generative diffusion process although computational requirement training implementing architecture low several design choice discussed work made keep manageable numerical result show conditional information allows properly bias parameter diffusion model improve quality initial reconstructed image eliminating artifact even reconstructing finer detail groundtruth image recoverable initial image reconstruction method also tested proposal experimental condition obtained result line corresponding numerical simulation improvement image quality term peak signaltonoise ratio observed
frequencydomain refinement multiscale diffusion super resolution performance single image superresolution depends heavily generate complement highfrequency detail lowresolution image recently diffusionbased model exhibit great potential generating highquality image superresolution task however existing model encounter difficulty directly predicting highfrequency information wide bandwidth solely utilizing highresolution ground truth target sampling timesteps tackle problem achieve higherquality superresolution propose novel frequency domainguided multiscale diffusion model fddiff decomposes highfrequency information complementing process finergrained step particular wavelet packetbased frequency complement chain developed provide multiscale intermediate target increasing bandwidth reverse diffusion process fddiff guide reverse diffusion process progressively complement missing highfrequency detail timesteps moreover design multiscale frequency refinement network predict required highfrequency component multiple scale within one unified network comprehensive evaluation popular benchmark conducted demonstrate fddiff outperforms prior generative method higherfidelity superresolution result
diffusion transformer capture spatialtemporal dependency theory gaussian process data diffusion transformer backbone sora video generation successfully scale capacity diffusion model pioneering new avenue highfidelity sequential data generation unlike static data image sequential data consists consecutive data frame indexed time exhibiting rich spatial temporal dependency dependency represent underlying dynamic model critical validate generated data paper make first theoretical step towards bridging diffusion transformer capturing spatialtemporal dependency specifically establish score approximation distribution estimation guarantee diffusion transformer learning gaussian process data covariance function various decay pattern highlight spatialtemporal dependency captured affect learning efficiency study proposes novel transformer approximation theory transformer act unroll algorithm support theoretical result numerical experiment providing strong evidence spatialtemporal dependency captured within attention layer aligning approximation theory
presto fast motion planning using diffusion model based keyconfiguration environment representation introduce learningguided motion planning framework generates seed trajectory using diffusion model trajectory optimization given workspace method approximates configuration space cspace obstacle environment representation consisting sparse set taskrelated key configuration used conditioning input diffusion model diffusion model integrates regularization term encourage smooth collisionfree trajectory training trajectory optimization refines generated seed trajectory correct colliding segment experimental result demonstrate highquality trajectory prior learned cspacegrounded diffusion model enable efficient generation collisionfree trajectory narrowpassage environment outperforming previous learning planningbased baseline video additional material found project page httpskiwisherbetgithubiopresto
casc conditionaware semantic communication latent diffusion model diffusionbased semantic communication method shown significant advantage image transmission harnessing generative power diffusion model however still face challenge including generation randomness lead distorted reconstruction high computational cost address issue propose casc conditionaware semantic communication framework incorporates latent diffusion model ldmbased denoiser ldm denoiser receiver utilizes received noisy latent code conditioning signal reconstruct latent code enabling decoder accurately recover source image operating latent space ldm reduces computational complexity compared traditional diffusion model dm additionally introduce conditionaware neural network dynamically adjusts weight hidden layer ldm based conditioning signal enables finer control generation process significantly improving perceptual quality reconstructed image experimental result show casc significantly outperforms deepjscc perceptual quality visual effect moreover casc reduces inference time compared existing dmbased semantic communication system maintaining comparable perceptual performance ablation study also validate effectiveness module improving image reconstruction quality
bracket diffusion hdr image generation consistent ldr denoising demonstrate generating hdr image using concerted action multiple blackbox pretrained ldr image diffusion model relying pretrained ldr generative diffusion model vital first sufficiently large hdr image dataset available retrain second even retraining model impossible compute budget instead seek inspiration hdr image capture literature traditionally fuse set ldr image called exposure bracket produce single hdr image operate multiple denoising process generate multiple ldr bracket together form valid hdr result key making work introduce consistency term diffusion process couple bracket agree across exposure range share accounting possible difference due quantization error demonstrate stateoftheart unconditional conditional restorationtype generative modeling result yet hdr
dynamic duo building block dimensional mechanic mechanic study relationship space time matter expressed term dimension length mathcall time mathcalt mass mathcalm dimension broadens scope mechanic geometric quantity dimension form mathcallx like length area kinematic quantity form mathcallxmathcalty like speed acceleration eventually masscarrying quantity mass force momentum energy action power viscosity etc standard mechanical quantity dimension form mathcalmmathcallxmathcalty x integer contribution use dimensional structure arrange masscarrying quantity table indexed x ratio quantity row provide characteristic length column characteristic time encompassing great variety physical phenomenon atomic astronomical scale generally show picking duo mechanical quantity neither row column yield dynamic one mechanical quantity understood impelling motion impeding force mass prototype impelling impeding factor many duo possible review provides novel synthesis revealing power dimensional analysis understand process governed interplay two mechanical quantity elementary decomposition space time motion pair mechanical factor foundation dimensional mechanic method review wish promote advance review complemented online video lecture initiate discussion elaborate interplay two mechanical quantity
postmastoidectomy surface multiview synthesis single microscopy image cochlear implant ci procedure involve performing invasive mastoidectomy insert electrode array cochlea paper introduce novel pipeline capable generating synthetic multiview video single ci microscope image approach use patient preoperative ct scan predict postmastoidectomy surface using method designed purpose manually align surface selected microscope frame obtain accurate initial pose reconstructed ct mesh relative microscope perform uv projection transfer color frame surface texture novel view textured surface used generate large dataset synthetic frame ground truth pose evaluated quality synthetic view rendered using pyvista found rendering engine lead similarly highquality synthetic novelview frame compared ground truth structural similarity index method averaging large dataset novel view known pose critical ongoing training method automatically estimate microscope pose registration preoperative ct facilitate augmented reality surgery dataset empower various downstream task integrating augmented reality ar tracking surgical tool supporting video analysis study
prototypical transformer unified motion learner work introduce prototypical transformer protoformer general unified framework approach various motion task prototype perspective protoformer seamlessly integrates prototype learning transformer thoughtfully considering motion dynamic introducing two innovative design first crossattention prototyping discovers prototype based signature motion pattern providing transparency understanding motion scene second latent synchronization guide feature representation learning via prototype effectively mitigating problem motion uncertainty empirical result demonstrate approach achieves competitive performance popular motion task optical flow scene depth furthermore exhibit generality across various downstream task including object tracking video stabilization
diffusionbased handobject interaction prediction egocentric video understanding human would behave handobject interaction vital application service robot manipulation extended reality achieve recent work proposed simultaneously forecast hand trajectory object affordances human egocentric video joint prediction serf comprehensive representation future handobject interaction space indicating potential human motion motivation however existing approach mostly adopt autoregressive paradigm unidirectional prediction lack mutual constraint within holistic future sequence accumulates error along time axis meanwhile work basically overlook effect camera egomotion firstperson view prediction address limitation propose novel diffusionbased interaction prediction method namely forecast future hand trajectory object affordances concurrently iterative nonautoregressive manner transform sequential image latent feature space design denoising diffusion model predict future latent interaction feature conditioned past one motion feature integrated conditional denoising process enable aware camera wearer dynamic accurate interaction prediction extensive experiment demonstrate method significantly outperforms stateoftheart baseline offtheshelf metric newly proposed evaluation protocol highlight efficacy leveraging generative paradigm handobject interaction prediction code released open source
lamod latent motion diffusion model myocardial strain generation motion deformation analysis cardiac magnetic resonance cmr imaging video crucial assessing myocardial strain patient abnormal heart function recent advance deep learningbased image registration algorithm shown promising result predicting motion field routinely acquired cmr sequence however accuracy often diminishes region subtle appearance change error propagating time advanced imaging technique displacement encoding stimulated echo dense cmr offer highly accurate reproducible motion data require additional image acquisition pose challenge busy clinical flow paper introduce novel latent motion diffusion model lamod predict highly accurate dense motion standard cmr video specifically method first employ encoder pretrained registration network learns latent motion feature also considered deformationbased shape feature image sequence supervised groundtruth motion provided dense lamod leverage probabilistic latent diffusion model reconstruct accurate motion extracted feature experimental result demonstrate proposed method lamod significantly improves accuracy motion analysis standard cmr image hence improving myocardial strain analysis clinical setting cardiac patient code publicly available httpsgithubcomjrxinglamod
crossmodality translation generative adversarial network unveil alzheimers disease biomarkers generative approach crossmodality transformation recently gained significant attention neuroimaging previous work focused casecontrol data application generative model disorderspecific datasets ability preserve diagnostic pattern remain relatively unexplored hence study investigated use generative adversarial network gan context alzheimers disease ad generate functional network connectivity fnc structural magnetic resonance imaging data employed cyclegan synthesize data unpaired data transition enhanced transition integrating weak supervision case paired data available finding revealed model could offer remarkable capability achieving structural similarity index measure ssim pm correlation pm fncs moreover qualitative analysis revealed similar pattern generated actual data comparing ad cognitively normal cn individual particular observed significantly increased functional connectivity cerebellarsensory motor cerebellarvisual network reduced connectivity cerebellarsubcortical auditorysensory motor sensory motorvisual cerebellarcognitive control network additionally image generated model showed similar pattern atrophy hippocampal temporal region alzheimers patient
synthesizing betaamyloid pet image structural mri preliminary study betaamyloid positron emission tomography abetapet imaging become critical tool alzheimers disease ad research diagnosis providing insight pathological accumulation amyloid plaque one hallmark ad however high cost limited availability exposure radioactivity restrict widespread use abetapet imaging leading scarcity comprehensive datasets previous study suggested structural magnetic resonance imaging mri readily available may serve viable alternative synthesizing abetapet image study propose approach utilize diffusion model synthesize abetapet image mri scan aiming overcome limitation associated direct pet imaging method generates highquality abetapet image cognitive normal case although less effective mild cognitive impairment mci patient due variability abeta deposition pattern among subject preliminary result suggest incorporating additional data larger sample mci case multimodality information including clinical demographic detail cognitive functional assessment longitudinal data may necessary improve abetapet image synthesis mci patient
neural differential appearance equation propose method reproduce dynamic appearance texture spacestationary timevarying visual statistic previous work decomposes dynamic texture static appearance motion focus dynamic appearance result motion variation fundamental property rusting decaying melting weathering end adopt neural ordinary differential equation ode learn underlying dynamic appearance target exemplar simulate ode two phase warmup phase ode diffuses random noise initial state constrain evolution ode replicate evolution visual feature statistic exemplar generation phase particular innovation work neural ode achieving denoising evolution dynamic synthesis proposed temporal training scheme study relightable brdf nonrelightable rgb appearance model introduce new pilot datasets allowing first time study phenomenon rgb provide dynamic texture acquired free online source brdfs acquire dataset flashlit video timevarying material enabled simpletoconstruct setup experiment show method consistently yield realistic coherent result whereas prior work falter pronounced temporal appearance variation user study confirms approach preferred previous work exemplar
textvideo retrieval globallocal semantic consistent learning adapting largescale imagetext pretraining model eg clip video domain represents current stateoftheart textvideo retrieval primary approach involve transferring textvideo pair common embedding space leveraging crossmodal interaction specific entity semantic alignment though effective paradigm entail prohibitive computational cost leading inefficient retrieval address propose simple yet effective method globallocal semantic consistent learning glscl capitalizes latent shared semantics across modality textvideo retrieval specifically introduce parameterfree global interaction module explore coarsegrained alignment devise shared local interaction module employ several learnable query capture latent semantic concept learning finegrained alignment furthermore interconsistency loss icl devised accomplish concept alignment visual query corresponding textual query intradiversity loss idl developed repulse distribution within visual textual query generate discriminative concept extensive experiment five widely used benchmark ie msrvtt msvd didemo lsmdc activitynet substantiate superior effectiveness efficiency proposed method remarkably method achieves comparable performance sota well nearly time faster term computational cost code available httpsgithubcomzchoiglscl
guiworld video benchmark dataset multimodal guioriented understanding recently multimodal large language model mllms used agent control keyboard mouse input directly perceiving graphical user interface gui generating corresponding command however current agent primarily demonstrate strong understanding capability static environment mainly applied relatively simple domain web mobile interface argue robust gui agent capable perceiving temporal information gui including dynamic web content multistep task additionally possess comprehensive understanding various gui scenario including desktop software multiwindow interaction end paper introduces new dataset termed guiworld feature meticulously crafted humanmllm annotation extensively covering six gui scenario eight type guioriented question three format evaluate capability current stateoftheart mllms including image llm video llm understanding various type gui content especially dynamic sequential content finding reveal current model struggle dynamic gui content without manually annotated keyframes operation history hand video llm fall short guioriented task given sparse gui video dataset therefore take initial step leveraging finetuned video llm guivid guioriented assistant demonstrating improved understanding various gui task however due limitation performance base llm conclude using video llm gui agent remains significant challenge believe work provides valuable insight future research dynamic gui content understanding dataset code publicly available httpsguiworldgithubio
dajc direct analog mjpeg converter video sensor node using lownoise switched capacitor macquantizer autocalibration sparsityaware adc advancement field internet thingsiot internet bodiesiob video camera application using video sensor nodesvsns gained importance field autonomous driving health monitoring robot control security camera application however application typically involve high data rate due transmission highresolution video signal resulting high data volume generated analogtodigital converter adcs significant data deluge pose processing storage overhead exacerbating problem address challenge propose lowpower solution aimed reducing power consumption video sensor node vsns shifting computation digital domain inherently energyefficient analog domain unlike standard architecture computation processing typically performed digital signal processing dsp block adcs approach eliminates need block instead leverage switched capacitorbased computation unit analog domain resulting reduction power consumption achieve reduction power consumption compared digital implementation furthermore employ sparsityaware adc enabled significant compressed sample contribute small fraction total captured analog sample achieve lower adc conversion energy without considerable degradation contributing overall energy saving system
moto latent motion token bridging language learning robot manipulation video recent development large language model pretrained extensive corpus shown significant success various natural language processing task minimal finetuning success offer new promise robotics long constrained high cost actionlabeled data ask given abundant video data containing interactionrelated knowledge available rich corpus similar generative pretraining approach effectively applied enhance robot learning key challenge identify effective representation autoregressive pretraining benefit robot manipulation task inspired way human learn new skill observing dynamic environment propose effective robotic learning emphasize motionrelated knowledge closely tied lowlevel action hardwareagnostic facilitating transfer learned motion actual robot action end introduce moto convert video content latent motion token sequence latent motion tokenizer learning bridging language motion video unsupervised manner pretrain motogpt motion token autoregression enabling capture diverse visual motion knowledge pretraining motogpt demonstrates promising ability produce semantically interpretable motion token predict plausible motion trajectory assess trajectory rationality output likelihood transfer learned motion prior real robot action implement cofinetuning strategy seamlessly bridge latent motion token prediction real robot control extensive experiment show finetuned motogpt exhibit superior robustness efficiency robot manipulation benchmark underscoring effectiveness transferring knowledge video data downstream visual manipulation task
uniadafocus spatialtemporal dynamic computation video recognition paper present comprehensive exploration phenomenon data redundancy video understanding aim improve computational efficiency investigation commences examination spatial redundancy refers observation informative region video frame usually corresponds small image patch whose shape size location shift smoothly across frame motivated phenomenon formulate patch localization problem dynamic decision task introduce spatially adaptive video recognition approach termed adafocus specific lightweight encoder first employed quickly process full video sequence whose feature utilized policy network identify taskrelevant region subsequently selected patch inferred highcapacity deep network final prediction full model trained endtoend conveniently furthermore adafocus extended considering temporal samplewise redundancy ie allocating majority computation taskrelevant frame minimizing computation spent relatively easier video resulting approach uniadafocus establishes comprehensive framework seamlessly integrates spatial temporal samplewise dynamic computation preserve merit adafocus term efficient endtoend training hardware friendliness addition uniadafocus general flexible compatible offtheshelf efficient backbone eg tsm readily deployed feature extractor yielding significantly improved computational efficiency empirically extensive experiment based seven benchmark datasets three application scenario substantiate uniadafocus considerably efficient competitive baseline
prime protect video malicious editing development generative model quality generated content keep increasing recently opensource model made surprisingly easy manipulate edit photo video simple prompt cuttingedge technology gained popularity also given rise concern regarding privacy portrait right individual malicious user exploit tool deceptive illegal purpose although previous work focus protecting photo generative model find still gap protecting video image aspect efficiency effectiveness therefore introduce protection method prime significantly reduce time cost improve protection performance moreover evaluate proposed protection method consider objective metric human subjective metric evaluation result indicate prime cost gpu hour cost previous stateoftheart method achieves better protection result human evaluation objective metric code found httpsgithubcomguanlinleeprime
learning expressive generalizable motion feature face forgery detection previous face forgery detection method mainly focus appearance feature may easily attacked sophisticated manipulation considering majority current face manipulation method generate fake face based single frame take frame consistency coordination consideration artifact frame sequence effective face forgery detection however current sequencebased face forgery detection method use general video classification network directly discard special discriminative motion information face manipulation detection end propose effective sequencebased forgery detection framework based existing video classification method make motion feature expressive manipulation detection propose alternative motion consistency block instead original motion feature module make learned feature generalizable propose auxiliary anomaly detection block two specially designed improvement make general video classification network achieve promising result three popular face forgery datasets
reacto reconstructing articulated object single video paper address challenge reconstructing general articulated object single video existing work employing dynamic neural radiance field advanced modeling articulated object like human animal video face challenge piecewise rigid general articulated object due limitation deformation model tackle propose quasirigid blend skinning novel deformation model enhances rigidity part maintaining flexible deformation joint primary insight combine three distinct approach enhanced bone rigging system improved component modeling use quasisparse skinning weight boost part rigidity reconstruction fidelity application geodesic point assignment precise motion seamless deformation method outperforms previous work producing higherfidelity reconstruction general articulated object demonstrated real synthetic datasets project page httpschaoyuesonggithubioreacto
multistream fusion approach oneclass learning audiovisual deepfake detection paper address challenge developing robust audiovisual deepfake detection model practical use case new generation algorithm continually emerging algorithm encountered development detection method call generalization ability method additionally ensure credibility detection method beneficial model interpret cue video indicate fake motivated consideration propose multistream fusion approach oneclass learning representationlevel regularization technique study generalization problem audiovisual deepfake detection creating new benchmark extending resplitting existing fakeavceleb dataset benchmark contains four category fake video real audiofake visual fake audiofake visual fake audioreal visual unsynchronized video experimental result demonstrate approach surpasses previous model large margin furthermore proposed framework offer interpretability indicating modality model identifies likely fake source code released httpsgithubcombokbokmsoc
mmsummary multimodal summary generation fetal ultrasound video present first automated multimodal summary generation system mmsummary medical imaging video particularly focus fetal ultrasound analysis imitating examination process performed human sonographer mmsummary designed threestage pipeline progressing keyframe detection keyframe captioning finally anatomy segmentation measurement keyframe detection stage innovative automated workflow proposed progressively select concise set keyframes preserving sufficient video information without redundancy subsequently adapt large language model generate meaningful caption fetal ultrasound keyframes keyframe captioning stage keyframe captioned fetal biometry segmentation measurement stage estimate biometric parameter segmenting region interest according textual prior mmsummary system provides comprehensive summary fetal ultrasound examination based reported experiment estimated reduce scanning time approximately thereby suggesting potential enhance clinical workflow efficiency
towards realtime generation delaycompensated video feed outdoor mobile robot teleoperation teleoperation important technology enable supervisor control agricultural robot remotely however environmental factor dense crop row limitation network infrastructure hinder reliability data streamed teleoperators issue result delayed variable frame rate video feed often deviate significantly robot actual viewpoint propose modular learningbased vision pipeline generate delaycompensated image realtime supervisor extensive offline evaluation demonstrate method generates accurate image compared stateoftheart approach setting additionally one work evaluate delaycompensation method outdoor field environment complex terrain data real robot realtime resulting video code provided httpssitesgooglecomillinoiseducompteleop
robust audiovisual speech recognition model mixtureofexperts visual signal enhance audiovisual speech recognition accuracy providing additional contextual information given complexity visual signal audiovisual speech recognition model requires robust generalization capability across diverse video scenario presenting significant challenge paper introduce eva leveraging mixtureofexperts audiovisual asr perform robust speech recognition inthewild video specifically first encode visual information visual token sequence map speech space lightweight projection build eva upon robust pretrained speech recognition model ensuring generalization ability moreover incorporate visual information effectively inject visual information asr model mixtureofexperts module experiment show model achieves stateoftheart result three benchmark demonstrates generalization ability eva across diverse video domain
enhancing motion variation texttomotion model via pose video conditioned editing texttomotion model generate sequence human pose textual description garnering significant attention however due data scarcity range motion model produce still limited instance current texttomotion model generate motion kicking football instep foot since training data includes martial art kick propose novel method us short video clip image condition modify existing basic motion approach model understanding kick serf prior video image football kick act posterior enabling generation desired motion incorporating additional modality condition method create motion present training set overcoming limitation textmotion datasets user study participant demonstrated approach produce unseen motion realism comparable commonly represented motion textmotion datasets eg walking running squatting kicking
mote reconciling generalization specialization visuallanguage video knowledge transfer transferring visuallanguage knowledge largescale foundation model video recognition proved effective bridge domain gap additional parametric module added capture temporal information however zeroshot generalization diminishes increase number specialized parameter making existing work tradeoff zeroshot closeset performance paper present mote novel framework enables generalization specialization balanced one unified model approach tune mixture temporal expert learn multiple task view various degree data fitting maximally preserve knowledge expert propose emphweight merging regularization regularizes merging process expert weight space additionally temporal feature modulation regularize contribution temporal feature test achieve sound balance zeroshot closeset video recognition task obtain stateoftheart competitive result various datasets including ucf hmdb code available urlhttpsgithubcomzmhhhmote
videotospeech generation speech decomposition rectified flow paper introduce novel videotospeech framework designed generate natural intelligible speech directly silent talking face video recent system shown promising result constrained datasets limited speaker vocabulary performance often degrades realworld unconstrained datasets due inherent variability complexity speech signal address challenge decompose speech signal manageable subspace content pitch speaker information representing distinct speech attribute predict directly visual input generate coherent realistic speech predicted attribute employ rectified flow matching decoder built transformer architecture model efficient probabilistic pathway random noise target speech distribution extensive experiment demonstrate significantly outperforms stateoftheart method even surpassing naturalness ground truth utterance
perse personalized generative avatar single portrait present perse method building animatable personalized generative avatar reference portrait avatar model enables facial attribute editing continuous disentangled latent space control facial attribute preserving individual identity achieve method begin synthesizing largescale synthetic video datasets video contains consistent change facial expression viewpoint combined variation specific facial attribute original input propose novel pipeline produce highquality photorealistic video facial attribute editing leveraging synthetic attribute dataset present personalized avatar creation method based gaussian splatting learning continuous disentangled latent space intuitive facial attribute manipulation enforce smooth transition latent space introduce latent space regularization technique using interpolated face supervision compared previous approach demonstrate perse generates highquality avatar interpolated attribute preserving identity reference person
analyzing tumor synthesis computeraided tumor detection shown great potential enhancing interpretation million ct scan performed annually united state however challenge arise due rarity ct scan tumor especially earlystage tumor developing ai real tumor data face issue scarcity annotation difficulty low prevalence tumor synthesis address challenge generating numerous tumor example medical image aiding ai training tumor detection segmentation successful synthesis requires realistic generalizable synthetic tumor across various organ chapter review ai development real synthetic data summarizes two key trend synthetic data cancer imaging research modelingbased learningbased approach modelingbased method like simulate tumor development time using generic rule learningbased method like difftumor learn annotated example one organ generate synthetic tumor others reader study expert radiologist show synthetic tumor convincingly realistic also present case study liver pancreas kidney reveal ai trained synthetic tumor achieve performance comparable better ai trained real data tumor synthesis hold significant promise expanding datasets enhancing ai reliability improving tumor detection performance preserving patient privacy
figclip finegrained clip adaptation via densely annotated video contrastive language image pretraining clip exhibited impressive performance learning highly semantic generalized representation recent work exposed fundamental drawback syntactic property includes interpreting finegrained attribute action spatial relation state detail require compositional reasoning one reason natural caption often capture visual detail scene lead unaddressed visual concept misattributed wrong word pooled image text feature end acting bag word hence losing syntactic information work ask possible enhance clip finegrained syntactic ability without compromising semantic property show possible adapting clip efficiently highquality comprehensive relatively small dataset demonstrate adaptation strategy vidsitu video situation recognition dataset annotated verb rich semantic role label srl use srl verb information create rulebased detailed caption making sure capture visual concept combined hard negative hierarchical loss annotation allow u learn powerful visual representation dubbed finegrained clip figclip preserve semantic understanding detailoriented evaluate five diverse visionlanguage task finetuning zeroshot setting achieving consistent improvement base clip model
mag reconstructing simulating dynamic object meshadsorbed gaussian splatting reconstruction simulation although interrelated distinct objective reconstruction requires flexible representation adapt diverse scene simulation need structured representation model motion principle effectively paper introduces meshadsorbed gaussian splatting mag method address challenge mag constrains gaussians roam near mesh creating mutually adsorbed meshgaussian representation representation harness rendering flexibility gaussians structured property mesh achieve introduce rmdnet network learns motion prior video data refine mesh deformation alongside rgdnet model relative displacement mesh gaussians enhance rendering fidelity mesh constraint generalize novel userdefined deformation beyond input video without reliance temporal data propose mpenet leverage inherent mesh information bootstrap rmdnet rgdnet due universality mesh mag compatible various deformation prior arap smpl soft physic simulation extensive experiment dnerf dgmesh peoplesnapshot datasets demonstrate mag achieves stateoftheart performance reconstruction simulation
learning precise affordances egocentric video robotic manipulation affordance defined potential action object offer crucial robotic manipulation task deep understanding affordance lead intelligent ai system example knowledge directs agent grasp knife handle cutting blade passing someone paper present streamlined affordance learning system encompasses data collection effective model training robot deployment first collect training data egocentric video automatic manner different previous method focus object graspable affordance represent coarse heatmaps cover graspable eg object handle functional affordances eg knife blade hammer head extract data precise segmentation mask propose effective model termed geometryguided affordance transformer gkt train collected data gkt integrates innovative depth feature injector dfi incorporate shape geometric prior enhancing model understanding affordances enable affordanceoriented manipulation introduce affgrasp framework combine gkt grasp generation model comprehensive evaluation create affordance evaluation dataset pixelwise annotation design realworld task robot experiment result show gkt surpasses stateoftheart miou affgrasp achieves high success rate affordance prediction successful grasping among trial including evaluation seen unseen object cluttered scene
vera explainable video anomaly detection via verbalized learning visionlanguage model rapid advancement visionlanguage model vlms established new paradigm video anomaly detection vad leveraging vlms simultaneously detect anomaly provide comprehendible explanation decision existing work direction often assumes complex reasoning required vad exceeds capability pretrained vlms consequently approach either incorporate specialized reasoning module inference rely instruction tuning datasets additional training adapt vlms vad however strategy often incur substantial computational cost data annotation overhead address challenge explainable vad introduce verbalized learning framework named vera enables vlms perform vad without model parameter modification specifically vera automatically decomposes complex reasoning required vad reflection simpler focused guiding question capturing distinct abnormal pattern treat reflective question learnable parameter optimizes datadriven verbal interaction learner optimizer vlms using coarsely labeled training data inference vera embeds learned question model prompt guide vlms generating segmentlevel anomaly score refined framelevel score via fusion scene temporal context experimental result challenging benchmark demonstrate learned question vera highly adaptable significantly improving detection performance explainability vlms vad
sora openais prelude social medium perspective sora openai future ai video generation rapid advancement generative ai genai transforming humancomputer interaction hci significant implication across various sector study investigates public perception sora openai pioneering genai video generation tool via social medium discussion reddit release center two main question envisioned application concern related soras integration analysis forecast positive shift content creation predicting sora democratize video marketing innovate game development making video production accessible economical conversely concern deepfakes potential disinformation underscoring need strategy address disinformation bias paper contributes genai discourse fostering discussion current future capability enriching understanding public expectation establishing temporal benchmark user anticipation research underscore necessity informed ethical approach ai development integration ensuring technological advancement align societal value user need
flow snapshot neuron action deep neural network generalize biological motion perception biological motion perception bmp refers human ability perceive recognize action living being solely motion pattern sometimes minimal depicted pointlight display human excel task without prior training current ai model struggle poor generalization performance close research gap propose motion perceiver mp mp solely relies patchlevel optical flow video clip input training learns prototypical flow snapshot competitive binding mechanism integrates invariant motion representation predict action label given video inference evaluate generalization ability ai model human video stimulus spanning bmp condition using pointlight display neuroscience remarkably mp outperforms existing ai model maximum improvement action recognition accuracy condition moreover benchmark ai model pointlight display two standard video datasets computer vision mp also demonstrates superior performance case interestingly via psychophysics experiment found mp recognizes biological movement way aligns human behavior data code available httpsgithubcomzhanglabdeepneurocoglabmotionperceiver
may dance dance generation framework nonhumanoids hypothesize dance motion form visual rhythm music visual rhythm perceived optical flow agent recognize relationship visual rhythm music able dance generating motion create visual rhythm match music based propose framework kind nonhumanoid agent learn dance human video framework work two process training reward model perceives relationship optical flow visual rhythm music human dance video training nonhumanoid dancer based reward model reinforcement learning reward model consists two feature encoders optical flow music trained based contrastive learning make higher similarity concurrent optical flow music feature reward model agent learns dancing getting higher reward action creates optical flow whose feature higher similarity given music feature experiment result show generated dance motion align music beat properly user study result indicates framework preferred human compared baseline best knowledge work nonhumanoid agent learn dance human video unprecedented example video found
shine saliencyaware hierarchical negative ranking compositional temporal grounding temporal grounding also known video moment retrieval aim locating video segment corresponding given query sentence compositional nature natural language enables localization beyond predefined event posing certain challenge compositional generalizability existing method recent study establish correspondence video query decomposereconstruct manner achieve compositional generalization however consider dominant primitive build negative query random sampling recombination resulting semantically implausible negative hinder model learning rational composition addition recent detrbased method still underperform compositional temporal grounding showing irrational saliency response given negative query subtle difference positive query address limitation first propose large language modeldriven method negative query construction utilizing generate semantically plausible hard negative query subsequently introduce coarsetofine saliency ranking strategy encourages model learn multigranularity semantic relationship video hierarchical negative query boost compositional generalization extensive experiment two challenging benchmark validate effectiveness generalizability proposed method code available httpsgithubcomzxccadeshine
effectively leveraging clip generating situational summary image video situation recognition refers ability agent identify understand various situation context based available information sensory input involves cognitive process interpreting data environment determine happening factor involved action caused situation interpretation situation formulated semantic role labeling problem computer visionbased situation recognition situation depicted image video hold pivotal information essential various application like image video captioning multimedia retrieval autonomous system event monitoring however existing method often struggle ambiguity lack context generating meaningful accurate prediction leveraging multimodal model clip propose clipsitu sidestep need full finetuning achieves stateoftheart result situation recognition localization task clipsitu harness clipbased image verb role embeddings predict noun fulfilling role associated verb providing comprehensive understanding depicted scenario crossattention transformer clipsitu xtf enhances connection semantic role query visual token representation leading superior performance situation recognition also propose verbwise role prediction model nearperfect accuracy create endtoend framework producing situational summary outofdomain image show situational summary empower clipsitu model produce structured description reduced ambiguity compared generic caption finally extend clipsitu video situation recognition showcase versatility produce comparable performance stateoftheart method
resyncer rewiring stylebased generator unified audiovisually synced facial performer lipsyncing video given audio foundation various application including creation virtual presenter performer recent study explore highfidelity lipsync different technique taskorientated model either require longterm video clipspecific training retain visible artifact paper propose unified effective framework resyncer synchronizes generalized audiovisual facial information key design revisiting rewiring stylebased generator efficiently adopt facial dynamic predicted principled styleinjected transformer simply reconfiguring information insertion mechanism within noise style space framework fuse motion appearance unified training extensive experiment demonstrate resyncer produce highfidelity lipsynced video according audio also support multiple appealing property suitable creating virtual presenter performer including fast personalized finetuning videodriven lipsyncing transfer speaking style even face swapping resource found
multireference generative face video compression contrastive learning generative face video coding gfvc demonstrated potential approach lowlatency low bitrate video conferencing gfvc framework achieve extreme gain coding efficiency bitrate saving compared conventional codecs bitrates recent mpegjvet standardization effort information required reconstruct video sequence using gfvc framework adopted part supplemental enhancement information sei existing compression pipeline light development aim address challenge weakly addressed prior gfvc framework ie reconstruction drift distance reference target frame increase challenge creates need update reference buffer frequently transmitting intrarefresh frame expensive element gfvc bitstream overcome problem propose instead multiple reference animation robust approach minimizing reconstruction drift especially used bidirectional prediction mode propose contrastive learning formulation multireference animation observe using contrastive learning framework enhances representation capability animation generator resulting framework mrdac multireference deep animation codec therefore used compress longer sequence fewer reference frame achieve significant gain reconstruction accuracy comparable bitrates previous framework quantitative qualitative result show significant coding reconstruction quality gain compared previous gfvc method accurate animation quality presence large pose facial expression change
towards synthetic data generation improved pain recognition video patient constraint recognizing pain video crucial improving patientcomputer interaction system yet traditional data collection domain raise significant ethical logistical challenge study introduces novel approach leverage synthetic data enhance videobased pain recognition model providing ethical scalable alternative present pipeline synthesizes realistic facial model capturing nuanced facial movement small participant pool mapping onto diverse synthetic avatar process generates synthetic face accurately reflecting genuine pain expression varied angle perspective utilizing advanced facial capture technique leveraging public datasets like celebvhq ffhquv demographic diversity new synthetic dataset significantly enhances model training ensuring privacy anonymizing identity facial replacement experimental result demonstrate model trained combination synthetic data paired small amount real participant achieve superior performance pain recognition effectively bridging gap synthetic simulation realworld application approach address data scarcity ethical concern offering new solution pain detection opening new avenue research privacypreserving dataset generation resource publicly available encourage innovation field
texttoon realtime text toonify head avatar single video propose texttoon method generate drivable toonified avatar given short monocular video sequence written instruction avatar style model generate highfidelity toonified avatar driven realtime another video arbitrary identity existing related work heavily rely multiview modeling recover geometry via texture embeddings presented static manner leading control limitation multiview video input also make difficult deploy model realworld application address issue adopt conditional embedding triplane learn realistic stylized facial representation gaussian deformation field additionally expand stylization capability gaussian splatting introducing adaptive pixeltranslation neural network leveraging patchaware contrastive learning achieve highquality image push work consumer application develop realtime system operate fps gpu machine fps mobile machine extensive experiment demonstrate efficacy approach generating textual avatar existing method term quality realtime animation please refer project page detail httpssongluchuangithubiotexttoon
animatex universal character image animation enhanced motion representation character image animation generates highquality video reference image target pose sequence seen significant progress recent year however existing method apply human figure usually generalize well anthropomorphic character commonly used industry like gaming entertainment indepth analysis suggests attribute limitation insufficient modeling motion unable comprehend movement pattern driving video thus imposing pose sequence rigidly onto target character end paper proposes animatex universal animation framework based ldm various character type collectively named x including anthropomorphic character enhance motion representation introduce pose indicator capture comprehensive motion pattern driving video implicit explicit manner former leverage clip visual feature driving video extract gist motion like overall movement pattern temporal relation among motion latter strengthens generalization ldm simulating possible input advance may arise inference moreover introduce new animated anthropomorphic benchmark evaluate performance animatex universal widely applicable animation image extensive experiment demonstrate superiority effectiveness animatex compared stateoftheart method
latent action pretraining video introduce latent action pretraining general action model lapa unsupervised method pretraining visionlanguageaction vla model without groundtruth robot action label existing visionlanguageaction model require action label typically collected human teleoperators pretraining significantly limit possible data source scale work propose method learn internetscale video robot action label first train action quantization model leveraging vqvaebased objective learn discrete latent action image frame pretrain latent vla model predict latent action observation task description finally finetune vla smallscale robot manipulation data map latent robot action experimental result demonstrate method significantly outperforms existing technique train robot manipulation policy largescale video furthermore outperforms stateoftheart vla model trained robotic action label realworld manipulation task require language conditioning generalization unseen object semantic generalization unseen instruction training human manipulation video also show positive transfer opening potential leveraging webscale data robotics foundation model
mmds multimodal medical diagnosis system integrating image analysis knowledgebased departmental consultation present mmds system capable recognizing medical image patient facial detail providing professional medical diagnosis system consists two core componentsthe first component analysis medical image video trained specialized multimodal medical model capable interpreting medical image accurately analyzing patient facial emotion facial paralysis condition model achieved accuracy facial emotion recognition dataset accuracy recognizing happy emotion facial paralysis recognition model reached accuracy higher based model developed parser analyzing facial movement video patient facial paralysis achieving precise grading paralysis severity test video facial paralysis patient system demonstrated grading accuracy second component generation professional medical response employed large language model integrated medical knowledge base generate professional diagnosis based analysis medical image video core innovation lie development departmentspecific knowledge base routing management mechanism large language model categorizes data medical department retrieval process determines appropriate knowledge base query significantly improves retrieval accuracy rag retrievalaugmented generation process
glcf globallocal multimodal coherence analysis framework talking face generation detection talking face generation tfg allows producing lifelike talking video character using facial image accompanying text abuse technology could pose significant risk society creating urgent need research corresponding detection method however research field hindered lack public datasets paper construct first largescale multiscenario talking face dataset mstf contains audio video forgery technique filling gap datasets field dataset cover generation scenario semantic scenario closer practical application scenario tfg besides also propose tfg detection framework leverage analysis global local coherence multimodal content tfg video therefore regionfocused smoothness detection module rsfdm discrepancy capturetime frame aggregation module dctam introduced evaluate global temporal coherence tfg video aggregating multigrained spatial information additionally visualaudio fusion module vafm designed evaluate audiovisual coherence within localized temporal perspective comprehensive experiment demonstrate reasonableness challenge datasets also indicating superiority proposed method compared stateoftheart deepfake detection approach
densepanet improved generative adversarial network photoacoustic tomography image reconstruction sparse data image reconstruction essential step every medical imaging method including photoacoustic tomography pat promising modality imaging unites benefit ultrasound optical imaging method reconstruction pat image using conventional method result rough artifact especially applied directly sparse pat data recent year generative adversarial network gans shown powerful performance image generation well translation rendering smart choice applied reconstruction task study proposed endtoend method called densepanet solve problem pat image reconstruction sparse data proposed model employ novel modification unet generator called fdunet considerably improves reconstruction performance evaluated method various invivo simulated datasets quantitative qualitative result show better performance model prevalent deep learning technique
exploring variational autoencoders medical image generation comprehensive study variational autoencoder vae one common technique field medical image generation architecture shown advanced researcher recent year developed various architecture vae advantage including improving datasets adding sample smaller datasets datasets imbalanced class data augmentation work paper provides comprehensive review study vae medical imaging special focus ability create synthetic image close real data used data augmentation study review important architecture method used develop vaes medical image provides comparison generative model gans issue image quality low diversity generated sample discus recent development application several medical field highlighting ability vaes improve segmentation classification accuracy
aasgan adversarially augmented social gan synthetic data accurately predicting pedestrian trajectory crucial application autonomous driving service robotics name deep generative model achieve top performance task assuming enough labelled trajectory available training end large amount synthetically generated labelled trajectory exist eg generated video game however trajectory meant represent pedestrian motion realistically ineffective training predictive model propose method architecture augment synthetic trajectory training time adversarial approach show trajectory augmentation training time unleashes significant gain stateoftheart generative model evaluated realworld trajectory
metadiffub contextualized sequencetosequence text diffusion model metaexploration diffusion model new generative modeling paradigm achieved significant success generating image audio video text adapted sequencetosequence text generation diffuseq termed diffusion existing model predominantly rely fixed handcrafted rule schedule noise diffusion denoising process however model limited noncontextualized noise fails fully consider characteristic task paper propose metadiffub framework novel schedulerexploiter paradigm designed overcome limitation existing model employ metaexploration train additional scheduler model dedicated scheduling contextualized noise sentence exploiter model model leverage noise scheduled scheduler model updating generation metadiffub achieves stateoftheart performance compared previous model finetuned pretrained language model plms across four benchmark datasets investigate visualize impact metadiffubs noise scheduling generation sentence varying difficulty additionally scheduler model function plugandplay model enhance diffuseq without need finetuning inference stage
physcene physically interactable scene synthesis embodied ai recent development embodied artificial intelligence eai research growing demand highquality largescale interactive scene generation prior method scene synthesis prioritized naturalness realism generated scene physical plausibility interactivity scene largely left unexplored address disparity introduce physcene novel method dedicated generating interactive scene characterized realistic layout articulated object rich physical interactivity tailored embodied agent based conditional diffusion model capturing scene layout devise novel physic interactivitybased guidance mechanism integrate constraint object collision room layout object reachability extensive experiment demonstrate physcene effectively leverage guidance function physically interactable scene synthesis outperforming existing stateoftheart scene synthesis method large margin finding suggest scene generated physcene hold considerable potential facilitating diverse skill acquisition among agent within interactive environment thereby catalyzing advancement embodied ai research project website httpphyscenegithubio
physicsinspired generative model medical imaging review physicsinspired generative model gm particular diffusion model dm poisson flow model pfms enhance bayesian method promise great utility medical imaging review examines transformative role generative method first variety physicsinspired gm including denoising diffusion probabilistic model ddpms scorebased diffusion model sdms poisson flow generative model pfgms pfgm revisited emphasis accuracy robustness well acceleration major application physicsinspired gm medical imaging presented comprising image reconstruction image generation image analysis finally future research direction brainstormed including unification physicsinspired gm integration visionlanguage model vlms potential novel application gm since development generative method rapid review hopefully give peer learner timely snapshot new family physicsdriven generative model help capitalize enormous potential medical imaging
diffusion meet option hierarchical generative skill composition temporallyextended task safe successful deployment robot requires ability generate complex plan also capacity frequently replan correct execution error paper address challenge longhorizon trajectory planning temporally extended objective receding horizon manner end propose doppler datadriven hierarchical framework generates update plan based instruction specified linear temporal logic ltl method decomposes temporal task chain option hierarchical reinforcement learning offline nonexpert datasets leverage diffusion model generate option lowlevel action devise determinantalguided posterior sampling technique batch generation improves speed diversity diffusion generated option leading efficient querying experiment robot navigation manipulation task demonstrate doppler generate sequence trajectory progressively satisfy specified formula obstacle avoidance sequential visitation demonstration video available online httpsphiliptheothergithubiodoppler
meddiffusion medical diffusion model controllable highquality medical image generation generation medical image present significant challenge due highresolution threedimensional nature existing method often yield suboptimal performance generating highquality medical image currently universal generative framework medical imaging paper introduce medical diffusion meddiffusion model controllable highquality medical image generation meddiffusion incorporates novel highly efficient patchvolume autoencoder compress medical image latent space patchwise encoding recovers back image space volumewise decoding additionally design new noise estimator capture local detail global structure information diffusion denoising process meddiffusion generate finedetailed highresolution image effectively adapt various downstream task trained largescale datasets covering ct mri modality different anatomical region head leg experimental result demonstrate meddiffusion surpasses stateoftheart method generative quality exhibit strong generalizability across task sparseview ct reconstruction fast mri reconstruction data augmentation
ganha generative adversarial network novel heterogeneous dualdiscriminator network new attentionbased fusion strategy infrared visible image fusion infrared visible image fusion ivif aim preserve thermal radiation information infrared image integrating texture detail visible image thermal radiation information mainly expressed image intensity texture detail typically expressed image gradient however existing dualdiscriminator generative adversarial network gans often rely two structurally identical discriminator learning fully account distinct learning need infrared visible image information end paper proposes novel gan heterogeneous dualdiscriminator network attentionbased fusion strategy ganha specifically recognizing intrinsic difference infrared visible image propose first time novel heterogeneous dualdiscriminator network simultaneously capture thermal radiation information texture detail two discriminator network structurally different including salient discriminator infrared image detailed discriminator visible image able learn rich image intensity information image gradient information respectively addition new attentionbased fusion strategy designed generator appropriately emphasize learned information different source image thereby improving information representation ability fusion result way fused image generated ganha effectively maintain salience thermal target sharpness texture extensive experiment various public datasets demonstrate superiority ganha stateoftheart sota algorithm showcasing higher potential practical application
tfsnerf templatefree nerf semantic reconstruction dynamic scene despite advancement neural implicit model surface reconstruction handling dynamic environment interaction arbitrary rigid nonrigid deformable entity remains challenging generic reconstruction method adaptable dynamic scene often require additional input like depth optical flow rely pretrained image feature reasonable outcome method typically use latent code capture framebyframe deformation another set dynamic scene reconstruction method entityspecific mostly focusing human relies template model contrast templatefree method bypass requirement adopt traditional lb linear blend skinning weight detailed representation deformable object motion although involve complex optimization leading lengthy training time end remedy paper introduces tfsnerf templatefree semantic nerf dynamic scene captured sparse singleview rgb video featuring interaction among two entity timeefficient lbsbased approach framework us invertible neural network inn lb prediction simplifying training process disentangling motion interacting entity optimizing perentity skinning weight method efficiently generates accurate semantically separable geometry extensive experiment demonstrate approach produce highquality reconstruction deformable nondeformable object complex interaction improved training efficiency compared existing method
dancefusion spatiotemporal skeleton diffusion transformer audiodriven dance motion reconstruction paper introduces dancefusion novel framework reconstructing generating dance movement synchronized music utilizing spatiotemporal skeleton diffusion transformer framework adeptly handle incomplete noisy skeletal data common shortform dance video social medium platform like tiktok dancefusion incorporates hierarchical transformerbased variational autoencoder vae integrated diffusion model significantly enhancing motion realism accuracy approach introduces sophisticated masking technique unique iterative diffusion process refines motion sequence ensuring high fidelity motion generation synchronization accompanying audio cue comprehensive evaluation demonstrate dancefusion surpasses existing method providing stateoftheart performance generating dynamic realistic stylistically diverse dance motion potential application framework extend content creation virtual reality interactive entertainment promising substantial advancement automated dance generation visit project page httpsthmlabgithubiodancefusion
deepfake detection impact limited computing capability rapid development technology artificial intelligence make deepfakes increasingly sophisticated challengingtoidentify technique ensure accuracy information control misinformation mass manipulation paramount importance discover develop artificial intelligence model enable generic detection forged video work aim address detection deepfakes across various existing datasets scenario limited computing resource goal analyze applicability different deep learning technique restriction explore possible approach enhance efficiency
singular value decomposition thirdorder reduced biquaternion tensor paper introduce application thirdorder reduced biquaternion tensor color video processing first develop algorithm computing singular value decomposition svd thirdorder reduced biquaternion tensor via new htproduct theoretical application define moorepenrose inverse thirdorder reduced biquaternion tensor develop characterization addition discus general hermitian solution reduced biquaternion tensor equation mathcalxmathcalb well leastsquare solution finally compress color video svd experimental data show method faster compared scheme
deepspeak dataset describe largescale datasetdeepspeakof real deepfake footage people talking gesturing front webcam real video first version dataset consist hour footage diverse individual constituting hour footage fake video consist range different stateoftheart faceswap lipsync deepfakes natural aigenerated voice expect release future version dataset different updated deepfake technology dataset made freely available research noncommercial us request commercial use considered
nimbled enhancing selfsupervised monocular depth estimation pseudolabels largescale video pretraining introduce nimbled efficient selfsupervised monocular depth estimation learning framework incorporates supervision pseudolabels generated large vision model framework require camera intrinsics enabling largescale pretraining publicly available video straightforward yet effective learning strategy significantly enhances performance fast lightweight model without introducing overhead allowing achieve performance comparable stateoftheart selfsupervised monocular depth estimation model advancement particularly beneficial virtual augmented reality application requiring low latency inference source code model weight acknowledgment available httpsgithubcomxapaxcanimbled
full transformerbased framework automatic pain estimation using video automatic estimation pain essential designing optimal pain management system offering reliable assessment reducing suffering patient study present novel full transformerbased framework consisting transformer transformer tnt model transformer leveraging crossattention selfattention block elaborating video biovid database demonstrate stateoftheart performance showing efficacy efficiency generalization capability across primary pain estimation task
computational complexity game boy game analyze computational complexity several popular video game released nintendo game boy video game console analyze complexity generalized version four popular game boy game donkey kong wario land harvest moon gb mole mania provide original proof showing game textbfnphard proof rely karp reduction four karps original textbfnpcomplete problem textscsat textschamiltonian cycle textscknapsack also discus proof easily derived known result demonstrating textbfnphardness lock n chase lion king
renoise real image inversion iterative noising recent advancement textguided diffusion model unlocked powerful image manipulation capability however applying method real image necessitates inversion image domain pretrained diffusion model achieving faithful inversion remains challenge particularly recent model trained generate image small number denoising step work introduce inversion method high qualitytooperation ratio enhancing reconstruction accuracy without increasing number operation building reversing diffusion sampling process method employ iterative renoising mechanism inversion sampling step mechanism refines approximation predicted point along forward diffusion trajectory iteratively applying pretrained diffusion model averaging prediction evaluate performance renoise technique using various sampling algorithm model including recent accelerated diffusion model comprehensive evaluation comparison show effectiveness term accuracy speed furthermore confirm method preserve editability demonstrating textdriven image editing real image
constrained diffusion implicit model paper describes efficient algorithm solving noisy linear inverse problem using pretrained diffusion model extending paradigm denoising diffusion implicit model ddim propose constrained diffusion implicit model cdim modify diffusion update enforce constraint upon final output noiseless inverse problem cdim exactly satisfies constraint noisy case generalize cdim satisfy exact constraint residual distribution noise experiment across variety task metric show strong performance cdim analogous inference acceleration unconstrained ddim time faster previous conditional diffusion method demonstrate versatility approach many problem including superresolution denoising inpainting deblurring point cloud reconstruction
forum fastforward caching diffusion transformer acceleration diffusion transformer dit become de facto choice generating highquality image video largely due scalability enables construction larger model enhanced performance however increased size model lead higher inference cost making less attractive realtime application present fastforward caching forum simple yet effective approach designed accelerate dit exploiting repetitive nature diffusion process forum implement caching mechanism store reuses intermediate output attention mlp layer across denoising step thereby reducing computational overhead approach require model retraining seamlessly integrates existing transformerbased diffusion model experiment show forum speed diffusion transformer several time minimally affecting performance metric score fid enabling faster processing minimal tradeoff quality forum represents significant advancement deploying diffusion transformer realtime application code made publicly available httpsgithubcomprathebaselvafora
igcfat improved ganbased framework effectively exploiting transformer realworld image superresolution field single image superresolution sisr transformerbased model demonstrated significant advancement however potential efficiency model applied field realworld image superresolution less noticed substantial opportunity improvement recently composite fusion attention transformer cfat outperformed previous stateoftheart sota model classic image superresolution paper propose novel ganbased framework incorporating cfat model effectively exploit performance transformer realworld image superresolution proposed approach integrate semanticaware discriminator reconstruct fine detail accurately employ adaptive degradation model better simulate realworld degradation moreover introduce new combination loss function adding wavelet loss loss function ganbased model better recover highfrequency detail empirical result demonstrate igcfat significantly outperforms existing sota model quantitative qualitative metric proposed model revolutionizes field realworld image superresolution demonstrates substantially better performance recovering fine detail generating realistic texture introduction igcfat offer robust adaptable solution realworld image superresolution task
latentsync taming audioconditioned latent diffusion model lip sync syncnet supervision endtoend audioconditioned latent diffusion model ldms widely adopted audiodriven portrait animation demonstrating effectiveness generating lifelike highresolution talking video however direct application audioconditioned ldms lipsynchronization lipsync task result suboptimal lipsync accuracy indepth analysis identified underlying cause shortcut learning problem wherein model predominantly learns visualvisual shortcut neglecting critical audiovisual correlation address issue explored different approach integrating syncnet supervision audioconditioned ldms explicitly enforce learning audiovisual correlation since performance syncnet directly influence lipsync accuracy supervised model training wellconverged syncnet becomes crucial conducted first comprehensive empirical study identify key factor affecting syncnet convergence based analysis introduce stablesyncnet architecture designed stable convergence stablesyncnet achieved significant improvement accuracy increasing hdtf test set additionally introduce novel temporal representation alignment trepa mechanism enhance temporal consistency generated video experimental result show method surpasses stateoftheart lipsync approach across various evaluation metric hdtf datasets
diffusion forcing nexttoken prediction meet fullsequence diffusion paper present diffusion forcing new training paradigm diffusion model trained denoise set token independent pertoken noise level apply diffusion forcing sequence generative modeling training causal nexttoken prediction model generate one several future token without fully diffusing past one approach shown combine strength nexttoken prediction model variablelength generation strength fullsequence diffusion model ability guide sampling desirable trajectory method offer range additional capability rollingout sequence continuous token video length past training horizon baseline diverge new sampling guiding scheme uniquely profit diffusion forcings variablehorizon causal architecture lead marked performance gain decisionmaking planning task addition empirical success method proven optimize variational lower bound likelihood subsequence token drawn true joint distribution project website httpsboyuanspacediffusionforcing
solution authenticity identification typical target remote sensing image paper propose basic rgb singlemode model based weakly supervised training pseudo label performs highprecision authenticity identification multiscene typical target remote sensing image due imprecision mask generation divide task two subtasks generating pseudomask finetuning model based generated mask generating pseudo mask use mmfusion base model generate mask large object plane ship manually calibrating mask small object car highly accurate pseudomask obtained task finetuning model based generating mask use wscl model base model worth noting due difference generated pseudomasks real mask discard image feature extractor srm noiseprint wscl select unscaled original image training alone greatly ensures match image original label final trained model achieved score test set
diffpcc diffusionbased neural compression point cloud stable diffusion network emerged groundbreaking development ability produce realistic detailed visual content characteristic render ideal decoder capable producing highquality aesthetically pleasing reconstruction paper introduce first diffusionbased point cloud compression method dubbed diffpcc leverage expressive power diffusion model generative aesthetically superior decoding different conventional autoencoder fashion dualspace latent representation devised paper compressor composed two independent encoding backbone considered extract expressive shape latents distinct latent space decoding side diffusionbased generator devised produce highquality reconstruction considering shape latents guidance stochastically denoise noisy point cloud experiment demonstrate proposed diffpcc achieves stateoftheart compression performance eg db bdpsnr gain latest gpcc standard ultralow bitrate attaining superior subjective quality source code made publicly available
heuristically adaptive diffusionmodel evolutionary strategy diffusion model represent significant advancement generative modeling employing dualphase process first degrades domainspecific information via gaussian noise restores trainable model framework enables pure noisetodata generation modular reconstruction image video concurrently evolutionary algorithm employ optimization method inspired biological principle refine set numerical parameter encoding potential solution rugged objective function research reveals fundamental connection diffusion model evolutionary algorithm shared underlying generative mechanism method generate highquality sample via iterative refinement random initial distribution employing deep learningbased diffusion model generative model across diverse evolutionary task iteratively refining diffusion model heuristically acquired database iteratively sample potentially betteradapted offspring parameter integrating successive generation diffusion model approach achieves efficient convergence toward highfitness parameter maintaining explorative diversity diffusion model introduce enhanced memory capability evolutionary algorithm retaining historical information across generation leveraging subtle data correlation generate refined sample elevate evolutionary algorithm procedure shallow heuristic framework deep memory deploying classifierfree guidance conditional sampling parameter level achieve precise control evolutionary search dynamic specific genotypical phenotypical populationwide trait framework mark major heuristic algorithmic transition offering increased flexibility precision control evolutionary optimization process
longitudinal causal image synthesis clinical decisionmaking relies heavily causal reasoning longitudinal analysis example patient alzheimers disease ad brain grey matter atrophy year intervened abeta level cerebrospinal fluid answer fundamental diagnosis followup treatment however kind inquiry involves counterfactual medical image acquired instrumental correlationbased image synthesis model yet query require counterfactual medical image obtainable standard image synthesis model hence causal longitudinal image synthesis cli method enabling synthesis image highly valuable however building cli model confronts three primary yet unmet challenge mismatched dimensionality highdimensional image lowdimensional tabular variable inconsistent collection interval followup data inadequate causal modeling capability existing causal graph method image data paper established tabularvisual causal graph tvcg cli overcoming challenge novel integration generative imaging continuoustime modeling structural causal model combined neural network train cli based adni dataset evaluate two ad datasets illustrate outstanding yet controllable quality synthesized image contribution synthesized mri characterization ad progression substantiating reliability utility clinic
progressive boundary guided anomaly synthesis industrial anomaly detection unsupervised anomaly detection method identify surface defect industrial image leveraging normal sample training due risk overfitting learning single class anomaly synthesis strategy introduced enhance detection capability generating artificial anomaly however existing strategy heavily rely anomalous texture auxiliary datasets moreover limitation coverage directionality anomaly synthesis may result failure capture useful information lead significant redundancy address issue propose novel progressive boundaryguided anomaly synthesis pbas strategy directionally synthesize crucial featurelevel anomaly without auxiliary texture consists three core component approximate boundary learning abl anomaly feature synthesis afs refined boundary optimization rbo make distribution normal sample compact abl first learns approximate decision boundary center constraint improves center initialization feature alignment afs directionally synthesizes anomaly flexible scale guided hypersphere distribution normal feature since boundary loose may contain real anomaly rbo refines decision boundary binary classification artificial anomaly normal feature experimental result show method achieves stateoftheart performance fastest detection speed three widely used industrial datasets including mvtec ad visa mpdd code available httpsgithubcomcqylunlunpbas
generating realistic xray scattering image using stable diffusion humanintheloop annotation finetuned foundational stable diffusion model using xray scattering image corresponding description generate new scientific image given prompt however generated image exhibit significant unrealistic artifact commonly known hallucination address issue trained various computer vision model dataset composed humanapproved generated image experimental image detect unrealistic image classified image reviewed corrected human expert subsequently used refine classifier next round training inference evaluation demonstrate feasibility generating highfidelity domainspecific image using finetuned diffusion model anticipate generative ai play crucial role enhancing data augmentation driving development digital twin scientific research facility
bridging gap learning inference diffusionbased molecule generation efficacy diffusion model generating spectrum data modality including image text video spurred inquiry utility molecular generation yielding significant advancement field however molecular generation process diffusion model involves multiple autoregressive step finite time horizon leading exposure bias issue inherently address exposure bias issue propose training framework named gapdiff core idea gapdiff utilize modelpredicted conformation ground truth probabilistically training aiming mitigate data distributional disparity training inference thereby enhancing affinity generated molecule conduct experiment using molecular generation model dataset vina energy diversity demonstrate potency framework superior affinity gapdiff available urlhttpsgithubcomhughnewgapdiff
explaining implicit neural canvas connecting pixel neuron tracing contribution many variation implicit neural representation inr neural network trained continuous representation signal tremendous practical utility downstream task including novel view synthesis video compression image superresolution unfortunately inner working network seriously understudied work explaining implicit neural canvas xinc unified framework explaining property inr examining strength neuron contribution output pixel call aggregate contribution map implicit neural canvas use concept demonstrate inr study learn see frame represent surprising way example inr tend highly distributed representation lacking highlevel object semantics significant bias color edge almost entirely spaceagnostic arrive conclusion examining object represented across time video inr using clustering visualize similar neuron across layer architecture show dominated motion insight demonstrate general usefulness analysis framework project page available
understanding generalizability diffusion model requires rethinking hidden gaussian structure work study generalizability diffusion model looking hidden property learned score function essentially series deep denoisers trained various noise level observe diffusion model transition memorization generalization corresponding nonlinear diffusion denoisers exhibit increasing linearity discovery lead u investigate linear counterpart nonlinear diffusion model series linear model trained match function mapping nonlinear diffusion denoisers surprisingly linear denoisers approximately optimal denoisers multivariate gaussian distribution characterized empirical mean covariance training dataset finding implies diffusion model inductive bias towards capturing utilizing gaussian structure covariance information training dataset data generation empirically demonstrate inductive bias unique property diffusion model generalization regime becomes increasingly evident model capacity relatively small compared training dataset size case model highly overparameterized inductive bias emerges initial training phase model fully memorizes training data study provides crucial insight understanding notable strong generalization phenomenon recently observed realworld diffusion model
acdit interpolating autoregressive conditional modeling diffusion transformer present acdit novel autoregressive blockwise conditional diffusion transformer innovatively combine autoregressive diffusion paradigm modeling continuous visual information introducing blockwise autoregressive unit acdit offer flexible interpolation tokenwise autoregression fullsequence diffusion bypassing limitation discrete tokenization generation block formulated conditional diffusion process conditioned prior block acdit easy implement simple creating skipcausal attention mask scam standard diffusion transformer training inference process iterates diffusion denoising autoregressive decoding make full use kvcache show acdit performs best among autoregressive baseline similar model scale image video generation task also demonstrate benefiting autoregressive modeling pretrained acdit transferred visual understanding task despite trained diffusion objective analysis tradeoff autoregressive modeling diffusion demonstrates potential acdit used longhorizon visual generation task hope acdit offer novel perspective visual autoregressive generation unlocks new avenue unified model
pixel cancer cellular automaton computed tomography ai cancer detection encounter bottleneck data scarcity annotation difficulty low prevalence early tumor tumor synthesis seek create artificial tumor medical image greatly diversify data annotation ai training however current tumor synthesis approach applicable across different organ due need specific expertise design paper establishes set generic rule simulate tumor development cell pixel initially assigned state zero ten represent tumor population tumor developed based three rule describe process growth invasion death apply three generic rule simulate tumor developmentfrom pixel cancerusing cellular automaton integrate tumor state original computed tomography ct image generate synthetic tumor across different organ tumor synthesis approach allows sampling tumor multiple stage analyzing tumororgan interaction clinically reader study involving three expert radiologist reveals synthetic tumor developing trajectory convincingly realistic technically analyze simulate tumor development various stage using raw unlabeled ct image sourced hospital worldwide performance segmenting tumor liver pancreas kidney exceeds prevailing literature benchmark underlining immense potential tumor synthesis especially earlier cancer detection code model available
hsigene foundation model hyperspectral image generation hyperspectral image hsi play vital role various field agriculture environmental monitoring however due expensive acquisition cost number hyperspectral image limited degenerating performance downstream task although recent study attempted employ diffusion model synthesize hsis still struggle scarcity hsis affecting reliability diversity generated image study propose incorporate multimodal data enhance spatial diversity spectral fidelity ensured addition existing hsi synthesis model typically uncontrollable support singlecondition control limiting ability generate accurate reliable hsis alleviate issue propose hsigene novel hsi generation foundation model based latent diffusion support multicondition control allowing precise reliable hsi generation enhance spatial diversity training data preserving spectral fidelity propose new data augmentation method based spatial superresolution hsis upscaled first thus abundant training patch could obtained cropping highresolution hsis addition improve perceptual quality augmented data introduce novel twostage hsi superresolution framework first applies rgb band superresolution utilizes proposed rectangular guided attention network rgan guided hsi superresolution experiment demonstrate proposed model capable generating vast quantity realistic hsis downstream task denoising superresolution code model available httpsgithubcomlipanghsigene
contextguided spatiotemporal video grounding spatiotemporal video grounding stvg task aim locating spatiotemporal tube specific instance given text query despite advancement current method easily suffer distractors heavy object appearance variation video due insufficient object information text leading degradation addressing propose novel framework contextguided stvg cgstvg mine discriminative instance context object video applies supplementary guidance target localization key cgstvg lie two specially designed module including instance context generation icg focus discovering visual context information appearance motion instance instance context refinement icr aim improve instance context icg eliminating irrelevant even harmful information context grounding icg together icr deployed decoding stage transformer architecture instance context learning particularly instance context learned one decoding stage fed next stage leveraged guidance containing rich discriminative object feature enhance targetawareness decoding feature conversely benefit generating better new instance context improving localization finally compared existing method cgstvg enjoys object information text query guidance mined instance visual context accurate target localization experiment three benchmark including vidstg cgstvg set new stateofthearts showing efficacy code released httpsgithubcomhenglancgstvg
image video reshaping receptive field via imagetovideo differentiable autoaugmentation fusion landscape deep learning research moving towards innovative strategy harness true potential data traditionally emphasis scaling model architecture resulting large complex neural network difficult train limited computational resource however independently model size data quality ie amount variability still major factor affect model generalization work propose novel technique exploit available data use automatic data augmentation task image classification semantic segmentation introduce first differentiable augmentation search method da generate variation image processed video compared previous approach da extremely fast flexible allowing search large search space less gpu day intuition increased receptive field temporal dimension provided da could lead benefit also spatial receptive field specifically leverage da guide reshaping spatial receptive field selecting taskdependant transformation result compared standard augmentation alternative improve term accuracy imagenet tinyimagenet cityscape datasets pluggingin da different lightweight video backbone
passive screentocamera communication recent technology known transparent screen transforming window display smart window present bus airport office remain transparent normal window display relevant information overlay panoramic view paper propose transforming window screen also wireless transmitter achieve goal build upon research area screentocamera communication area video modified way smartphone camera decode data data remains invisible viewer person see normal video camera see video plus additional information communication method one biggest disadvantage traditional screen power consumption used generate light solve employ novel transparent screen relying ambient light display picture hence eliminating power source however come cost lower image quality since use variable outofcontrol environment light instead generating constant strong light led panel work dubbed passivecam overcomes challenge creating first screentocamera communication link using passive display paper present two main contribution first analyze modify existing screen encoding method embed information reliably ambient light second develop android app optimizes decoding process obtaining realtime performance evaluation considers musical application show packet success rate psr close addition realtime application achieves response time m m camera static handheld respectively
trafficvlm controllable visual language model traffic video captioning traffic video description analysis received much attention recently due growing demand efficient reliable urban surveillance system existing method focus locating traffic event segment severely lack descriptive detail related behaviour context subject interest event paper present trafficvlm novel multimodal dense video captioning model vehicle ego camera view trafficvlm model traffic video event different level analysis spatially temporally generates long finegrained description vehicle pedestrian different phase event also propose conditional component trafficvlm control generation output multitask finetuning paradigm enhance trafficvlms learning capability experiment show trafficvlm performs well vehicle overhead camera view solution achieved outstanding result track ai city challenge ranking u third challenge standing code publicly available httpsgithubcomquangminhdinhtrafficvlm
ladder efficient framework video frame interpolation video frame interpolation vfi crucial technique various application slowmotion generation frame rate conversion video frame restoration etc paper introduces efficient video frame interpolation framework aim strike favorable balance efficiency quality framework follows general paradigm consisting flow estimator refinement module incorporating carefully designed component first adopt depthwise convolution large kernel flow estimator simultaneously reduces parameter enhances receptive field encoding rich context handling complex motion secondly diverging common design refinement module unetstructure encoderdecoder structure find redundant decoderonly refinement module directly enhances result coarse fine feature offering efficient process addition address challenge handling highdefinition frame also introduce innovative hdaware augmentation strategy training leading consistent enhancement hd image extensive experiment conducted diverse datasets xiph snufilm result demonstrate approach achieves stateoftheart performance clear improvement requiring much less flop parameter reaching better spot balancing efficiency quality
multiframe fusion video stabilization paper present rstab novel framework video stabilization integrates multiframe fusion volume rendering departing conventional method introduce multiframe perspective generate stabilized image addressing challenge fullframe generation preserving structure core approach lie stabilized rendering sr volume rendering module extends beyond image fusion incorporating feature fusion core rstab framework lie stabilized rendering sr volume rendering module fusing multiframe information space specifically sr involves warping feature color multiple frame projection fusing descriptor render stabilized image however precision warped information depends projection accuracy factor significantly influenced dynamic region response introduce adaptive ray range arr module integrate depth prior adaptively defining sampling range projection process additionally propose color correction cc assisting geometric constraint optical flow accurate color aggregation thanks three module rstab demonstrates superior performance compared previous stabilizer field view fov image quality video stability across various datasets
uncertaintyboosted robust video activity anticipation video activity anticipation aim predict happen future embracing broad application prospect ranging robot vision autonomous driving despite recent progress data uncertainty issue reflected content evolution process dynamic correlation event label somehow ignored reduces model generalization ability deep understanding video content leading serious error accumulation degraded performance paper address uncertainty learning problem propose uncertaintyboosted robust video activity anticipation framework generates uncertainty value indicate credibility anticipation result uncertainty value used derive temperature parameter softmax function modulate predicted target activity distribution guarantee distribution adjustment construct reasonable target activity label representation incorporating activity evolution temporal class correlation semantic relationship moreover quantify uncertainty relative value comparing uncertainty among sample pair temporallengths relative strategy provides accessible way uncertainty modeling quantifying absolute uncertainty value whole dataset experiment multiple backbone benchmark show framework achieves promising performance better robustnessinterpretability source code available
behavior imitation manipulator control grasping deep reinforcement learning existing motion imitation model typically require expert data obtained mocap device vast amount training data needed difficult acquire necessitating substantial investment financial resource manpower time project combine human pose estimation reinforcement learning proposing novel model simplifies motion imitation prediction problem joint angle value reinforcement learning significantly reduces reliance vast amount training data enabling agent learn imitation policy second video exhibit strong generalization capability quickly apply learned policy imitate human arm motion unfamiliar video model first extract skeletal motion human arm given video using human pose estimation extracted arm motion morphologically retargeted onto robotic manipulator subsequently retargeted motion used generate reference motion finally reference motion used formulate reinforcement learning problem enabling agent learn policy imitating human arm motion project excels imitation task demonstrates robust transferability accurately imitating human arm motion unfamiliar video project provides lightweight convenient efficient accurate motion imitation model simplifying complex process motion imitation achieves notably outstanding performance
supergaussian repurposing video model super resolution present simple modular generic method upsamples coarse model adding geometric appearance detail generative model exist yet match quality counterpart image video domain demonstrate possible directly repurpose existing pretrained video model superresolution thus sidestep problem shortage large repository highquality training model describe repurpose video upsampling model consistent combine consolidation produce result output produce high quality gaussian splat model object centric effective method category agnostic easily incorporated existing workflow evaluate proposed supergaussian variety input diverse term complexity representation eg gaussian splat nerfs demonstrate simple method significantly improves fidelity final model check project website detail supergaussiangithubio
advancing compressed video action recognition progressive knowledge distillation compressed video action recognition classifies video sample leveraging different modality compressed video namely motion vector residual intraframes purpose three neural network deployed dedicated processing one modality observation indicate network processing intraframes tend converge flatter minimum network processing residual turn converges flatter minimum motion vector network hierarchy convergence motivates strategy knowledge transfer among modality achieve flatter minimum generally associated better generalization insight propose progressive knowledge distillation pkd technique incrementally transfer knowledge across modality method involves attaching early exit internal classifier ic three network pkd distills knowledge starting motion vector network followed residual finally intraframe network sequentially improving ic accuracy propose weighted inference scaled ensemble wise combine output ic using learned weight boosting accuracy inference experiment demonstrate effectiveness training ic pkd compared standard crossentropybased training showing ic accuracy improvement datasets respectively additionally wise improves accuracy respectively
bootstrapping visionlanguage model selfsupervised remote physiological measurement facial videobased remote physiological measurement promising research area detecting human vital sign eg heart rate respiration frequency noncontact way conventional approach mostly supervised learning requiring extensive collection facial video synchronously recorded photoplethysmography ppg signal tackle selfsupervised learning recently gained attention due lack ground truth ppg signal performance however limited paper propose novel selfsupervised framework successfully integrates popular visionlanguage model vlms remote physiological measurement task given facial video first augment positive negative video sample varying rppg signal frequency next introduce frequencyoriented visiontext pair generation method carefully creating contrastive spatiotemporal map positive negative sample designing proper text prompt describe relative ratio signal frequency pretrained vlm employed extract feature formed visiontext pair estimate rppg signal thereafter develop series generative contrastive learning mechanism optimize vlm including textguided visual map reconstruction task visiontext contrastive learning task frequency contrastive ranking task overall method first time adapts vlms digest align frequencyrelated knowledge vision text modality extensive experiment four benchmark datasets demonstrate significantly outperforms state art selfsupervised method
personal action recommendation suggestion egocentric video intelligent assistance involves understanding also action existing egocentric video datasets contain rich annotation video action intelligent assistant could perform moment address gap release new set personal action recommendation annotation dataset take multistage approach generating evaluating annotation first used promptengineered large language model llm generate contextaware action suggestion identified action suggestion synthetic action suggestion valuable inherent limitation llm necessitate human evaluation ensure highquality usercentered recommendation conducted largescale human annotation study provides grounding human preference analyze interrater agreement evaluate subjective preference participant based synthetic dataset complete human annotation propose several new task action suggestion based egocentric video encourage novel solution improve latency energy requirement annotation support researcher developer working building action recommendation system augmented virtual reality system
endtoend openvocabulary video visual relationship detection using multimodal prompting openvocabulary video visual relationship detection aim expand video visual relationship detection beyond annotated category detecting unseen relationship seen unseen object video existing method usually use trajectory detector trained closed datasets detect object trajectory feed trajectory largescale pretrained visionlanguage model achieve openvocabulary classification heavy dependence pretrained trajectory detector limit ability generalize novel object category leading performance degradation address challenge propose unify object trajectory detection relationship classification endtoend openvocabulary framework framework propose relationshipaware openvocabulary trajectory detector primarily consists querybased transformer decoder visual encoder clip distilled framewise openvocabulary object detection trajectory associator exploit relationship context trajectory detection relationship query embedded transformer decoder accordingly auxiliary relationship loss designed enable decoder perceive relationship object explicitly moreover propose openvocabulary relationship classifier leverage rich semantic knowledge clip discover novel relationship adapt clip well relationship classification design multimodal prompting method employ spatiotemporal visual prompting visual representation visionguided language prompting language input extensive experiment two public datasets vidvrd vidor demonstrate effectiveness framework framework also applied difficult crossdataset scenario demonstrate generalization ability
skill made order efficient acquisition robot cooking skill guided multiple form internet data study explores utility various internet data source select among set template robot behavior perform skill learning contactrich skill involving tool use internet data source typically challenging due lack physical information contact existence location area force data prior work generally used internet data foundation model trained data generate lowlevel robot behavior hypothesize data model may better suited selecting among set basic robot behavior perform contactrich skill explore three method template selection querying large language model comparing video robot execution retrieved human video using feature pretrained video encoder common prior work performing comparison using feature optic flow encoder trained internet data result show llm surprisingly capable template selector despite lack visual information optical flow encoding significantly outperforms video encoders trained order magnitude data important synergy exist various form internet data template selection exploiting synergy create template selector using multiple form internet data achieves success rate set different cooking skill involving tooluse
rexplain translating radiology patientfriendly video report radiology report designed efficient communication medical expert often remain incomprehensible patient inaccessibility could potentially lead anxiety decreased engagement treatment decision poorer health outcome undermining patientcentered care present rexplain radiology explanation innovative aidriven system translates radiology finding patientfriendly video report rexplain uniquely integrates large language model medical text simplification textanatomy association image segmentation model anatomical region identification avatar generation tool engaging interface visualization rexplain enables producing comprehensive explanation plain language highlighted imagery organ rendering form video report evaluate utility rexplaingenerated explanation conducted two round user feedback collection six boardcertified radiologist result proofofconcept study indicate rexplain could accurately deliver radiological information effectively simulate oneonone consultation shedding light enhancing patientcentered radiology potential clinical usage work demonstrates new paradigm aiassisted medical communication potentially improving patient engagement satisfaction radiology care open new avenue research multimodal medical communication
efficient transfer learning videolanguage foundation model pretrained visionlanguage model provide robust foundation efficient transfer learning across various downstream task field video action recognition mainstream approach often introduce additional module capture temporal information although additional module increase capacity model enabling better capture videospecific inductive bias existing method typically introduce substantial number new parameter prone catastrophic forgetting previously acquired generalizable knowledge paper propose parameterefficient multimodal spatiotemporal adapter msta enhance alignment textual visual representation achieving balance generalizable knowledge taskspecific adaptation furthermore mitigate overfitting enhance generalizability introduce spatiotemporal descriptionguided consistency constraintthis constraint involves providing template input eg video textbfcls trainable language branch llmgenerated spatiotemporal description pretrained language branch enforcing output consistency branch approach reduces overfitting downstream task enhances distinguishability trainable branch within spatiotemporal semantic space evaluate effectiveness approach across four task zeroshot transfer fewshot learning basetonovel generalization fullysupervised learning compared many stateoftheart method msta achieves outstanding performance across evaluation using trainable parameter original model
videotext dataset construction multiai feedback promoting weaktostrong preference learning video large language model highquality videotext preference data crucial multimodal large language model mllms alignment however existing preference data scarce obtaining vqa preference data preference training costly manually annotating response highly unreliable could result lowquality pair meanwhile aigenerated response controlled temperature adjustment lack diversity address issue propose highquality vqa preference dataset called textittextbfmultiple textbfmultimodal textbfartificial textbfintelligence textbfpreference datasets textbfvqa textbfmmaipv constructed sampling response distribution set using external scoring function response evaluation furthermore fully leverage preference knowledge mmaipv ensure sufficient optimization propose textittextbfiterative textbfweaktotextbfstrong textbfreinforcement textbflearning textbfai textbffeedback video mllms framework gradually enhances mllms alignment capability iteratively updating reference model performing parameter extrapolation finally propose unbiased informationcomplete evaluation scheme vqa evaluation experiment demonstrate mmaipv beneficial mllms preference learning fully exploit alignment information mmaipv believe proposed automatic vqa preference data generation pipeline based ai feedback greatly promote future work mllms alignment textbfcode dataset available
instit boosting multimodal instance understanding via explicit visual prompt instruction tuning large multimodal model lmms made significant breakthrough advancement instruction tuning however existing model understand image video holistic level still struggle instancelevel understanding requires nuanced comprehension alignment instancelevel understanding crucial focus specific element interested excitingly existing work find stateoftheart lmms exhibit strong instance understanding capability provided explicit visual cue motivated introduce automated annotation pipeline assisted extract instancelevel information image video explicit visual prompting instance guidance building upon pipeline proposed instit solution enhance lmms instance understanding via explicit visual prompt instruction tuning instit consists benchmark diagnose multimodal instancelevel understanding largescale instructiontuning dataset continuous instructiontuning training paradigm effectively enhance spatialtemporal instance understanding capability existing lmms experimental result show boost instit model achieve outstanding performance instit bench also demonstrate significant improvement across various generic image video understanding benchmark highlight dataset boost instancelevel understanding also strengthens overall capability generic image video comprehension
shotvl humancentric highlight frame retrieval via language query existing work humancentric video understanding typically focus analyzing specific moment entire video however many application require higher precision frame level work propose novel task bestshot aim locate highlight frame within humancentric video via language query task demand deep semantic comprehension human action also precise temporal localization support task introduce bestshot benchmark benchmark meticulously constructed combining human detection tracking potential frame selection based human judgment detailed textual description crafted human input ensure precision benchmark meticulously constructed combining humanannotated highlight frame detailed textual description duration labeling description encompass three critical element visual content finegrained action human pose description together element provide necessary precision identify exact highlight frame video tackle problem collected two distinct datasets dataset algorithmically generated ii imagesmpltext dataset dataset largescale accurate perframe pose description leveraging posescript existing pose estimation datasets based datasets present strong baseline model shotvl finetuned internvl specifically bestshot highlight impressive zeroshot capability model offer comparative analysis existing sota model shotvl demonstrates significant improvement internvl bestshot benchmark notable improvement benchmark maintaining sota performance general image classification retrieval
exploiting multimodal spatialtemporal pattern video object tracking multimodal tracking garnered widespread attention result ability effectively address inherent limitation traditional rgb tracking however existing multimodal tracker mainly focus fusion enhancement spatial feature merely leverage sparse temporal relationship video frame approach fully exploit temporal correlation multimodal video making difficult capture dynamic change motion information target complex scenario alleviate problem propose unified multimodal spatialtemporal tracking approach named sttrack contrast previous paradigm solely relied updating reference information introduced temporal state generator tsg continuously generates sequence token containing multimodal temporal information temporal information token used guide localization target next time state establish longrange contextual relationship video frame capture temporal trajectory target furthermore spatial level introduced mamba fusion background suppression interactive bsi module module establish dualstage mechanism coordinating information interaction fusion modality extensive comparison five benchmark datasets illustrate sttrack achieves stateoftheart performance across various multimodal tracking scenario code available httpsgithubcomnjupcalabsttrack
wemgan wavelet transform based facial expression manipulation facial expression manipulation aim change human facial expression without affecting face recognition order transform facial expression target expression previous method relied expression label guide manipulation process however method failed preserve detail facial feature cause weakening loss identity information output image work propose wemgan short waveletbased expression manipulation gan put effort preserving detail original image editing process firstly take advantage wavelet transform technique combine generator unet autoencoder backbone order improve generator ability preserve detail facial feature secondly also implement highfrequency component discriminator use highfrequency domain adversarial loss constrain optimization model providing generated face image abundant detail additionally order narrow gap generated facial expression target expression use residual connection encoder decoder also using relative action unit au several time extensive qualitative quantitative experiment demonstrated model performs better preserving identity feature editing capability image generation quality affectnet dataset also show superior performance metric average content distance acd expression distance ed
generative ai machine learning approach robust efficient lung segmentation chest radiography climacteric identifying different pulmonary disease yet radiologist workload inefficiency lead misdiagnoses automatic accurate efficient segmentation lung xray image chest paramount early disease detection study develops deep learning framework using generative adversarial network gan segment pulmonary abnormality cxr image framework image preprocessing augmentation technique properly incorporated unetinspired generatordiscriminator architecture initially loaded cxr image manual mask montgomery shenzhen datasets preprocessing resizing performed unet generator applied processed cxr image yield segmented mask discriminator network differentiates generated real mask montgomery dataset served model training set study shenzhen dataset used test robustness used first time adversarial loss distance used optimize model training metric assess precision recall score dice coefficient prove effectiveness framework pulmonary abnormality segmentation therefore set basis future study performed shortly using diverse datasets could confirm clinical applicability medical imaging
lcbnet longcontext biasing audiovisual speech recognition growing prevalence online conference course present new challenge improving automatic speech recognition asr enriched textual information video slide contrast rare phrase list slide within video synchronized realtime speech enabling extraction long contextual bias therefore propose novel longcontext biasing network lcbnet audiovisual speech recognition avsr leverage longcontext information available video effectively specifically adopt biencoder architecture simultaneously model audio longcontext biasing besides also propose biasing prediction module utilizes binary cross entropy bce loss explicitly determine biased phrase longcontext biasing furthermore introduce dynamic contextual phrase simulation enhance generalization robustness lcbnet experiment slidespeech largescale audiovisual corpus enriched slide reveal proposed lcbnet outperforms general asr model relative weruwerbwer reduction test set enjoys high unbiased biased performance moreover also evaluate model librispeech corpus leading relative weruwerbwer reduction asr model
highquality mesh blendshape generation face video via neural inverse rendering readily editable mesh blendshapes widely used animation pipeline recent advancement neural geometry appearance representation enabled highquality inverse rendering building upon observation introduce novel technique reconstructs meshbased blendshape rig single sparse multiview video leveraging stateoftheart neural inverse rendering begin constructing deformation representation parameterizes vertex displacement differential coordinate tetrahedral connection allowing highquality vertex deformation highresolution mesh constructing set semantic regulation representation achieve joint optimization blendshapes expression coefficient furthermore enable userfriendly multiview setup unsynchronized camera propose neural regressor model timevarying motion parameter approach implicitly considers time difference across multiple camera enhancing accuracy motion modeling experiment demonstrate flexible input single sparse multiview video reconstruct personalized highfidelity blendshapes blendshapes geometrically semantically accurate compatible industrial animation pipeline code data available httpsgithubcomgrignarderhighqualityblendshapegeneration
gptsee enhancing moment retrieval highlight detection via descriptionbased similarity feature moment retrieval mr highlight detection hd aim identify relevant moment highlight video corresponding natural language query large language model llm demonstrated proficiency various computer vision task however existing method mrhd yet integrated llm letter propose novel twostage model take output llm input secondstage transformer encoderdecoder first employed generate detailed description video frame rewrite query statement fed encoder new feature semantic similarity computed generated description rewritten query finally continuous highsimilarity video frame converted span anchor serving prior position information decoder experiment demonstrate approach achieves stateoftheart result using span anchor similarity score output positioning accuracy outperforms traditional method like momentdetr
ridtwin endtoend pipeline automatic face deidentification video face deidentification video challenging task domain computer vision primarily used privacypreserving application despite considerable progress achieved generative vision model remain multiple challenge latest approach lack comprehensive discussion evaluation aspect realism temporal coherence preservation nonidentifiable feature work propose ridtwin novel pipeline leverage stateoftheart generative model decouples identity motion perform automatic face deidentification video investigate task holistic point view discus approach address pertinent existing challenge domain evaluate performance methodology widely employed dataset also custom dataset designed accommodate limitation certain behavioral variation absent dataset discus implication advantage work suggest direction future research
plottal prompt learning optimal transport fewshot temporal action localization paper introduces novel approach temporal action localization tal fewshot learning work address inherent limitation conventional singleprompt learning method often lead overfitting due inability generalize across varying context realworld video recognizing diversity camera view background object video propose multiprompt learning framework enhanced optimal transport design allows model learn set diverse prompt action capturing general characteristic effectively distributing representation mitigate risk overfitting furthermore employing optimal transport theory efficiently align prompt action feature optimizing comprehensive representation adapts multifaceted nature video data experiment demonstrate significant improvement action localization accuracy robustness fewshot setting standard challenging datasets highlighting efficacy multiprompt optimal transport approach overcoming challenge conventional fewshot tal method
unified framework humancentric point cloud video understanding humancentric point cloud video understanding pvu emerging field focused extracting interpreting humanrelated feature sequence human point cloud advancing downstream humancentric task application previous work usually focus tackling one specific task rely huge labeled data poor generalization capability considering human specific characteristic including structural semantics human body dynamic human motion propose unified framework make full use prior knowledge explore inherent feature data generalized humancentric point cloud video understanding extensive experiment demonstrate method achieves stateoftheart performance various humanrelated task including action recognition pose estimation datasets code released soon
perceptionoriented video frame interpolation via asymmetric blending previous method video frame interpolation vfi encountered challenge notably manifestation blur ghosting effect issue traced back two pivotal factor unavoidable motion error misalignment supervision practice motion estimate often prove errorprone resulting misaligned feature furthermore reconstruction loss tends bring blurry result particularly misaligned region mitigate challenge propose new paradigm called pervfi perceptionoriented video frame interpolation approach incorporates asymmetric synergistic blending module asb utilizes feature side synergistically blend intermediate feature one reference frame emphasizes primary content contributes complementary information impose stringent constraint blending process introduce selflearned sparse quasibinary mask effectively mitigates ghosting blur artifact output additionally employ normalizing flowbased generator utilize negative loglikelihood loss learn conditional distribution output facilitates generation clear fine detail experimental result validate superiority pervfi demonstrating significant improvement perceptual quality compared existing method code available urlhttpsgithubcommulnspervfi
watching popular musician learn ear hypothesisgenerating study humanrecording interaction youtube video popular musician often learn music ear unclear role technology play experience task search opportunity development novel humanrecording interaction analyze youtube video depicting realworld example byear learning discus preliminary phase research online video appropriate data observation generate hypothesis inform future work example musician scope learning may influence technological interaction would help could benefit tool accommodate working memory transcription appear play key role ear learning based finding pose number research question discus methodological consideration guide future study
using deep convolutional neural network detect rendered glitch video game paper present method using deep convolutional neural network dcnns detect common glitch video game problem setting consists image rgb input classified one five defined class normal image one four different kind glitch stretched low resolution missing placeholder texture using supervised approach train using generated data work focus detecting texture graphical anomaly achieving arguably good performance accuracy detecting glitch false positive rate model able generalize detect glitch even unseen object apply confidence measure well tackle issue false positive well effective way aggregating image achieve better detection production main use work partial automatization graphical testing final stage video game development
fullstage pseudo label quality enhancement weaklysupervised temporal action localization weaklysupervised temporal action localization wstal aim localize action untrimmed video using videolevel supervision latest wstal method introduce pseudo label learning framework bridge gap classificationbased training inferencing target localization achieve cuttingedge result framework classificationbased model used generate pseudo label regressionbased student model learn however quality pseudo label framework key factor final result carefully studied paper propose set simple yet efficient pseudo label quality enhancement mechanism build fustal framework fustal enhances pseudo label quality three stage crossvideo contrastive learning proposal generationstage priorbased filtering proposal selectionstage emabased distillation trainingstage design enhance pseudo label quality different stage framework help produce informative less false smoother action proposal help comprehensive design stage fustal achieves average map outperforming previous best method becomes first method reach milestone
arbitraryscale video superresolution structural textural prior arbitraryscale video superresolution avsr aim enhance resolution video frame potentially various scaling factor present several challenge regarding spatial detail reproduction temporal consistency computational complexity paper first describe strong baseline avsr putting together three variant elementary building block flowguided recurrent unit aggregate spatiotemporal information previous frame flowrefined crossattention unit selects spatiotemporal information future frame hyperupsampling unit generates scaleaware contentindependent upsampling kernel introduce stavsr equipping baseline multiscale structural textural prior computed pretrained vgg network prior proven effective discriminating structure texture across different location scale beneficial avsr comprehensive experiment show stavsr significantly improves superresolution quality generalization ability inference speed stateoftheart code available
ophthalmic biomarker detection highlight ieee video image processing cup student competition vip cup offer unique experience undergraduate allowing student work together solve challenging realworld problem video image processing technique iteration vip cup challenged student balance personalization generalization performing biomarker detection optical coherence tomography oct image balancing personalization generalization important challenge tackle variation within oct scan patient visit minimal difference manifestation disease across different patient may substantial domain difference oct scan arise due pathology manifestation across patient clinical label visit along treatment process scan taken hence provided multimodal oct dataset allow team effectively target challenge overall competition gave undergraduate opportunity learn artificial intelligence powerful tool medical field well unique challenge one face applying machine learning biomedical data
sam sam slicer segmentwithsam extension annotating medical image creating annotation medical data timeconsuming often requires highly specialized expertise various tool implemented aid process segment anything model sam offer generalpurpose promptbased segmentation algorithm designed annotate video paper adapt model annotation medical image offer implementation form extension popular annotation software slicer extension allows user place point prompt slice generate annotation mask propagate annotation across entire volume either singledirectional bidirectional manner code publicly available httpsgithubcommazurowskilabslicersegmentwithsam easily installed directly extension manager slicer well
taptotab videobased guitar tab generation using ai audio analysis automation guitar tablature generation video input hold significant promise enhancing music education transcription accuracy performance analysis existing method face challenge consistency completeness particularly detecting fretboards accurately identifying note address issue paper introduces advanced approach leveraging deep learning specifically yolo model realtime fretboard detection fourier transformbased audio analysis precise note identification experimental result demonstrate substantial improvement detection accuracy robustness compared traditional technique paper outline development implementation evaluation methodology aiming revolutionize guitar instruction automating creation guitar tab video recording
talkinnerf animatable neural field fullbody talking human introduce novel framework learns dynamic neural radiance field nerf fullbody talking human monocular video prior work represents body pose face however human communicate full body combining body pose hand gesture well facial expression work propose talkinnerf unified nerfbased network represents holistic human motion given monocular video subject learn corresponding module body face hand combined together generate final result capture complex finger articulation learn additional deformation field hand multiidentity representation enables simultaneous training multiple subject well robust animation completely unseen pose also generalize novel identity given short video input demonstrate stateoftheart performance animating fullbody talking human finegrained hand articulation facial expression
show guide instructionalplan grounded vision language model guiding user complex procedural plan inherently multimodal task visually illustrated plan step crucial deliver effective plan guidance however existing work planfollowing language model lm often capable multimodal input output work present mmplanllm first multimodal llm designed assist user executing instructional task leveraging textual plan visual information specifically bring crossmodality two key task conversational video moment retrieval model retrieves relevant stepvideo segment based user query visuallyinformed step generation model generates next step plan conditioned image user current progress mmplanllm trained using novel multitaskmultistage approach designed gradually expose model multimodal instructionalplans semantic layer achieving strong performance multimodal textual dialogue plangrounded setting furthermore show model delivers crossmodal temporal planstructure representation aligned textual plan step instructional video moment
expert public governing multimodal language model politically sensitive video analysis paper examines governance multimodal large language model mmllms individual collective deliberation focusing analysis politically sensitive video conducted twostep study first interview journalist established baseline understanding expert video interpretation second individual general public engaged deliberation using inclusiveai platform facilitates democratic decisionmaking decentralized autonomous organization dao mechanism finding show expert emphasized emotion narrative general public prioritized factual clarity objectivity situation emotional neutrality additionally explored impact different governance mechanism quadratic v weighted ranking voting equal v power distribution user decisionmaking ai behave specifically quadratic voting enhanced perception liberal democracy political equality participant optimistic ai perceived voting process higher level participatory democracy result suggest potential applying dao mechanism help democratize ai governance
multi class activity classification video using motion history image generation human action recognition topic interest across multiple field ranging security entertainment system tracking motion identifying action performed real time basis necessary critical security system entertainment especially gaming need immediate response action gesture paramount success system show motion history image well established framework capture temporal activity information multi dimensional detail enabling various usecases including classification utilize mhi produce sample data train classifier demonstrate effectiveness action classification across six different activity single multiaction video analyze classifier performance identify usecases mhi struggle generate appropriate activity image discus mechanism future work overcome limitation
describe userdriven audio description blind low vision individual audio description ad make video accessible blind low vision blv user describing visual element understood main audio track ad created professional novice describers timeconsuming lack scalability offering little control blv viewer description length content receive address gap explore userdriven aigenerated description blv viewer control receive description study blv participant activated audio description seven different video genre two level detail concise detailed result show difference ad frequency level detail blv user wanted different video sense control style ad delivery limitation variation among blv user ad need perception aigenerated description discus implication finding future aibased ad tool
enhancing videollm reasoning via agentofthoughts distillation paper tackle problem video question answering videoqa task often requires multistep reasoning profound understanding spatialtemporal dynamic large videolanguage model perform well benchmark often lack explainability spatialtemporal grounding paper propose agentofthoughts distillation aotd method enhances model incorporating automatically generated chainofthoughts cot instructiontuning process specifically leverage agentbased system decompose complex question subtasks address specialized vision model intermediate result treated reasoning chain also introduce verification mechanism using large language model llm ensure reliability generated cot extensive experiment demonstrate aotd improves performance multiplechoice openended benchmark
scenefactor factored latent diffusion controllable scene generation present scenefactor diffusionbased approach largescale scene generation enables controllable generation effortless editing scenefactor enables textguided scene synthesis factored diffusion formulation leveraging latent semantic geometric manifold generation arbitrarysized scene text input enables easy controllable generation text guidance remains imprecise intuitive localized editing manipulation generated scene factored semantic diffusion generates proxy semantic space composed semantic box enables controllable editing generated scene adding removing changing size semantic proxy box guide highfidelity consistent geometric editing extensive experiment demonstrate approach enables highfidelity scene synthesis effective controllable editing factored diffusion approach
domaintransferred synthetic data generation improving monocular depth estimation major obstacle development effective monocular depth estimation algorithm difficulty obtaining highquality depth data corresponds collected rgb image collecting data timeconsuming costly even data collected modern sensor limited range resolution subject inconsistency noise combat propose method data generation simulation using synthetic environment cyclegan domain transfer compare method data generation popular nyudepth dataset training depth estimation model based densedepth structure using different training set real simulated data evaluate performance model newly collected image lidar depth data husky robot verify generalizability approach show gantransformed data serve effective alternative realworld data particularly depth estimation
towards physicsinformed cyclic adversarial multipsf lensless imaging lensless imaging emerged promising field within inverse imaging offering compact costeffective solution potential revolutionize computational camera market circumventing traditional optical component like lens mirror novel approach like maskbased lensless imaging eliminate need conventional hardware however advancement lensless image reconstruction particularly leveraging generative adversarial network gans hindered reliance datadriven training process resulting network specificity point spread function psf imaging system necessitates complete retraining minor psf change limiting adaptability generalizability across diverse imaging scenario paper introduce novel approach multipsf lensless imaging employing dual discriminator cyclic adversarial framework propose unique generator architecture sparse convolutional psfaware auxiliary branch coupled forward model integrated training loop facilitate physicsinformed learning handle substantial domain gap lensless lensed image comprehensive performance evaluation ablation study underscore effectiveness model offering robust adaptable lensless image reconstruction capability method achieves comparable performance existing psfagnostic generative method single psf case demonstrates resilience psf change without need retraining
moving object proposal deep learned optical flow video object segmentation dynamic scene understanding one conspicuous field interest among computer vision community order enhance dynamic scene understanding pixelwise segmentation neural network widely accepted latest research pixelwise segmentation combined semantic motion information produced good performance work propose state art architecture neural network accurately efficiently get moving object proposal mop first train unsupervised convolutional neural network unflow generate optical flow estimation render output optical flow net fully convolutional segnet model main contribution work finetuning pretrained optical flow model brand new davis dataset leveraging fully convolutional neural network encoderdecoder architecture segment object developed code tensorflow executed training evaluation process aws instance
gaussian shadow casting neural character neural character model reconstruct detailed geometry texture video lack explicit shadow shading leading artifact generating novel view pose relighting particularly difficult include shadow global effect required casting secondary ray costly propose new shadow model using gaussian density proxy replaces sampling simple analytic formula support dynamic motion tailored shadow computation thereby avoiding affine projection approximation sorting required closely related gaussian splatting combined deferred neural rendering model gaussian shadow enable lambertian shading shadow casting minimal overhead demonstrate improved reconstruction better separation albedo shading shadow challenging outdoor scene direct sun light hard shadow method able optimize light direction without input user result novel pose fewer shadow artifact relighting novel scene realistic compared stateoftheart method providing new way pose neural character novel environment increasing applicability
pixelwise color constancy via smoothness technique multiilluminant scene scene illuminated several light source traditional assumption uniform illumination invalid issue ignored color constancy method primarily due complex spatial impact multiple light source image moreover existing multiilluminant method fail preserve smooth change illumination stem spatial dependency natural image motivated propose novel multiilluminant color constancy method learning pixelwise illumination map caused multiple light source proposed method enforces smoothness within neighboring pixel regularizing training total variation loss moreover bilateral filter provisioned enhance natural appearance estimated image preserving edge additionally propose labelsmoothing technique enables model generalize well despite uncertainty ground truth quantitative qualitative experiment demonstrate proposed method outperforms stateoftheart
eventbased asynchronous hdr imaging temporal incident light modulation dynamic range dr pivotal characteristic imaging system current framebased camera struggle achieve high dynamic range imaging due conflict globally uniform exposure spatially variant scene illumination paper propose asynhdr pixelasynchronous hdr imaging system based key insight challenge hdr imaging unique eventgenerating mechanism dynamic vision sensor dvs proposed asynhdr system integrates dvs set lcd panel lcd panel modulate irradiance incident upon dvs altering transparency thereby triggering pixelindependent event stream hdr image subsequently decoded event stream temporalweighted algorithm experiment standard test platform several challenging scene verified feasibility system hdr imaging task
visiongpt llmassisted realtime anomaly detection safe visual navigation paper explores potential large language modelsllms zeroshot anomaly detection safe visual navigation assistance stateoftheart realtime openworld object detection model yoloworld specialized prompt proposed framework identify anomaly within cameracaptured frame include possible obstacle generate concise audiodelivered description emphasizing abnormality assist safe visual navigation complex circumstance moreover proposed framework leverage advantage llm openvocabulary object detection model achieve dynamic scenario switch allows user transition smoothly scene scene address limitation traditional visual navigation furthermore paper explored performance contribution different prompt component provided vision future improvement visual accessibility paved way llm video anomaly detection visionlanguage understanding
pointsoup highperformance extremely lowdecodinglatency learned geometry codec largescale point cloud scene despite considerable progress achieved point cloud geometry compression still remains challenge effectively compressing largescale scene sparse surface another key challenge lie reducing decoding latency crucial requirement realworld application paper propose pointsoup efficient learningbased geometry codec attains highperformance extremely lowdecodinglatency simultaneously inspired conventional trisoup codec point modelbased strategy devised characterize local surface specifically skin feature embedded local window via attentionbased encoder dilated window introduced crossscale prior infer distribution quantized feature parallel decoding feature undergo fast refinement followed foldingbased point generator reconstructs point coordinate fairly fast speed experiment show pointsoup achieves stateoftheart performance multiple benchmark significantly lower decoding complexity ie faster trisoup decoder comparatively lowend platform eg one rtx furthermore offer variablerate control single neural model attractive industrial practitioner
investigation unsupervised supervised hyperspectral anomaly detection hyperspectral sensing valuable tool detecting anomaly distinguishing material scene hyperspectral anomaly detection hsad help characterize captured scene separate anomaly background class vital agriculture environment military application rsta reconnaissance surveillance target acquisition mission previously designed equal voting ensemble hyperspectral unmixing three unsupervised hsad algorithm later utilized supervised classifier determine weight voting ensemble creating hybrid heterogeneous unsupervised hsad algorithm supervised classifier model stacking improved detection accuracy however supervised classification method usually fail detect novel unknown pattern substantially deviate seen previously work evaluate technique supervised unsupervised method using general hyperspectral data provide new insight
towards infusing auxiliary knowledge distracted driver detection distracted driving leading cause road accident globally identification distracted driving involves reliably detecting classifying various form driver distraction eg texting eating using incar device invehicle camera feed enhance road safety task challenging due need robust model generalize diverse set driver behavior without requiring extensive annotated datasets paper propose novel method distracted driver detection ddd infusing auxiliary knowledge semantic relation entity scene structural configuration driver pose specifically construct unified framework integrates scene graph driver pose information visual cue video frame create holistic representation driver actionsour result indicate achieves accuracy improvement visiononly baseline incorporating auxiliary knowledge visual information
eventbased mosaicing bundle adjustment tackle problem mosaicing bundle adjustment ie simultaneous refinement camera orientation scene map purely rotating event camera formulate problem regularized nonlinear least square optimization objective function defined using linearized event generation model camera orientation panoramic gradient map scene show ba optimization exploitable blockdiagonal sparsity structure problem solved efficiently best knowledge first work leverage sparsity speed optimization context eventbased camera without need convert event imagelike representation evaluate method called emba synthetic realworld datasets show effectiveness photometric error decrease yielding result unprecedented quality addition demonstrate emba using high spatial resolution event camera yielding delicate panorama wild even without initial map project page httpsgithubcomtubripemba
local policy enable zeroshot longhorizon manipulation robotic manipulation difficult due challenge simulating complex contact generating realistic task distribution tackle latter problem introduce manipgen leverage new class policy transfer local policy locality enables variety appealing property including invariance absolute robot object pose skill ordering global scene configuration combine policy foundation model vision language motion planning demonstrate sota zeroshot performance method robosuite benchmark task simulation transfer local policy simulation reality observe solve unseen longhorizon manipulation task stage significant pose object scene configuration variation manipgen outperforms sota approach saycan openvla llmtrajgen voxposer across realworld manipulation task respectively video result httpsmihdalalgithubiomanipgen
splat splat embedding invisible watermark within gaussian splatting gaussian splatting demonstrated impressive reconstruction performance explicit scene representation given widespread application reconstruction generation task urgent need protect copyright asset however existing copyright protection technique overlook usability asset posing challenge practical deployment describe watergs first watermarking framework embeds content without modifying attribute vanilla achieve take deep insight spherical harmonic sh devise importancegraded sh coefficient encryption strategy embed hidden sh coefficient furthermore employ convolutional autoencoder establish mapping original gaussian primitive opacity hidden gaussian primitive opacity extensive experiment indicate watergs significantly outperforms existing steganography technique higher scene fidelity faster rendering speed ensuring security robustness user experience code data released httpswatergsgithubio
towards physicallybased skymodeling accurate environment map key component rendering photorealistic outdoor scene coherent illumination enable captivating visual art immersive virtual reality wide range engineering scientific application recent work extended skymodels comprehensive inclusive cloud formation existing approach fall short faithfully recreating keycharacteristics physically captured hdri demonstrate environment map produced skymodels relight scene tone shadow illumination coherence physically captured hdr imagery though visual quality dnngenerated ldr hdr imagery greatly progressed recent year demonstrate progress tangential skymodelling due extended dynamic range edr required outdoor environment map inclusive sun skymodelling extends beyond conventional paradigm high dynamic range imagery hdri work propose allweather skymodel learning weatheredskies directly physically captured hdr imagery per usercontrolled positioning sun cloud formation model allsky allows emulation physically captured environment map improved retention extended dynamic range edr sky
enhancing autonomous vehicle safety rain datacentric approach clear vision autonomous vehicle face significant challenge navigating adverse weather particularly rain due visual impairment camerabased system study leveraged contemporary deep learning technique mitigate challenge aiming develop vision model process live vehicle camera feed eliminate raininduced visual hindrance yielding visuals closely resembling clear rainfree scene using car learning act carla simulation environment generated comprehensive dataset clear rainy image model training testing model employed classic encoderdecoder architecture skip connection concatenation operation trained using novel batching scheme designed effectively distinguish highfrequency rain pattern lowfrequency scene feature across successive image frame evaluate model performance integrated steering module process frontview image input result demonstrated notable improvement steering accuracy underscoring model potential enhance navigation safety reliability rainy weather condition
neural network diffusion diffusion model achieved remarkable success image video generation work demonstrate diffusion model also textitgenerate highperforming neural network parameter approach simple utilizing autoencoder diffusion model autoencoder extract latent representation subset trained neural network parameter next diffusion model trained synthesize latent representation random noise model generates new representation passed autoencoders decoder produce new subset highperforming network parameter across various architecture datasets approach consistently generates model comparable improved performance trained network minimal additional cost notably empirically find generated model memorizing trained one result encourage exploration versatile use diffusion model code available hrefhttpsgithubcomnushpcailabneuralnetworkdiffusionhere
cosign fewstep guidance consistency model solve general inverse problem diffusion model demonstrated strong prior solving general inverse problem existing diffusion modelbased inverse problem solver dis employ plugandplay approach guide sampling trajectory either projection gradient though effective method generally necessitate hundred sampling step posing dilemma inference time reconstruction quality work try push boundary inference step nfes still maintaining high reconstruction quality achieve propose leverage pretrained distillation diffusion model namely consistency model data prior key achieving fewstep guidance enforce two type constraint sampling process consistency model soft measurement constraint controlnet hard measurement constraint via optimization supporting singlestep reconstruction multistep refinement proposed framework provides way trade image quality additional computational cost within comparable nfes method achieves new stateoftheart diffusionbased inverse problem solving showcasing significant potential employing priorbased inverse problem solver realworld application code available httpsgithubcombiomedailabumichgancosign
grin zeroshot metric depth pixellevel diffusion reconstruction single image longstanding problem computer vision learningbased method address inherent scale ambiguity leveraging increasingly large labeled unlabeled datasets produce geometric prior capable generating accurate prediction across domain result state art approach show impressive performance zeroshot relative metric depth estimation recently diffusion model exhibited remarkable scalability generalizable property learned representation however model repurpose tool originally designed image generation operate dense groundtruth available depth label especially realworld setting paper present grin efficient diffusion model designed ingest sparse unstructured training data use image feature geometric positional encoding condition diffusion process globally locally generating depth prediction pixellevel comprehensive experiment across eight indoor outdoor datasets show grin establishes new state art zeroshot metric monocular depth estimation even trained scratch
zeroshot dynamic mri reconstruction globaltolocal diffusion model diffusion model recently demonstrated considerable advancement generation reconstruction magnetic resonance imaging mri data model exhibit great potential handling unsampled data reducing noise highlighting promise generative model however application dynamic mri remains relatively underexplored primarily due substantial amount fullysampled data typically required training difficult obtain dynamic mri due spatiotemporal complexity high acquisition cost address challenge propose dynamic mri reconstruction method based timeinterleaved acquisition scheme termed globaltolocal diffusion model specifically fully encoded fullresolution reference data constructed merging undersampled kspace data adjacent time frame generating two distinct bulk training datasets global local model globaltolocal diffusion framework alternately optimizes global information local image detail enabling zeroshot reconstruction extensive experiment demonstrate proposed method performs well term noise reduction detail preservation achieving reconstruction quality comparable supervised approach
evaluation agent efficient promptable evaluation framework visual generative model recent advancement visual generative model enabled highquality image video generation opening diverse application however evaluating model often demand sampling hundred thousand image video making process computationally expensive especially diffusionbased model inherently slow sampling moreover existing evaluation method rely rigid pipeline overlook specific user need provide numerical result without clear explanation contrast human quickly form impression model capability observing sample mimic propose evaluation agent framework employ humanlike strategy efficient dynamic multiround evaluation using sample per round offering detailed usertailored analysis offer four key advantage efficiency promptable evaluation tailored diverse user need explainability beyond single numerical score scalability across various model tool experiment show evaluation agent reduces evaluation time traditional method delivering comparable result evaluation agent framework fully opensourced advance research visual generative model efficient evaluation
cortical surface diffusion generative model cortical surface analysis gained increased prominence given potential implication neurological developmental disorder traditional vision diffusion model effective generating natural image present limitation capturing intricate development pattern neuroimaging due limited datasets particularly true generating cortical surface individual variability cortical morphology high leading urgent need better method model brain development diverse variability inherent across different individual work proposed novel diffusion model generation cortical surface metric using modified surface vision transformer principal architecture validate method developing human connectome project dhcp result suggest model demonstrates superior performance capturing intricate detail evolving cortical surface furthermore model generate highquality realistic sample cortical surface conditioned postmenstrual agepma scan
paired diffusion generation related synthetic petctsegmentation scan using linked denoising diffusion probabilistic model rapid advancement artificial intelligence ai biomedical imaging radiotherapy hindered limited availability large imaging data repository recent research improvement denoising diffusion probabilistic model ddpm high quality synthetic medical scan possible despite currently way generating multiple related image corresponding ground truth used train model synthetic scan often manually annotated use research introduces novel architecture able generate multiple related petcttumour mask pair using paired network conditional encoders approach includes innovative time stepcontrolled mechanism noiseseeding strategy improve ddpm sampling consistency model requires modified perceptual loss function ensure accurate feature alignment show generation clearly aligned synthetic image improvement segmentation accuracy generated image
diffusion sound propagation physicsinspired model ultrasound image generation deep learning dl method typically require large datasets effectively learn data distribution however medical field data often limited quantity acquiring labeled data costly mitigate data scarcity data augmentation technique commonly employed among technique generative model play pivotal role expanding datasets however come ultrasound u imaging authenticity generated data often diminishes due oversight ultrasound physic propose novel approach improve quality generated u image introducing physicsbased diffusion model specifically designed image modality proposed model incorporates usspecific scheduler scheme mimic natural behavior sound wave propagation ultrasound imaging analysis demonstrates proposed method aid modeling attenuation dynamic u imaging present qualitative quantitative result based standard generative model metric showing proposed method result overall plausible image code available httpsgithubcommarinadominguezdiffusionforusimages
smgdiff soccer motion generation using diffusion probabilistic model soccer globally renowned sport significant application video game vrar however generating realistic soccer motion remains challenging due intricate interaction human player ball paper introduce smgdiff novel twostage framework generating realtime usercontrollable soccer motion key idea integrate realtime character control powerful diffusionbased generative model ensuring highquality diverse output motion first stage instantly transform coarse user control diverse global trajectory character second stage employ transformerbased autoregressive diffusion model generate soccer motion based trajectory conditioning incorporate contact guidance module inference optimize contact detail realistic ballfoot interaction moreover contribute largescale soccer motion dataset consisting million frame diverse soccer motion extensive experiment demonstrate smgdiff significantly outperforms existing method term motion quality condition alignment
pixelwise recognition holistic surgical scene understanding paper present holistic multigranular surgical scene understanding prostatectomy grasp dataset curated benchmark model surgical scene understanding hierarchy complementary task varying level granularity approach encompasses longterm task surgical phase step recognition shortterm task including surgical instrument segmentation atomic visual action detection exploit proposed benchmark introduce transformer action phase step instrument segmentation tapis model general architecture combine global video feature extractor localized region proposal instrument segmentation model tackle multigranularity benchmark extensive experimentation alternative benchmark demonstrate tapis versatility stateoftheart performance across different task work represents foundational step forward endoscopic vision offering novel framework future research towards holistic surgical scene understanding
sgiformer semanticguided geometricenhanced interleaving transformer instance segmentation recent year transformerbased model exhibited considerable potential point cloud instance segmentation despite promising performance achieved existing method encounter challenge instance query initialization problem excessive reliance stacked layer rendering incompatible largescale scene paper introduces novel method named sgiformer instance segmentation composed semanticguided mix query smq initialization geometricenhanced interleaving transformer git decoder specifically principle smq initialization scheme leverage predicted voxelwise semantic information implicitly generate sceneaware query yielding adequate scene prior compensating learnable query set subsequently feed formed overall query git decoder alternately refine instance query global scene feature capturing finegrained information reducing complex design intricacy simultaneously emphasize geometric property consider bias estimation auxiliary task progressively integrate shifted point coordinate embedding reinforce instance localization sgiformer attains stateoftheart performance scannet datasets challenging highfidelity scannet benchmark striking balance accuracy efficiency code weight demo video publicly available httpsrayyohgithubiosgiformer
iterative approach reconstructing neural disparity field lightfield data study proposes neural disparity field ndf establishes implicit continuous representation scene disparity based neural field iterative approach address inverse problem ndf reconstruction lightfield data ndf enables seamless precise characterization disparity variation threedimensional scene discretize disparity arbitrary resolution overcoming limitation traditional disparity map prone sampling error interpolation inaccuracy proposed ndf network architecture utilizes hash encoding combined multilayer perceptrons capture detailed disparity texture level thereby enhancing ability represent geometric information complex scene leveraging spatialangular consistency inherent lightfield data differentiable forward model generate central view image lightfield data developed based forward model optimization scheme inverse problem ndf reconstruction using differentiable propagation operator established furthermore iterative solution method adopted reconstruct ndf optimization scheme require training datasets applies lightfield data captured various acquisition method experimental result demonstrate highquality ndf reconstructed lightfield data using proposed method highresolution disparity effectively recovered ndf demonstrating capability implicit continuous representation scene disparity
dataset benchmark hand motion generation piano performance recently artificial intelligence technique education received increasing attention still remains open problem design effective music instrument instructing system although key press directly derived sheet music transitional movement among key press require extensive guidance piano performance work construct pianohand motion generation benchmark guide hand movement fingering piano playing end collect annotated dataset consisting hour piano playing video birdseye view million annotated hand pose also introduce powerful baseline model generates hand motion piano audio position predictor positionguided gesture generator furthermore series evaluation metric designed assess performance baseline model including motion similarity smoothness positional accuracy left right hand overall fidelity movement distribution despite piano key press respect music score audio already accessible aim provide guidance piano fingering instruction purpose source code dataset accessed
timestep embedding tell time cache video diffusion model fundamental backbone video generation diffusion model challenged low inference speed due sequential nature denoising previous method speed model caching reusing model output uniformly selected timesteps however strategy neglect fact difference among model output uniform across timesteps hinders selecting appropriate model output cache leading poor balance inference efficiency visual quality study introduce timestep embedding aware cache teacache trainingfree caching approach estimate leverage fluctuating difference among model output across timesteps rather directly using timeconsuming model output teacache focus model input strong correlation modeloutputs incurring negligible computational cost teacache first modulates noisy input using timestep embeddings ensure difference better approximating model output teacache introduces rescaling strategy refine estimated difference utilizes indicate output caching experiment show teacache achieves acceleration opensoraplan negligible vbench score degradation visual quality
rdeic accelerating diffusionbased extreme image compression relay residual diffusion diffusionbased extreme image compression method achieved impressive performance extremely low bitrates however constrained iterative denoising process start pure noise method limited fidelity efficiency address two issue present relay residual diffusion extreme image compression rdeic leverage compressed feature initialization residual diffusion specifically first use compressed latent feature image added noise instead pure noise starting point eliminate unnecessary initial stage denoising process second directly derive novel residual diffusion equation stable diffusion original diffusion equation reconstructs raw image iteratively removing added noise residual compressed target latent feature way effectively combine efficiency residual diffusion powerful generative capability stable diffusion third propose fixedstep finetuning strategy eliminate discrepancy training inference phase thereby improving reconstruction quality extensive experiment demonstrate proposed rdeic achieves stateoftheart visual quality outperforms existing diffusionbased extreme image compression method fidelity efficiency source code provided httpsgithubcomhuaichangrdeic
ddmi domainagnostic latent diffusion model synthesizing highquality implicit neural representation recent study introduced new class generative model synthesizing implicit neural representation inr capture arbitrary continuous signal various domain model opened door domainagnostic generative model often fail achieve highquality generation observed existing method generate weight neural network parameterize inr evaluate network fixed positional embeddings pe arguably architecture limit expressive power generative model result lowquality inr generation address limitation propose domainagnostic latent diffusion model inr ddmi generates adaptive positional embeddings instead neural network weight specifically develop discretetocontinuous space variational autoencoder seamlessly connects discrete data continuous signal function shared latent space additionally introduce novel conditioning mechanism evaluating inr hierarchically decomposed pe enhance expressive power extensive experiment across four modality eg image shape neural radiance field video seven benchmark datasets demonstrate versatility ddmi superior performance compared existing inr generative model
lediff latent exposure diffusion hdr generation consumer display increasingly support stop dynamic range image asset internet photograph generative ai content remain limited low dynamic range ldr constraining utility across high dynamic range hdr application currently generative model produce highbit highdynamic range content generalizable way existing ldrtohdr conversion method often struggle produce photorealistic detail physicallyplausible dynamic range clipped area introduce lediff method enables generative model hdr content generation latent space fusion inspired imagespace exposure fusion technique also function ldrtohdr converter expanding dynamic range existing lowdynamic range image approach us small hdr dataset enable pretrained diffusion model recover detail dynamic range clipped highlight shadow lediff brings hdr capability existing generative model convert ldr image hdr creating photorealistic hdr output image generation imagebased lighting hdr environment map generation photographic effect depth field simulation linear hdr data essential realistic quality
head neck tumor segmentation petct image based diffusion model head neck hn cancer among prevalent type cancer worldwide petct widely used hn cancer management recently diffusion model demonstrated remarkable performance various imagegeneration task work proposed diffusion model accurately perform hn tumor segmentation pet ct volume diffusion model developed considering nature pet ct image acquired reverse process model utilized unet structure took concatenation pet ct gaussian noise volume network input generate tumor mask experiment based hecktor challenge dataset conducted evaluate effectiveness proposed diffusion model several stateoftheart technique based unet transformer structure adopted reference method benefit employing pet ct network input well extending diffusion model investigated based various quantitative metric uncertainty map generated result showed proposed diffusion model could generate accurate segmentation result compared method compared diffusion model format proposed model yielded superior result experiment also highlighted advantage utilizing dualmodality pet ct data singlemodality data hn tumor segmentation
latent diffusion implicit amplification efficient continuousscale superresolution remote sensing image recent advancement diffusion model significantly improved performance superresolution sr task however previous research often overlook fundamental difference sr general image generation general image generation involves creating image scratch sr focus specifically enhancing existing lowresolution lr image adding typically missing highfrequency detail oversight increase training difficulty also limit inference efficiency furthermore previous diffusionbased sr method typically trained inferred fixed integer scale factor lacking flexibility meet need upsampling noninteger scale factor address issue paper proposes efficient elastic diffusionbased sr model specially designed continuousscale sr remote sensing imagery employ twostage latent diffusion paradigm first stage autoencoder trained capture differential prior highresolution hr lr image encoder intentionally ignores existing lr content alleviate encoding burden decoder introduces sr branch equipped continuous scale upsampling module accomplish reconstruction guidance differential prior second stage conditional diffusion model learned within latent space predict true differential prior encoding experimental result demonstrate achieves superior objective metric visual quality compared stateoftheart sr method additionally reduces inference time diffusionbased sr method level comparable nondiffusion method
neural radiance fieldsbased holography invited study present novel approach generating hologram based neural radiance field nerf technique generating threedimensional data difficult hologram computation nerf stateoftheart technique lightfield reconstruction image based volume rendering nerf rapidly predict newview image include training dataset study constructed rendering pipeline directly light field generated image nerf hologram generation using deep neural network within reasonable time pipeline comprises three main component nerf depth predictor hologram generator constructed using deep neural network pipeline include physical calculation predicted hologram scene viewed direction computed using proposed pipeline simulation experimental result presented
atombot embodied fulfillment unspoken human need affective theory mind propose atombot novel task generation execution framework proactive robothuman interaction leverage human mental physical state inference capability vision language model vlm prompted affective theory mind atom without requiring explicit command human atombot proactively generates follows feasible task improve general human wellbeing around human atombot first detects current human need based inferred human state observation surrounding environment generates task fulfill need taking account embodied constraint designed daily life scenario spanning common scene tasked visual stimulus human subject robot used similarity human openended answer robot output human satisfaction score metric robot performance atombot received high human evaluation need detection embodied solution task execution show atombot excels generating executing feasible plan fulfill unspoken human need video code available httpsaffectivetombotgithubio
solution deepfakes camera hardware cryptography deep learning verify real image exponential progress generative ai pose serious implication credibility real image video exist point future digital content produced generative ai indistinguishable created camera highquality generative algorithm accessible anyone ratio synthetic real image large imperative establish method separate real data synthetic data high confidence define real image produced camera hardware capturing realworld scene synthetic generation image alteration real image generative ai computer graphic technique labeled synthetic image end document aim present known strategy detection cryptography employed verify image real weight strength weakness strategy suggest additional improvement alleviate shortcoming
diffusion multidomain adaptation method eosinophil segmentation eosinophilic esophagitis eoe represents challenging condition medical provider today cause currently unknown impact patient daily life significant increasing prevalence traditional approach medical image diagnosis standard deep learning algorithm limited relatively small amount data difficulty generalization response two method arisen seem perform well diffusion multidomain method current research effort favoring diffusion method eoe dataset discovered multidomain adversarial network outperformed diffusion based method fid compared future work diffusion method include comparison multidomain adaptation method ensure best performance achieved
fddm frequencydecomposed diffusion model rectum cancer dose prediction radiotherapy accurate dose distribution prediction crucial radiotherapy planning although previous method based convolutional neural network shown promising performance problem oversmoothing leading prediction without important highfrequency detail recently diffusion model achieved great success computer vision excels generating image highfrequency detail yet suffers timeconsuming extensive computational resource consumption alleviate problem propose frequencydecomposed diffusion model fddm refines highfrequency subbands dose map specific design coarse dose prediction module cdpm first predict coarse dose map utilize discrete wavelet transform decompose coarse dose map lowfrequency subband three highfrequency subbands notable difference coarse predicted result ground truth highfrequency subbands therefore design diffusionbased module called highfrequency refinement module hfrm performs diffusion operation highfrequency component dose map instead original dose map extensive experiment inhouse dataset verify effectiveness approach
dynamic point cloud sequence video dynamic point cloud sequence serve one common practical representation modality dynamic realworld environment however unstructured nature spatial temporal domain pose significant challenge effective efficient processing existing deep point cloud sequence modeling approach imitate mature video learning mechanism developing complex spatiotemporal point neighbor grouping feature aggregation scheme often resulting method lacking effectiveness efficiency expressive power paper propose novel generic representation called textitstructured point cloud video spcvs intuitively leveraging fact geometric shape essentially manifold spcv reorganizes point cloud sequence video spatial smoothness temporal consistency pixel value correspond coordinate point structured nature spcv representation allows seamless adaptation wellestablished imagevideo technique enabling efficient effective processing analysis point cloud sequence achieve reorganization design selfsupervised learning pipeline geometrically regularized driven selfreconstructive deformation field learning objective additionally construct spcvbased framework lowlevel highlevel point cloud sequence processing analysis task including action recognition temporal interpolation compression extensive experiment demonstrate versatility superiority proposed spcv potential offer new possibility deep learning unstructured point cloud sequence code released httpsgithubcomzengyimingeamonspcv
spatial cognition egocentric video sight mind human move around performing daily task able recall positioned object environment even object currently sight paper aim mimic spatial cognition ability thus formulate task sight mind tracking active object using observation captured egocentric camera introduce simple effective approach address challenging problem called lift match keep lmk lmk lift partial observation world coordinate match time using visual appearance location interaction form object track keep object track even go outofview camera benchmark lmk long video epickitchens result demonstrate spatial cognition critical correctly locating object short long time scale eg one long egocentric video estimate location active object second object correctly localised lmk compared recent method egocentric video general tracking method
turbulence strength estimation video using physicsbased deep learning image captured long distance suffer dynamic image distortion due turbulent flow air cell random temperature thus refractive index phenomenon known image dancing commonly characterized refractiveindex structure constant measure turbulence strength many application atmospheric forecast model longrangeastronomy imaging aviation safety optical communication technology estimation critical accurately sensing turbulent environment previous method estimation include estimation meteorological data temperature relative humidity wind shear etc singlepoint measurement twoended pathlength measurement optical scintillometer pathaveraged recently estimating passive video camera low cost hardware complexity paper present comparative analysis classical image gradient method estimation modern deep learningbased method leveraging convolutional neural network enable collect dataset video capture along reference scintillometer measurement ground truth release unique dataset scientific community observe deep learning method achieve higher accuracy trained similar data suffer generalization error unseen imagery compared classical method overcome tradeoff present novel physicsbased network architecture combine learned convolutional layer differentiable image gradient method maintains high accuracy generalizable across image datasets
echofm foundation model generalizable echocardiogram analysis foundation model recently gained significant attention generalizability adaptability across multiple task data distribution although medical foundation model emerged solution cardiac imaging especially echocardiography video still unexplored paper introduce echofm foundation model specifically designed represent analyze echocardiography video echofm propose selfsupervised learning framework capture spatial temporal variability pattern spatiotemporal consistent masking strategy periodicdriven contrastive learning framework effectively capture spatiotemporal dynamic echocardiography learn representative video feature without label pretrain model extensive dataset comprising echocardiography video covering scan view across different imaging mode million frame image pretrained echofm easily adapted finetuned variety downstream task serving robust backbone model evaluation systemically designed four downstream task echocardiography examination routine experiment result show echofm surpasses stateoftheart method including specialized echocardiography method selfsupervised pretraining model generalpurposed pretrained foundation model across downstream task
exploiting vlm localizability semantics open vocabulary action detection action detection aim detect recognize localize human action spatially temporally video existing approach focus closedset setting action detector trained tested video fixed set action category however constrained setting viable open world test video inevitably come beyond trained action category paper address practical yet challenging openvocabulary action detection ovad problem aim detect action test video training model fixed set action category achieve openvocabulary capability propose novel method openmixer exploit inherent semantics localizability large visionlanguage model vlm within family querybased detection transformer detr specifically openmixer developed spatial temporal openmixer block somb tomb dynamically fused alignment dfa module three component collectively enjoy merit strong generalization pretrained vlms endtoend learning detr design moreover established ovad benchmark various setting experimental result show openmixer performs best baseline detecting seen unseen action release code model dataset split
compactflownet efficient realtime optical flow estimation mobile device present compactflownet first realtime mobile neural network optical flow prediction involves determining displacement pixel initial frame relative corresponding pixel subsequent frame optical flow serf fundamental building block various videorelated task video restoration motion estimation video stabilization object tracking action recognition video generation current stateoftheart method prioritize accuracy often overlook constraint regarding speed memory usage existing light model typically focus reducing size still exhibit high latency compromise significantly quality optimized highperformance gpus resulting suboptimal performance mobile device study aim develop mobileoptimized optical flow model proposing novel mobile devicecompatible architecture well enhancement training pipeline optimize model reduced weight low memory utilization increased speed maintaining minimal error approach demonstrates superior comparable performance stateoftheart lightweight model challenging kitti sintel benchmark furthermore attains significantly accelerated inference speed thereby yielding realtime operational efficiency iphone surpassing realtime performance level advanced mobile device
sportsqa largescale video question answering benchmark complex professional sport reasoning sport video question answering important task numerous application player training information retrieval however task explored due lack relevant datasets challenging nature present datasets video question answering videoqa focus mainly general coarsegrained understanding dailylife video applicable sport scenario requiring professional action understanding finegrained motion analysis paper introduce first dataset named sportsqa specifically designed sport videoqa task sportsqa dataset includes various type question description chronology causality counterfactual condition covering multiple sport furthermore address characteristic sport videoqa task propose new autofocus transformer aft capable automatically focusing particular scale temporal information question answering conduct extensive experiment sportsqa including baseline study evaluation different method result demonstrate aft achieves stateoftheart performance
efficient uavs deployment resource allocation uavrelay assisted public safety network video transmission wireless communication highly depends cellular ground base station gb failure cellular gb fully partially natural manmade disaster creates communication gap disasteraffected area situation public safety communication psc significantly save national infrastructure property life throughout emergency psc provide missioncritical communication video transmission service affected area unmanned aerial vehicle uavs flying base station uavbss particularly suitable psc service flexible mobile easily deployable manuscript considers multiuavassisted psc network observational uav receiving video affected area ground user agus transmitting nearby gb via relay uav objective proposed study maximize average utility video stream generated agus upon reaching gb achieved optimizing position observational relay uavs well distribution communication resource bandwidth transmit power satisfying systemdesigned constraint transmission rate rate outage probability transmit power budget available bandwidth end joint uavs placement resource allocation problem mathematically formulated proposed problem pose significant challenge solution considering block coordinate descent successive convex approximation technique efficient iterative algorithm proposed finally simulation result provided show proposed approach outperforms existing method
scaling masking new paradigm data sampling image video quality assessment quality assessment image video emphasizes local detail global semantics whereas general data sampling method eg resizing cropping gridbased fragment fail catch simultaneously address deficiency current approach adopt multibranch model take input multiresolution data burden model complexity work instead stacking model elegant data sampling method named sama scaling masking explored compact local global content regular input size basic idea scale data pyramid first reduce pyramid regular data dimension masking strategy benefiting spatial temporal redundancy image video processed data maintains multiscale characteristic regular input size thus processed singlebranch model verify sampling method image video quality assessment experiment show sampling method improve performance current singlebranch model significantly achieves competitive performance multibranch model without extra model complexity source code available httpsgithubcomsissuiresama
experimental evaluation interactive edgecloud virtual reality gaming wifi using unity render streaming virtual reality vr streaming enables endusers seamlessly immerse interactive virtual environment using even lowend device however quality vr experience heavily relies wireless fidelity wifi performance since serf last hop network chain study delf intricate interplay wifi vr traffic drawing upon empirical data leveraging wifi simulator work evaluate wifi suitability vr streaming term quality service qos provides particular employ unity render streaming remotely stream realtime vr gaming content wifi using web realtime communication webrtc considering server physically located network edge near end user finding demonstrate system sustained network performance showcasing minimal roundtrip time rtt jitter frame per second fps addition uncover characteristic pattern generated traffic stream unveiling distinctive video transmission approach inherent webrtcbased service systematic packetization video frame vfs transmission discrete batch regular interval regardless targeted frame rate intervalbased transmission strategy maintains consistent video packet delay across video frame rate lead increased wifi airtime consumption result demonstrate shortening interval batch advantageous enhances wifi efficiency reduces delay delivering complete frame
univs unified universal video segmentation prompt query despite recent advance unified image segmentation developing unified video segmentation v model remains challenge mainly generic categoryspecified v task need detect object track across consecutive frame promptguided v task require reidentifying target visualtext prompt throughout entire video making hard handle different task architecture make attempt address issue present novel unified v architecture namely univs using prompt query univs average prompt feature target previous frame initial query explicitly decode mask introduces targetwise prompt crossattention layer mask decoder integrate prompt feature memory pool taking predicted mask entity previous frame visual prompt univs convert different v task promptguided target segmentation eliminating heuristic interframe matching process framework unifies different v task also naturally achieves universal training testing ensuring robust performance across different scenario univs show commendable balance performance universality challenging v benchmark covering video instance semantic panoptic object referring segmentation task code found urlhttpsgithubcomminghanliunivs
diverse dataset youtube video comment stance data programming model public opinion military organization significantly influence ability recruit talented individual recruitment effort increasingly extend digital space like social medium becomes essential assess stance social medium user toward online military content however notable lack data analyzing opinion military recruiting effort online compounded challenge stance labeling crucial understanding public perception despite importance stance analysis successful online military recruitment creating humanannotated indomain stance label resourceintensive paper address challenge stance labeling scarcity data public opinion online military recruitment introducing releasing diverse dataset dataset comprises comment u army official youtube channel video employed stateoftheart weak supervision approach leveraging large language model label stance comment toward respective video u army finding indicate u army video began attracting significant number comment stance distribution generally balanced among supportive oppositional neutral comment slight skew towards oppositional versus supportive comment
detection object throwing behavior surveillance video anomalous behavior detection challenging research area within computer vision progress area enables automated detection dangerous behavior using surveillance camera feed dangerous behavior often overlooked research throwing action traffic flow one unique requirement smart city project enhance public safety paper proposes solution throwing action detection surveillance video using deep learning present datasets throwing action publicly available address usecase smart city project first generate novel public throwing action dataset consisting video throwing action performed traffic participant pedestrian bicyclist car driver normal video without throwing action second compare performance different feature extractor anomaly detection method ucfcrime throwingaction datasets explored feature extractor convolutional network inflated convnet network multifiber network mfnet finally performance anomaly detection algorithm improved applying adam optimizer instead adadelta proposing mean normal loss function cover multitude normal situation traffic aspect yield better anomaly detection performance besides proposed mean normal loss function lower false alarm rate combined dataset experimental result reach area roc curve throwingaction dataset combined dataset respectively
weaksurg weakly supervised surgical instrument segmentation using temporal equivariance semantic continuity robotic surgical video instrument presence annotation typically recorded video stream offering potential reduce manually annotated cost segmentation however weakly supervised surgical instrument segmentation instrument presence label rarely explored surgical domain due highly underconstrained challenge temporal property enhance representation learning capturing sequential dependency pattern time even incomplete supervision situation take inherent temporal attribute surgical video account extend twostage weakly supervised segmentation paradigm different perspective firstly make temporal equivariance constraint enhance pixelwise temporal consistency adjacent feature secondly constrain classaware semantic continuity global local region across temporal dimension finally generate temporalenhanced pseudo mask consecutive frame suppress irrelevant region extensive experiment validated two surgical video datasets including one cholecystectomy surgery benchmark one real robotic left lateral segment liver surgery dataset annotate instancewise instrument label fixed timesteps double checked clinician experience evaluate segmentation result experimental result demonstrate promising performance method consistently achieves comparable favorable result previous stateoftheart approach
testtime zeroshot temporal action localization zeroshot temporal action localization zstal seek identify locate action untrimmed video unseen training existing zstal method involve finetuning model large amount annotated training data effective trainingbased zstal approach assume availability labeled data supervised learning impractical application furthermore training process naturally induces domain bias learned model may adversely affect model generalization ability arbitrary video consideration prompt u approach zstal problem radically novel perspective relaxing requirement training data aim introduce novel method performs testtime adaptation temporal action localization nutshell adapts pretrained vision language model vlm operates three step first videolevel pseudolabel action category computed aggregating information entire video action localization performed adopting novel procedure inspired selfsupervised learning finally framelevel textual description extracted stateoftheart captioning model employed refining action region proposal validate effectiveness conducting experiment datasets result demonstrate significantly outperforms zeroshot baseline based stateoftheart vlms confirming benefit testtime adaptation approach
reconstructing handheld object image video object manipulated hand ie manipulanda particularly challenging reconstruct internet video hand occlude much object also object often visible small number image pixel time two strong anchor emerge setting estimated hand help disambiguate location scale object set manipulanda small relative possible object insight mind present scalable paradigm handheld object reconstruction build recent breakthrough large languagevision model object datasets given monocular rgb video aim reconstruct handheld object geometry time order obtain best performing single frame model first present mcchandobject mccho jointly reconstructs hand object geometry given single rgb image inferred hand input subsequently prompt generative model using retrieve object model match object image call alignment retrievalaugmented reconstruction rar rar provides unified object geometry across frame result rigidly aligned input image mccho observation temporally consistent manner experiment demonstrate approach achieves stateoftheart performance lab internet imagevideo datasets make code model available project website httpsjanehwugithubiomccho
animationbased augmentation approach action recognition discontinuous video action recognition essential component computer vision play pivotal role multiple application despite significant improvement brought convolutional neural network cnns model suffer performance decline trained discontinuous video frame frequent scenario realworld setting decline primarily result loss temporal continuity crucial understanding semantics human action overcome issue introduce action animationbased augmentation approach pipeline employ series sophisticated technique starting human pose estimation rgb video followed quaternionbased graph convolution network joint orientation trajectory prediction dynamic skeletal interpolation creating smoother diversified action using game engine technology innovative approach generates realistic animation varied game environment viewed multiple viewpoint way method effectively bridge domain gap virtual realworld data experimental evaluation pipeline achieves comparable even superior performance traditional training approach using realworld data requiring original data volume additionally approach demonstrates enhanced performance inthewild video marking significant advancement field action recognition
mumpy multilateral temporalview pyramid transformer video inpainting detection task video inpainting detection expose pixellevel inpainted region within video sequence existing method usually focus leveraging spatial temporal inconsistency however method typically employ fixed operation combine spatial temporal clue limiting applicability different scenario paper introduce novel multilateral temporalview pyramid transformer em mumpy collaborates spatialtemporal clue flexibly method utilizes newly designed multilateral temporalview encoder extract various collaboration spatialtemporal clue introduces deformable windowbased temporalview interaction module enhance diversity collaboration subsequently develop multipyramid decoder aggregate various type feature generate detection map adjusting contribution strength spatial temporal clue method effectively identify inpainted region validate method existing datasets also introduce new challenging largescale video inpainting dataset based youtubevos dataset employ several recent inpainting method result demonstrate superiority method indomain crossdomain evaluation scenario
onthefly point annotation fast medical video labeling purpose medical research deep learning model rely highquality annotated data process often laborious timeconsuming particularly true detection task bounding box annotation required need adjust two corner make process inherently framebyframe given scarcity expert time efficient annotation method suitable clinician needed method propose onthefly method live video annotation enhance annotation efficiency approach continuous singlepoint annotation maintained keeping cursor object live video mitigating need tedious pausing repetitive navigation inherent traditional annotation method novel annotation paradigm inherits point annotation ability generate pseudolabels using pointtobox teacher model empirically evaluate approach developing dataset comparing onthefly annotation time traditional annotation method result using method annotation speed faster traditional annotation technique achieved mean improvement conventional method equivalent annotation budget developed dataset conclusion without bell whistle approach offer significant speedup annotation task easily implemented annotation platform accelerate integration deep learning videobased medical research
skim focus integrating contextual finegrained view repetitive action counting key action counting accurately locating video repetitive action instead estimating probability frame belonging action directly propose dualbranch network ie skimfocusnet working twostep manner model draw inspiration empirical observation indicating human typically engage coarse skimming entire sequence grasp general action pattern initially followed finer framebyframe focus determine aligns target action specifically skimfocusnet incorporates skim branch focus branch skim branch scan global contextual information throughout sequence identify potential target action guidance subsequently focus branch utilizes guidance diligently identify repetitive action using longshort adaptive guidance lsag block additionally observed video existing datasets often feature one type repetitive action inadequately represents realworld scenario accurately describe reallife situation establish multirepcount dataset includes video containing multiple repetitive motion multirepcount skimfoucsnet perform specified action counting enable counting particular action type referencing exemplary video capability substantially exhibit robustness method extensive experiment demonstrate skimfocusnet achieves stateoftheart performance significant improvement also conduct thorough ablation study evaluate network component source code published upon acceptance
unsupervised learning categorylevel pose objectcentric video categorylevel pose estimation fundamentally important problem computer vision robotics eg embodied agent train generative model however far method estimate categorylevel object pose require either large amount human annotation cad model input rgbd sensor contrast tackle problem learning estimate categorylevel pose casually taken objectcentric video without human supervision propose twostep pipeline first introduce multiview alignment procedure determines canonical camera pose across video novel robust cyclic distance formulation geometric appearance matching using reconstructed coarse mesh feature second step canonical pose reconstructed mesh enable u train model pose estimation single image particular model learns estimate dense correspondence image prototypical template predicting pixel image feature vector corresponding vertex template mesh demonstrate method outperforms baseline unsupervised alignment objectcentric video large margin provides faithful robust prediction inthewild code data available
benchmark tracking point introduce new benchmark evaluating task longrange tracking point point tracking two dimension tap many benchmark measuring performance realworld video tapviddavis threedimensional point tracking none end leveraging existing footage build new benchmark point tracking featuring realworld video composed three different data source spanning variety object type motion pattern indoor outdoor environment measure performance task formulate collection metric extend jaccardbased metric used tap handle complexity ambiguous depth scale across model occlusion multitrack spatiotemporal smoothness manually verify large sample trajectory ensure correct video annotation assess current state task constructing competitive baseline using existing tracking model anticipate benchmark serve guidepost improve ability understand precise motion surface deformation monocular video code dataset download generation model evaluation available
comprehensive review fewshot action recognition fewshot action recognition aim address high cost impracticality manually labeling complex variable video data action recognition requires accurately classifying human action video using labeled example per class compared fewshot learning image scenario fewshot action recognition challenging due intrinsic complexity video data recognizing action involves modeling intricate temporal sequence extracting rich semantic information go beyond mere human object identification frame furthermore issue intraclass variance becomes particularly pronounced limited video sample complicating learning representative feature novel action category overcome challenge numerous approach driven significant advancement fewshot action recognition underscore need comprehensive survey unlike early survey focus fewshot image text classification deeply consider unique challenge fewshot action recognition survey review wide variety recent method summarize general framework additionally survey present commonly used benchmark discusses relevant advanced topic promising future direction hope survey serve valuable resource researcher offering essential guidance newcomer stimulating seasoned researcher fresh insight
unqa unified noreference quality assessment audio image video audiovisual content multimedia data flourish internet quality assessment qa multimedia data becomes paramount digital medium application since multimedia data includes multiple modality including audio image video audiovisual av content researcher developed range qa method evaluate quality different modality data exclusively focus addressing single modality qa issue unified qa model handle diverse medium across multiple modality still missing whereas latter better resemble human perception behaviour also wider range application paper propose unified noreference quality assessment model unqa audio image video av content try train single qa model across different medium modality tackle issue inconsistent quality scale among different qa database develop multimodality strategy jointly train unqa multiple qa database based input modality unqa selectively extract spatial feature motion feature audio feature calculates final quality score via four corresponding modality regression module compared existing qa method unqa two advantage multimodality training strategy make qa model learn general robust qualityaware feature representation evidenced superior performance unqa compared stateoftheart qa method unqa reduces number model required assess multimedia data across different modality friendly deploy practical application
dynamic compressive adaptation transformer image video recently remarkable success pretrained vision transformer vits imagetext matching sparked interest imagetovideo adaptation however current approach retain full forward pas frame leading high computation overhead processing entire video paper present inti novel approach compressive imagetovideo adaptation using dynamic interframe token interpolation inti aim softly preserve informative token without disrupting coherent spatiotemporal structure specifically token pair identical position within neighbor frame linearly aggregated new token aggregation weight generated multiscale contextaware network way information neighbor frame adaptively compressed pointbypoint manner thereby effectively reducing number processed frame half time importantly inti seamlessly integrated existing adaptation method achieving strong performance without extracomplex design inti reach accuracy remarkable reduction gflops compared naive adaptation combined additional temporal module inti achieves accuracy reduction gflops similar conclusion verified common datasets
classification endoscopy video capsule image using cnntransformer model gastrointestinal cancer leading cause cancerrelated incidence death making crucial develop novel computeraided diagnosis system early detection enhanced treatment traditional approach rely expertise gastroenterologist identify disease however process subjective interpretation vary even among expert clinician considering recent advancement classifying gastrointestinal anomaly landmark endoscopic video capsule endoscopy image study proposes hybrid model combine advantage transformer convolutional neural network cnns enhance classification performance model utilizes cnn branch extract local feature integrates swin transformer branch global feature understanding combining perform classification task gastrovision dataset proposed model demonstrates excellent performance precision recall score accuracy matthew correlation coefficient mcc respectively showcasing robustness class imbalance surpassing cnns well swin transformer model similarly kvasircapsule large video capsule endoscopy dataset model outperforms others achieving overall precision recall score accuracy mcc moreover generated saliency map explain model focus area demonstrating reliable decisionmaking process result underscore potential hybrid cnntransformer model aiding early accurate detection gastrointestinal gi anomaly
cascaded temporal updating network efficient video superresolution existing video superresolution vsr method generally adopt recurrent propagation network extract spatiotemporal information entire video sequence exhibiting impressive performance however key component recurrentbased vsr network significantly impact model efficiency eg alignment module occupies substantial portion model parameter bidirectional propagation mechanism significantly amplifies inference time consequently developing compact efficient vsr method deployed resourceconstrained device eg smartphones remains challenging end propose cascaded temporal updating network ctun efficient vsr first develop implicit cascaded alignment module explore spatiotemporal correspondence adjacent frame moreover propose unidirectional propagation updating network efficiently explore longrange temporal information crucial highquality video reconstruction specifically develop simple yet effective hidden updater leverage future information update hidden feature forward propagation significantly reducing inference time maintaining performance finally formulate component endtoend trainable vsr network extensive experimental result show ctun achieves favorable tradeoff efficiency performance compared existing method notably compared basicvsr method obtains better result employing parameter running time source code pretrained model available httpsgithubcomhouseleoctun
learning play video game intuitive physic prior video game playing extremely structured domain algorithmic decisionmaking tested without adverse realworld consequence prevailing method rely image input avoid problem handcrafting state space representation approach systematically diverges way human actually learn play game paper design objectbased input representation generalize well across number video game using representation evaluate agent ability learn game similar infant limited world experience employing simple inductive bias derived intuitive representation physic real world using bias construct object category representation used qlearning algorithm assess well learns play multiple game based observed object affordances result suggest humanlike object interaction setup capably learns play several video game demonstrates superior generalizability particularly unfamiliar object exploring method allow machine learn humancentric way thus incorporating humanlike learning benefit
xprompt multimodal visual prompt video object segmentation multimodal video object segmentation vos including rgbthermal rgbdepth rgbevent garnered attention due capability address challenging scenario traditional vos method struggle extreme illumination rapid motion background distraction existing approach often involve designing specific additional branch performing fullparameter finetuning fusion task however paradigm duplicate research effort hardware cost also risk model collapse limited multimodal annotated data paper propose universal framework named xprompt multimodal video object segmentation task designated rgbx xprompt framework first pretrains video object segmentation foundation model using rgb data utilize additional modality prompt adapt downstream multimodal task limited data within xprompt framework introduce multimodal visual prompter mvp allows prompting foundation model various modality segment object precisely propose multimodal adaptation expert maes adapt foundation model pluggable modalityspecific knowledge without compromising generalization capacity evaluate effectiveness xprompt framework conduct extensive experiment task across benchmark proposed universal xprompt framework consistently outperforms full finetuning paradigm achieves stateoftheart performance code httpsgithubcompinxueguoxpromptgit
recording dynamic facial microexpressions multifocus camera array present approach utilizing multicamera array system capturing dynamic highresolution video human face improved imaging performance compared traditional singlecamera configuration employing array individual highresolution camera megapixel sensor megapixels total uniquely focus camera different plane across curved surface human face order capture dynamic facial expression postprocessing method stitch together synchronized set image composite video frame multifocus strategy overcomes resolution depthoffield dof limitation capturing macroscopically curved surface human face maintaining high lateral resolution specifically demonstrate setup achieves generally uniform lateral resolution micrometer across composite dof cover entire face fov compared singlefocus configuration almost increase effective dof believe new approach multifocus camera array video set stage future video capture variety dynamic macroscopically curved surface microscopic resolution
flaash flowattention adaptive semantic hierarchical fusion multimodal tobacco content analysis proliferation tobaccorelated content social medium platform pose significant challenge public health monitoring intervention paper introduces novel multimodal deep learning framework named flowattention adaptive semantic hierarchical fusion flaash designed analyze tobaccorelated video content comprehensively flaash address complexity integrating visual textual information shortform video leveraging hierarchical fusion mechanism inspired flow network theory approach incorporates three key innovation including flowattention mechanism capture nuanced interaction visual textual modality adaptive weighting scheme balance contribution different hierarchical level gating mechanism selectively emphasizes relevant feature multifaceted approach enables flaash effectively process analyze diverse tobaccorelated content product showcase usage scenario evaluate flaash multimodal tobacco content analysis dataset mtcad largescale collection tobaccorelated video popular social medium platform result demonstrate significant improvement existing method outperforming stateoftheart approach classification accuracy score temporal consistency proposed method also show strong generalization capability tested standard video questionanswering datasets surpassing current model work contributes intersection public health artificial intelligence offering effective tool analyzing tobacco promotion digital medium
msegvcuq multimodal segmentation enhanced vision foundation model convolutional neural network uncertainty quantification highspeed video phase detection data highspeed video hsv phase detection pd segmentation crucial monitoring vapor liquid microlayer phase industrial process cnnbased model like unet shown success simplified shadowgraphybased twophase flow tpf analysis application complex hsv pd task remains unexplored vision foundation model vfms yet address complexity either shadowgraphybased pd tpf video segmentation existing uncertainty quantification uq method lack pixellevel reliability critical metric like contact line density dry area fraction absence largescale multimodal experimental datasets tailored pd segmentation impedes progress address gap propose msegvcuq hybrid framework integrates unet cnns transformerbased segment anything model sam achieve enhanced segmentation accuracy crossmodality generalization approach incorporates systematic uq robust error assessment introduces first opensource multimodal hsv pd datasets empirical result demonstrate msegvcuq outperforms baseline cnns vfms enabling scalable reliable pd segmentation realworld boiling dynamic
principle visual token efficient video understanding video understanding made huge stride recent year relying largely power transformer architecture notoriously expensive video data highly redundant research improving efficiency become particularly relevant creative solution include token selection merging method succeed reducing cost model maintaining accuracy interesting pattern arises method outperform baseline randomly discarding token paper take closer look phenomenon observe principle nature visual token example observe value token follows clear paretodistribution token remarkably low value carry perceptual information build insight propose lightweight video model lite select small number token effectively outperforming stateoftheart existing baseline across datasets challenging tradeoff computation gflops v accuracy experiment also show lite generalizes across datasets even task without need retraining
selfsupervised video instance segmentation boost geographic entity alignment historical map tracking geographic entity historical map building offer valuable insight cultural heritage urbanization pattern environmental change various historical research endeavor however linking entity across diverse map remains persistent challenge researcher traditionally addressed twostep process detecting entity within individual map associating via heuristicbased postprocessing step paper propose novel approach combine segmentation association geographic entity historical map using video instance segmentation vi method significantly streamlines geographic entity alignment enhances automation however acquiring highquality videoformat training data vi model prohibitively expensive especially historical map often contain hundred thousand geographic entity mitigate challenge explore selfsupervised learning ssl technique enhance vi performance historical map evaluate performance vi model different pretraining configuration introduce novel method generating synthetic video unlabeled historical map image pretraining proposed selfsupervised vi method substantially reduces need manual annotation experimental result demonstrate superiority proposed selfsupervised vi approach achieving improvement ap increase score compared model trained scratch
