text,topic,probability,topic_2,probability_2
video video generative adversarial network fewshot learning based policy gradient development sophisticated models videotovideo synthesis facilitated recent advances deep reinforcement learning generative adversarial networks gans paper propose new deep neural network approach based reinforcement learning unsupervised conditional videotovideo synthesis preserving unique style source video domain approach aims learn mapping source video domain target video domain train model using policy gradient employ convlstm layers capture spatial temporal information designing finegrained gan architecture incorporating spatiotemporal adversarial goals adversarial losses aid content translation preserving style unlike traditional videotovideo synthesis methods requiring paired inputs proposed approach general require paired inputs thus dealing limited videos target domain ie fewshot learning particularly effective experiments show produce temporally coherent video results results highlight potential approach advances videotovideo synthesis,-1,0.0,-1,0.0
contrastive sequentialdiffusion learning nonlinear multiscene instructional video synthesis generated video scenes actioncentric sequence descriptions recipe instructions doityourself projects often include nonlinear patterns next video may need visually consistent immediately preceding video earlier ones current multiscene video synthesis approaches fail meet consistency requirements address propose contrastive sequential video diffusion method selects suitable previously generated scene guide condition denoising process next scene result multiscene video grounded scene descriptions coherent wrt scenes require visual consistency experiments actioncentered data real world demonstrate practicality improved consistency model compared previous work,-1,0.0,-1,0.0
recapture generative video camera controls userprovided videos using masked video finetuning recently breakthroughs video modeling allowed controllable camera trajectories generated videos however methods directly applied userprovided videos generated video model paper present recapture method generating new videos novel camera trajectories single userprovided video method allows us regenerate reference video existing scene motion vastly different angles cinematic camera motion notably using method also plausibly hallucinate parts scene observable reference video method works generating noisy anchor video new camera trajectory using multiview diffusion models depthbased point cloud rendering regenerating anchor video clean temporally consistent reangled video using proposed masked video finetuning technique,1,1.0,1,1.0
consistent dynamic extendable long video generation text texttovideo diffusion models enable generation highquality videos follow text instructions making easy create diverse individual content however existing approaches mostly focus highquality short video generation typically frames ending hardcuts naively extended case long video synthesis overcome limitations introduce autoregressive approach long video generation frames smooth transitions key components arei shortterm memory block called conditional attention module cam conditions current generation features extracted previous chunk via attentional mechanism leading consistent chunk transitions ii longterm memory block called appearance preservation module extracts highlevel scene object features first video chunk prevent model forgetting initial scene iii randomized blending approach enables apply video enhancer autoregressively infinitely long videos without inconsistencies chunks experiments show generates high motion amount contrast competing imagetovideo methods prone video stagnation applied naively autoregressive manner thus propose highquality seamless texttolong video generator outperforms competitors consistency motion code available,-1,0.0,-1,0.0
freelong trainingfree long video generation spectralblend temporal attention video diffusion models made substantial progress various video generation applications however training models long video generation tasks require significant computational data resources posing challenge developing long video diffusion models paper investigates straightforward trainingfree approach extend existing short video diffusion model eg pretrained videos consistent long video generation eg frames preliminary observation found directly applying short video diffusion model generate long videos lead severe video quality degradation investigation reveals degradation primarily due distortion highfrequency components long videos characterized decrease spatial highfrequency components increase temporal highfrequency components motivated propose novel solution named freelong balance frequency distribution long video features denoising process freelong blends lowfrequency components global video features encapsulate entire video sequence highfrequency components local video features focus shorter subsequences frames approach maintains global consistency incorporating diverse highquality spatiotemporal details local videos enhancing consistency fidelity long video generation evaluated freelong multiple base video diffusion models observed significant improvements additionally method supports coherent multiprompt generation ensuring visual coherence seamless transitions scenes,-1,0.0,-1,0.0
svg stereoscopic video generation via denoising frame matrix video generation models demonstrated great capabilities producing impressive monocular videos however generation stereoscopic video remains underexplored propose posefree trainingfree approach generating stereoscopic videos using offtheshelf monocular video generation model method warps generated monocular video camera views stereoscopic baseline using estimated video depth employs novel frame matrix video inpainting framework framework leverages video generation model inpaint frames observed different timestamps views effective approach generates consistent semantically coherent stereoscopic videos without scene optimization model finetuning moreover develop disocclusion boundary reinjection scheme improves quality video inpainting alleviating negative effects propagated disoccluded areas latent space validate efficacy proposed method conducting experiments videos various generative models including sora lumiere walt zeroscope experiments demonstrate method significant improvement previous methods code released,1,0.895218963728503,1,0.895218963728503
collaborative video diffusion consistent multivideo generation camera control research video generation recently made tremendous progress enabling highquality videos generated text prompts images adding control video generation process important goal moving forward recent approaches condition video generation models camera trajectories make strides towards yet remains challenging generate video scene multiple different camera trajectories solutions multivideo generation problem could enable largescale scene generation editable camera trajectories among applications introduce collaborative video diffusion cvd important step towards vision cvd framework includes novel crossvideo synchronization module promotes consistency corresponding frames video rendered different camera poses using epipolar attention mechanism trained top stateoftheart cameracontrol module video generation cvd generates multiple videos rendered different camera trajectories significantly better consistency baselines shown extensive experiments project page httpscollaborativevideodiffusiongithubio,-1,0.0,-1,0.0
dynamicscaler seamless scalable video generation panoramic scenes increasing demand immersive arvr applications spatial intelligence heightened need generate highquality scenelevel panoramic video however video diffusion models constrained limited resolution aspect ratio restricts applicability scenelevel dynamic content synthesis work propose dynamicscaler addressing challenges enabling spatially scalable panoramic dynamic scene synthesis preserves coherence across panoramic scenes arbitrary size specifically introduce offset shifting denoiser facilitating efficient synchronous coherent denoising panoramic dynamic scenes via diffusion model fixed resolution seamless rotating window ensures seamless boundary transitions consistency across entire panoramic space accommodating varying resolutions aspect ratios additionally employ global motion guidance mechanism ensure local detail fidelity global motion continuity extensive experiments demonstrate method achieves superior content motion quality panoramic scenelevel video generation offering trainingfree efficient scalable solution immersive dynamic scene creation constant vram consumption regardless output video resolution project page available urlhttpsdynamicscalerpagesdev,1,1.0,1,1.0
accelerating video diffusion models via distribution matching generative models particularly diffusion models made significant success data synthesis across various modalities including images videos assets however current diffusion models computationally intensive often requiring numerous sampling steps limit practical application especially video generation work introduces novel framework diffusion distillation distribution matching dramatically reduces number inference steps maintainingand potentially improvinggeneration quality approach focuses distilling pretrained diffusion models efficient fewstep generator specifically targeting video generation leveraging combination video gan loss novel score distribution matching loss demonstrate potential generate highquality video frames substantially fewer sampling steps specific proposed method incorporates denoising gan discriminator distil real data pretrained image diffusion model enhance frame quality promptfollowing capabilities experimental results using animatediff teacher model showcase methods effectiveness achieving superior performance four sampling steps compared existing techniques,-1,0.0,-1,0.0
generative adversarial synthesis radar point cloud scenes validation verification automotive radars datasets realistic traffic scenarios required ever laborious acquire paper introduce radar scene synthesis using gans alternative real dataset acquisition simulationbased approaches train pointnet based gan model generate realistic radar point cloud scenes use binary classifier evaluate performance scenes generated using model test set real scenes demonstrate gan model achieves similar performance real scenes test set,-1,0.0,-1,0.0
mimo controllable character video synthesis spatial decomposed modeling character video synthesis aims produce realistic videos animatable characters within lifelike scenes fundamental problem computer vision graphics community works typically require multiview captures percase training severely limits applicability modeling arbitrary characters short time recent methods break limitation via pretrained diffusion models struggle pose generality scene interaction end propose mimo novel framework synthesize character videos controllable attributes ie character motion scene provided simple user inputs also simultaneously achieve advanced scalability arbitrary characters generality novel motions applicability interactive realworld scenes unified framework core idea encode video compact spatial codes considering inherent nature video occurrence concretely lift frame pixels using monocular depth estimators decompose video clip three spatial components ie main human underlying scene floating occlusion hierarchical layers based depth components encoded canonical identity code structured motion code full scene code utilized control signals synthesis process design spatial decomposed modeling enables flexible user control complex motion expression well synthesis scene interactions experimental results demonstrate effectiveness robustness proposed method,-1,0.0,-1,0.0
multisentence video grounding long video generation video generation witnessed great success recently application generating long videos still remains challenging due difficulty maintaining temporal consistency generated videos high memory cost generation tackle problems paper propose brave new idea multisentence video grounding long video generation connecting massive video moment retrieval video generation task first time providing new paradigm long video generation method work summarized three steps design sequential scene text prompts queries video grounding utilizing massive video moment retrieval search video moment segments meet text requirements video database ii based source frames retrieved video moment segments adopt video editing methods create new video content preserving temporal consistency retrieved video since editing conducted segment segment even frame frame largely reduces memory cost iii also attempt video morphing personalized generation methods improve subject consistency long video generation providing ablation experimental results subtasks long video generation approach seamlessly extends development imagevideo editing video morphing personalized generation video grounding long video generation offering effective solutions generating long videos low memory cost,-1,0.0,-1,0.0
image free stepping stone texttovideo generation texttovideo generation trailed behind texttoimage generation terms quality diversity primarily due inherent complexities spatiotemporal modeling limited availability videotext datasets recent texttovideo diffusion models employ image intermediate step significantly enhancing overall performance incurring high training costs paper present novel video diffusion inference pipeline leverage advanced image techniques enhance pretrained texttovideo diffusion models requires additional training instead vanilla texttovideo inference pipeline consists two stages anchor image synthesis anchor imageaugmented texttovideo synthesis correspondingly simple yet effective generationselection strategy employed achieve visuallyrealistic semanticallyfaithful anchor image innovative noiseinvariant video score distillation sampling nivsds developed animate image dynamic video distilling motion knowledge video diffusion models followed video regeneration process refine video extensive experiments show proposed method produces videos higher visual realism textual fidelity furthermore also supports seamlessly integrated existing imagetovideo diffusion models thereby improving overall video quality,11,1.0,11,1.0
exvideo extending video diffusion models via parameterefficient posttuning recently advancements video synthesis attracted significant attention video synthesis models animatediff stable video diffusion demonstrated practical applicability diffusion models creating dynamic visual content emergence sora spotlighted potential video generation technologies nonetheless extension video lengths constrained limitations computational resources existing video synthesis models generate short video clips paper propose novel posttuning methodology video synthesis models called exvideo approach designed enhance capability current video synthesis models allowing produce content extended temporal durations incurring lower training expenditures particular design extension strategies across common temporal model architectures respectively including convolution temporal attention positional embedding evaluate efficacy proposed posttuning approach conduct extension training stable video diffusion model approach augments models capacity generate original number frames requiring gpu hours training dataset comprising videos importantly substantial increase video length doesnt compromise models innate generalization capabilities model showcases advantages generating videos diverse styles resolutions release source code enhanced model publicly,-1,0.0,-1,0.0
turns im real towards robust detection aigenerated videos impressive achievements generative models creating highquality videos raised concerns digital integrity privacy vulnerabilities recent works combat deepfakes videos developed detectors highly accurate identifying gangenerated samples however robustness detectors diffusiongenerated videos generated video creation tools eg sora openai runway pika etc still unexplored paper propose novel framework detecting videos synthesized multiple stateoftheart sota generative models stable video diffusion find sota methods detecting diffusiongenerated images lack robustness identifying diffusiongenerated videos analysis reveals effectiveness detectors diminishes applied outofdomain videos primarily struggle track temporal features dynamic variations frames address abovementioned challenge collect new benchmark video dataset diffusiongenerated videos using sota video creation tools extract representation within explicit knowledge diffusion model video frames train detector cnn lstm architecture evaluation shows framework well capture temporal features frames achieves detection accuracy indomain videos improves accuracy outdomain videos points,4,1.0,4,1.0
create anything multiview video diffusion models present method creating dynamic scenes monocular video leverages multiview video diffusion model trained diverse combination datasets enable novel view synthesis specified camera poses timestamps combined novel sampling approach model transform single monocular video multiview video enabling robust reconstruction via optimization deformable gaussian representation demonstrate competitive performance novel view synthesis dynamic scene reconstruction benchmarks highlight creative capabilities scene generation real generated videos see project page results interactive demos,1,1.0,1,1.0
cono consistency noise injection tuningfree long video diffusion tuningfree long video diffusion proposed generate extendedduration videos enriched content reusing knowledge pretrained short video diffusion model without retraining however works overlook finegrained longterm video consistency modeling resulting limited scene consistency ie unreasonable object background transitions especially multiple text inputs mitigate propose consistency noise injection dubbed cono introduces lookback mechanism enhance finegrained scene transition different video clips designs longterm consistency regularization eliminate content shifts extending video contents noise prediction particular lookback mechanism breaks noise scheduling process three essential parts one internal noise prediction part injected two videoextending parts intending achieve finegrained transition two video clips longterm consistency regularization focuses explicitly minimizing pixelwise distance predicted noises extended video clip original one thereby preventing abrupt scene transitions extensive experiments shown effectiveness strategies performing longvideo generation single multitext prompt conditions project available,2,0.6860564220557998,2,0.6860564220557998
video diffusion models effective generators automatic generation recently attracted widespread attention recent methods greatly accelerated generation speed usually produce lessdetailed objects due limited model capacity data motivated recent advancements video diffusion models introduce leverages world simulation capacity pretrained video diffusion models facilitate generation fully unleash potential video diffusion perceive world introduce geometrical consistency prior extend video diffusion model multiview consistent generator benefiting stateoftheart video diffusion model could finetuned generate orbit frames surrounding object given single image tailored reconstruction pipelines generate highquality meshes gaussians within minutes furthermore method extended scenelevel novel view synthesis achieving precise control camera path sparse input views extensive experiments demonstrate superior performance proposed approach especially terms generation quality multiview consistency code available,-1,0.0,-1,0.0
video worth thousand images exploring latest trends long video generation image may convey thousand words video composed hundreds thousands image frames tells intricate story despite significant progress multimodal large language models mllms generating extended videos remains formidable challenge writing openais sora current stateoftheart system still limited producing videos one minute length limitation stems complexity long video generation requires generative ai techniques approximating density functions essential aspects planning story development maintaining spatial temporal consistency present additional hurdles integrating generative ai divideandconquer approach could improve scalability longer videos offering greater control survey examine current landscape long video generation covering foundational techniques like gans diffusion models video generation strategies largescale training datasets quality metrics evaluating long videos future research areas address limitations existing video generation capabilities believe would serve comprehensive foundation offering extensive information guide future advancements research field long video generation,10,0.8000297836772766,10,0.8000297836772766
videostudio generating consistentcontent multiscene videos recent innovations breakthroughs diffusion models significantly expanded possibilities generating highquality videos given prompts existing works tackle singlescene scenario one video event occurring single background extending generate multiscene videos nevertheless trivial necessitates nicely manage logic preserving consistent visual appearance key content across video scenes paper propose novel framework namely videostudio consistentcontent multiscene video generation technically videostudio leverages large language models llm convert input prompt comprehensive multiscene script benefits logical knowledge learnt llm script scene includes prompt describing event foregroundbackground entities well camera movement videostudio identifies common entities throughout script asks llm detail entity resultant entity description fed texttoimage model generate reference image entity finally videostudio outputs multiscene video generating scene video via diffusion process takes reference images descriptive prompt event camera movement account diffusion model incorporates reference images condition alignment strengthen content consistency multiscene videos extensive experiments demonstrate videostudio outperforms sota video generation models terms visual quality content consistency user preference source code available urlhttpsgithubcomfuchenustcvideostudio,-1,0.0,-1,0.0
human motion generation generating realistic human videos remains challenging task effective methods currently relying human motion sequence control signal existing approaches often use existing motion extracted videos restricts applications specific motion types global scene matching propose novel approach generate human motion sequences conditioned scene image allowing diverse motion adapts different scenes approach utilizes diffusion model accepts scene image text prompt inputs producing motion sequence tailored scene train model collect largescale video dataset featuring singlehuman activities annotating video corresponding human motion target output experiments demonstrate method effectively predicts human motion aligns scene image projection furthermore show generated motion sequence improves human motion quality video synthesis tasks,18,1.0,18,1.0
synthesis dynamic scenes using video diffusion recent frontier computer vision task video generation consists generating timevarying representation scene generate dynamic scenes current methods explicitly model temporal dynamics jointly optimizing consistency across time views scene paper instead investigate whether necessary explicitly enforce multiview consistency time current approaches sufficient model generate representations timestep independently hence propose model leverages video diffusion generate videos first generating seed videos temporal dynamics independently generating representation timestep seed video evaluate two stateoftheart video generation methods find achieves comparable results despite explicitly modeling temporal dynamics ablate quality depends number views generated per frame observe degradation fewer views performance degradation remains minor results thus suggest temporal knowledge may necessary generate highquality dynamic scenes potentially enabling simpler generative algorithms task,1,1.0,1,1.0
spatialdreamer selfsupervised stereo video synthesis monocular input stereo video synthesis monocular input demanding task fields spatial computing virtual reality main challenges task lie insufficiency highquality paired stereo videos training difficulty maintaining spatiotemporal consistency frames existing methods primarily address issues directly applying novel view synthesis nvs techniques video facing limitations inability effectively represent dynamic scenes requirement large amounts training data paper introduce novel selfsupervised stereo video synthesis paradigm via video diffusion model termed spatialdreamer meets challenges headon firstly address stereo video data insufficiency propose depth based video generation module dvg employs forwardbackward rendering mechanism generate paired videos geometric temporal priors leveraging data generated dvg propose refinernet along selfsupervised synthetic framework designed facilitate efficient dedicated training importantly devise consistency control module consists metric stereo deviation strength temporal interaction learning module til geometric temporal consistency ensurance respectively evaluated proposed method various benchmark methods results showcasing superior performance,-1,0.0,-1,0.0
vidpanos generative panoramic videos casual panning videos panoramic image stitching provides unified wideangle view scene extends beyond cameras field view stitching frames panning video panoramic photograph wellunderstood problem stationary scenes objects moving still panorama capture scene present method synthesizing panoramic video casuallycaptured panning video original video captured wideangle camera pose panorama synthesis spacetime outpainting problem aim create full panoramic video length input video consistent completion spacetime volume requires powerful realistic prior video content motion adapt generative video models existing generative models however immediately extend panorama completion show instead apply video generation component panorama synthesis system demonstrate exploit strengths models minimizing limitations system create video panoramas range inthewild scenes including people vehicles flowing water well stationary background features,1,0.9396901793453583,1,0.9396901793453583
generative camera dolly extreme monocular dynamic novel view synthesis accurate reconstruction complex dynamic scenes single viewpoint continues challenging task computer vision current dynamic novel view synthesis methods typically require videos many different camera viewpoints necessitating careful recording setups significantly restricting utility wild well terms embodied ai applications paper propose textbfgcd controllable monocular dynamic view synthesis pipeline leverages largescale diffusion priors given video scene generate synchronous video chosen perspective conditioned set relative camera pose parameters model require depth input explicitly model scene geometry instead performing endtoend videotovideo translation order achieve goal efficiently despite trained synthetic multiview video data zeroshot realworld generalization experiments show promising results multiple domains including robotics object permanence driving environments believe framework potentially unlock powerful applications rich dynamic scene understanding perception robotics interactive video viewing experiences virtual reality,1,1.0,1,1.0
longtake video dataset temporally dense captions efficacy video generation models heavily depends quality training datasets previous video generation models trained short video clips recently increasing interest training long video generation models directly longer videos however lack highquality long videos impedes advancement long video generation promote research long video generation desire new dataset four key features essential training long video generation models long videos covering least seconds longtake videos without cuts large motion diverse contents temporally dense captions achieve introduce new pipeline selecting highquality longtake videos generating temporally dense captions specifically define set metrics quantitatively assess video quality including scene cuts dynamic degrees semanticlevel quality enabling us filter highquality longtake videos large amount source videos subsequently develop hierarchical video captioning pipeline annotate long videos temporallydense captions pipeline curate first longtake video dataset comprising million longtake videos covering seconds annotated temporally dense captions validate effectiveness finetuning video generation models generate long videos dynamic motions believe work significantly contribute future research long video generation,0,1.0,0,1.0
svsgan leveraging gans semantic video synthesis recent years growing interest semantic image synthesis sis use generative adversarial networks gans diffusion models field seen innovations implementation specialized loss functions tailored task diverging general approaches imagetoimage translation concept semantic video synthesis generation temporally coherent realistic sequences images semantic newly formalized paper existing methods already explored aspects field approaches rely generic loss functions designed videotovideo translation require additional data achieve temporal coherence paper introduce svsgan framework specifically designed svs featuring custom architecture loss functions approach includes triplepyramid generator utilizes spade blocks additionally employ unetbased network image discriminator performs semantic segmentation oasis loss combination tailored architecture objective engineering framework aims bridge existing gap sis svs outperforming current stateoftheart models datasets like cityscapes,-1,0.0,-1,0.0
finegained zeroshot video sampling incorporating temporal dimension pretrained image diffusion models video generation prevalent approach however method computationally demanding necessitates largescale video datasets critically heterogeneity image video datasets often results catastrophic forgetting image expertise recent attempts directly extract video snippets image diffusion models somewhat mitigated problems nevertheless methods generate brief video clips simple movements fail capture finegrained motion nongrid deformation paper propose novel zeroshot video sampling algorithm denoted capable directly sampling highquality video clips existing image synthesis methods stable diffusion without training optimization specifically utilizes dependency noise model temporal momentum attention ensure content consistency animation coherence respectively ability enables excel related tasks conditional contextspecialized video generation instructionguided video editing experimental results demonstrate achieves stateoftheart performance zeroshot video generation occasionally outperforming recent supervised methods homepage urlhttpsdensechengithubiozss,-1,0.0,-1,0.0
gaussianstolife textdriven animation gaussian splatting scenes stateoftheart novel view synthesis methods achieve impressive results multiview captures static scenes however reconstructed scenes still lack liveliness key component creating engaging experiences recently novel video diffusion models generate realistic videos complex motion enable animations images however naively used animate scenes lack multiview consistency breathe life static world propose method animating parts highquality scenes gaussian splatting representation key idea leverage powerful video diffusion models generative component model combine robust technique lift videos meaningful motion find contrast prior work enables realistic animations complex preexisting scenes enables animation large variety object classes related work mostly focused priorbased character animation single objects model enables creation consistent immersive experiences arbitrary scenes,1,1.0,1,1.0
driving scene synthesis freeform trajectories generative prior driving scene synthesis along freeform trajectories essential driving simulations enable closedloop evaluation endtoend driving policies existing methods excel novel view synthesis recorded trajectories face challenges novel trajectories due limited views driving videos vastness driving environments tackle challenge propose novel freeform driving view synthesis approach dubbed drivex leveraging video generative prior optimize model across variety trajectories concretely crafted inverse problem enables video diffusion model utilized prior manytrajectory optimization parametric model eg gaussian splatting seamlessly use generative prior iteratively conduct process optimization resulting model produce highfidelity virtual driving environments outside recorded trajectory enabling freeform trajectory driving simulation beyond real driving scenes drivex also utilized simulate virtual driving worlds aigenerated videos,16,1.0,16,1.0
highfidelity texttovideo synthesis compressed representations present texttovideo generation model capable producing realistic scenes textual descriptions building recent advancements openais sora explore latent diffusion model ldm architecture introduce video variational autoencoder vidvae vidvae compresses video data spatially temporally significantly reducing length visual tokens computational demands associated generating longsequence videos address computational costs propose divideandmerge strategy maintains temporal consistency across video segments diffusion transformer dit model incorporates spatial temporal selfattention layers enabling robust generalization across different timeframes aspect ratios devised data processing pipeline beginning collected highquality videotext pairs pipeline includes multiple steps clipping text detection motion estimation aesthetics scoring dense captioning based inhouse videollm model training vidvae dit models required approximately days respectively model supports video generation endtoend way demonstrates competitive performance stateoftheart models,-1,0.0,-1,0.0
progressive autoregressive video diffusion models current frontier video diffusion models demonstrated remarkable results generating highquality videos however generate short video clips normally around seconds frames due computation limitations training work show existing models naturally extended autoregressive video diffusion models without changing architectures key idea assign latent frames progressively increasing noise levels rather single noise level allows finegrained condition among latents large overlaps attention windows progressive video denoising allows models autoregressively generate video frames without quality degradation abrupt scene changes present stateoftheart results long video generation minute frames fps videos paper available httpsdesaixiegithubiopavdm,-1,0.0,-1,0.0
drivegenvlm realworld video generation vision language model based autonomous driving advancement autonomous driving technologies necessitates increasingly sophisticated methods understanding predicting realworld scenarios vision language models vlms emerging revolutionary tools significant potential influence autonomous driving paper propose drivegenvlm framework generate driving videos use vlms understand achieve employ video generation framework grounded denoising diffusion probabilistic models ddpm aimed predicting realworld video sequences explore adequacy generated videos use vlms employing pretrained model known efficient incontext learning egocentric videos eilev diffusion model trained waymo open dataset evaluated using frechet video distance fvd score ensure quality realism generated videos corresponding narrations provided eilev generated videos may beneficial autonomous driving domain narrations enhance traffic scene understanding aid navigation improve planning capabilities integration video generation vlms drivegenvlm framework represents significant step forward leveraging advanced ai models address complex challenges autonomous driving,16,1.0,16,1.0
raccoon versatile instructional video editing framework autogenerated narratives recent video generative models primarily rely carefully written text prompts specific tasks like inpainting style editing require laborintensive textual descriptions input videos hindering flexibility adapt personalraw videos user specifications paper proposes raccoon versatile userfriendly videotoparagraphtovideo generative framework supports multiple video editing capabilities removal addition modification unified pipeline raccoon consists two principal stages videotoparagraph paragraphtovideo stage automatically describe video scenes wellstructured natural language capturing holistic context focused object details subsequently stage users optionally refine descriptions guide video diffusion model enabling various modifications input video removing changing subjects andor adding new objects proposed approach stands methods several significant contributions raccoon suggests multigranular spatiotemporal pooling strategy generate wellstructured video descriptions capturing broad context object details without requiring complex human annotations simplifying precise video content editing based text users video generative model incorporates autogenerated narratives instructions enhance quality accuracy generated content raccoon also plans imagine new objects given video users simply prompt model receive detailed video editing plan complex video editing proposed framework demonstrates impressive versatile capabilities videotoparagraph generation video content editing incorporated sota video generative models enhancement,15,1.0,15,1.0
illumination histogram consistency metric quantitative assessment video sequences advances deep generative models greatly accelerate process video procession video enhancement synthesis learning spatiotemporal video models requires capture temporal dynamics scene addition visual appearance individual frames illumination consistency reflects variations illumination dynamic video sequences play vital role video processing unfortunately date wellaccepted quantitative metric proposed video illumination consistency evaluation paper propose illumination histogram consistency ihc metric quantitatively automatically evaluate illumination consistency video sequences ihc measures illumination variation video sequence based illumination histogram discrepancies across frames video sequence specifically given video sequence first estimate illumination map individual frame using retinex model using illumination maps mean illumination histogram video sequence computed mean operation across frames next compute illumination histogram discrepancy individual frame mean illumination histogram sum illumination histogram discrepancies represent illumination variations video sequence finally obtain ihc score illumination histogram discrepancies via normalization subtraction operations experiments conducted illustrate performance proposed ihc metric capability measure illumination variations video sequences source code available urlhttpsgithubcomlongchencvihcmetric,-1,0.0,-1,0.0
dynamic content generation multiframe multiview consistency present stable video latent video diffusion model multiframe multiview consistent dynamic content generation unlike previous methods rely separately trained generative models video generation novel view synthesis design unified diffusion model generate novel view videos dynamic objects specifically given monocular reference video generates novel views video frame temporally consistent use generated novel view videos optimize implicit representation dynamic nerf efficiently without need cumbersome sdsbased optimization used prior works train unified novel view video generation model curate dynamic object dataset existing objaverse dataset extensive experimental results multiple datasets user studies demonstrate stateoftheart performance novelview video synthesis well generation compared prior works,1,0.9297607770797686,1,0.9297607770797686
analyzing improving camera control video diffusion transformers numerous works recently integrated camera control foundational texttovideo models resulting camera control often imprecise video generation quality suffers work analyze camera motion first principles perspective uncovering insights enable precise camera manipulation without compromising synthesis quality first determine motion induced camera movements videos lowfrequency nature motivates us adjust train test pose conditioning schedules accelerating training convergence improving visual motion quality probing representations unconditional video diffusion transformer observe implicitly perform camera pose estimation hood subportion layers contain camera information suggested us limit injection camera conditioning subset architecture prevent interference video features leading reduction training parameters improved training speed higher visual quality finally complement typical dataset camera control learning curated dataset diverse dynamic videos stationary cameras helps model distinguish camera scene motion improves dynamics generated poseconditioned videos compound findings design advanced camera control architecture new stateoftheart model generative video modeling camera control,-1,0.0,-1,0.0
redefining temporal modeling video diffusion vectorized timestep approach diffusion models revolutionized image generation extension video generation shown promise however current video diffusion modelsvdms rely scalar timestep variable applied clip level limits ability model complex temporal dependencies needed various tasks like imagetovideo generation address limitation propose frameaware video diffusion modelfvdm introduces novel vectorized timestep variablevtv unlike conventional vdms approach allows frame follow independent noise schedule enhancing models capacity capture finegrained temporal dependencies fvdms flexibility demonstrated across multiple tasks including standard video generation imagetovideo generation video interpolation long video synthesis diverse set vtv configurations achieve superior quality generated videos overcoming challenges catastrophic forgetting finetuning limited generalizability zeroshot methodsour empirical evaluations show fvdm outperforms stateoftheart methods video generation quality also excelling extended tasks addressing fundamental shortcomings existing vdms fvdm sets new paradigm video synthesis offering robust framework significant implications generative modeling multimedia applications,11,0.9235240508025361,11,0.9235240508025361
nvssolver video diffusion model zeroshot novel view synthesizer harnessing potent generative capabilities pretrained large video diffusion models propose nvssolver new novel view synthesis nvs paradigm operates textitwithout need training nvssolver adaptively modulates diffusion sampling process given views enable creation remarkable visual experiences single multiple views static scenes monocular videos dynamic scenes specifically built upon theoretical modeling iteratively modulate score function given scene priors represented warped input views control video diffusion process moreover theoretically exploring boundary estimation error achieve modulation adaptive fashion according view pose number diffusion steps extensive evaluations static dynamic scenes substantiate significant superiority nvssolver stateoftheart methods quantitatively qualitatively textit source code,11,0.9321277318126886,11,0.9321277318126886
zeroshot scene texturing video diffusion models meshes widely used computer vision graphics efficiency animation minimal memory use playing crucial role movies games ar vr however creating temporally consistent realistic textures mesh sequences remains laborintensive professional artists hand video diffusion models excel textdriven video generation often lack geometry awareness struggle achieving multiview consistent texturing meshes work present zeroshot approach integrates inherent geometry knowledge mesh sequences expressiveness video diffusion models produce multiview temporally consistent textures given untextured mesh sequence text prompt inputs method enhances multiview consistency synchronizing diffusion process across different views latent aggregation uv space ensure temporal consistency leverage prior knowledge conditional video generation model texture synthesis however straightforwardly combining video diffusion model uv texture aggregation leads blurry results analyze underlying causes propose simple yet effective modification ddim sampling process address issue additionally introduce reference latent texture strengthen correlation frames denoising process best knowledge first method specifically designed scene texturing extensive experiments demonstrate superiority producing multiview multiframe consistent videos based untextured mesh sequences,-1,0.0,-1,0.0
video diffusion transformers incontext learners paper investigates solution enabling incontext capabilities video diffusion transformers minimal tuning required activation specifically propose simple pipeline leverage incontext generation textbfi concatenate videos along spacial time dimension textbfii jointly caption multiscene video clips one source textbfiii apply taskspecific finetuning using carefully curated small datasets series diverse controllable tasks demonstrate qualitatively existing advanced texttovideo models effectively perform incontext generation notably allows creation consistent multiscene videos exceeding seconds duration without additional computational overhead importantly method requires modifications original models results highfidelity video outputs better align prompt specifications maintain role consistency framework presents valuable tool research community offers critical insights advancing productlevel controllable video generation systems data code model weights publicly available httpsgithubcomfeizcvideoincontext,-1,0.0,-1,0.0
controllable panorama video generation video diffusion model panorama video recently attracts interest study application courtesy immersive experience due expensive cost capturing panoramic videos generating desirable panorama videos prompts urgently required lately emerging texttovideo diffusion methods demonstrate notable effectiveness standard video generation however due significant gap content motion patterns panoramic standard videos methods encounter challenges yielding satisfactory panoramic videos paper propose pipeline named video diffusion model generating panoramic videos based given prompts motion conditions specifically introduce lightweight accompanied enhancement techniques transform pretrained models panorama video generation propose new panorama dataset named consisting panoramic videotext pairs training addressing absence captioned panoramic video datasets extensive experiments demonstrate superiority effectiveness panorama video generation project page,1,0.9930240152310875,1,0.9930240152310875
divot diffusion powers video tokenizer comprehension generation recent years significant surge interest unifying image comprehension generation within large language models llms growing interest prompted us explore extending unification videos core challenge lies developing versatile video tokenizer captures spatial characteristics temporal dynamics videos obtain representations llms representations decoded realistic video clips enable video generation work introduce divot diffusionpowered video tokenizer leverages diffusion process selfsupervised video representation learning posit video diffusion model effectively denoise video clips taking features video tokenizer condition tokenizer successfully captured robust spatial temporal information additionally video diffusion model inherently functions detokenizer decoding videos representations building upon divot tokenizer present divotvicuna videototext autoregression texttovideo generation modeling distributions continuousvalued divot features gaussian mixture model experimental results demonstrate diffusionbased video tokenizer integrated pretrained llm achieves competitive performance across various video comprehension generation benchmarks instruction tuned divotvicuna also excels video storytelling generating interleaved narratives corresponding videos,-1,0.0,-1,0.0
dimensionx create scenes single image controllable video diffusion paper introduce textbfdimensionx framework designed generate photorealistic scenes single image video diffusion approach begins insight spatial structure scene temporal evolution scene effectively represented sequences video frames recent video diffusion models shown remarkable success producing vivid visuals face limitations directly recovering scenes due limited spatial temporal controllability generation overcome propose stdirector decouples spatial temporal factors video diffusion learning dimensionaware loras dimensionvariant data controllable video diffusion approach enables precise manipulation spatial structure temporal dynamics allowing us reconstruct representations sequential frames combination spatial temporal dimensions additionally bridge gap generated videos realworld scenes introduce trajectoryaware mechanism generation identitypreserving denoising strategy generation extensive experiments various realworld synthetic datasets demonstrate dimensionx achieves superior results controllable video generation well scene generation compared previous methods,-1,0.0,-1,0.0
osv one step enough highquality image video generation video diffusion models shown great potential generating highquality videos making increasingly popular focus however inherent iterative nature leads substantial computational time costs efforts made accelerate video diffusion reducing inference steps techniques like consistency distillation gan training approaches often fall short either performance training stability work introduce twostage training framework effectively combines consistency distillation gan training address challenges additionally propose novel video discriminator design eliminates need decoding video latents improves final performance model capable producing highquality videos merely onestep flexibility perform multistep refinement performance enhancement quantitative evaluation benchmark shows model significantly outperforms existing methods notably performancefvd exceeds performance consistency distillation based method animatelcm fvd approaches performance advanced stable video diffusion fvd,-1,0.0,-1,0.0
lvcd referencebased lineart video colorization diffusion models propose first video diffusion framework referencebased lineart video colorization unlike previous works rely solely image generative models colorize lineart frame frame approach leverages largescale pretrained video diffusion model generate colorized animation videos approach leads temporally consistent results better equipped handle large motions firstly introduce sketchguided controlnet provides additional control finetune imagetovideo diffusion model controllable video synthesis enabling generation animation videos conditioned lineart propose reference attention facilitate transfer colors reference frame frames containing fast expansive motions finally present novel scheme sequential sampling incorporating overlapped blending module prevreference attention extend video diffusion model beyond original fixedlength limitation long video colorization qualitative quantitative results demonstrate method significantly outperforms stateoftheart techniques terms frame video quality well temporal consistency moreover method capable generating highquality long temporalconsistent animation videos large motions achievable previous works code model available httpsluckyhztgithubiolvcd,-1,0.0,-1,0.0
highresolution long video generation autonomous driving adaptive control rapid advancement diffusion models greatly improved video synthesis especially controllable video generation vital applications like autonomous driving although dit vae become standard framework video generation introduces challenges controllable driving video generation especially geometry control rendering existing control methods ineffective address issues propose novel approach integrates mvdit block spatialtemporal conditional encoding enable multiview video generation precise geometric control additionally introduce efficient method obtaining contextual descriptions videos support diverse textual control along progressive training strategy using mixed video data enhance training efficiency generalizability consequently enables multiview driving video synthesis resolution frame count compared current sota rich contextual control geometric controls extensive experiments demonstrate ability unlocking broader applications autonomous driving,16,0.9029518853928603,16,0.9029518853928603
vstar generative temporal nursing longer dynamic video synthesis despite tremendous progress field texttovideo synthesis opensourced diffusion models struggle generate longer videos dynamically varying evolving content tend synthesize quasistatic videos ignoring necessary visual changeovertime implied text prompt time scaling models enable longer dynamic video synthesis often remains computationally intractable address challenge introduce concept generative temporal nursing gtn aim alter generative process fly inference improve control temporal dynamics enable generation longer videos propose method gtn dubbed vstar consists two key ingredients video synopsis prompting vsp automatic generation video synopsis based original single prompt leveraging llms gives accurate textual guidance different visual states longer videos temporal attention regularization tar regularization technique refine temporal attention units pretrained diffusion models enables control video dynamics experimentally showcase superiority proposed approach generating longer visually appealing videos existing opensourced models additionally analyze temporal attention maps realized without vstar demonstrating importance applying method mitigate neglect desired visual change time,-1,0.0,-1,0.0
ctrlv higher fidelity video generation boundingbox controlled object motion controllable video generation attracted significant attention largely due advances video diffusion models domains autonomous driving essential develop highly accurate predictions object motions paper tackles crucial challenge exert precise control object motion realistic video synthesis accomplish control object movements using bounding boxes extend control renderings boxes pixel space employ distinct specialized model forecast trajectories object bounding boxes based previous desired future positions adapt enhance separate video diffusion network create video content based high quality trajectory forecasts method ctrlv leverages modified finetuned stable video diffusion svd models solve trajectory video generation extensive experiments conducted kitti virtualkitti nuscenes datasets validate effectiveness approach producing realistic controllable video generation,-1,0.0,-1,0.0
read watch scream sound generation text video despite impressive progress multimodal generative models videotoaudio generation still suffers limited performance limits flexibility prioritize sound synthesis specific objects within scene conversely texttoaudio generation methods generate highquality audio pose challenges ensuring comprehensive scene depiction timevarying control tackle challenges propose novel videoandtexttoaudio generation method called video serves conditional control texttoaudio generation model especially method estimates structural information sound namely energy video receiving key content cues user prompt employ wellperforming texttoaudio model consolidate video control much efficient training multimodal diffusion models massive tripletpaired audiovideotext data addition separating generative components audio becomes flexible system allows users freely adjust energy surrounding environment primary sound source according preferences experimental results demonstrate method shows superiority terms quality controllability training efficiency code demo available httpsnaveraigithubiorewas,8,0.9764208793506434,8,0.9764208793506434
sfv single forward video generation model diffusionbased video generation models demonstrated remarkable success obtaining highfidelity videos iterative denoising process however models require multiple denoising steps sampling resulting high computational costs work propose novel approach obtain singlestep video generation models leveraging adversarial training finetune pretrained video diffusion models show adversarial training multisteps video diffusion model ie stable video diffusion svd trained perform single forward pass synthesize highquality videos capturing temporal spatial dependencies video data extensive experiments demonstrate method achieves competitive generation quality synthesized videos significantly reduced computational overhead denoising process ie around speedup compared svd speedup compared existing works even better generation quality paving way realtime video synthesis editing visualization results made publicly available httpssnapresearchgithubiosfv,-1,0.0,-1,0.0
towards photorealistic scene generation via video diffusion models existing dynamic scene generation methods mostly rely distilling knowledge pretrained generative models typically finetuned synthetic object datasets result generated scenes often objectcentric lack photorealism address limitations introduce novel pipeline designed photorealistic scene generation discarding dependency multiview generative models instead fully utilizing video generative models trained diverse realworld datasets method begins generating reference video using video generation model learn canonical representation video using freezetime video delicately generated reference video handle inconsistencies freezetime video jointly learn perframe deformation model imperfections learn temporal deformation based canonical representation capture dynamic interactions reference video pipeline facilitates generation dynamic scenes enhanced photorealism structural integrity viewable multiple perspectives thereby setting new standard scene generation,1,1.0,1,1.0
sned superposition network architecture search efficient video diffusion model aigenerated content garnered significant attention achieving photorealistic video synthesis remains formidable challenge despite promising advances diffusion models video generation quality complex model architecture substantial computational demands training inference create significant gap models realworld applications paper presents sned superposition network architecture search method efficient video diffusion model method employs supernet training paradigm targets various model cost resolution options using weightsharing method moreover propose supernet training sampling warmup fast training optimization showcase flexibility method conduct experiments involving pixelspace latentspace video diffusion models results demonstrate framework consistently produces comparable results across different model options high efficiency according experiment pixelspace video diffusion model achieve consistent video generation results simultaneously across x x resolutions large range model sizes number parameters pixelspace video diffusion models,-1,0.0,-1,0.0
videorepair improving texttovideo generation via misalignment evaluation localized refinement recent texttovideo diffusion models demonstrated impressive generation capabilities across various domains however models often generate videos misalignments text prompts especially prompts describe complex scenes multiple objects attributes address introduce videorepair novel modelagnostic trainingfree video refinement framework automatically identifies finegrained textvideo misalignments generates explicit spatial textual feedback enabling diffusion model perform targeted localized refinements videorepair consists two stages video refinement planning first detect misalignments generating finegrained evaluation questions answering using mllm based video evaluation outputs identify accurately generated objects construct localized prompts precisely refine misaligned regions localized refinement enhance video alignment repairing misaligned regions original video preserving correctly generated areas achieved framewise region decomposition using regionpreserving segmentation rps module two popular video generation benchmarks evalcrafter videorepair substantially outperforms recent baselines across various textvideo alignment metrics provide comprehensive analysis videorepair components qualitative examples,-1,0.0,-1,0.0
lumiere spacetime diffusion model video generation introduce lumiere texttovideo diffusion model designed synthesizing videos portray realistic diverse coherent motion pivotal challenge video synthesis end introduce spacetime unet architecture generates entire temporal duration video single pass model contrast existing video models synthesize distant keyframes followed temporal superresolution approach inherently makes global temporal consistency difficult achieve deploying spatial importantly temporal upsampling leveraging pretrained texttoimage diffusion model model learns directly generate fullframerate lowresolution video processing multiple spacetime scales demonstrate stateoftheart texttovideo generation results show design easily facilitates wide range content creation tasks video editing applications including imagetovideo video inpainting stylized generation,11,0.9235240508025361,11,0.9235240508025361
comparative analysis generative models enhancing image synthesis vaes gans stable diffusion paper examines three major generative modelling frameworks variational autoencoders vaes generative adversarial networks gans stable diffusion models vaes effective learning latent representations frequently yield blurry results gans generate realistic images face issues mode collapse stable diffusion models producing highquality images strong semantic coherence demanding terms computational resources additionally paper explores incorporating grounding dino grounded sam stable diffusion improves image accuracy utilising sophisticated segmentation inpainting techniques analysis guides selecting suitable models various applications highlights areas research,13,0.9149390547725647,13,0.9149390547725647
searching priors makes texttovideo synthesis better significant advancements video diffusion models brought substantial progress field texttovideo synthesis however existing synthesis model struggle accurately generate complex motion dynamics leading reduction video realism one possible solution collect massive data train model would extremely expensive alleviate problem paper reformulate typical generation process searchbased generation pipeline instead scaling model training employ existing videos motion prior database specifically divide generation process two steps given prompt input search existing textvideo datasets find videos text labels closely match prompt motions propose tailored search algorithm emphasizes object motion features ii retrieved videos processed distilled motion priors finetune pretrained base model followed generating desired videos using input prompt utilizing priors gleaned searched videos enhance realism generated videos motion operations finished single nvidia rtx gpu validate method stateoftheart models across diverse prompt inputs code public,-1,0.0,-1,0.0
multimodal semantic communication generative audiodriven video conferencing paper studies efficient multimodal data communication scheme video conferencing considered system speaker gives talk audiences talking head video audio transmitted since speaker frequently change posture highfidelity transmission audio speech music required redundant visual video data exists removed generating video audio end propose wavetovideo system efficient video transmission framework reduces transmitted data generating talking head video audio particular fullduration audio shortduration video data synchronously transmitted wireless channel neural networks nns extracting encoding audio video semantics receiver combines decoded audio video data well uses generative adversarial network gan based model generate lip movement videos speaker simulation results show proposed system reduce amount transmitted data maintaining perceptual quality generated conferencing video,-1,0.0,-1,0.0
matten video generation mambaattention paper introduce matten cuttingedge latent diffusion model mambaattention architecture video generation minimal computational cost matten employs spatialtemporal attention local video content modeling bidirectional mamba global video content modeling comprehensive experimental evaluation demonstrates matten competitive performance current transformerbased ganbased models benchmark performance achieving superior fvd scores efficiency additionally observe direct positive correlation complexity designed model improvement video quality indicating excellent scalability matten,-1,0.0,-1,0.0
realistic geometryaware transition compositional synthesis recent advances diffusion models demonstrated exceptional capabilities image video generation improving effectiveness synthesis existing generation methods generate highquality objects scenes based userfriendly conditions benefiting gaming video industries however methods struggle synthesize significant object deformation complex transitions interactions within scenes address challenge propose novel synthesis framework enables realistic complex scene transitions specifically first use multimodal large language models mllms produce physicaware scene description scene initialization effective transition timing planning propose geometryaware transition network realize complex scenelevel transition based plan involves expressive geometrical object deformation extensive experiments demonstrate consistently outperforms existing stateoftheart methods generating scenes accurate highquality transitions validating effectiveness code,-1,0.0,-1,0.0
zerosmooth trainingfree diffuser adaptation high frame rate video generation video generation made remarkable progress recent years especially since advent video diffusion models many video generation models produce plausible synthetic videos eg stable video diffusion svd however video models generate low frame rate videos due limited gpu memory well difficulty modeling large set frames training videos always uniformly sampled specified interval temporal compression previous methods promote frame rate either training video interpolation model pixel space postprocessing stage training interpolation model latent space specific base video model paper propose trainingfree video interpolation method generative video diffusion models generalizable different models plugandplay manner investigate nonlinearity feature space video diffusion models transform video model selfcascaded video diffusion model incorporating designed hidden state correction modules selfcascaded architecture correction module proposed retain temporal consistency key frames interpolated frames extensive evaluations preformed multiple popular video models demonstrate effectiveness propose method especially trainingfree method even comparable trained interpolation models supported huge compute resources largescale datasets,-1,0.0,-1,0.0
amg avatar motion guided video generation human video generation task gained significant attention advancement deep generative models generating realistic videos human movements challenging nature due intricacies human body topology sensitivity visual artifacts extensively studied media generation methods take advantage massive human media datasets struggle control whereas avatarbased approaches offering freedom control lack photorealism harmonized seamlessly background scene propose amg method combines photorealism controllability conditioning video diffusion models controlled rendering avatars additionally introduce novel data processing pipeline reconstructs renders human avatar movements dynamic camera videos amg first method enables multiperson diffusion video generation precise control camera positions human motions background style also demonstrate extensive evaluation outperforms existing human video generation methods conditioned pose sequences driving videos terms realism adaptability,1,1.0,1,1.0
hybrid video diffusion models triplane wavelet representation generating highquality videos synthesize desired realistic content challenging task due intricate highdimensionality complexity videos several recent diffusionbased methods shown comparable performance compressing videos lowerdimensional latent space using traditional video autoencoder architecture however method employ standard framewise convolution fail fully exploit spatiotemporal nature videos address issue propose novel hybrid video diffusion model called hvdm capture spatiotemporal dependencies effectively hvdm trained hybrid video autoencoder extracts disentangled representation video including global context information captured projected latent ii local volume information captured convolutions wavelet decomposition iii frequency information improving video reconstruction based disentangled representation hybrid autoencoder provide comprehensive video latent enriching generated videos fine structures details experiments video generation benchamarks skytimelapse taichi demonstrate proposed approach achieves stateoftheart video generation quality showing wide range video applications eg long video generation imagetovideo video dynamics control,2,0.7829341467962297,2,0.7829341467962297
moviebench hierarchical movie level dataset long video generation recent advancements video generation models like stable video diffusion show promising results primarily focus short singlescene videos models struggle generating long videos involve multiple scenes coherent narratives consistent characters furthermore publicly available dataset tailored analysis evaluation training long video generation models paper present moviebench hierarchical movielevel dataset long video generation addresses challenges providing unique contributions movielength videos featuring rich coherent storylines multiscene narratives consistency character appearance audio across scenes hierarchical data structure contains highlevel movie information detailed shotlevel descriptions experiments demonstrate moviebench brings new insights challenges maintaining character id consistency across multiple scenes various characters dataset public continuously maintained aiming advance field long video generation data found httpsweijiawugithubiomoviebench,-1,0.0,-1,0.0
dreamrunner finegrained compositional storytovideo generation retrievalaugmented motion adaptation storytelling video generation svg aims produce coherent visually rich multiscene videos follow structured narrative existing methods primarily employ llm highlevel planning decompose story scenelevel descriptions independently generated stitched together however approaches struggle generating highquality videos aligned complex singlescene description visualizing complex description involves coherent composition multiple characters events complex motion synthesis muticharacter customization address challenges propose dreamrunner novel storytovideo generation method first structure input script using large language model llm facilitate coarsegrained scene planning well finegrained objectlevel layout motion planning next dreamrunner presents retrievalaugmented testtime adaptation capture target motion priors objects scene supporting diverse motion customization based retrieved videos thus facilitating generation new videos complex scripted motions lastly propose novel spatialtemporal regionbased attention prior injection module finegrained objectmotion binding framebyframe semantic control compare dreamrunner various svg baselines demonstrating stateoftheart performance character consistency text alignment smooth transitions additionally dreamrunner exhibits strong finegrained conditionfollowing ability compositional texttovideo generation significantly outperforming baselines finally validate dreamrunners robust ability generate multiobject interactions qualitative examples,15,1.0,15,1.0
ditctrl exploring attention control multimodal diffusion transformer tuningfree multiprompt longer video generation soralike video generation models achieved remarkable progress multimodal diffusion transformer mmdit architecture however current video generation models predominantly focus singleprompt struggling generate coherent scenes multiple sequential prompts better reflect realworld dynamic scenarios pioneering works explored multiprompt video generation face significant challenges including strict training data requirements weak prompt following unnatural transitions address problems propose ditctrl trainingfree multiprompt video generation method mmdit architectures first time key idea take multiprompt video generation task temporal video editing smooth transitions achieve goal first analyze mmdits attention mechanism finding full attention behaves similarly crossselfattention blocks unetlike diffusion models enabling maskguided precise semantic control across different prompts attention sharing multiprompt video generation based careful design video generated ditctrl achieves smooth transitions consistent object motion given multiple sequential prompts without additional training besides also present mpvbench new benchmark specially designed multiprompt video generation evaluate performance multiprompt generation extensive experiments demonstrate method achieves stateoftheart performance without additional training,-1,0.0,-1,0.0
novel multiview synthesis generation single image using latent video diffusion present stable video latent video diffusion model highresolution imagetomultiview generation orbital videos around object recent work generation propose techniques adapt generative models novel view synthesis nvs optimization however methods several disadvantages due either limited views inconsistent nvs thereby affecting performance object generation work propose adapts imagetovideo diffusion model novel multiview synthesis generation thereby leveraging generalization multiview consistency video models adding explicit camera control nvs also propose improved optimization techniques use nvs outputs generation extensive experimental results multiple datasets metrics well user study demonstrate stateoftheart performance nvs well reconstruction compared prior works,1,0.9229937125506151,1,0.9229937125506151
anything scene photorealistic video object insertion realistic video simulation shown significant potential across diverse applications virtual reality film production particularly true scenarios capturing videos realworld settings either impractical expensive existing approaches video simulation often fail accurately model lighting environment represent object geometry achieve high levels photorealism paper propose anything scene novel generic framework realistic video simulation seamlessly inserts object existing dynamic video strong emphasis physical realism proposed general framework encompasses three key processes integrating realistic object given scene video proper placement ensure geometric realism estimating sky environmental lighting distribution simulating realistic shadows enhance light realism employing style transfer network refines final video output maximize photorealism experimentally demonstrate anything scene framework produces simulated videos great geometric realism lighting realism photorealism significantly mitigating challenges associated video data generation framework offers efficient costeffective solution acquiring highquality videos furthermore applications extend well beyond video data augmentation showing promising potential virtual reality video editing various videocentric applications please check project website httpsanythinginanyscenegithubio access project code highresolution video results,1,1.0,1,1.0
motioncraft physicsbased zeroshot video generation generating videos realistic physically plausible motion one main recent challenges computer vision diffusion models achieving compelling results image generation video diffusion models limited heavy training huge models resulting videos still biased training dataset work propose motioncraft new zeroshot video generator craft physicsbased realistic videos motioncraft able warp noise latent space image diffusion model stable diffusion applying optical flow derived physics simulation show warping noise latent space results coherent application desired motion allowing model generate missing elements consistent scene evolution would otherwise result artefacts missing content flow applied pixel space compare method stateoftheart reporting qualitative quantitative improvements demonstrating effectiveness approach generate videos finelyprescribed complex motion dynamics project page httpsmezzelfogithubiomotioncraft,-1,0.0,-1,0.0
comuni decomposing common unique video signals diffusionbased video generation since videos record objects moving coherently adjacent video frames commonness similar object appearances uniqueness slightly changed postures prevent redundant modeling common video signals propose novel diffusionbased framework named comuni decomposes common unique video signals enable efficient video generation approach separates decomposition video signals task video generation thus reducing computation complexity generative models particular introduce cuvae decompose video signals encode latent features train cuvae selfsupervised manner employ cascading merge module reconstitute video signals timeagnostic video decoder reconstruct video frames propose culdm model latent features video generation adopts two specific diffusion streams simultaneously model common unique latent features utilize additional joint modules cross modeling common unique latent features novel position embedding method ensure content consistency motion coherence generated videos position embedding method incorporates spatial temporal absolute position information joint modules extensive experiments demonstrate necessity decomposing common unique video signals video generation effectiveness efficiency proposed method,-1,0.0,-1,0.0
aligned monocular depth estimation dynamic videos recent developments monocular depth estimation methods enable highquality depth estimation singleview images fail estimate consistent video depth across different frames recent works address problem applying video diffusion model generate video depth conditioned input video trainingexpensive produce scaleinvariant depth values without camera poses paper propose novel videodepth estimation method called estimate temporal consistent depth maps dynamic video key idea utilize recent model align estimated monocular depth maps different timesteps first finetune model additional estimated monocular depth inputs dynamic scenes apply optimization reconstruct depth maps camera poses extensive experiments demonstrate estimates consistent video depth camera poses monocular video superior performance baseline methods,1,1.0,1,1.0
msg score comprehensive evaluation multiscene video generation paper addresses metrics required generating multiscene videos based continuous scenario opposed traditional short video generation scenariobased videos require comprehensive evaluation considers multiple factors character consistency artistic coherence aesthetic quality alignment generated content intended prompt additionally video generation unlike single images movement characters across frames introduces potential issues like distortion unintended changes must effectively evaluated corrected context probabilistic models like diffusion generating desired scene requires repeated sampling manual selection akin film director chooses best shots numerous takes propose scorebased evaluation benchmark automates process enabling objective efficient assessment complexities approach allows generation highquality multiscene videos selecting best outcomes based automated scoring rather manual inspection,-1,0.0,-1,0.0
continuous video process modeling videos continuous multidimensional processes video prediction diffusion models made significant strides image generation mastering tasks unconditional image synthesis textimage translation imagetoimage conversions however capability falls short realm video prediction mainly treat videos collection independent images relying external constraints temporal attention mechanisms enforce temporal coherence paper introduce novel model class treats video continuous multidimensional process rather series discrete frames also report reduction sampling steps required sample new frame thus making framework efficient inference time extensive experimentation establish stateoftheart performance video prediction validated benchmark datasets including kth bair navigate project page httpswwwcsumdedugauravshcvpsuppwebsitehtml video results,-1,0.0,-1,0.0
streetcrafter street view synthesis controllable video diffusion models paper aims tackle problem photorealistic view synthesis vehicle sensor data recent advancements neural scene representation achieved notable success rendering highquality autonomous driving scenes performance significantly degrades viewpoint deviates training trajectory mitigate problem introduce streetcrafter novel controllable video diffusion model utilizes lidar point cloud renderings pixellevel conditions fully exploits generative prior novel view synthesis preserving precise camera control moreover utilization pixellevel lidar conditions allows us make accurate pixellevel edits target scenes addition generative prior streetcrafter effectively incorporated dynamic scene representations achieve realtime rendering experiments waymo open dataset pandaset demonstrate model enables flexible control viewpoint changes enlarging view synthesis regions satisfying rendering outperforms existing methods,1,1.0,1,1.0
spatialme stereo video conversion using depthwarping blendinpainting stereo video conversion aims transform monocular videos immersive stereo format despite advancements novel view synthesis still remains two major challenges difficulty achieving highfidelity stable results ii insufficiency highquality stereo video data paper introduce spatialme novel stereo video conversion framework based depthwarping blendinpainting specifically propose maskbased hierarchy feature update mhfu refiner integrate refine outputs designed multibranch inpainting module using feature update unit fuu mask mechanism also propose disparity expansion strategy address problem foreground bleeding furthermore conduct highquality realworld stereo video dataset alleviate data shortage contains stereo videos captured realworld resolution x covering various indoor outdoor scenes extensive experiments demonstrate superiority approach generating stereo videos stateoftheart methods,-1,0.0,-1,0.0
hyperglm hypergraph video scene graph generation anticipation multimodal llms advanced visionlanguage tasks still struggle understanding video scenes bridge gap video scene graph generation vidsgg emerged capture multiobject relationships across video frames however prior methods rely pairwise connections limiting ability handle complex multiobject interactions reasoning end propose multimodal llms scene hypergraph hyperglm promoting reasoning multiway interactions higherorder relationships approach uniquely integrates entity scene graphs capture spatial relationships objects procedural graph models causal transitions forming unified hypergraph significantly hyperglm enables reasoning injecting unified hypergraph llms additionally introduce new video scene graph reasoning vsgr dataset featuring frames thirdperson egocentric drone views supports five tasks scene graph generation scene graph anticipation video question answering video captioning relation reasoning empirically hyperglm consistently outperforms stateoftheart methods across five tasks effectively modeling reasoning complex relationships diverse video scenes,-1,0.0,-1,0.0
visage video synthesis using action graphs surgery surgical data science sds field analyzes patient data surgery improve surgical outcomes skills however surgical data scarce heterogeneous complex limits applicability existing machine learning methods work introduce novel task future video generation laparoscopic surgery task augment enrich existing surgical data enable various applications simulation analysis robotaided surgery ultimately involves understanding current state operation also accurately predicting dynamic often unpredictable nature surgical procedures proposed method visage video synthesis using action graphs surgery leverages power action scene graphs capture sequential nature laparoscopic procedures utilizes diffusion models synthesize temporally coherent video sequences visage predicts future frames given single initial frame action graph triplets incorporating domainspecific knowledge action graph visage ensures generated videos adhere expected visual motion patterns observed real laparoscopic procedures results experiments demonstrate highfidelity video generation laparoscopy procedures enables various applications sds,-1,0.0,-1,0.0
cfsynthesis controllable freeview human video synthesis human video synthesis aims create lifelike characters various environments wide applications vr storytelling content creation diffusionbased methods made significant progress struggle generalize complex poses varying scene backgrounds address limitations introduce cfsynthesis novel framework generating highquality human videos customizable attributes including identity motion scene configurations method leverages texturesmplbased representation ensure consistent stable character appearances across free viewpoints additionally introduce novel foregroundbackground separation strategy effectively decomposes scene foreground background enabling seamless integration userdefined backgrounds experimental results multiple datasets show cfsynthesis achieves stateoftheart performance complex human animations also adapts effectively motions freeview userspecified scenarios,1,1.0,1,1.0
compressing scene dynamics generative approach paper proposes learn generative priors motion patterns instead video contents generative video compression priors derived small motion dynamics common scenes swinging trees wind floating boat sea utilizing compact motion priors novel generative scene dynamics compression framework built realize ultralow bitrate communication highquality reconstruction diverse scene contents encoder side motion priors characterized compact representations densetosparse manner decoder side decoded motion priors serve trajectory hints scene dynamics reconstruction via diffusionbased flowdriven generator experimental results illustrate proposed method achieve superior ratedistortion performance outperform stateoftheart conventional video codec versatile video coding vvc scene dynamics sequences project page found httpsgithubcomxyzyszgnvdc,-1,0.0,-1,0.0
dreamforge motionaware autoregressive video generation multiview driving scenes recent advances diffusion models improved controllable streetscape generation supported downstream perception planning tasks however challenges remain accurately modeling driving scenes generating long videos alleviate issues propose dreamforge advanced diffusionbased autoregressive video generation model tailored longterm generation enhance lane foreground generation introduce perspective guidance integrate objectwise position encoding incorporate local correlation improve foreground object modeling also propose motionaware temporal attention capture motion cues appearance changes videos leveraging motion frames autoregressive generation paradigmwe autoregressively generate long videos frames using model trained short sequences achieving superior quality compared baseline video evaluations finally integrate method realistic simulator drivearena provide reliable openloop closedloop evaluations visionbased driving agents project page httpspjlabadggithubiodrivearenadreamforge,11,1.0,11,1.0
viewcrafter taming video diffusion models highfidelity novel view synthesis despite recent advancements neural reconstruction dependence dense multiview captures restricts broader applicability work propose textbfviewcrafter novel method synthesizing highfidelity novel views generic scenes single sparse images prior video diffusion model method takes advantage powerful generation capabilities video diffusion model coarse clues offered pointbased representation generate highquality video frames precise camera pose control enlarge generation range novel views tailored iterative view synthesis strategy together camera trajectory planning algorithm progressively extend clues areas covered novel views viewcrafter facilitate various applications immersive experiences realtime rendering efficiently optimizing representation using reconstructed points generated novel views scenelevel generation imaginative content creation extensive experiments diverse datasets demonstrate strong generalization capability superior performance method synthesizing highfidelity consistent novel views,1,0.9716650665679084,1,0.9716650665679084
motion consistency model accelerating video diffusion disentangled motionappearance distillation image diffusion distillation achieves highfidelity generation sampling steps however applying techniques directly video diffusion often results unsatisfactory frame quality due limited visual quality public video datasets affects performance teacher student video diffusion models study aims improve video diffusion distillation improving frame appearance using abundant highquality image data propose motion consistency model mcm singlestage video diffusion distillation method disentangles motion appearance learning specifically mcm includes video consistency model distills motion video teacher model image discriminator enhances frame appearance match highquality image data combination presents two challenges conflicting frame learning objectives video distillation learns lowquality video frames image discriminator targets highquality images traininginference discrepancies due differing quality video samples used training inference address challenges introduce disentangled motion distillation mixed trajectory distillation former applies distillation objective solely motion representation latter mitigates traininginference discrepancies mixing distillation trajectories low highquality video domains extensive experiments show mcm achieves stateoftheart video diffusion distillation performance additionally method enhance frame quality video diffusion models producing frames high aesthetic scores specific styles without corresponding video data,-1,0.0,-1,0.0
fashionvdm video diffusion model virtual tryon present fashionvdm video diffusion model vdm generating virtual tryon videos given input garment image person video method aims generate highquality tryon video person wearing given garment preserving persons identity motion imagebased virtual tryon shown impressive results however existing video virtual tryon vvt methods still lacking garment details temporal consistency address issues propose diffusionbased architecture video virtual tryon split classifierfree guidance increased control conditioning inputs progressive temporal training strategy singlepass video generation also demonstrate effectiveness joint imagevideo training video tryon especially video data limited qualitative quantitative experiments show approach sets new stateoftheart video virtual tryon additional results visit project page httpsjohannakarrasgithubiofashionvdm,11,1.0,11,1.0
tango cospeech gesture video reenactment hierarchical audio motion embedding diffusion interpolation present tango framework generating cospeech bodygesture videos given fewminute singlespeaker reference video target speech audio tango produces highfidelity videos synchronized body gestures tango builds gesture video reenactment gvr splits retrieves video clips using directed graph structure representing video frames nodes valid transitions edges address two key limitations gvr audiomotion misalignment visual artifacts gangenerated transition frames particular propose retrieving gestures using latent feature distance improve crossmodal alignment ensure latent features could effectively model relationship speech audio gesture motion implement hierarchical joint embedding space aumoclip ii introduce diffusionbased model generate highquality transition frames diffusion model appearance consistent interpolation acinterp built upon animateanyone includes reference motion module homography background flow preserve appearance consistency generated reference videos integrating components graphbased retrieval framework tango reliably produces realistic audiosynchronized videos outperforms existing generative retrieval methods codes pretrained models available urlhttpspantomatrixgithubiotango,-1,0.0,-1,0.0
dynamic multiobject scene generation monocular videos viewpredictive generative models provide strong priors lifting objectcentric images videos rendering score distillation objectives question remains lifting complete multiobject dynamic scenes two challenges direction first rendering error gradients often insufficient recover fast object motion second view predictive generative models work much better objects whole scenes score distillation objectives currently applied scene level directly present first approach generate dynamic scenes multiple objects monocular videos via novel view synthesis key insight decomposerecompose approach factorizes video scene background object tracks also factorizing object motion components objectcentric deformation objecttoworldframe transformation camera motion decomposition permits rendering error gradients object viewpredictive models recover object completions deformations bounding box tracks guide large object movements scene show extensive results challenging davis kubric selfcaptured videos quantitative comparisons user preference study besides scene generation obtains accurate persistent point track projecting inferred trajectories release code hope work stimulate research finegrained understanding videos,1,1.0,1,1.0
slow bidirectional fast autoregressive video diffusion models current video diffusion models achieve impressive generation quality struggle interactive applications due bidirectional attention dependencies generation single frame requires model process entire sequence including future address limitation adapting pretrained bidirectional diffusion transformer autoregressive transformer generates frames onthefly reduce latency extend distribution matching distillation dmd videos distilling diffusion model generator enable stable highquality distillation introduce student initialization scheme based teachers ode trajectories well asymmetric distillation strategy supervises causal student model bidirectional teacher approach effectively mitigates error accumulation autoregressive generation allowing longduration video synthesis despite training short clips model achieves total score vbenchlong benchmark surpassing previous video generation models enables fast streaming generation highquality videos fps single gpu thanks kv caching approach also enables streaming videotovideo translation imagetovideo dynamic prompting zeroshot manner release code based opensource model future,-1,0.0,-1,0.0
motioncom automatic motionaware image composition llm video diffusion prior work presents motioncom trainingfree motionaware diffusion based image composition enabling automatic seamless integration target objects new scenes dynamically coherent results without finetuning optimization traditional approaches area suffer two significant limitations require manual planning object placement often generate static compositions lacking motion realism motioncom addresses issues utilizing large vision language model lvlm intelligent planning video diffusion prior motioninfused image synthesis streamlining composition process multimodal chainofthought cot prompting lvlm automates strategic placement planning foreground objects considering potential motion interaction within scenes complementing propose novel method motionpaint distill motionaware information pretrained video diffusion models generation phase ensuring objects seamlessly integrated also endowed realistic motion extensive quantitative qualitative results highlight motioncoms superiority showcasing efficiency streamlining planning process capability produce compositions authentically depict motion interaction,18,1.0,18,1.0
immersive video generation perspective anchor videos offer hyperimmersive experience allows viewers explore dynamic scene full degrees achieve userfriendly personalized content creation video format seek lift standard perspective videos equirectangular videos end introduce first video generation framework creates highquality videos rich diverse motion patterns video anchors learns finegrained spherical visual motion patterns limited video data several key designs firstly adopt dualbranch design including perspective panorama video denoising branch provide local global constraints video generation motion module spatial lora layers finetuned extended web videos additionally antipodal mask devised capture longrange motion dependencies enhancing reversed camera motion antipodal pixels across hemispheres handle diverse perspective video inputs propose elevationaware designs adapt varying video masking due changing elevations across frames extensive experiments show achieves superior graphics quality motion coherence among stateoftheart video generation methods believe holds promise advancing personalized immersive video creation,1,1.0,1,1.0
gamegenx interactive openworld game video generation introduce gamegenx first diffusion transformer model specifically designed generating interactively controlling openworld game videos model facilitates highquality opendomain generation simulating extensive array game engine features innovative characters dynamic environments complex actions diverse events additionally provides interactive controllability predicting altering future content based current clip thus allowing gameplay simulation realize vision first collected built openworld video game dataset scratch first largest dataset openworld game video generation control comprises million diverse gameplay video clips sampling games informative captions gamegenx undergoes twostage training process consisting foundation model pretraining instruction tuning firstly model pretrained via texttovideo generation video continuation endowing capability longsequence highquality opendomain game video generation achieve interactive controllability designed instructnet incorporate gamerelated multimodal control signal experts allows model adjust latent representations based user inputs unifying character interaction scene content control first time video generation instruction tuning instructnet updated pretrained foundation model frozen enabling integration interactive controllability without loss diversity quality generated video content,-1,0.0,-1,0.0
venhancer generative spacetime enhancement video generation present venhancer generative spacetime enhancement framework improves existing texttovideo results adding details spatial domain synthetic detailed motion temporal domain given generated lowquality video approach increase spatial temporal resolution simultaneously arbitrary upsampling space time scales unified video diffusion model furthermore venhancer effectively removes generated spatial artifacts temporal flickering generated videos achieve basing pretrained video diffusion model train video controlnet inject diffusion model condition low framerate lowresolution videos effectively train video controlnet design spacetime data augmentation well videoaware conditioning benefiting designs venhancer yields stable training shares elegant endtoend training manner extensive experiments show venhancer surpasses existing stateoftheart video superresolution spacetime superresolution methods enhancing aigenerated videos moreover venhancer exisiting opensource stateoftheart texttovideo method reaches top one video generation benchmark vbench,-1,0.0,-1,0.0
towards chunkwise generation long videos generating longduration videos always significant challenge due inherent complexity spatiotemporal domain substantial gpu memory demands required calculate huge size tensors diffusion based generative models achieve stateoftheart performance video generation task typically trained predefined video resolutions lengths inference noise tensor specific resolution length specified first model perform denoising entire video tensor simultaneously frames together approach easily raise outofmemory oom problem specified resolution andor length exceed certain limit one solutions problem generate many short video chunks autoregressively strong interchunk spatiotemporal relation concatenate together form long video approach long video generation task divided multiple short video generation subtasks cost subtask reduced feasible level paper conduct detailed survey long video generation autoregressive chunkbychunk strategy address common problems caused applying short imagetovideo models long video tasks design efficient kstep search solution mitigate problems,-1,0.0,-1,0.0
representing long volumetric video temporal gaussian hierarchy paper aims address challenge reconstructing long volumetric videos multiview rgb videos recent dynamic view synthesis methods leverage powerful representations like feature grids point cloud sequences achieve highquality rendering results however typically limited short video clips often suffer large memory footprints dealing longer videos solve issue propose novel representation named temporal gaussian hierarchy compactly model long volumetric videos key observation generally various degrees temporal redundancy dynamic scenes consist areas changing different speeds motivated approach builds multilevel hierarchy gaussian primitives level separately describes scene regions different degrees content change adaptively shares gaussian primitives represent unchanged scene content different temporal segments thus effectively reducing number gaussian primitives addition treelike structure gaussian hierarchy allows us efficiently represent scene particular moment subset gaussian primitives leading nearly constant gpu memory usage training rendering regardless video length extensive experimental results demonstrate superiority method alternative methods terms training cost rendering speed storage usage knowledge work first approach capable efficiently handling minutes volumetric video data maintaining stateoftheart rendering quality project page available,-1,0.0,-1,0.0
triergon finegrained videotoaudio generation multimodal conditions lufs control videotoaudio generation utilizes visualonly video features produce realistic sounds correspond scene however current models often lack finegrained control generated audio especially terms loudness variation incorporation multimodal conditions overcome limitations introduce triergon diffusionbased model incorporates textual auditory pixellevel visual prompts enable detailed semantically rich audio synthesis additionally introduce loudness units relative full scale lufs embedding allows precise manual control loudness changes time individual audio channels enabling model effectively address intricate correlation video audio realworld foley workflows triergon capable creating khz highfidelity stereo audio clips varying lengths seconds significantly outperforms existing stateoftheart methods typically generate mono audio fixed duration,8,1.0,8,1.0
talc timealigned captions multiscene texttovideo generation texttovideo generative models often produce singlescene video clips depict entity performing particular action eg red panda climbing tree however pertinent generate multiscene videos since ubiquitous realworld eg red panda climbing tree followed red panda sleeps top tree generate multiscene videos pretrained model introduce simple effective timealigned captions talc framework specifically enhance textconditioning mechanism architecture recognize temporal alignment video scenes scene descriptions instance condition visual features earlier later scenes generated video representations first scene description eg red panda climbing tree second scene description eg red panda sleeps top tree respectively result show model generate multiscene videos adhere multiscene text descriptions visually consistent eg entity background finetune pretrained model multiscene videotext data using talc framework show talcfinetuned model outperforms baseline achieving relative gain overall score averages visual consistency text adherence using human evaluation,-1,0.0,-1,0.0
video recap recursive captioning hourlong videos video captioning models designed process short video clips seconds output text describing lowlevel visual concepts eg objects scenes atomic actions however realworld videos last minutes hours complex hierarchical structure spanning different temporal granularities propose video recap recursive video captioning model process video inputs dramatically different lengths second hours output video captions multiple hierarchy levels recursive videolanguage architecture exploits synergy different video hierarchies process hourlong videos efficiently utilize curriculum learning training scheme learn hierarchical structure videos starting cliplevel captions describing atomic actions focusing segmentlevel descriptions concluding generating summaries hourlong videos furthermore introduce dataset augmenting manually collected longrange video summaries recursive model flexibly generate captions different hierarchy levels also useful complex video understanding tasks videoqa egoschema data code models available httpssitesgooglecomviewvidrecap,0,0.9573972759127061,0,0.9573972759127061
diffted oneshot audiodriven ted talk video generation diffusionbased cospeech gestures audiodriven talking video generation advanced significantly existing methods often depend videotovideo translation techniques traditional generative networks like gans typically generate taking heads cospeech gestures separately leading less coherent outputs furthermore gestures produced methods often appear overly smooth subdued lacking diversity many gesturecentric approaches integrate talking head generation address limitations introduce diffted new approach oneshot audiodriven tedstyle talking video generation single image specifically leverage diffusion model generate sequences keypoints thinplate spline motion model precisely controlling avatars animation ensuring temporally coherent diverse gestures innovative approach utilizes classifierfree guidance empowering gestures flow naturally audio input without relying pretrained classifiers experiments demonstrate diffted generates temporally coherent talking videos diverse cospeech gestures,6,0.6082337712174998,6,0.6082337712174998
latte latent diffusion transformer video generation propose novel latent diffusion transformer namely latte video generation latte first extracts spatiotemporal tokens input videos adopts series transformer blocks model video distribution latent space order model substantial number tokens extracted videos four efficient variants introduced perspective decomposing spatial temporal dimensions input videos improve quality generated videos determine best practices latte rigorous experimental analysis including video clip patch embedding model variants timestepclass information injection temporal positional embedding learning strategies comprehensive evaluation demonstrates latte achieves stateoftheart performance across four standard video generation datasets ie faceforensics skytimelapse taichihd addition extend latte texttovideo generation task latte achieves comparable results compared recent models strongly believe latte provides valuable insights future research incorporating transformers diffusion models video generation,-1,0.0,-1,0.0
vivid video virtual tryon using diffusion models video virtual tryon aims transfer clothing item onto video target person directly applying technique imagebased tryon video domain framewise manner cause temporalinconsistent outcomes previous videobased tryon solutions generate low visual quality blurring results work present vivid novel framework employing powerful diffusion models tackle task video virtual tryon specifically design garment encoder extract finegrained clothing semantic features guiding model capture garment details inject target video proposed attention feature fusion mechanism ensure spatialtemporal consistency introduce lightweight pose encoder encode pose signals enabling model learn interactions clothing human posture insert hierarchical temporal modules texttoimage stable diffusion model coherent lifelike video synthesis furthermore collect new dataset largest diverse types garments highest resolution task video virtual tryon date extensive experiments demonstrate approach able yield satisfactory video tryon results dataset codes weights publicly available project page,-1,0.0,-1,0.0
synthesis synchronized sound effects temporal semantic controls sound designers foley artists usually sonorize scene movie video game manually annotating sonorizing action interest video case intent leave full creative control sound designers tool allows bypass repetitive parts work thus able focus creative aspects sound production achieve presenting twostage model consisting rmsmapper estimates envelope representative audio characteristics associated input video stablefoley diffusion model based stable audio open generates audio semantically temporally aligned target video temporal alignment guaranteed use envelope controlnet input semantic alignment achieved use sound representations chosen designer crossattention conditioning diffusion process train test model greatest hits dataset commonly used evaluate models addition test model case study interest introduce walking maps dataset videos extracted video games depicting animated characters walking different locations samples code available demo page,8,0.4476953153210132,8,0.4476953153210132
one prompt scene generation via videoassisted consistencyenhanced mae artificial intelligence generated content aigc advances variety methods developed generate text images videos objects single multimodal inputs contributing efforts emulate humanlike cognitive content creation however generating realistic largescale scenes single input presents challenge due complexities involved ensuring consistency across extrapolated views generated models benefiting recent video generation models implicit neural representations propose scene generation model ensures realism diversity video generation framework also uses implicit neural fields combined masked autoencoders mae effectively ensures consistency unseen areas across views specifically initially warp input image image generated text simulate adjacent views filling invisible areas mae model however filled images usually fail maintain view consistency thus utilize produced views optimize neural radiance field enhancing geometric consistency moreover enhance details texture fidelity generated views employ ganbased loss images derived input image video generation model extensive experiments demonstrate method generate realistic consistent scenes single prompt qualitative quantitative results indicate approach surpasses existing stateoftheart methods show encourage video examples,-1,0.0,-1,0.0
vlasik consistent glassesremoval videos using synthetic data diffusionbased generative models recently shown remarkable image video editing capabilities however local video editing particularly removal small attributes like glasses remains challenge existing methods either alter videos excessively generate unrealistic artifacts fail perform requested edit consistently throughout video work focus consistent identitypreserving removal glasses videos using case study consistent local attribute removal videos due lack paired data adopt weakly supervised approach generate synthetic imperfect data using adjusted pretrained diffusion model show despite data imperfection learning generated data leveraging prior pretrained diffusion models model able perform desired edit consistently preserving original video content furthermore exemplify generalization ability method local video editing tasks applying successfully facial stickerremoval approach demonstrates significant improvement existing methods showcasing potential leveraging synthetic data strong video priors local video editing tasks,6,0.38883585413404625,6,0.38883585413404625
video creation demonstration explore novel video creation experience namely video creation demonstration given demonstration video context image different scene generate physically plausible video continues naturally context image carries action concepts demonstration enable capability present deltadiffusion selfsupervised training approach learns unlabeled videos conditional future frame prediction unlike existing video generation controls based explicit signals adopts form implicit latent control maximal flexibility expressiveness required general videos leveraging video foundation model appearance bottleneck design top extract action latents demonstration videos conditioning generation process minimal appearance leakage empirically deltadiffusion outperforms related baselines terms human preference largescale machine evaluations demonstrates potentials towards interactive world simulation sampled video generation results available httpsdeltadiffusiongithubio,-1,0.0,-1,0.0
egosonics generating synchronized audio silent egocentric videos introduce egosonics method generate semantically meaningful synchronized audio tracks conditioned silent egocentric videos generating audio silent egocentric videos could open new applications virtual reality assistive technologies augmenting existing datasets existing work limited domains like speech music impact sounds capture broad range audio frequencies found egocentric videos egosonics addresses limitations building strengths latent diffusion models conditioned audio synthesis first encode process paired audiovideo data make suitable generation encoded data used train model generate audio track captures semantics input video proposed syncronet builds top controlnet provide control signals enables generation temporally synchronized audio extensive evaluations comprehensive user study show model outperforms existing work audio quality proposed synchronization evaluation method furthermore demonstrate downstream applications model improving video summarization,8,0.5162175545990493,8,0.5162175545990493
mvoc trainingfree multiple video object composition method diffusion models video composition core task video editing although image composition based diffusion models highly successful straightforward extend achievement video object composition tasks exhibit corresponding interaction effects also ensure objects composited video maintain motion identity consistency necessary composite physical harmony video address challenge propose multiple video object composition mvoc method based diffusion models specifically first perform ddim inversion video object obtain corresponding noise features secondly combine edit object image editing methods obtain first frame composited video finally use imagetovideo generation model composite video feature attention injections video object dependence module trainingfree conditional guidance operation video generation enables coordination features attention maps various objects nonindependent composited video final generative model constrains objects generated video consistent original object motion identity also introduces interaction effects objects extensive experiments demonstrated proposed method outperforms existing stateoftheart approaches project page httpssobeymilgithubiomvoccom,-1,0.0,-1,0.0
leveraging compressed frame sizes ultrafast video classification classifying videos distinct categories sport music video crucial multimedia understanding retrieval especially immense volume video content constantly generated traditional methods require video decompression extract pixellevel features like color texture motion thereby increasing computational storage demands moreover methods often suffer performance degradation lowquality videos present novel approach examines postcompression bitstream video perform classification eliminating need bitstream decoding validate approach built comprehensive data set comprising youtube video clips totaling hours spanning distinct categories evaluations indicate precision accuracy recall rates consistently many exceeding reaching algorithm operates approximately times faster realtime videos outperforming traditional dynamic time warping dtw algorithm seven orders magnitude,2,0.7992394517025053,2,0.7992394517025053
worlddreamer towards general world models video generation via predicting masked tokens world models play crucial role understanding predicting dynamics world essential video generation however existing world models confined specific scenarios gaming driving limiting ability capture complexity general world dynamic environments therefore introduce worlddreamer pioneering world model foster comprehensive comprehension general world physics motions significantly enhances capabilities video generation drawing inspiration success large language models worlddreamer frames world modeling unsupervised visual sequence modeling challenge achieved mapping visual inputs discrete tokens predicting masked ones process incorporate multimodal prompts facilitate interaction within world model experiments show worlddreamer excels generating videos across different scenarios including natural scenes driving environments worlddreamer showcases versatility executing tasks texttovideo conversion imagetovideo synthesis video editing results underscore worlddreamers effectiveness capturing dynamic elements within diverse general world environments,-1,0.0,-1,0.0
reconx reconstruct scene sparse views video diffusion model advancements scene reconstruction transformed images real world models producing realistic results hundreds input photos despite great success denseview reconstruction scenarios rendering detailed scene insufficient captured views still illposed optimization problem often resulting artifacts distortions unseen areas paper propose reconx novel scene reconstruction paradigm reframes ambiguous reconstruction challenge temporal generation task key insight unleash strong generative prior large pretrained video diffusion models sparseview reconstruction however view consistency struggles accurately preserved directly generated video frames pretrained models address given limited input views proposed reconx first constructs global point cloud encodes contextual space structure condition guided condition video diffusion model synthesizes video frames detailpreserved exhibit high degree consistency ensuring coherence scene various perspectives finally recover scene generated video confidenceaware gaussian splatting optimization scheme extensive experiments various realworld datasets show superiority reconx stateoftheart methods terms quality generalizability,1,1.0,1,1.0
enhancing unbounded gaussian splatting viewconsistent diffusion priors novelview synthesis aims generate novel views scene multiple input images videos recent advancements like gaussian splatting achieved notable success producing photorealistic renderings efficient pipelines however generating highquality novel views challenging settings sparse input views remains difficult due insufficient information undersampled areas often resulting noticeable artifacts paper presents novel pipeline enhancing representation quality representations leverage video diffusion priors address challenging view consistency problem reformulating achieving temporal consistency within video generation process restores viewconsistent latent features rendered novel views integrates input views spatialtemporal decoder enhanced views used finetune initial model significantly improving rendering performance extensive experiments largescale datasets unbounded scenes demonstrate yields superior reconstruction performance highfidelity rendering results compared stateoftheart methods project webpage,1,1.0,1,1.0
unianimate taming unified video diffusion models consistent human image animation recent diffusionbased human image animation techniques demonstrated impressive success synthesizing videos faithfully follow given reference identity sequence desired movement poses despite still two limitations extra reference model required align identity image main video branch significantly increases optimization burden model parameters ii generated video usually short time eg frames hampering practical applications address shortcomings present unianimate framework enable efficient longterm human video generation first reduce optimization difficulty ensure temporal coherence map reference image along posture guidance noise video common feature space incorporating unified video diffusion model second propose unified noise input supports random noised input well first frame conditioned input enhances ability generate longterm video finally efficiently handle long sequences explore alternative temporal modeling architecture based state space model replace original computationconsuming temporal transformer extensive experimental results indicate unianimate achieves superior synthesis results existing stateoftheart counterparts quantitative qualitative evaluations notably unianimate even generate highly consistent oneminute videos iteratively employing first frame conditioning strategy code models publicly available project page httpsunianimategithubio,-1,0.0,-1,0.0
surgen textguided diffusion model surgical video generation diffusionbased video generation models made significant strides producing outputs improved visual fidelity temporal coherence user control advancements hold great promise improving surgical education enabling realistic diverse interactive simulation environments study introduce surgen textguided diffusion model tailored surgical video synthesis surgen produces videos highest resolution longest duration among existing surgical video generation models validate visual temporal quality outputs using standard image video generation metrics additionally assess alignment corresponding text prompts deep learning classifier trained surgical data results demonstrate potential diffusion models serve valuable educational tools surgical trainees,-1,0.0,-1,0.0
investigating effectiveness crossattention unlock zeroshot editing texttovideo diffusion models recent advances image video diffusion models content creation plethora techniques proposed customizing generated content particular manipulating crossattention layers texttoimage diffusion models shown great promise controlling shape location objects scene transferring imageediting techniques video domain however extremely challenging object motion temporal consistency difficult capture accurately work take first look role crossattention texttovideo diffusion models zeroshot video editing oneshot models shown potential controlling motion camera movement demonstrate zeroshot control object shape position movement models show despite limitations current models crossattention guidance promising approach editing videos,9,0.778886103870331,9,0.778886103870331
teaching video diffusion models track points improves video generation recent foundational video generators produce visually rich output still struggle appearance drift objects gradually degrade change inconsistently across frames breaking visual coherence hypothesize explicit supervision terms spatial tracking feature level propose spatially aware video generator combines video diffusion loss point tracking across frames providing enhanced spatial supervision diffusion features merges video generation point tracking tasks single network making minimal changes existing video generation architectures using stable video diffusion backbone demonstrates possible unify video generation point tracking typically handled separate tasks extensive evaluations show effectively reduces appearance drift resulting temporally stable visually coherent video generation project page,9,1.0,9,1.0
slicedit zeroshot video editing texttoimage diffusion models using spatiotemporal slices texttoimage diffusion models achieve stateoftheart results image synthesis editing however leveraging pretrained models video editing considered major challenge many existing works attempt enforce temporal consistency edited video explicit correspondence mechanisms either pixel space deep features methods however struggle strong nonrigid motion paper introduce fundamentally different approach based observation spatiotemporal slices natural videos exhibit similar characteristics natural images thus diffusion model normally used prior video frames also serve strong prior enhancing temporal consistency applying spatiotemporal slices based observation present slicedit method textbased video editing utilizes pretrained diffusion model process spatial spatiotemporal slices method generates videos retain structure motion original video adhering target text extensive experiments demonstrate slicedits ability edit wide range realworld videos confirming clear advantages compared existing competing methods webpage httpsmatankleinergithubioslicedit,-1,0.0,-1,0.0
scene copilot procedural text video generation human loop video generation achieved impressive quality still suffers artifacts temporal inconsistency violation physical laws leveraging scenes fundamentally resolve issues providing precise control scene entities facilitate easy generation diverse photorealistic scenes propose scene copilot framework combining large language models llms procedural scene generator specifically scene copilot consists scene codex blendergpt human loop scene codex designed translate textual user input commands understandable scene generator blendergpt provides users intuitive direct way precisely control generated scene final output video furthermore users utilize blender ui receive instant visual feedback additionally curated procedural dataset objects code format enhance systems capabilities component works seamlessly together support users generating desired scenes extensive experiments demonstrate capability framework customizing scenes video generation,-1,0.0,-1,0.0
animate motion turning still images dynamic videos recent years diffusion models made remarkable strides texttovideo generation sparking quest enhanced control video outputs accurately reflect user intentions traditional efforts predominantly focus employing either semantic cues like images depth maps motionbased conditions like moving sketches object bounding boxes semantic inputs offer rich scene context lack detailed motion specificity conversely motion inputs provide precise trajectory information miss broader semantic narrative first time integrate semantic motion cues within diffusion model video generation demonstrated fig end introduce scene motion conditional diffusion smcd novel methodology managing multimodal inputs incorporates recognized motion conditioning module investigates various approaches integrate scene conditions promoting synergy different modalities model training separate conditions two modalities introducing twostage training pipeline experimental results demonstrate design significantly enhances video quality motion precision semantic coherence,9,0.748625935133494,9,0.748625935133494
vividdream generating scene ambient dynamics introduce vividdream method generating explorable scenes ambient dynamics single input image text prompt vividdream first expands input image static point cloud iterative inpainting geometry merging ensemble animated videos generated using video diffusion models quality refinement techniques conditioned renderings static scene sampled camera trajectories optimize canonical scene representation using animated video ensemble pervideo motion embeddings visibility masks mitigate inconsistencies resulting scene enables freeview exploration scene plausible ambient scene dynamics experiments demonstrate vividdream provide human viewers compelling experiences generated based diverse real images text prompts,1,1.0,1,1.0
replace anyone videos recent advancements controllable humancentric video generation particularly rise diffusion models demonstrated considerable progress however achieving precise localized control human motion eg replacing inserting individuals videos exhibiting desired motion patterns still remains challenging work propose replaceanyone framework focuses localizing manipulating human motion videos diverse intricate backgrounds specifically formulate task imageconditioned posedriven video inpainting paradigm employing unified video diffusion architecture facilitates imageconditioned posedriven video generation inpainting within masked video regions moreover introduce diverse mask forms involving regular irregular shapes avoid shape leakage allow granular local control additionally implement twostage training methodology initially training imageconditioned pose driven video generation model followed joint training video inpainting within masked areas way approach enables seamless replacement insertion characters maintaining desired pose motion reference appearance within single framework experimental results demonstrate effectiveness method generating realistic coherent video content,9,1.0,9,1.0
dreamfactory pioneering multiscene long video generation multiagent framework current video generation models excel creating short realistic clips struggle longer multiscene videos introduce textttdreamfactory llmbased framework tackles challenge textttdreamfactory leverages multiagent collaboration principles key frames iteration design method ensure consistency style across long videos utilizes chain thought cot address uncertainties inherent large language models textttdreamfactory generates long stylistically coherent complex videos evaluating longform videos presents challenge propose novel metrics crossscene face distance score crossscene style consistency score research area contribute multiscene videos dataset containing humanrated videos,0,1.0,0,1.0
enhanced creativity ideation stable video synthesis paper explores innovative application stable video diffusion svd diffusion model revolutionizes creation dynamic video content static images digital media design industries accelerate svd emerges powerful generative tool enhances productivity introduces novel creative possibilities paper examines technical underpinnings diffusion models practical effectiveness potential future developments particularly context video generation svd operates probabilistic framework employing gradual denoising process transform random noise coherent video frames addresses challenges visual consistency natural movement stylistic reflection generated videos showcasing high generalization capabilities integration svd design tasks promises enhanced creativity rapid prototyping significant time cost efficiencies particularly impactful areas requiring frametoframe consistency natural motion capture creative diversity animation visual effects advertising educational content creation paper concludes svd catalyst design innovation offering wide array applications promising avenue future research development field digital media design,10,1.0,10,1.0
matters detecting aigenerated videos like sora recent advancements diffusionbased video generation showcased remarkable results yet gap synthetic realworld videos remains underexplored study examine gap three fundamental perspectives appearance motion geometry comparing realworld videos generated stateoftheart ai model stable video diffusion achieve train three classifiers using convolutional networks targeting distinct aspects vision foundation model features appearance optical flow motion monocular depth geometry classifier exhibits strong performance fake video detection qualitatively quantitatively indicates aigenerated videos still easily detectable significant gap real fake videos persists furthermore utilizing gradcam pinpoint systematic failures aigenerated videos appearance motion geometry finally propose ensembleofexperts model integrates appearance optical flow depth information fake video detection resulting enhanced robustness generalization ability model capable detecting videos generated sora high accuracy even without exposure sora videos training suggests gap real fake videos generalized across various video generative models project page,4,1.0,4,1.0
animatelcm computationefficient personalized style video generation without personalized video data paper introduces effective method computationefficient personalized style video generation without requiring access personalized video data reduces necessary generation time similarly sized video diffusion models seconds around second maintaining level performance methods effectiveness lies duallevel decoupling learning approach separating learning video style video generation acceleration allows personalized style video generation without personalized style video data separating acceleration image generation acceleration video motion generation enhancing training efficiency mitigating negative effects lowquality video data,-1,0.0,-1,0.0
firstframeguided video editing via imagetovideo diffusion models remarkable generative capabilities diffusion models motivated extensive research image video editing compared video editing faces additional challenges time dimension image editing witnessed development diverse highquality approaches capable software like photoshop light gap introduce novel generic solution extends applicability image editing tools videos propagating edits single frame entire video using pretrained imagetovideo model method dubbed adaptively preserves visual motion integrity source video depending extent edits effectively handling global edits local edits moderate shape changes existing methods fully achieve core method two main processes coarse motion extraction align basic motion patterns original video appearance refinement precise adjustments using finegrained attention matching also incorporate skipinterval strategy mitigate quality degradation autoregressive generation across multiple video clips experimental results demonstrate frameworks superior performance finegrained video editing proving capability produce highquality temporally consistent outputs,9,1.0,9,1.0
captioning videos multiple crossmodality teachers quality data annotation upperbounds quality downstream model exist large text corpora imagetext pairs highquality videotext data much harder collect first manual labeling timeconsuming requires annotator watch entire video second videos temporal dimension consisting several scenes stacked together showing multiple actions accordingly establish video dataset highquality captions propose automatic approach leveraging multimodal inputs textual video description subtitles individual video frames specifically curate highresolution videos publicly available dataset split semantically consistent video clips apply multiple crossmodality teacher models obtain captions video next finetune retrieval model small subset best caption video manually selected employ model whole dataset select best caption annotation way get videos paired highquality text captions dub dataset show value proposed dataset three downstream tasks video captioning video text retrieval textdriven video generation models trained proposed data score substantially better majority metrics across tasks,0,1.0,0,1.0
latent gaussian diffusion propose first approach generative modeling gaussians latent gaussian diffusion formulation enables effective generative modeling scaling generation entire roomscale scenes efficiently rendered enable effective synthesis gaussians propose latent diffusion formulation operating compressed latent space gaussians compressed latent space learned vectorquantized variational autoencoder vqvae employ sparse convolutional architecture efficiently operate roomscale scenes way complexity costly generation process via diffusion substantially reduced allowing higher detail objectlevel generation well scalability large scenes leveraging gaussian representation generated scenes rendered arbitrary viewpoints realtime demonstrate approach significantly improves visual quality prior work unconditional objectlevel radiance field synthesis showcase applicability roomscale scene generation,-1,0.0,-1,0.0
posecrafter oneshot personalized video synthesis following flexible pose control paper introduce posecrafter oneshot method personalized video generation following control flexible poses built upon stable diffusion controlnet carefully design inference process produce highquality videos without corresponding groundtruth frames first select appropriate reference frame training video invert initialize latent variables generation insert corresponding training pose target pose sequences enhance faithfulness trained temporal attention module furthermore alleviate face hand degradation resulting discrepancies poses training videos inference poses implement simple latent editing affine transformation matrix involving facial hand landmarks extensive experiments several datasets demonstrate posecrafter achieves superior results baselines pretrained vast collection videos commonly used metrics besides posecrafter follow poses different individuals artificial edits simultaneously retain human identity opendomain training video project page available httpsmlgsaigithubioposecrafterdemo,11,1.0,11,1.0
framebridge improving imagetovideo generation bridge models imagetovideo generation gaining increasing attention wide application video synthesis recently diffusionbased models achieved remarkable progress given novel design network architecture cascaded framework motion representation however restricted noisetodata generation process diffusionbased methods inevitably suffer difficulty generate video samples appearance consistency temporal coherence uninformative gaussian noise may limit synthesis quality work present framebridge taking given static image prior video target establishing tractable bridge model formulating synthesis framestoframes generation task modelling datatodata process fully exploit information input image facilitate generative model learn image animation process two popular settings training models namely finetuning pretrained texttovideo model training scratch propose two techniques snraligned finetuning saf neural prior improve finetuning efficiency diffusionbased models framebridge synthesis quality bridgebased models respectively experiments conducted demonstrate framebridge achieves superior quality comparison diffusion counterpart zeroshot fvd vs msrvtt nonzeroshot fvd vs proposed saf neural prior effectively enhance ability bridgebased models scenarios finetuning training scratch demo samples visited httpsframebridgedemogithubio,-1,0.0,-1,0.0
taming large video diffusion transformers camera control modern texttovideo synthesis models demonstrate coherent photorealistic generation complex videos text description however existing models lack finegrained control camera movement critical downstream applications related content creation visual effects vision recently new methods demonstrate ability generate videos controllable camera poses techniques leverage pretrained unetbased diffusion models explicitly disentangle spatial temporal generation still existing approach enables camera control new transformerbased video diffusion models process spatial temporal information jointly propose tame video transformers camera control using controlnetlike conditioning mechanism incorporates spatiotemporal camera embeddings based plucker coordinates approach demonstrates stateoftheart performance controllable video generation finetuning dataset best knowledge work first enable camera control transformerbased video diffusion models,-1,0.0,-1,0.0
wonderland navigating scenes single image paper addresses challenging question efficiently create highquality widescope scenes single arbitrary image existing methods face several constraints requiring multiview data timeconsuming perscene optimization low visual quality backgrounds distorted reconstructions unseen areas propose novel pipeline overcome limitations specifically introduce largescale reconstruction model uses latents video diffusion model predict gaussian splattings scenes feedforward manner video diffusion model designed create videos precisely following specified camera trajectories allowing generate compressed video latents contain multiview information maintaining consistency train reconstruction model operate video latent space progressive training strategy enabling efficient generation highquality widescope generic scenes extensive evaluations across various datasets demonstrate model significantly outperforms existing methods singleview scene generation particularly outofdomain images first time demonstrate reconstruction model effectively built upon latent space diffusion model realize efficient scene generation,1,0.9558639821203874,1,0.9558639821203874
streetscapes largescale consistent street view generation using autoregressive video diffusion present method generating streetscapeslong sequences views onthefly synthesized cityscale scene generation conditioned language input eg city name weather well underlying maplayout hosting desired trajectory compared recent models video generation view synthesis method scale much longerrange camera trajectories spanning several city blocks maintaining visual quality consistency achieve goal build recent work video diffusion used within autoregressive framework easily scale long sequences particular introduce new temporal imputation method prevents autoregressive approach drifting distribution realistic city imagery train streetscapes system compelling source dataposed imagery google street view along contextual map datawhich allows users generate city views conditioned desired city layout controllable camera poses please see results project page httpsboyangdengcomstreetscapes,11,1.0,11,1.0
cavia cameracontrollable multiview video diffusion viewintegrated attention recent years remarkable breakthroughs imagetovideo generation however consistency camera controllability generated frames remained unsolved recent studies attempted incorporate camera control generation process results often limited simple trajectories lack ability generate consistent videos multiple distinct camera paths scene address limitations introduce cavia novel framework cameracontrollable multiview video generation capable converting input image multiple spatiotemporally consistent videos framework extends spatial temporal attention modules viewintegrated attention modules improving viewpoint temporal consistency flexible design allows joint training diverse curated data sources including scenelevel static videos objectlevel synthetic multiview dynamic videos realworld monocular dynamic videos best knowledge cavia first kind allows user precisely specify camera motion obtaining object motion extensive experiments demonstrate cavia surpasses stateoftheart methods terms geometric consistency perceptual quality project page,-1,0.0,-1,0.0
unlearning concepts texttovideo diffusion models advancement computer vision natural language processing texttovideo generation enabled texttovideo diffusion models become prevalent models trained using large amount data internet however training data often contain copyrighted content including cartoon character icons artist styles private portraits unsafe videos since filtering data retraining model challenging methods unlearning specific concepts texttovideo diffusion models investigated however due high computational complexity relative large optimization scale little work unlearning methods texttovideo diffusion models propose novel conceptunlearning method transferring unlearning capability text encoder texttoimage diffusion models texttovideo diffusion models specifically method optimizes text encoder using fewshot unlearning several generated images used use optimized text encoder texttovideo diffusion models generate videos method costs low computation resources small optimization scale discuss generated videos unlearning concept experiments demonstrates method unlearn copyrighted cartoon characters artist styles objects peoples facial characteristics method unlearn concept within seconds rtx since concept unlearning method texttovideo diffusion models make concept unlearning feasible accessible texttovideo domain,-1,0.0,-1,0.0
qbenchvideo benchmarking video quality understanding lmms rising interest research large multimodal models lmms video understanding many studies emphasized general video comprehension capabilities neglecting systematic exploration video quality understanding address oversight introduce qbenchvideo paper new benchmark specifically designed evaluate lmms proficiency discerning video quality ensure video source diversity qbenchvideo encompasses videos natural scenes aigenerated content aigc computer graphics cg b building traditional multiplechoice questions format yesorno whathow categories include openended questions better evaluate complex scenarios additionally incorporate video pair quality comparison question enhance comprehensiveness c beyond traditional technical aesthetic temporal distortions expanded evaluation aspects include dimension aigc distortions addresses increasing demand video generation finally collect total questionanswer pairs test opensource proprietary lmms findings indicate lmms foundational understanding video quality performance remains incomplete imprecise notable discrepancy compared human performance qbenchvideo seek catalyze community interest stimulate research unlock untapped potential lmms close gap video quality understanding,10,0.6431727539269747,10,0.6431727539269747
decoupled video generation chain trainingfree diffusion model experts video generation models hold substantial potential areas filmmaking however current video diffusion models need high computational costs produce suboptimal results due extreme complexity video generation task paper propose textbfconfiner efficient video generation framework decouples video generation easier subtasks structure textbfcontrol spatialtemporal retextbffinement generate highquality videos chain offtheshelf diffusion model experts expert responsible decoupled subtask refinement introduce coordinated denoising merge multiple diffusion experts capabilities single sampling furthermore design confinerlong framework generate long coherent video three constraint strategies confiner experimental results indicate inference cost confiner surpasses representative models like lavie modelscope across objective subjective metrics confinerlong generate highquality coherent videos frames,2,0.6860564220557998,2,0.6860564220557998
stereocrafterzero zeroshot stereo video generation noisy restart generating highquality stereo videos mimic human binocular vision requires consistent depth perception temporal coherence across frames despite advances image video synthesis using diffusion models producing highquality stereo videos remains challenging task due difficulty maintaining consistent temporal spatial coherence left right views introduce stereocrafterzero novel framework zeroshot stereo video generation leverages video diffusion priors without requiring paired training data key innovations include noisy restart strategy initialize stereoaware latent representations iterative refinement process progressively harmonizes latent space addressing issues like temporal flickering view inconsistencies addition propose use dissolved depth maps streamline latent space operations reducing highfrequency depth information comprehensive evaluations including quantitative metrics user studies demonstrate stereocrafterzero produces highquality stereo videos enhanced depth consistency temporal smoothness even depth estimations imperfect framework robust adaptable across various diffusion models setting new benchmark zeroshot stereo video generation enabling immersive visual experiences code httpsgithubcomshijianjianstereocrafterzero,-1,0.0,-1,0.0
opensora democratizing efficient video production vision language two foundational senses humans build cognitive ability intelligence significant breakthroughs made ai language ability artificial visual intelligence especially ability generate simulate world see far lagging behind facilitate development accessibility artificial visual intelligence created opensora opensource video generation model designed produce highfidelity video content opensora supports wide spectrum visual generation tasks including texttoimage generation texttovideo generation imagetovideo generation model leverages advanced deep learning architectures traininginference techniques enable flexible video synthesis could generate video content seconds resolution arbitrary aspect ratios specifically introduce spatialtemporal diffusion transformer stdit efficient diffusion framework videos decouples spatial temporal attention also introduce highly compressive autoencoder make representations compact accelerate training ad hoc training strategy initiative aim foster innovation creativity inclusivity within community ai content creation embracing opensource principle opensora democratizes full access traininginferencedata preparation codes well model weights resources publicly available httpsgithubcomhpcaitechopensora,-1,0.0,-1,0.0
effects short videosharing services video copy detection short videosharing services allow users post second videos eg youtube shorts tiktok attracted lot attention recent years however conventional video copy detection vcd methods mainly focus general videosharing services eg youtube bilibili effects short videosharing services video copy detection still unclear considering illegally copied videos short videosharing services servicedistinctive characteristics especially time lengths pros cons vcd services required analyzed paper examine effects short videosharing services vcd constructing dataset short videosharing service characteristics novel dataset automatically constructed publicly available dataset reference videos fixed shorttimelength query videos automation procedures assure reproducibility data privacy preservation paper experimental results focusing segmentlevel videolevel situations see three effects segmentlevel vcd short videosharing services difficult general videosharing services videolevel vcd short videosharing services easier general videosharing services video alignment component mainly suppress detection performance short videosharing services,-1,0.0,-1,0.0
motionflow attentiondriven motion transfer video diffusion models texttovideo models demonstrated impressive capabilities producing diverse captivating video content showcasing notable advancement generative ai however models generally lack finegrained control motion patterns limiting practical applicability introduce motionflow novel framework designed motion transfer video diffusion models method utilizes crossattention maps accurately capture manipulate spatial temporal dynamics enabling seamless motion transfers across various contexts approach require training works testtime leveraging inherent capabilities pretrained video diffusion models contrast traditional approaches struggle comprehensive scene changes maintaining consistent motion motionflow successfully handles complex transformations attentionbased mechanism qualitative quantitative experiments demonstrate motionflow significantly outperforms existing models fidelity versatility even drastic scene alterations,9,0.667900576133551,9,0.667900576133551
generative omnimatte learning decompose video layers given video set input object masks omnimatte method aims decompose video semantically meaningful layers containing individual objects along associated effects shadows reflections existing omnimatte methods assume static background accurate pose depth estimation produce poor decompositions assumptions violated furthermore due lack generative prior natural videos existing methods complete dynamic occluded regions present novel generative layered video decomposition framework address omnimatte problem method assume stationary scene require camera pose depth information produces clean complete layers including convincing completions occluded dynamic regions core idea train video diffusion model identify remove scene effects caused specific object show model finetuned existing video inpainting model small carefully curated dataset demonstrate highquality decompositions editing results wide range casually captured videos containing soft shadows glossy reflections splashing water,1,1.0,1,1.0
letstalk latent diffusion transformer talking video synthesis portrait image animation using audio rapidly advanced enabling creation increasingly realistic expressive animated faces challenges multimodalityguided video generation task involve fusing various modalities ensuring consistency timing portrait seek produce vivid talking heads address challenges present letstalk latent diffusion transformer talking video synthesis diffusion transformer incorporates modular temporal spatial attention mechanisms merge multimodality enhance spatialtemporal consistency handle multimodal conditions first summarize three fusion schemes ranging shallow deep fusion compactness thoroughly explore impact applicability propose suitable solution according modality differences image audio video generation portrait utilize deep fusion scheme symbiotic fusion ensure portrait consistency audio implement shallow fusion scheme direct fusion achieve audioanimation alignment preserving diversity extensive experiments demonstrate approach generates temporally coherent realistic videos enhanced diversity liveliness,-1,0.0,-1,0.0
videostar selftraining enables video instruction tuning supervision performance large vision language models lvlms dependent size quality training datasets existing video instruction tuning datasets lack diversity derived prompting large language models video captions generate questionanswer pairs therefore mostly descriptive meanwhile many labeled video datasets diverse labels supervision exist however find integration lvlms nontrivial herein present video selftraining augmented reasoning videostar first video selftraining approach videostar allows utilization labeled video dataset video instruction tuning videostar lvlm cycles instruction generation finetuning show improves general video understanding ii adapts lvlms novel downstream tasks existing supervision generation lvlm prompted propose answer answers filtered contain original video labels lvlm retrained generated dataset training generated answers contain correct video labels videostar utilizes existing video labels weak supervision video instruction tuning results demonstrate videostarenhanced lvlms exhibit improved performance general video qa tempcompass performance improved ii downstream tasks videostar improved accuracy action quality assessment finediving,-1,0.0,-1,0.0
worldconsistent video diffusion explicit modeling recent advancements diffusion models set new benchmarks image video generation enabling realistic visual synthesis across single multiframe contexts however models still struggle efficiently explicitly generating content address propose worldconsistent video diffusion wvd novel framework incorporates explicit supervision using xyz images encode global coordinates image pixel specifically train diffusion transformer learn joint distribution rgb xyz frames approach supports multitask adaptability via flexible inpainting strategy example wvd estimate xyz frames groundtruth rgb generate novel rgb frames using xyz projections along specified camera trajectory wvd unifies tasks like generation multiview stereo cameracontrolled video generation approach demonstrates competitive performance across multiple benchmarks providing scalable solution video image generation single pretrained model,1,1.0,1,1.0
hifivfs high fidelity video face swapping face swapping aims generate results combine identity source attributes target existing methods primarily focus imagebased face swapping processing videos frame handled independently making difficult ensure temporal stability model perspective face swapping gradually shifting generative adversarial networks gans diffusion models dms dms shown possess stronger generative capabilities current diffusionbased approaches often employ inpainting techniques struggle preserve finegrained attributes like lighting makeup address challenges propose high fidelity video face swapping hifivfs framework leverages strong generative capability temporal prior stable video diffusion svd build finegrained attribute module extract identitydisentangled finegrained attribute features identity desensitization adversarial learning additionally introduce detailed identity injection enhance identity similarity extensive experiments demonstrate method achieves stateoftheart sota video face swapping qualitatively quantitatively,6,0.4572411721522535,6,0.4572411721522535
controlling space time diffusion models present cascaded diffusion model novel view synthesis nvs conditioned one images general scene set camera poses timestamps overcome challenges due limited availability training data advocate joint training camera pose posetime video time pose data propose new architecture enables advocate calibration sfm posed data using monocular metric depth estimators metric scale camera control model evaluation introduce new metrics enrich overcome shortcomings current evaluation schemes demonstrating stateoftheart results fidelity pose control compared existing diffusion models nvs time adding ability handle temporal dynamics also used improved panorama stitching poseconditioned video video translation several tasks overview see,1,1.0,1,1.0
pursuing highresolution generation video diffusion models despite tremendous progress generation existing methods still struggle produce multiview consistent images highresolution textures detail especially paradigm diffusion lacks awareness work present highresolution model new video diffusion based paradigm redefines single image multiview images sequential image generation ie orbital video generation methodology delves underlying temporal consistency knowledge video diffusion model generalizes well geometry consistency across multiple views generation technically first empowers pretrained video diffusion model prior camera pose condition yielding multiview images lowresolution texture details videotovideo refiner learnt scale multiview images highresolution texture details highresolution multiview images augmented novel views gaussian splatting finally leveraged obtain highfidelity meshes via reconstruction extensive experiments novel view synthesis single view reconstruction demonstrate manages produce superior multiview consistency images highlydetailed textures source code data available,1,1.0,1,1.0
multidiff consistent novel view synthesis single image introduce multidiff novel approach consistent novel view synthesis scenes single rgb image task synthesizing novel views single reference image highly illposed nature exist multiple plausible explanations unobserved areas address issue incorporate strong priors form monocular depth predictors videodiffusion models monocular depth enables us condition model warped reference images target views increasing geometric stability videodiffusion prior provides strong proxy scenes allowing model learn continuous pixelaccurate correspondences across generated images contrast approaches relying autoregressive image generation prone drifts error accumulation multidiff jointly synthesizes sequence frames yielding highquality multiview consistent results even longterm scene generation large camera movements reducing inference time order magnitude additional consistency image quality improvements introduce novel structured noise distribution experimental results demonstrate multidiff outperforms stateoftheart methods challenging realworld datasets scannet finally model naturally supports multiview consistent editing without need tuning,1,1.0,1,1.0
transformerbased image video inpainting current challenges future directions image inpainting currently hot topic within field computer vision offers viable solution various applications including photographic restoration video editing medical imaging deep learning advancements notably convolutional neural networks cnns generative adversarial networks gans significantly enhanced inpainting task improved capability fill missing damaged regions image video incorporation contextually appropriate details advancements improved aspects including efficiency information preservation achieving realistic textures structures recently visual transformers exploited offer improvements image video inpainting advent transformerbased architectures initially designed natural language processing also integrated computer vision tasks methods utilize selfattention mechanisms excel capturing longrange dependencies within data therefore particularly effective tasks requiring comprehensive understanding global context image video paper provide comprehensive review current image video inpainting approaches specific focus transformerbased techniques goal highlight significant improvements provide guideline new researchers field image video inpainting using visual transformers categorized transformerbased techniques architectural configurations types damage performance metrics furthermore present organized synthesis current challenges suggest directions future research field image video inpainting,-1,0.0,-1,0.0
video diffusion models strong video inpainter propagationbased video inpainting using optical flow pixel feature level recently garnered significant attention however limitations inaccuracy optical flow prediction propagation noise time issues result nonuniform noise time consistency problems throughout video particularly pronounced removed area large involves substantial movement address issues propose novel first frame filling video diffusion inpainting model fffvdi design fffvdi inspired capabilities pretrained imagetovideo diffusion models transform first frame image highly natural video apply video inpainting task propagate noise latent information future frames fill masked areas first frames noise latent code next finetune pretrained imagetovideo diffusion model generate inpainted video proposed model addresses limitations existing methods rely optical flow quality producing much natural temporally consistent videos proposed approach first effectively integrate imagetovideo diffusion models video inpainting tasks various comparative experiments demonstrate proposed model robustly handle diverse inpainting types high quality,2,0.6962709329283329,2,0.6962709329283329
enhancing temporal consistency video editing reconstructing videos gaussian splatting recent advancements zeroshot video diffusion models shown promise textdriven video editing challenges remain achieving high temporal consistency address introduce gaussian splatting video refiner designed enhance temporal consistency zeroshot video editors approach utilizes twostage gaussian optimizing process tailored editing dynamic monocular videos first stage employs improved version colmap referred mccolmap processes original videos using masked clipped approach video clip mccolmap generates point clouds dynamic foreground objects complex backgrounds point clouds utilized initialize two sets gaussians aiming represent foreground background views foreground background views merged learnable parameter map reconstruct full views second stage leverage reconstruction ability developed first stage impose temporal constraints video diffusion model demonstrate efficacy stages conduct extensive experiments across two related tasks video reconstruction video editing trained iterations significantly improves video reconstruction quality psnr psnr increase training efficiency times faster nerfbased stateofart methods davis dataset respectively moreover enhances video editing ensuring temporal consistency across dynamic monocular videos,9,1.0,9,1.0
contextaware talking face video generation paper consider novel practical case talking face video generation specifically focus scenarios involving multipeople interactions talking context audience surroundings present situations video generation take context consideration order generate video content naturally aligned driving audios spatially coherent context achieve provide twostage crossmodal controllable video generation pipeline taking facial landmarks explicit compact control signal bridge driving audio talking context generated videos inside pipeline devise video diffusion model allowing efficient contort spatial conditions landmarks context video well audio condition temporally coherent generation experimental results verify advantage proposed method baselines terms audiovideo synchronization video fidelity frame consistency,-1,0.0,-1,0.0
vlogger make dream vlog work present vlogger generic ai system generating minutelevel video blog ie vlog user descriptions different short videos seconds vlog often contains complex storyline diversified scenes challenging existing video generation approaches break bottleneck vlogger smartly leverages large language model llm director decomposes long video generation task vlog four key stages invoke various foundation models play critical roles vlog professionals including script actor showmaker voicer design mimicking human beings vlogger generate vlogs explainable cooperation topdown planning bottomup shooting moreover introduce novel video diffusion model showmaker serves videographer vlogger generating video snippet shooting scene incorporating script actor attentively textual visual prompts effectively enhance spatialtemporal coherence snippet besides design concise mixed training paradigm showmaker boosting capacity generation prediction finally extensive experiments show method achieves stateoftheart performance zeroshot generation prediction tasks importantly vlogger generate vlogs openworld descriptions without loss video coherence script actor code model available httpsgithubcomzhuangshaobinvlogger,-1,0.0,-1,0.0
effivedefficient video editing via textinstruction diffusion models largescale texttovideo models shown remarkable abilities direct application video editing remains challenging due limited available datasets current video editing methods commonly require pervideo finetuning diffusion models specific inversion optimization ensure highfidelity edits paper introduce effived efficient diffusionbased model directly supports instructionguided video editing achieve present two efficient workflows gather video editing pairs utilizing augmentation fundamental visionlanguage techniques workflows transform vast image editing datasets openworld videos highquality dataset training effived experimental results reveal effived generates highquality editing videos also executes rapidly finally demonstrate data collection method significantly improves editing performance potentially tackle scarcity video editing data code found httpsgithubcomalibabaeffived,-1,0.0,-1,0.0
efficient video audio mapper visual scene detection videotoaudio generation aims produce corresponding audio given silent video inputs task particularly challenging due crossmodality sequential nature audiovisual features involved recent works made significant progress bridging domain gap video audio generating audio semantically aligned video content however critical limitation approaches inability effectively recognize handle multiple scenes within video often leading suboptimal audio generation cases paper first reimplement stateoftheart model slightly modified lightweight architecture achieving results outperform baseline propose improved model incorporates scene detector address challenge switching multiple visual scenes results vggsound show model recognize handle multiple scenes within video achieve superior performance baseline fidelity relevance,-1,0.0,-1,0.0
odvae omnidimensional video compressor improving latent video diffusion model variational autoencoder vae compressing videos latent representations crucial preceding component latent video diffusion models lvdms reconstruction quality sufficient vaes compression videos efficient lvdms however lvdms utilize image vae whose compression videos spatial dimension often ignored temporal dimension conduct temporal compression videos vae obtain concise latent representations promising accurate reconstruction seldom explored fill gap propose omnidimension compression vae named odvae temporally spatially compress videos although odvaes sufficient compression brings great challenge video reconstruction still achieve high reconstructed accuracy fine design obtain better tradeoff video reconstruction quality compression speed four variants odvae introduced analyzed addition novel tail initialization designed train odvae efficiently novel inference strategy proposed enable odvae handle videos arbitrary length limited gpu memory comprehensive experiments video reconstruction lvdmbased video generation demonstrate effectiveness efficiency proposed methods,2,0.8100290375777531,2,0.8100290375777531
moviedreamer hierarchical generation coherent long visual sequence recent advancements video generation primarily leveraged diffusion models shortduration content however approaches often fall short modeling complex narratives maintaining character consistency extended periods essential longform video production like movies propose moviedreamer novel hierarchical framework integrates strengths autoregressive models diffusionbased rendering pioneer longduration video generation intricate plot progressions high visual fidelity approach utilizes autoregressive models global narrative coherence predicting sequences visual tokens subsequently transformed highquality video frames diffusion rendering method akin traditional movie production processes complex stories factorized manageable scene capturing employ multimodal script enriches scene descriptions detailed character information visual style enhancing continuity character identity across scenes present extensive experiments across various movie genres demonstrating approach achieves superior visual narrative quality also effectively extends duration generated content significantly beyond current capabilities homepage httpsaimuofagithubiomoviedreamer,-1,0.0,-1,0.0
transforming static images using generative models video salient object detection many video processing tasks leveraging largescale image datasets common strategy image data abundant facilitates comprehensive knowledge transfer typical approach simulating video static images involves applying spatial transformations affine transformations spline warping create sequences mimic temporal progression however tasks like video salient object detection appearance motion cues critical basic imagetovideo techniques fail produce realistic optical flows capture independent motion properties object study show imagetovideo diffusion models generate realistic transformations static images understanding contextual relationships image components ability allows model generate plausible optical flows preserving semantic integrity reflecting independent motion scene elements augmenting individual images way create largescale imageflow pairs significantly enhance model training approach achieves stateoftheart performance across public benchmark datasets outperforming existing approaches,9,0.7268500122592858,9,0.7268500122592858
temporal plugin unsupervised video denoising pretrained image denoisers recent advancements deep learning shown impressive results image video denoising leveraging extensive pairs noisy noisefree data supervision however challenge acquiring paired videos dynamic scenes hampers practical deployment deep video denoising techniques contrast obstacle less pronounced image denoising paired data readily available thus welltrained image denoiser could serve reliable spatial prior video denoising paper propose novel unsupervised video denoising framework named temporal plugin tap integrates tunable temporal modules pretrained image denoiser incorporating temporal modules method harness temporal information across noisy frames complementing power spatial denoising furthermore introduce progressive finetuning strategy refines temporal module using generated pseudo clean video frames progressively enhancing networks denoising performance compared unsupervised video denoising methods framework demonstrates superior performance srgb raw video denoising datasets,-1,0.0,-1,0.0
predicting longhorizon futures conditioning geometry time work explores task generating future sensor observations conditioned past motivated predictive coding concepts neuroscience well robotic applications selfdriving vehicles predictive video modeling challenging future may multimodal learning scale remains computationally expensive video processing address challenges key insight leverage largescale pretraining image diffusion models handle multimodality repurpose image models video prediction conditioning new frame timestamps models trained videos static dynamic scenes allow trained modestlysized datasets introduce invariances factoring illumination texture forcing model predict pseudo depth readily obtained inthewild videos via offtheshelf monocular depth networks fact show simply modifying networks predict grayscale pixels already improves accuracy video prediction given extra controllability timestamp conditioning propose sampling schedules work better traditional autoregressive hierarchical sampling strategies motivated probabilistic metrics object forecasting literature create benchmark video prediction diverse set videos spanning indoor outdoor scenes large vocabulary objects experiments illustrate effectiveness learning condition timestamps show importance predicting future invariant modalities,-1,0.0,-1,0.0
opticalflow guided prompt optimization coherent video generation texttovideo diffusion models made significant strides many still face challenges generating videos temporal consistency within diffusion frameworks guidance techniques proven effective enhancing output quality inference however applying methods video diffusion models introduces additional complexity handling computations across entire sequences address propose novel framework called motionprompt guides video generation process via optical flow specifically train discriminator distinguish optical flow random pairs frames real videos generated ones given prompts influence entire video optimize learnable token embeddings reverse sampling steps using gradients trained discriminator applied random frame pairs approach allows method generate visually coherent video sequences closely reflect natural motion dynamics without compromising fidelity generated content demonstrate effectiveness approach across various models,9,0.7676269057144607,9,0.7676269057144607
fastercache trainingfree video diffusion model acceleration high quality paper present textbftextitfastercache novel trainingfree strategy designed accelerate inference video diffusion models highquality generation analyzing existing cachebased methods observe textitdirectly reusing adjacentstep features degrades video quality due loss subtle variations perform pioneering investigation acceleration potential classifierfree guidance cfg reveal significant redundancy conditional unconditional features within timestep capitalizing observations introduce fastercache substantially accelerate diffusionbased video generation key contributions include dynamic feature reuse strategy preserves feature distinction temporal continuity cfgcache optimizes reuse conditional unconditional outputs enhance inference speed without compromising video quality empirically evaluate fastercache recent video diffusion models experimental results show fastercache significantly accelerate video generation eg speedup keeping video quality comparable baseline consistently outperform existing methods inference speed video quality,-1,0.0,-1,0.0
diffbgm diffusion model video background music generation editing video piece attractive background music indispensable however video background music generation tasks face several challenges example lack suitable training datasets difficulties flexibly controlling music generation process sequentially aligning video music work first propose highquality musicvideo dataset detailed annotation shot detection provide multimodal information video music present evaluation metrics assess music quality including music diversity alignment music video retrieval precision metrics finally propose diffbgm framework automatically generate background music given video uses different signals control different aspects music generation process ie uses dynamic video features control music rhythm semantic features control melody atmosphere propose align video music sequentially introducing segmentaware crossattention layer experiments verify effectiveness proposed method code models available httpsgithubcomsizheleediffbgm,8,0.48856600801587136,8,0.48856600801587136
overcoming data limitations highquality video diffusion models texttovideo generation aims produce video based given prompt recently several commercial video models able generate plausible videos minimal noise excellent details high aesthetic scores however models rely largescale wellfiltered highquality videos accessible community many existing research works train models using lowquality dataset struggle generate highquality videos models optimized fit work explore training scheme video models extended stable diffusion investigate feasibility leveraging lowquality videos synthesized highquality images obtain highquality video model first analyze connection spatial temporal modules video models distribution shift lowquality videos observe full training modules results stronger coupling spatial temporal modules training temporal modules based stronger coupling shift distribution higher quality without motion degradation finetuning spatial modules highquality images resulting generic highquality video model evaluations conducted demonstrate superiority proposed method particularly picture quality motion concept composition,2,0.6860564220557998,2,0.6860564220557998
salova segmentaugmented long video assistant targeted retrieval routing longform video analysis despite advances large multimodal models applying long untrimmed video content remains challenging due limitations context length substantial memory overhead constraints often lead significant information loss reduced relevance model responses exponential growth video data across web platforms understanding longform video crucial advancing generalized intelligence paper introduce salova segmentaugmented long video assistant novel videollm framework designed enhance comprehension lengthy video content targeted retrieval process address two main challenges achieve present scenewalk dataset highquality collection long videos densely captioned segment level enable models capture scene continuity maintain rich descriptive context ii develop robust architectural designs integrating dynamic routing mechanism spatiotemporal projector efficiently retrieve process relevant video segments based user queries framework mitigates limitations current videolmms allowing precise identification retrieval relevant video segments response queries thereby improving contextual relevance generated responses extensive experiments salova demonstrates enhanced capability processing complex longform videos showing significant capability maintain contextual integrity across extended sequences,-1,0.0,-1,0.0
disentangling foreground background motion enhanced realism human video generation recent advancements human video synthesis enabled generation highquality videos application stable diffusion models however existing methods predominantly concentrate animating solely human element foreground guided pose information leaving background entirely static contrary authentic highquality videos backgrounds often dynamically adjust harmony foreground movements eschewing stagnancy introduce technique concurrently learns foreground background dynamics segregating movements using distinct motion representations human figures animated leveraging posebased motion capturing intricate actions conversely backgrounds employ sparse tracking points model motion thereby reflecting natural interaction foreground activity environmental changes training realworld videos enhanced innovative motion depiction approach model generates videos exhibiting coherent movement foreground subjects surrounding contexts extend video generation longer sequences without accumulating errors adopt clipbyclip generation strategy introducing global features step ensure seamless continuity across segments ingeniously link final frame produced clip input noise spawn succeeding one maintaining narrative flow throughout sequential generation process infuse feature representation initial reference image network effectively curtailing cumulative color inconsistencies may otherwise arise empirical evaluations attest superiority method producing videos exhibit harmonious interplay foreground actions responsive background dynamics surpassing prior methodologies regard,-1,0.0,-1,0.0
vidu highly consistent dynamic skilled texttovideo generator diffusion models introduce vidu highperformance texttovideo generator capable producing videos seconds single generation vidu diffusion model uvit backbone unlocks scalability capability handling long videos vidu exhibits strong coherence dynamism capable generating realistic imaginative videos well understanding professional photography techniques par sora powerful reported texttovideo generator finally perform initial experiments controllable video generation including cannytovideo generation video prediction subjectdriven generation demonstrate promising results,11,0.9235240508025361,11,0.9235240508025361
image scene learning imagine world million videos threedimensional understanding objects scenes play key role humans ability interact world active area research computer vision graphics robotics large scale synthetic objectcentric datasets shown effective training models understanding objects however applying similar approach realworld objects scenes difficult due lack largescale data videos potential source realworld data finding diverse yet corresponding views content shown difficult scale furthermore standard videos come fixed viewpoints determined time capture restricts ability access scenes variety diverse potentially useful perspectives argue large scale videos address limitations provide scalable corresponding frames diverse views paper introduce video dataset process efficiently finding corresponding frames diverse viewpoints scale train diffusionbased model odin empowered largest realworld multiview dataset date odin able freely generate novel views realworld scenes unlike previous methods odin move camera environment enabling model infer geometry layout scene additionally show improved performance standard novel view synthesis reconstruction benchmarks,1,0.9372892339140257,1,0.9372892339140257
vast unified framework controllable consistent video generation generating highquality videos textual descriptions poses challenges maintaining temporal coherence control subject motion propose vast video storyboard text twostage framework address challenges enable highquality video generation first stage storyforge transforms textual descriptions detailed storyboards capturing human poses object layouts represent structural essence scene second stage visionforge generates videos storyboards producing highquality videos smooth motion temporal consistency spatial coherence decoupling text understanding video generation vast enables precise control subject dynamics scene composition experiments vbench benchmark demonstrate vast outperforms existing methods visual quality semantic expression setting new standard dynamic coherent video generation,-1,0.0,-1,0.0
actanywhere subjectaware video background generation generating video background tailors foreground subject motion important problem movie industry visual effects community task involves synthesizing background aligns motion appearance foreground subject also complies artists creative intention introduce actanywhere generative model automates process traditionally requires tedious manual efforts model leverages power largescale video diffusion models specifically tailored task actanywhere takes sequence foreground subject segmentation input image describes desired scene condition produce coherent video realistic foregroundbackground interactions adhering condition frame train model largescale dataset humanscene interaction videos extensive evaluations demonstrate superior performance model significantly outperforming baselines moreover show actanywhere generalizes diverse outofdistribution samples including nonhuman subjects please visit project webpage httpsactanywheregithubio,-1,0.0,-1,0.0
onlinevpo align video diffusion model online videocentric preference optimization recent years field texttovideo generation made significant strides despite progress still gap theoretical advancements practical application amplified issues like degraded image quality flickering artifacts recent advancements enhancing video diffusion model vdm feedback learning shown promising results however methods still exhibit notable limitations misaligned feedback inferior scalability tackle issues introduce onlinevpo efficient preference learning approach tailored specifically video diffusion models method features two novel designs firstly instead directly using imagebased reward feedback leverage video quality assessment vqa model trained synthetic data reward model provide distribution modalityaligned feedback video diffusion model additionally introduce online dpo algorithm address offpolicy optimization scalability issue existing video preference learning frameworks employing video reward model offer concise video feedback fly onlinevpo offers effective efficient preference guidance extensive experiments opensource videodiffusion model demonstrate onlinevpo simple yet effective importantly scalable preference learning algorithm video diffusion models offering valuable insights future advancements domain,-1,0.0,-1,0.0
covert hiding visual editing robust generative video steganography traditional video steganography methods based modifying covert space embedding whereas propose innovative approach embeds secret message within semantic feature steganography video editing process although existing traditional video steganography methods display certain level security embedding capacity lack adequate robustness common distortions online social networks osns paper introduce endtoend robust generative video steganography network rogvs achieves visual editing modifying semantic feature videos embed secret message employ faceswapping scenario showcase visual editing effects first design secret message embedding module adaptively hide secret message semantic feature videos extensive experiments display proposed rogvs method applied facial video datasets demonstrate superiority existing video image steganography techniques terms robustness capacity,4,0.8713470412900549,4,0.8713470412900549
zerohsi zeroshot humanscene interaction video generation humanscene interaction hsi generation crucial applications embodied ai virtual reality robotics yet existing methods synthesize interactions unseen environments inthewild scenes reconstructed scenes rely paired scenes captured human motion data training unavailable unseen environments present zerohsi novel approach enables zeroshot humanscene interaction synthesis eliminating need training mocap data key insight distill humanscene interactions stateoftheart video generation models trained vast amounts natural human movements interactions use differentiable rendering reconstruct humanscene interactions zerohsi synthesize realistic human motions static scenes environments dynamic objects without requiring groundtruth motion data evaluate zerohsi curated dataset different types various indoor outdoor scenes different interaction prompts demonstrating ability generate diverse contextually appropriate humanscene interactions,-1,0.0,-1,0.0
avlink temporallyaligned diffusion features crossmodal audiovideo generation propose avlink unified framework videotoaudio audiotovideo generation leverages activations frozen video audio diffusion models temporallyaligned crossmodal conditioning key framework fusion block facilitates bidirectional information exchange video audio diffusion models temporallyaligned self attention operations unlike prior work uses dedicated models tasks relies pretrained feature extractors avlink achieves tasks single selfcontained framework directly leveraging features obtained complementary modality ie video features generate audio audio features generate video extensive automatic subjective evaluations demonstrate method achieves substantial improvement audiovideo synchronization outperforming expensive baselines moviegen videotoaudio model,8,0.6316186047574378,8,0.6316186047574378
promptus prompts streaming replace video streaming stable diffusion exponential growth video traffic traditional video streaming systems approaching limits compression efficiency communication capacity reduce bitrate maintaining quality propose promptus disruptive novel system streaming prompts instead video content stable diffusion converts video frames series prompts delivery ensure pixel alignment gradient descentbased prompt fitting framework proposed achieve adaptive bitrate prompts lowrank decompositionbased bitrate control algorithm introduced interframe compression prompts temporal smoothingbased prompt interpolation algorithm proposed evaluations across various video domains real network traces demonstrate promptus enhance perceptual quality lpips compared vae respectively decreases ratio severely distorted frames moreover promptus achieves realtime video generation prompts fps best knowledge promptus first attempt replace video codecs prompt inversion first use prompt streaming instead video streaming work opens new paradigm efficient video communication beyond shannon limit,2,1.0,2,1.0
deco decoupled humancentered diffusion video editing motion consistency diffusion models usher new era video editing flexibly manipulating video contents text prompts despite widespread application demand editing humancentered videos models face significant challenges handling complex objects like humans paper introduce deco novel video editing framework specifically designed treat humans background separate editable targets ensuring global spatialtemporal consistency maintaining coherence individual component specifically propose decoupled dynamic human representation utilizes parametric human body prior generate tailored humans preserving consistent motions original video addition consider background layered atlas apply textguided image editing approaches enhance geometry texture humans optimization extend calculation score distillation sampling normal space image space moreover tackle inconsistent lighting edited targets leveraging lightingaware video harmonizer problem previously overlooked decomposeeditcombine approaches extensive qualitative numerical experiments demonstrate deco outperforms prior video editing methods humancentered videos especially longer videos,9,0.8939498424067003,9,0.8939498424067003
towards motion video diffusion models textconditioned video diffusion models emerged powerful tool realm video generation editing ability capture nuances human movement remains underexplored indeed ability models faithfully model array text prompts lead wide host applications human character animation work take initial steps investigate whether models effectively guide synthesis realistic human body animations specifically propose synthesize human motion deforming smplx body representation guided score distillation sampling sds calculated using video diffusion model analyzing fidelity resulting animations gain insights extent obtain motion using publicly available texttovideo diffusion models using sds findings shed light potential limitations models generating diverse plausible human motions paving way research exciting area,-1,0.0,-1,0.0
crossmodal video summarization temporal prompt instruction tuning video summarization aims create short accurate cohesive summaries longer videos despite existence various video summarization datasets notable limitation limited amount source videos hampers effective training advanced large visionlanguage models vlms additionally existing datasets created videotovideo summarization overlooking contemporary need multimodal video content summarization recent efforts made expand unimodal multimodal video summarization categorizing task three subtasks based summarys modality videotovideo videototext combination video text summarization however textual summaries previous multimodal datasets inadequate address issues introduce crossmodal video summarization dataset featuring diverse videos sourced youtube lengths ranging seconds average summarization ratio video summary paired textual summary references specific frame indexes facilitating generation aligned video textual summaries addition propose new video summarization framework named specifically study first framework unifies different video summarization tasks one large language models llm text decoder achieves taskcontrollable video summarization temporal prompts task instructions experiments show outperforms strong baseline models multiple video summarization tasks furthermore propose enhanced evaluation metric summarization tasks,0,1.0,0,1.0
gendds generating diverse driving video scenarios prompttovideo generative model autonomous driving training requires diverse range datasets encompassing various traffic conditions weather scenarios road types traditional data augmentation methods often struggle generate datasets represent rare occurrences address challenge propose gendds novel approach generating driving scenarios generation leveraging capabilities stable diffusion xl sdxl advanced latent diffusion model methodology involves use descriptive prompts guide synthesis process aimed producing realistic diverse driving scenarios power latest computer vision techniques controlnet hotshotxl built complete pipeline video generation together sdxl employ kitti dataset includes realworld driving videos train model series experiments demonstrate model generate highquality driving videos closely replicate complexity variability realworld driving scenarios research contributes development sophisticated training data autonomous driving systems opens new avenues creating virtual environments simulation validation purposes,16,0.923693530365291,16,0.923693530365291
emo emote portrait alive generating expressive portrait videos diffusion model weak conditions work tackle challenge enhancing realism expressiveness talking head video generation focusing dynamic nuanced relationship audio cues facial movements identify limitations traditional techniques often fail capture full spectrum human expressions uniqueness individual facial styles address issues propose emo novel framework utilizes direct audiotovideo synthesis approach bypassing need intermediate models facial landmarks method ensures seamless frame transitions consistent identity preservation throughout video resulting highly expressive lifelike animations experimental results demonsrate emo able produce convincing speaking videos also singing videos various styles significantly outperforming existing stateoftheart methodologies terms expressiveness realism,6,1.0,6,1.0
divd deblurring improved video diffusion model video deblurring presents considerable challenge owing complexity blur frequently results combination camera shakes object motions field video deblurring many previous works primarily concentrated distortionbased metrics psnr however approach often results weak correlation human perception yields reconstructions lack realism diffusion models video diffusion models respectively excelled fields image video generation particularly achieving remarkable results terms image authenticity realistic perception however due computational complexity challenges inherent adapting diffusion models still uncertainty regarding potential video diffusion models video deblurring tasks explore viability video diffusion models task video deblurring introduce diffusion model specifically purpose field leveraging highly correlated information adjacent frames addressing challenge temporal misalignment crucial research directions tackle challenges many improvements based video diffusion model introduced work result model outperforms existing models achieves stateoftheart results range perceptual metrics model preserves significant amount detail images maintaining competitive distortion metrics furthermore best knowledge first time diffusion model applied video deblurring overcome limitations mentioned,2,0.6940616246766085,2,0.6940616246766085
consistent human image video generation spatially conditioned diffusion consistent humancentric image video synthesis aims generate images videos new poses preserving appearance consistency given reference image crucial lowcost visual content creation recent advances based diffusion models typically rely separate networks reference appearance feature extraction target visual generation leading inconsistent domain gaps references targets paper frame task spatiallyconditioned inpainting problem target image inpainted maintain appearance consistency reference approach enables reference features guide generation posecompliant targets within unified denoising network thereby mitigating domain gaps additionally better maintain reference appearance information impose causal feature interaction framework reference features query target features query appearance information reference target enhance computational efficiency flexibility practical implementation decompose spatiallyconditioned generation process two stages reference appearance extraction conditioned target generation stages share single denoising network interactions restricted selfattention layers proposed method ensures flexible control appearance generated human images videos finetuning existing base diffusion models human video data method demonstrates strong generalization unseen human identities poses without requiring additional perinstance finetuning experimental results validate effectiveness approach showing competitive performance compared existing methods consistent human image video synthesis,-1,0.0,-1,0.0
efficient video diffusion models via contentframe motionlatent decomposition video diffusion models recently made great progress generation quality still limited high memory computational requirements current video diffusion models often attempt process highdimensional videos directly tackle issue propose contentmotion latent diffusion model cmd novel efficient extension pretrained image diffusion models video generation specifically propose autoencoder succinctly encodes video combination content frame like image lowdimensional motion latent representation former represents common content latter represents underlying motion video respectively generate content frame finetuning pretrained image diffusion model generate motion latent representation training new lightweight diffusion model key innovation design compact latent space directly utilizes pretrained image diffusion model done previous latent video diffusion models leads considerably better quality generation reduced computational costs instance cmd sample video faster prior approaches generating video resolution length seconds moreover cmd achieves fvd score better previous stateoftheart,2,0.6962709329283329,2,0.6962709329283329
cvvae compatible video vae latent generative video models spatiotemporal compression videos utilizing networks variational autoencoders vae plays crucial role openais sora numerous video generative models instance many llmlike video models learn distribution discrete tokens derived vaes within vqvae framework diffusionbased video models capture distribution continuous latent extracted vaes without quantization temporal compression simply realized uniform frame sampling results unsmooth motion consecutive frames currently lacks commonly used continuous video vae latent diffusionbased video models research community moreover since current diffusionbased approaches often implemented using pretrained texttoimage models directly training video vae without considering compatibility existing models result latent space gap take huge computational resources training bridge gap even models initialization address issue propose method training video vae latent video models namely cvvae whose latent space compatible given image vae eg image vae stable diffusion sd compatibility achieved proposed novel latent space regularization involves formulating regularization loss using image vae benefiting latent space compatibility video models trained seamlessly pretrained video models truly spatiotemporally compressed latent space rather simply sampling video frames equal intervals cvvae existing video models generate four times frames minimal finetuning extensive experiments conducted demonstrate effectiveness proposed video vae,2,0.7350491287834391,2,0.7350491287834391
invi object insertion videos using offtheshelf diffusion models introduce invi approach inserting replacing objects within videos referred inpainting using offtheshelf texttoimage latent diffusion models invi targets controlled manipulation objects blending seamlessly background video unlike existing video editing methods focus comprehensive restyling entire scene alterations achieve goal tackle two key challenges firstly high quality control blending employ twostep process involving inpainting matching process begins inserting object single frame using controlnetbased inpainting diffusion model generating subsequent frames conditioned features inpainted frame anchor minimize domain gap background object secondly ensure temporal coherence replace diffusion models selfattention layers extendedattention layers anchor frame features serve keys values layers enhancing consistency across frames approach removes need videospecific finetuning presenting efficient adaptable solution experimental results demonstrate invi achieves realistic object insertion consistent blending coherence across frames outperforming existing methods,9,1.0,9,1.0
track answer extending textvqa image video spatiotemporal clues video textbased visual question answering video textvqa practical task aims answer questions jointly reasoning textual visual information given video inspired development textvqa image domain existing video textvqa approaches leverage language model eg process textrich multiple frames generate answers autoregressively nevertheless spatiotemporal relationships among visual entities including scene text objects disrupted models susceptible interference unrelated information resulting irrational reasoning inaccurate answering tackle challenges propose tea stands textbftrack thtextbfe textbfanswer method better extends generative textvqa framework image video tea recovers spatiotemporal relationships complementary way incorporates ocraware clues enhance quality reasoning questions extensive experiments several public video textvqa datasets validate effectiveness generalization framework tea outperforms existing textvqa methods videolanguage pretraining methods video large language models great margins,0,1.0,0,1.0
jvid joint videoimage diffusion visualquality temporalconsistency video generation introduce joint videoimage diffusion model jvid novel approach generating highquality temporally coherent videos achieve integrating two diffusion models latent image diffusion model lidm trained images latent video diffusion model lvdm trained video data method combines models reverse diffusion process lidm enhances image quality lvdm ensures temporal consistency unique combination allows us effectively handle complex spatiotemporal dynamics video generation results demonstrate quantitative qualitative improvements producing realistic coherent videos,-1,0.0,-1,0.0
frame familiar frame understanding replication video diffusion models building momentum image generation diffusion models increasing interest videobased diffusion models however video generation poses greater challenges due higherdimensional nature scarcity training data complex spatiotemporal relationships involved image generation models due extensive data requirements already strained computational resources limits instances models reproducing elements training samples leading concerns even legal disputes sample replication video diffusion models operate even constrained datasets tasked generating spatial temporal content may prone replicating samples training sets compounding issue models often evaluated using metrics inadvertently reward replication paper present systematic investigation phenomenon sample replication video diffusion models scrutinize various recent diffusion models video synthesis assessing tendency replicate spatial temporal content unconditional conditional generation scenarios study identifies strategies less likely lead replication furthermore propose new evaluation strategies take replication account offering accurate measure models ability generate original content,-1,0.0,-1,0.0
long video diffusion generation segmented crossattention contentrich video data curation introduce presto novel video diffusion model designed generate videos longrange coherence rich content extending video generation methods maintain scenario diversity long durations presents significant challenges address propose segmented crossattention sca strategy splits hidden states segments along temporal dimension allowing segment crossattend corresponding subcaption sca requires additional parameters enabling seamless incorporation current ditbased architectures facilitate highquality long video generation build longtakehd dataset consisting contentrich videos scenario coherence annotated overall video caption five progressive subcaptions experiments show presto achieves vbench semantic score dynamic degree outperforming existing stateoftheart video generation methods demonstrates proposed presto significantly enhances content richness maintains longrange coherence captures intricate textual details details displayed project page httpsprestovideogithubio,-1,0.0,-1,0.0
vidprom millionscale real promptgallery dataset texttovideo diffusion models arrival sora marks new era texttovideo diffusion models bringing significant advancements video generation potential applications however sora along texttovideo diffusion models highly reliant prompts publicly available dataset features study texttovideo prompts paper introduce vidprom first largescale dataset comprising million unique texttovideo prompts real users additionally dataset includes million videos generated four stateoftheart diffusion models alongside related data initially discuss curation largescale dataset process timeconsuming costly subsequently underscore need new prompt dataset specifically designed texttovideo generation illustrating vidprom differs diffusiondb largescale promptgallery dataset image generation extensive diverse dataset also opens many exciting new research areas instance suggest exploring texttovideo prompt engineering efficient video generation video copy detection diffusion models develop better efficient safer models project including collected dataset vidprom related code publicly available httpsvidpromgithubio ccbync license,-1,0.0,-1,0.0
extreme video compression pretrained diffusion models diffusion models achieved remarkable success generating high quality image video data recently also used image compression high perceptual quality paper present novel approach extreme video compression leveraging predictive power diffusionbased generative models decoder conditional diffusion model takes several neural compressed frames generates subsequent frames reconstruction quality drops desired level new frames encoded restart prediction entire video sequentially encoded achieve visually pleasing reconstruction considering perceptual quality metrics learned perceptual image patch similarity lpips frechet video distance fvd bit rates low bits per pixel bpp experimental results demonstrate effectiveness proposed scheme compared standard codecs low bpp regime results showcase potential exploiting temporal relations video data using generative models code available httpsgithubcomelesionkyrieextremevideocompressionwithpredictionusingpretraindeddiffusionmodels,-1,0.0,-1,0.0
customizeavideo oneshot motion customization texttovideo diffusion models image customization extensively studied texttoimage diffusion models leading impressive outcomes applications emergence texttovideo diffusion models temporal counterpart motion customization yet well investigated address challenge oneshot video motion customization propose customizeavideo models motion single reference video adapts new subjects scenes spatial temporal varieties leverages lowrank adaptation lora temporal attention layers tailor pretrained diffusion model specific motion modeling disentangle spatial temporal information training introduce novel concept appearance absorbers detach original appearance reference video prior motion learning proposed modules trained staged pipeline inferred plugandplay fashion enabling easy extensions various downstream tasks custom video generation editing video appearance customization multiple motion combination project page found httpscustomizeavideogithubio,-1,0.0,-1,0.0
grid diffusion models texttovideo generation recent advances diffusion models significantly improved texttoimage generation however generating videos text challenging task generating images text due much larger dataset higher computational cost required existing video generation methods use either unet architecture considers temporal dimension autoregressive generation methods require large datasets limited terms computational costs compared texttoimage generation tackle challenges propose simple effective novel grid diffusion texttovideo generation without temporal dimension architecture large textvideo paired dataset generate highquality video using fixed amount gpu memory regardless number frames representing video grid image additionally since method reduces dimensions video dimensions image various imagebased methods applied videos textguided video manipulation image manipulation proposed method outperforms existing methods quantitative qualitative evaluations demonstrating suitability model realworld video generation,-1,0.0,-1,0.0
warped diffusion solving video inverse problems image diffusion models using image models naively solving inverse video problems often suffers flickering texturesticking temporal inconsistency generated videos tackle problems paper view frames continuous functions space videos sequence continuous warping transformations different frames perspective allows us train function space diffusion models images utilize solve temporally correlated inverse problems function space diffusion models need equivariant respect underlying spatial transformations ensure temporal consistency introduce simple posthoc testtime guidance towards selfequivariant solutions method allows us deploy stateoftheart latent diffusion models stable diffusion xl solve video inverse problems demonstrate effectiveness method video inpainting video superresolution outperforming existing techniques based noise transformations provide generated video results,-1,0.0,-1,0.0
towards multitask multimodal models video generative perspective advancements language foundation models primarily fueled recent surge artificial intelligence contrast generative learning nontextual modalities especially videos significantly trails behind language modeling thesis chronicles endeavor build multitask models generating videos modalities diverse conditions well understanding compression applications given high dimensionality visual data pursue concise accurate latent representations videonative spatialtemporal tokenizers preserve high fidelity unveil novel approach mapping bidirectionally visual observation interpretable lexical terms furthermore scalable visual token representation proves beneficial across generation compression understanding tasks achievement marks first instances language models surpassing diffusion models visual synthesis video tokenizer outperforming industrystandard codecs within multimodal latent spaces study design multitask generative models masked multitask transformer excels quality efficiency flexibility video generation enable frozen language model trained solely text generate visual content finally build scalable generative multimodal transformer trained scratch enabling generation videos containing highfidelity motion corresponding audio given diverse conditions throughout course shown effectiveness integrating multiple tasks crafting highfidelity latent representation generating multiple modalities work suggests intriguing potential future exploration generating nontextual data enabling realtime interactive experiences across various media forms,-1,0.0,-1,0.0
reenact anything semantic video motion transfer using motiontextual inversion recent years seen tremendous improvement quality video generation editing approaches several techniques focus editing appearance address motion current approaches using text trajectories bounding boxes limited simple motions specify motions single motion reference video instead propose use pretrained imagetovideo model rather texttovideo model approach allows us preserve exact appearance position target object scene helps disentangle appearance motion method called motiontextual inversion leverages observation imagetovideo models extract appearance mainly latent image input textimage embedding injected via crossattention predominantly controls motion thus represent motion using textimage embedding tokens operating inflated motiontext embedding containing multiple textimage embedding tokens per frame achieve high temporal motion granularity optimized motion reference video embedding applied various target images generate videos semantically similar motions approach require spatial alignment motion reference video target image generalizes across various domains applied various tasks fullbody face reenactment well controlling motion inanimate objects camera empirically demonstrate effectiveness method semantic video motion transfer task significantly outperforming existing methods context,-1,0.0,-1,0.0
generative video diffusion unseen novel semantic video moment retrieval video moment retrieval vmr aims locate likely video moments corresponding text query untrimmed videos training existing methods limited lack diverse generalisable vmr datasets hindering ability generalise momenttext associations queries containing novel semantic concepts unseen visually textually training source domain model generalisation novel semantics existing methods rely heavily assuming access video text sentence pairs target domain addition source domain pairwise training data neither practical scalable work introduce generalisable approach assuming text sentences describing new semantics available model training without seen videos target domain end propose finegrained video editing framework termed fve explores generative video diffusion facilitate finegrained video editing seen source concepts unseen target sentences consisting new concepts enables generative hypotheses unseen video moments corresponding novel concepts target domain finegrained generative video diffusion retains original video structure subject specifics source domain introducing semantic distinctions unseen novel vocabularies target domain critical challenge enable generative finegrained diffusion process meaningful optimising vmr synthesising visually pleasing videos solve problem introducing hybrid selection mechanism integrates three quantitative metrics selectively incorporate synthetic video moments novel video hypotheses enlarged additions original source training data whilst minimising potential,-1,0.0,-1,0.0
rethinking clipbased video learners crossdomain openvocabulary action recognition building upon impressive success clip contrastive languageimage pretraining recent pioneer works proposed adapt powerful clip video data leading efficient effective video learners openvocabulary action recognition inspired humans perform actions diverse environments work delves intriguing question clipbased video learners effectively generalize video domains encountered training answer establish crossdomain openvocabulary action recognition benchmark named xovaction conduct comprehensive evaluation five stateoftheart clipbased video learners various types domain gaps evaluation demonstrates previous methods exhibit limited action recognition performance unseen video domains revealing potential challenges crossdomain openvocabulary action recognition task paper focus one critical challenge task namely scene bias accordingly contribute novel sceneaware videotext alignment method key idea distinguish video representations apart sceneencoded text representations aiming learn sceneagnostic video representations recognizing actions across domains extensive experiments demonstrate effectiveness method benchmark code available httpsgithubcomkunyulinxovaction,7,0.7997363254563419,7,0.7997363254563419
optical flow representation alignment mamba diffusion model medical video generation medical video generation models expected profound impact healthcare industry including limited medical education training surgical planning simulation current video diffusion models typically build image diffusion architecture incorporating temporal operations convolution temporal attention although approach effective oversimplification limits spatiotemporal performance consumes substantial computational resources counter propose medical simulation video generator medsora incorporates three key elements video diffusion framework integrates advantages attention mamba balancing low computational load highquality video generation ii optical flow representation alignment method implicitly enhances attention interframe pixels iii video variational autoencoder vae frequency compensation addresses information loss medical features occurs transforming pixel space latent features back pixel frames extensive experiments applications demonstrate medsora exhibits superior visual quality generating medical videos outperforming advanced baseline methods results code available httpswongzbbgithubiomedsora,-1,0.0,-1,0.0
controllable longer image animation diffusion models generating realistic animated videos static images important area research computer vision methods based physical simulation motion prediction achieved notable advances often limited specific object textures motion trajectories failing exhibit highly complex environments physical dynamics paper introduce opendomain controllable image animation method using motion priors video diffusion models method achieves precise control direction speed motion movable region extracting motion field information videos learning moving trajectories strengths current pretrained video generation models typically limited producing short videos typically less frames contrast propose efficient longduration video generation method based noise reschedule specifically tailored image animation tasks facilitating creation videos frames length maintaining consistency content scenery motion coordination specifically decompose denoise process two distinct phases shaping scene contours refining motion details reschedule noise control generated frame sequences maintaining longdistance noise correlation conducted extensive experiments baselines encompassing commercial tools academic methodologies demonstrate superiority method project page,-1,0.0,-1,0.0
tcbench benchmarking temporal compositionality texttovideo imagetovideo generation video generation many unique challenges beyond image generation temporal dimension introduces extensive possible variations across frames consistency continuity may violated study move beyond evaluating simple actions argue generated videos incorporate emergence new concepts relation transitions like realworld videos time progresses assess temporal compositionality video generation models propose tcbench benchmark meticulously crafted text prompts corresponding ground truth videos robust evaluation metrics prompts articulate initial final states scenes effectively reducing ambiguities frame development simplifying assessment transition completion addition collecting aligned realworld videos corresponding prompts expand tcbenchs applicability textconditional models imageconditional ones perform generative frame interpolation also develop new metrics measure completeness component transitions generated videos demonstrate significantly higher correlations human judgments existing metrics comprehensive experimental results reveal video generators achieve less compositional changes highlighting enormous space future improvement analysis indicates current video generation models struggle interpret descriptions compositional changes synthesize various components across different time steps,-1,0.0,-1,0.0
fastvideoedit leveraging consistency models efficient texttovideo editing diffusion models demonstrated remarkable capabilities texttoimage texttovideo generation opening possibilities video editing based textual input however computational cost associated sequential sampling diffusion models poses challenges efficient video editing existing approaches relying image generation models video editing suffer timeconsuming oneshot finetuning additional condition extraction ddim inversion making realtime applications impractical work propose fastvideoedit efficient zeroshot video editing approach inspired consistency models cms leveraging selfconsistency property cms eliminate need timeconsuming inversion additional condition extraction reducing editing time method enables direct mapping source video target video strong preservation ability utilizing special variance schedule results improved speed advantages fewer sampling steps used maintaining comparable generation quality experimental results validate stateoftheart performance speed advantages fastvideoedit across evaluation metrics encompassing editing speed temporal consistency textvideo alignment,-1,0.0,-1,0.0
towards retrieval augmented generation large video libraries video content creators need efficient tools repurpose content task often requires complex manual automated searches crafting new video large video libraries remains challenge paper introduce task video library question answering vlqa interoperable architecture applies retrieval augmented generation rag video libraries propose system uses large language models llms generate search queries retrieving relevant video moments indexed speech visual metadata answer generation module integrates user queries metadata produce responses specific video timestamps approach shows promise multimedia content retrieval aiassisted video content creation,-1,0.0,-1,0.0
factorizeddreamer training highquality video generator limited lowquality data texttovideo generation gained significant attention due wide applications video generation editing enhancement translation etc however highquality hq video synthesis extremely challenging diverse complex motions existed real world existing works struggle address problem collecting largescale hq videos inaccessible community work show publicly available limited lowquality lq data sufficient train hq video generator without recaptioning finetuning factorize whole generation process two steps generating image conditioned highly descriptive caption synthesizing video conditioned generated image concise caption motion details specifically present emphfactorizeddreamer factorized spatiotemporal framework several critical designs generation including adapter combine text image embeddings pixelaware cross attention module capture pixellevel image information text encoder better understand motion description predictnet supervise optical flows present noise schedule plays key role ensuring quality stability video generation model lowers requirements detailed captions hq videos directly trained limited lq datasets noisy brief captions largely alleviating cost collect largescale hq videotext pairs extensive experiments variety imagetovideo generation tasks demonstrate effectiveness proposed factorizeddreamer source codes available urlhttpsgithubcomyangxyfactorizeddreamer,-1,0.0,-1,0.0
dreamhead learning spatialtemporal correspondence via hierarchical diffusion audiodriven talking head synthesis audiodriven talking head synthesis strives generate lifelike video portraits provided audio diffusion model recognized superior quality robust generalization explored task however establishing robust correspondence temporal audio cues corresponding spatial facial expressions diffusion models remains significant challenge talking head generation bridge gap present dreamhead hierarchical diffusion framework learns spatialtemporal correspondences talking head synthesis without compromising models intrinsic quality adaptabilitydreamhead learns predict dense facial landmarks audios intermediate signals model spatial temporal correspondencesspecifically first hierarchy audiotolandmark diffusion first designed predict temporally smooth accurate landmark sequences given audio sequence signals second hierarchy landmarktoimage diffusion proposed produce spatially consistent facial portrait videos modeling spatial correspondences dense facial landmark appearance extensive experiments show proposed dreamhead effectively learn spatialtemporal consistency designed hierarchical diffusion produce highfidelity audiodriven talking head videos multiple identities,6,0.8495497276148749,6,0.8495497276148749
lvmark robust watermark latent video diffusion models rapid advancements generative models made possible create hyperrealistic videos applicability increases unauthorized use raised significant concerns leading growing demand techniques protect ownership generative model existing watermarking methods effectively embed watermarks imagegenerative models fail account temporal information resulting poor performance applied videogenerative models address issue introduce novel watermarking method called lvmark embeds watermarks video diffusion models key component lvmark selective weight modulation strategy efficiently embeds watermark messages video diffusion model preserving quality generated videos accurately decode messages presence malicious attacks design watermark decoder leverages spatiotemporal information wavelet domain crossattention module best knowledge approach first highlight potential videogenerative model watermarking valuable tool enhancing effectiveness ownership protection videogenerative models,4,1.0,4,1.0
efficient long video tokenization via coordinatebased patch reconstruction efficient tokenization videos remains challenge training vision models process long videos one promising direction develop tokenizer encode long video clips would enable tokenizer leverage temporal coherence videos better tokenization however training existing tokenizers long videos often incurs huge training cost trained reconstruct frames paper introduce coordtok video tokenizer learns mapping coordinatebased representations corresponding patches input videos inspired recent advances generative models particular coordtok encodes video factorized triplane representations reconstructs patches correspond randomly sampled xyt coordinates allows training large tokenizer models directly long videos without requiring excessive training resources experiments show coordtok drastically reduce number tokens encoding long video clips instance coordtok encode video resolution tokens baselines need tokens achieve similar reconstruction quality show efficient video tokenization enables memoryefficient training diffusion transformer generate frames,-1,0.0,-1,0.0
directorllm humancentric video generation paper introduce directorllm novel video generation model employs large language model llm orchestrate human poses within videos foundational texttovideo models rapidly evolve demand highquality human motion interaction grows address need enhance authenticity human motions extend llm text generator video director human motion simulator utilizing opensource resources llama train directorllm generate detailed instructional signals human poses guide video generation approach offloads simulation human motion video generator llm effectively creating informative outlines humancentric scenes signals used conditions video renderer facilitating realistic promptfollowing video generation independent llm module applied different video renderers including unet dit minimal effort experiments automatic evaluation benchmarks human evaluations show model outperforms existing ones generating videos higher human motion fidelity improved prompt faithfulness enhanced rendered subject naturalness,-1,0.0,-1,0.0
vividzoo multiview video generation diffusion model diffusion models shown impressive performance imagevideo generation diffusionbased texttomultiviewvideo generation remains underexplored new challenges posed generation lie lack massive captioned multiview videos complexity modeling multidimensional distribution end propose novel diffusionbased pipeline generates highquality multiview videos centered around dynamic object text specifically factor problem viewpointspace time components factorization allows us combine reuse layers advanced pretrained multiview image video diffusion models ensure multiview consistency well temporal coherence generated multiview videos largely reducing training cost introduce alignment modules align latent spaces layers pretrained multiview video diffusion models addressing reused layers incompatibility arises domain gap multiview data support future research contribute captioned multiview video dataset experimental results demonstrate method generates highquality multiview videos exhibiting vivid motions temporal coherence multiview consistency given variety text prompts,-1,0.0,-1,0.0
synchronized video storytelling generating video narrations structured storyline video storytelling engaging multimedia content utilizes video accompanying narration attract audience key challenge creating narrations recorded visual scenes previous studies dense video captioning video story generation made progress however practical applications typically require synchronized narrations ongoing visual scenes work introduce new task synchronized video storytelling aims generate synchronous informative narrations videos narrations associated video clip relate visual content integrate relevant knowledge appropriate word count corresponding clips duration specifically structured storyline beneficial guide generation process ensuring coherence integrity support exploration task introduce new benchmark dataset esyncvidstory rich annotations since existing multimodal llms effective addressing task oneshot fewshot settings propose framework named videonarrator generate storyline input videos simultaneously generate narrations guidance generated predefined storyline introduce set evaluation metrics thoroughly assess generation automatic human evaluations validate effectiveness approach dataset codes evaluations released,-1,0.0,-1,0.0
video diffusion alignment via reward gradients made significant progress towards building foundational video diffusion models models trained using largescale unsupervised data become crucial adapt models specific downstream tasks adapting models via supervised finetuning requires collecting target datasets videos challenging tedious work utilize pretrained reward models learned via preferences top powerful vision discriminative models adapt video diffusion models models contain dense gradient information respect generated rgb pixels critical efficient learning complex search spaces videos show backpropagating gradients reward models video diffusion model allow compute sample efficient alignment video diffusion model show results across variety reward models video diffusion models demonstrating approach learn much efficiently terms reward queries computation prior gradientfree approaches code model weightsand visualization available httpsvadervidgithubio,-1,0.0,-1,0.0
genlit reformulating singleimage relighting video generation manipulating illumination within single image represents fundamental challenge computer vision graphics problem traditionally addressed using inverse rendering techniques require explicit asset reconstruction costly ray tracing simulations meanwhile recent advancements visual foundation models suggest new paradigm could soon practical possible one replaces explicit physical models networks trained massive amounts image video data paper explore potential exploiting video diffusion models particular stable video diffusion svd understanding physical world perform relighting tasks given single image specifically introduce genlit framework distills ability graphics engine perform light manipulation video generation model enabling users directly insert manipulate point light world within given image generate results directly video sequence find model finetuned small synthetic dataset objects able generalize real images enabling singleimage relighting realistic ray tracing effects cast shadows results reveal ability video foundation models capture rich information lighting material shape findings suggest models minimal training used physicallybased rendering without explicit physically asset reconstruction complex ray tracing suggests potential models controllable physically accurate image synthesis tasks,1,0.9398402126869332,1,0.9398402126869332
highly dynamic realistic portrait image animation video diffusion transformer existing methodologies animating portrait images face significant challenges particularly handling nonfrontal perspectives rendering dynamic objects around portrait generating immersive realistic backgrounds paper introduce first application pretrained transformerbased video generative model demonstrates strong generalization capabilities generates highly dynamic realistic videos portrait animation effectively addressing challenges adoption new video backbone model makes previous unetbased methods identity maintenance audio conditioning video extrapolation inapplicable address limitation design identity reference network consisting causal vae combined stacked series transformer layers ensuring consistent facial identity across video sequences additionally investigate various speech audio conditioning motion frame mechanisms enable generation continuous video driven speech audio method validated experiments benchmark newly proposed wild datasets demonstrating substantial improvements prior methods generating realistic portraits characterized diverse orientations within dynamic immersive scenes visualizations source code available,1,1.0,1,1.0
anchored diffusion video face reenactment video generation drawn significant interest recently pushing development largescale models capable producing realistic videos coherent motion due memory constraints models typically generate short video segments combined long videos merging process poses significant challenge requires ensuring smooth transitions overall consistency paper introduce anchored diffusion novel method synthesizing relatively long seamless videos extend diffusion transformers dits incorporate temporal information creating sequencedit sdit model generating short video segments unlike previous works train model video sequences random nonuniform temporal spacing incorporate temporal information via external guidance increasing flexibility allowing capture short longterm relationships furthermore inference leverage transformer architecture modify diffusion process generating batch nonuniform sequences anchored common frame ensuring consistency regardless temporal distance demonstrate method focus face reenactment task creating video source image replicates facial expressions movements driving video comprehensive experiments show approach outperforms current techniques producing longer consistent highquality videos offering editing capabilities,9,1.0,9,1.0
kubrick multimodal agent collaborations synthetic video generation texttovideo generation dominated endtoend diffusionbased autoregressive models one hand novel models provide plausible versatility criticized physical correctness shading illumination camera motion temporal consistency hand film industry relies manuallyedited computergenerated imagery cgi using modeling software humandirected synthetic videos animations address aforementioned shortcomings extremely tedious requires tight collaboration movie makers rendering experts paper introduce automatic synthetic video generation pipeline based vision large language model vlm agent collaborations given natural language description video multiple vlm agents autodirect various processes generation pipeline cooperate create blender scripts render video best aligns given description based film making inspiration augmented blenderbased movie making knowledge director agent decomposes input textbased video description subprocesses subprocess programmer agent produces pythonbased blender scripts based customized function composing api calling reviewer agent augmented knowledge video reviewing character motion coordinates intermediate screenshots uses compositional reasoning ability provide feedback programmer agent programmer agent iteratively improves scripts yield best overall video outcome generated videos show better quality commercial video generation models metrics video quality instructionfollowing performance moreover framework outperforms approaches comprehensive user study quality consistency rationality,15,0.9935341153607914,15,0.9935341153607914
lova longform videotoaudio generation videotoaudio generation important video editing postprocessing enabling creation semanticsaligned audio silent video however existing methods focus generating shortform audio short video segment less seconds giving little attention scenario longform video inputs current unetbased diffusion models inevitable problem handling longform audio generation inconsistencies within final concatenated audio paper first highlight importance longform problem besides propose lova novel model longform videotoaudio generation based diffusion transformer dit architecture lova proves effective generating longform audio compared existing autoregressive models unetbased diffusion models extensive objective subjective experiments demonstrate lova achieves comparable performance benchmark outperforms baselines benchmark longform video input,8,1.0,8,1.0
individual content motion dynamics preserved pruning video diffusion models high computational cost slow inference time major obstacles deploying video diffusion model vdm practical applications overcome introduce new video diffusion model compression approach using individual content motion dynamics preserved pruning consistency loss first empirically observe deeper vdm layers crucial maintaining quality textbfmotion dynamics eg coherence entire video shallower layers focused textbfindividual content eg individual frames therefore prune redundant blocks shallower layers preserving deeper layers resulting lightweight vdm variant called vdmini additionally propose textbfindividual content motion dynamics icmd consistency loss gain comparable generation performance larger vdm ie teacher vdmini ie student particularly first use individual content distillation icd loss ensure consistency features generated frame teacher student models next introduce multiframe content adversarial mca loss enhance motion dynamics across generated video whole method significantly accelerates inference time maintaining highquality video generation extensive experiments demonstrate effectiveness vdmini two important video generation tasks texttovideo imagetovideo respectively achieve average times times speed method sfv method maintaining quality generated videos two benchmarks ie vbench,-1,0.0,-1,0.0
tavgbench benchmarking text audiblevideo generation text audiblevideo generation tavg task involves generating videos accompanying audio based text descriptions achieving requires skillful alignment audio video elements support research field developed comprehensive text audiblevideo generation benchmark tavgbench contains million clips total duration thousand hours propose automatic annotation pipeline ensure audible video detailed descriptions audio video contents also introduce audiovisual harmoni score avhscore provide quantitative measure alignment generated audio video modalities additionally present baseline model tavg called tavdiffusion uses twostream latent diffusion model provide fundamental starting point research area achieve alignment audio video employing crossattention contrastive learning extensive experiments evaluations tavgbench demonstrate effectiveness proposed model conventional metrics proposed metrics,8,0.47232472921176605,8,0.47232472921176605
latent trees scene diffusion present novel latent diffusion model largescale scene generation recent advances diffusion models shown impressive results object generation limited spatial extent quality extended scenes generate complex diverse scene structures introduce latent tree representation effectively encode lowerfrequency geometry higherfrequency detail coarsetofine hierarchy learn generative diffusion process latent scene space modeling latent components scene resolution level synthesize largescale scenes varying sizes train diffusion model scene patches synthesize arbitrarysized output scenes shared diffusion generation across multiple scene patches extensive experiments demonstrate efficacy benefits largescale highquality unconditional scene generation probabilistic completion partial scene observations,1,1.0,1,1.0
exploring interplay video generation world models autonomous driving survey world models video generation pivotal technologies domain autonomous driving playing critical role enhancing robustness reliability autonomous systems world models simulate dynamics realworld environments video generation models produce realistic video sequences increasingly integrated improve situational awareness decisionmaking capabilities autonomous vehicles paper investigates relationship two technologies focusing structural parallels particularly diffusionbased models contribute accurate coherent simulations driving scenarios examine leading works jepa genie sora exemplify different approaches world model design thereby highlighting lack universally accepted definition world models diverse interpretations underscore fields evolving understanding world models optimized various autonomous driving tasks furthermore paper discusses key evaluation metrics employed domain chamfer distance scene reconstruction frechet inception distance fid assessing quality generated video content analyzing interplay video generation world models survey identifies critical challenges future research directions emphasizing potential technologies jointly advance performance autonomous driving systems findings presented paper aim provide comprehensive understanding integration video generation world models drive innovation development safer reliable autonomous vehicles,16,0.7735534730397697,16,0.7735534730397697
labelefficient data augmentation video diffusion models guidewire segmentation cardiac fluoroscopy accurate segmentation guidewires interventional cardiac fluoroscopy videos crucial computeraided navigation tasks although deep learning methods demonstrated high accuracy robustness wire segmentation require substantial annotated datasets generalizability underscoring need extensive labeled data enhance model performance address challenge propose segmentationguided frameconsistency video diffusion model sfvd generate large collections labeled fluoroscopy videos augmenting training data wire segmentation networks sfvd leverages videos limited annotations independently modeling scene distribution motion distribution first samples scene distribution generating fluoroscopy images wires positioned according specified input mask samples motion distribution progressively generating subsequent frames ensuring frametoframe coherence frameconsistency strategy segmentationguided mechanism refines process adjusting wire contrast ensuring diverse range visibility synthesized image evaluation fluoroscopy dataset confirms superior quality generated videos shows significant improvements guidewire segmentation,3,0.7129598493407869,3,0.7129598493407869
vjt video transformer joint tasks deblurring lowlight enhancement denoising video restoration task aims recover highquality videos lowquality observations contains various important subtasks video denoising deblurring lowlight enhancement since video often faces different types degradation blur low light noise even worse kinds degradation could happen simultaneously taking videos extreme environments poses significant challenges one wants remove artifacts time paper best knowledge first propose efficient endtoend video transformer approach joint task video deblurring lowlight enhancement denoising work builds novel multitier transformer tier uses different level degraded video target learn features video effectively moreover carefully design new tiertotier feature fusion scheme learn video features incrementally accelerate training process suitable adaptive weighting scheme also provide new multiscenelowlightblurnoise mlbn dataset generated according characteristics joint task based realblur dataset youtube videos simulate realistic scenes far possible conducted extensive experiments compared many previous stateoftheart methods show effectiveness approach clearly,-1,0.0,-1,0.0
snap video scaled spatiotemporal transformers texttovideo synthesis contemporary models generating images show remarkable quality versatility swayed advantages research community repurposes generate videos since video content highly redundant argue naively bringing advances image models video generation domain reduces motion fidelity visual quality impairs scalability work build snap video videofirst model systematically addresses challenges first extend edm framework take account spatially temporally redundant pixels naturally support video generation second show unet workhorse behind image generation scales poorly generating videos requiring significant computational overhead hence propose new transformerbased architecture trains times faster unets faster inference allows us efficiently train texttovideo model billions parameters first time reach stateoftheart results number benchmarks generate videos substantially higher quality temporal consistency motion complexity user studies showed model favored large margin recent methods see website httpssnapresearchgithubiosnapvideo,-1,0.0,-1,0.0
compositional video generation flow equalization largescale texttovideo diffusion models recently demonstrated unprecedented capability transform natural language descriptions stunning photorealistic videos despite promising results significant challenge remains models struggle fully grasp complex compositional interactions multiple concepts actions issue arises words dominantly influence final video overshadowing conceptsto tackle problem introduce textbfvico generic framework compositional video generation explicitly ensures concepts represented properly core vico analyzes input tokens influence generated video adjusts model prevent single concept dominating specifically vico extracts attention weights layers build spatialtemporal attention graph estimates influence emphmaxflow source text token video target token although direct computation attention flow diffusion models typically infeasible devise efficient approximation based subgraph flows employ fast vectorized implementation turn makes flow computation manageable differentiable updating noisy latent balance flows vico captures complex interactions consequently produces videos closely adhere textual descriptions apply method multiple diffusionbased video models compositional video editing empirical results demonstrate framework significantly enhances compositional richness accuracy generated videos visit website athrefhttpsadamdadgithubiovicourlhttpsadamdadgithubiovico,-1,0.0,-1,0.0
videoagent selfimproving video generation video generation used generate visual plans controlling robotic systems given image observation language instruction previous work generated video plans converted robot controls executed however major bottleneck leveraging video generation control lies quality generated videos often suffer hallucinatory content unrealistic physics resulting low task success control actions extracted generated videos scaling dataset model size provides partial solution integrating external feedback natural essential grounding video generation real world observation propose videoagent selfimproving generated video plans based external feedback instead directly executing generated video plan videoagent first refines generated video plans using novel procedure call selfconditioning consistency allowing inferencetime compute turned better generated video plans refined video plan executed videoagent collect additional data environment improve video plan generation experiments simulated robotic manipulation metaworld ithor show videoagent drastically reduces hallucination thereby boosting success rate downstream manipulation tasks illustrate videoagent effectively refine realrobot videos providing early indicator robots effective tool grounding video generation physical world video demos code found httpsvideoasagentgithubio,5,0.26764501740164565,5,0.26764501740164565
dcgaussian improving gaussian splatting reflective dash cam videos present dcgaussian new method generating novel views invehicle dash cam videos neural rendering techniques made significant strides driving scenarios existing methods primarily designed videos collected autonomous vehicles however videos limited quantity diversity compared dash cam videos widely used across various types vehicles capture broader range scenarios dash cam videos often suffer severe obstructions reflections occlusions windshields significantly impede application neural rendering techniques address challenge develop dcgaussian based recent realtime neural rendering technique gaussian splatting approach includes adaptive image decomposition module model reflections occlusions unified manner additionally introduce illuminationaware obstruction modeling manage reflections occlusions varying lighting conditions lastly employ geometryguided gaussian enhancement strategy improve rendering details incorporating additional geometry priors experiments selfcaptured public dash cam videos show method achieves stateoftheart performance novel view synthesis also accurately reconstructing captured scenes getting rid obstructions see project page code data httpslinhanwanggithubiodcgaussian,1,1.0,1,1.0
enhancing multitext long video generation consistency without tuning timefrequency analysis prompt alignment theory despite considerable progress achieved long video generation problem still significant room improve consistency videos particularly terms smoothness transitions scenes address issues enhance consistency coherence videos generated either single multiple prompts propose timefrequency based temporal attention reweighting algorithm tiara meticulously edits attention score matrix based discrete shorttime fourier transform method supported theoretical guarantee firstofitskind frequencybased methods diffusion models videos generated multiple prompts investigate key factors affecting prompt interpolation quality propose promptblend advanced prompt interpolation pipeline efficacy proposed method validated via extensive experimental results exhibiting consistent impressive improvements baseline methods code released upon acceptance,-1,0.0,-1,0.0
fourplane factorized video autoencoders latent variable generative models emerged powerful tools generative tasks including image video synthesis models enabled pretrained autoencoders map high resolution data compressed lower dimensional latent space generative models subsequently developed requiring fewer computational resources despite effectiveness direct application latent variable models higher dimensional domains videos continues pose challenges efficient training inference paper propose autoencoder projects volumetric data onto fourplane factorized latent space grows sublinearly input size making ideal higher dimensional data like videos design factorized model supports straightforward adoption number conditional generation tasks latent diffusion models ldms classconditional generation frame prediction video interpolation results show proposed fourplane latent space retains rich representation needed highfidelity reconstructions despite heavy compression simultaneously enabling ldms operate significant improvements speed memory,-1,0.0,-1,0.0
reattentional controllable video diffusion editing editing videos textual guidance garnered popularity due streamlined process mandates users solely edit text prompt corresponding source video recent studies explored exploited largescale texttoimage diffusion models textguided video editing resulting remarkable video editing capabilities however may still suffer limitations mislocated objects incorrect number objects therefore controllability video editing remains formidable challenge paper aim challenge limitations proposing reattentional controllable video diffusion editing reatco method specially align spatial placement target objects edited text prompt trainingfree manner propose reattentional diffusion rad refocus crossattention activation responses edited text prompt target video denoising stage resulting spatially locationaligned semantically highfidelity manipulated video particular faithfully preserve invariant region content less border artifacts propose invariant regionguided joint sampling irjs strategy mitigate intrinsic sampling errors wrt invariant regions denoising timestep constrain generated content harmonized invariant region content experimental results verify reatco consistently improves controllability video diffusion editing achieves superior video editing performance,9,0.9819974418647023,9,0.9819974418647023
video diffusion models survey diffusion generative models recently become powerful technique creating modifying highquality coherent video content survey provides comprehensive overview critical components diffusion models video generation including applications architectural design temporal dynamics modeling paper begins discussing core principles mathematical formulations explores various architectural choices methods maintaining temporal consistency taxonomy applications presented categorizing models based input modalities text prompts images videos audio signals advancements texttovideo generation discussed illustrate stateoftheart capabilities limitations current approaches additionally survey summarizes recent developments training evaluation practices including use diverse video image datasets adoption various evaluation metrics assess model performance survey concludes examination ongoing challenges generating longer videos managing computational costs offers insights potential future directions field consolidating latest research developments survey aims serve valuable resource researchers practitioners working video diffusion models website httpsgithubcomndrwmlnkawesomevideodiffusionmodels,-1,0.0,-1,0.0
cage unsupervised visual composition animation controllable video generation field video generation expanded significantly recent years controllable compositional video generation garnering considerable interest methods rely leveraging annotations text objects bounding boxes motion cues require substantial human effort thus limit scalability contrast address challenge controllable compositional video generation without annotations introducing novel unsupervised approach model trained scratch dataset unannotated videos inference time compose plausible novel scenes animate objects placing object parts desired locations space time core innovation method lies unified control format training process video generation conditioned randomly selected subset pretrained selfsupervised local features conditioning compels model learn inpaint missing information video spatially temporally thereby learning inherent compositionality scene dynamics moving objects abstraction level imposed invariance conditioning input minor visual perturbations enable control object motion simply using features desired future locations call model cage stands visual composition animation video generation conduct extensive experiments validate effectiveness cage across various scenarios demonstrating capability accurately follow control generate highquality videos exhibit coherent scene composition realistic animation,-1,0.0,-1,0.0
ardup active region video diffusion universal policies sequential decisionmaking formulated textconditioned video generation problem video planner guided textdefined goal generates future frames visualizing planned actions control actions subsequently derived work introduce active region video diffusion universal policies ardup novel framework videobased policy learning emphasizes generation active regions ie potential interaction areas enhancing conditional policys focus interactive areas critical task execution innovative framework integrates active region conditioning latent diffusion models video planning employs latent representations direct action decoding inverse dynamic modeling utilizing motion cues videos automatic active region discovery method eliminates need manual annotations active regions validate ardups efficacy via extensive experiments simulator cliport realworld dataset bridgedata achieving notable improvements success rates generating convincingly realistic video plans,-1,0.0,-1,0.0
beyouroutpainter mastering video outpainting inputspecific adaptation video outpainting challenging task aiming generating video content outside viewport input video maintaining interframe intraframe consistency existing methods fall short either generation quality flexibility introduce motia mastering video outpainting inputspecific adaptation diffusionbased pipeline leverages intrinsic dataspecific patterns source video imagevideo generative prior effective outpainting motia comprises two main phases inputspecific adaptation patternaware outpainting inputspecific adaptation phase involves conducting efficient effective pseudo outpainting learning singleshot source video process encourages model identify learn patterns within source video well bridging gap standard generative processes outpainting subsequent phase patternaware outpainting dedicated generalization learned patterns generate outpainting outcomes additional strategies including spatialaware insertion noise travel proposed better leverage diffusion models generative prior acquired video patterns source videos extensive evaluations underscore motias superiority outperforming existing stateoftheart methods widely recognized benchmarks notably advancements achieved without necessitating extensive taskspecific tuning,-1,0.0,-1,0.0
videoinfinity distributed long video generation diffusion models recently achieved remarkable results video generation despite encouraging performances generated videos typically constrained small number frames resulting clips lasting merely seconds primary challenges producing longer videos include substantial memory requirements extended processing time required single gpu straightforward solution would split workload across multiple gpus however leads two issues ensuring gpus communicate effectively share timing context information modifying existing video diffusion models usually trained short sequences create longer videos without additional training tackle paper introduce videoinfinity distributed inference pipeline enables parallel processing across multiple gpus longform video generation specifically propose two coherent mechanisms clip parallelism dualscope attention clip parallelism optimizes gathering sharing context information across gpus minimizes communication overhead dualscope attention modulates temporal selfattention balance local global contexts efficiently across devices together two mechanisms join forces distribute workload enable fast generation long videos x nvidia ada gpu setup method generates videos frames approximately minutes enabling long video generation speed times faster prior methods,-1,0.0,-1,0.0
idol unified dualmodal latent diffusion humancentric joint videodepth generation significant advances made humancentric video generation yet joint videodepth generation problem remains underexplored existing monocular depth estimation methods may generalize well synthesized images videos multiviewbased methods difficulty controlling human appearance motion work present idol unified dualmodal latent diffusion highquality humancentric joint videodepth generation idol consists two novel designs first enable dualmodal generation maximize information exchange video depth generation propose unified dualmodal unet parametersharing framework joint video depth denoising wherein modality label guides denoising target crossmodal attention enables mutual information flow second ensure precise videodepth spatial alignment propose motion consistency loss enforces consistency video depth feature motion fields leading harmonized outputs additionally crossattention map consistency loss applied align crossattention map video denoising depth denoising facilitating spatial alignment extensive experiments tiktok datasets show superior performance significantly surpassing existing methods terms video fvd depth accuracy,-1,0.0,-1,0.0
onlyflow optical flow based motion conditioning video diffusion models consider problem texttovideo generation tasks precise control various applications camera movement control videotovideo editing methods tacking problem rely providing userdefined controls binary masks camera movement embeddings approach propose onlyflow approach leveraging optical flow firstly extracted input video condition motion generated videos using text prompt input video onlyflow allows user generate videos respect motion input video well text prompt implemented optical flow estimation model applied input video fed trainable optical flow encoder output feature maps injected texttovideo backbone model perform quantitative qualitative user preference studies show onlyflow positively compares stateoftheart methods wide range tasks even though onlyflow specifically trained tasks onlyflow thus constitutes versatile lightweight yet efficient method controlling motion texttovideo generation models code made available github huggingface,-1,0.0,-1,0.0
consistent scene generation text prompts recent advances diffusion models revolutionized content creation yet generating photorealistic dynamic scenes remains significant challenge existing dynamic generation methods typically rely distilling knowledge pretrained generative models often finetuned synthetic object datasets consequently resulting scenes tend objectcentric lack photorealism texttovideo models generate realistic scenes motion often struggle spatial understanding provide limited control camera viewpoints rendering address limitations present novel scene generation framework departs conventional multiview generative models favor streamlined architecture harnesses video generative models trained diverse realworld datasets method first generates reference video using video generation model employs strategic camera array selection rendering apply progressive warping inpainting technique ensure spatial temporal consistency across multiple viewpoints finally optimize multiview images using dynamic renderer enabling flexible camera control based user preferences adopting trainingfree architecture efficiently produces realistic scenes viewed arbitrary trajectories code made publicly available project page,1,1.0,1,1.0
topa extending large language models video understanding via textonly prealignment recent advancements image understanding benefited extensive use web imagetext pairs however video understanding remains challenge despite availability substantial web videotext data difficulty primarily arises inherent complexity videos inefficient language supervision recent webcollected videotext datasets paper introduce textonly prealignment topa novel approach extend large language models llms video understanding without need pretraining real video data specifically first employ advanced llm automatically generate textual videos comprising continuous textual frames along corresponding annotations simulate real videotext data annotated textual videos used prealign languageonly llm video modality bridge gap textual real videos employ clip model feature extractor align image text modalities textonly prealignment continuous textual frames encoded sequence clip text features analogous continuous clip image features thus aligning llm real video representation extensive experiments including zeroshot evaluation finetuning various video understanding tasks demonstrate topa effective efficient framework aligning video content llms particular without training video data model achieves accuracy challenging longform video understanding benchmark egoschema performance surpasses previous videotext pretraining approaches proves competitive recent video agents,0,0.9987645937649585,0,0.9987645937649585
realmdreamer textdriven scene generation inpainting depth diffusion introduce realmdreamer technique generating forwardfacing scenes text descriptions method optimizes gaussian splatting representation match complex text prompts using pretrained diffusion models key insight leverage inpainting diffusion models conditioned initial scene estimate provide low variance supervision unknown regions distillation conjunction imbue highfidelity geometry geometric distillation depth diffusion model conditioned samples inpainting model find initialization optimization crucial provide principled methodology notably technique doesnt require video multiview data synthesize various highquality scenes different styles complex layouts generality method allows synthesis single image measured comprehensive user study method outperforms existing approaches preferred project page httpsrealmdreamergithubio,-1,0.0,-1,0.0
wildvidfit video virtual tryon wild via imagebased controlled diffusion models video virtual tryon aims generate realistic sequences maintain garment identity adapt persons pose body shape source videos traditional imagebased methods relying warping blending struggle complex human movements occlusions limiting effectiveness video tryon applications moreover videobased models require extensive highquality data substantial computational resources tackle issues reconceptualize video tryon process generating videos conditioned garment descriptions human motion solution wildvidfit employs imagebased controlled diffusion models streamlined onestage approach model conditioned specific garments individuals trained still images rather videos leverages diffusion guidance pretrained models including video masked autoencoder segment smoothness improvement selfsupervised model feature alignment adjacent frame latent space integration markedly boosts models ability maintain temporal coherence enabling effective video tryon within imagebased framework experiments vitonhd dresscode datasets along tests vvt tiktok datasets demonstrate wildvidfits capability generate fluid coherent videos project page website wildvidfitprojectgithubio,-1,0.0,-1,0.0
univg towards unifiedmodal video generation diffusion based video generation received extensive attention achieved considerable success within academic industrial communities however current efforts mainly concentrated singleobjective singletask video generation generation driven text image combination text image fully meet needs realworld application scenarios users likely input images text conditions flexible manner either individually combination address propose unifiedmodal video genearation system capable handling multiple video generation tasks across text image modalities end revisit various video generation tasks within system perspective generative freedom classify highfreedom lowfreedom video generation categories highfreedom video generation employ multicondition cross attention generate videos align semantics input images text lowfreedom video generation introduce biased gaussian noise replace pure random gaussian noise helps better preserve content input conditions method achieves lowest frechet video distance fvd public academic benchmark msrvtt surpasses current opensource methods human evaluations par current closesource method samples visit httpsunivgbaidugithubio,-1,0.0,-1,0.0
trainingfree condition video diffusion models single frame spatialsemantic echocardiogram synthesis conditional video diffusion models cdm shown promising results video synthesis potentially enabling generation realistic echocardiograms address problem data scarcity however current cdms require paired segmentation map echocardiogram dataset present new method called freeecho generating realistic echocardiograms single enddiastolic segmentation map without additional training data method based temporal attention layers model conditioned segmentation map using trainingfree conditioning method based sdedit evaluate model two public echocardiogram datasets camus echonetdynamic show model generate plausible echocardiograms spatially aligned input segmentation map achieving performance comparable trainingbased cdms work opens new possibilities generating echocardiograms single segmentation map used data augmentation domain adaptation applications medical imaging code available,-1,0.0,-1,0.0
inflation diffusion efficient temporal adaptation texttovideo superresolution propose efficient diffusionbased texttovideo superresolution sr tuning approach leverages readily learned capacity pixel level image diffusion model capture spatial information video generation accomplish goal design efficient architecture inflating weightings texttoimage sr model video generation framework additionally incorporate temporal adapter ensure temporal coherence across video frames investigate different tuning approaches based inflated architecture report tradeoffs computational costs superresolution quality empirical evaluation quantitative qualitative shutterstock video dataset demonstrates approach able perform texttovideo sr generation good visual quality temporal consistency evaluate temporal coherence also present visualizations video format,11,0.9235240508025361,11,0.9235240508025361
feedforward scene synthesis sparse views introduce feedforward approach novel view synthesis nvs diverse realworld scenes using sparse observations setting inherently illposed due minimal overlap among input views insufficient visual information provided making challenging conventional methods achieve highquality results addresses effectively combining geometryaware reconstruction temporally consistent video generation specifically refactors feedforward gaussian splatting model render features directly latent space pretrained stable video diffusion svd model features act pose visual cues guide denoising process produce photorealistic views model endtoend trainable supports rendering arbitrary views sparse input views evaluate performance introduce new benchmark using challenging dataset achieves superior visual quality compared stateoftheart methods widesweeping even nvs tasks experiments existing benchmark also confirm effectiveness model video results available project page,1,1.0,1,1.0
vca video curious agent long video understanding long video understanding poses unique challenges due temporal complexity low information density recent works address task sampling numerous frames incorporating auxiliary tools using llms result high computational costs work introduce curiositydriven video agent selfexploration capability dubbed vca built upon vlms vca autonomously navigates video segments efficiently builds comprehensive understanding complex video sequences instead directly sampling frames vca employs treesearch structure explore video segments collect frames rather relying external feedback reward vca leverages vlms selfgenerated intrinsic reward guide exploration enabling capture crucial information reasoning experimental results multiple long video benchmarks demonstrate approachs superior effectiveness efficiency,-1,0.0,-1,0.0
harnessing metalearning improving fullframe video stabilization video stabilization longstanding computer vision problem particularly pixellevel synthesis solutions video stabilization synthesize full frames add complexity task techniques aim stabilize videos synthesizing full frames enhancing stability considered video intensifies complexity task due distinct mix unique motion profiles visual content present video sequence making robust generalization fixed parameters difficult study introduce novel approach enhance performance pixellevel synthesis solutions video stabilization adapting models individual input video sequences proposed adaptation exploits lowlevel visual cues accessible testtime improve stability quality resulting videos highlight efficacy methodology testtime adaptation simple finetuning one models followed significant stability gain via integration metalearning techniques notably significant improvement achieved single adaptation step versatility proposed algorithm demonstrated consistently improving performance various pixellevel synthesis models video stabilization realworld scenarios,9,0.811926514313562,9,0.811926514313562
rethinking video deblurring waveletaware dynamic transformer diffusion model current video deblurring methods limitations recovering highfrequency information since regression losses conservative highfrequency details since diffusion models dms strong capabilities generating highfrequency details consider introducing dms video deblurring task however found directly applying dms video deblurring task following problems dms require many iteration steps generate videos gaussian noise consumes many computational resources dms easily misled blurry artifacts video resulting irrational content distortion deblurred video address issues propose novel video deblurring framework vddiff integrates diffusion model waveletaware dynamic transformer wadt specifically perform diffusion model highly compact latent space generate prior features containing highfrequency information conforms ground truth distribution design wadt preserve recover lowfrequency information video utilizing highfrequency information generated diffusion model extensive experiments show proposed vddiff outperforms sota methods gopro dvd bsd realworld video datasets,-1,0.0,-1,0.0
llmenhanced graph prior meets indoor scene explicit regularization compositional scene synthesis diverse applications across spectrum industries robotics films video games closely mirrors complexity realworld multiobject environments conventional works typically employ shape retrieval based frameworks naturally suffer limited shape diversity recent progresses made object shape generation generative models diffusion models increases shape fidelity however approaches separately treat shape generation layout generation synthesized scenes usually hampered layout collision suggests scenelevel fidelity still underexplored paper aim generating realistic reasonable indoor scenes scene graph enrich priors given scene graph inputs large language model utilized aggregate globalwise features local nodewise edgewise features unified graph encoder graph features extracted guide joint layoutshape generation additional regularization introduced explicitly constrain produced layouts benchmarked sgfront dataset method achieves better scene synthesis especially terms scenelevel fidelity source code released publication,-1,0.0,-1,0.0
sar image synthesis diffusion models recent years diffusion models dms become popular method generating synthetic data achieving samples higher quality quickly became superior generative adversarial networks gans current stateoftheart method generative modeling however potential yet exploited radar lack available training data longstanding problem work specific type dms namely denoising diffusion probabilistic model ddpm adapted sar domain investigate network choice specific diffusion parameters conditional unconditional sar image generation experiments show ddpm qualitatively quantitatively outperforms stateoftheart ganbased methods sar image generation finally show ddpm profits pretraining largescale clutter data generating sar images even higher quality,17,0.9766772052777516,17,0.9766772052777516
feedforward bullettime reconstruction dynamic scenes monocular videos recent advancements static feedforward scene reconstruction demonstrated significant progress highquality novel view synthesis however models often struggle generalizability across diverse environments fail effectively handle dynamic content present btimer short bullettimer first motionaware feedforward model realtime reconstruction novel view synthesis dynamic scenes approach reconstructs full scene gaussian splatting representation given target bullet timestamp aggregating information context frames formulation allows btimer gain scalability generalization leveraging static dynamic scene datasets given casual monocular dynamic video btimer reconstructs bullettime scene within reaching stateoftheart performance static dynamic scene datasets even compared optimizationbased approaches,1,1.0,1,1.0
exploring aigc video quality focus visual harmony videotext consistency domain distribution gap recent advancements texttovideo artificial intelligence generated content aigc remarkable compared traditional videos assessment aigc videos encounters various challenges visual inconsistency defy common sense discrepancies content textual prompt distribution gap various generative models etc target challenges work categorize assessment aigc video quality three dimensions visual harmony videotext consistency domain distribution gap dimension design specific modules provide comprehensive quality assessment aigc videos furthermore research identifies significant variations visual quality fluidity style among videos generated different texttovideo models predicting source generative model make aigc video features discriminative enhances quality assessment performance proposed method used thirdplace winner ntire quality assessment aigenerated content track video demonstrating effectiveness code available httpsgithubcomcoobiwtrivqa,12,0.38016268723809327,12,0.38016268723809327
vcbench controllable benchmark symbolic abstract challenges video cognition recent advancements large videolanguage models lvlms driven development benchmarks designed assess cognitive abilities videobased tasks however existing benchmarks heavily rely webcollected videos paired human annotations modelgenerated questions limit control video content fall short evaluating advanced cognitive abilities involving symbolic elements abstract concepts address limitations introduce vcbench controllable benchmark assess lvlms cognitive abilities involving symbolic abstract concepts varying difficulty levels generating video data pythonbased engine vcbench allows precise control video content creating dynamic taskoriented videos feature complex scenes abstract concepts task pairs tailored question templates target specific cognitive challenges providing rigorous evaluation test evaluation reveals even stateoftheart sota models struggle simple video cognition tasks involving abstract concepts performance sharply dropping video complexity rises findings reveal current limitations lvlms advanced cognitive tasks highlight critical role vcbench driving research toward robust lvlms complex video cognition challenges,0,1.0,0,1.0
conclvd controllable chinese landscape video generation via diffusion model chinese landscape painting gem chinese cultural artistic heritage showcases splendor nature deep observations imaginations painters limited traditional techniques artworks confined static imagery ancient times leaving dynamism landscapes subtleties artistic sentiment viewers imagination recently emerging texttovideo diffusion methods shown significant promise video generation providing hope creation dynamic chinese landscape paintings however challenges lack specific datasets intricacy artistic styles creation extensive highquality videos pose difficulties models generating chinese landscape painting videos paper propose clvhd chinese landscape videohigh definition novel dataset chinese landscape painting videos conclvd controllable chinese landscape video diffusion model utilizes stable diffusion specifically present motion module featuring dual attention mechanism capture dynamic transformations landscape imageries alongside noise adapter leverage unsupervised contrastive learning latent space following generation keyframes employ optical flow frame interpolation enhance video smoothness method retains essence landscape painting imageries also achieves dynamic transitions significantly advancing field artistic video generation source code dataset available,1,1.0,1,1.0
videofoley twostage videotosound generation via temporal event condition foley sound foley sound synthesis crucial multimedia production enhancing user experience synchronizing audio video temporally semantically recent studies automating laborintensive process videotosound generation face significant challenges systems lacking explicit temporal features suffer poor alignment controllability timestampbased models require costly subjective human annotation propose videofoley videotosound system using root mean square rms intuitive condition semantic timbre prompts audio text rms framelevel intensity envelope closely related audio semantics acts temporal event feature guide audio generation video annotationfree selfsupervised learning framework consists two stages incorporating novel ideas including rms discretization rmscontrolnet pretrained texttoaudio model extensive evaluation shows videofoley achieves stateoftheart performance audiovisual alignment controllability sound timing intensity timbre nuance source code model weights demos available companion website httpsjnwnleegithubiovideofoleydemo,8,1.0,8,1.0
ppvf efficient privacypreserving online video fetching framework correlated differential privacy online video streaming evolved integral component contemporary internet landscape yet disclosure user requests presents formidable privacy challenges users stream preferred online videos requests automatically seized video content providers potentially leaking users privacy unfortunately current protection methods wellsuited preserving user request privacy content providers maintaining highquality online video services tackle challenge introduce novel privacypreserving video fetching ppvf framework utilizes trusted edge devices prefetch cache videos ensuring privacy users requests optimizing efficiency edge caching specifically design ppvf three core components textitonline privacy budget scheduler employs theoretically guaranteed online algorithm select nonrequested videos candidates assigned privacy budgets alternative videos chosen online algorithm theoretically guaranteed consider video utilities available privacy budgets textitnoisy video request generator generates redundant video requests addition original ones utilizing correlated differential privacy obfuscate request privacy textitonline video utility predictor leverages federated learning collaboratively evaluate video utility online fashion aiding video selection noise generation finally conduct extensive experiments using realworld video request traces tencent video results demonstrate ppvf effectively safeguards user request privacy upholding high video caching performance,2,1.0,2,1.0
omni world model consistent long video generation video generation models vgms received extensive attention recently serve promising candidates generalpurpose large vision models generate short videos time existing methods achieve long video generation iteratively calling vgms using lastframe output condition nextround generation however last frame contains shortterm finegrained information scene resulting inconsistency long horizon address propose omni world model produce longterm coherent comprehensive conditions consistent long video generation videos observations underlying evolving world propose model longterm developments latent space use vgms film videos specifically represent world latent state variable decoded explicit video observations observations serve basis anticipating temporal dynamics turn update state variable interaction evolving dynamics persistent state enhances diversity consistency long videos extensive experiments show achieves comparable performance sota methods vbenchlong validating ability generate highquality video observations code httpsgithubcomhuangyhowl,-1,0.0,-1,0.0
heartbeat towards controllable echocardiography video synthesis multimodal conditionsguided diffusion models echocardiography echo video widely used cardiac examination clinical procedure heavily relies operator experience needs years training maybe assistance deep learningbased systems enhanced accuracy efficiency however challenging since acquiring sufficient customized data eg abnormal cases novice training deep model development clinically unrealistic hence controllable echo video synthesis highly desirable paper propose novel diffusionbased framework named heartbeat towards controllable highfidelity echo video synthesis highlight threefold first heartbeat serves unified framework enables perceiving multimodal conditions simultaneously guide controllable generation second factorize multimodal conditions local global ones two insertion strategies separately provided fine coarsegrained controls composable flexible manner way users synthesize echo videos conform mental imagery combining multimodal control signals third propose decouple visual concepts temporal dynamics learning using twostage training scheme simplifying model training one interesting thing heartbeat easily generalize maskguided cardiac mri synthesis shots showcasing scalability broader applications extensive experiments two public datasets show efficacy proposed heartbeat,-1,0.0,-1,0.0
trainingfree camera control video generation propose trainingfree robust solution offer camera movement control offtheshelf video diffusion models unlike previous work method require supervised finetuning cameraannotated datasets selfsupervised training via data augmentation instead plugandplay pretrained video diffusion models generate cameracontrollable videos single image text prompt input inspiration work comes layout prior intermediate latents encode generated results thus rearranging noisy pixels cause output content relocate well camera moving could also seen type pixel rearrangement caused perspective change videos reorganized following specific camera motion noisy latents change accordingly building propose camtrol enables robust camera control video diffusion models achieved twostage process first model image layout rearrangement explicit camera movement point cloud space second generate videos camera motion leveraging layout prior noisy latents formed series rearranged images extensive experiments demonstrated superior performance video generation camera motion alignment compared finetuned methods furthermore show capability camtrol generalize various base models well impressive applications scalable motion control dealing complicated trajectories unsupervised video generation videos available httpslifedecodergithubiocamtrol,-1,0.0,-1,0.0
image conductor precision control interactive video synthesis filmmaking animation production often require sophisticated techniques coordinating camera transitions object movements typically involving laborintensive realworld capturing despite advancements generative ai video creation achieving precise control motion interactive video asset generation remains challenging end propose image conductor method precise control camera transitions object movements generate video assets single image wellcultivated training strategy proposed separate distinct camera object motion camera lora weights object lora weights address cinematographic variations illposed trajectories introduce camerafree guidance technique inference enhancing object movements eliminating camera transitions additionally develop trajectoryoriented video motion data curation pipeline training quantitative qualitative experiments demonstrate methods precision finegrained control generating motioncontrollable videos images advancing practical application interactive video synthesis project webpage available httpsliyaoweistugithubioprojectimageconductor,-1,0.0,-1,0.0
vimi grounding video generation multimodal instruction existing texttovideo diffusion models rely solely textonly encoders pretraining limitation stems absence largescale multimodal prompt video datasets resulting lack visual grounding restricting versatility application multimodal integration address construct largescale multimodal prompt dataset employing retrieval methods pair incontext examples given text prompts utilize twostage training strategy enable diverse video generation tasks within model first stage propose multimodal conditional video generation framework pretraining augmented datasets establishing foundational model grounded video generation secondly finetune model first stage three video generation tasks incorporating multimodal instructions process refines models ability handle diverse inputs tasks ensuring seamless integration multimodal information twostage training process vimi demonstrates multimodal understanding capabilities producing contextually rich personalized videos grounded provided inputs shown figure compared previous visual grounded video generation methods vimi synthesize consistent temporally coherent videos large motion retaining semantic control lastly vimi also achieves stateoftheart texttovideo generation results benchmark,-1,0.0,-1,0.0
tugofwar deepfake generation detection multimodal generative models rapidly evolving leading surge generation realistic video audio offers exciting possibilities also serious risks deepfake videos convincingly impersonate individuals particularly garnered attention due potential misuse spreading misinformation creating fraudulent content survey paper examines dual landscape deepfake video generation detection emphasizing need effective countermeasures potential abuses provide comprehensive overview current deepfake generation techniques including face swapping reenactment audiodriven animation leverage cuttingedge technologies like gans diffusion models produce highly realistic fake videos additionally analyze various detection approaches designed differentiate authentic altered videos detecting visual artifacts deploying advanced algorithms pinpoint inconsistencies across video audio signals effectiveness detection methods heavily relies diversity quality datasets used training evaluation discuss evolution deepfake datasets highlighting importance robust diverse frequently updated collections enhance detection accuracy generalizability deepfakes become increasingly indistinguishable authentic content developing advanced detection techniques keep pace generation technologies crucial advocate proactive approach tugofwar deepfake creators detectors emphasizing need continuous research collaboration standardization evaluation metrics creation comprehensive benchmarks,4,0.9319041568896691,4,0.9319041568896691
vitondit learning inthewild video tryon human dance videos via diffusion transformers video tryon stands promising area tremendous realworld potential prior works limited transferring product clothing images onto person videos simple poses backgrounds underperforming casually captured videos recently sora revealed scalability diffusion transformer dit generating lifelike videos featuring realworld scenarios inspired explore propose first ditbased video tryon framework practical inthewild applications named vitondit specifically vitondit consists garment extractor spatialtemporal denoising dit identity preservation controlnet faithfully recover clothing details extracted garment features fused selfattention outputs denoising dit controlnet also introduce novel random selection strategies training interpolated autoregressive iar technique inference facilitate long video generation unlike existing attempts require laborious restrictive construction paired training dataset severely limiting scalability vitondit alleviates relying solely unpaired human dance videos carefully designed multistage training strategy furthermore curate challenging benchmark dataset evaluate performance casual video tryon extensive experiments demonstrate superiority vitondit generating spatiotemporal consistent tryon results inthewild videos complicated human poses,-1,0.0,-1,0.0
trajectoryconditioned generation recent techniques generation synthesize dynamic scenes using supervision pretrained texttovideo models however existing representations motion deformation models timedependent neural representations limited amount motion generatethey synthesize motion extending far beyond bounding box used volume rendering lack flexible motion model contributes gap realism generation methods recent nearphotorealistic video generation models propose trajectoryconditioned generation factors motion global local components represent global motion scenes bounding box using rigid transformation along trajectory parameterized spline learn local deformations conform global trajectory using supervision texttovideo model approach enables synthesis scenes animated along arbitrary trajectories compositional scene generation significant improvements realism amount generated motion evaluate qualitatively user study video results viewed website,1,0.9243191768091636,1,0.9243191768091636
generative video propagation largescale video generation models inherent ability realistically model natural scenes paper demonstrate careful design generative video propagation framework various video tasks addressed unified way leveraging generative power models specifically framework genprop encodes original video selective content encoder propagates changes made first frame using imagetovideo generation model propose data generation scheme cover multiple video tasks based instancelevel video segmentation datasets model trained incorporating mask prediction decoder head optimizing regionaware loss aid encoder preserve original content generation model propagates modified region novel design opens new possibilities editing scenarios genprop allows substantial changes objects shape insertion inserted objects exhibit independent motion removal genprop effectively removes effects like shadows reflections whole video tracking genprop capable tracking objects associated effects together experiment results demonstrate leading performance model various video tasks provide indepth analyses proposed framework,-1,0.0,-1,0.0
omnitokenizer joint imagevideo tokenizer visual generation tokenizer serving translator map intricate visual data compact latent space lies core visual generative models based finding existing tokenizers tailored image video inputs paper presents omnitokenizer transformerbased tokenizer joint image video tokenization omnitokenizer designed spatialtemporal decoupled architecture integrates window causal attention spatial temporal modeling exploit complementary nature image video data propose progressive training strategy omnitokenizer first trained image data fixed resolution develop spatial encoding capacity jointly trained image video data multiple resolutions learn temporal dynamics omnitokenizer first time handles image video inputs within unified framework proves possibility realizing synergy extensive experiments demonstrate omnitokenizer achieves stateoftheart sota reconstruction performance various image video datasets eg reconstruction fid imagenet reconstruction fvd beating previous sota methods respectively additionally also show integrated omnitokenizer language modelbased approaches diffusion models realize advanced visual synthesis performance underscoring superiority versatility method code available httpsgithubcomfoundationvisionomnitokenizer,-1,0.0,-1,0.0
mavin multiaction video generation diffusion models via transition video infilling diffusionbased video generation achieved significant progress yet generating multiple actions occur sequentially remains formidable task directly generating video sequential actions extremely challenging due scarcity finegrained action annotations difficulty establishing temporal semantic correspondences maintaining longterm consistency tackle propose intuitive straightforward solution splicing multiple singleaction video segments sequentially core challenge lies generating smooth natural transitions segments given inherent complexity variability action transitions introduce mavin multiaction video infilling model designed generate transition videos seamlessly connect two given videos forming cohesive integrated sequence mavin incorporates several innovative techniques address challenges transition video infilling task firstly consecutive noising strategy coupled variablelength sampling employed handle large infilling gaps varied generation lengths secondly boundary frame guidance bfg proposed address lack semantic guidance transition generation lastly gaussian filter mixer gfm dynamically manages noise initialization inference mitigating traintest discrepancy preserving generation flexibility additionally introduce new metric cliprs clip relative smoothness evaluate temporal coherence smoothness complementing traditional qualitybased metrics experimental results horse tiger scenarios demonstrate mavins superior performance generating smooth coherent video transitions compared existing methods,9,0.8074240529281956,9,0.8074240529281956
bora biomedical generalist video generation model generative models hold promise revolutionizing medical education robotassisted surgery data augmentation medical ai development diffusion models generate realistic images text prompts recent advancements demonstrated ability create diverse highquality videos however models often struggle generating accurate representations medical procedures detailed anatomical structures paper introduces bora first spatiotemporal diffusion probabilistic model designed textguided biomedical video generation bora leverages transformer architecture pretrained generalpurpose video generation tasks finetuned model alignment instruction tuning using newly established medical video corpus includes paired textvideo data various biomedical fields best knowledge first attempt establish comprehensive annotated biomedical video dataset bora capable generating highquality video data across four distinct biomedical domains adhering medical expert standards demonstrating consistency diversity generalist video generative model holds significant potential enhancing medical consultation decisionmaking particularly resourcelimited settings additionally bora could pave way immersive medical training procedure planning extensive experiments distinct medical modalities endoscopy ultrasound mri cell tracking validate effectiveness model understanding biomedical instructions superior performance across subjects compared stateoftheart generation models,-1,0.0,-1,0.0
dicode diffusioncompressed deep tokens autoregressive video generation language models videos inherently temporal sequences nature work explore potential modeling videos chronological scalable manner autoregressive ar language models inspired success natural language processing introduce dicode novel approach leverages diffusioncompressed deep tokens generate videos language model autoregressive manner unlike existing methods employ lowlevel representations limited compression rates dicode utilizes deep tokens considerable compression rate reduction token count significant compression made possible tokenizer trained leveraging prior knowledge video diffusion models deep tokens enable dicode employ vanilla ar language models video generation akin translating one visual language another treating videos temporal sequences dicode fully harnesses capabilities language models autoregressive generation dicode scalable using readily available ar architectures capable generating videos ranging seconds one minute using gpus training evaluate dicode quantitatively qualitatively demonstrating performs comparably existing methods terms quality ensuring efficient training showcase scalability release series dicode configurations varying parameter sizes observe consistent improvement performance model size increases believe dicodes exploration academia represents promising initial step toward scalable video modeling ar language models paving way development larger powerful video generation models,-1,0.0,-1,0.0
camco cameracontrollable imagetovideo generation recently video diffusion models emerged expressive generative tools highquality video content creation readily available general users however models often offer precise control camera poses video generation limiting expression cinematic language user control address issue introduce camco allows finegrained camera pose control imagetovideo generation equip pretrained imagetovideo generator accurately parameterized camera pose input using plucker coordinates enhance consistency videos produced integrate epipolar attention module attention block enforces epipolar constraints feature maps additionally finetune camco realworld videos camera poses estimated structurefrommotion algorithms better synthesize object motion experiments show camco significantly improves consistency camera control capabilities compared previous models effectively generating plausible object motion project page,-1,0.0,-1,0.0
lumisculpt consistency lighting control network video generation lighting plays pivotal role ensuring naturalness video generation significantly influencing aesthetic quality generated content however due deep coupling lighting temporal features videos remains challenging disentangle model independent coherent lighting attributes limiting ability control lighting video generation paper inspired established controllable models propose lumisculpt first time enables precise consistent lighting control generation modelslumisculpt equips video generation strong interactive capabilities allowing input custom lighting reference image sequences furthermore core learnable plugandplay module lumisculpt facilitates remarkable control lighting intensity position trajectory latent video diffusion models based advanced dit backboneadditionally effectively train lumisculpt address issue insufficient lighting data construct lumihuman new lightweight flexible dataset portrait lighting images videos experimental results demonstrate lumisculpt achieves precise highquality lighting control video generation,1,1.0,1,1.0
exposing aigenerated videos benchmark dataset localandglobal temporal defect based detection method generative model made significant advancements creation realistic videos causes security issues however emerging risk adequately addressed due absence benchmark dataset aigenerated videos paper first construct video dataset using advanced diffusionbased video generation algorithms various semantic contents besides typical video lossy operations network transmission adopted generate degraded samples analyzing local global temporal defects current aigenerated videos novel detection framework adaptively learning local motion information global appearance variation constructed expose fake videos finally experiments conducted evaluate generalization robustness different spatial temporal domain detection methods results serve baseline demonstrate research challenge future studies,4,0.9275141991094649,4,0.9275141991094649
shaping stabilized video mitigating unintended changes conceptaugmented video editing textdriven video editing utilizing generative diffusion models garnered significant attention due potential applications however existing approaches constrained limited word embeddings provided pretraining hinders nuanced editing targeting open concepts specific attributes directly altering keywords target prompts often results unintended disruptions attention mechanisms achieve flexible editing easily work proposes improved conceptaugmented video editing approach generates diverse stable target videos flexibly devising abstract conceptual pairs specifically framework involves conceptaugmented textual inversion dual prior supervision mechanism former enables plugandplay guidance stable diffusion video editing effectively capturing target attributes stylized results dual prior supervision mechanism significantly enhances video stability fidelity comprehensive evaluations demonstrate approach generates stable lifelike videos outperforming stateoftheart methods,9,0.7501081692553011,9,0.7501081692553011
immersepro endtoend stereo video synthesis via implicit disparity learning introduce textitimmersepro innovative framework specifically designed transform singleview videos stereo videos framework utilizes novel dualbranch architecture comprising disparity branch context branch video data leveraging spatialtemporal attention mechanisms textitimmersepro employs implicit disparity guidance enabling generation stereo pairs video sequences without need explicit disparity maps thus reducing potential errors associated disparity estimation models addition technical advancements introduce youtubesbs dataset comprehensive collection stereo videos sourced youtube dataset unprecedented scale featuring million stereo pairs designed facilitate training benchmarking stereo video generation models experiments demonstrate effectiveness textitimmersepro producing highquality stereo videos offering significant improvements existing methods compared best competitor stereofrommono quantitatively improve results ssim psnr,-1,0.0,-1,0.0
sportshhi dataset humanhuman interaction detection sports videos videobased visual relation detection tasks video scene graph generation play important roles finegrained video understanding however current video visual relation detection datasets two main limitations hinder progress research area first explore complex humanhuman interactions multiperson scenarios second relation types existing datasets relatively lowlevel semantics often recognized appearance simple prior information without need detailed spatiotemporal context reasoning nevertheless comprehending highlevel interactions humans crucial understanding complex multiperson videos sports surveillance videos address issue propose new video visual relation detection task video humanhuman interaction detection build dataset named sportshhi sportshhi contains highlevel interaction classes basketball volleyball sports human bounding boxes interaction instances annotated keyframes benchmark propose twostage baseline method conduct extensive experiments reveal key factors successful humanhuman interaction detector hope sportshhi stimulate research human interaction understanding videos promote development spatiotemporal context modeling techniques video visual relation detection,-1,0.0,-1,0.0
zeroshot surgical tool segmentation monocular video using segment anything model segment anything model sam latest generation foundation model image video segmentation trained expansive segment anything video sav dataset comprises million masks across videos sam advances predecessors capabilities supporting zeroshot segmentation various prompts eg points boxes masks robust zeroshot performance efficient memory usage make sam particularly appealing surgical tool segmentation videos especially given scarcity labeled data diversity surgical procedures study evaluate zeroshot video segmentation performance sam model across different types surgeries including endoscopy microscopy also assess performance videos featuring single multiple tools varying lengths demonstrate sam applicability effectiveness surgical domain found sam demonstrates strong capability segmenting various surgical videos new tools enter scene additional prompts necessary maintain segmentation accuracy specific challenges inherent surgical videos impact robustness sam,-1,0.0,-1,0.0
noise crystallization liquid noise zeroshot video generation using image diffusion models although powerful image generation consistent controllable video longstanding problem diffusion models video models require extensive training computational resources leading high costs large environmental impacts moreover video models currently offer limited control output motion paper introduces novel approach video generation augmenting image diffusion models create sequential animation frames maintaining fine detail techniques applied existing image models without training video parameters zeroshot altering input noise latent diffusion model two complementary methods presented noise crystallization ensures consistency limited large movements due reduced latent embedding sizes liquid noise trades consistency greater flexibility without resolution limitations core concepts also allow applications relighting seamless upscaling improved video style transfer furthermore exploration vae embedding used latent diffusion models performed resulting interesting theoretical insights method humaninterpretable latent spaces,-1,0.0,-1,0.0
latentcolorization latent diffusionbased speaker video colorization current research predominantly focuses imagebased colorization domain videobased colorization remains relatively unexplored existing video colorization techniques operate framebyframe basis often overlooking critical aspect temporal coherence successive frames approach result inconsistencies across frames leading undesirable effects like flickering abrupt color transitions frames address challenges harness generative capabilities finetuned latent diffusion model designed specifically video colorization introducing novel solution achieving temporal consistency video colorization well demonstrating strong improvements established image quality metrics compared existing methods furthermore perform subjective study users preferred approach existing state art dataset encompasses combination conventional datasets videos televisionmovies short leveraging power finetuned latent diffusionbased colorization system temporal consistency mechanism improve performance automatic video colorization addressing challenges temporal inconsistency short demonstration results seen example videos available httpsyoutubevdbzszdfuxm,-1,0.0,-1,0.0
live stream translation via unidirectional attention video diffusion models large language models shown remarkable efficacy generating streaming data text audio thanks temporally unidirectional attention mechanism models correlations current token previous tokens however video streaming remains much less explored despite growing need live video processing stateoftheart video diffusion models leverage bidirectional temporal attention model correlations current frame surrounding ie including future frames hinders processing streaming videos address problem present first attempt designing video diffusion model unidirectional temporal attention specifically targeting live streaming video translation compared previous works approach ensures temporal consistency smoothness correlating current frame predecessors initial warmup frames without future frames additionally use highly efficient denoising scheme featuring kvcache mechanism pipelining facilitate streaming video translation interactive framerates extensive experiments demonstrate effectiveness proposed attention mechanism pipeline outperforming previous methods terms temporal smoothness andor efficiency,-1,0.0,-1,0.0
humanvdm learning singleimage human gaussian splatting video diffusion models generating lifelike humans single rgb image remains challenging task computer vision requires accurate modeling geometry highquality texture plausible unseen parts existing methods typically use multiview diffusion models generation often face inconsistent view issues hinder highquality human generation address propose humanvdm novel method generating human single rgb image using video diffusion models humanvdm provides temporally consistent views human generation using gaussian splatting consists three modules viewconsistent human video diffusion module video augmentation module gaussian splatting module first single image fed human video diffusion module generate coherent human video next video augmentation module applies superresolution video interpolation enhance textures geometric smoothness generated video finally human gaussian splatting module learns lifelike humans guidance highresolution viewconsistent images experiments demonstrate humanvdm achieves highquality human single image outperforming stateoftheart methods generation quality quantity project page httpshumanvdmgithubiohumanvdm,-1,0.0,-1,0.0
transforming drone videos bevs videobased geolocalization existing approaches drone visual geolocalization predominantly adopt imagebased setting single droneview snapshot matched images platforms task formulation however underutilizes inherent video output drone sensitive occlusions viewpoint disparity address limitations formulate new videobased drone geolocalization task propose paradigm paradigm transforms video birds eye view bev simplifying subsequent textbfinterplatform matching process particular employ gaussian splatting reconstruct scene obtain bev projection different existing transform methods eg polar transform bevs preserve finegrained details without significant distortion facilitate discriminative textbfintraplatform representation learning paradigm also incorporates diffusionbased module generating hard negative samples validate approach introduce univ new videobased geolocalization dataset extends imagebased dataset univ features flight paths elevation angles increased frame rates frames per second fps extensive experiments univ dataset show paradigm achieves competitive recall rates outperforms conventional videobased methods compared competitive methods proposed approach exhibits robustness lower elevations occlusions,-1,0.0,-1,0.0
easyanimate highperformance long video generation method based transformer architecture paper presents easyanimate advanced method video generation leverages power transformer architecture highperformance outcomes expanded dit framework originally designed image synthesis accommodate complexities video generation incorporating motion module block used capture temporal dynamics thereby ensuring production consistent frames seamless motion transitions motion module adapted various dit baseline methods generate video different styles also generate videos different frame rates resolutions training inference phases suitable images videos moreover introduce slice vae novel approach condense temporal axis facilitating generation long duration videos currently easyanimate exhibits proficiency generate videos frames provide holistic ecosystem video production based dit encompassing aspects data preprocessing vae training dit models training baseline model lora model endtoend video inference code available httpsgithubcomaigcappseasyanimate continuously working enhance performance method,-1,0.0,-1,0.0
hierarchical patch diffusion models highresolution video generation diffusion models demonstrated remarkable performance image video synthesis however scaling highresolution inputs challenging requires restructuring diffusion pipeline multiple independent components limiting scalability complicating downstream applications makes efficient training unlocks endtoend optimization highresolution videos improve pdms two principled ways first enforce consistency patches develop deep context fusion architectural technique propagates context information lowscale highscale patches hierarchical manner second accelerate training inference propose adaptive computation allocates network capacity computation towards coarse image details resulting model sets new stateoftheart fvd score inception score classconditional video generation surpassing recent methods show rapidly finetuned base lowresolution generator highresolution times times texttovideo synthesis best knowledge model first diffusionbased architecture trained high resolutions entirely endtoend project webpage httpssnapresearchgithubiohpdm,-1,0.0,-1,0.0
diving deep motion representation videotext models videos informative images capture dynamics scene representing motion videos capture dynamic activities work introduce generated motion descriptions capture finegrained motion descriptions activities apply three action datasets evaluated several videotext models task retrieval motion descriptions found fall far behind human expert performance two action datasets raising question whether videotext models understand motion videos address introduce method improving motion understanding videotext models utilizing motion descriptions method proves effective two action datasets motion description retrieval task results draw attention need quality captions involving finegrained motion information existing datasets demonstrate effectiveness proposed pipeline understanding finegrained motion videotext retrieval,0,0.8910470884561363,0,0.8910470884561363
toolchain comprehensive audiovideo analysis using deep learning based multimodal approach use case riot violent context detection paper present toolchain comprehensive audiovideo analysis leveraging deep learning based multimodal approach end different specific tasks speech text acoustic scene classification asc acoustic event detection aed visual object detection vod image captioning ic video captioning vc conducted integrated toolchain combining individual tasks analyzing audio visual data extracted input video toolchain offers various audiovideobased applications two general applications audiovideo clustering comprehensive audiovideo summary specific application riot violent context detection furthermore toolchain presents flexible adaptable architecture effective integrate new models audiovideobased applications,8,0.8123071189219913,8,0.8123071189219913
multigranularity video object segmentation current benchmarks video segmentation limited annotating salient objects ie foreground instances despite impressive architectural designs previous works trained benchmarks struggled adapt realworld scenarios thus developing new video segmentation dataset aimed tracking multigranularity segmentation target video scene necessary work aim generate multigranularity video segmentation dataset annotated salient nonsalient masks achieve propose largescale densely annotated multigranularity video object segmentation mugvos dataset includes various types granularities mask annotations automatically collected training set assists tracking salient nonsalient objects also curated humanannotated test set reliable evaluation addition present memorybased mask propagation model mmpm trained evaluated mugvos dataset leads best performance among existing video object segmentation methods segment sambased video segmentation methods project page available httpscvlabkaistgithubiomugvos,-1,0.0,-1,0.0
motion control enhanced complex action video generation existing texttovideo models often struggle generating videos sufficiently pronounced complex actions key limitation lies text prompts inability precisely convey intricate motion details address propose novel framework mvideo designed produce longduration videos precise fluid actions mvideo overcomes limitations text prompts incorporating mask sequences additional motion condition input providing clearer accurate representation intended actions leveraging foundational vision models groundingdino mvideo automatically generates mask sequences enhancing efficiency robustness results demonstrate training mvideo effectively aligns text prompts motion conditions produce videos simultaneously meet criteria dual control mechanism allows dynamic video generation enabling alterations either text prompt motion condition independently tandem furthermore mvideo supports motion condition editing composition facilitating generation videos complex actions mvideo thus advances motion generation setting strong benchmark improved action depiction current video diffusion models project page available,9,0.7676269057144607,9,0.7676269057144607
mimosa humanai cocreation computational spatial audio effects videos spatial audio offers immersive video consumption experiences viewers however creating editing spatial audio often expensive requires specialized equipment skills posing high barrier amateur video creators present mimosa humanai cocreation tool enables amateur users computationally generate manipulate spatial audio effects video monaural stereo audio mimosa automatically grounds sound source corresponding sounding object visual scene enables users validate fix errors locations sounding objects users also augment spatial audio effect flexibly manipulating sounding source positions creatively customizing audio effect design mimosa exemplifies humanai collaboration approach instead utilizing stateof art endtoend blackbox ml models uses multistep pipeline aligns interpretable intermediate results users workflow lab user study participants demonstrates mimosas usability usefulness expressiveness capability creating immersive spatial audio effects collaboration users,-1,0.0,-1,0.0
improving multicenter generalizability ganbased fat suppression using federated learning generative adversarial network ganbased synthesis fat suppressed fs mris nonfs proton density sequences potential accelerate acquisition knee mris however gans trained singlesite data poor generalizability external data show federated learning improve multicenter generalizability gans synthesizing fs mris facilitating privacypreserving multiinstitutional collaborations,-1,0.0,-1,0.0
splatter video video gaussian representation versatile processing video representation longstanding problem crucial various downstream tasks trackingdepth predictionsegmentationview synthesisand editing however current methods either struggle model complex motions due absence structure rely implicit representations illsuited manipulation tasks address challenges introduce novel explicit representationvideo gaussian representation embeds video gaussians proposed representation models video appearance canonical space using explicit gaussians proxies associates gaussian motions video motion approach offers intrinsic explicit representation layered atlas volumetric pixel matrices obtain representation distill priors optical flow depth foundation models regularize learning illposed setting extensive applications demonstrate versatility new video representation proven effective numerous video processing tasks including tracking consistent video depth feature refinement motion appearance editing stereoscopic video generation project page,-1,0.0,-1,0.0
intentiondriven egotoexo video generation egotoexo video generation refers generating corresponding exocentric video according egocentric video providing valuable applications arvr embodied ai benefiting advancements diffusion model techniques notable progress achieved video generation however existing methods build upon spatiotemporal consistency assumptions adjacent frames satisfied egotoexo scenarios due drastic changes views end paper proposes intentiondriven egotoexo video generation framework ide leverages action intention consisting human movement action description viewindependent representation guide video generation preserving consistency content motion specifically egocentric head trajectory first estimated multiview stereo matching crossview feature perception module introduced establish correspondences exo ego views guiding trajectory transformation module infer human fullbody movement head trajectory meanwhile present action description unit maps action semantics feature space consistent exocentric image finally inferred human movement highlevel action descriptions jointly guide generation exocentric motion interaction content ie corresponding optical flow occlusion maps backward process diffusion model ultimately warping corresponding exocentric video conduct extensive experiments relevant dataset diverse exoego video pairs ide outperforms stateoftheart models subjective objective assessments demonstrating efficacy egotoexo video generation,-1,0.0,-1,0.0
video editing via factorized diffusion distillation introduce emu video edit eve model establishes new stateofthe art video editing without relying supervised video editing data develop eve separately train image editing adapter video generation adapter attach texttoimage model align adapters towards video editing introduce new unsupervised distillation procedure factorized diffusion distillation procedure distills knowledge one teachers simultaneously without supervised data utilize procedure teach eve edit videos jointly distilling knowledge precisely edit individual frame image editing adapter ii ensure temporal consistency among edited frames using video generation adapter finally demonstrate potential approach unlocking capabilities align additional combinations adapters,-1,0.0,-1,0.0
tell hear see video audio generation text content visual audio scenes multifaceted video paired various audio viceversa thereby videotoaudio generation task imperative introduce steering approaches controlling generated audio videotoaudio generation wellestablished generative task existing methods lack controllability work propose vatt multimodal generative framework takes video optional text prompt input generates audio optional textual description audio framework two advantages videotoaudio generation process refined controlled via text complements context visual information ii model suggest audio generate video generating audio captions vatt consists two key modules vatt converter llm finetuned instructions includes projection layer maps video features llm vector space vatt audio transformer generates audio tokens visual frames optional text prompt using iterative parallel decoding audio tokens converted waveform pretrained neural codec experiments show vatt compared existing videotoaudio generation methods objective metrics achieves competitive performance audio caption provided audio caption provided prompt vatt achieves even refined performance lowest kld score furthermore subjective studies show vatt audio chosen preferred generated audio audio generated existing methods vatt enables controllable videotoaudio generation text well suggesting text prompts videos audio captions unlocking novel applications textguided videotoaudio generation videotoaudio captioning,8,1.0,8,1.0
dynamic tryon taming video virtual tryon dynamic attention mechanism video tryon stands promising area tremendous realworld potential previous research video tryon primarily focused transferring product clothing images videos simple human poses performing poorly complex movements better preserve clothing details approaches armed additional garment encoder resulting higher computational resource consumption primary challenges domain twofold leveraging garment encoders capabilities video tryon lowering computational requirements ensuring temporal consistency synthesis human body parts especially rapid movements tackle issues propose novel video tryon framework based diffusion transformerdit named dynamic tryon reduce computational overhead adopt straightforward approach utilizing dit backbone garment encoder employing dynamic feature fusion module store integrate garment features ensure temporal consistency human body parts introduce limbaware dynamic attention module enforces dit backbone focus regions human limbs denoising process extensive experiments demonstrate superiority dynamic tryon generating stable smooth tryon results even videos featuring complicated human postures,-1,0.0,-1,0.0
eva embodied world model future video anticipation world models integrate raw data various modalities images language simulate comprehensive interactions world thereby displaying crucial roles fields like mixed reality robotics yet applying world model accurate video prediction quite challenging due complex dynamic intentions various scenes practice paper inspired human rethinking process decompose complex video prediction four metatasks enable world model handle issue finegrained manner alongside tasks introduce new benchmark named embodied video anticipation benchmark evabench provide wellrounded evaluation evabench focused evaluating video prediction ability human robot actions presenting significant challenges language model generation model targeting embodied video prediction propose embodied video anticipator eva unified framework aiming video understanding generation eva integrates video generation model visual language model effectively combining reasoning capabilities highquality generation moreover enhance generalization framework tailordesigned multistage pretraining paradigm adaptatively ensembles lora produce highfidelity results extensive experiments evabench highlight potential eva significantly improve performance embodied scenes paving way largescale pretrained models realworld prediction tasks,-1,0.0,-1,0.0
flexifilm long video generation flexible conditions generating long consistent videos emerged significant yet challenging problem existing diffusionbased video generation models derived image generation models demonstrate promising performance generating short videos simple conditioning mechanism sampling strategyoriginally designed image generationcause severe performance degradation adapted long video generation results prominent temporal inconsistency overexposure thus work introduce flexifilm new diffusion model tailored long video generation framework incorporates temporal conditioner establish consistent relationship generation multimodal conditions resampling strategy tackle overexposure empirical results demonstrate flexifilm generates long consistent videos seconds length outperforming competitors qualitative quantitative analyses project page httpsyichengithubioflexifilmpage,11,0.9179474933069307,11,0.9179474933069307
video interpolation diffusion models present vidim generative model video interpolation creates short videos given start end frame order achieve high fidelity generate motions unseen input data vidim uses cascaded diffusion models first generate target video low resolution generate highresolution video conditioned lowresolution generated video compare vidim previous stateoftheart methods video interpolation demonstrate works fail settings underlying motion complex nonlinear ambiguous vidim easily handle cases additionally demonstrate classifierfree guidance start end frame conditioning superresolution model original highresolution frames without additional parameters unlocks highfidelity results vidim fast sample jointly denoises frames generated requires less billion parameters per diffusion model produce compelling results still enjoys scalability improved quality larger parameter counts,-1,0.0,-1,0.0
ufo enhancing diffusionbased video generation uniform frame organizer recently diffusionbased video generation models achieved significant success however existing models often suffer issues like weak consistency declining image quality time overcome challenges inspired aesthetic principles propose noninvasive plugin called uniform frame organizer ufo compatible diffusionbased video generation model ufo comprises series adaptive adapters adjustable intensities significantly enhance consistency foreground background videos improve image quality without altering original model parameters integrated training ufo simple efficient requires minimal resources supports stylized training modular design allows combination multiple ufos enabling customization personalized video generation models furthermore ufo also supports direct transferability across different models specification without need specific retraining experimental results indicate ufo effectively enhances video generation quality demonstrates superiority public video generation benchmarks code publicly available httpsgithubcomdelongliubuptufo,-1,0.0,-1,0.0
realtime onestep diffusionbased expressive portrait videos generation latent diffusion models made great strides generating expressive portrait videos accurate lipsync natural motion single reference image audio input however models far realtime often requiring many sampling steps take minutes generate even one second videosignificantly limiting practical use introduce osalcm onestep avatar latent consistency model paving way realtime diffusionbased avatars method achieves comparable video quality existing methods requires one sampling step making faster accomplish propose novel avatar discriminator design guides lipaudio consistency motion expressiveness enhance video quality limited sampling steps additionally employ secondstage training architecture using editing finetuned method eft transforming video generation editing task training effectively address temporal gap challenge singlestep generation experiments demonstrate osalcm outperforms existing opensource portrait video generation models operating efficiently single sampling step,6,0.38504700256141255,6,0.38504700256141255
perceptual video quality assessment survey perceptual video quality assessment plays vital role field video processing due existence quality degradations introduced various stages video signal acquisition compression transmission display advancement internet communication cloud service technology video content traffic growing exponentially emphasizes requirement accurate rapid assessment video quality therefore numerous subjective objective video quality assessment studies conducted past two decades generic videos specific videos streaming usergenerated content ugc virtual augmented reality vr ar high frame rate hfr audiovisual etc survey provides uptodate comprehensive review video quality assessment studies specifically first review subjective video quality assessment methodologies databases necessary validating performance video quality metrics second objective video quality assessment algorithms general purposes surveyed concluded according methodologies utilized quality measures third overview objective video quality assessment measures specific applications emerging topics finally performances stateoftheart video quality assessment measures compared analyzed survey provides systematic overview classical works recent progresses realm video quality assessment help researchers quickly access field conduct relevant research,12,1.0,12,1.0
faster projected gan towards faster fewshot image generation order solve problems long training time large consumption computing resources huge parameter amount gan network image generation paper proposes improved gan network model named faster projected gan based projected gan proposed network mainly focuses improvement generator projected gan introducing depth separable convolution dsc number parameters projected gan reduced training speed accelerated memory saved experimental results show artpainting landscape fewshot image datasets speed increase memory saving achieved time fid loss less loss amount model parameters better controlled time significant training speed improvement achieved small sample image generation task special scenes earthquake scenes public datasets,-1,0.0,-1,0.0
idanimator zeroshot identitypreserving human video generation generating highfidelity human video specified identities attracted significant attention content generation community however existing techniques struggle strike balance training efficiency identity preservation either requiring tedious casebycase finetuning usually missing identity details video generation process study present textbfidanimator zeroshot humanvideo generation approach perform personalized video generation given single reference facial image without training idanimator inherits existing diffusionbased video generation backbones face adapter encode idrelevant embeddings learnable facial latent queries facilitate extraction identity information video generation introduce idoriented dataset construction pipeline incorporates unified human attributes action captioning techniques constructed facial image pool based pipeline random reference training strategy devised precisely capture idrelevant embeddings idpreserving loss thus improving fidelity generalization capacity model idspecific video generation extensive experiments demonstrate superiority idanimator generate personalized human videos previous models moreover method highly compatible popular pretrained models like animatediff various community backbone models showing high extendability realworld applications video generation identity preservation highly desired codes checkpoints released httpsgithubcomidanimatoridanimator,-1,0.0,-1,0.0
wfvae enhancing video vae waveletdriven energy flow latent video diffusion model video variational autoencoder vae encodes videos lowdimensional latent space becoming key component latent video diffusion models lvdms reduce model training costs however resolution duration generated videos increase encoding cost video vaes becomes limiting bottleneck training lvdms moreover blockwise inference method adopted lvdms lead discontinuities latent space processing longduration videos key addressing computational bottleneck lies decomposing videos distinct components efficiently encoding critical information wavelet transform decompose videos multiple frequencydomain components improve efficiency significantly thus propose wavelet flow vae wfvae autoencoder leverages multilevel wavelet transform facilitate lowfrequency energy flow latent representation furthermore introduce method called causal cache maintains integrity latent space blockwise inference compared stateoftheart video vaes wfvae demonstrates superior performance psnr lpips metrics achieving higher throughput lower memory consumption maintaining competitive reconstruction quality code models available httpsgithubcompkuyuangroupwfvae,2,0.7874212550834417,2,0.7874212550834417
videoqa era llms empirical study video large language models videollms flourishing advanced many videolanguage tasks golden testbed video question answering videoqa plays pivotal role videollm developing work conducts timely comprehensive study videollms behavior videoqa aiming elucidate success failure modes provide insights towards humanlike video understanding question answering analyses demonstrate videollms excel videoqa correlate contextual cues generate plausible responses questions varied video contents however models falter handling video temporality reasoning temporal content ordering grounding qarelevant temporal moments moreover models behave unintuitively unresponsive adversarial video perturbations sensitive simple variations candidate answers questions also necessarily generalize better findings demonstrate videollms qa capability standard condition yet highlight severe deficiency robustness interpretability suggesting urgent need rationales videollm developing,0,1.0,0,1.0
towards long video understanding via finedetailed video story generation long video understanding become critical task computer vision driving advancements across numerous applications surveillance content retrieval existing video understanding methods suffer two challenges dealing long video understanding intricate longcontext relationship modeling interference redundancy tackle challenges introduce finedetailed video story generation fdvs interprets long videos detailed textual representations specifically achieve finegrained modeling longtemporal content propose bottomup video interpretation mechanism progressively interprets video content clips video avoid interference redundant information videos introduce semantic redundancy reduction mechanism removes redundancy visual textual levels method transforms long videos hierarchical textual representations contain multigranularity information video representations fdvs applicable various tasks without finetuning evaluate proposed method across eight datasets spanning three tasks performance demonstrates effectiveness versatility method,-1,0.0,-1,0.0
subjective objective quality assessment methods stereoscopic videos visibility affecting distortions present two major contributions work create full hd resolution stereoscopic video dataset comprised reference distorted videos test stimuli produced simulating five levels fog haze ambiances pristine left right video sequences perform subjective analysis created video dataset viewers compute difference mean opinion scores dmos quality representative dataset opinion unaware ou distortion unaware du video quality assessment model developed videos construct cyclopean frames individual views video partition nonoverlapping blocks analyze natural scene statistics nss patches pristine test videos empirically model nss features univariate generalized gaussian distribution uggd compute uggd model parameters alpha beta multiple spatial scales multiple orientations spherical steerable pyramid decomposition show uggd parameters distortion discriminable perform multivariate gaussian mvg modeling pristine distorted video feature sets compute corresponding mean vectors covariance matrices mvg fits compute bhattacharyya distance measure mean vectors covariance matrices estimate perceptual deviation test video pristine video set finally pool distance measures estimate overall quality score video performance proposed objective algorithm verified popular video datasets irccyn proposed vad stereo dataset algorithm delivers consistent performance across datasets shows competitive performance offtheshelf image video quality assessment algorithms,12,0.7838719137603694,12,0.7838719137603694
gradientfree path integral control enhancing texttovideo generation large visionlanguage models diffusion models achieved impressive results generative tasks like texttoimage texttovideo synthesis however achieving accurate text alignment generation remains challenging due complex temporal dependency across frames existing reinforcement learning rlbased approaches enhance text alignment often require differentiable reward functions constrained limited prompts hindering scalability applicability paper propose novel gradientfree framework aligning generated videos text prompts without requiring additional model training leveraging principles path integral control approximates guidance diffusion models using nondifferentiable reward functions thereby enabling integration powerful blackbox large visionlanguage models lvlms reward model additionally framework supports flexible ensembling multiple reward models including largescale imagebased models synergistically enhance alignment without incurring substantial computational overhead demonstrate significantly improves text alignment across various dimensions enhances overall quality generated videos,-1,0.0,-1,0.0
boximator generating rich controllable motions video synthesis generating rich controllable motion pivotal challenge video synthesis propose boximator new approach finegrained motion control boximator introduces two constraint types hard box soft box users select objects conditional frame using hard boxes use either type boxes roughly rigorously define objects position shape motion path future frames boximator functions plugin existing video diffusion models training process preserves base models knowledge freezing original weights training control module address training challenges introduce novel selftracking technique greatly simplifies learning boxobject correlations empirically boximator achieves stateoftheart video quality fvd scores improving two base models enhanced incorporating box constraints robust motion controllability validated drastic increases bounding box alignment metric human evaluation also shows users favor boximator generation results base model,-1,0.0,-1,0.0
exploring pretrained texttovideo diffusion models referring video object segmentation paper explore visual representations produced pretrained texttovideo diffusion model video understanding tasks hypothesize latent representation learned pretrained generative model encapsulates rich semantics coherent temporal correspondences thereby naturally facilitating video understanding hypothesis validated classic referring video object segmentation rvos task introduce novel framework termed vdit tailored dedicatedly designed components built upon fixed pretrained model specifically vdit uses textual information conditional input ensuring semantic consistency across time precise temporal instance matching incorporates image tokens supplementary textual inputs enriching feature set generate detailed nuanced masks besides instead using standard gaussian noise propose predict videospecific noise extra noise prediction module help preserve feature fidelity elevates segmentation quality extensive experiments surprisingly observe fixed generative diffusion models unlike commonly used video backbones eg video swin transformer pretrained discriminative imagevideo pretasks exhibit better potential maintain semantic alignment temporal consistency existing standard benchmarks vdit achieves highly competitive results surpassing many existing stateoftheart methods code available httpsgithubcombuxiangzhirenvdit,-1,0.0,-1,0.0
memo memoryguided diffusion expressive talking video generation recent advances video diffusion models unlocked new potential realistic audiodriven talking video generation however achieving seamless audiolip synchronization maintaining longterm identity consistency producing natural audioaligned expressions generated talking videos remain significant challenges address challenges propose memoryguided emotionaware diffusion memo endtoend audiodriven portrait animation approach generate identityconsistent expressive talking videos approach built around two key modules memoryguided temporal module enhances longterm identity consistency motion smoothness developing memory states store information longer past context guide temporal modeling via linear attention emotionaware audio module replaces traditional cross attention multimodal attention enhance audiovideo interaction detecting emotions audio refine facial expressions via emotion adaptive layer norm extensive quantitative qualitative results demonstrate memo generates realistic talking videos across diverse image audio types outperforming stateoftheart methods overall quality audiolip synchronization identity consistency expressionemotion alignment,6,1.0,6,1.0
undive generalized underwater video enhancement using generative priors rise marine exploration underwater imaging gained significant attention research topic underwater video enhancement become crucial realtime computer vision tasks marine exploration however existing methods focus enhancing individual frames neglect video temporal dynamics leading visually poor enhancements furthermore lack groundtruth references limits use abundant available underwater video data many applications address issues propose twostage framework enhancing underwater videos first stage uses denoising diffusion probabilistic model learn generative prior unlabeled data capturing robust descriptive feature representations second stage prior incorporated physicsbased image formulation spatial enhancement also enforcing temporal consistency video frames method enables realtime computationallyefficient processing highresolution underwater videos lower resolutions offers efficient enhancement presence diverse watertypes extensive experiments four datasets show approach generalizes well outperforms existing enhancement methods code available githubcomsuhassrinathundive,-1,0.0,-1,0.0
multimodal emotion recognition fusing video semantic mooc learning scenarios massive open online courses mooc learning scenario semantic information instructional videos crucial impact learners emotional state learners mainly acquire knowledge watching instructional videos semantic information videos directly affects learners emotional states however studies paid attention potential influence semantic information instructional videos learners emotional states deeply explore impact video semantic information learners emotions paper innovatively proposes multimodal emotion recognition method fusing video semantic information physiological signals generate video descriptions pretrained large language model llm obtain highlevel semantic information instructional videos using crossattention mechanism modal interaction semantic information fused eye movement photoplethysmography ppg signals obtain features containing critical information three modes accurate recognition learners emotional states realized emotion classifier experimental results show method significantly improved emotion recognition performance providing new perspective efficient method emotion recognition research mooc learning scenarios method proposed paper contributes deeper understanding impact instructional videos learners emotional states also provides beneficial reference future research emotion recognition mooc learning scenarios,-1,0.0,-1,0.0
reanimating images using neural representations dynamic stimuli computer vision models made incredible strides static image recognition still match human performance tasks require understanding complex dynamic motion notably true realworld scenarios embodied agents face complex motionrich environments approach brainnrds brainneural representations dynamic stimuli leverages stateoftheart video diffusion models decouple static image representation motion generation enabling us utilize fmri brain activity deeper understanding human responses dynamic visual stimuli conversely also demonstrate information brains representation motion enhance prediction optical flow artificial systems novel approach leads four main findings visual motion represented finegrained objectlevel resolution optical flow decoded brain activity generated participants viewing video stimuli video encoders outperform imagebased models predicting videodriven brain activity braindecoded motion signals enable realistic video reanimation based initial frame video extend prior work achieve full video decoding videodriven brain activity brainnrds advances understanding brain represents spatial temporal information dynamic visual scenes findings demonstrate potential combining brain imaging video diffusion models developing robust biologicallyinspired computer vision systems show additional decoding encoding examples site httpsbrainnrdsgithubio,-1,0.0,-1,0.0
mardini masked autoregressive diffusion video generation scale introduce mardini new family video diffusion models integrate advantages masked autoregression mar unified diffusion model dm framework mar handles temporal planning dm focuses spatial generation asymmetric network design marbased planning model containing parameters generates planning signals masked frame using lowresolution input ii lightweight generation model uses signals produce highresolution frames via diffusion denoising mardinis mar enables video generation conditioned number masked frames frame positions single model handle video interpolation eg masking middle frames imagetovideo generation eg masking second frame onward video expansion eg masking half frames efficient design allocates computational resources lowresolution planning model making computationally expensive important spatiotemporal attention feasible scale mardini sets new stateoftheart video interpolation meanwhile within inference steps efficiently generates videos par much expensive advanced imagetovideo models,2,0.6860564220557998,2,0.6860564220557998
matchdiffusion trainingfree generation matchcuts matchcuts powerful cinematic tools create seamless transitions scenes delivering strong visual metaphorical connections however crafting matchcuts challenging resourceintensive process requiring deliberate artistic planning matchdiffusion present first trainingfree method matchcut generation using texttovideo diffusion models matchdiffusion leverages key property diffusion models early denoising steps define scenes broad structure later steps add details guided insight matchdiffusion employs joint diffusion initialize generation two prompts shared noise aligning structure motion applies disjoint diffusion allowing videos diverge introduce unique details approach produces visually coherent videos suited matchcuts user studies metrics demonstrate matchdiffusions effectiveness potential democratize matchcut creation,-1,0.0,-1,0.0
endora video generation models endoscopy simulators generative models hold promise revolutionizing medical education robotassisted surgery data augmentation machine learning despite progress generating medical images complex domain clinical video generation largely remained untappedthis paper introduces model innovative approach generate medical videos simulate clinical endoscopy scenes present novel generative model design integrates meticulously crafted spatialtemporal video transformer advanced vision foundation model priors explicitly modeling spatialtemporal dynamics video generation also pioneer first public benchmark endoscopy simulation video generation models adapting existing stateoftheart methods endeavorendora demonstrates exceptional visual quality generating endoscopy videos surpassing stateoftheart methods extensive testing moreover explore endoscopy simulator empower downstream video analysis tasks even generate medical scenes multiview consistency nutshell endora marks notable breakthrough deployment generative ai clinical endoscopy research setting substantial stage advances medical content generation details please visit project page httpsendoramedvidgengithubio,-1,0.0,-1,0.0
draw audio leveraging multiinstruction videotoaudio synthesis foley term commonly used filmmaking referring addition daily sound effects silent films videos enhance auditory experience videotoaudio particular type automatic foley task presents inherent challenges related audiovisual synchronization challenges encompass maintaining content consistency input video generated audio well alignment temporal loudness properties within video address issues construct controllable videotoaudio synthesis model termed draw audio supports multiple input instructions drawn masks loudness signals ensure content consistency synthesized audio target video introduce maskattention module mam employs masked video instruction enable model focus regions interest additionally implement timeloudness module tlm uses auxiliary loudness signal ensure synthesis sound aligns video loudness temporal dimensions furthermore extended largescale dataset named vggsoundcaption annotating caption prompts extensive experiments challenging benchmarks across two largescale datasets verify draw audio achieves stateoftheart project page httpsyannqigithubiodrawanaudio,8,1.0,8,1.0
sora agi world model complete survey texttovideo generation evolution video generation text starting animating mnist numbers simulating physical world sora progressed breakneck speed past seven years often seen superficial expansion predecessor texttoimage generation model texttovideo generation models developed upon carefully engineered constituents systematically discuss elements consisting limited core building blocks vision language temporal supporting features perspective contributions achieving world model employ prisma framework curate impactful research articles renowned scientific databases primarily studying video synthesis using text conditions upon minute exploration manuscripts observe texttovideo generation involves intricate technologies beyond plain extension texttoimage generation additional review shortcomings soragenerated videos pinpoints call indepth studies various enabling aspects video generation dataset evaluation metric efficient architecture humancontrolled generation finally conclude study texttovideo generation may still infancy requiring contribution crossdiscipline research community towards advancement first step realize artificial general intelligence agi,-1,0.0,-1,0.0
video summarization using denoising diffusion probabilistic model video summarization aims eliminate visual redundancy retaining key parts video construct concise comprehensive synopses existing methods use discriminative models predict importance scores video frames however methods susceptible annotation inconsistency caused inherent subjectivity different annotators annotating video paper introduce generative framework video summarization learns generate summaries probability distribution perspective effectively reducing interference subjective annotation noise specifically propose novel diffusion summarization method based denoising diffusion probabilistic model ddpm learns probability distribution training data noise prediction generates summaries iterative denoising method resistant subjective annotation noise less prone overfitting training data discriminative methods strong generalization ability moreover facilitate training ddpm limited data employ unsupervised video summarization model implement earlier denoising process extensive experiments various datasets tvsum summe fpvsum demonstrate effectiveness method,-1,0.0,-1,0.0
generating videos unposed internet photos address problem generating videos unposed internet photos handful input images serve keyframes model interpolates simulate path moving cameras given random images models ability capture underlying geometry recognize scene identity relate frames terms camera position orientation reflects fundamental understanding structure scene layout however existing video models luma dream machine fail task design selfsupervised method takes advantage consistency videos variability multiview internet photos train scalable video model without annotations camera parameters validate method outperforms baselines terms geometric appearance consistency also show model benefits applications enable camera control gaussian splatting results suggest scale scenelevel learning using data videos multiview internet photos,-1,0.0,-1,0.0
textanimator controllable visual text video generation video generation challenging yet pivotal task various industries gaming ecommerce advertising one significant unresolved aspect within effective visualization text within generated videos despite progress achieved generation current methods still effectively visualize texts videos directly mainly focus summarizing semantic scene information understanding depicting actions recent advances imagelevel visual text generation show promise transitioning techniques video domain faces problems notably preserving textual fidelity motion coherence paper propose innovative approach termed textanimator visual text video generation textanimator contains text embedding injection module precisely depict structures visual text generated videos besides develop camera control module text refinement module improve stability generated visual text controlling camera movement well motion visualized text quantitative qualitative experimental results demonstrate superiority approach accuracy generated visual text stateoftheart video generation methods project page found httpslaulampaulgithubiotextanimatorhtml,-1,0.0,-1,0.0
uvcg leveraging temporal consistency universal video protection security risks aidriven video editing garnered significant attention although recent studies indicate adding perturbations images protect malicious edits directly applying imagebased methods perturb frame video becomes ineffective video editing techniques leverage consistency interframe information restore individually perturbed content address challenge leverage temporal consistency video content propose straightforward efficient yet highly effective broadly applicable approach universal video consistency guard uvcg uvcg embeds content another videotarget video within protected video introducing continuous imperceptible perturbations ability force encoder editing models map continuous inputs misaligned continuous outputs thereby inhibiting generation videos consistent intended textual prompts additionally leveraging similarity perturbations adjacent frames improve computational efficiency perturbation generation employing perturbationreuse strategy applied uvcg across various versions latent diffusion models ldm assessed effectiveness generalizability across multiple ldmbased editing pipelines results confirm effectiveness transferability efficiency approach safeguarding video content unauthorized modifications,9,0.6937557504478232,9,0.6937557504478232
towards better metric texttovideo generation generative models demonstrated remarkable capability synthesizing highquality text images videos video generation contemporary texttovideo models exhibit impressive capabilities crafting visually stunning videos nonetheless evaluating videos poses significant challenges current research predominantly employs automated metrics fvd clip score however metrics provide incomplete analysis particularly temporal assessment video content thus rendering unreliable indicators true video quality furthermore user studies potential reflect human perception accurately hampered timeintensive laborious nature outcomes often tainted subjective bias paper investigate limitations inherent existing metrics introduce novel evaluation pipeline texttovideo score metric integrates two pivotal criteria textvideo alignment scrutinizes fidelity video representing given text description video quality evaluates videos overall production caliber mixture experts moreover evaluate proposed metrics facilitate future improvements present tvge dataset collecting human judgements texttovideo generated videos two criteria experiments tvge dataset demonstrate superiority proposed offering better metric texttovideo generation,-1,0.0,-1,0.0
videogpt integrating image video encoders enhanced video understanding building advances language models large multimodal models lmms contributed significant improvements video understanding current video lmms utilize advanced large language models llms rely either image video encoders process visual inputs limitations image encoders excel capturing rich spatial details frame sequences lack explicit temporal context important videos intricate action sequences hand video encoders provide temporal context often limited computational constraints lead processing sparse frames lower resolutions resulting reduced contextual spatial understanding end introduce videogpt combines complementary benefits image encoder detailed spatial understanding video encoder global temporal context modeling model processes videos dividing smaller segments applies adaptive pooling strategy features extracted image video encoders architecture showcases improved performance across multiple video benchmarks including vcgbench mvbench zeroshot questionanswering develop videoinstruction set using novel semiautomatic annotation pipeline improves model performance additionally comprehensively evaluate video lmms present vcgbenchdiverse covering broad video categories lifestyle sports science gaming surveillance videos benchmark questionanswer pairs evaluates generalization existing lmms dense video captioning spatial temporal understanding complex reasoning ensuring comprehensive assessment across diverse video types dynamics code httpsgithubcommbzuaioryxvideogptplus,0,1.0,0,1.0
motionaura generating highquality motion consistent videos using discrete diffusion spatiotemporal complexity video data presents significant challenges tasks compression generation inpainting present four key contributions address challenges spatiotemporal video processing first introduce mobile inverted vectorquantization variational autoencoder combines variational autoencoders vaes masked token modeling enhance spatiotemporal video compression model achieves superior temporal consistency stateoftheart sota reconstruction quality employing novel training strategy full frame masking second present motionaura texttovideo generation framework utilizes vectorquantized diffusion models discretize latent space capture complex motion dynamics producing temporally coherent videos aligned text prompts third propose spectral transformerbased denoising network processes video data frequency domain using fourier transform method effectively captures global context longrange dependencies highquality video generation denoising lastly introduce downstream task sketch guided video inpainting task leverages lowrank adaptation lora parameterefficient finetuning models achieve sota performance range benchmarks work offers robust frameworks spatiotemporal modeling userdriven video content manipulation release code datasets models opensource,-1,0.0,-1,0.0
towards understanding unsafe video generation video generation models vgms demonstrated capability synthesize highquality output important understand potential produce unsafe content violent terrifying videos work provide comprehensive understanding unsafe video generation first confirm possibility models could indeed generate unsafe videos choose unsafe content generation prompts collected lexica three opensource sota vgms generate unsafe videos filtering duplicates poorly generated content created initial set unsafe videos original pool videos clustering thematic coding analysis generated videos identify unsafe video categories distortedweird terrifying pornographic violentbloody political irb approval recruit online participants help label generated videos based annotations submitted participants identified unsafe videos initial video set labeled information corresponding prompts created first dataset unsafe videos generated vgms study possible defense mechanisms prevent generation unsafe videos existing defense methods image generation focus filtering either input prompt output results propose new approach called latent variable defense lvd works within models internal sampling process lvd achieve defense accuracy reducing time computing resources sampling large number unsafe prompts,4,1.0,4,1.0
easycontrol transfer controlnet video diffusion controllable generation interpolation following advancements textguided image generation technology exemplified stable diffusion video generation gaining increased attention academic community however relying solely text guidance video generation serious limitations videos contain much richer content images especially terms motion information hardly adequately described plain text fortunately computer vision various visual representations serve additional control signals guide generation help signals video generation controlled finer detail allowing greater flexibility different applications integrating various controls however nontrivial paper propose universal framework called easycontrol propagating injecting condition features condition adapters method enables users control video generation single condition map framework various conditions including raw pixels depth hed etc integrated different unetbased pretrained video diffusion models low practical cost conduct comprehensive experiments public datasets quantitative qualitative results indicate method outperforms stateoftheart methods easycontrol significantly improves various evaluation metrics across multiple validation datasets compared previous works specifically sketchtovideo generation task easycontrol achieves improvement fvd respectively compared videocomposer fidelity model demonstrates powerful image retention ability resulting high fvd msrvtt compared imagetovideo models,-1,0.0,-1,0.0
dreamitate realworld visuomotor policy learning via video generation key challenge manipulation learning policy robustly generalize diverse visual environments promising mechanism learning robust policies leverage video generative models pretrained largescale datasets internet videos paper propose visuomotor policy learning framework finetunes video diffusion model human demonstrations given task test time generate example execution task conditioned images novel scene use synthesized execution directly control robot key insight using common tools allows us effortlessly bridge embodiment gap human hand robot manipulator evaluate approach four tasks increasing complexity demonstrate harnessing internetscale generative models allows learned policy achieve significantly higher degree generalization existing behavior cloning approaches,5,1.0,5,1.0
zeroshot image conditioning texttovideo diffusion models textconditioned imagetovideo generation aims synthesize realistic video starting given image eg womans photo text description eg woman drinking water existing frameworks often require costly training videotext datasets specific model designs text image conditioning paper propose zeroshot tuningfree method empowers pretrained texttovideo diffusion model conditioned provided image enabling generation without optimization finetuning introducing external modules approach leverages pretrained diffusion foundation model generative prior guide video generation additional image input propose repeatandslide strategy modulates reverse denoising process allowing frozen diffusion model synthesize video framebyframe starting provided image ensure temporal continuity employ ddpm inversion strategy initialize gaussian noise newly synthesized frame resampling technique help preserve visual details conduct comprehensive experiments domainspecific opendomain datasets consistently outperforms recent opendomain model furthermore show seamlessly extend tasks video infilling prediction provided images autoregressive design also supports long video generation,-1,0.0,-1,0.0
aicl action incontext learning video diffusion model opendomain video generation models constrained scale training video datasets less common actions still generated researchers explore video editing methods achieve action generation editing spatial information action video however method mechanically generates identical actions without understanding align characteristics opendomain scenarios paper propose aicl empowers generative model ability understand action information reference videos similar humans incontext learning extensive experiments demonstrate aicl effectively captures action achieves stateoftheart generation performance across three typical video diffusion models five metrics using randomly selected categories nontraining datasets,-1,0.0,-1,0.0
magictime timelapse video generation models metamorphic simulators recent advances texttovideo generation achieved remarkable success synthesizing highquality general videos textual descriptions largely overlooked problem existing models adequately encoded physical knowledge real world thus generated videos tend limited motion poor variations paper propose textbfmagictime metamorphic timelapse video generation model learns realworld physics knowledge timelapse videos implements metamorphic generation first design magicadapter scheme decouple spatial temporal training encode physical knowledge metamorphic videos transform pretrained models generate metamorphic videos second introduce dynamic frames extraction strategy adapt metamorphic timelapse videos wider variation range cover dramatic object metamorphic processes thus embodying physical knowledge general videos finally introduce magic textencoder improve understanding metamorphic video prompts furthermore create timelapse videotext dataset called textbfchronomagic specifically curated unlock metamorphic video generation ability extensive experiments demonstrate superiority effectiveness magictime generating highquality dynamic metamorphic videos suggesting timelapse video generation promising path toward building metamorphic simulators physical world,-1,0.0,-1,0.0
semantically consistent video inpainting conditional diffusion models current stateoftheart methods video inpainting typically rely optical flow attentionbased approaches inpaint masked regions propagating visual information across frames approaches led significant progress standard benchmarks struggle tasks require synthesis novel content present frames paper reframe video inpainting conditional generative modeling problem present framework solving problems conditional video diffusion models introduce inpaintingspecific sampling schemes capture crucial longrange dependencies context devise novel method conditioning known pixels incomplete frames highlight advantages using generative approach task showing method capable generating diverse highquality inpaintings synthesizing new content spatially temporally semantically consistent provided context,-1,0.0,-1,0.0
videoelevator elevating video generation quality versatile texttoimage diffusion models texttoimage diffusion models demonstrated unprecedented capabilities creating realistic aesthetic images contrary texttovideo diffusion models still lag far behind frame quality text alignment owing insufficient quality quantity training videos paper introduce videoelevator trainingfree plugandplay method elevates performance using superior capabilities different conventional sampling ie temporal spatial modeling videoelevator explicitly decomposes sampling step temporal motion refining spatial quality elevating specifically temporal motion refining uses encapsulated enhance temporal consistency followed inverting noise distribution required spatial quality elevating harnesses inflated directly predict less noisy latent adding photorealistic details conducted experiments extensive prompts combination various results show videoelevator improves performance baselines foundational also facilitates stylistic video synthesis personalized code available httpsgithubcomybybzhangvideoelevator,11,1.0,11,1.0
mofavideo controllable image animation via generative motion field adaptions frozen imagetovideo diffusion model present mofavideo advanced controllable image animation method generates video given image using various additional controllable signals human landmarks reference manual trajectories another even provided video combinations different previous methods work specific motion domain show weak control abilities diffusion prior achieve goal design several domainaware motion field adapters ie mofaadapters control generated motions video generation pipeline mofaadapters consider temporal motion consistency video generate dense motion flow given sparse control conditions first multiscale features given image wrapped guided feature stable video diffusion generation naively train two motion adapters manual trajectories human landmarks individually since contain sparse information control training mofaadapters different domains also work together controllable video generation project page,-1,0.0,-1,0.0
loong generating minutelevel long videos autoregressive language models desirable challenging generate contentrich long videos scale minutes autoregressive large language models llms achieved great success generating coherent long sequences tokens domain natural language processing exploration autoregressive llms video generation limited generating short videos several seconds work conduct deep analysis challenges prevent autoregressive llmbased video generators generating long videos based observations analysis propose loong new autoregressive llmbased video generator generate minutelong videos specifically model text tokens video tokens unified sequence autoregressive llms train model scratch propose progressive shorttolong training loss reweighting scheme mitigate loss imbalance problem long video training investigate inference strategies including video token reencoding sampling strategies diminish error accumulation inference proposed loong trained videos extended generate minutelevel long videos conditioned text prompts demonstrated results samples available httpsepiphqnygithubioloongvideo,0,1.0,0,1.0
generate scene evaluating improving texttovision generation scene graph programming dalle sora gained attention producing implausible images astronauts riding horse space despite proliferation texttovision models inundated internet synthetic visuals images assets current benchmarks predominantly evaluate models realworld scenes paired captions introduce generate scene framework systematically enumerates scene graphs representing vast array visual scenes spanning realistic imaginative compositions generate scene leverages scene graph programming method dynamically constructing scene graphs varying complexity structured taxonomy visual elements taxonomy includes numerous objects attributes relations enabling synthesis almost infinite variety scene graphs using structured representations generate scene translates scene graph caption enabling scalable evaluation texttovision models standard metrics conduct extensive evaluations across multiple texttoimage texttovideo models presenting key findings model performance find ditbackbone texttoimage models align closely input captions unetbackbone models texttovideo models struggle balancing dynamics consistency texttovideo models show notable gaps human preference alignment demonstrate effectiveness generate scene conducting three practical applications leveraging captions generated generate scene selfimproving framework models iteratively enhance performance using generated data distillation process transfer specific strengths proprietary models opensource counterparts improvements content moderation identifying generating challenging synthetic data,-1,0.0,-1,0.0
omnidrag enabling motion control omnidirectional imagetovideo generation virtual reality gains popularity demand controllable creation immersive dynamic omnidirectional videos odvs increasing previous texttoodv generation methods achieve impressive results struggle content inaccuracies inconsistencies due reliance solely textual inputs although recent motion control techniques provide finegrained control video generation directly applying methods odvs often results spatial distortion unsatisfactory performance especially complex spherical motions tackle challenges propose omnidrag first approach enabling scene objectlevel motion control accurate highquality omnidirectional imagetovideo generation building pretrained video diffusion models introduce omnidirectional control module jointly finetuned temporal attention layers effectively handle complex spherical motion addition develop novel spherical motion estimator accurately extracts motioncontrol signals allows users perform dragstyle odv generation simply drawing handle target points also present new dataset named addressing scarcity odv data large scene object motions experiments demonstrate significant superiority omnidrag achieving holistic scenelevel finegrained objectlevel control odv generation project page available,1,1.0,1,1.0
multiscale temporal map diffusion models natural language video localization natural language video localization nlvl grounding phrases natural language descriptions corresponding video segments complex yet critical task video understanding despite ongoing advancements many existing solutions lack capability globally capture temporal dynamics video data study present novel approach nlvl aims address issue method involves direct generation global temporal map via conditional denoising diffusion process based input video language query main challenges inherent sparsity discontinuity temporal map devising diffusion decoder address challenges introduce multiscale technique develop innovative diffusion decoder approach effectively encapsulates interaction query video data across various time scales experiments charades didemo datasets underscore potency design,-1,0.0,-1,0.0
largescale videoaction dataset egocentric video generation video generation emerged promising tool world simulation leveraging visual data replicate realworld environments within context egocentric video generation centers human perspective holds significant potential enhancing applications virtual reality augmented reality gaming however generation egocentric videos presents substantial challenges due dynamic nature egocentric viewpoints intricate diversity actions complex variety scenes encountered existing datasets inadequate addressing challenges effectively bridge gap present first highquality dataset specifically curated egocentric video generation encompasses million egocentric video clips enriched detailed action annotations including finegrained kinematic control highlevel textual descriptions ensure integrity usability dataset implement sophisticated data cleaning pipeline designed maintain frame consistency action coherence motion smoothness egocentric conditions furthermore introduce egodreamer capable generating egocentric videos driven simultaneously action descriptions kinematic control signals dataset associated action annotations data cleansing metadata released advancement research egocentric video generation,-1,0.0,-1,0.0
multimodal fusion coherence modeling video topic segmentation video topic segmentation vts task segments videos intelligible nonoverlapping topics facilitating efficient comprehension video content quick access specific content vts also critical various downstream video understanding tasks traditional vts methods using shallow features unsupervised approaches struggle accurately discern nuances topical transitions recently supervised approaches achieved superior performance video action scene segmentation unsupervised approaches work improve supervised vts thoroughly exploring multimodal fusion multimodal coherence modeling specifically enhance multimodal fusion exploring different architectures using crossattention mixture experts generally strengthen multimodality alignment fusion pretrain finetune model multimodal contrastive learning propose new pretraining task tailored vts task novel finetuning task enhancing multimodal coherence modeling vts evaluate proposed approaches educational videos form lectures due vital role topic segmentation educational videos boosting learning experiences additionally introduce largescale chinese lecture video dataset augment existing english corpus promoting research vts experiments english chinese lecture datasets demonstrate model achieves superior vts performance compared competitive unsupervised supervised baselines,-1,0.0,-1,0.0
driveeditor unified informationguided framework controllable object editing driving scenes visioncentric autonomous driving systems require diverse data robust training evaluation augmented manipulating object positions appearances within existing scene captures recent advancements diffusion models shown promise video editing application object manipulation driving scenarios remains challenging due imprecise positional control difficulties preserving highfidelity object appearances address challenges position appearance control introduce driveeditor diffusionbased framework object editing driving videos driveeditor offers unified framework comprehensive object editing operations including repositioning replacement deletion insertion diverse manipulations achieved shared set varying inputs processed identical position control appearance maintenance modules position control module projects given bounding box preserving depth information hierarchically injects diffusion process enabling precise control object position orientation appearance maintenance module preserves consistent attributes single reference image employing threetiered approach lowlevel detail preservation highlevel semantic maintenance integration priors novel view synthesis model extensive qualitative quantitative evaluations nuscenes dataset demonstrate driveeditors exceptional fidelity controllability generating diverse driving scene edits well remarkable ability facilitate downstream tasks project page httpsyvanlianggithubiodriveeditor,11,1.0,11,1.0
latentreframe enabling camera control video diffusion model without training precise camera pose control crucial video generation diffusion models existing methods require finetuning additional datasets containing paired videos camera pose annotations dataintensive computationally costly disrupt pretrained model distribution introduce latentreframe enables camera control pretrained video diffusion model without finetuning unlike existing methods latentreframe operates sampling stage maintaining efficiency preserving original model distribution approach reframes latent code video frames align input camera trajectory timeaware point clouds latent code inpainting harmonization refine model latent space ensuring highquality video generation experimental results demonstrate latentreframe achieves comparable superior camera control precision video quality trainingbased methods without need finetuning additional datasets,-1,0.0,-1,0.0
mygo consistent controllable multiview driving video generation camera control highquality driving video generation crucial providing training data autonomous driving models however current generative models rarely focus enhancing camera motion control multiview tasks essential driving video generation therefore propose mygo endtoend framework video generation introducing motion onboard cameras conditions make progress camera controllability multiview consistency mygo employs additional plugin modules inject camera parameters pretrained video diffusion model retains extensive knowledge pretrained model much possible furthermore use epipolar constraints neighbor view information generation process view enhance spatialtemporal consistency experimental results show mygo achieved stateoftheart results general cameracontrolled video generation multiview driving video generation tasks lays foundation accurate environment simulation autonomous driving project page,-1,0.0,-1,0.0
avid adapting video diffusion models world models largescale generative models achieved remarkable success number domains however sequential decisionmaking problems robotics actionlabelled data often scarce therefore scalingup foundation models decisionmaking remains challenge potential solution lies leveraging widelyavailable unlabelled videos train world models simulate consequences actions world model accurate used optimize decisionmaking downstream tasks imagetovideo diffusion models already capable generating highly realistic synthetic videos however models actionconditioned powerful models closedsource means finetuned work propose adapt pretrained video diffusion models actionconditioned world models without access parameters pretrained model approach avid trains adapter small domainspecific dataset actionlabelled videos avid uses learned mask modify intermediate outputs pretrained model generate accurate actionconditioned videos evaluate avid video game realworld robotics data show outperforms existing baselines diffusion model results demonstrate utilized correctly pretrained video models potential powerful tools embodied ai,-1,0.0,-1,0.0
cameractrl enabling camera control texttovideo generation controllability plays crucial role video generation allows users create edit content precisely existing models however lack control camera pose serves cinematic language express deeper narrative nuances alleviate issue introduce cameractrl enabling accurate camera pose control video diffusion models approach explores effective camera trajectory parameterization along plugandplay camera pose control module trained top video diffusion model leaving modules base model untouched moreover comprehensive study effect various training datasets conducted suggesting videos diverse camera distributions similar appearance base model indeed enhance controllability generalization experimental results demonstrate effectiveness cameractrl achieving precise camera control different video generation models marking step forward pursuit dynamic customized video storytelling textual camera pose inputs,9,0.7468407821116368,9,0.7468407821116368
depthcrafter generating consistent long depth sequences openworld videos estimating video depth openworld scenarios challenging due diversity videos appearance content motion camera movement length present depthcrafter innovative method generating temporally consistent long depth sequences intricate details openworld videos without requiring supplementary information camera poses optical flow generalization ability openworld videos achieved training videotodepth model pretrained imagetovideo diffusion model meticulously designed threestage training strategy training approach enables model generate depth sequences variable lengths one time frames harvest precise depth details rich content diversity realistic synthetic datasets also propose inference strategy process extremely long videos segmentwise estimation seamless stitching comprehensive evaluations multiple datasets reveal depthcrafter achieves stateoftheart performance openworld video depth estimation zeroshot settings furthermore depthcrafter facilitates various downstream applications including depthbased visual effects conditional video generation,1,0.9672655896901609,1,0.9672655896901609
videotoaudio generation semantic temporal alignment visual auditory perception two crucial ways humans experience world texttovideo generation made remarkable progress past year absence harmonious audio generated video limits broader applications paper propose semantic temporal aligned videotoaudio approach enhances audio generation videos extracting local temporal global semantic video features combining refined video features text crossmodal guidance address issue information redundancy videos propose onset prediction pretext task local temporal feature extraction attentive pooling module global semantic feature extraction supplement insufficient semantic information videos propose latent diffusion model texttoaudio priors initialization crossmodal guidance also introduce audioaudio align new metric assess audiotemporal alignment subjective objective metrics demonstrate method surpasses existing videotoaudio models generating audio better quality semantic consistency temporal alignment ablation experiment validated effectiveness module audio samples available,8,0.6670673136431505,8,0.6670673136431505
adaptive caching faster video generation diffusion transformers generating temporallyconsistent highfidelity videos computationally expensive especially longer temporal spans morerecent diffusion transformers dits despite making significant headway context heightened challenges rely larger models heavier attention mechanisms resulting slower inference speeds paper introduce trainingfree method accelerate video dits termed adaptive caching adacache motivated fact videos created equal meaning videos require fewer denoising steps attain reasonable quality others building cache computations diffusion process also devise caching schedule tailored video generation maximizing qualitylatency tradeoff introduce motion regularization moreg scheme utilize video information within adacache essentially controlling compute allocation based motion content altogether plugandplay contributions grant significant inference speedups eg opensora video generation without sacrificing generation quality across multiple video dit baselines,20,1.0,20,1.0
needle video haystack scalable synthetic evaluator video mllms video understanding crucial next step multimodal large language models mllms various benchmarks introduced better evaluating mllms nevertheless current video benchmarks still inefficient evaluating video models iterative development due high cost constructing datasets difficulty isolating specific skills paper propose videoniah video needle haystack benchmark construction framework synthetic video generation videoniah decouples video content queryresponses inserting unrelated visual needles original videos framework automates generation queryresponse pairs using predefined rules minimizing manual labor queries focus specific aspects video understanding enabling skillspecific evaluations separation video content queries also allow increased video variety evaluations across different lengths utilizing videoniah compile video benchmark vnbench includes tasks retrieval ordering counting evaluate three key aspects video understanding temporal perception chronological ordering spatiotemporal coherence conduct comprehensive evaluation proprietary opensource models uncovering significant differences video understanding capabilities across various tasks additionally perform indepth analysis test results model configurations based findings provide advice improving video mllm training offering valuable insights guide future research model development code data available,0,0.9572662478212324,0,0.9572662478212324
zeroshot video editing adaptive sliding score distillation rapidly evolving field texttovideo generation catalyzed renewed interest controllable video editing research application editing prompts guide diffusion model denoising gained prominence mirroring advancements image editing noisebased inference process inherently compromises original videos integrity resulting unintended overediting temporal discontinuities address challenges study proposes novel paradigm videobased score distillation facilitating direct manipulation original video content specifically distinguishing imagebased score distillation propose adaptive sliding score distillation strategy incorporates global local video guidance reduce impact editing errors combined proposed imagebased joint guidance mechanism ability mitigate inherent instability model singlestep sampling additionally design weighted attention fusion module preserve key features original video avoid overediting extensive experiments demonstrate strategies effectively address existing challenges achieving superior performance compared current stateoftheart methods,-1,0.0,-1,0.0
dive taming dino subjectdriven video editing building success diffusion models image generation editing video editing recently gained substantial attention however maintaining temporal consistency motion alignment still remains challenging address issues paper proposes dinoguided video editing dive framework designed facilitate subjectdriven editing source videos conditioned either target text prompts reference images specific identities core dive lies leveraging powerful semantic features extracted pretrained model implicit correspondences guide editing process specifically ensure temporal motion consistency dive employs dino features align motion trajectory source video extensive experiments diverse realworld videos demonstrate framework achieve highquality editing results robust motion consistency highlighting potential dino contribute video editing precise subject editing dive incorporates dino features reference images pretrained texttoimage model learn lowrank adaptations loras effectively registering target subjects identity project page httpsdinovideoeditinggithubio,-1,0.0,-1,0.0
compositional video generation llm director significant progress made texttovideo generation use powerful generative models largescale internet data however substantial challenges remain precisely controlling individual concepts within generated video motion appearance specific characters movement viewpoints work propose novel paradigm generates concept representation separately composes priors large language models llm diffusion models specifically given input textual prompt scheme consists three stages leverage llm director first decompose complex query several subprompts indicate individual concepts within videotextiteg scene objects motions let llm invoke pretrained expert models obtain corresponding representations concepts compose representations prompt multimodal llm produce coarse guidance scales coordinates trajectories objects make generated frames adhere natural image distribution leverage diffusion priors use score distillation sampling refine composition extensive experiments demonstrate method generate highfidelity videos text diverse motion flexible control concept project page,-1,0.0,-1,0.0
learning multimodal forgery representation diffusion generated video detection large numbers synthesized videos diffusion models pose threats information security authenticity leading increasing demand generated content detection however existing videolevel detection algorithms primarily focus detecting facial forgeries often fail identify diffusiongenerated content diverse range semantics advance field video forensics propose innovative algorithm named multimodal detectionmmdet detecting diffusiongenerated videos mmdet utilizes profound perceptual comprehensive abilities large multimodal models lmms generating multimodal forgery representation mmfr lmms multimodal space enhancing ability detect unseen forgery content besides mmdet leverages inandacross frame attention iafa mechanism feature augmentation spatiotemporal domain dynamic fusion strategy helps refine forgery representations fusion moreover construct comprehensive diffusion video dataset called diffusion video forensics dvf across wide range forgery videos mmdet achieves stateoftheart performance dvf demonstrating effectiveness algorithm source code dvf available httpsgithubcomsparklexfantasymmdet,4,1.0,4,1.0
novel view extrapolation video diffusion priors field novel view synthesis made significant strides thanks development radiance field methods however radiance field techniques far better novel view interpolation novel view extrapolation synthesis novel views far beyond observed training views design viewextrapolator novel view synthesis approach leverages generative priors stable video diffusion svd realistic novel view extrapolation redesigning svd denoising process viewextrapolator refines artifactprone views rendered radiance fields greatly enhancing clarity realism synthesized novel views viewextrapolator generic novel view extrapolator work different types rendering views rendered point clouds single view monocular video available additionally viewextrapolator requires finetuning svd making dataefficient computationefficient extensive experiments demonstrate superiority viewextrapolator novel view extrapolation project page urlhttpskunhaoliugithubioviewextrapolator,1,1.0,1,1.0
motrans customized motion transfer textdriven video diffusion models existing pretrained texttovideo models demonstrated impressive abilities generating realistic videos basic motion camera movement however models exhibit significant limitations generating intricate humancentric motions current efforts primarily focus finetuning models small set videos containing specific motion often fail effectively decouple motion appearance limited reference videos thereby weakening modeling capability motion patterns end propose motrans customized motion transfer method enabling video generation similar motion new context specifically introduce multimodal large language model mllmbased recaptioner expand initial prompt focus appearance appearance injection module adapt appearance prior video frames motion modeling process complementary multimodal representations recaptioned prompt video frames promote modeling appearance facilitate decoupling appearance motion addition devise motionspecific embedding enhancing modeling specific motion experimental results demonstrate method effectively learns specific motion pattern singular multiple reference videos performing favorably existing methods customized video generation,-1,0.0,-1,0.0
vgtvp multimodal procedural planning via visually grounded textvideo prompting large language model llmbased agents shown promise procedural tasks potential multimodal instructions augmented texts videos assist users remains underexplored address gap propose visually grounded textvideo prompting vgtvp method novel llmempowered multimodal procedural planning mpp framework generates cohesive text video procedural plans given specified highlevel objective main challenges achieving textual visual informativeness temporal coherence accuracy procedural plans vgtvp leverages zeroshot reasoning capability llms videototext generation ability video captioning models texttovideo generation ability diffusion models vgtvp improves interaction modalities proposing novel fusion captioning foc method using texttovideo bridge videototext bridge allow llms guide generation visuallygrounded text plans textualgrounded video plans address scarcity datasets suitable mpp curated new dataset called dailylife task procedural plans dailypp conduct comprehensive experiments benchmarks evaluate human preferences regarding textual visual informativeness temporal coherence plan accuracy vgtvp method outperforms unimodal baselines dailypp dataset,-1,0.0,-1,0.0
syncflow toward temporally aligned joint audiovideo generation text video audio closely correlated modalities humans naturally perceive together recent advancements enabled generation audio video text producing modalities simultaneously still typically relies either cascaded process multimodal contrastive encoders approaches however often lead suboptimal results due inherent information losses inference conditioning paper introduce syncflow system capable simultaneously generating temporally synchronized audio video text core syncflow proposed dualdiffusiontransformer ddit architecture enables joint video audio modelling proper information fusion efficiently manage computational cost joint audio video modelling syncflow utilizes multistage training strategy separates video audio learning joint finetuning empirical evaluations demonstrate syncflow produces audio video outputs correlated baseline methods significantly enhanced audio quality audiovisual correspondence moreover demonstrate strong zeroshot capabilities syncflow including zeroshot videotoaudio generation adaptation novel video resolutions without training,8,0.5322777230037121,8,0.5322777230037121
enhancing visual consistency imagetovideo generation imagetovideo generation aims use initial frame alongside text prompt create video sequence grand challenge generation maintain visual consistency throughout video existing methods often struggle preserve integrity subject background style first frame well ensure fluid logical progression within video narrative mitigate issues propose diffusionbased method enhance visual consistency generation specifically introduce spatiotemporal attention first frame maintain spatial motion consistency noise initialization lowfrequency band first frame enhance layout consistency two approaches enable generate highly consistent videos also extend proposed approaches show potential improve consistency autoregressive long video generation camera motion control verify effectiveness method propose comprehensive evaluation benchmark generation automatic human evaluation results demonstrate superiority existing methods,9,0.7757653141570444,9,0.7757653141570444
controllable video compression multimodal generative models traditional neural video codecs commonly encounter limitations controllability generality ultralowbitrate coding scenarios overcome challenges propose controllable video compression framework incorporating multimodal generative models framework utilizes semanticmotion composite strategy keyframe selection retain critical information keyframe corresponding video clip dialoguebased large multimodal model lmm approach extracts hierarchical spatiotemporal details enabling interframe intraframe representations improved video fidelity enhancing encoding interpretability employs conditional diffusionbased textguided keyframe compression method achieving high fidelity frame reconstruction decoding textual descriptions derived lmms guide diffusion process restore original videos content accurately experimental results demonstrate significantly outperforms stateoftheart vvc standard ultralow bitrate scenarios particularly preserving semantic perceptual fidelity,2,1.0,2,1.0
repurposing pretrained video diffusion models eventbased video interpolation video frame interpolation aims recover realistic missing frames observed frames generating highframerate video lowframerate video however without additional guidance large motion frames makes problem illposed eventbased video frame interpolation evfi addresses challenge using sparse hightemporalresolution event measurements motion guidance guidance allows evfi methods significantly outperform frameonly methods however date evfi methods relied limited set paired eventframe training data severely limiting performance generalization capabilities work overcome limited data challenge adapting pretrained video diffusion models trained internetscale datasets evfi experimentally validate approach realworld evfi datasets including new one introduce method outperforms existing methods generalizes across cameras far better existing approaches,-1,0.0,-1,0.0
put shoes lifting egocentric perspective exocentric videos investigate exocentrictoegocentric crossview translation aims generate firstperson egocentric view actor based video recording captures actor thirdperson exocentric perspective end propose generative framework called decouples translation process two stages highlevel structure transformation explicitly encourages crossview correspondence exocentric egocentric views diffusionbased pixellevel hallucination incorporates hand layout prior enhance fidelity generated egocentric view pave way future advancements field curate comprehensive exotoego crossview translation benchmark consists diverse collection synchronized egoexo tabletop activity video pairs sourced three public datasets aria pilot experimental results validate delivers photorealistic video results clear hand manipulation details outperforms several baselines terms synthesis quality generalization ability new actions,-1,0.0,-1,0.0
temporally consistent object editing videos using extended attention image generation editing seen great deal advancements rise largescale diffusion models allow user control different modalities text mask depth maps etc however controlled editing videos still lags behind prior work area focused using diffusion models globally change style existing video hand many practical applications editing localized parts video critical work propose method edit videos using pretrained inpainting image diffusion model systematically redesign forward path model replacing selfattention modules extended version attention modules creates framelevel dependencies way ensure edited information consistent across video frames matter shape position masked area qualitatively compare results stateoftheart terms accuracy several video editing tasks like object retargeting object replacement object removal tasks simulations demonstrate superior performance proposed strategy,9,0.9876049190970271,9,0.9876049190970271
lingen towards highresolution minutelength texttovideo generation linear computational complexity texttovideo generation enhances content creation highly computationally intensive computational cost diffusion transformers dits scales quadratically number pixels makes minutelength video generation extremely expensive limiting existing models generating videos seconds length propose linearcomplexity texttovideo generation lingen framework whose cost scales linearly number pixels first time lingen enables highresolution minutelength video generation single gpu without compromising quality replaces computationallydominant quadraticcomplexity block selfattention linearcomplexity block called mate consists mabranch tebranch mabranch targets shorttolongrange correlations combining bidirectional block token rearrangement method rotary major scan review tokens developed long video generation tebranch novel temporal swin attention block focuses temporal correlations adjacent tokens mediumrange tokens mate block addresses adjacency preservation issue mamba improves consistency generated videos significantly experimental results show lingen outperforms dit win rate video quality flops latency reduction furthermore automatic metrics human evaluation demonstrate yields comparable video quality stateoftheart models win rate respect lumalabs kling respectively paves way hourlength movie generation realtime interactive video generation provide video generation results examples project website httpslineargengithubio,-1,0.0,-1,0.0
ssm meets video diffusion models efficient longterm video generation structured state spaces given remarkable achievements image generation diffusion models research community shown increasing interest extending models video generation recent diffusion models video generation predominantly utilized attention layers extract temporal features however attention layers limited computational costs increase quadratically sequence length limitation presents significant challenges generating longer video sequences using diffusion models overcome challenge propose leveraging statespace models ssms temporal feature extractors ssms eg mamba recently gained attention promising alternatives due lineartime memory consumption relative sequence length line previous research suggesting using bidirectional ssms effective understanding spatial features image generation found bidirectionality also beneficial capturing temporal features video data rather relying traditional unidirectional ssms conducted comprehensive evaluations multiple longterm video datasets minerl navigate across various model sizes sequences frames ssmbased models require less memory achieve fvd attentionbased models moreover ssmbased models often deliver better performance comparable gpu memory usage codes available,-1,0.0,-1,0.0
frchet video motion distance metric evaluating motion consistency videos significant advancements made video generative models recently unlike image generation video generation presents greater challenges requiring generating highquality frames also ensuring temporal consistency across frames despite impressive progress research metrics evaluating quality generated videos especially concerning temporal motion consistency remains underexplored bridge research gap propose frechet video motion distance fvmd metric focuses evaluating motion consistency video generation specifically design explicit motion features based key point tracking measure similarity features via frechet distance conduct sensitivity analysis injecting noise real videos verify effectiveness fvmd carry largescale human study demonstrating metric effectively detects temporal noise aligns better human perceptions generated video quality existing metrics additionally motion features consistently improve performance video quality assessment vqa models indicating approach also applicable unary video quality evaluation code available,-1,0.0,-1,0.0
genrec unifying video generation recognition diffusion models video diffusion models able generate highquality videos learning strong spatialtemporal priors largescale datasets paper aim investigate whether priors derived generative process suitable video recognition eventually joint optimization generation recognition building upon stable video diffusion introduce genrec first unified framework trained randomframe conditioning process learn generalized spatialtemporal representations resulting framework naturally supports generation recognition importantly robust even visual inputs contain limited information extensive experiments demonstrate efficacy genrec recognition generation particular genrec achieves competitive recognition performance offering accuracy respectively genrec also performs best classconditioned imagetovideo generation achieving fvd scores datasets furthermore genrec demonstrates extraordinary robustness scenarios limited frames observed code available,-1,0.0,-1,0.0
shape motion reconstruction single video monocular dynamic reconstruction challenging longstanding vision problem due highly illposed nature task existing approaches limited either depend templates effective quasistatic scenes fail model motion explicitly work introduce method capable reconstructing generic dynamic scenes featuring explicit fullsequencelong motion casually captured monocular videos tackle underconstrained nature problem two key insights first exploit lowdimensional structure motion representing scene motion compact set motion bases points motion expressed linear combination bases facilitating soft decomposition scene multiple rigidlymoving groups second utilize comprehensive set datadriven priors including monocular depth maps longrange tracks devise method effectively consolidate noisy supervisory signals resulting globally consistent representation dynamic scene experiments show method achieves stateoftheart performance longrange motion estimation novel view synthesis dynamic scenes project page httpsshapeofmotiongithubio,1,1.0,1,1.0
akira augmentation kit rays optical video generation recent advances textconditioned video diffusion greatly improved video quality however methods offer limited sometimes control users camera aspects including dynamic camera motion zoom distorted lens focus shifts motion optical aspects crucial adding controllability cinematic elements generation frameworks ultimately resulting visual content draws focus enhances mood guides emotions according filmmakers controls paper aim close gap controllable video generation camera optics achieve propose akira augmentation kit rays novel augmentation framework builds trains camera adapter complex camera model existing video generation backbone enables finetuned control camera motion well complex optical parameters focal length distortion aperture achieve cinematic effects zoom fisheye effect bokeh extensive experiments demonstrate akiras effectiveness combining composing camera optics outperforming stateoftheart methods work sets new landmark controlled optically enhanced video generation paving way future optical video generation methods,-1,0.0,-1,0.0
slowfastvgen slowfast learning actiondriven long video generation human beings endowed complementary learning system bridges slow learning general world dynamics fast storage episodic memory new experience previous video generation models however primarily focus slow learning pretraining vast amounts data overlooking fast learning phase crucial episodic memory storage oversight leads inconsistencies across temporally distant frames generating longer videos frames fall beyond models context window end introduce slowfastvgen novel dualspeed learning system actiondriven long video generation approach incorporates masked conditional video diffusion model slow learning world dynamics alongside inferencetime fast learning strategy based temporal lora module specifically fast learning process updates temporal lora parameters based local inputs outputs thereby efficiently storing episodic memory parameters propose slowfast learning loop algorithm seamlessly integrates inner fast learning loop outer slow learning loop enabling recall prior multiepisode experiences contextaware skill learning facilitate slow learning approximate world model collect largescale dataset videos language action annotations covering wide range scenarios extensive experiments show slowfastvgen outperforms baselines across various metrics actiondriven video generation achieving fvd score compared maintaining consistency longer videos average scene cuts versus slowfast learning loop algorithm significantly enhances performances longhorizon planning tasks well project website httpsslowfastvgengithubio,-1,0.0,-1,0.0
soaf scene occlusionaware neural acoustic field paper tackles problem novel view audiovisual synthesis along arbitrary trajectory indoor scene given audiovideo recordings known trajectories scene existing methods often overlook effect room geometry particularly wall occlusion sound propagation making less accurate multiroom environments work propose new approach called scene occlusionaware acoustic field soaf accurate sound generation approach derives prior sound energy field using distanceaware parametric soundpropagation modelling transforms based scene transmittance learned input video extract features local acoustic field centred around receiver using fibonacci sphere generate binaural audio novel views directionaware attention mechanism extensive experiments real dataset rwavs synthetic dataset soundspaces demonstrate method outperforms previous stateoftheart techniques audio generation project page httpsgithubcomhuiyugaosoaf,8,0.5937276359547481,8,0.5937276359547481
lifting single image gaussians video generation priors singleimage reconstruction remains fundamental challenge computer vision due inherent geometric ambiguities limited viewpoint information recent advances latent video diffusion models lvdms offer promising priors learned largescale video data however leveraging priors effectively faces three key challenges degradation quality across large camera motions difficulties achieving precise camera control geometric distortions inherent diffusion process damage consistency address challenges proposing framework effectively releases lvdms generative priors ensuring consistency specifically design articulated trajectory strategy generate video frames decomposes video sequences large camera motions ones controllable small motions use robust neural matching models ie calibrate camera poses generated frames produce corresponding point clouds finally propose distortionaware gaussian splatting representation learn independent distortions frames output undistorted canonical gaussians extensive experiments demonstrate achieves stateoftheart performance two challenging datasets ie llff tanks temples generalizes well diverse inthewild images cartoon illustrations complex realworld scenes,1,1.0,1,1.0
timeseries initialization conditioning videoagnostic stabilization video superresolution using recurrent networks recurrent neural network rnn video super resolution vsr generally trained randomly clipped cropped short videos extracted original training videos due various challenges learning rnns however since rnn optimized superresolve short videos vsr long videos degraded due domain gap preliminary experiments reveal degradation changes depending video properties video length dynamics avoid degradation paper proposes training strategy rnn vsr work efficiently stably independently video length dynamics proposed training strategy stabilizes vsr training vsr network various rnn hidden states changed depending video properties since computing variety hidden states timeconsuming computational cost reduced reusing hidden states efficient training addition training stability improved framenumber conditioning experimental results demonstrate proposed method performed better base methods videos various lengths dynamics,2,0.7034049191334812,2,0.7034049191334812
towards holistic languagevideo representation language modelenhanced msrvideo text dataset robust holistic languagevideo representation key pushing video understanding forward despite improvement training strategies quality languagevideo dataset less attention current plain simple text descriptions visualonly focus languagevideo tasks result limited capacity realworld natural language video retrieval tasks queries much complex paper introduces method automatically enhance videolanguage datasets making modality contextaware sophisticated representation learning needs hence helping downstream tasks multifaceted video captioning method captures entities actions speech transcripts aesthetics emotional cues providing detailed correlating information text side video side training also develop agentlike strategy using language models generate highquality factual textual descriptions reducing human intervention enabling scalability methods effectiveness improving languagevideo representation evaluated textvideo retrieval using msrvtt dataset several multimodal retrieval models,0,1.0,0,1.0
egocvr egocentric benchmark finegrained composed video retrieval composed video retrieval video textual description modifies video content provided inputs model aim retrieve relevant video modified content database videos challenging task first step acquire largescale training datasets collect highquality benchmarks evaluation work introduce egocvr new evaluation benchmark finegrained composed video retrieval using largescale egocentric video datasets egocvr consists queries specifically focus highquality temporal video understanding find existing composed video retrieval frameworks achieve necessary highquality temporal video understanding task address shortcoming adapt simple trainingfree method propose generic reranking framework composed video retrieval demonstrate achieves strong results egocvr code benchmark freely available httpsgithubcomexplainablemlegocvr,-1,0.0,-1,0.0
videoorion tokenizing object dynamics videos present videoorion video large language model videollm explicitly captures key semantic information videos spatialtemporal dynamics objects throughout videos videoorion employs expert vision models extract object dynamics detectsegmenttrack pipeline encoding set object tokens aggregating spatialtemporal object features method addresses persistent challenge videollms efficiently compressing highdimensional video data semantic tokens comprehensible llms compared prior methods resort downsampling original video aggregating visual tokens using resamplers leading information loss entangled semantics videoorion offers natural efficient way derive compact disentangled semantic representations also enables explicit object modeling video content minimal computational cost moreover introduced object tokens naturally allow videoorion accomplish videobased referring tasks experimental results show videoorion learn make good use object tokens achieves competitive results general video question answering videobased referring benchmarks,0,0.8353329187441559,0,0.8353329187441559
abductive egoview accident video understanding safe driving perception present mmau novel dataset multimodal accident video understanding mmau contains inthewild egoview accident videos temporally aligned text descriptions annotate million object boxes pairs videobased accident reasons covering accident categories mmau supports various accident understanding tasks particularly multimodal video diffusion understand accident causeeffect chains safe driving mmau present abductive accident video understanding framework safe driving perception adversasd adversasd performs video diffusion via objectcentric video diffusion oavd method driven abductive clip model model involves contrastive interaction loss learn pair cooccurrence normal nearaccident accident frames corresponding text descriptions accident reasons prevention advice accident categories oavd enforces causal region learning fixing content original frame background video generation find dominant causeeffect chain certain accidents extensive experiments verify abductive ability adversasd superiority oavd stateoftheart diffusion models additionally provide careful benchmark evaluations object detection accident reason answering since adversasd relies precise object accident reason information,-1,0.0,-1,0.0
cardiff video salient object ranking chain thought reasoning saliency prediction diffusion video saliency prediction aims identify regions video attract human attention gaze driven bottomup features video topdown processes like memory cognition among topdown influences language plays crucial role guiding attention shaping visual information interpreted existing methods primarily focus modeling perceptual information neglecting reasoning process facilitated language ranking cues crucial outcomes process practical guidance saliency prediction paper propose cardiff caption rank generate diffusion framework imitates process integrating multimodal large language model mllm grounding module diffusion model enhance video saliency prediction specifically introduce novel prompting method vsorcot video salient object ranking chain thought utilizes mllm grounding module caption video content infer salient objects along rankings positions process derives ranking maps sufficiently leveraged diffusion model decode saliency maps given video accurately extensive experiments show effectiveness vsorcot improving performance video saliency prediction proposed cardiff performs better stateoftheart models mvs dataset demonstrates crossdataset capabilities dataset zeroshot evaluation,-1,0.0,-1,0.0
collaboratively selfsupervised video representation learning action recognition considering close connection action recognition human pose estimation design collaboratively selfsupervised video representation csvr learning framework specific action recognition jointly factoring generative pose prediction discriminative context matching pretext tasks specifically csvr consists three branches generative pose prediction branch discriminative context matching branch video generating branch among first one encodes dynamic motion feature utilizing conditionalgan predict human poses future frames second branch extracts static context features contrasting positive negative video feature iframe feature pairs third branch designed generate current future video frames purpose collaboratively improving dynamic motion features static context features extensive experiments demonstrate method achieves stateoftheart performance multiple popular video datasets,-1,0.0,-1,0.0
multistage highaesthetic video generation growing demand highfidelity video generation textual descriptions catalyzed significant research field work introduce integrates texttoimage model video motion generator reference image embedding module frame interpolation module endtoend video generation pipeline benefiting architecture designs generate aesthetically pleasing highresolution video remarkable fidelity smoothness demonstrates superior performance leading texttovideo systems runway pika morph moon valley stable video diffusion model via user evaluation large scale,11,0.9235240508025361,11,0.9235240508025361
savgbench benchmarking spatially aligned audiovideo generation work addresses lack multimodal generative models capable producing highquality videos spatially aligned audio recent advancements generative models successful video generation often overlook spatial alignment audio visuals essential immersive experiences tackle problem establish new research direction benchmarking spatially aligned audiovideo generation savg propose three key components benchmark dataset baseline metrics introduce spatially aligned audiovisual dataset derived audiovisual dataset consisting multichannel audio video spatiotemporal annotations sound events propose baseline audiovisual diffusion model focused stereo audiovisual joint learning accommodate spatial sound finally present metrics evaluate video spatial audio quality including new spatial audiovisual alignment metric experimental result demonstrates gaps exist baseline model ground truth terms video audio quality spatial alignment modalities,8,0.6400162087457085,8,0.6400162087457085
highfrequency enhanced hybrid neural representation video compression neural representations videos nerv simplified video codec process achieved swift decoding speeds encoding video content neural network presenting promising solution video compression however existing work overlooks crucial issue videos reconstructed methods lack highfrequency details address problem paper introduces highfrequency enhanced hybrid neural representation network method focuses leveraging highfrequency information improve synthesis fine details network specifically design wavelet highfrequency encoder incorporates wavelet frequency decomposer wfd blocks generate highfrequency feature embeddings next design highfrequency feature modulation hfm block leverages extracted highfrequency embeddings enhance fitting process decoder finally refined harmonic decoder block dynamic weighted frequency loss reduce potential loss highfrequency information experiments bunny uvg datasets demonstrate method outperforms methods showing notable improvements detail preservation compression performance,2,0.8762362064427304,2,0.8762362064427304
humanvbench exploring humancentric video understanding capabilities mllms synthetic benchmark data domain multimodal large language models mllms achieving humancentric video understanding remains formidable challenge existing benchmarks primarily emphasize object action recognition often neglecting intricate nuances human emotions behaviors speechvisual alignment within video content present humanvbench innovative benchmark meticulously crafted bridge gaps evaluation video mllms humanvbench comprises carefully designed tasks explore two primary dimensions inner emotion outer manifestations spanning static dynamic basic complex well singlemodal crossmodal aspects two advanced automated pipelines video annotation distractorincluded qa generation humanvbench utilizes diverse stateoftheart sota techniques streamline benchmark data synthesis quality assessment minimizing human annotation dependency tailored humancentric multimodal attributes comprehensive evaluation across sota video mllms reveals notable limitations current performance especially crossmodal emotion perception underscoring necessity refinement toward achieving humanlike understanding humanvbench opensourced facilitate future advancements realworld applications video mllms,0,1.0,0,1.0
depth video scalable synthetic data video depth estimation long hindered scarcity consistent scalable ground truth data leading inconsistent unreliable results paper introduce depth video model tackles challenge two key innovations first develop scalable synthetic data pipeline capturing realtime video depth data diverse virtual environments yielding video clips duration precise depth annotations second leverage powerful priors generative video diffusion models handle realworld videos effectively integrating advanced techniques rotary position encoding flow matching enhance flexibility efficiency unlike previous models limited fixedlength video sequences approach introduces novel mixedduration training strategy handles videos varying lengths performs robustly across different frame rateseven single frames inference propose depth interpolation method enables model infer highresolution video depth across sequences frames model outperforms previous generative depth models terms spatial accuracy temporal consistency code model weights opensourced,1,1.0,1,1.0
blended latent diffusion attention control realworld video editing due lack fully publicly available texttovideo models current video editing methods tend build pretrained texttoimage generation models however still face grand challenges dealing local editing video temporal information first although existing methods attempt focus local area editing predefined mask preservation outsidearea background nonideal due spatially entire generation frame addition specially providing mask user additional costly undertaking autonomous masking strategy integrated editing process desirable last least imagelevel pretrained model hasnt learned temporal information across frames video vital expressing motion dynamics paper propose adapt imagelevel blended latent diffusion model perform local video editing tasks specifically leverage ddim inversion acquire latents background latents instead randomly noised ones better preserve background information input video introduce autonomous mask manufacture mechanism derived crossattention maps diffusion steps finally enhance temporal consistency across video frames transforming selfattention blocks unet temporalspatial blocks extensive experiments proposed approach demonstrates effectiveness different realworld video editing tasks,9,0.8335025404703441,9,0.8335025404703441
learning see dazzle machine vision susceptible laser dazzle intense laser light blind distort perception environment oversaturation permanent damage sensor pixels employ wavefrontcoded phase mask diffuse energy laser light introduce sandwich generative adversarial network sgan restore images complex image degradations varying laserinduced image saturation maskinduced image blurring unknown lighting conditions various noise corruptions sgan architecture combines discriminative generative methods wrapping two gans around learnable image deconvolution module addition make use fourier feature representations reduce spectral bias neural networks improve learning highfrequency image details endtoend training includes realistic physicsbased synthesis large set training data publicly available images trained sgan suppress peak laser irradiance high times sensor saturation threshold point camera sensors may experience damage without mask trained model evaluated synthetic data set data collected laboratory proposed image restoration model quantitatively qualitatively outperforms stateoftheart methods wide range scene contents laser powers incident laser angles ambient illumination strengths noise characteristics,-1,0.0,-1,0.0
vgmshield mitigating misuse video generative models rapid advancement video generation people conveniently utilize video generation models create videos tailored specific desires nevertheless also growing concerns potential misuse creating disseminating false information work introduce vgmshield set three straightforward pioneering mitigations lifecycle fake video generation start textitfake video detection trying understand whether uniqueness generated videos whether differentiate real videos investigate textittracing problem maps fake video back model generates towards propose leverage pretrained models focus spatialtemporal dynamics backbone identify inconsistencies videos experiments seven stateoftheart opensource models demonstrate current models still perfectly handle spatialtemporal relationships thus accomplish detection tracing nearly perfect accuracy furthermore anticipating future generative model improvements propose prevention method adds invisible perturbations images make generated videos look unreal together fake video detection tracing multifaceted set solutions effectively mitigate misuse video generative models,4,0.9064551183707601,4,0.9064551183707601
vmas videotomusic generation via semantic alignment web music videos present framework learning generate background music video inputs unlike existing works rely symbolic musical annotations limited quantity diversity method leverages largescale web videos accompanied background music enables model learn generate realistic diverse music accomplish goal develop generative videomusic transformer novel semantic videomusic alignment scheme model uses joint autoregressive contrastive learning objective encourages generation music aligned highlevel video content also introduce novel videobeat alignment scheme match generated music beats lowlevel motions video lastly capture finegrained visual cues video needed realistic background music generation introduce new temporal video encoder architecture allowing us efficiently process videos consisting many densely sampled frames train framework newly curated discomv dataset consisting videomusic samples orders magnitude larger prior datasets used video music generation method outperforms existing approaches discomv musiccaps datasets according various music generation evaluation metrics including human evaluation results available,8,0.4972001295940391,8,0.4972001295940391
largescale highquality multiracial human face video dataset generating talking face videos various conditions recently become highly popular research area within generative tasks however building highquality face video generation model requires wellperforming pretrained backbone key obstacle universal models fail adequately address existing works rely universal video image generation models optimize control mechanisms neglect evident upper bound video quality due limited capabilities backbones result lack highquality human face video datasets work investigate unsatisfactory results related studies gather trim existing public talking face video datasets additionally collect annotate largescale dataset resulting comprehensive highquality multiracial face collection named using dataset craft several effective pretrained backbone models face video generation specifically conduct experiments several wellestablished video generation models including texttovideo imagetovideo unconditional video generation various settings obtain corresponding performance benchmarks compared trained public datasets demonstrate superiority dataset experiments also allow us investigate empirical strategies crafting domainspecific video generation tasks costeffective settings make curated dataset along pretrained talking face video generation models publicly available resource contribution hopefully advance research field,-1,0.0,-1,0.0
decof generated video detection via frame consistency first benchmark dataset escalating quality video generated advanced video generation methods results new security challenges relevant research efforts opensource dataset generated video detection generated video detection method proposed far end propose opensource dataset detection method generated video first time first propose scalable dataset consisting prompts covering various forgery targets scenes behaviors actions well various generation models different architectures generation methods including popular commercial models like openais sora googles veo second found via probing experiments spatial artifactbased detectors lack generalizability hence propose simple yet effective textbfdetection model based textbfframe textbfconsistency textbfdecof focuses temporal artifacts eliminating impact spatial artifacts feature learning extensive experiments demonstrate efficacy decof detecting videos generated unseen video generation models confirm powerful generalizability across several commercially proprietary models code dataset released urlhttpsgithubcomwuwuwuyuedecof,4,1.0,4,1.0
image secretly last frame pseudo video diffusion models viewed special case hierarchical variational autoencoders hvaes shown profound success generating photorealistic images contrast standard hvaes often produce images inferior quality compared diffusion models paper hypothesize success diffusion models partly attributed additional selfsupervision information intermediate latent states provided corrupted images along original image form pseudo video based hypothesis explore possibility improving types generative models pseudo videos specifically first extend given image generative model video generative model counterpart train video generative model pseudo videos constructed applying data augmentation original images furthermore analyze potential issues firstorder markov data augmentation methods typically used diffusion models propose use expressive data augmentation construct useful information pseudo videos empirical results celeba datasets demonstrate improved image generation quality achieved additional selfsupervised information pseudo videos,-1,0.0,-1,0.0
raformer redundancyaware transformer video wire inpainting video wire inpainting vwi prominent application video inpainting aimed flawlessly removing wires films tv series offering significant time labor savings compared manual framebyframe removal however wire removal poses greater challenges due wires longer slimmer objects typically targeted general video inpainting tasks often intersecting people background objects irregularly adds complexity inpainting process recognizing limitations posed existing video wire datasets characterized small size poor quality limited variety scenes introduce new vwi dataset novel mask generation strategy namely wire removal video dataset pseudo wireshaped pws masks dataset comprises videos average length frames designed facilitate development efficacy inpainting models building upon research proposes redundancyaware transformer raformer method addresses unique challenges wire removal video inpainting unlike conventional approaches indiscriminately process frame patches raformer employs novel strategy selectively bypass redundant parts static background segments devoid valuable information inpainting core raformer redundancyaware attention raa module isolates accentuates essential content coarsegrained windowbased attention mechanism complemented soft feature alignment sfa module refines features achieves endtoend feature alignment extensive experiments traditional video inpainting datasets proposed dataset demonstrate raformer outperforms stateoftheart methods,-1,0.0,-1,0.0
interest summaries queryfocused long video summarization generating concise informative video summary long video important yet subjective due varying scene importance users ability specify scene importance text queries enhances relevance summaries paper introduces approach queryfocused video summarization aiming align video summaries closely user queries end propose fully convolutional sequence network attention fcsnaqfvs novel approach designed task leveraging temporal convolutional attention mechanisms model effectively extracts highlights relevant content based userspecified queries experimental validation benchmark dataset queryfocused video summarization demonstrates effectiveness approach,0,1.0,0,1.0
turbsegres segmentthenrestore pipeline dynamic videos atmospheric turbulence tackling image degradation due atmospheric turbulence particularly dynamic environment remains challenge longrange imaging systems existing techniques primarily designed static scenes scenes small motion paper presents first segmentthenrestore pipeline restoring videos dynamic scenes turbulent environment leverage mean optical flow unsupervised motion segmentation method separate dynamic static scene components prior restoration camera shake compensation segmentation introduce foregroundbackground enhancement leveraging statistics turbulence strength transformer model trained novel noisebased procedural turbulence generator fast dataset augmentation benchmarked existing restoration methods approach restores geometric distortion enhances sharpness videos make code simulator data publicly available advance field video restoration turbulence riponcsgithubioturbsegres,-1,0.0,-1,0.0
semisupervised video semantic segmentation using unreliable pseudo labels pixellevel scene understanding one fundamental problems computer vision aims recognizing object classes masks semantics pixel given image compared image scene parsing video scene parsing introduces temporal information effectively improve consistency accuracy predictionbecause realworld actually videobased rather static state paper adopt semisupervised video semantic segmentation method based unreliable pseudo labels ensemble teacher network model student network model generate pseudo labels retrain student network method achieves miou scores development test final test respectively finally obtain place video scene parsing wild challenge cvpr,-1,0.0,-1,0.0
oneshot learning meets depth diffusion multiobject videos creating editable videos depict complex interactions multiple objects various artistic styles long challenging task filmmaking progress often hampered scarcity data sets contain paired text descriptions corresponding videos showcase interactions paper introduces novel depthconditioning approach significantly advances field enabling generation coherent diverse videos single textvideo pair using pretrained depthaware texttoimage model method finetunes pretrained model capture continuous motion employing customdesigned spatial temporal attention mechanisms inference use ddim inversion provide structural guidance video generation innovative technique allows continuously controllable depth videos facilitating generation multiobject interactions maintaining concept generation compositional strengths original model across various artistic styles photorealism animation impressionism,1,1.0,1,1.0
trajectory attention finegrained video motion control recent advancements video generation greatly driven video diffusion models camera motion control emerging crucial challenge creating viewcustomized visual content paper introduces trajectory attention novel approach performs attention along available pixel trajectories finegrained camera motion control unlike existing methods often yield imprecise outputs neglect temporal correlations approach possesses stronger inductive bias seamlessly injects trajectory information video generation process importantly approach models trajectory attention auxiliary branch alongside traditional temporal attention design enables original temporal attention trajectory attention work synergy ensuring precise motion control new content generation capability critical trajectory partially available experiments camera motion control images videos demonstrate significant improvements precision longrange consistency maintaining highquality generation furthermore show approach extended video motion control tasks firstframeguided video editing excels maintaining content consistency large spatial temporal ranges,9,0.6781850815000784,9,0.6781850815000784
openhumanvid largescale highquality dataset enhancing humancentric video generation recent advancements visual generation technologies markedly increased scale availability video datasets crucial training effective video generation models however significant lack highquality humancentric video datasets presents challenge progress field bridge gap introduce openhumanvid largescale highquality humancentric video dataset characterized precise detailed captions encompass human appearance motion states along supplementary human motion conditions including skeleton sequences speech audio validate efficacy dataset associated training strategies propose extension existing classical diffusion transformer architectures conduct pretraining models proposed dataset findings yield two critical insights first incorporation largescale highquality dataset substantially enhances evaluation metrics generated human videos preserving performance general video generation tasks second effective alignment text human appearance human motion facial motion essential producing highquality video outputs based insights corresponding methodologies straightforward extended network trained proposed dataset demonstrates obvious improvement generation humancentric videos project page httpsfudangenerativevisiongithubioopenhumanvid,-1,0.0,-1,0.0
lost melody empirical observations texttovideo generation storytelling perspective texttovideo generation task witnessed notable progress generated outcomes reflecting text prompts high fidelity impressive visual qualities however current texttovideo generation models invariably focused conveying visual elements single scene far indifferent another important potential medium namely storytelling paper examine texttovideo generation storytelling perspective hardly investigated make empirical remarks spotlight limitations current texttovideo generation scheme also propose evaluation framework storytelling aspects videos discuss potential future directions,-1,0.0,-1,0.0
lifelong learning video diffusion models single video stream work demonstrates training autoregressive video diffusion models single continuous video stream possible remarkably also competitive standard offline training approaches given number gradient steps demonstration reveals main result achieved using experience replay retains subset preceding video stream also contribute three new single video generative modeling datasets suitable evaluating lifelong video model learning lifelong bouncing balls lifelong maze lifelong plaicraft dataset contains million consecutive frames synthetic environment increasing complexity,-1,0.0,-1,0.0
towards realistic driving simulation video generation model driving simulation essential developing realistic autonomous driving simulators despite advancements existing methods generating driving scenes significant challenges remain view transformation spatialtemporal dynamic modeling address limitations propose spatialtemporal simulation driving model reconstruct realworld scenes design controllable generative network achieve simulation constructs continuous point cloud scenes using surroundview data autonomous vehicles decouples spatialtemporal relationships produces coherent keyframe videos additionally leverages video generation models obtain photorealistic controllable driving simulation videos perspective expand range view generation train vehicle motion videos based decomposed camera poses enhancing modeling capabilities distant scenes furthermore reconstruct vehicle camera trajectories integrate points across consecutive views enabling comprehensive scene understanding along temporal dimension following extensive multilevel scene training simulate desired viewpoint achieve deep understanding scene evolution static spatialtemporal conditions compared existing methods approach shows promising performance multiview scene consistency background coherence accuracy contributes ongoing advancements realistic autonomous driving simulation code httpsgithubcomwzzhengstag,16,1.0,16,1.0
foundation models video understanding survey video foundation models vifms aim learn generalpurpose representation various video understanding tasks leveraging largescale datasets powerful models vifms achieve capturing robust generic features video data survey analyzes video foundational models offering comprehensive overview benchmarks evaluation metrics across distinct video tasks categorized main categories additionally offer indepth performance analysis models common video tasks categorize vifms three categories imagebased vifms adapt existing image models video tasks videobased vifms utilize videospecific encoding methods universal foundational models ufms combine multiple modalities image video audio text etc within single framework comparing performance various vifms different tasks survey offers valuable insights strengths weaknesses guiding future advancements video understanding analysis surprisingly reveals imagebased foundation models consistently outperform videobased models video understanding tasks additionally ufms leverage diverse modalities demonstrate superior performance video tasks share comprehensive list vifms studied work,0,0.9418695013869387,0,0.9418695013869387
motionmaster trainingfree camera motion transfer video generation emergence diffusion models greatly propelled progress image video generation recently efforts made controllable video generation including texttovideo generation video motion control among camera motion control important topic however existing camera motion control methods rely training temporal camera module necessitate substantial computation resources due large amount parameters video generation models moreover existing methods predefine camera motion types training limits flexibility camera control therefore reduce training costs achieve flexible camera control propose comd novel trainingfree video motion transfer model disentangles camera motions object motions source videos transfers extracted camera motions new videos first propose oneshot camera motion disentanglement method extract camera motion single source video separates moving objects background estimates camera motion moving objects region based motion background solving poisson equation furthermore propose fewshot camera motion disentanglement method extract common camera motion multiple videos similar camera motions employs windowbased clustering technique extract common features temporal attention maps multiple videos finally propose motion combination method combine different types camera motions together enabling model controllable flexible camera control extensive experiments demonstrate trainingfree approach effectively decouple cameraobject motion apply decoupled camera motion wide range controllable video generation tasks achieving flexible diverse camera motion control,-1,0.0,-1,0.0
largescale highquality dataset texttovideo generation texttovideo generation recently garnered significant attention thanks large multimodality model sora however generation still faces two important challenges lacking precise open sourced highquality dataset previous popular video datasets eg either low quality large research institutions therefore challenging crucial collect precise highquality textvideo pairs generation ignoring fully utilize textual information recent methods focused vision transformers using simple cross attention module video generation falls short thoroughly extracting semantic information text prompt address issues introduce precise highquality dataset expressive captions openscenario dataset contains million textvideo pairs facilitating research generation furthermore curate videos create advancing highdefinition video generation additionally propose novel multimodal video diffusion transformer mvdit capable mining structure information visual tokens semantic information text tokens extensive experiments ablation studies verify superiority previous datasets effectiveness mvdit,-1,0.0,-1,0.0
mind time temporallycontrolled multievent video generation realworld videos consist sequences events generating sequences precise temporal control infeasible existing video generators rely single paragraph text input tasked generating multiple events described using single prompt methods often ignore events fail arrange correct order address limitation present mint multievent video generator temporal control key insight bind event specific period generated video allows model focus one event time enable timeaware interactions event captions video tokens design timebased positional encoding method dubbed rerope encoding helps guide crossattention operation finetuning pretrained video diffusion transformer temporally grounded data approach produces coherent videos smoothly connected events first time literature model offers control timing events generated videos extensive experiments demonstrate mint outperforms existing commercial opensource models large margin,-1,0.0,-1,0.0
video generation replace cinematographers research cinematic language generated video recent advancements texttovideo generation leveraged diffusion models enhance visual coherence videos generated textual descriptions however research primarily focused object motion limited attention given cinematic language videos crucial cinematographers convey emotion narrative pacing address limitation propose threefold approach enhance ability models generate controllable cinematic language specifically introduce cinematic language dataset encompasses shot framing angle camera movement enabling models learn diverse cinematic styles building facilitate robust cinematic alignment evaluation present cameraclip model finetuned proposed dataset excels understanding complex cinematic language generated videos provide valuable guidance multishot composition process finally propose cliplora costguided dynamic lora composition method facilitates smooth transitions realistic blending cinematic language dynamically fusing multiple pretrained cinematic loras within single video experiments demonstrate cameraclip outperforms existing models assessing alignment cinematic language video achieving score additionally cliplora improves ability multishot composition potentially bridging gap automatically generated videos shot professional cinematographers,-1,0.0,-1,0.0
stereocrafter diffusionbased generation long highfidelity stereoscopic monocular videos paper presents novel framework converting videos immersive stereoscopic addressing growing demand content immersive experience leveraging foundation models priors approach overcomes limitations traditional methods boosts performance ensure highfidelity generation required display devices proposed system consists two main steps depthbased video splatting warping extracting occlusion mask stereo video inpainting utilize pretrained stable video diffusion backbone introduce finetuning protocol stereo video inpainting task handle input video varying lengths resolutions explore autoregressive strategies tiled processing finally sophisticated data processing pipeline developed reconstruct largescale highquality dataset support training framework demonstrates significant improvements video conversion offering practical solution creating immersive content devices like apple vision pro displays summary work contributes field presenting effective method generating highquality stereoscopic videos monocular input potentially transforming experience digital media,1,1.0,1,1.0
stream spatiotemporal evaluation analysis metric video generative models image generative models made significant progress generating realistic diverse images supported comprehensive guidance various evaluation metrics however current video generative models struggle generate even short video clips limited tools provide insights improvements current video evaluation metrics simple adaptations image metrics switching embeddings video embedding networks may underestimate unique characteristics video analysis reveals widely used frechet video distance fvd stronger emphasis spatial aspect temporal naturalness video inherently constrained input size embedding networks used limiting frames additionally demonstrates considerable instability diverges human evaluations address limitations propose stream new video evaluation metric uniquely designed independently evaluate spatial temporal aspects feature allows comprehensive analysis evaluation video generative models various perspectives unconstrained video length provide analytical experimental evidence demonstrating stream provides effective evaluation tool visual temporal quality videos offering insights area improvement video generative models best knowledge stream first evaluation metric separately assess temporal spatial aspects videos code available,-1,0.0,-1,0.0
dvos selfsupervised densepattern video object segmentation video object segmentation approaches primarily rely largescale pixelaccurate humanannotated datasets model development dense video object segmentation dvos scenarios video frame encompasses hundreds small dense partially occluded objects accordingly laborintensive manual annotation even single frame often takes hours hinders development dvos many applications furthermore videos dense patterns following large number objects move different directions poses additional challenges address challenges proposed semiselfsupervised spatiotemporal approach dvos utilizing diffusionbased method multitask learning emulating real videos optical flow simulating motion developed methodology synthesize computationally annotated videos used training dvos models model performance improved utilizing weakly labeled computationally generated imprecise data demonstrate utility efficacy proposed approach developed dvos models wheat head segmentation handheld dronecaptured videos capturing wheat crops fields different locations across various growth stages spanning heading maturity despite using manually annotated video frames proposed approach yielded highperforming models achieving dice score tested dronecaptured external test set showed efficacy proposed approach wheat head segmentation application extended crops dvos domains crowd analysis microscopic image analysis,-1,0.0,-1,0.0
freemask rethinking importance attention masks zeroshot video editing texttovideo diffusion models made remarkable advancements driven ability generate temporally coherent videos research zeroshot video editing using fundamental models expanded rapidly enhance editing quality structural controls frequently employed video editing among techniques crossattention mask control stands effectiveness efficiency however crossattention masks naively applied video editing introduce artifacts blurring flickering experiments uncover critical factor overlooked previous video editing research crossattention masks consistently clear vary model structure denoising timestep address issue propose metric mask matching cost mmc quantifies variability propose freemask method selecting optimal masks tailored specific video editing tasks using mmcselected masks improve masked fusion mechanism within comprehensive attention features eg temp cross selfattention modules approach seamlessly integrated existing zeroshot video editing frameworks better performance requiring control assistance parameter finetuning enabling adaptive decoupling unedited semantic layouts mask precision control extensive experiments demonstrate freemask achieves superior semantic fidelity temporal consistency editing quality compared stateoftheart methods,9,0.8820118618540811,9,0.8820118618540811
vividface diffusionbased hybrid framework highfidelity video face swapping video face swapping becoming increasingly popular across various applications yet existing methods primarily focus static images struggle video face swapping temporal consistency complex scenarios paper present first diffusionbased framework specifically designed video face swapping approach introduces novel imagevideo hybrid training framework leverages abundant static image data temporal video sequences addressing inherent limitations videoonly training framework incorporates specially designed diffusion model coupled vidfacevae effectively processes types data better maintain temporal coherence generated videos disentangle identity pose features construct attributeidentity disentanglement triplet aidt dataset triplet three face images two images sharing pose two sharing identity enhanced comprehensive occlusion augmentation dataset also improves robustness occlusions additionally integrate reconstruction techniques input conditioning network handling large pose variations extensive experiments demonstrate framework achieves superior performance identity preservation temporal consistency visual quality compared existing methods requiring fewer inference steps approach effectively mitigates key challenges video face swapping including temporal flickering identity preservation robustness occlusions pose variations,6,0.4268315432316747,6,0.4268315432316747
selfcalibrating novel view synthesis monocular videos using gaussian splatting gaussian splatting gs significantly elevated scene reconstruction efficiency novel view synthesis nvs accuracy compared neural radiance fields nerf particularly dynamic scenes however current nvs methods whether based gs nerf primarily rely camera parameters provided colmap even utilize sparse point clouds generated colmap initialization lack accuracy well timeconsuming sometimes results poor dynamic scene representation especially scenes large object movements extreme camera conditions eg small translations combined large rotations studies simultaneously optimize estimation camera parameters scenes supervised additional information like depth optical flow etc obtained offtheshelf models using unverified information ground truth reduce robustness accuracy frequently occur long monocular videos eg hundreds frames propose novel approach learns highfidelity gs scene representation selfcalibration camera parameters includes extraction point features robustly represent structure use subsequent joint optimization camera parameters structure towards overall scene optimization demonstrate accuracy time efficiency method extensive quantitative qualitative experimental results several standard benchmarks results show significant improvements stateoftheart methods novel view synthesis source code released soon,1,1.0,1,1.0
editboard towards comprehensive evaluation benchmark textbased video editing models rapid development diffusion models significantly advanced aigenerated content aigc particularly texttoimage texttovideo generation textbased video editing leveraging generative capabilities emerged promising field enabling precise modifications videos based text prompts despite proliferation innovative video editing models conspicuous lack comprehensive evaluation benchmarks holistically assess models performance across various dimensions existing evaluations limited inconsistent typically summarizing overall performance single score obscures models effectiveness individual editing tasks address gap propose editboard first comprehensive evaluation benchmark textbased video editing models editboard encompasses nine automatic metrics across four dimensions evaluating models four task categories introducing three new metrics assess fidelity taskoriented benchmark facilitates objective evaluation detailing model performance providing insights models strengths weaknesses opensourcing editboard aim standardize evaluation advance development robust video editing models,10,0.6205721243145245,10,0.6205721243145245
data collectionfree masked video modeling pretraining video transformers generally requires large amount data presenting significant challenges terms data collection costs concerns related privacy licensing inherent biases synthesizing data one promising ways solve issues yet pretraining solely synthetic data challenges paper introduce effective selfsupervised learning framework videos leverages readily available less costly static images specifically define pseudo motion generator pmg module recursively applies image transformations generate pseudomotion videos images pseudomotion videos leveraged masked video modeling approach applicable synthetic images well thus entirely freeing video pretraining data collection costs concerns real data experiments action recognition tasks demonstrate framework allows effective learning spatiotemporal features pseudomotion videos significantly improving existing methods also use static images partially outperforming using real synthetic videos results uncover fragments video transformers learn masked video modeling,-1,0.0,-1,0.0
virbo multimodal multilingual avatar video generation digital marketing widespread popularity internet celebrity marketing world short video production gradually become popular way presenting products information however traditional video production industry usually includes series procedures script writing video filming professional studio video clipping special effects rendering customized postprocessing forth mention multilingual videos accessible could speak multilingual languages complicated procedures usually needs professional team complete made short video production costly time money paper presents intelligent system supports automatic generation talking avatar videos namely virbo simply userspecified script virbo could use deep generative model generate target talking videos meanwhile system also supports multimodal inputs customize video specified face specified voice special effects system also integrated multilingual customization module supports generate multilingual talking avatar videos batch hundreds delicate templates creative special effects series user studies demo tests found virbo generate talking avatar videos maintained high quality videos professional team reducing entire production costs significantly intelligent system effectively promote video production industry facilitate internet marketing neglecting language barriers cost challenges,-1,0.0,-1,0.0
learning summarize videos generating captions rapid growth video data internet video summarization becoming important ai technology however due high labelling cost video summarization existing studies conducted smallscale datasets leading limited performance generalization capacity work introduce use dense video captions supervision signal train video summarization models motivated propose model learns summarize videos generating captions exploit dense video caption annotations weaklysupervised approach allows us train models largescale dense video caption datasets achieve better performance generalization capacity improve generalization capacity introduce clip strong visionlanguage model prior mechanism enhance learning important objects captions may ignore videos practice perform zeroshot video summarization finetuned groundtruth summary video caption target dataset examine performance weaklysupervised finetuning video captions propose two new datasets tvsumcaption summecaption derived two common video summarization datasets publicly released conduct extensive experiments results demonstrate method achieves significant improvements performance generalization capacity compared previous methods,0,1.0,0,1.0
learning actionable discrete diffusion policy via largescale actionless video pretraining learning generalist embodied agent capable completing multiple tasks poses challenges primarily stemming scarcity actionlabeled robotic datasets contrast vast amount human videos exist capturing intricate tasks interactions physical world promising prospects arise utilizing actionless human videos pretraining transferring knowledge facilitate robot policy learning limited robot demonstrations however remains challenge due domain gap humans robots moreover difficult extract useful information representing dynamic world human videos noisy multimodal data structure paper introduce novel framework tackle challenges leverages unified discrete diffusion combine generative pretraining human videos policy finetuning small number actionlabeled robot videos start compressing human robot videos unified video tokens pretraining stage employ discrete diffusion model maskandreplace diffusion strategy predict future video tokens latent space finetuning stage harness imagined future videos guide lowlevel action learning limited set robot data experiments demonstrate method generates highfidelity future videos planning enhances finetuned policies compared previous stateoftheart approaches superior performance project website available httpsvideodiffgithubio,5,0.5009134851114918,5,0.5009134851114918
moonshot towards controllable video generation editing multimodal conditions existing video diffusion models vdms limited mere text conditions thereby usually lacking control visual appearance geometry structure generated videos work presents moonshot new video generation model conditions simultaneously multimodal inputs image text model builts upon core module called multimodal video block mvb consists conventional spatialtemporal layers representing video features decoupled crossattention layer address image text inputs appearance conditioning addition carefully design model architecture optionally integrate pretrained image controlnet modules geometry visual conditions without needing extra training overhead opposed prior methods experiments show versatile multimodal conditioning mechanisms moonshot demonstrates significant improvement visual quality temporal consistency compared existing models addition model easily repurposed variety generative applications personalized video generation image animation video editing unveiling potential serve fundamental architecture controllable video generation models made public httpsgithubcomsalesforcelavis,-1,0.0,-1,0.0
tvg trainingfree transition video generation method diffusion models transition videos play crucial role media production enhancing flow coherence visual narratives traditional methods like morphing often lack artistic appeal require specialized skills limiting effectiveness recent advances diffusion modelbased video generation offer new possibilities creating transitions face challenges poor interframe relationship modeling abrupt content changes propose novel trainingfree transition video generation tvg approach using videolevel diffusion models addresses limitations without additional training method leverages gaussian process regression mathcalgpr model latent representations ensuring smooth dynamic transitions frames additionally introduce interpolationbased conditional controls frequencyaware bidirectional fusion fbif architecture enhance temporal control transition reliability evaluations benchmark datasets custom image pairs demonstrate effectiveness approach generating highquality smooth transition videos code provided httpssobeymilgithubiotvgcom,-1,0.0,-1,0.0
dreamphysics learning physicsbased dynamics video diffusion priors dynamic interaction attracting lot attention recently however creating content remains challenging one solution animate scenes physicsbased simulation requires manually assigning precise physical properties object simulated results would become unnatural another solution learn deformation objects distillation video generative models however tends produce videos small discontinuous motions due inappropriate extraction application physics priors work combine strengths complementing shortcomings two solutions propose learn physical properties material field video diffusion priors utilize physicsbased materialpointmethod mpm simulator generate content realistic motions particular propose motion distillation sampling emphasize video motion information distillation addition facilitate optimization propose kanbased material field frame boosting experimental results demonstrate method enjoys realistic motions stateofthearts,-1,0.0,-1,0.0
neural graph matching video retrieval largescale videodriven ecommerce rapid development short video industry traditional ecommerce encountered new paradigm videodriven ecommerce leverages attractive videos product showcases provides video item services users benefitting dynamic visualized introduction itemsvideodriven ecommerce shown huge potential stimulating consumer confidence promoting sales paper focus video retrieval task facing following challenges howto handle heterogeneities among users items videos mine complementarity items videos better user understanding paper first leverage dual graph model coexisting uservideo useritem interactions videodriven ecommerce innovatively reduce user preference understanding graph matching problem solve propose novel bilevel graph matching networkgmn mainly consists node preferencelevel graph matching given user nodelevel graph matching aims match videos items preferencelevel graph matching aims match multiple user preferences extracted videos items proposed gmn generate improve user embedding aggregating matched nodes preferences dual graph bilevel manner comprehensive experiments show superiority proposed gmn significant improvements stateoftheart approaches eg developed wellknown videodriven ecommerce platform serving hundreds millions users every day,-1,0.0,-1,0.0
youtube video analytics patient engagement evidence colonoscopy preparation videos videos effective way deliver contextualized justintime medical information patient education however video analysis topic identification retrieval extraction analysis medical information understandability patient perspective extremely challenging tasks study demonstrates data analysis pipeline utilizes methods retrieve medical information youtube videos preparing colonoscopy exam much maligned disliked procedure patients find challenging get adequately prepared first use youtube data api collect metadata desired videos select search keywords use google video intelligence api analyze texts frames objects data annotate youtube video materials medical information video understandability overall recommendation develop bidirectional long shortterm memory bilstm model identify medical terms videos build three classifiers group videos based levels encoded medical information video understandability whether videos recommended study provides healthcare stakeholders guidelines scalable approach generating new educational video content enhance management vast number health conditions,-1,0.0,-1,0.0
agentbased video trimming information becomes accessible usergenerated videos increasing length placing burden viewers sift vast content valuable insights trend underscores need algorithm extract key video information efficiently despite significant advancements highlight detection moment retrieval video summarization current approaches primarily focus selecting specific time intervals often overlooking relevance segments potential segment arranging paper introduce novel task called video trimming vt focuses detecting wasted footage selecting valuable segments composing final video coherent story address task propose agentbased video trimming avt structured three phases video structuring clip filtering story composition specifically employ video captioning agent convert video slices structured textual descriptions filtering module dynamically discard lowquality footage based structured information clip video arrangement agent select compile valid clips coherent final narrative evaluation develop video evaluation agent assess trimmed videos conducting assessments parallel human evaluations additionally curate new benchmark dataset video trimming using raw user videos internet result avt received favorable evaluations user studies demonstrated superior map precision youtube highlights tvsum dataset highlight detection task code models available httpsylingfenggithubioavt,-1,0.0,-1,0.0
annotated biomedical video generation using denoising diffusion probabilistic models flow fields segmentation tracking living cells play vital role within biomedical domain particularly cancer research drug development developmental biology usually tedious timeconsuming tasks traditionally done biomedical experts recently automatize processes deep learning based segmentation tracking methods proposed methods require largescale datasets full potential constrained scarcity annotated data biomedical imaging domain address limitation propose biomedical video diffusion model bvdm capable generating realisticlooking synthetic microscopy videos trained single real video bvdm generate videos arbitrary length pixellevel annotations used training datahungry models composed denoising diffusion probabilistic model ddpm generating highfidelity synthetic cell microscopy images flow prediction model fpm predicting nonrigid transformation consecutive video frames inference initially ddpm imposes realistic cell textures synthetic cell masks generated based real data statistics flow prediction model predicts flow field consecutive masks applies ddpm output previous time frame create next one keeping temporal consistency bvdm outperforms stateoftheart synthetic live cell microscopy video generation models furthermore demonstrate sufficiently large synthetic dataset enhances performance cell segmentation tracking models compared using limited amount available real data,-1,0.0,-1,0.0
improved video vae latent video diffusion model variational autoencoder vae aims compress pixel data lowdimensional latent space playing important role openais sora latent video diffusion generation models existing video vaes inflate pretrained image vae causal structure temporalspatial compression paper presents two astonishing findings initialization welltrained image vae latent dimensions suppresses improvement subsequent temporal compression capabilities adoption causal reasoning leads unequal information interactions unbalanced performance frames alleviate problems propose keyframebased temporal compression ktc architecture group causal convolution gcconv module improve video vae ivvae specifically ktc architecture divides latent space two branches one half completely inherits compression prior keyframes lowerdimension image vae half involves temporal compression group causal convolution reducing temporalspatial conflicts accelerating convergence speed video vae gcconv half uses standard convolution within frame group ensure interframe equivalence employs causal logical padding groups maintain flexibility processing variable frame video extensive experiments five benchmarks demonstrate sota video reconstruction generation capabilities proposed ivvae,2,0.7218790562814247,2,0.7218790562814247
sectorshaped diffusion models video generation diffusion models achieved great success image generation however leveraging idea video generation face significant challenges maintaining consistency continuity across video frames mainly caused lack effective framework align frames videos desired temporal features preserving consistent semantic stochastic features work propose novel sectorshaped diffusion model whose sectorshaped diffusion region formed set rayshaped reverse diffusion processes starting noise point generate group intrinsically related data sharing semantic stochastic features varying temporal features appropriate guided conditions apply video generation tasks explore use optical flow temporal conditions experimental results show outperforms many existing methods task video generation without temporalfeature modelling modules texttovideo generation tasks temporal conditions explicitly given propose twostage generation strategy decouple generation temporal features semanticcontent features show without additional training model integrated another temporal conditions generative model still achieve comparable performance existing works results viewd,-1,0.0,-1,0.0
mastering trajectory multientity motion video generation paper aims manipulate multientity motions video generation previous methods controllable video generation primarily leverage control signals manipulate object motions achieved remarkable synthesis results however control signals inherently limited expressing nature object motions overcome problem introduce robust controller regulates multientity dynamics space given userdesired pose location rotation sequences entities core approach plugandplay grounded object injector fuses multiple input entities respective trajectories gated selfattention mechanism addition exploit injector architecture preserve video diffusion prior crucial generalization ability mitigate video quality degradation introduce domain adaptor training employ annealed sampling strategy inference address lack suitable training data construct dataset first correlates collected human animal assets gptgenerated trajectory captures motion evenlysurround cameras diverse ue platforms extensive experiments show sets new stateoftheart accuracy generalization controlling multientity motions project page,-1,0.0,-1,0.0
vidgpt introducing gptstyle autoregressive generation video diffusion models advance diffusion models todays video generation achieved impressive quality generating temporal consistent long videos still challenging majority video diffusion models vdms generate long videos autoregressive manner ie generating subsequent clips conditioned last frames previous clip however existing approaches involve bidirectional computations restricts receptive context autoregression step results model lacking longterm dependencies inspired huge success large language models llms following gpt generative pretrained transformer bring causal ie unidirectional generation vdms use past frames prompt generate future frames causal generation introduce causal temporal attention vdm forces generated frame depend previous frames frame prompt inject conditional frames concatenating noisy frames frames generated along temporal axis consequently present video diffusion gpt vidgpt based two key designs autoregression step able acquire longterm context prompting frames concatenated previously generated frames additionally bring kvcache mechanism vdms eliminates redundant computation overlapped frames significantly boosting inference speed extensive experiments demonstrate vidgpt achieves stateoftheart performance quantitatively qualitatively long video generation code available httpsgithubcomdawnlxcausalvideogen,-1,0.0,-1,0.0
physgen rigidbody physicsgrounded imagetovideo generation present physgen novel imagetovideo generation method converts single image input condition eg force torque applied object image produce realistic physically plausible temporally consistent video key insight integrate modelbased physical simulation datadriven video generation process enabling plausible imagespace dynamics heart system three core components image understanding module effectively captures geometry materials physical parameters image ii imagespace dynamics simulation model utilizes rigidbody physics inferred parameters simulate realistic behaviors iii imagebased rendering refinement module leverages generative video diffusion produce realistic video footage featuring simulated motion resulting videos realistic physics appearance even precisely controllable showcasing superior results existing datadriven imagetovideo generation works quantitative comparison comprehensive user study physgens resulting videos used various downstream applications turning image realistic animation allowing users interact image create various dynamics project page httpsstevenlswgithubiophysgen,-1,0.0,-1,0.0
reducio generating video within seconds using extremely compressed motion latents commercial video generation models exhibited realistic highfidelity results still restricted limited access one crucial obstacle largescale applications expensive training inference cost paper argue videos contain much redundant information images thus encoded motion latents based content image towards goal design imageconditioned vae encode video extremely compressed motion latent space magic reducio charm enables reduction latents compared common vae without sacrificing quality training diffusion models compact representation easily allows generating resolution videos adopt twostage video generation paradigm performs texttoimage textimagetovideo sequentially extensive experiments show reduciodit achieves strong performance evaluation though trained limited gpu resources importantly method significantly boost efficiency video ldms training inference train reduciodit around training hours total generate video clip within seconds single gpu code released httpsgithubcommicrosoftreduciovae,-1,0.0,-1,0.0
ytcommentqa video question answerability instructional videos instructional videos provide detailed howto guides various tasks viewers often posing questions regarding content addressing questions vital comprehending content yet receiving immediate answers difficult numerous computational models developed video question answering video qa tasks primarily trained questions generated based video content aiming produce answers within content however realworld situations users may pose questions go beyond videos informational boundaries highlighting necessity determine video provide answer discerning whether question answered video content challenging due multimodal nature videos visual verbal information intertwined bridge gap present ytcommentqa dataset contains naturallygenerated questions youtube categorized answerability required modality answer visual script experiments answerability classification tasks demonstrate complexity ytcommentqa emphasize need comprehend combined role visual script information video reasoning dataset available httpsgithubcomlgresearchytcommentqa,0,0.9206594400262387,0,0.9206594400262387
learning video representations without natural videos show useful video representations learned synthetic videos natural images without incorporating natural videos training propose progression video datasets synthesized simple generative processes model growing set natural video properties eg motion acceleration shape transformations downstream performance video models pretrained generated datasets gradually increases dataset progression videomae model pretrained synthetic videos closes performance gap action classification training scratch selfsupervised pretraining natural videos outperforms pretrained model introducing crops static images pretraining stage results similar performance pretraining outperforms pretrained model outofdistribution datasets analyzing lowlevel properties datasets identify correlations frame diversity frame similarity natural data downstream performance approach provides controllable transparent alternative video data curation processes pretraining,-1,0.0,-1,0.0
sokbench situated video reasoning benchmark aligned openworld knowledge learning commonsense reasoning visual contexts scenes realworld crucial step toward advanced artificial intelligence however existing video reasoning benchmarks still inadequate since mainly designed factual situated reasoning rarely involve broader knowledge real world work aims delve deeper reasoning evaluations specifically within dynamic openworld structured context knowledge propose new benchmark sokbench consisting questions situations instancelevel annotations depicted videos reasoning process required understand apply situated knowledge general knowledge problemsolving create dataset propose automatic scalable generation method generate questionanswer pairs knowledge graphs rationales instructing combinations llms mllms concretely first extract observable situated entities relations processes videos situated knowledge extend openworld knowledge beyond visible content task generation facilitated multiple dialogues iterations subsequently corrected refined designed selfpromptings demonstrations corpus explicit situated facts implicit commonsense generate associated questionanswer pairs reasoning processes finally followed manual reviews quality assurance evaluated recent mainstream large visionlanguage models benchmark found several insightful conclusions information please refer benchmark wwwbobbywucomsokbench,0,1.0,0,1.0
crossmodal angiography video generation static fundus photography clinical knowledge guidance fundus fluorescein angiography ffa critical tool assessing retinal vascular dynamics aiding diagnosis eye diseases however invasive nature less accessibility compared color fundus cf images pose significant challenges current cf ffa translation methods limited static generation work pioneer dynamic ffa video generation static cf images introduce autoregressive gan smooth memorysaving framebyframe ffa synthesis enhance focus dynamic lesion changes ffa regions design knowledge mask based clinical experience leveraging mask approach integrates innovative knowledge maskguided techniques including knowledgeboosted attention knowledgeaware discriminators maskenhanced patchnce loss aimed refining generation critical areas addressing pixel misalignment challenge method achieves best fvd psnr compared common video generation approaches human assessment ophthalmologist confirms high generation quality notably knowledge mask surpasses supervised lesion segmentation masks offering promising noninvasive alternative traditional ffa research clinical applications code available,19,1.0,19,1.0
generative inbetweening adapting imagetovideo models keyframe interpolation present method generating video sequences coherent motion pair input key frames adapt pretrained largescale imagetovideo diffusion model originally trained generate videos moving forward time single input image key frame interpolation ie produce video two input frames accomplish adaptation lightweight finetuning technique produces version model instead predicts videos moving backwards time single input image model along original forwardmoving model subsequently used dualdirectional diffusion sampling process combines overlapping model estimates starting two keyframes experiments show method outperforms existing diffusionbased methods traditional frame interpolation techniques,-1,0.0,-1,0.0
medical imaging complexity effects gan performance proliferation machine learning models diverse clinical applications led growing need highfidelity medical image training data data often scarce due cost constraints privacy concerns alleviating burden medical image synthesis via generative adversarial networks gans emerged powerful method synthetically generating photorealistic images based existing sets real medical images however exact image set size required efficiently train gan unclear work experimentally establish benchmarks measure relationship sample dataset size fidelity generated images given datasets distribution image complexities analyze statistical metrics based delentropy image complexity measure rooted shannons entropy information theory pipeline conduct experiments two stateoftheart gans stylegan spadegan trained multiple medical imaging datasets variable sample sizes across gans general performance improved increasing training set size suffered increasing complexity,17,0.9877086249719165,17,0.9877086249719165
deep generative models medical image synthesis deep generative modeling emerged powerful tool synthesizing realistic medical images driving advances medical image analysis disease diagnosis treatment planning chapter explores various deep generative models medical image synthesis focus variational autoencoders vaes generative adversarial networks gans denoising diffusion models ddms discuss fundamental principles recent advances well strengths weaknesses models examine applications clinically relevant problems including unconditional conditional generation tasks like imagetoimage translation image reconstruction additionally review commonly used evaluation metrics assessing image fidelity diversity utility privacy provide overview current challenges field,-1,0.0,-1,0.0
cogvideox texttovideo diffusion models expert transformer present cogvideox largescale texttovideo generation model based diffusion transformer generate continuous videos aligned text prompt frame rate fps resolution pixels previous video generation models often limited movement short durations difficult generate videos coherent narratives based text propose several designs address issues first propose variational autoencoder vae compress videos along spatial temporal dimensions improve compression rate video fidelity second improve textvideo alignment propose expert transformer expert adaptive layernorm facilitate deep fusion two modalities third employing progressive training multiresolution frame pack technique cogvideox adept producing coherent longduration different shape videos characterized significant motions addition develop effective textvideo data processing pipeline includes various data preprocessing strategies video captioning method greatly contributing generation quality semantic alignment results show cogvideox demonstrates stateoftheart performance across multiple machine metrics human evaluations model weight causal vae video caption model cogvideox publicly available httpsgithubcomthudmcogvideo,-1,0.0,-1,0.0
gaussian splatting decoder generative adversarial networks nerfbased generative adversarial networks gans like giraffe shown high rendering quality large representational variety however rendering neural radiance fields poses challenges applications first significant computational demands nerf rendering preclude use lowpower devices mobiles vrar headsets second implicit representations based neural networks difficult incorporate explicit scenes vr environments video games gaussian splatting overcomes limitations providing explicit representation rendered efficiently high frame rates work present novel approach combines high rendering quality nerfbased gans flexibility computational advantages training decoder maps implicit nerf representations explicit gaussian splatting attributes integrate representational diversity quality gans ecosystem gaussian splatting first time additionally approach allows high resolution gan inversion realtime gan editing gaussian splatting scenes project page,1,0.9017358373274306,1,0.9017358373274306
distilling visionlanguage models millions videos recent advance visionlanguage models largely attributed abundance imagetext data aim replicate success videolanguage models simply enough humancurated videotext data available thus resort finetuning videolanguage model strong imagelanguage baseline synthesized instructional data resulting video model videoinstructiontuning viit used autolabel millions videos generate highquality captions show adapted videolanguage model performs well wide range videolanguage benchmarks instance surpasses best prior result openended nextqa besides model generates detailed descriptions previously unseen videos provide better textual supervision existing methods experiments show videolanguage dualencoder model contrastively trained autogenerated captions better strongest baseline also leverages visionlanguage models best model outperforms stateoftheart methods msrvtt zeroshot texttovideo retrieval side product generate largest video caption dataset date,0,0.8433116490756853,0,0.8433116490756853
singer vivid audiodriven singing video generation multiscale spectral diffusion model recent advancements generative models significantly enhanced talking face video generation yet singing video generation remains underexplored differences human talking singing limit performance existing talking face video generation models applied singing fundamental differences talking singingspecifically audio characteristics behavioral expressionslimit effectiveness existing models observe differences singing talking audios manifest terms frequency amplitude address designed multiscale spectral module help model learn singing patterns spectral domain additionally develop spectralfiltering module aids model learning human behaviors associated singing audio two modules integrated diffusion model enhance singing video generation performance resulting proposed model singer furthermore lack highquality realworld singing face videos hindered development singing video generation community address gap collected inthewild audiovisual singing dataset facilitate research area experiments demonstrate singer capable generating vivid singing videos outperforms stateoftheart methods objective subjective evaluations,6,0.9562929796259503,6,0.9562929796259503
sonicvisionlm playing sound vision language models growing interest task generating sound silent videos primarily practicality streamlining video postproduction however existing methods videosound generation attempt directly create sound visual representations challenging due difficulty aligning visual representations audio representations paper present sonicvisionlm novel framework aimed generating wide range sound effects leveraging visionlanguage modelsvlms instead generating audio directly video use capabilities powerful vlms provided silent video approach first identifies events within video using vlm suggest possible sounds match video content shift approach transforms challenging task aligning image audio wellstudied subproblems aligning imagetotext texttoaudio popular diffusion models improve quality audio recommendations llms collected extensive dataset maps text descriptions specific sound effects developed timecontrolled audio adapter approach surpasses current stateoftheart methods converting video audio enhancing synchronization visuals improving alignment audio video components project page httpsyusiissygithubiosonicvisionlmgithubio,8,0.8316763916786816,8,0.8316763916786816
syncammaster synchronizing multicamera video generation diverse viewpoints recent advancements video diffusion models shown exceptional abilities simulating realworld dynamics maintaining consistency progress inspires us investigate potential models ensure dynamic consistency across various viewpoints highly desirable feature applications virtual filming unlike existing methods focused multiview generation single objects reconstruction interest lies generating openworld videos arbitrary viewpoints incorporating dof camera poses achieve propose plugandplay module enhances pretrained texttovideo model multicamera video generation ensuring consistent content across different viewpoints specifically introduce multiview synchronization module maintain appearance geometry consistency across viewpoints given scarcity highquality training data design hybrid training scheme leverages multicamera images monocular videos supplement unreal enginerendered multicamera videos furthermore method enables intriguing extensions rerendering video novel viewpoints also release multiview synchronized video dataset named syncamvideodataset project page httpsjianhongbaigithubiosyncammaster,-1,0.0,-1,0.0
motionbridge dynamic video inbetweening flexible controls generating plausible smooth transitions two image frames video inbetweening essential tool video editing long video synthesis traditional works lack capability generate complex large motions recent video generation techniques powerful creating highquality results often lack fine control details intermediate frames lead results align creative mind introduce motionbridge unified video inbetweening framework allows flexible controls including trajectory strokes keyframes masks guide pixels text however learning multimodal controls unified framework challenging task thus design two generators extract control signal faithfully encode feature dualbranch embedders resolve ambiguities introduce curriculum training strategy smoothly learn various controls extensive qualitative quantitative experiments demonstrated multimodal controls enable dynamic customizable contextually accurate visual narrative,-1,0.0,-1,0.0
objectcentric diffusion efficient video editing diffusionbased video editing reached impressive quality transform either global style local structure attributes given video inputs following textual edit prompts however solutions typically incur heavy memory computational costs generate temporallycoherent frames either form diffusion inversion andor crossframe attention paper conduct analysis inefficiencies suggest simple yet effective modifications allow significant speedups whilst maintaining quality moreover introduce objectcentric diffusion fix generation artifacts reduce latency allocating computations towards foreground edited regions arguably important perceptual quality achieve two novel proposals objectcentric sampling decoupling diffusion steps spent salient background regions spending former ii objectcentric token merging reduces cost crossframe attention fusing redundant tokens unimportant background regions techniques readily applicable given video editing model without retraining drastically reduce memory computational cost evaluate proposals inversionbased controlsignalbased editing pipelines show latency reduction comparable synthesis quality project page qualcommairesearchgithubioobjectcentricdiffusion,11,0.9686532961594841,11,0.9686532961594841
genuine knowledge practice diffusion testtime adaptation video adverse weather removal realworld vision tasks frequently suffer appearance unexpected adverse weather conditions including rain haze snow raindrops last decade convolutional neural networks vision transformers yielded outstanding results singleweather video removal however due absence appropriate adaptation fail generalize weather conditions although viwsnet proposed remove adverse weather conditions videos single set pretrained weights seriously blinded seen weather traintime degenerates coming unseen weather testtime work introduce testtime adaptation adverse weather removal videos propose first framework integrates testtime adaptation iterative diffusion reverse process specifically devise diffusionbased network novel temporal noise model efficiently explore framecorrelated information degraded video clips training stage inference stage introduce proxy task named diffusion tubelet selfcalibration learn primer distribution test video stream optimize model approximating temporal noise model online adaptation experimental results benchmark datasets demonstrate testtime adaptation method diffusionbased networkdifftta outperforms stateoftheart methods terms restoring videos degraded seen weather conditions generalizable capability also validated unseen weather conditions synthesized realworld videos,-1,0.0,-1,0.0
mmego towards building egocentric multimodal llms research aims comprehensively explore building multimodal foundation model egocentric video understanding achieve goal work three fronts first lack qa data egocentric video understanding develop data engine efficiently generates highquality qa samples egocentric videos ranging seconds one hour long based humanannotated data currently largest egocentric qa dataset second contribute challenging egocentric qa benchmark videos questions evaluate models ability recognizing memorizing visual details across videos varying lengths introduce new debiasing evaluation method help mitigate unavoidable language bias present models evaluated third propose specialized multimodal architecture featuring novel memory pointer prompting mechanism design includes global glimpse step gain overarching understanding entire video identify key visual information followed fallback step utilizes key visual information generate responses enables model effectively comprehend extended video content data benchmark model successfully build mmego egocentric multimodal llm shows powerful performance egocentric video understanding,0,0.9965170736187715,0,0.9965170736187715
emotion future motion simulation via event sequence diffusion forecasting typical objects future motion critical task interpreting interacting dynamic environments computer vision eventbased sensors could capture changes scene exceptional temporal granularity may potentially offer unique opportunity predict future motion level detail precision previously unachievable inspired propose integrate strong learning capacity video diffusion model rich motion information event camera motion simulation framework specifically initially employ pretrained stable video diffusion models adapt event sequence dataset process facilitates transfer extensive knowledge rgb videos eventcentric domain moreover introduce alignment mechanism utilizes reinforcement learning techniques enhance reverse generation trajectory diffusion model ensuring improved performance accuracy extensive testing validation demonstrate effectiveness method various complex scenarios showcasing potential revolutionize motion flow prediction computer vision applications autonomous vehicle guidance robotic navigation interactive media findings suggest promising direction future research enhancing interpretative power predictive accuracy computer vision systems,-1,0.0,-1,0.0
instructional video generation despite recent strides video generation stateoftheart methods still struggle elements visual detail one particularly challenging case class egocentric instructional videos intricate motion hand coupled mostly stable nondistracting environment necessary convey appropriate visual action instruction address challenges introduce new method instructional video generation diffusionbased method incorporates two distinct innovations first propose automatic method generate expected region motion guided visual context action text second introduce critical hand structure loss guide diffusion model focus smooth consistent hand poses evaluate method augmented instructional datasets based epickitchens demonstrating significant improvements stateoftheart methods terms instructional clarity especially hand motion target region across diverse environments actions video results found httpsexcitedbuttergithubioinstructionalvideogeneration,-1,0.0,-1,0.0
surgsora decoupled rgbdflow diffusion model controllable surgical video generation medical video generation transformative potential enhancing surgical understanding pathology insights precise controllable visual representations however current models face limitations controllability authenticity bridge gap propose surgsora motioncontrollable surgical video generation framework uses single input frame usercontrollable motion cues surgsora consists three key modules dual semantic injector dsi extracts objectrelevant rgb depth features input frame integrates segmentation cues capture detailed spatial features complex anatomical structures decoupled flow mapper dfm fuses optical flow semanticrgbd features multiple scales enhance temporal understanding object spatial dynamics trajectory controller tc allows users specify motion directions estimates sparse optical flow guiding video generation process fused features used conditions frozen stable diffusion model produce realistic temporally coherent surgical videos extensive evaluations demonstrate surgsora outperforms stateoftheart methods controllability authenticity showing potential advance surgical video generation medical education training research,-1,0.0,-1,0.0
uncovering hidden subspaces video diffusion models using reidentification latent video diffusion models easily deceive casual observers domain experts alike thanks produced image quality temporal consistency beyond entertainment creates opportunities around safe data sharing fully synthetic datasets crucial healthcare well domains relying sensitive personal information however privacy concerns approach fully addressed yet models trained synthetic data specific downstream tasks still perform worse trained real data discrepancy may partly due sampling space subspace training videos effectively reducing training data size downstream models additionally reduced temporal consistency generating long videos could contributing factor paper first show training privacypreserving models latent space computationally efficient generalize better furthermore investigate downstream degradation factors propose use reidentification model previously employed privacy preservation filter demonstrate sufficient train model latent space video generator subsequently use models evaluate subspace covered synthetic video datasets thus introduce new way measure faithfulness generative machine learning models focus specific application healthcare echocardiography illustrate effectiveness novel methods findings indicate training videos learned latent video diffusion models could explain lack performance training downstream tasks synthetic data,-1,0.0,-1,0.0
aid adapting diffusion models instructionguided video prediction textguided video prediction tvp involves predicting motion future frames initial frame according instruction wide applications virtual reality robotics content creation previous tvp methods make significant breakthroughs adapting stable diffusion task however struggle frame consistency temporal stability primarily due limited scale video datasets observe pretrained diffusion models possess good priors video dynamics lack textual control hence transferring models leverage video dynamic priors injecting instruction control generate controllable videos meaningful challenging task achieve introduce multimodal large language model mllm predict future video states based initial frames text instructions specifically design dual query transformer dqformer architecture integrates instructions frames conditional embeddings future frame prediction additionally develop longshort term temporal adapters spatial adapters quickly transfer general video diffusion models specific scenarios minimal training costs experimental results show method significantly outperforms stateoftheart techniques four datasets something something epic bridge data notably aid achieves fvd improvements bridge respectively demonstrating effectiveness various domains examples found website httpschenhsinggithubioaid,-1,0.0,-1,0.0
narcan natural refined canonical image integration diffusion prior video editing propose video editing framework narcan integrates hybrid deformation field diffusion prior generate highquality natural canonical images represent input video approach utilizes homography model global motion employs multilayer perceptrons mlps capture local residual deformations enhancing models ability handle complex video dynamics introducing diffusion prior early stages training model ensures generated images retain highquality natural appearance making produced canonical images suitable various downstream tasks video editing capability achieved current canonicalbased methods furthermore incorporate lowrank adaptation lora finetuning introduce noise diffusion prior update scheduling technique accelerates training process times extensive experimental results show method outperforms existing approaches various video editing tasks produces coherent highquality edited video sequences see project page video results,-1,0.0,-1,0.0
videomaker zeroshot customized video generation inherent force video diffusion models zeroshot customized video generation gained significant attention due substantial application potential existing methods rely additional models extract inject reference subject features assuming video diffusion model vdm alone insufficient zeroshot customized video generation however methods often struggle maintain consistent subject appearance due suboptimal feature extraction injection techniques paper reveal vdm inherently possesses force extract inject subject features departing previous heuristic approaches introduce novel framework leverages vdms inherent force enable highquality zeroshot customized video generation specifically feature extraction directly input reference images vdm use intrinsic feature extraction process provides finegrained features also significantly aligns vdms pretrained knowledge feature injection devise innovative bidirectional interaction subject features generated content spatial selfattention within vdm ensuring vdm better subject fidelity maintaining diversity generated video experiments customized human object video generation validate effectiveness framework,9,0.7676269057144607,9,0.7676269057144607
learning localize actions instructional videos llmbased multipathway textvideo alignment learning localize temporal boundaries procedure steps instructional videos challenging due limited availability annotated largescale training videos recent works focus learning crossmodal alignment video segments asrtranscripted narration texts contrastive learning however methods fail account alignment noise ie irrelevant narrations instructional task videos unreliable timestamps narrations address challenges work proposes novel training framework motivated strong capabilities large language models llms procedure understanding text summarization first apply llm filter taskirrelevant information summarize taskrelated procedure steps llmsteps narrations generate reliable pseudomatching llmsteps video training propose multipathway textvideo alignment mptva strategy key idea measure alignment llmsteps videos via multiple pathways including stepnarrationvideo alignment using narration timestamps direct steptovideo alignment based longterm semantic similarity direct steptovideo alignment focusing shortterm finegrained semantic similarity learned general video domains results different pathways fused generate reliable pseudo stepvideo matching conducted extensive experiments across various tasks problem settings evaluate proposed method approach surpasses stateoftheart methods three downstream tasks procedure step grounding step localization narration grounding,0,1.0,0,1.0
freesurgs sfmfree gaussian splatting surgical scene reconstruction realtime reconstruction surgical scenes plays vital role computerassisted surgery holding promise enhance surgeons visibility recent advancements gaussian splatting shown great potential realtime novel view synthesis general scenes relies accurate poses point clouds generated structurefrommotion sfm initialization however sfm fails recover accurate camera poses geometry surgical scenes due challenges minimal textures photometric inconsistencies tackle problem paper propose first sfmfree method surgical scene reconstruction jointly optimizing camera poses scene representation based video continuity key method exploit immediate optical flow priors guide projection flow derived gaussians unlike previous methods relying photometric loss formulate pose estimation problem minimizing flow loss projection flow optical flow consistency check introduced filter flow outliers detecting rigid reliable points satisfy epipolar geometry gaussian optimization randomly sample frames optimize scene representations grow gaussian progressively experiments scared dataset demonstrate superior performance existing methods novel view synthesis pose estimation high efficiency code available httpsgithubcomwrldfreesurgs,1,1.0,1,1.0
urban scene generation satellite images diffusion directly generating scenes satellite imagery offers exciting possibilities integration applications like games map services however challenges arise significant view changes scene scale previous efforts mainly focused image video generation lacking exploration adaptability scene generation arbitrary views existing generation works either operate object level difficult utilize geometry obtained satellite imagery overcome limitations propose novel architecture direct scene generation introducing diffusion models sparse representations combining neural rendering techniques specifically approach generates texture colors point level given geometry using diffusion model first transformed scene representation feedforward manner representation utilized render arbitrary views would excel singleframe quality interframe consistency experiments two cityscale datasets show model demonstrates proficiency generating photorealistic streetview image sequences crossview urban scenes satellite imagery,-1,0.0,-1,0.0
trainingfree semantic video composition via pretrained diffusion model video composition task aims integrate specified foregrounds backgrounds different videos harmonious composite current approaches predominantly trained videos adjusted foreground color lighting struggle address deep semantic disparities beyond superficial adjustments domain gaps therefore propose trainingfree pipeline employing pretrained diffusion model imbued semantic prior knowledge process composite videos broader semantic disparities specifically process video frames cascading manner handle frame two processes diffusion model inversion process propose balanced partial inversion obtain generation initial points balance reversibility modifiability generation process propose interframe augmented attention augment foreground continuity across frames experimental results reveal pipeline successfully ensures visual harmony interframe coherence outputs demonstrating efficacy managing broader semantic disparities,-1,0.0,-1,0.0
efficient diffusion models comprehensive survey principles practices one popular soughtafter generative models recent years diffusion models sparked interests many researchers steadily shown excellent advantage various generative tasks image synthesis video generation molecule design scene rendering multimodal generation relying dense theoretical principles reliable application practices remarkable success recent efforts diffusion models comes largely progressive design principles efficient architecture training inference deployment methodologies however comprehensive indepth review summarize principles practices help rapid understanding application diffusion models survey provide new efficiencyoriented perspective existing efforts mainly focuses profound principles efficient practices architecture designs model training fast inference reliable deployment guide theoretical research algorithm migration model application new scenarios readerfriendly way urlhttpsgithubcomponyzymefficientdmssurvey,-1,0.0,-1,0.0
automatic generation interactive game scenes users casual sketches content generation heart many computer graphics applications including video gaming filmmaking virtual augmented reality etc paper proposes novel deeplearning based approach automatically generating interactive playable game scenes users casual prompts handdrawn sketch sketchbased input offers natural convenient way convey users design intention content creation process circumvent datadeficient challenge learning ie lack large training data scenes method leverages pretrained denoising diffusion model generate image scene conceptual guidance process adopt isometric projection mode factor unknown camera poses obtaining scene layout generated isometric image use pretrained image understanding method segment image meaningful parts offground objects trees buildings extract scene layout segments layouts subsequently fed procedural content generation pcg engine video game engine like unity unreal create scene resulting scene seamlessly integrated game development environment readily playable extensive tests demonstrate method efficiently generate highquality interactive game scenes layouts closely follow users intention,-1,0.0,-1,0.0
customvideo customizing texttovideo generation multiple subjects customized texttovideo generation aims generate highquality videos guided text prompts subject references current approaches personalizing texttovideo generation suffer tackling multiple subjects challenging practical scenario work aim promote multisubject guided texttovideo customization propose customvideo novel framework generate identitypreserving videos guidance multiple subjects specific firstly encourage cooccurrence multiple subjects via composing single image upon basic texttovideo diffusion model design simple yet effective attention control strategy disentangle different subjects latent space diffusion model moreover help model focus specific area object segment object given reference images provide corresponding object mask attention learning also collect multisubject texttovideo generation dataset comprehensive benchmark individual subjects different categories meaningful pairs extensive qualitative quantitative user study results demonstrate superiority method compared previous stateoftheart approaches project page httpskyfafydwangprojectscustomvideo,-1,0.0,-1,0.0
magicme identityspecific video customized diffusion creating content specified identities id attracted significant interest field generative models field texttoimage generation subjectdriven creation achieved great progress identity controlled via reference images however extension video generation well explored work propose simple yet effective subject identity controllable video generation framework termed video custom diffusion vcd specified identity defined images vcd reinforces identity characteristics injects framewise correlation initialization stage stable video outputs achieve propose three novel components essential highquality identity preservation stable video generation noise initialization method gaussian noise prior better interframe stability id module based extended textual inversion trained cropped identity disentangle id information background face vcd tiled vcd modules reinforce faces upscale video higher resolution preserving identitys features conducted extensive experiments verify vcd able generate stable videos better id baselines besides transferability encoded identity id module vcd also working well personalized texttoimage models available publicly codes available httpsgithubcomzhendongmagicme,11,1.0,11,1.0
motionzero zeroshot moving object control framework diffusionbased video generation recent largescale pretrained diffusion models demonstrated powerful generative ability produce highquality videos detailed text descriptions however exerting control motion objects videos generated video diffusion model challenging problem paper propose novel zeroshot moving object trajectory control framework motionzero enable boundingboxtrajectoriescontrolled texttovideo diffusion model end initial noise prior module designed provide positionbased prior improve stability appearance moving object accuracy position addition based attention map unet spatial constraints directly applied denoising process diffusion models ensures positional spatial consistency moving objects inference furthermore temporal consistency guaranteed proposed shift temporal attention mechanism method flexibly applied various stateoftheart video diffusion models without training process extensive experiments demonstrate proposed method control motion trajectories objects generate highquality videos project page httpsvpxecnugithubiomotionzerowebsite,-1,0.0,-1,0.0
textbased talking video editing cascaded conditional diffusion textbased talkinghead video editing aims efficiently insert delete substitute segments talking videos userfriendly text editing approach challenging generalizable talkingface representation seamless audiovisual transitions identitypreserved talking faces previous works either require minutes talkingface video training data expensive testtime optimization customized talking video editing directly generate video sequence without considering incontext information leading poor generalizable representation incoherent transitions even inconsistent identity paper propose efficient cascaded conditional diffusionbased framework consists two stages audio denselandmark motion motion video textittextbfin first stage first propose dynamic weighted incontext diffusion module synthesize denselandmark motions given edited audio textittextbfin second stage introduce warpingguided conditional diffusion module module first interpolates start end frames editing interval generate smooth intermediate frames help audiotodense motion images intermediate frames warped obtain coarse intermediate frames conditioned warped intermedia frames diffusion model adopted generate detailed highresolution target frames guarantees coherent identitypreserved transitions cascaded conditional diffusion model decomposes complex talking editing task two flexible generation tasks provides generalizable talkingface representation seamless audiovisual transitions identitypreserved faces small dataset experiments show effectiveness superiority proposed method,-1,0.0,-1,0.0
fast memoryefficient video diffusion using streamlined inference rapid progress artificial intelligencegenerated content aigc especially diffusion models significantly advanced development highquality video generation however current video diffusion models exhibit demanding computational requirements high peak memory usage especially generating longer higherresolution videos limitations greatly hinder practical application video diffusion models standard hardware platforms tackle issue present novel trainingfree framework named streamlined inference leverages temporal spatial properties video diffusion models approach integrates three core components feature slicer operator grouping step rehash specifically feature slicer effectively partitions input features subfeatures operator grouping processes subfeature group consecutive operators resulting significant memory reduction without sacrificing quality speed step rehash exploits similarity adjacent steps diffusion accelerates inference skipping unnecessary steps extensive experiments demonstrate approach significantly reduces peak memory computational overhead making feasible generate highquality videos single consumer gpu eg reducing peak memory animatediff featuring faster inference,-1,0.0,-1,0.0
promptavideo prompt video diffusion model via preferencealigned llm texttovideo models made remarkable advancements optimization highquality textvideo pairs textual prompts play pivotal role determining quality output videos however achieving desired output often entails multiple revisions iterative inference refine userprovided prompts current automatic methods refining prompts encounter challenges modalityinconsistency costdiscrepancy modelunaware applied texttovideo diffusion models address problem introduce llmbased prompt adaptation framework termed promptavideo excels crafting videocentric laborfree preferencealigned prompts tailored specific video diffusion model approach involves meticulously crafted twostage optimization alignment system initially conduct rewardguided prompt evolution pipeline automatically create optimal prompts pool leverage supervised finetuning sft llm multidimensional rewards employed generate pairwise data sft model followed direct preference optimization dpo algorithm facilitate preference alignment extensive experimentation comparative analyses validate effectiveness promptavideo across diverse generation models highlighting potential push boundaries video generation,-1,0.0,-1,0.0
foodmem near realtime precise food video segmentation food segmentation including videos vital addressing realworld health agriculture food biotechnology issues current limitations lead inaccurate nutritional analysis inefficient crop management suboptimal food processing impacting food security public health improving segmentation techniques enhance dietary assessments agricultural productivity food production process study introduces development robust framework highquality nearrealtime segmentation tracking food items videos using minimal hardware resources present foodmem novel framework designed segment food items video sequences unbounded scenes foodmem consistently generate masks food portions video sequence overcoming limitations existing semantic segmentation models flickering prohibitive inference speeds video processing contexts address issues foodmem leverages twophase solution transformer segmentation phase create initial segmentation masks memorybased tracking phase monitor food masks complex scenes framework outperforms current stateoftheart food segmentation models yielding superior performance across various conditions camera angles lighting reflections scene complexity food diversity results reduced segmentation noise elimination artifacts completion missing segments also introduce new annotated food dataset encompassing challenging scenarios absent previous benchmarks extensive experiments conducted vegetables fruits datasets demonstrate foodmem enhances stateoftheart mean average precision food video segmentation x faster average,-1,0.0,-1,0.0
expressedit video editing natural language sketching informational videos serve crucial source explaining conceptual procedural knowledge novices experts alike producing informational videos editors edit videos overlaying textimages trimming footage enhance video quality make engaging however video editing difficult timeconsuming especially novice video editors often struggle expressing implementing editing ideas address challenge first explored multimodalitynatural language nl sketching natural modalities humans use expressioncan utilized support video editors expressing video editing ideas gathered multimodal expressions editing commands video editors revealed patterns use nl sketching describing edit intents based findings present expressedit system enables editing videos via nl text sketching video frame powered llm vision models system interprets temporal spatial operational references nl command spatial references sketching system implements interpreted edits user iterate observational study showed expressedit enhanced ability novice video editors express implement edit ideas system allowed participants perform edits efficiently generate ideas generating edits based users multimodal edit commands supporting iterations editing commands work offers insights design future multimodal interfaces aibased pipelines video editing,15,1.0,15,1.0
raven rethinking adversarial video generation efficient triplane networks present novel unconditional video generative model designed address longterm spatial temporal dependencies attention computational dataset efficiency capture long spatiotemporal dependencies approach incorporates hybrid explicitimplicit triplane representation inspired generative frameworks developed threedimensional object representation employs single latent code model entire video clip individual video frames synthesized intermediate triplane representation derived primary latent code novel strategy halves computational complexity measured flops compared efficient stateoftheart methods consequently approach facilitates efficient temporally coherent generation videos moreover joint frame modeling approach contrast autoregressive methods mitigates generation visual artifacts enhance models capabilities integrating optical flowbased module within generative adversarial network gan based generator architecture thereby compensating constraints imposed smaller generator size result model synthesizes highfidelity video clips resolution pixels durations extending seconds frame rate fps efficacy versatility approach empirically validated qualitative quantitative assessments across three different datasets comprising synthetic real video clips make training inference code public,-1,0.0,-1,0.0
sora detector unified hallucination detection large texttovideo models rapid advancement texttovideo generative models enabled synthesis highfidelity video content guided textual descriptions despite significant progress models often susceptible hallucination generating contents contradict input text poses challenge reliability practical deployment address critical issue introduce soradetector novel unified framework designed detect hallucinations across diverse large models including cuttingedge sora model framework built upon comprehensive analysis hallucination phenomena categorizing based manifestation video content leveraging stateoftheart keyframe extraction techniques multimodal large language models soradetector first evaluates consistency extracted video content summary textual prompts constructs static dynamic knowledge graphs kgs frames detect hallucination single frames across frames sora detector provides robust quantifiable measure consistency static dynamic hallucination addition developed sora detector agent automate hallucination detection process generate complete video quality report input video lastly present novel metaevaluation benchmark meticulously crafted facilitate evaluation advancements hallucination detection extensive experiments videos generated sora large models demonstrate efficacy approach accurately detecting hallucinations code dataset accessed via github,-1,0.0,-1,0.0
robot shape location retention video generation using diffusion models diffusion models marked significant milestone enhancement image video generation technologies however generating videos precisely retain shape location moving objects robots remains challenge paper presents diffusion models specifically tailored generate videos accurately maintain shape location mobile robots development offers substantial benefits working detecting dangerous interactions humans robots facilitating creation training data collision detection models circumventing need collecting data real world often involves legal ethical issues models incorporate techniques embedding accessible robot pose information applying semantic mask regulation within convnext backbone network techniques designed refine intermediate outputs therefore improving retention performance shape location extensive experimentation models demonstrated notable improvements maintaining shape location different robots well enhancing overall video generation quality compared benchmark diffusion model codes opensourced hrefhttpsgithubcompengpaulwangdiffusionrobotsgithub,-1,0.0,-1,0.0
generative video models help pose estimation pairwise pose estimation images little overlap open challenge computer vision existing methods even trained largescale datasets struggle scenarios due lack identifiable correspondences visual overlap inspired human ability infer spatial relationships diverse scenes propose novel approach interpose leverages rich priors encoded within pretrained generative video models propose use video model hallucinate intermediate frames two input images effectively creating dense visual transition significantly simplifies problem pose estimation since current video models still produce implausible motion inconsistent geometry introduce selfconsistency score evaluates consistency pose predictions sampled videos demonstrate approach generalizes among three stateoftheart video models show consistent improvements stateoftheart four diverse datasets encompassing indoor outdoor objectcentric scenes findings suggest promising avenue improving pose estimation models leveraging large generative models trained vast amounts video data readily available data see project page results httpsinterposegithubio,1,1.0,1,1.0
animate thoughts decoupled reconstruction dynamic natural vision slow brain activity reconstructing human dynamic vision brain activity challenging task great scientific significance although prior video reconstruction methods made substantial progress still suffer several limitations including difficulty simultaneously reconciling semantic eg categorical descriptions structure eg size color consistent motion information eg order frames low temporal resolution fmri poses challenge decoding multiple frames video dynamics single fmri frame reliance video generation models introduces ambiguity regarding whether dynamics observed reconstructed videos genuinely derived fmri data hallucinations generative model overcome limitations propose twostage model named mindanimator fmritofeature stage decouple semantic structure motion features fmri specifically employ fmrivisionlanguage trimodal contrastive learning decode semantic feature fmri design sparse causal attention mechanism decoding multiframe video motion features nextframeprediction task featuretovideo stage features integrated videos using inflated stable diffusion effectively eliminating external video data interference extensive experiments multiple videofmri datasets demonstrate model achieves stateoftheart performance comprehensive visualization analyses elucidate interpretability model neurobiological perspective project page httpsmindanimatordesigngithubio,-1,0.0,-1,0.0
makima tuningfree multiattribute opendomain video editing via maskguided attention modulation diffusionbased texttoimage models demonstrated remarkable results global video editing tasks however focus primarily global video modifications achieving desired attributespecific changes remains challenging task specifically multiattribute editing mae video contemporary video editing approaches either require extensive finetuning rely additional networks controlnet modeling multiobject appearances yet remain infancy offering coarsegrained mae solutions paper present makima tuningfree mae framework built upon pretrained models opendomain video editing approach preserves video structure appearance information incorporating attention maps features inversion process denoising facilitate precise editing multiple attributes introduce maskguided attention modulation enhancing correlations spatially corresponding tokens suppressing crossattribute interference selfattention crossattention layers balance video frame generation quality efficiency implement consistent feature propagation generates frame sequences editing keyframes propagating features throughout sequence extensive experiments demonstrate makima outperforms existing baselines opendomain multiattribute video editing tasks achieving superior results editing accuracy temporal consistency maintaining computational efficiency,9,1.0,9,1.0
sparse input view synthesis representations reliable priors novel view synthesis refers problem synthesizing novel viewpoints scene given images viewpoints fundamental problem computer vision graphics enables vast variety applications metaverse freeview watching events video gaming video stabilization video compression recent representations radiance fields multiplane images significantly improve quality images rendered novel viewpoints however models require dense sampling input views high quality renders performance goes significantly input views available thesis focus sparse input novel view synthesis problem static dynamic scenes first part work mainly focus sparse input novel view synthesis static scenes using neural radiance fields nerf study design reliable dense priors better regularize nerf situations particular propose prior visibility pixels pair input views show visibility prior related relative depth objects dense reliable existing priors absolute depth compute visibility prior using plane sweep volumes without need train neural network large datasets evaluate approach multiple datasets show model outperforms existing approaches sparse input novel view synthesis second part aim improve regularization learning scenespecific prior suffer generalization issues achieve learning prior given scene alone without pretraining large datasets particular design augmented nerfs obtain better depth supervision certain regions scene main nerf extend framework also apply newer faster radiance field models tensorf zipnerf extensive experiments multiple datasets show superiority approach sparse input novel view synthesis design sparse input fast dynamic radiance fields severely constrained lack suitable representations reliable priors motion address first challenge designing explicit motion model based factorized volumes compact optimizes quickly also introduce reliable sparse flow priors constrain motion field since find popularly employed dense optical flow priors unreliable show benefits motion representation reliable priors multiple datasets final part thesis study application view synthesis frame rate upsampling video gaming specifically consider problem temporal view synthesis goal predict future frames given past frames camera motion key challenge predicting future motion objects estimating past motion extrapolating explore use multiplane image representations scene depth reliably estimate object motion particularly occluded regions design new database effectively evaluate approach temporal view synthesis dynamic scenes show achieve stateoftheart performance,1,1.0,1,1.0
teaching video diffusion model latent physical phenomenon knowledge video diffusion models exhibited tremendous progress various video generation tasks however existing models struggle capture latent physical knowledge failing infer physical phenomena challenging articulate natural language generating videos following fundamental physical laws still opening challenge address challenge propose novel method teach video diffusion models latent physical phenomenon knowledge enabling accurate generation physically informed phenomena specifically first pretrain masked autoencoders mae reconstruct physical phenomena resulting output embeddings encapsulate latent physical phenomenon knowledge leveraging embeddings could generate pseudolanguage prompt features based aligned spatial relationships clip vision language encoders particularly given diffusion models typically use clips language encoder text prompt embeddings approach integrates clip visual features informed latent physical knowledge quaternion hidden space enables modeling spatial relationships produce physical knowledgeinformed pseudolanguage prompts incorporating prompt features finetuning video diffusion model parameterefficient manner physical knowledgeinformed videos successfully generated validate method extensively numerical simulations realworld observations physical phenomena demonstrating remarkable performance across diverse scenarios,-1,0.0,-1,0.0
freetraj tuningfree trajectory control video diffusion models diffusion model demonstrated remarkable capability video generation sparks interest introducing trajectory control generation process existing works mainly focus trainingbased methods eg conditional adapter argue diffusion model allows decent control generated content without requiring training study introduce tuningfree framework achieve trajectorycontrollable video generation imposing guidance noise construction attention computation specifically first show several instructive phenomenons analyze initial noises influence motion trajectory generated content subsequently propose freetraj tuningfree approach enables trajectory control modifying noise sampling attention mechanisms furthermore extend freetraj facilitate longer larger video generation controllable trajectories equipped designs users flexibility provide trajectories manually opt trajectories automatically generated llm trajectory planner extensive experiments validate efficacy approach enhancing trajectory controllability video diffusion models,-1,0.0,-1,0.0
wavelet diffusion gan image superresolution recent years diffusion models emerged superior alternative generative adversarial networks gans highfidelity image generation wide applications texttoimage generation imagetoimage translation superresolution however realtime feasibility hindered slow training inference speeds study addresses challenge proposing waveletbased conditional diffusion gan scheme singleimage superresolution sisr approach utilizes diffusion gan paradigm reduce timesteps required reverse diffusion process discrete wavelet transform dwt achieve dimensionality reduction decreasing training inference times significantly results experimental validation celebahq dataset confirm effectiveness proposed scheme approach outperforms stateoftheart methodologies successfully ensuring highfidelity output overcoming inherent drawbacks associated diffusion models timesensitive applications,13,0.9089689237263221,13,0.9089689237263221
loopy taming audiodriven portrait avatar longterm motion dependency introduction diffusionbased video generation techniques audioconditioned human video generation recently achieved significant breakthroughs naturalness motion synthesis portrait details due limited control audio signals driving human motion existing methods often add auxiliary spatial signals stabilize movements may compromise naturalness freedom motion paper propose endtoend audioonly conditioned video diffusion model named loopy specifically designed inter intraclip temporal module audiotolatents module enabling model leverage longterm motion information data learn natural motion patterns improving audioportrait movement correlation method removes need manually specified spatial motion templates used existing methods constrain motion inference extensive experiments show loopy outperforms recent audiodriven portrait diffusion models delivering lifelike highquality results across various scenarios,6,0.4442882888585556,6,0.4442882888585556
makeyouranchor diffusionbased avatar generation framework despite remarkable process talkingheadbased avatarcreating solutions directly generating anchorstyle videos fullbody motions remains challenging study propose makeyouranchor novel system necessitating oneminute video clip individual training subsequently enabling automatic generation anchorstyle videos precise torso hand movements specifically finetune proposed structureguided diffusion model input video render mesh conditions human appearances adopt twostage training strategy diffusion model effectively binding movements specific appearances produce arbitrary long temporal video extend unet framewise diffusion model style without additional training cost simple yet effective batchoverlapped temporal denoising module proposed bypass constraints video length inference finally novel identityspecific face enhancement module introduced improve visual quality facial regions output videos comparative experiments demonstrate effectiveness superiority system terms visual quality temporal coherence identity preservation outperforming sota diffusionnondiffusion methods project page urlhttpsgithubcomictmcgmakeyouranchor,-1,0.0,-1,0.0
beyond raw videos understanding edited videos large multimodal model emerging video lmms large multimodal models achieved significant improvements generic video understanding form vqa visual question answering raw videos captured cameras however large portion videos realworld applications edited videos textiteg users usually cut add effectsmodifications raw video publishing social media platforms edited videos usually high view counts covered existing benchmarks video lmms textitie activitynetqa videochatgpt benchmark paper leverage edited videos popular short video platform textitie tiktok build video vqa benchmark named editvidqa covering four typical editing categories ie effect funny meme game funny meme videos benchmark nuanced understanding highlevel reasoning effect game evaluate understanding capability artificial design opensource video lmms perform poorly editvidqa benchmark indicating huge domain gap edited short videos social media regular raw videos improve generalization ability lmms collect training set proposed benchmark based raw videos smallscale tiktokcapcut edited videos boosts performance proposed editvidqa benchmark indicating effectiveness highquality training data also identified serious issue existing evaluation protocol using judge namely sorry attack sorrystyle naive answer achieve extremely high rating gpt judge eg correctness score videochatgpt evaluation protocol avoid sorry attacks evaluate results judge keyword filtering dataset released httpsgithubcomxenonlambeditvidqa,0,1.0,0,1.0
unictrl improving spatiotemporal consistency texttovideo diffusion models via trainingfree unified attention control video diffusion models developed video generation usually integrating text image conditioning enhance control generated content despite progress ensuring consistency across frames remains challenge particularly using text prompts control conditions address problem introduce unictrl novel plugandplay method universally applicable improve spatiotemporal consistency motion diversity videos generated texttovideo models without additional training unictrl ensures semantic consistency across different frames crossframe selfattention control meanwhile enhances motion quality spatiotemporal consistency motion injection spatiotemporal synchronization experimental results demonstrate unictrls efficacy enhancing various texttovideo models confirming effectiveness universality,9,0.685521245983137,9,0.685521245983137
crossconditioned diffusion model medical image image translation multimodal magnetic resonance imaging mri provides rich complementary information analyzing diseases however practical challenges acquiring multiple mri modalities cost scan time safety considerations often result incomplete datasets affects quality diagnosis performance deep learning models trained data recent advancements generative adversarial networks gans denoising diffusion models shown promise natural medical imagetoimage translation tasks however complexity training gans computational expense associated diffusion models hinder development application task address issues introduce crossconditioned diffusion model cdm medical imagetoimage translation core idea cdm use distribution target modalities guidance improve synthesis quality achieving higher generation efficiency compared conventional diffusion models first propose modalityspecific representation model mrm model distribution target modalities design modalitydecoupled diffusion network mdn efficiently effectively learn distribution mrm finally crossconditioned unet cunet condition embedding module designed synthesize target modalities source modalities input target distribution guidance extensive experiments conducted upenngbm benchmark datasets demonstrate superiority method,3,0.6424546522796686,3,0.6424546522796686
neurosymbolic evaluation texttovideo models using formal verification recent advancements texttovideo models sora moviegen cogvideox pushing boundaries synthetic video generation adoption seen fields like robotics autonomous driving entertainment models become prevalent various metrics benchmarks emerged evaluate quality generated videos however metrics emphasize visual quality smoothness neglecting temporal fidelity texttovideo alignment crucial safetycritical applications address gap introduce neusv novel synthetic video evaluation metric rigorously assesses texttovideo alignment using neurosymbolic formal verification techniques approach first converts prompt formally defined temporal logic tl specification translates generated video automaton representation evaluates texttovideo alignment formally checking video automaton tl specification furthermore present dataset temporally extended prompts evaluate stateoftheart video generation models benchmark find neusv demonstrates higher correlation human evaluations compared existing metrics evaluation reveals current video generation models perform poorly temporally complex prompts highlighting need future work improving texttovideo generation capabilities,-1,0.0,-1,0.0
spectral motion alignment video motion transfer using diffusion models evolution diffusion models greatly impacted video generation understanding particularly texttovideo diffusion models vdms significantly facilitated customization input video target appearance motion etc despite advances challenges persist accurately distilling motion information video frames existing works leverage consecutive frame residual target motion vector inherently lack global motion context vulnerable framewise distortions address present spectral motion alignment sma novel framework refines aligns motion vectors using fourier wavelet transforms sma learns motion patterns incorporating frequencydomain regularization facilitating learning wholeframe global motion dynamics mitigating spatial artifacts extensive experiments demonstrate smas efficacy improving motion transfer maintaining computational efficiency compatibility across various video customization frameworks,9,0.685521245983137,9,0.685521245983137
ganesh generalizable nerf lensless imaging lensless imaging offers significant opportunity develop ultracompact cameras removing conventional bulky lens system however without focusing element sensors output longer direct image complex multiplexed scene representation traditional methods attempted address challenge employing learnable inversions refinement models methods primarily designed reconstruction generalize well reconstruction introduce ganesh novel framework designed enable simultaneous refinement novel view synthesis multiview lensless images unlike existing methods require scenespecific training approach supports onthefly inference without retraining scene moreover framework allows us tune model specific scenes enhancing rendering refinement quality facilitate research area also present first multiview lensless dataset lenslessscenes extensive experiments demonstrate method outperforms current approaches reconstruction accuracy refinement quality code video results available,-1,0.0,-1,0.0
genmac compositional texttovideo generation multiagent collaboration texttovideo generation models shown significant progress recent years however still struggle generating complex dynamic scenes based compositional text prompts attribute binding multiple objects temporal dynamics associated different objects interactions objects key motivation complex tasks decomposed simpler ones handled rolespecialized mllm agent multiple agents collaborate together achieve collective intelligence complex goals propose genmac iterative multiagent framework enables compositional texttovideo generation collaborative workflow includes three stages design generation redesign iterative loop generation redesign stages progressively verify refine generated videos redesign stage challenging stage aims verify generated videos suggest corrections redesign text prompts framewise layouts guidance scales next iteration generation avoid hallucination single mllm agent decompose stage four sequentiallyexecuted mllmbased agents verification agent suggestion agent correction agent output structuring agent furthermore tackle diverse scenarios compositional texttovideo generation design selfrouting mechanism adaptively select proper correction agent collection correction agents specialized one scenario extensive experiments demonstrate effectiveness genmac achieving stateofthe art performance compositional texttovideo generation,-1,0.0,-1,0.0
ivmixed sampler leveraging image diffusion models enhanced video synthesis multistep sampling mechanism key feature visual diffusion models significant potential replicate success openais strawberry enhancing performance increasing inference computational cost sufficient prior studies demonstrated correctly scaling computation sampling process successfully lead improved generation quality enhanced image editing compositional generalization rapid advancements developing inferenceheavy algorithms improved image generation relatively little work explored inference scaling laws video diffusion models vdms furthermore existing research shows minimal performance gains perceptible naked eye address design novel trainingfree algorithm ivmixed sampler leverages strengths image diffusion models idms assist vdms surpass current capabilities core ivmixed sampler use idms significantly enhance quality video frame vdms ensure temporal coherence video sampling process experiments demonstrated ivmixed sampler achieves stateoftheart performance benchmarks including msrvttfvd example opensource animatediff ivmixed sampler reduces umtfvd score closing closedsource,-1,0.0,-1,0.0
beyond gfvc progressive face video compression framework adaptive visual tokens recently deep generative models greatly advanced progress face video coding towards promising ratedistortion performance diverse application functionalities beyond traditional hybrid video coding paradigms generative face video compression gfvc relying strong capabilities deep generative models philosophy early modelbased coding mbc facilitate compact representation realistic reconstruction visual face signal thus achieving ultralow bitrate face video communication however gfvc algorithms sometimes faced unstable reconstruction quality limited bitrate ranges address problems paper proposes novel progressive face video compression framework namely pfvc utilizes adaptive visual tokens realize exceptional tradeoffs reconstruction robustness bandwidth intelligence particular encoder proposed pfvc projects highdimensional face signal adaptive visual tokens progressive manner whilst decoder reconstruct adaptive visual tokens motion estimation signal synthesis different granularity levels experimental results demonstrate proposed pfvc framework achieve better coding flexibility superior ratedistortion performance comparison latest versatile video coding vvc codec stateoftheart gfvc algorithms project page found,2,1.0,2,1.0
listen move improving gans coherency agnostic soundtovideo generation deep generative models demonstrated ability create realistic audiovisual content sometimes driven domains different nature however smooth temporal dynamics video generation challenging problem work focuses generic soundtovideo generation proposes three main features enhance image quality temporal coherency generative adversarial models triple sound routing scheme multiscale residual dilated recurrent network extended sound analysis novel recurrent directional convolutional layer video prediction proposed features improves quality coherency baseline neural architecture typically used sota video prediction layer providing extra temporal refinement,-1,0.0,-1,0.0
deformationaware gan medical image synthesis substantially misaligned pairs medical image synthesis generates additional imaging modalities costly invasive harmful acquire helps facilitate clinical workflow training pairs substantially misaligned eg lung mrict pairs respiratory motion accurate image synthesis remains critical challenge recent works explored directional registration module adjust misalignment generative adversarial networks gans however substantial misalignment lead suboptimal data mapping caused correspondence ambiguity degraded image fidelity caused morphology influence discriminators address challenges propose novel deformationaware gan dagan dynamically correct misalignment image synthesis based multiobjective inverse consistency specifically generative process three levels inverse consistency cohesively optimise symmetric registration image generation improved correspondence adversarial process improve image fidelity misalignment design deformationaware discriminators disentangle mismatched spatial morphology judgement image fidelity experimental results show dagan achieved superior performance public dataset simulated misalignments realworld lung mrict dataset respiratory motion misalignment results indicate potential wide range medical image synthesis tasks radiotherapy planning,3,0.7025205844300239,3,0.7025205844300239
scaling foundation models multimodal video understanding introduce new family video foundation models vifm achieve stateoftheart results video recognition videotext tasks videocentric dialogue core design progressive training approach unifies masked video modeling crossmodal contrastive learning next token prediction scaling video encoder size parameters data level prioritize spatiotemporal consistency semantically segmenting videos generating videoaudiospeech captions improves alignment video text extensive experiments validate designs demonstrate superior performance video audio tasks notably model outperforms others various videorelated dialogue long video understanding benchmarks highlighting ability reason comprehend longer contexts code models available,0,1.0,0,1.0
storyboard guided alignment finegrained video action recognition finegrained video action recognition conceptualized videotext matching problem previous approaches often rely global video semantics consolidate video embeddings lead misalignment videotext pairs due lack understanding action semantics atomic granularity level tackle challenge propose multigranularity framework based two observations videos different global semantics may share similar atomic actions appearances ii atomic actions within video momentary slow even nondirectly related global video semantics inspired concept storyboarding disassembles script individual shots enhance global video semantics generating finegrained descriptions using pretrained large language model detailed descriptions capture common atomic actions depicted videos filtering metric proposed select descriptions correspond atomic actions present videos descriptions employing global semantics finegrained descriptions identify key frames videos utilize aggregate embeddings thereby making embedding accurate extensive experiments various video action recognition datasets demonstrate superior performance proposed method supervised fewshot zeroshot settings,-1,0.0,-1,0.0
consistent controllable imagetovideo generation explicit motion modeling introduce novel framework consistent controllable imagetovideo generation contrast previous methods directly learn complicated imagetovideo mapping factorizes two stages explicit motion modeling first stage propose diffusionbased motion field predictor focuses deducing trajectories reference images pixels second stage propose motionaugmented temporal attention enhance limited temporal attention video latent diffusion models module effectively propagate reference images feature synthesized frames guidance predicted trajectories first stage compared existing methods generate consistent videos even presence large motion viewpoint variation training sparse trajectory controlnet first stage support users precisely control motion trajectories motion regions sparse trajectory region annotations offers controllability process solely relying textual instructions additionally second stage naturally supports zeroshot videotovideo translation qualitative quantitative comparisons demonstrate advantages prior approaches consistent controllable imagetovideo generation please see project page,-1,0.0,-1,0.0
towards detection aisynthesized human face images past years image generation manipulation achieved remarkable progress due rapid development generative ai based deep learning recent studies devoted significant efforts address problem face image manipulation caused deepfake techniques however problem detecting purely synthesized face images explored lesser extent particular recent popular diffusion models dms shown remarkable success image synthesis existing detectors struggle generalize synthesized images created different generative models work comprehensive benchmark including human face images produced generative adversarial networks gans variety dms established evaluate generalization ability robustness stateoftheart detectors forgery traces introduced different generative models analyzed frequency domain draw various insights paper demonstrates detector trained frequency representation generalize well unseen generative models,-1,0.0,-1,0.0
videoclusternet selfsupervised adaptive face clustering videos rise digital media content production need analyzing movies tv series episodes locate main cast characters precisely gaining importancespecifically video face clustering aims group together detected video face tracks common facial identities problem challenging due large range pose expression appearance lighting variations given face across video frames generic pretrained face identification id models fail adapt well video production domain given high dynamic range content also unique cinematic style furthermore traditional clustering algorithms depend hyperparameters requiring individual tuning across datasets paper present novel video face clustering approach learns adapt generic face id model new video face tracks fully selfsupervised fashion also propose parameterfree clustering algorithm capable automatically adapting finetuned models embedding space input video due lack comprehensive movie face clustering benchmarks also present firstofkind movie dataset moviefacecluster dataset handpicked film industry professionals contains extremely challenging face id scenarios experiments show methods effectiveness handling difficult mainstream movie scenes benchmark dataset stateoftheart performance traditional tv series datasets,-1,0.0,-1,0.0
step enhancing videollms compositional reasoning spatiotemporal graphguided selftraining video large language models videollms recently shown strong performance basic video understanding tasks captioning coarsegrained question answering struggle compositional reasoning requires multistep spatiotemporal inference across object relations interactions events hurdles enhancing capability include extensive manual labor lack spatiotemporal compositionality existing data absence explicit reasoning supervision paper propose step novel graphguided selftraining method enables videollms generate reasoningrich finetuning data raw videos improve specifically first induce spatiotemporal scene graph stsg representation diverse videos capture finegrained multigranular video semantics stsgs guide derivation multistep reasoning questionanswer qa data chainofthought cot rationales answers rationales integrated training objective aiming enhance models reasoning abilities supervision explicit reasoning steps experimental results demonstrate effectiveness step across models varying scales significant improvement tasks requiring three reasoning steps furthermore achieves superior performance minimal amount selfgenerated rationaleenriched training samples compositional reasoning comprehensive understanding benchmarks highlighting broad applicability vast potential,0,0.9985592620476745,0,0.9985592620476745
combining genre classification harmonicpercussive features diffusion models musicvideo generation study presents novel method generating music visualisers using diffusion models combining audio input userselected artwork process involves two main stages image generation video creation first music captioning genre classification performed followed retrieval artistic style descriptions diffusion model generates images based users input image derived artistic style descriptions video generation stage utilises diffusion model interpolate frames controlled audio energy vectors derived key musical features harmonics percussives method demonstrates promising results across various genres new metric audiovisual synchrony avs introduced quantitatively evaluate synchronisation visual audio elements comparative analysis shows significantly higher avs values videos generated using proposed method audio energy vectors compared linear interpolation approach potential applications diverse fields including independent music video creation film production live music events enhancing audiovisual experiences public spaces,8,0.6112585103606117,8,0.6112585103606117
robodreamer learning compositional world models robot imagination texttovideo models demonstrated substantial potential robotic decisionmaking enabling imagination realistic plans future actions well accurate environment simulation however one major issue models generalization models limited synthesizing videos subject language instructions similar seen training time heavily limiting decisionmaking seek powerful world model synthesize plans unseen combinations objects actions order solve previously unseen tasks new environments resolve issue introduce robodreamer innovative approach learning compositional world model factorizing video generation leverage natural compositionality language parse instructions set lowerlevel primitives condition set models generate videos illustrate factorization naturally enables compositional generalization allowing us formulate new natural language instruction combination previously seen components show factorization enables us add additional multimodal goals allowing us specify video wish generate given natural language instructions goal image approach successfully synthesize video plans unseen goals rtx enables successful robot execution simulation substantially outperforms monolithic baseline approaches video generation,5,0.3204267833590643,5,0.3204267833590643
denoising reuse exploiting interframe motion consistency efficient video latent generation video generation using diffusionbased models constrained high computational costs due framewise iterative diffusion process work presents diffusion reuse motion dr mo network accelerate latent video generation key discovery coarsegrained noises earlier denoising steps demonstrated high motion consistency across consecutive video frames following observation dr mo propagates coarsegrained noises onto next frame incorporating carefully designed lightweight interframe motions eliminating massive computational redundancy framewise diffusion models sensitive finegrained noises still acquired via later denoising steps essential retain visual qualities deciding intermediate steps switch motionbased propagations denoising crucial problem key tradeoff efficiency quality dr mo employs metanetwork named denoising step selector dss dynamically determine desirable intermediate steps across video frames extensive evaluations video generation editing tasks shown dr mo substantially accelerate diffusion models video tasks improved visual qualities,-1,0.0,-1,0.0
vidhalluc evaluating temporal hallucinations multimodal large language models video understanding multimodal large language models mllms recently shown significant advancements video understanding excelling content reasoning instructionfollowing tasks however problem hallucination models generate inaccurate misleading content remains underexplored video domain building observation visual encoder mllms often struggles differentiate video pairs visually distinct semantically similar introduce vidhalluc largest benchmark designed examine hallucinations mllms video understanding tasks vidhalluc assesses hallucinations across three critical dimensions action temporal sequence scene transition vidhalluc consists videos paired based semantic similarity visual differences focusing cases hallucinations likely occur comprehensive testing experiments show mllms vulnerable hallucinations across dimensions furthermore propose dinoheal trainingfree method reduces hallucinations incorporating spatial saliency information reweight visual features inference results demonstrate dinoheal consistently improves performance vidhalluc achieving average improvement mitigating hallucinations among tasks vidhalluc benchmark dinoheal code accessed via hrefhttpsvidhallucgithubiotextthis link,-1,0.0,-1,0.0
fada fast diffusion avatar synthesis mixedsupervised multicfg distillation diffusionbased audiodriven talking avatar methods recently gained attention highfidelity vivid expressive results however slow inference speed limits practical applications despite development various distillation techniques diffusion models found naive diffusion distillation methods yield satisfactory results distilled models exhibit reduced robustness openset input images decreased correlation audio video compared teacher models undermining advantages diffusion models address propose fada fast diffusion avatar synthesis mixedsupervised multicfg distillation first designed mixedsupervised loss leverage data varying quality enhance overall model capability well robustness additionally propose multicfg distillation learnable tokens utilize correlation audio reference image conditions reducing threefold inference runs caused multicfg acceptable quality degradation extensive experiments across multiple datasets show fada generates vivid videos comparable recent diffusion modelbased methods achieving nfe speedup times demos available webpage httpfadavatargithubio,11,0.9670946838379297,11,0.9670946838379297
unified editing panorama scenes videos disentangled selfattention injection texttoimage models achieved impressive capabilities image generation editing application across various modalities often necessitates training separate models inspired existing method single image editing self attention injection video editing shared attention propose novel unified editing framework combines strengths approaches utilizing basic image texttoimage diffusion model specifically design sampling method facilitates editing consecutive images maintaining semantic consistency utilizing shared selfattention features reference consecutive image sampling processes experimental results confirm method enables editing across diverse modalities including scenes videos panorama images,9,1.0,9,1.0
diffusionpromoted hdr video reconstruction high dynamic range hdr video reconstruction aims generate hdr videos low dynamic range ldr frames captured alternating exposures existing works solely rely regressionbased paradigm leading adverse effects ghosting artifacts missing details saturated regions paper propose diffusionpromoted method hdr video reconstruction termed hdrvdiff incorporates diffusion model capture hdr distribution hdrvdiff reconstruct hdr videos realistic details alleviating ghosting artifacts however direct introduction video diffusion models would impose massive computational burden instead alleviate burden first propose hdr latent diffusion model hdrldm learn distribution prior single hdr frames specifically hdrldm incorporates tonemapping strategy compress hdr frames latent space novel exposure embedding aggregate exposure information diffusion process propose temporalconsistent alignment module tcam learn temporal information complement hdrldm conducts coarsetofine feature alignment different scales among video frames finally design zeroinit crossattention zica mechanism effectively integrate learned distribution prior temporal information generating hdr frames extensive experiments validate hdrvdiff achieves stateoftheart results several representative datasets,-1,0.0,-1,0.0
diffpano scalable consistent text panorama generation spherical epipolaraware diffusion diffusionbased methods achieved remarkable achievements image object generation however generation scenes even images remains constrained due limited number scene datasets complexity scenes difficulty generating consistent multiview images address issues first establish largescale panoramic videotext dataset containing millions consecutive panoramic keyframes corresponding panoramic depths camera poses text descriptions propose novel textdriven panoramic generation framework termed diffpano achieve scalable consistent diverse panoramic scene generation specifically benefiting powerful generative capabilities stable diffusion finetune singleview texttopanorama diffusion model lora established panoramic videotext dataset design spherical epipolaraware multiview diffusion model ensure multiview consistency generated panoramic images extensive experiments demonstrate diffpano generate scalable consistent diverse panoramic images given unseen text descriptions camera poses,1,1.0,1,1.0
nerfnqa noreference quality assessment scenes generated nerf neural view synthesis methods neural view synthesis nvs demonstrated efficacy generating highfidelity dense viewpoint videos using image set sparse views however existing quality assessment methods like psnr ssim lpips tailored scenes dense viewpoints synthesized nvs nerf variants thus often fall short capturing perceptual quality including spatial angular aspects nvssynthesized scenes furthermore lack dense ground truth views makes full reference quality assessment nvssynthesized scenes challenging instance datasets llff provide sparse images insufficient complete fullreference assessments address issues propose nerfnqa first noreference quality assessment method denselyobserved scenes synthesized nvs nerf variants nerfnqa employs joint quality assessment strategy integrating viewwise pointwise approaches evaluate quality nvsgenerated scenes viewwise approach assesses spatial quality individual synthesized view overall interviews consistency pointwise approach focuses angular qualities scene surface points compound interpoint quality extensive evaluations conducted compare nerfnqa mainstream visual quality assessment methods fields image video lightfield assessment results demonstrate nerfnqa outperforms existing assessment methods significantly shows substantial superiority assessing nvssynthesized scenes without references implementation paper available httpsgithubcomvincentqqunerfnqa,12,1.0,12,1.0
video generation consistency tuning currently various studies exploring generation long videos however generated frames videos often exhibit jitter noise therefore order generate videos without noise propose novel framework composed four modules separate tuning module average fusion module combined tuning module interframe consistency module applying newly proposed modules subsequently consistency background foreground video frames optimized besides experimental results demonstrate videos generated method exhibit high quality comparison stateoftheart methods,-1,0.0,-1,0.0
ltxvideo realtime video latent diffusion introduce ltxvideo transformerbased latent diffusion model adopts holistic approach video generation seamlessly integrating responsibilities videovae denoising transformer unlike existing methods treat components independent ltxvideo aims optimize interaction improved efficiency quality core carefully designed videovae achieves high compression ratio spatiotemporal downscaling x x pixels per token enabled relocating patchifying operation transformers input vaes input operating highly compressed latent space enables transformer efficiently perform full spatiotemporal selfattention essential generating highresolution videos temporal consistency however high compression inherently limits representation fine details address vae decoder tasked latenttopixel conversion final denoising step producing clean result directly pixel space approach preserves ability generate fine details without incurring runtime cost separate upsampling module model supports diverse use cases including texttovideo imagetovideo generation capabilities trained simultaneously achieves fasterthanrealtime generation producing seconds fps video resolution seconds nvidia gpu outperforming existing models similar scale source code pretrained models publicly available setting new benchmark accessible scalable video generation,11,0.9235240508025361,11,0.9235240508025361
masked generative videotoaudio transformers enhanced synchronicity videotoaudio generation leverages visualonly video features render plausible sounds match scene importantly generated sound onsets match visual actions aligned otherwise unnatural synchronization artifacts arise recent works explored progression conditioning sound generators still images video features focusing quality semantic matching ignoring synchronization sacrificing amount quality focus improving synchronization work propose generative model named maskvat interconnects fullband highquality general audio codec sequencetosequence masked generative model combination allows modeling high audio quality semantic matching temporal synchronicity time results show combining highquality codec proper pretrained audiovisual features sequencetosequence parallel structure able yield highly synchronized results one hand whilst competitive state art noncodec generative audio models sample videos generated audios available httpsmaskvatgithubio,8,1.0,8,1.0
adapting imagetovideo diffusion models largemotion frame interpolation development video generation models advanced significantly recent years adopt largescale imagetovideo diffusion models video frame interpolation present conditional encoder designed adapt imagetovideo model largemotion frame interpolation enhance performance integrate dualbranch feature extractor propose crossframe attention mechanism effectively captures spatial temporal information enabling accurate interpolations intermediate frames approach demonstrates superior performance frechet video distance fvd metric evaluated stateoftheart approaches particularly handling large motion scenarios highlighting advancements generativebased methodologies,11,0.9235240508025361,11,0.9235240508025361
speechguided diffusion model realtime mri video vocal tract speech understanding speech production visually kinematically inform second language learning system designs well creation speaking characters video games animations work introduce datadriven method visually represent articulator motion magnetic resonance imaging mri videos human vocal tract speech based arbitrary audio speech input leverage large pretrained speech models embedded prior knowledge generalize visual domain unseen data using speechtovideo diffusion model findings demonstrate visual generation significantly benefits pretrained speech representations also observed evaluating phonemes isolation challenging becomes straightforward assessed within context spoken words limitations current results include presence unsmooth tongue motion video distortion tongue contacts palate,-1,0.0,-1,0.0
motionstone decoupled motion intensity modulation diffusion transformer imagetovideo generation imagetovideo generation conditioned static image enhanced recently motion intensity additional control signal motionaware models appealing generate diverse motion patterns yet lacks reliable motion estimator training models largescale video set wild traditional metrics eg ssim optical flow hard generalize arbitrary videos tough human annotators label abstract motion intensity neither furthermore motion intensity shall reveal local object motion global camera movement studied paper addresses challenge new motion estimator capable measuring decoupled motion intensities objects cameras video leverage contrastive learning randomly paired videos distinguish video greater motion intensity paradigm friendly annotation easy scale achieve stable performance motion estimation present new model named motionstone developed decoupled motion estimator experimental results demonstrate stability proposed motion estimator stateoftheart performance motionstone generation advantages warrant decoupled motion estimator serve general plugin enhancer data processing video generation training,-1,0.0,-1,0.0
scaling video summarization pretraining large language models longform video content constitutes significant portion internet traffic making automated video summarization essential research problem however existing video summarization datasets notably limited size constraining effectiveness stateoftheart methods generalization work aims overcome limitation capitalizing abundance longform videos dense speechtovideo alignment remarkable capabilities recent large language models llms summarizing long text introduce automated scalable pipeline generating largescale video summarization dataset using llms oracle summarizers leveraging generated dataset analyze limitations existing approaches propose new video summarization model effectively addresses facilitate research field work also presents new benchmark dataset contains long videos highquality summaries annotated professionals extensive experiments clearly indicate proposed approach sets new stateoftheart video summarization across several benchmarks,0,1.0,0,1.0
wdm wavelet diffusion models highresolution medical image synthesis due threedimensional nature ct mrscans generative modeling medical images particularly challenging task existing approaches mostly apply patchwise slicewise cascaded generation techniques fit highdimensional data limited gpu memory however approaches may introduce artifacts potentially restrict models applicability certain downstream tasks work presents wdm waveletbased medical image synthesis framework applies diffusion model wavelet decomposed images presented approach simple yet effective way scaling diffusion models high resolutions trained single gpu experimental results brats lidcidri unconditional image generation resolution times times demonstrate stateoftheart image fidelity fid sample diversity msssim scores compared recent gans diffusion models latent diffusion models proposed method one capable generating highquality images resolution times times outperforming comparing methods,3,0.5226724514858553,3,0.5226724514858553
learning scalable generative models video diffusion models paper presents novel method building scalable generative models utilizing pretrained video diffusion models primary obstacle developing foundation generative models limited availability data unlike images texts videos data readily accessible difficult acquire results significant disparity scale compared vast quantities types data address issue propose using video diffusion model trained extensive volumes text images videos knowledge source data unlocking multiview generative capabilities finetuning generate largescale synthetic multiview dataset train feedforward generative model proposed model trained nearly synthetic multiview data generate asset single image seconds achieves superior performance compared current sota feedforward generative models users preferring results time,-1,0.0,-1,0.0
dualpath collaborative generation network emotional video captioning emotional video captioning emerging task aims describe factual content intrinsic emotions expressed videos essential evc task effectively perceive subtle ambiguous visual emotional cues caption generation neglected traditional video captioning existing emotional video captioning methods perceive global visual emotional cues first combine video features guide emotional caption generation neglects two characteristics evc task firstly methods neglect dynamic subtle changes intrinsic emotions video makes difficult meet needs common scenes diverse changeable emotions secondly methods incorporate emotional cues step guidance role emotion overemphasized makes factual content less ignored generation end propose dualpath collaborative generation network dynamically perceives visual emotional cues evolutions generating emotional captions collaborative learning specifically dynamic emotion perception path propose dynamic emotion evolution module first aggregates visual features historical caption features summarize global visual emotional cues dynamically selects emotional cues required recomposed stage besides adaptive caption generation path balance description factual content emotional cues propose emotion adaptive decoder thus methods generate emotionrelated words necessary time step caption generation balances guidance factual content emotional cues well extensive experiments three challenging datasets demonstrate superiority approach proposed module,-1,0.0,-1,0.0
flexcache flexible approximate cache system video diffusion texttovideo applications receive increasing attention public among diffusion models emerged prominent approach offering impressive quality visual content generation however still suffers substantial computational complexity often requiring several minutes generate single video prior research addressed computational overhead texttoimage diffusion models techniques developed directly suitable video diffusion models due significantly larger cache requirements enhanced computational demands associated video generation present flexcache flexible approximate cache system addresses challenges two main designs first compress caches saving storage compression strategy reduce times consumption average find approximate cache system achieve higher hit rate computation savings decoupling object background design tailored cache replacement policy support two techniques mentioned better evaluation flexcache reaches times higher throughput lower cost compared stateoftheart diffusion approximate cache system,2,0.7611654677354586,2,0.7611654677354586
customttt motion appearance customized video generation via testtime training benefiting largescale pretraining textvideo pairs current texttovideo diffusion models generate highquality videos text description besides given reference images videos parameterefficient finetuning method ie lora generate highquality customized concepts eg specific subject motions reference video however combining trained multiple concepts different references single network shows obvious artifacts end propose customttt joint custom appearance motion given video easily detail first analyze prompt influence current video diffusion model find loras needed specific layers appearance motion customization besides since lora trained individually propose novel testtime training technique update parameters combination utilizing trained customized models conduct detailed experiments verify effectiveness proposed methods method outperforms several stateoftheart works qualitative quantitative evaluations,2,0.6860564220557998,2,0.6860564220557998
tweediemix improving multiconcept fusion diffusionbased imagevideo generation despite significant advancements customizing texttoimage video generation models generating images videos effectively integrate multiple personalized concepts remains challenging task address present tweediemix novel method composing customized diffusion models inference phase analyzing properties reverse diffusion sampling approach divides sampling process two stages initial steps apply multiple objectaware sampling technique ensure inclusion desired target objects later steps blend appearances custom concepts denoised image space using tweedies formula results demonstrate tweediemix generate multiple personalized concepts higher fidelity existing methods moreover framework effortlessly extended imagetovideo diffusion models enabling generation videos feature multiple personalized concepts results source code anonymous project page,-1,0.0,-1,0.0
improving video understanding generation better captions present series aiming facilitate video understanding large videolanguage models lvlms video generation texttovideo models via dense precise captions series comprises annotated dense captions videos various lengths sources developed carefully designed data filtering annotating strategy sharecaptionervideo efficient capable captioning model arbitrary videos highquality aesthetic videos annotated simple yet superb lvlm reached sota performance three advancing video benchmarks achieve taking aside nonscalable costly human annotators find using caption video naive multiframe frameconcatenation input strategy leads less detailed sometimes temporalconfused results argue challenge designing highquality video captioning strategy lies three aspects interframe precise temporal change understanding intraframe detailed content description framenumber scalability arbitrarylength videos end meticulously designed differential video captioning strategy stable scalable efficient generating captions videos arbitrary resolution aspect ratios length based construct contains highquality videos spanning wide range categories resulting captions encompass rich world knowledge object attributes camera movements crucially detailed precise temporal descriptions events based develop sharecaptionervideo superior captioner capable efficiently generating highquality captions arbitrary videos,0,1.0,0,1.0
dive ditbased video generation enhanced control generating highfidelity temporally consistent videos autonomous driving scenarios faces significant challenge eg problematic maneuvers corner cases despite recent video generation works proposed tackcle mentioned problem ie models built top diffusion transformers dit works still missing targeted exploring potential multiview videos generation scenarios noticeably propose first ditbased framework specifically designed generating temporally multiview consistent videos precisely match given birdseye view layouts control specifically proposed framework leverages parameterfree spatial viewinflated attention mechanism guarantee crossview consistency joint crossattention modules controlnettransformer integrated improve precision control demonstrate advantages extensively investigate qualitative comparisons nuscenes dataset particularly challenging corner cases summary effectiveness proposed method producing long controllable highly consistent videos difficult conditions proven effective,-1,0.0,-1,0.0
using diffusion priors video amodal segmentation object permanence humans fundamental cue helps understanding persistence objects even fully occluded scene present day methods object segmentation account amodal nature world work segmentation visible modal objects amodal methods exist singleimage segmentation methods handle highlevels occlusions better inferred using temporal information multiframe methods focused solely segmenting rigid objects end propose tackle video amodal segmentation formulating conditional generation task capitalizing foundational knowledge video generative models method simple repurpose models condition sequence modal mask frames object along contextual pseudodepth maps learn object boundary may occluded therefore extended hallucinate complete extent object followed content completion stage able inpaint occluded regions object benchmark approach alongside wide array stateoftheart methods four datasets show dramatic improvement upto amodal segmentation objects occluded region,-1,0.0,-1,0.0
odvista omnidirectional video dataset superresolution quality enhancement tasks omnidirectional video increasingly deployed largely due latest advancements immersive virtual reality vr extended reality xr technology however adoption videos streaming encounters challenges related bandwidth latency particularly mobility conditions unmanned aerial vehicles uavs adaptive resolution compression aim preserve quality maintaining low latency constraints yet downscaling encoding still degrade quality introduce artifacts machine learning mlbased superresolution sr quality enhancement techniques offer promising solution enhancing detail recovery reducing compression artifacts however current publicly available video sr datasets lack compression artifacts limit research field bridge gap paper introduces omnidirectional video streaming dataset odvista comprises highresolution high quality videos downscaled encoded four bitrate ranges using highefficiency video coding standard evaluations show dataset features wide variety scenes also spans different levels content complexity crucial robust solutions perform well realworld scenarios generalize across diverse visual environments additionally evaluate performance considering quality enhancement runtime two handcrafted two mlbased sr models validation testing sets odvista,-1,0.0,-1,0.0
hawk learning understand openworld video anomalies video anomaly detection vad systems autonomously monitor identify disturbances reducing need manual labor associated costs however current vad systems often limited superficial semantic understanding scenes minimal user interaction additionally prevalent data scarcity existing datasets restricts applicability openworld scenarios paper introduce hawk novel framework leverages interactive large visual language models vlm interpret video anomalies precisely recognizing difference motion information abnormal normal videos hawk explicitly integrates motion modality enhance anomaly identification reinforce motion attention construct auxiliary consistency loss within motion video space guiding video branch focus motion modality moreover improve interpretation motiontolanguage establish clear supervisory relationship motion linguistic representation furthermore annotated anomaly videos language descriptions enabling effective training across diverse openworld scenarios also created questionanswering pairs users openworld questions final results demonstrate hawk achieves sota performance surpassing existing baselines video description generation questionanswering codesdatasetdemo released httpsgithubcomjqtangusthawk,0,0.8608360296357914,0,0.8608360296357914
llmenhanced world models diverse driving video generation world models demonstrated superiority autonomous driving particularly generation multiview driving videos however significant challenges still exist generating customized driving videos paper propose builds upon framework drivedreamer incorporates large language model llm generate userdefined driving videos specifically llm interface initially incorporated convert users query agent trajectories subsequently hdmap adhering traffic regulations generated based trajectories ultimately propose unified multiview model enhance temporal spatial coherence generated driving videos first world model generate customized driving videos generate uncommon driving videos eg vehicles abruptly cut userfriendly manner besides experimental results demonstrate generated videos enhance training driving perception methods eg detection tracking furthermore video generation quality surpasses stateoftheart methods showcasing fid fvd scores representing relative improvements,16,1.0,16,1.0
vimo generating motions casual videos although humans innate ability imagine multiple possible actions videos remains extraordinary challenge computers due intricate camera movements montages existing motion generation methods predominantly rely manually collected motion datasets usually tediously sourced motion capture mocap systems multiview cameras unavoidably resulting limited size severely undermines generalizability inspired recent advance diffusion models probe simple effective way capture motions videos propose novel videotomotiongeneration framework vimo could leverage immense trove untapped video content produce abundant diverse human motions distinct prior work videos could causal including complicated camera movements occlusions striking experimental results demonstrate proposed model could generate natural motions even videos rapid movements varying perspectives frequent occlusions might exist also show work could enable three important downstream applications generating dancing motions according arbitrary music source video style extensive experimental results prove model offers effective scalable way generate diversity realistic motions code demos public soon,18,1.0,18,1.0
followyourcanvas higherresolution video outpainting extensive content generation paper explores higherresolution video outpainting extensive content generation point common issues faced existing methods attempting largely outpaint videos generation lowquality content limitations imposed gpu memory address challenges propose diffusionbased method called textitfollowyourcanvas builds upon two core designs first instead employing common practice singleshot outpainting distribute task across spatial windows seamlessly merge allows us outpaint videos size resolution without constrained gpu memory second source video relative positional relation injected generation process window makes generated spatial layout within window harmonize source video coupling two designs enables us generate higherresolution outpainting videos rich content keeping spatial temporal consistency followyourcanvas excels largescale video outpainting eg producing highquality aesthetically pleasing results achieves best quantitative results across various resolution scale setups code released httpsgithubcommayuelalafollowyourcanvas,-1,0.0,-1,0.0
loopanimate loopable salient object animation research diffusion modelbased video generation advanced rapidly however limitations object fidelity generation length hinder practical applications additionally specific domains like animated wallpapers require seamless looping first last frames video match seamlessly address challenges paper proposes loopanimate novel method generating videos consistent start end frames enhance object fidelity introduce framework decouples multilevel image appearance textual semantic information building upon imagetoimage diffusion model approach incorporates pixellevel featurelevel information input image injecting image appearance textual semantic embeddings different positions diffusion model existing unetbased video generation models require input entire videos training encode temporal positional information however due limitations gpu memory number frames typically restricted address paper proposes threestage training strategy progressively increasing frame numbers reducing finetuning modules additionally introduce temporal e nhanced motion moduletemm extend capacity encoding temporal positional information frames proposed loopanimate first time extends singlepass generation length unetbased video generation models frames maintaining highquality video generation experiments demonstrate loopanimate achieves stateoftheart performance objective metrics fidelity temporal consistency subjective evaluation results,-1,0.0,-1,0.0
stereotalker audiodriven human synthesis priorguided mixtureofexperts paper introduces stereotalker novel oneshot audiodriven human video synthesis system generates talking videos precise lip synchronization expressive body gestures temporally consistent photorealistic quality continuous viewpoint control process follows twostage approach first stage system maps audio input highfidelity motion sequences encompassing upperbody gestures facial expressions enrich motion diversity authenticity large language model llm priors integrated textaligned semantic audio features leveraging llms crossmodal generalization power enhance motion quality second stage improve diffusionbased video generation models incorporating priorguided mixtureofexperts moe mechanism viewguided moe focuses viewspecific attributes maskguided moe enhances regionbased rendering stability additionally mask prediction module devised derive human masks motion data enhancing stability accuracy masks enabling mask guiding inference also introduce comprehensive human video dataset identities covering diverse body gestures detailed annotations facilitating broad generalization code data pretrained models released research purposes,6,0.5841282035730899,6,0.5841282035730899
finevq finegrained user generated content video quality assessment rapid growth usergenerated content ugc videos produced urgent need effective video quality assessment vqa algorithms monitor video quality guide optimization recommendation procedures however current vqa models generally give overall rating ugc video lacks finegrained labels serving video processing recommendation applications address challenges promote development ugc videos establish first largescale finegrained video quality assessment database termed finevd comprises ugc videos finegrained quality scores descriptions across multiple dimensions based database propose finegrained video quality assessment finevq model learn finegrained quality ugc videos capabilities quality rating quality scoring quality attribution extensive experimental results demonstrate proposed finevq produce finegrained videoquality results achieve stateoftheart performance finevd commonly used ugcvqa datasets finevd finevq made publicly available,12,1.0,12,1.0
step differences instructional video comparing user video reference howto video key requirement arvr technology delivering personalized assistance tailored users progress however current approaches languagebased assistance answer questions single video propose approach first automatically generates large amounts visual instruction tuning data involving pairs videos leveraging existing step annotations accompanying narrations trains videoconditioned language model jointly reason across multiple raw videos model achieves stateoftheart performance identifying differences video pairs ranking videos based severity differences shows promising ability perform general reasoning multiple videos project page httpsgithubcomfacebookresearchstepdiff,0,1.0,0,1.0
navero unlocking finegrained semantics videolanguage compositionality study capability videolanguage vidl models understanding compositions objects attributes actions relations composition understanding becomes particularly challenging video data since compositional relations rapidly change time videos first build benchmark named aaro evaluate composition understanding related actions top spatial concepts benchmark constructed generating negative texts incorrect action descriptions given video model expected pair positive text corresponding video furthermore propose training method called navero utilizes videotext data augmented negative texts enhance composition understanding also develop negativeaugmented visuallanguage matching loss used explicitly benefit generated negative text compare navero stateoftheart methods terms compositional understanding well videotext retrieval performance navero achieves significant improvement methods videolanguage imagelanguage composition understanding maintaining strong performance traditional textvideo retrieval tasks,0,1.0,0,1.0
textconditioned hdr image generation realtime ondevice video portrait relighting paper present approach realtime video portrait relighting mobile devices utilizing textconditioned generation high dynamic range image hdri maps method proposes diffusionbased image generation hdr domain taking advantage standard technique facilitates generation highquality realistic lighting conditions textual descriptions offering flexibility control portrait video relighting task unlike previous relighting frameworks proposed system performs video relighting directly ondevice enabling realtime inference real hdri maps ondevice processing ensures privacy guarantees low runtime providing immediate response changes lighting conditions user inputs approach paves way new possibilities realtime video applications including video conferencing gaming augmented reality allowing dynamic textbased control lighting conditions,1,1.0,1,1.0
trackgo flexible efficient method controllable video generation recent years seen substantial progress diffusionbased controllable video generation however achieving precise control complex scenarios including finegrained object parts sophisticated motion trajectories coherent background movement remains challenge paper introduce trackgo novel approach leverages freeform masks arrows conditional video generation method offers users flexible precise mechanism manipulating video content also propose trackadapter control implementation efficient lightweight adapter designed seamlessly integrated temporal selfattention layers pretrained video generation model design leverages observation attention map layers accurately activate regions corresponding motion videos experimental results demonstrate new approach enhanced trackadapter achieves stateoftheart performance key metrics fvd fid objmc scores,-1,0.0,-1,0.0
artificial intelligence biomedical video generation prominent subfield artificial intelligence generated content aigc video generation achieved notable advancements recent years introduction soraalike models represents pivotal breakthrough video generation technologies significantly enhancing quality synthesized videos particularly realm biomedicine video generation technology shown immense potential medical concept explanation disease simulation biomedical data augmentation article thoroughly examine latest developments video generation models explore applications challenges future opportunities biomedical sector conducted extensive review compiled comprehensive list datasets various sources facilitate development evaluation video generative models biomedicine given rapid progress field also created github repository regularly update advances biomedical video generation,10,0.9775848203496618,10,0.9775848203496618
intragen trajectorycontrolled video generation object interactions advances video generation significantly improved realism quality created scenes fueled interest developing intuitive tools let users leverage video generation world simulators texttovideo generation one approach enabling video creation text descriptions yet due inherent ambiguity texts limited temporal information offered text prompts researchers explored additional control signals like trajectoryguided systems accurate generation nonetheless methods evaluate whether models generate realistic interactions multiple objects lacking introduce intragen pipeline improved trajectorybased generation object interaction scenarios propose new datasets novel trajectory quality metric evaluate performance proposed intragen achieve object interaction introduce multimodal interaction encoding pipeline object id injection mechanism enriches objectenvironment interactions results demonstrate improvements visual fidelity quantitative performance code datasets available httpsgithubcominsaitinstituteintragen,-1,0.0,-1,0.0
video diffusion models trainingfree motion interpreter controller video generation primarily aims model authentic customized motion across frames making understanding controlling motion crucial topic diffusionbased studies video motion focus motion customization trainingbased paradigms however demands substantial training resources necessitates retraining diverse models crucially approaches explore video diffusion models encode crossframe motion information features lacking interpretability transparency effectiveness answer question paper introduces novel perspective understand localize manipulate motionaware features video diffusion models analysis using principal component analysis pca work discloses robust motionaware feature already exists video diffusion models present new motion feature moft eliminating content correlation information filtering motion channels moft provides distinct set benefits including ability encode comprehensive motion information clear interpretability extraction without need training generalizability across diverse architectures leveraging moft propose novel trainingfree video motion control framework method demonstrates competitive performance generating natural faithful motion providing architectureagnostic insights applicability variety downstream tasks,9,0.7411487847543149,9,0.7411487847543149
kinetic typography diffusion model paper introduces method realistic kinetic typography generates userpreferred animatable text content draw recent advances guided video diffusion models achieve visuallypleasing text appearances first construct kinetic typography dataset comprising videos dataset made variety combinations templates designed professional motion graphics designers involves changing letters position glyph size ie flying glitches chromatic aberration reflecting effects etc next propose video diffusion model kinetic typography three requirements aesthetic appearances motion effects readable letters paper identifies requirements present static dynamic captions used spatial temporal guidance video diffusion model respectively static caption describes overall appearance video colors texture glyph represent shape letter dynamic caption accounts movements letters backgrounds add one guidance zero convolution determine text content visible video apply zero convolution text content impose diffusion model lastly glyph loss minimizing difference predicted word groundtruth proposed make prediction letters readable experiments show model generates kinetic typography videos legible artistic letter motions based text prompts,-1,0.0,-1,0.0
seeclear semantic distillation enhances pixel condensation video superresolution diffusionbased video superresolution vsr renowned generating perceptually realistic videos yet grapples maintaining detail consistency across frames due stochastic fluctuations traditional approach pixellevel alignment ineffective diffusionprocessed frames iterative disruptions overcome introduce seecleara novel vsr framework leveraging conditional video generation orchestrated instancecentric channelwise semantic controls framework integrates semantic distiller pixel condenser synergize extract upscale semantic details lowresolution frames instancecentric alignment module incam utilizes videoclipwise tokens dynamically relate pixels within across frames enhancing coherency additionally channelwise texture aggregation memory category infuses extrinsic knowledge capitalizing longstanding semantic textures method also innovates blurring diffusion process resshift mechanism finely balancing sharpness diffusion effects comprehensive experiments confirm frameworks advantage stateoftheart diffusionbased vsr techniques code available,-1,0.0,-1,0.0
reinforcement learningbased automatic video editing method using pretrained visionlanguage model era videos automatic video editing techniques attract attention industry academia since reduce workloads lower requirements human editors existing automatic editing systems mainly scene eventspecific eg soccer game broadcasting yet automatic systems general editing eg movie vlog editing covers various scenes events rarely studied converting eventdriven editing method general scene nontrivial paper propose twostage scheme general editing firstly unlike previous works extract scenespecific features leverage pretrained visionlanguage model vlm extract editingrelevant representations editing context moreover close gap professionallooking videos automatic productions generated simple guidelines propose reinforcement learning rlbased editing framework formulate editing problem train virtual editor make better sequential editing decisions finally evaluate proposed method general editing task real movie dataset experimental results demonstrate effectiveness benefits proposed context representation learning ability rlbased editing framework,-1,0.0,-1,0.0
moditalker motiondisentangled diffusion model highfidelity talking head generation conventional ganbased models talking head generation often suffer limited quality unstable training recent approaches based diffusion models aimed address limitations improve fidelity however still face challenges including extensive sampling times difficulties maintaining temporal consistency due high stochasticity diffusion models overcome challenges propose novel motiondisentangled diffusion model highquality talking head generation dubbed moditalker introduce two modules audiotomotion atom designed generate synchronized lip motion audio motiontovideo mtov designed produce highquality head video following generated motion atom excels capturing subtle lip movements leveraging audio attention mechanism addition mtov enhances temporal consistency leveraging efficient triplane representation experiments conducted standard benchmarks demonstrate model achieves superior performance compared existing models also provide comprehensive ablation studies user study results,6,0.6181310795486454,6,0.6181310795486454
contextaware video anomaly detection longterm datasets video anomaly detection research generally evaluated short isolated benchmark videos minutes long however realworld environments security cameras observe scene months years time notion anomalous behavior critically depends context time day day week schedule events propose contextaware video anomaly detection algorithm trinity specifically targeted scenarios trinity especially wellsuited crowded scenes individuals easily tracked anomalies due speed direction absence group motion trinity contrastive learning framework aims learn alignments context appearance motion uses alignment quality classify videos normal anomalous evaluate algorithm conventional benchmarks public webcambased dataset collected spans three months activity,-1,0.0,-1,0.0
genad generalized predictive model autonomous driving paper introduce first largescale video prediction model autonomous driving discipline eliminate restriction highcost data collection empower generalization ability model acquire massive data web pair diverse highquality text descriptions resultant dataset accumulates hours driving videos spanning areas world diverse weather conditions traffic scenarios inheriting merits recent latent diffusion models model dubbed genad handles challenging dynamics driving scenes novel temporal reasoning blocks showcase generalize various unseen driving datasets zeroshot manner surpassing general drivingspecific video prediction counterparts furthermore genad adapted actionconditioned prediction model motion planner holding great potential realworld driving applications,-1,0.0,-1,0.0
generative expansion small datasets expansive graph approach limited data availability machine learning significantly impacts performance generalization traditional augmentation methods enhance moderately sufficient datasets gans struggle convergence generating diverse samples diffusion models effective high computational costs introduce expansive synthesis model generating largescale informationrich datasets minimal samples uses expander graph mappings feature interpolation preserve data distribution feature relationships model leverages neural networks nonlinear latent space captured koopman operator create linear feature space dataset expansion autoencoder selfattention layers optimal transport refines distributional consistency validate comparing classifiers trained generated data trained original datasets results show comparable performance demonstrating models potential augment training data effectively work advances data generation addressing scarcity machine learning applications,-1,0.0,-1,0.0
animating model multiview video diffusion recent advances generation mainly focus generating content distilling pretrained text singleview imageconditioned models inconvenient take advantage various offtheshelf assets multiview attributes results suffer spatiotemporal inconsistency owing inherent ambiguity supervision signals work present novel framework animating static model core idea twofold propose novel multiview video diffusion model mvvdm conditioned multiview renderings static object trained presented largescale multiview video dataset mvvideo based mvvdm introduce framework combining reconstruction score distillation sampling leverage multiview video diffusion priors animating objects specifically mvvdm design new spatiotemporal attention module enhance spatial temporal consistency integrating video diffusion models additionally leverage static models multiview renderings conditions preserve identity animating models effective twostage pipeline proposed first reconstruct motions directly generated multiview videos followed introduced refine appearance motion benefiting accurate motion learning could achieve straightforward mesh animation qualitative quantitative experiments demonstrate significantly outperforms previous approaches data code models openreleased,-1,0.0,-1,0.0
nuclass net novel approach video quality enhancement video content experienced surge popularity asserting dominance internet traffic internet things iot networks video compression long regarded primary means efficiently managing substantial multimedia traffic generated videocapturing devices nevertheless video compression algorithms entail significant computational demands order achieve substantial compression ratios complexity presents formidable challenge implementing efficient video coding standards resourceconstrained embedded systems iot edge node cameras tackle challenge paper introduces nuclass net innovative deeplearning model designed mitigate compression artifacts stemming lossy compression codecs enhancement significantly elevates perceptible quality lowbitrate videos employing nuclass net video encoder within videocapturing node reduce output quality thereby generating lowbitrate videos effectively curtailing computation bandwidth requirements edge decoder side typically less encumbered resource limitations nuclass net applied video decoder compensate artifacts approximate quality original video experimental results affirm efficacy proposed model enhancing perceptible quality videos especially streamed low bit rates,2,1.0,2,1.0
vanebench video anomaly evaluation benchmark conversational lmms recent developments large multimodal video models videolmms significantly enhanced ability interpret analyze video data despite impressive capabilities current videolmms evaluated anomaly detection tasks critical deployment practical scenarios eg towards identifying deepfakes manipulated video content traffic accidents crimes paper introduce vanebench benchmark designed assess proficiency videolmms detecting localizing anomalies inconsistencies videos dataset comprises array videos synthetically generated using existing stateoftheart texttovideo generation models encompassing variety subtle anomalies inconsistencies grouped five categories unnatural transformations unnatural appearance passthrough disappearance sudden appearance additionally benchmark features realworld samples existing anomaly detection datasets focusing crimerelated irregularities atypical pedestrian behavior unusual events task structured visual questionanswering challenge gauge models ability accurately detect localize anomalies within videos evaluate nine existing videolmms open closed sources benchmarking task find models encounter difficulties effectively identifying subtle anomalies conclusion research offers significant insights current capabilities videolmms realm anomaly detection highlighting importance work evaluating improving models realworld applications code data available httpshananshafigithubiovanebenchmark,0,0.9105078460130153,0,0.9105078460130153
scbench sports commentary benchmark video llms recently significant advances made video large language models video llms academia industry however methods evaluate benchmark performance different video llms especially finegrained temporal visual capabilities remain limited one hand current benchmarks use relatively simple videos eg subtitled movie clips model understand entire video processing frames hand datasets lack diversity task format comprising qa multichoice qa overlooks models capacity generating indepth precise texts sports videos feature intricate visual information sequential events emotionally charged commentary present critical challenge video llms making sports commentary ideal benchmarking task inspired challenges propose novel task sports video commentary generation developed textbfscbench video llms construct benchmark introduce textbfscores sixdimensional metric specifically designed task upon propose gptbased evaluation method textbfcommentaryset dataset consisting annotated video clips groundtruth labels tailored metric based scbench conduct comprehensive evaluations multiple video llms eg vila videollava etc chainofthought baseline methods results found achieves best performance surpassing secondbest work provides fresh perspective future research aiming enhance models overall capabilities complex visual understanding tasks dataset released soon,0,1.0,0,1.0
video prediction models general visual encoders study explores potential opensource video conditional generation models encoders downstream tasks focusing instance segmentation using bair robot pushing dataset researchers propose using video prediction models general visual encoders leveraging ability capture critical spatial temporal information essential tasks instance segmentation inspired human vision studies particularly gestalts principle common fate approach aims develop latent space representative motion images effectively discern foreground background information researchers utilize vectorquantized variational autoencoder vqvae video generative encoder model conditioned input frame coupled downstream segmentation tasks experiments involve adapting pretrained video generative models analyzing latent spaces training custom decoders foregroundbackground segmentation findings demonstrate promising results leveraging generative pretext learning downstream tasks working towards enhanced scene analysis segmentation computer vision applications,-1,0.0,-1,0.0
msc multiscale spatiotemporal causal attention autoregressive video diffusion diffusion transformers enable flexible generative modeling video however still technically challenging computationally expensive generate highresolution videos rich semantics complex motion similar languages video data also autoregressive nature counterintuitive use attention mechanism bidirectional dependency model propose multiscale causal msc framework address problems specifically introduce multiple resolutions spatial dimension highlow frequencies temporal dimension realize efficient attention calculation furthermore attention blocks multiple scales combined controlled way allow causal conditioning noisy image frames diffusion training based idea noise destroys information different rates different resolutions theoretically show approach greatly reduce computational complexity enhance efficiency training causal attention diffusion framework also used autoregressive long video generation without violating natural order frame sequences,-1,0.0,-1,0.0
vibidsampler enhancing video interpolation using bidirectional diffusion sampler recent progress largescale texttovideo imagetovideo diffusion models greatly enhanced video generation especially terms keyframe interpolation however current imagetovideo diffusion models powerful generating videos single conditioning frame need adaptation twoframe start end conditioned generation essential effective bounded interpolation unfortunately existing approaches fuse temporally forward backward paths parallel often suffer offmanifold issues leading artifacts requiring multiple iterative renoising steps work introduce novel bidirectional sampling strategy address offmanifold issues without requiring extensive renoising finetuning method employs sequential sampling along forward backward paths conditioned start end frames respectively ensuring coherent onmanifold generation intermediate frames additionally incorporate advanced guidance techniques cfg dds enhance interpolation process integrating method achieves stateoftheart performance efficiently generating highquality smooth videos keyframes single gpu method interpolate frames x resolution seconds establishing leading solution keyframe interpolation,-1,0.0,-1,0.0
harivo harnessing texttoimage models video generation present method create diffusionbased video models pretrained texttoimage models recently animatediff proposed freezing model training temporal layers advance method proposing unique architecture incorporating mapping network framewise tokens tailored video generation maintaining diversity creativity original model key innovations include novel loss functions temporal smoothness mitigating gradient sampling technique ensuring realistic temporally consistent video generation despite limited public video data successfully integrated videospecific inductive biases architecture loss functions method built frozen stablediffusion model simplifies training processes allows seamless integration offtheshelf models like controlnet dreambooth project page httpskwonminkigithubioharivo,-1,0.0,-1,0.0
starvid enhancing semantic alignment video diffusion models via spatial syntactic guided attention refocusing recent advances texttovideo generation diffusion models garnered significant attention however typically perform well scenes single object motion struggling compositional scenarios multiple objects distinct motions accurately reflect semantic content text prompts address challenges propose textbfstarvid plugandplay trainingfree method improves semantic alignment multiple subjects motions text prompts models starvid first leverages spatial reasoning capabilities large language models llms twostage motion trajectory planning based text prompts trajectories serve spatial priors guiding spatialaware loss refocus crossattention ca maps distinctive regions furthermore propose syntaxguided contrastive constraint strengthen correlation ca maps verbs corresponding nouns enhancing motionsubject binding qualitative quantitative evaluations demonstrate proposed framework significantly outperforms baseline methods delivering videos higher quality improved semantic consistency,6,0.42317474257619964,6,0.42317474257619964
mmdisco multimodal discriminatorguided cooperative diffusion joint audio video generation study aims construct audiovideo generative model minimal computational cost leveraging pretrained singlemodal generative models audio video achieve propose novel method guides singlemodal models cooperatively generate wellaligned samples across modalities specifically given two pretrained base diffusion models train lightweight joint guidance module adjust scores separately estimated base models match score joint distribution audio video show guidance computed using gradient optimal discriminator distinguishes real audiovideo pairs fake ones independently generated base models based analysis construct joint guidance module training discriminator additionally adopt loss function stabilize discriminators gradient make work noise estimator standard diffusion models empirical evaluations several benchmark datasets demonstrate method improves singlemodal fidelity multimodal alignment relatively parameters code available httpsgithubcomsonyresearchmmdisco,8,0.4960357999684478,8,0.4960357999684478
mojito motion trajectory intensity control video generation recent advancements diffusion models shown great promise producing highquality video content however efficiently training video diffusion models capable integrating directional guidance controllable motion intensity remains challenging underexplored area tackle challenges paper introduces mojito diffusion model incorporates motion trajectory intensity control texttovideo generation specifically mojito features directional motion control dmc module leverages crossattention efficiently direct generated objects motion without training alongside motion intensity modulator mim uses optical flow maps generated videos guide varying levels motion intensity extensive experiments demonstrate mojitos effectiveness achieving precise trajectory intensity control high computational efficiency generating motion patterns closely match specified directions intensities providing realistic dynamics align well natural motion realworld scenarios,9,0.7676269057144607,9,0.7676269057144607
videodpo omnipreference alignment video diffusion generation recent progress generative diffusion models greatly advanced texttovideo generation texttovideo models trained largescale diverse datasets produce varied outputs generations often deviate user preferences highlighting need preference alignment pretrained models although direct preference optimization dpo demonstrated significant improvements language image generation pioneer adaptation video diffusion models propose videodpo pipeline making several key adjustments unlike previous image alignment methods focus solely either visual quality ii semantic alignment text videos comprehensively consider dimensions construct preference score accordingly term omniscore design pipeline automatically collect preference pair data based proposed omniscore discover reweighting pairs based score significantly impacts overall preference alignment experiments demonstrate substantial improvements visual quality semantic alignment ensuring preference aspect neglected code data shared httpsvideodpogithubio,-1,0.0,-1,0.0
lightningdrag lightning fast accurate dragbased image editing emerging videos accuracy speed critical image editing tasks pan et al introduced dragbased image editing framework achieves pixellevel control using generative adversarial networks gans flurry subsequent studies enhanced frameworks generality leveraging largescale diffusion models however methods often suffer inordinately long processing times exceeding minute per edit low success rates addressing issues head present lightningdrag rapid approach enabling high quality dragbased image editing second unlike previous methods redefine dragbased editing conditional generation task eliminating need timeconsuming latent optimization gradientbased guidance inference addition design pipeline allows us train model largescale paired video frames contain rich motion information object translations changing poses orientations zooming etc learning videos approach significantly outperform previous methods terms accuracy consistency despite trained solely videos model generalizes well perform local shape deformations presented training data eg lengthening hair twisting rainbows etc extensive qualitative quantitative evaluations benchmark datasets corroborate superiority approach code model released httpsgithubcommagicresearchlightningdrag,-1,0.0,-1,0.0
drsm efficient neural decomposition dynamic reconstruction stationary monocular cameras popularity monocular videos generated video sharing live broadcasting applications reconstructing editing dynamic scenes stationary monocular cameras become special anticipated technology contrast scene reconstructions exploit multiview observations problem modeling dynamic scene single view significantly underconstrained illposed inspired recent progress neural rendering present novel framework tackle decomposition problem dynamic scenes monocular cameras framework utilizes decomposed static dynamic feature planes represent scenes emphasizes learning dynamic regions dense ray casting inadequate clues singleview occlusion also particular challenges scene reconstruction overcome difficulties propose deep supervised optimization ray casting strategies experiments various videos method generates higherfidelity results existing methods singleview dynamic scene representation,1,1.0,1,1.0
see matters novel visual physicsbased metric evaluating video generation quality video generation models advance rapidly assessing quality generated videos become increasingly critical existing metrics frechet video distance fvd inception score clipsim measure quality primarily latent space rather human visual perspective often overlooking key aspects like appearance motion consistency physical laws paper propose novel metric vamp visual appearance motion plausibility evaluates visual appearance physical plausibility generated videos vamp composed two main components appearance score assesses color shape texture consistency across frames motion score evaluates realism object movements validate vamp two experiments corrupted video evaluation generated video evaluation corrupted video evaluation introduce various types corruptions real videos measure correlation corruption severity vamp scores generated video evaluation use stateoftheart models generate videos carefully designed prompts compare vamps performance human evaluators rankings results demonstrate vamp effectively captures visual fidelity temporal consistency offering comprehensive evaluation video quality traditional methods,-1,0.0,-1,0.0
far video generation world model physical law perspective openais sora highlights potential video generation developing world models adhere fundamental physical laws however ability video generation models discover laws purely visual data without human priors questioned world model learning true law give predictions robust nuances correctly extrapolate unseen scenarios work evaluate across three key scenarios indistribution outofdistribution combinatorial generalization developed simulation testbed object movement collisions generate videos deterministically governed one classical mechanics laws provides unlimited supply data largescale experimentation enables quantitative evaluation whether generated videos adhere physical laws trained diffusionbased video generation models predict object movements based initial frames scaling experiments show perfect generalization within distribution measurable scaling behavior combinatorial generalization failure outofdistribution scenarios experiments reveal two key insights generalization mechanisms models models fail abstract general physical rules instead exhibit casebased generalization behavior ie mimicking closest training example generalizing new cases models observed prioritize different factors referencing training data color size velocity shape study suggests scaling alone insufficient video generation models uncover fundamental physical laws despite role soras broader success see project page httpsphyworldgithubio,-1,0.0,-1,0.0
mvbind selfsupervised music recommendation videos via embedding space binding recent years witnessed rapid development short videos usually contain visual audio modalities background music important short videos significantly influence emotions viewers however present background music short videos generally chosen video producer lack automatic music recommendation methods short videos paper introduces mvbind innovative musicvideo embedding space binding model crossmodal retrieval mvbind operates selfsupervised approach acquiring inherent knowledge intermodal relationships directly data without need manual annotations additionally compensate lack corresponding musicalvisual pair dataset short videos construct dataset video mainly consists meticulously selected short videos dataset mvbind manifests significantly improved performance compared baseline methods constructed dataset code released facilitate future research,-1,0.0,-1,0.0
humanannotated video dataset training evaluation video summarization methods paper introduce new dataset video summarization transformation video content concise summaries consumed via traditional devices tv sets smartphones dataset includes groundtruth humangenerated summaries used training objectively evaluating video summarization methods using dataset train assess two stateoftheart summarization methods originally proposed summarization serve baseline future comparisons summarization methods specifically tailored video finally present interactive tool developed facilitate data annotation process assist annotation activities rely video fragment selection,0,1.0,0,1.0
vidcompress memoryenhanced temporal compression video understanding large language models videobased multimodal large language models videollms possess significant potential video understanding tasks however videollms treat videos sequential set individual frames results insufficient temporalspatial interaction hinders finegrained comprehension difficulty processing longer videos due limited visual token capacity address challenges propose vidcompress novel videollm featuring memoryenhanced temporal compression vidcompress employs dualcompressor approach memoryenhanced compressor captures shortterm longterm temporal relationships videos compresses visual tokens using multiscale transformer memorycache mechanism textperceived compressor generates condensed visual tokens utilizing qformer integrating temporal contexts query embeddings cross attention experiments several videoqa datasets comprehensive benchmarks demonstrate vidcompress efficiently models complex temporalspatial relations significantly outperforms existing videollms,0,1.0,0,1.0
syncvis synchronized video instance segmentation recent detrbased methods advanced development video instance segmentation vis transformers efficiency capability modeling spatial temporal information despite harvesting remarkable progress existing works follow asynchronous designs model video sequences via either videolevel queries adopting querysensitive cascade structures resulting difficulties handling complex challenging video scenarios work analyze cause phenomenon limitations current solutions propose conduct synchronized modeling via new framework named syncvis specifically syncvis explicitly introduces videolevel query embeddings designs two key modules synchronize videolevel query framelevel query embeddings synchronized videoframe modeling paradigm synchronized embedding optimization strategy former attempts promote mutual learning frame videolevel embeddings latter divides large video sequences small clips easier optimization extensive experimental evaluations conducted challenging youtubevis ovis benchmarks syncvis achieves stateoftheart results demonstrates effectiveness generality proposed approach code available,-1,0.0,-1,0.0
snapgenv generating fivesecond video within five seconds mobile device witnessed unprecedented success diffusionbased video generation past year recently proposed models community wielded power generate cinematic highresolution videos smooth motions arbitrary input prompts however supertask image generation video generation models require computation thus hosted mostly cloud servers limiting broader adoption among content creators work propose comprehensive acceleration framework bring power largescale video diffusion model hands edge users network architecture scope initialize compact image backbone search design arrangement temporal layers maximize hardware efficiency addition propose dedicated adversarial finetuning algorithm efficient model reduce denoising steps model parameters generate video iphone pm within seconds compared serverside models take minutes powerful gpus generate single video accelerate generation magnitudes delivering onpar quality,-1,0.0,-1,0.0
pretraining action recognition automatically generated fractal datasets recent years interest synthetic data grown particularly context pretraining image modality support range computer vision tasks including object classification medical imaging etc previous work demonstrated synthetic samples automatically produced various generative processes replace real counterparts yield strong visual representations approach resolves issues associated real data collection labeling costs copyright privacy extend trend video domain applying task action recognition employing fractal geometry present methods automatically produce largescale datasets short synthetic video clips utilized pretraining neural models generated video clips characterized notable variety stemmed innate ability fractals generate complex multiscale structures narrow domain gap identify key properties real videos carefully emulate pretraining thorough ablations determine attributes strengthen downstream results offer general guidelines pretraining synthetic videos proposed approach evaluated finetuning pretrained models established action recognition datasets well four video benchmarks related group action recognition finegrained action recognition dynamic scenes compared standard kinetics pretraining reported results come close even superior portion downstream datasets code samples synthetic videos available,-1,0.0,-1,0.0
mobile video diffusion video diffusion models achieved impressive realism controllability limited high computational demands restricting use mobile devices paper introduces first mobileoptimized video diffusion model starting spatiotemporal unet stable video diffusion svd reduce memory computational cost reducing frame resolution incorporating multiscale temporal representations introducing two novel pruning schema reduce number channels temporal blocks furthermore employ adversarial finetuning reduce denoising single step model coined mobilevd efficient vs tflops slight quality drop fvd vs generating latents px clip seconds pro results available httpsqualcommairesearchgithubiomobilevideodiffusion,-1,0.0,-1,0.0
audiodriven emotional talkinghead generation audiodriven video portrait synthesis crucial useful technology virtual human interaction filmmaking applications recent advancements focused improving image fidelity lipsynchronization however generating accurate emotional expressions important aspect realistic talkinghead generation remained underexplored previous works present novel system paper synthesizing highfidelity audiodriven video portraits accurate emotional expressions specifically utilize variational autoencoder vaebased audiotomotion module generate facial landmarks landmarks concatenated emotional embeddings produce emotional landmarks motiontoemotion module emotional landmarks used render realistic emotional talkinghead video using neural radiance fields nerfbased emotiontovideo module additionally propose pose sampling method generates natural idlestate nonspeaking videos response silent audio inputs extensive experiments demonstrate method obtains accurate emotion generation higher fidelity,6,1.0,6,1.0
enhancing motion texttovideo generation decomposed encoding conditioning despite advancements texttovideo generation producing videos realistic motion remains challenging current models often yield static minimally dynamic outputs failing capture complex motions described text issue stems internal biases text encoding overlooks motions inadequate conditioning mechanisms generation models address propose novel framework called decomposed motion demo enhances motion synthesis generation decomposing text encoding conditioning content motion components method includes content encoder static elements motion encoder temporal dynamics alongside separate content motion conditioning mechanisms crucially introduce textmotion videomotion supervision improve models understanding generation motion evaluations benchmarks msrvtt evalcrafter vbench demonstrate demos superior ability produce videos enhanced motion dynamics maintaining high visual quality approach significantly advances generation integrating comprehensive motion understanding directly textual descriptions project page httpsprryangithubiodemoproject,-1,0.0,-1,0.0
survey generative ai llm video generation understanding streaming paper offers insightful examination currently toptrending ai technologies ie generative artificial intelligence generative ai large language models llms reshaping field video technology including video generation understanding streaming highlights innovative use technologies producing highly realistic videos significant leap bridging gap realworld dynamics digital creation study also delves advanced capabilities llms video understanding demonstrating effectiveness extracting meaningful information visual content thereby enhancing interaction videos realm video streaming paper discusses llms contribute efficient usercentric streaming experiences adapting content delivery individual viewer preferences comprehensive review navigates current achievements ongoing challenges future possibilities applying generative ai llms videorelated tasks underscoring immense potential technologies hold advancing field video technology related multimedia networking ai communities,10,1.0,10,1.0
mmldm multimodal latent diffusion model sounding video generation sounding video generation svg audiovideo joint generation task challenged highdimensional signal spaces distinct data formats different patterns content information address issues introduce novel multimodal latent diffusion model mmldm svg task first unify representation audio video data converting single couple images introduce hierarchical multimodal autoencoder constructs lowlevel perceptual latent space modality shared highlevel semantic feature space former space perceptually equivalent raw signal space modality drastically reduces signal dimensions latter space serves bridge information gap modalities provides insightful crossmodal guidance proposed method achieves new stateoftheart results significant quality efficiency gains specifically method achieves comprehensive improvement evaluation metrics faster training sampling speed landscape aist datasets moreover explore performance opendomain sounding video generation long sounding video generation audio continuation video continuation conditional singlemodal generation tasks comprehensive evaluation mmldm demonstrates exciting adaptability generalization ability,-1,0.0,-1,0.0
spectrum semantic processing emotioninformed videocaptioning retrieval understanding modalities capturing videos meaning critical concepts analyzing subtle details fundamental yet challenging task video captioning identifying dominant emotional tone video significantly enhances perception context despite strong emphasis video captioning existing models often need adequately address emotional themes resulting suboptimal captioning results address limitations paper proposes novel semantic processing emotioninformed videocaptioning retrieval understanding modalities spectrum framework empower generation emotionally semantically credible captions leveraging pioneering structure spectrum discerns multimodal semantics emotional themes using visual text attribute investigation vtai determines orientation descriptive captions holistic conceptoriented theme hcot expressing emotionallyinformed fieldacquainted references exploit videototext retrieval capabilities multifaceted nature video content estimate emotional probabilities candidate captions dominant theme video determined appropriately weighting embedded attribute vectors applying coarse finegrained emotional concepts define videos contextual alignment furthermore using two loss functions spectrum optimized integrate emotional information minimize prediction errors extensive experiments emvidcap msvd msrvtt video captioning datasets demonstrate model significantly surpasses stateoftheart methods quantitative qualitative evaluations highlight models ability accurately capture convey video emotions multimodal attributes,-1,0.0,-1,0.0
cgbench cluegrounded question answering benchmark long video understanding existing video understanding benchmarks multimodal large language models mllms focus short videos limited number benchmarks long video understanding often rely solely multiplechoice questions mcqs however inherent limitation mcqbased evaluation increasing reasoning ability mllms models give current answer purely combining short video understanding elimination without genuinely understanding video content address gap introduce cgbench novel benchmark designed cluegrounded question answering long videos cgbench emphasizes models ability retrieve relevant clues questions enhancing evaluation credibility features manually curated videos categorized granular system primary categories secondary categories tertiary categories making largest benchmark long video analysis benchmark includes qa pairs three major question types perception reasoning hallucination compensating drawbacks pure mcqbased evaluation design two novel cluebased evaluation methods cluegrounded white box black box evaluations assess whether model generates answers based correct understanding video evaluate multiple closedsource opensource mllms cgbench results indicate current models significantly underperform understanding long videos compared short ones significant gap exists opensource commercial models hope cgbench advance development trustworthy capable mllms long video understanding annotations video data released httpscgbenchgithubioleaderboard,0,1.0,0,1.0
longvlm efficient long video understanding via large language models empowered large language models llms recent advancements videobased llms videollms driven progress various video understanding tasks models encode video representations pooling query aggregation vast number visual tokens making computational memory costs affordable despite successfully providing overall comprehension video content existing videollms still face challenges achieving detailed understanding due overlooking local information longterm videos tackle challenge introduce longvlm simple yet powerful videollm long video understanding building upon observation long videos often consist sequential key events complex actions camera movements approach proposes decompose long videos multiple shortterm segments encode local features segment via hierarchical token merging module features concatenated temporal order maintain storyline across sequential shortterm segments additionally propose integrate global semantics local feature enhance context understanding way encode video representations incorporate local global information enabling llm generate comprehensive responses longterm videos experimental results videochatgpt benchmark zeroshot video questionanswering datasets demonstrate superior capabilities model previous stateoftheart methods qualitative examples show model produces precise responses long video understanding code available httpsgithubcomziplablongvlm,0,1.0,0,1.0
streaming long video understanding large language models paper presents videostreaming advanced visionlanguage large model vllm video understanding capably understands arbitrarylength video constant number video tokens streamingly encoded adaptively selected challenge video understanding vision language area mainly lies significant computational burden caused great number tokens extracted long videos previous works rely sparse sampling frame compression reduce tokens however approaches either disregard temporal information long time span sacrifice spatial details resulting flawed compression address limitations videostreaming two core designs memorypropagated streaming encoding adaptive memory selection memorypropagated streaming encoding architecture segments long videos short clips sequentially encodes clip propagated memory iteration utilize encoded results preceding clip historical memory integrated current clip distill condensed representation encapsulates video content current timestamp encoding process adaptive memory selection strategy selects constant number questionrelated memories historical memories feeds llm generate informative responses questionrelated selection reduces redundancy within memories enabling efficient precise video understanding meanwhile disentangled video extraction reasoning design allows llm answer different questions video directly selecting corresponding memories without need encode whole video question model achieves superior performance higher efficiency long video benchmarks showcasing precise temporal comprehension detailed question answering,-1,0.0,-1,0.0
groundedvideollm sharpening finegrained temporal grounding video large language models video large language models videollms demonstrated remarkable capabilities coarsegrained video understanding however struggle finegrained temporal grounding paper introduce groundedvideollm novel videollm adept perceiving reasoning specific video moments finegrained manner identify current videollms limitations finegrained video understanding since lack effective temporal modeling timestamp representation light sharpen model incorporating additional temporal stream encode relationships frames discrete temporal tokens enriched specific time knowledge represent timestamps optimize training groundedvideollm employ multistage training scheme beginning simple videocaptioning tasks progressively introducing video temporal grounding tasks increasing complexity enhance groundedvideollms temporal reasoning capability also curate grounded videoqa dataset automatic annotation pipeline extensive experiments demonstrate groundedvideollm excels finegrained grounding tasks temporal sentence grounding dense video captioning grounded videoqa also shows great potential versatile video assistant general video understanding,0,0.8795767043356593,0,0.8795767043356593
dawn dynamic frame avatar nonautoregressive diffusion framework talking head video generation talking head generation intends produce vivid realistic talking head videos single portrait speech audio clip although significant progress made diffusionbased talking head generation almost methods rely autoregressive strategies suffer limited context utilization beyond current generation step error accumulation slower generation speed address challenges present dawn dynamic frame avatar nonautoregressive diffusion framework enables allatonce generation dynamiclength video sequences specifically consists two main components audiodriven holistic facial dynamics generation latent motion space audiodriven head pose blink generation extensive experiments demonstrate method generates authentic vivid videos precise lip motions natural poseblink movements additionally high generation speed dawn possesses strong extrapolation capabilities ensuring stable production highquality long videos results highlight considerable promise potential impact dawn field talking head video generation furthermore hope dawn sparks exploration nonautoregressive approaches diffusion models code publicly available httpsgithubcomhanbochengdawnpytorch,6,0.7071569361425841,6,0.7071569361425841
animatedifflightning crossmodel diffusion distillation present animatedifflightning lightningfast video generation model uses progressive adversarial diffusion distillation achieve new stateoftheart fewstep video generation discuss modifications adapt video modality furthermore propose simultaneously distill probability flow multiple base diffusion models resulting single distilled motion module broader style compatibility pleased release distilled animatedifflightning model communitys use,11,0.9321277318126886,11,0.9321277318126886
deep video representation learning survey paper provides review representation learning videos classify recent spatiotemporal feature learning methods sequential visual data compare pros cons general video analysis building effective features videos fundamental problem computer vision tasks involving video analysis understanding existing features generally categorized spatial temporal features effectiveness variations illumination occlusion view background discussed finally discuss remaining challenges existing deep video representation learning studies,-1,0.0,-1,0.0
video generation learned action prior stochastic video generation particularly challenging camera mounted moving platform camera motion interacts observed image pixels creating complex spatiotemporal dynamics making problem partially observable existing methods typically address focusing raw pixellevel image reconstruction without explicitly modelling camera motion dynamics propose solution considering camera motion action part observed image state modelling image action within multimodal learning framework introduce three models video generation learning action prior vgleap treats imageaction pair augmented state generated single latent stochastic process uses variational inference learn imageaction latent prior causalleap establishes causal relationship action observed image frame time learning action prior conditioned observed image states rafi integrates augmented imageaction state concept flow matching diffusion generative processes demonstrating actionconditioned image generation concept extended diffusionbased models emphasize importance multimodal training partially observable video generation problems detailed empirical studies new video action dataset roam,-1,0.0,-1,0.0
depth prediction autonomous driving using selfsupervised learning perception environment critical component enabling autonomous driving provides vehicle ability comprehend surroundings make informed decisions depth prediction plays pivotal role process helps understanding geometry motion environment thesis focuses challenge depth prediction using monocular selfsupervised learning techniques problem approached broader perspective first exploring conditional generative adversarial networks cgans potential technique achieve better generalization performed fundamental contribution conditional gans acontrario cgan proposed second contribution entails single imagetodepth selfsupervised method proposing solution rigidscene assumption using novel transformerbased method outputs pose dynamic object third significant aspect involves introduction videotodepth map forecasting approach method serves extension selfsupervised techniques predict future depths involves creation novel transformer model capable predicting future depth given scene moreover various limitations aforementioned methods addressed videotovideo depth maps model proposed model leverages spatiotemporal consistency input output sequence predict accurate depth sequence output methods significant applications autonomous driving ad advanced driver assistance systems adas,-1,0.0,-1,0.0
screenwriter automatic screenplay generation movie summarisation proliferation creative video content driven demand textual descriptions summaries allow users recall key plot points get overview without watching volume movie content speed turnover motivates automatic summarisation nevertheless challenging requiring identifying character intentions longrange temporal dependencies existing methods attempting task rely heavily textual screenplays input greatly limiting applicability work propose task automatic screenplay generation method screenwriter operates video produces output includes dialogue speaker names scene breaks visual descriptions screenwriter introduces novel algorithm segment video scenes based sequence visual vectors novel method challenging problem determining character names based database actors faces demonstrate automatic screenplays used generate plot synopses hierarchical summarisation method based scene breaks test quality final summaries recent moviesum dataset augment videos show superior number comparison models assume access goldstandard screenplays,-1,0.0,-1,0.0
knowledge nerf fewshot novel view synthesis dynamic articulated objects present knowledge nerf synthesize novel views dynamic scenes reconstructing dynamic scenes sparse views rendering arbitrary perspectives challenging problem applications various domains previous dynamic nerf methods learn deformation articulated objects monocular videos however qualities reconstructed scenes limited clearly reconstruct dynamic scenes propose new framework considering two frames timewe pretrain nerf model articulated objectwhen articulated objects moves knowledge nerf learns generate novel views new state incorporating past knowledge pretrained nerf model minimal observations present state propose projection module adapt nerf dynamic scenes learning correspondence pretrained knowledge base current states experimental results demonstrate effectiveness method reconstructing dynamic scenes input images one state knowledge nerf new pipeline promising solution novel view synthesis dynamic articulated objects data implementation publicly available,1,1.0,1,1.0
adaptive super resolution oneshot talkinghead generation oneshot talkinghead generation learns synthesize talkinghead video one source portrait image driving different identity video usually methods require planebased pixel transformations via jacobin matrices facial image warps novel poses generation constraints using single image source pixel displacements often compromise clarity synthesized images methods try improve quality synthesized videos introducing additional superresolution modules undoubtedly increase computational consumption destroy original data distribution work propose adaptive highquality talkinghead video generation method synthesizes highresolution video without additional pretrained modules specifically inspired existing superresolution methods downsample oneshot source image adaptively reconstruct highfrequency details via encoderdecoder module resulting enhanced video clarity method consistently improves quality generated videos straightforward yet effective strategy substantiated quantitative qualitative evaluations code demo video available urlhttpsgithubcomsongluchuanadasrtalkinghead,11,1.0,11,1.0
improving generative adversarial networks video superresolution research explore different ways improve generative adversarial networks video superresolution tasks base single image superresolution gan model primary objective identify potential techniques enhance models analyze techniques yield significant improvements evaluate results using peak signaltonoise ratio psnr structural similarity index ssim findings indicate effective techniques include temporal smoothing long shortterm memory lstm layers temporal loss function integration methods results improvement psnr improvement ssim compared baseline video superresolution generative adversarial network gan model substantial improvement suggests potential applications enhance current stateoftheart models,-1,0.0,-1,0.0
vistadream sampling multiview consistent images singleview scene reconstruction paper propose vistadream novel framework reconstruct scene singleview image recent diffusion models enable generating highquality novelview images singleview input image existing methods concentrate building consistency input image generated images losing consistency generated images vistadream addresses problem twostage pipeline first stage vistadream begins building global coarse scaffold zooming little step inpainted boundaries estimated depth map global scaffold use iterative diffusionbased rgbd inpainting generate novelview images inpaint holes scaffold second stage enhance consistency generated novelview images novel trainingfree multiview consistency sampling mcs introduces multiview consistency constraints reverse sampling process diffusion models experimental results demonstrate without training finetuning existing diffusion models vistadream achieves consistent highquality novel view synthesis using singleview images outperforms baseline methods large margin code videos interactive demos available httpsvistadreamprojectpagegithubio,1,0.9567646731039043,1,0.9567646731039043
tuningfree noise rectification high fidelity imagetovideo generation imagetovideo generation tasks always suffer keeping high fidelity open domains traditional image animation techniques primarily focus specific domains faces human poses making difficult generalize open domains several recent frameworks based diffusion models generate dynamic content open domain images fail maintain fidelity found two main factors low fidelity loss image details noise prediction biases denoising process end propose effective method applied mainstream video diffusion models method achieves high fidelity based supplementing precise image information noise rectification specifically given specified image method first adds noise input image latent keep details denoises noisy latent proper rectification alleviate noise prediction biases method tuningfree plugandplay experimental results demonstrate effectiveness approach improving fidelity generated videos imagetovideo generated results please refer project website httpsnoiserectificationgithubio,-1,0.0,-1,0.0
videotetris towards compositional texttovideo generation diffusion models demonstrated great success texttovideo generation however existing methods may face challenges handling complex long video generation scenarios involve multiple objects dynamic changes object numbers address limitations propose videotetris novel framework enables compositional generation specifically propose spatiotemporal compositional diffusion precisely follow complex textual semantics manipulating composing attention maps denoising networks spatially temporally moreover propose enhanced video data preprocessing enhance training data regarding motion dynamics prompt understanding equipped new reference frame attention mechanism improve consistency autoregressive video generation extensive experiments demonstrate videotetris achieves impressive qualitative quantitative results compositional generation code available,-1,0.0,-1,0.0
boosting camera motion control video diffusion transformers recent advancements diffusion models significantly enhanced quality video generation however finegrained control camera pose remains challenge unetbased models shown promising results camera control transformerbased diffusion models ditthe preferred architecture largescale video generation suffer severe degradation camera motion accuracy paper investigate underlying causes issue propose solutions tailored dit architectures study reveals camera control performance depends heavily choice conditioning methods rather camera pose representations commonly believed address persistent motion degradation dit introduce camera motion guidance cmg based classifierfree guidance boosts camera control additionally present sparse camera control pipeline significantly simplifying process specifying camera poses long videos method universally applies unet dit models offering improved camera control video generation tasks,-1,0.0,-1,0.0
ctrladapter efficient versatile framework adapting diverse controls diffusion model controlnets widely used adding spatial control texttoimage diffusion models different conditions depth maps scribblessketches human poses however comes controllable video generation controlnets directly integrated new backbones due feature space mismatches training controlnets new backbones significant burden many users furthermore applying controlnets independently different frames effectively maintain object temporal consistency address challenges introduce ctrladapter efficient versatile framework adds diverse controls imagevideo diffusion model adaptation pretrained controlnets ctrladapter offers strong diverse capabilities including image video control sparseframe video control finegrained patchlevel multicondition control via moe router zeroshot adaptation unseen conditions supports variety downstream tasks beyond spatial control including video editing video style transfer textguided motion control six diverse unetditbased imagevideo diffusion models sdxl pixartalpha svd latte hotshotxl ctrladapter matches performance pretrained controlnets coco achieves stateoftheart davis significantly lower computation gpu hours,-1,0.0,-1,0.0
evaluating safety texttovideo generative models recent development sora leads new era texttovideo generation along comes rising concern security risks generated videos may contain illegal unethical content lack comprehensive quantitative understanding safety posing challenge reliability practical deployment previous evaluations primarily focus quality video generation evaluations texttoimage models considered safety cover fewer aspects address unique temporal risk inherent video generation bridge research gap introduce new benchmark designed conducting safetycritical assessments texttovideo models define critical aspects video generation safety construct malicious prompt dataset including realworld prompts llmgenerated prompts jailbreak attackbased prompts based evaluation results draw several important findings including single model excels aspects different models showing various strengths correlation assessments manual reviews generally high tradeoff usability safety texttovideo generative models indicates field video generation rapidly advances safety risks set surge highlighting urgency prioritizing video safety hope provide insights better understanding safety video generation era generative ai,-1,0.0,-1,0.0
showhowto generating sceneconditioned stepbystep visual instructions goal work generate stepbystep visual instructions form sequence images given input image provides scene context sequence textual instructions challenging problem requires generating multistep image sequences achieve complex goal grounded specific environment part challenge stems lack largescale training data problem contribution work thus threefold first introduce automatic approach collecting large stepbystep visual instruction training data instructional videos apply approach one million videos create largescale highquality dataset sequences imagetext pairs second develop train showhowto video diffusion model capable generating stepbystep visual instructions consistent provided input image third evaluate generated image sequences across three dimensions accuracy step scene task show model achieves stateoftheart results code dataset trained models publicly available,-1,0.0,-1,0.0
autoregressive video generation without vector quantization paper presents novel approach enables autoregressive video generation high efficiency propose reformulate video generation problem nonquantized autoregressive modeling temporal framebyframe prediction spatial setbyset prediction unlike rasterscan prediction prior autoregressive models joint distribution modeling fixedlength tokens diffusion models approach maintains causal property gptstyle models flexible incontext capabilities leveraging bidirectional modeling within individual frames efficiency proposed approach train novel video autoregressive model without vector quantization termed nova results demonstrate nova surpasses prior autoregressive video models data efficiency inference speed visual fidelity video fluency even much smaller model capacity ie parameters nova also outperforms stateoftheart image diffusion models texttoimage generation tasks significantly lower training cost additionally nova generalizes well across extended video durations enables diverse zeroshot applications one unified model code models publicly available httpsgithubcombaaivisionnova,11,0.9235240508025361,11,0.9235240508025361
generative human video compression multigranularity temporal trajectory factorization paper propose novel multigranularity temporal trajectory factorization framework generative human video compression holds great potential bandwidthconstrained humancentric video communication particular proposed motion factorization strategy facilitate implicitly characterize highdimensional visual signal compact motion vectors representation compactness transform vectors finegrained field motion expressibility coded bitstream entailed enough visual motion information lowest representation cost meanwhile resolutionexpandable generative module developed enhanced background stability proposed framework optimized towards higher reconstruction robustness flexible resolution adaptation experimental results show proposed method outperforms latest generative models stateoftheart video coding standard versatile video coding vvc talkingface videos movingbody videos terms objective subjective quality project page found httpsgithubcomxyzyszextremehumanvideocompressionwithmttf,-1,0.0,-1,0.0
instancecap improving texttovideo generation via instanceaware structured caption texttovideo generation evolved rapidly recent years delivering remarkable results training typically relies videocaption paired data plays crucial role enhancing generation performance however current video captions often suffer insufficient details hallucinations imprecise motion depiction affecting fidelity consistency generated videos work propose novel instanceaware structured caption framework termed instancecap achieve instancelevel finegrained video caption first time based scheme design auxiliary models cluster convert original video instances enhance instance fidelity video instances used refine dense prompts structured phrases achieving concise yet precise descriptions furthermore instancevid dataset curated training enhancement pipeline tailored instancecap structure proposed inference experimental results demonstrate proposed instancecap significantly outperform previous models ensuring high fidelity captions videos reducing hallucinations,-1,0.0,-1,0.0
polysmart trecvid medical video question answering video corpus visual answer localization vcval includes questionrelated video retrieval visual answer localization videos specifically use texttotext retrieval find relevant videos medical question based similarity video transcript answers generated visual answer localization start end timestamps answer predicted alignments visual content subtitles queries queryfocused instructional step captioning qfisc task step captions generated specifically provide video captions generated llavanextvideo model video subtitles timestamps context ask generate step captions given medical query submit one run evaluation obtains fscore mean iou,0,1.0,0,1.0
occsora occupancy generation models world simulators autonomous driving understanding evolution scenes important effective autonomous driving conventional methods mode scene development motion individual instances world models emerge generative framework describe general scene dynamics however existing methods adopt autoregressive framework perform nexttoken prediction suffer inefficiency modeling longterm temporal evolutions address propose diffusionbased occupancy generation model occsora simulate development world autonomous driving employ scene tokenizer obtain compact discrete spatialtemporal representations occupancy input achieve highquality reconstruction longsequence occupancy videos learn diffusion transformer spatialtemporal representations generate occupancy conditioned trajectory prompt conduct extensive experiments widely used nuscenes dataset occupancy annotations occsora generate authentic layout temporal consistency demonstrating ability understand spatial temporal distributions driving scenes trajectoryaware generation occsora potential serve world simulator decisionmaking autonomous driving code available httpsgithubcomwzzhengoccsora,-1,0.0,-1,0.0
faker fullbody anonymization human keypoint extraction realtime video deidentification contemporary digital era protection personal information become paramount issue exponential growth media industry heightened concerns regarding anonymization individuals captured video footage traditional methods blurring pixelation commonly employed recent advancements introduced generative adversarial networks gan redraw faces videos study propose novel approach employs significantly smaller model achieve realtime fullbody anonymization individuals videos unlike conventional techniques often fail effectively remove personal identification information skin color clothing accessories body shape method successfully eradicates details furthermore leveraging pose estimation algorithms approach accurately represents information regarding individuals positions movements postures algorithm seamlessly integrated cctv ip camera systems installed various industrial settings functioning realtime thus facilitating widespread adoption fullbody anonymization technology,4,0.7443247985959874,4,0.7443247985959874
benchmarking multidimensional aigc video quality assessment dataset unified model recent years artificial intelligence aidriven video generation gained significant attention consequently growing need accurate video quality assessment vqa metrics evaluate perceptual quality aigenerated content aigc videos optimize video generation models however assessing quality aigc videos remains significant challenge videos often exhibit highly complex distortions unnatural actions irrational objects address challenge systematically investigate aigcvqa problem considering subjective objective quality assessment perspectives subjective perspective construct largescale generated video quality assessment lgvq dataset consisting aigc videos generated video generation models using carefully curated text prompts evaluate perceptual quality aigc videos three critical dimensions spatial quality temporal quality textvideo alignment objective perspective establish benchmark evaluating existing quality assessment metrics lgvq dataset findings show current metrics perform poorly dataset highlighting gap effective evaluation tools bridge gap propose unify generated video quality assessment ugvq model designed accurately evaluate multidimensional quality aigc videos ugvq model integrates visual motion features videos textual features corresponding prompts forming unified qualityaware feature representation tailored aigc videos experimental results demonstrate ugvq achieves stateoftheart performance lgvq dataset across three quality dimensions lgvq dataset ugvq model publicly available httpsgithubcomzczhangsjtuugvqgit,12,0.9576803392121078,12,0.9576803392121078
statespace decomposition model video prediction considering longterm motion trend stochastic video prediction enables consideration uncertainty future motion thereby providing better reflection dynamic nature environment stochastic video prediction methods based image autoregressive recurrent models need feed predictions back latent space conversely statespace models decouple frame synthesis temporal prediction proves efficient however inferring longterm temporal information motion generalizing dynamic scenarios nonstationary assumptions remains unresolved challenge paper propose statespace decomposition stochastic video prediction model decomposes overall video frame generation deterministic appearance prediction stochastic motion prediction adaptive decomposition models generalization capability dynamic scenarios enhanced context motion prediction obtaining prior longterm trend future motion crucial thus stochastic motion prediction branch infer longterm motion trend conditional frames guide generation future frames exhibit high consistency conditional frames experimental results demonstrate model outperforms baselines multiple datasets,-1,0.0,-1,0.0
avdit efficient audiovisual diffusion transformer joint audio video generation recent diffusion transformers dits shown impressive capabilities generating highquality singlemodality content including images videos audio however still underexplored whether transformerbased diffuser efficiently denoise gaussian noises towards superb multimodal content creation bridge gap introduce avdit novel efficient audiovisual diffusion transformer designed generate highquality realistic videos visual audio tracks minimize model complexity computational costs avdit utilizes shared dit backbone pretrained imageonly data lightweight newly inserted adapters trainable shared backbone facilitates audio video generation specifically video branch incorporates trainable temporal attention layer frozen pretrained dit block temporal consistency additionally small number trainable parameters adapt imagebased dit block audio generation extra shared dit block equipped lightweight parameters facilitates feature interaction audio visual modalities ensuring alignment extensive experiments aist landscape datasets demonstrate avdit achieves stateoftheart performance joint audiovisual generation significantly fewer tunable parameters furthermore results highlight single shared image generative backbone modalityspecific adaptations sufficient constructing joint audiovideo generator source code pretrained models released,-1,0.0,-1,0.0
spagent adaptive task decomposition model selection general video generation editing opensource video generation editing models made significant progress individual models typically limited specific tasks failing meet diverse needs users effectively coordinating models unlock wide range video generation editing capabilities however manual coordination complex timeconsuming requiring users deeply understand task requirements possess comprehensive knowledge models performance applicability limitations thereby increasing barrier entry address challenges propose novel video generation editing system powered semantic planning agent spagent spagent bridges gap diverse user intents effective utilization existing generative models enhancing adaptability efficiency overall quality video generation editing specifically spagent assembles tool library integrating stateoftheart opensource image video generation editing models tools finetuning manually annotated dataset spagent automatically coordinate tools video generation editing novelly designed threestep framework decoupled intent recognition principleguided route planning capabilitybased execution model selection additionally enhance spagents video quality evaluation capability enabling autonomously assess incorporate new video generation editing models tool library without human intervention experimental results demonstrate spagent effectively coordinates models generate edit videos highlighting versatility adaptability across various video tasks,15,0.9200149995853233,15,0.9200149995853233
coherent video inpainting using optical flowguided efficient diffusion textguided video inpainting technique significantly improved performance content generation applications recent family improvements uses diffusion models become essential achieving highquality video inpainting results yet still face performance bottlenecks temporal consistency computational efficiency motivates us propose new video inpainting framework using optical flowguided efficient diffusion floed higher video coherence specifically floed employs dualbranch architecture timeagnostic flow branch restores corrupted flow first multiscale flow adapters provide motion guidance main inpainting branch besides trainingfree latent interpolation method proposed accelerate multistep denoising process using flow warping flow attention cache mechanism floed efficiently reduces computational cost incorporating optical flow extensive experiments background restoration object removal tasks show floed outperforms stateoftheart diffusionbased methods quality efficiency codes models made publicly available,-1,0.0,-1,0.0
dynamic scene understanding objectcentric voxelization neural rendering learning objectcentric representations unsupervised videos challenging unlike previous approaches focus decomposing images present generative model named dynavols dynamic scenes enables objectcentric learning within differentiable volume rendering framework key idea perform objectcentric voxelization capture nature scene infers perobject occupancy probabilities individual spatial locations voxel features evolve canonicalspace deformation function optimized inverse rendering pipeline compositional nerf additionally approach integrates semantic features create semantic grids representing scene multiple disentangled voxel grids dynavols significantly outperforms existing models novel view synthesis unsupervised decomposition tasks dynamic scenes jointly considering geometric structures semantic features effectively addresses challenging realworld scenarios involving complex object interactions furthermore trained explicitly meaningful voxel features enable additional capabilities scene decomposition methods achieve novel scene generation editing geometric shapes manipulating motion trajectories objects,1,0.9396901793453583,1,0.9396901793453583
efficient continuous video flow model video prediction multistep prediction models diffusion rectified flow models emerged stateoftheart solutions generation tasks however models exhibit higher latency sampling new frames compared singlestep methods latency issue becomes significant bottleneck adapting methods video prediction tasks given typical video comprises approximately frames paper propose novel approach modeling multistep process aimed alleviating latency constraints facilitating adaptation processes video prediction tasks approach reduces number sample steps required predict next frame also minimizes computational demands reducing model size onethird original size evaluate method standard video prediction datasets including kth bair action robot demonstrating efficacy achieving stateoftheart performance benchmarks,-1,0.0,-1,0.0
makes video radicalizing identifying sources influence qanon videos recent years radicalization increasingly attempted videosharing platforms previous studies proposed identify online radicalization using generic social context analysis without taking account comprehensive viewer traits affect viewers perception radicalizing content address challenge examine qanon conspiracybased radicalizing group designed comprehensive questionnaire aiming understand viewers perceptions qanon videos outline traits viewers qanon videos appealing identify influential factors impact viewers perception videos,-1,0.0,-1,0.0
poseguided finegrained sign language video generation sign language videos important medium spreading learning sign language however existing human image synthesis methods produce sign language images details distorted blurred structurally incorrect also produce sign language video frames poor temporal consistency anomalies flickering abrupt detail changes previous next frames address limitations propose novel poseguided motion model pgmm generating finegrained motionconsistent sign language videos firstly propose new coarse motion module cmm completes deformation features optical flow warping thus transfering motion coarsegrained structures without changing appearance secondly propose new pose fusion module pfm guides modal fusion rgb pose features thus completing finegrained generation finally design new metric temporal consistency difference tcd quantitatively assess degree temporal consistency video comparing difference frames reconstructed video previous next frames target video extensive qualitative quantitative experiments show method outperforms stateoftheart methods benchmark tests visible improvements details temporal consistency,-1,0.0,-1,0.0
video coding meets multimodal large language models unified paradigm video coding existing codecs designed eliminate intrinsic redundancies create compact representation compression however strong external priors multimodal large language models mllms explicitly explored video compression herein introduce unified paradigm crossmodality video coding cmvc pioneering approach explore multimodality representation video generative models video coding specifically encoder side disentangle video spatial content motion components subsequently transformed distinct modalities achieve compact representation leveraging mllms decoding previously encoded components video generation models leveraged create multiple encodingdecoding modes optimize video reconstruction quality specific decoding requirements including texttexttovideo mode ensure highquality semantic information imagetexttovideo mode achieve superb perceptual consistency addition propose efficient frame interpolation model mode via lowrank adaption lora tuning guarantee perceptual quality allows generated motion cues behave smoothly experiments benchmarks indicate achieves effective semantic reconstruction exhibits competitive perceptual consistency results highlight potential directions future research video coding,2,1.0,2,1.0
largescale video dataset improving consistency finegrained conditions video content visual generation technologies continue advance scale video datasets expanded rapidly quality datasets critical performance video generation models argue temporal splitting detailed captions video quality filtering three key factors determine dataset quality however existing datasets exhibit various limitations areas address challenges introduce largescale highquality video dataset featuring accurate temporal splitting detailed captions superior video quality core approach lies improving consistency finegrained conditions video content specifically employ linear classifier probability distributions enhance accuracy transition detection ensuring better temporal consistency provide structured captions splitted videos average length words improve textvideo alignment additionally develop video training suitability score vtss integrates multiple submetrics allowing us filter highquality videos original corpus finally incorporate several metrics training process generation model refining finegrained conditions experiments demonstrate effectiveness data processing pipeline quality proposed dataset dataset code released,-1,0.0,-1,0.0
seeing beyond views multiview driving scene video generation holistic attention generating multiview videos autonomous driving training recently gained much attention challenge addressing crossview crossframe consistency existing methods typically apply decoupled attention mechanisms spatial temporal view dimensions however approaches often struggle maintain consistency across dimensions particularly handling fastmoving objects appear different times viewpoints paper present cogdriving novel network designed synthesizing highquality multiview driving videos cogdriving leverages diffusion transformer architecture attention modules enabling simultaneous associations across spatial temporal viewpoint dimensions also propose lightweight controller tailored cogdriving ie microcontroller uses parameters standard controlnet enabling precise control birdseyeview layouts enhance generation object instances crucial autonomous driving propose reweighted learning objective dynamically adjusting learning weights object instances training cogdriving demonstrates strong performance nuscenes validation set achieving fvd score highlighting ability generate realistic driving videos project found httpsluhannangithubiocogdrivingpage,-1,0.0,-1,0.0
ditto motionspace diffusion controllable realtime talking head synthesis recent advances diffusion models revolutionized audiodriven talking head synthesis beyond precise lip synchronization diffusionbased methods excel generating subtle expressions natural head movements wellaligned audio signal however methods confronted slow inference speed insufficient finegrained control facial motions occasional visual artifacts largely due implicit latent space derived variational autoencoders vae prevent adoption realtime interaction applications address issues introduce ditto diffusionbased framework enables controllable realtime talking head synthesis key innovation lies bridging motion generation photorealistic neural rendering explicit identityagnostic motion space replacing conventional vae representations design substantially reduces complexity diffusion learning enabling precise control synthesized talking heads propose inference strategy jointly optimizes three key components audio feature extraction motion generation video synthesis optimization enables streaming processing realtime inference low firstframe delay functionalities crucial interactive applications ai assistants extensive experimental results demonstrate ditto generates compelling talking head videos substantially outperforms existing methods motion control realtime performance,-1,0.0,-1,0.0
direct preference optimization video large multimodal models language model reward preference modeling techniques direct preference optimization dpo shown effective enhancing generalization abilities large language model llm however tasks involving video instructionfollowing providing informative feedback especially detecting hallucinations generated responses remains significant challenge previous studies explored using large large multimodal models lmms reward models guide preference modeling ability accurately assess factuality generated responses compared corresponding videos conclusively established paper introduces novel framework utilizes detailed video captions proxy video content enabling language models incorporate information supporting evidence scoring video question answering qa predictions approach demonstrates robust alignment openai models reward mechanism directly takes video frames input furthermore show applying tailored reward dpo significantly improves performance video lmms video qa tasks,0,0.8580448205442166,0,0.8580448205442166
svastin sparse video adversarial attack via spatiotemporal invertible neural networks robust imperceptible adversarial video attack challenging due spatial temporal characteristics videos existing video adversarial attack methods mainly take gradientbased approach generate adversarial videos noticeable perturbations paper propose novel sparse adversarial video attack via spatiotemporal invertible neural networks svastin generate adversarial videos spatiotemporal feature space information exchanging consists guided target video learning gtvl module balance perturbation budget optimization speed spatiotemporal invertible neural network stin module perform spatiotemporal feature space information exchanging source video target feature tensor learned gtvl module extensive experiments demonstrate proposed svastin generate adversarial examples higher imperceptibility stateoftheart methods higher fooling rate code available hrefhttpsgithubcombrittanychensvastinhttpsgithubcombrittanychensvastin,-1,0.0,-1,0.0
interdyn controllable interactive dynamics video diffusion models predicting dynamics interacting objects essential humans intelligent systems however existing approaches limited simplified toy settings lack generalizability complex realworld environments recent advances generative models enabled prediction state transitions based interventions focus generating single future state neglects continuous dynamics resulting interaction address gap propose interdyn novel framework generates videos interactive dynamics given initial frame control signal encoding motion driving object actor key insight large video generation models act neural renderers implicit physics simulators learned interactive dynamics largescale video data effectively harness capability introduce interactive control mechanism conditions video generation process motion driving entity qualitative results demonstrate interdyn generates plausible temporally consistent videos complex object interactions generalizing unseen objects quantitative evaluations show interdyn outperforms baselines focus static state transitions work highlights potential leveraging video generative models implicit physics engines code trained models released httpsinterdynistuempgde,18,1.0,18,1.0
enhancing gans contrastive learningbased multistage progressive finetuning snn rlbased external optimization generative adversarial networks gans forefront image synthesis especially medical fields like histopathology help address challenges data scarcity patient privacy class imbalance however several inherent domainspecific issues remain gans training instability mode collapse insufficient feedback binary classification undermine performance challenges particularly pronounced highresolution histopathology images due complex feature representation high spatial detail response challenges work proposes novel framework integrating contrastive learningbased multistage progressive finetuning siamese neural network mftsnn reinforcement learningbased external optimizer rleo mftsnn improves feature similarity extraction histopathology data rleo acts rewardbased guide balance gan training addressing mode collapse enhancing output quality proposed approach evaluated stateoftheart sota gan models demonstrates superior performance across multiple metrics,17,0.9912963490882731,17,0.9912963490882731
revideo remake video motion content control despite significant advancements video generation editing using diffusion models achieving accurate localized video editing remains substantial challenge additionally existing video editing methods primarily focus altering visual content limited research dedicated motion editing paper present novel attempt remake video revideo stands existing methods allowing precise video editing specific areas specification content motion content editing facilitated modifying first frame trajectorybased motion control offers intuitive user interaction experience revideo addresses new task involving coupling training imbalance content motion control tackle develop threestage training strategy progressively decouples two aspects coarse fine furthermore propose spatiotemporal adaptive fusion module integrate content motion control across various sampling steps spatial locations extensive experiments demonstrate revideo promising performance several accurate video editing applications ie locally changing video content keeping motion constant keeping content unchanged customizing new motion trajectories modifying content motion trajectories method also seamlessly extend applications multiarea editing without specific training demonstrating flexibility robustness,9,1.0,9,1.0
videollm knows speak enhancing timesensitive video comprehension videotext duet interaction format recent researches video large language models videollm predominantly focus model architectures training datasets leaving interaction format user model underexplored existing works users often interact videollms using entire video query input model generates response interaction format constrains application videollms scenarios livestreaming comprehension videos end responses required realtime manner also results unsatisfactory performance timesensitive tasks requires localizing video segments paper focus videotext duet interaction format interaction format characterized continuous playback video user model insert text messages position video playback text message ends video continues play akin alternative two performers duet construct mmduetit videotext training dataset designed adapt videollms videotext duet interaction format also introduce multianswer grounded video question answering magqa task benchmark realtime response ability videollms trained mmduetit mmduet demonstrates adopting videotext duet interaction format enables model achieve significant improvements various timesensitive tasks cider dense video captioning map qvhighlights highlight detection charadessta temporal video grounding minimal training efforts also enable videollms reply realtime manner video plays code data demo available httpsgithubcomyellowbinarytreemmduet,0,1.0,0,1.0
taming multimodal joint training highquality videotoaudio synthesis propose synthesize highquality synchronized audio given video optional text conditions using novel multimodal joint training framework mmaudio contrast singlemodality training conditioned limited video data mmaudio jointly trained largerscale readily available textaudio data learn generate semantically aligned highquality audio samples additionally improve audiovisual synchrony conditional synchronization module aligns video conditions audio latents frame level trained flow matching objective mmaudio achieves new videotoaudio stateoftheart among public models terms audio quality semantic alignment audiovisual synchronization low inference time generate clip parameters mmaudio also achieves surprisingly competitive performance texttoaudio generation showing joint training hinder singlemodality performance code demo available httpshkchengrexgithubiommaudio,8,0.6545382396290252,8,0.6545382396290252
human video generation novel scenarios enables generalizable robot manipulation robot manipulation policies generalize novel tasks involving unseen object types new motions paper provide solution terms predicting motion information web data human video generation conditioning robot policy generated video instead attempting scale robot data collection expensive show leverage video generation models trained easily available web data enabling generalization approach casts languageconditioned manipulation zeroshot human video generation followed execution single policy conditioned generated video train policy use order magnitude less robot interaction data compared video prediction model trained doesnt require finetuning video model directly use pretrained model generating human videos results diverse realworld scenarios show enables manipulating unseen object types performing novel motions tasks present robot data videos,5,0.6566779309519957,5,0.6566779309519957
llmguided compositional scene generation recent advancements diffusion models content creation sparked surge interest generating content however scarcity scene datasets constrains current methodologies primarily objectcentric generation overcome limitation present novel framework compositional generation unlike conventional methods generate singular representation entire scene innovatively constructs object within scene separately utilizing large language models llms framework begins decomposing input text prompt distinct entities maps trajectories constructs compositional scene accurately positioning objects along designated paths refine scene method employs compositional score distillation technique guided predefined trajectories utilizing pretrained diffusion models across texttoimage texttovideo domains extensive experiments demonstrate outstanding content creation capability compared prior arts showcasing superior visual quality motion fidelity enhanced object interactions,-1,0.0,-1,0.0
fundus fluorescein angiography video generation retinal generative foundation model fundus fluorescein angiography ffa crucial diagnosing monitoring retinal vascular issues limited invasive nature restricted accessibility compared color fundus cf imaging existing methods convert cf images ffa confined static image generation missing dynamic lesional changes introduce autoregressive generative adversarial network gan model generates dynamic ffa videos single cf images excels video generation achieving fvd psnr clinical experts validated fidelity generated videos additionally models generator demonstrates remarkable downstream transferability across ten external public datasets including blood vessel segmentation retinal disease diagnosis systemic disease prediction multimodal retrieval showcasing impressive zeroshot fewshot capabilities findings position powerful noninvasive alternative ffa exams versatile retinal generative foundation model captures static temporal retinal features enabling representation complex intermodality relationships,19,1.0,19,1.0
segment anything videos systematic survey recent wave foundation models witnessed tremendous success computer vision cv beyond segment anything model sam sparked passion exploring taskagnostic visual foundation models empowered remarkable zeroshot generalization sam currently challenging numerous traditional paradigms cv delivering extraordinary performance various image segmentation multimodal segmentation eg texttomask tasks also video domain additionally latest released sam sparking research enthusiasm realm promptable visual segmentation images videos however existing surveys mainly focus sam various image processing tasks comprehensive indepth review video domain notably absent address gap work conducts systematic review sam videos era foundation models first review progress sam videos work focuses applications various tasks discussing recent advances innovation opportunities developing foundation models broad applications begin brief introduction background sam videorelated research domains subsequently present systematic taxonomy categorizes existing methods three key areas video understanding video generation video editing analyzing summarizing advantages limitations furthermore comparative results sambased current stateoftheart methods representative benchmarks well insightful analysis offered finally discuss challenges faced current research envision several future research directions field sam video beyond,10,1.0,10,1.0
ifmdm implicit face motion diffusion model highfidelity realtime talking head generation introduce novel approach highresolution talking head generation single image audio input prior methods using explicit face models like morphable models facial landmarks often fall short generating highfidelity videos due lack appearanceaware motion representation generative approaches video diffusion models achieve high video quality slow processing speeds limit practical application proposed model implicit face motion diffusion model ifmdm employs implicit motion encode human faces appearanceaware compressed facial latents enhancing video generation although implicit motion lacks spatial disentanglement explicit models complicates alignment subtle lip movements introduce motion statistics help capture finegrained motion information additionally model provides motion controllability optimize tradeoff motion intensity visual quality inference ifmdm supports realtime generation resolution videos frames per second fps extensive evaluations demonstrate superior performance existing diffusion explicit face models code released publicly available alongside supplementary materials video results found,6,0.4819476250477237,6,0.4819476250477237
worldgpt sorainspired video ai agent rich world models text image inputs several texttovideo diffusion models demonstrated commendable capabilities synthesizing highquality video content however remains formidable challenge pertaining maintaining temporal consistency ensuring action smoothness throughout generated sequences paper present innovative video generation ai agent harnesses power sorainspired multimodal learning build skilled world models framework based textual prompts accompanying images framework includes two parts prompt enhancer full video translation first part employs capabilities chatgpt meticulously distill proactively construct precise prompts subsequent step thereby guaranteeing utmost accuracy prompt communication accurate execution following model operations second part employ compatible existing advanced diffusion techniques expansively generate refine key frame conclusion video expertly harness power leading trailing key frames craft videos enhanced temporal consistency action smoothness experimental results confirm method strong effectiveness novelty constructing world models text image inputs methods,-1,0.0,-1,0.0
mimir improving video diffusion models precise text understanding text serves key control signal video generation due narrative nature render text descriptions video clips current video diffusion models borrow features text encoders yet struggle limited text comprehension recent success large language models llms showcases power decoderonly transformers offers three clear benefits texttovideo generation namely precise text understanding resulting superior scalability imagination beyond input text enabled next token prediction flexibility prioritize user interests instruction tuning nevertheless feature distribution gap emerging two different text modeling paradigms hinders direct use llms established models work addresses challenge mimir endtoend training framework featuring carefully tailored token fuser harmonize outputs text encoders llms design allows model fully leverage learned video priors capitalizing textrelated capability llms extensive quantitative qualitative results demonstrate effectiveness mimir generating highquality videos excellent text comprehension especially processing short captions managing shifting motions project page httpslucariaacademygithubiomimir,-1,0.0,-1,0.0
rain realtime animation infinite video stream live animation gained immense popularity enhancing online engagement yet achieving highquality realtime stable animation diffusion models remains challenging especially consumergrade gpus existing methods struggle generating long consistent video streams efficiently often limited latency issues degraded visual quality extended periods paper introduce rain pipeline solution capable animating infinite video streams realtime low latency using single rtx gpu core idea rain efficiently compute frametoken attention across different noise levels long timeintervals simultaneously denoising significantly larger number frametokens previous streambased methods design allows rain generate video frames much shorter latency faster speed maintaining longrange attention extended video streams resulting enhanced continuity consistency consequently stable diffusion model finetuned rain epochs produce video streams realtime low latency without much compromise quality consistency infinite long despite advanced capabilities rain introduces additional attention blocks imposing minimal additional burden experiments benchmark datasets generating superlong videos demonstrating rain animate characters realtime much better quality accuracy consistency competitors costing less latency code models made publicly available,-1,0.0,-1,0.0
romo robust motion segmentation improves structure motion extensive progress reconstruction generation scenes monocular casuallycaptured video tasks rely heavily known camera poses problem finding poses using structurefrommotion sfm often depends robustly separating static dynamic parts video lack robust solution problem limits performance sfm cameracalibration pipelines propose novel approach videobased motion segmentation identify components scene moving wrt fixed world frame simple effective iterative method romo combines optical flow epipolar cues pretrained video segmentation model outperforms unsupervised baselines motion segmentation well supervised baselines trained synthetic data importantly combination offtheshelf sfm pipeline segmentation masks establishes new stateoftheart camera calibration scenes dynamic content outperforming existing methods substantial margin,1,1.0,1,1.0
languageguided selfsupervised video summarization using text semantic matching considering diversity video current video summarization methods rely heavily supervised computer vision techniques demands timeconsuming subjective manual annotations overcome limitations investigated selfsupervised video summarization inspired success large language models llms explored feasibility transforming video summarization task natural language processing nlp task leveraging advantages llms context understanding aim enhance effectiveness selfsupervised video summarization method begins generating captions individual video frames synthesized text summaries llms subsequently measure semantic distance captions text summary notably propose novel loss function optimize model according diversity video finally summarized video generated selecting frames captions similar text summary method achieves stateoftheart performance summe dataset rank correlation coefficients addition method novel feature able achieve personalized summarization,0,1.0,0,1.0
largescale dataset texttovideo generation quality videotext pairs fundamentally determines upper bound texttovideo models currently datasets used training models suffer significant shortcomings including low temporal consistency poorquality captions substandard video quality imbalanced data distribution prevailing video curation process depends image models tagging manual rulebased curation leads high computational load leaves behind unclean data result lack appropriate training datasets texttovideo models address problem present superior training dataset texttovideo models produced coarsetofine curation strategy dataset guarantees highquality videos detailed captions excellent temporal consistency used train video generation model dataset led experimental results surpass obtained models,-1,0.0,-1,0.0
sdipaste synthetic dynamic instance copypaste video instance segmentation data augmentation methods copypaste studied effective ways expand training datasets incurring minimal costs methods extensively implemented image level tasks found scalable implementation copypaste built specifically video tasks paper leverage recent growth video fidelity generative models explore effective ways incorporating synthetically generated objects existing video datasets artificially expand object instance pools first procure synthetic video sequences featuring objects morph dynamically time carefully devised pipeline automatically segments copypastes dynamic instances across frames target background video sequence name video data augmentation pipeline synthetic dynamic instance copypaste test complex task video instance segmentation combines detection segmentation tracking object instances across video sequence extensive experiments popular youtubevis dataset using two separate popular networks baselines achieve strong gains ap ap make code models publicly available,7,0.9058553770214619,7,0.9058553770214619
video repurposing user generated content largescale dataset benchmark demand producing shortform videos sharing social media platforms experienced significant growth recent times despite notable advancements fields video summarization highlight detection create partially usable short films raw videos approaches often domainspecific require indepth understanding realworld video content tackle predicament propose extensive dataset comprising videos annotated clips aimed resolving video longtoshort task recognizing inherent constraints posed untrained human annotators result inaccurate annotations repurposed videos propose twostage solution obtain annotations realworld usergenerated content furthermore offer baseline model address challenging task integrating audio visual caption aspects crossmodal fusion alignment framework aspire work ignite groundbreaking research lesserexplored realms video repurposing,0,0.9087378028107999,0,0.9087378028107999
disentangled motion modeling video frame interpolation video frame interpolation vfi aims synthesize intermediate frames existing frames enhance visual smoothness quality beyond conventional methods based reconstruction loss recent works employed generative models improved perceptual quality however require complex training large computational costs pixel space modeling paper introduce disentangled motion modeling momo diffusionbased approach vfi enhances visual quality focusing intermediate motion modeling propose disentangled twostage training process initial stage frame synthesis flow models trained generate accurate frames flows optimal synthesis subsequent stage introduce motion diffusion model incorporates novel unet architecture specifically designed optical flow generate bidirectional flows frames learning simpler lowfrequency representation motions momo achieves superior perceptual quality reduced computational demands compared generative modeling methods pixel space momo surpasses stateoftheart methods perceptual metrics across various benchmarks demonstrating efficacy efficiency vfi,-1,0.0,-1,0.0
general method incorporate spatial information loss functions ganbased superresolution models generative adversarial networks gans shown great performance superresolution problems since generate visually realistic images video frames however models often introduce side effects outputs unexpected artifacts noises reduce artifacts enhance perceptual quality results paper propose general method effectively used ganbased superresolution sr models introducing essential spatial information training process extract spatial information input data incorporate training loss making corresponding loss spatially adaptive sa one utilize guide training process show proposed approach independent methods used extract spatial information independent sr tasks models method consistently guides training process towards generating visually pleasing sr images video frames substantially mitigating artifacts noise ultimately leading enhanced perceptual quality,-1,0.0,-1,0.0
fade dataset detecting falling objects around buildings video falling objects buildings cause severe injuries pedestrians due great impact force exert although surveillance cameras installed around buildings challenging humans capture events surveillance videos due small size fast motion falling objects well complex background therefore necessary develop methods automatically detect falling objects around buildings surveillance videos facilitate investigation falling object detection propose large diverse video dataset called fade falling object detection around buildings first time fade contains videos scenes featuring falling object categories weather conditions video resolutions additionally develop new object detection method called fadenet effectively leverages motion information produces smallsized highquality proposals detecting falling objects around buildings importantly method extensively evaluated analyzed comparing previous approaches used generic object detection video object detection moving object detection fade dataset experimental results show proposed fadenet significantly outperforms methods providing effective baseline future research dataset code publicly available httpsfadedatasetgithubiofadegithubio,-1,0.0,-1,0.0
retomeva recursive token merging video diffusionbased unrestricted adversarial attack recent diffusionbased unrestricted attacks generate imperceptible adversarial examples high transferability compared previous unrestricted attacks restricted attacks however existing works diffusionbased unrestricted attacks mostly focused images yet seldom explored videos paper propose recursive token merging video diffusionbased unrestricted adversarial attack retomeva first framework generate imperceptible adversarial video clips higher transferability specifically achieve spatial imperceptibility retomeva adopts timestepwise adversarial latent optimization talo strategy optimizes perturbations diffusion models latent space denoising step talo offers iterative accurate updates generate powerful adversarial frames talo reduce memory consumption gradient computation moreover achieve temporal imperceptibility retomeva introduces recursive token merging retome mechanism matching merging tokens across video frames selfattention module resulting temporally consistent adversarial videos retome concurrently facilitates interframe interactions attack process inducing diverse robust gradients thus leading better adversarial transferability extensive experiments demonstrate efficacy retomeva particularly surpassing stateoftheart attacks adversarial transferability average,4,0.9955334186393171,4,0.9955334186393171
unimlvg unified framework multiview long video generation comprehensive control capabilities autonomous driving creation diverse realistic driving scenarios become essential enhance perception planning capabilities autonomous driving system however generating longduration surroundview consistent driving videos remains significant challenge address present unimlvg unified framework designed generate extended street multiperspective videos precise control integrating single multiview driving videos training data approach updates ditbased diffusion model equipped crossframe crossview modules across three stages multi training objectives substantially boosting diversity quality generated visual content importantly propose innovative explicit viewpoint modeling approach multiview video generation effectively improve motion transition consistency capable handling various input reference formats eg text images video unimlvg generates highquality multiview videos according corresponding condition constraints bounding boxes framelevel text descriptions compared best models similar capabilities framework achieves improvements fid fvd,16,1.0,16,1.0
fifodiffusion generating infinite videos text without training propose novel inference technique based pretrained diffusion model textconditional video generation approach called fifodiffusion conceptually capable generating infinitely long videos without additional training achieved iteratively performing diagonal denoising simultaneously processes series consecutive frames increasing noise levels queue method dequeues fully denoised frame head enqueuing new random noise frame tail however diagonal denoising doubleedged sword frames near tail take advantage cleaner frames forward reference strategy induces discrepancy training inference hence introduce latent partitioning reduce traininginference gap lookahead denoising leverage benefit forward referencing practically fifodiffusion consumes constant amount memory regardless target video length given baseline model wellsuited parallel inference multiple gpus demonstrated promising results effectiveness proposed methods existing texttovideo generation baselines generated video examples source codes available project page,-1,0.0,-1,0.0
comprehensive survey synthetic infrared image synthesis synthetic infrared ir scene target generation important computer vision problem allows generation realistic ir images targets training testing various applications remote sensing surveillance target recognition also helps reduce cost risk associated collecting realworld ir data survey paper aims provide comprehensive overview conventional mathematical modellingbased methods deep learningbased methods used generating synthetic ir scenes targets paper discusses importance synthetic ir scene target generation briefly covers mathematics blackbody grey body radiations well ir imagecapturing methods potential use cases synthetic ir scenes target generation also described highlighting significance techniques various fields additionally paper explores possible new ways developing new techniques enhance efficiency effectiveness synthetic ir scenes target generation highlighting need research advance field,14,0.9981447790758675,14,0.9981447790758675
controlnext powerful efficient control image video generation diffusion models demonstrated remarkable robust abilities image video generation achieve greater control generated results researchers introduce additional architectures controlnet adapters referencenet integrate conditioning controls however current controllable generation methods often require substantial additional computational resources especially video generation face challenges training exhibit weak control paper propose controlnext powerful efficient method controllable image video generation first design straightforward efficient architecture replacing heavy additional branches minimal additional cost compared base model concise structure also allows method seamlessly integrate lora weights enabling style alteration without need additional training training reduce learnable parameters compared alternatives furthermore propose another method called cross normalization cn replacement zeroconvolution achieve fast stable training convergence conducted various experiments different base models across images videos demonstrating robustness method,-1,0.0,-1,0.0
cyclo cyclic graph transformer approach multiobject relationship modeling aerial videos video scene graph generation vidsgg emerged transformative approach capturing interpreting intricate relationships among objects temporal dynamics video sequences paper introduce new aeroeye dataset focuses multiobject relationship modeling aerial videos aeroeye dataset features various drone scenes includes visually comprehensive precise collection predicates capture intricate relationships spatial arrangements among objects end propose novel cyclic graph transformer cyclo approach allows model capture direct longrange temporal dependencies continuously updating history interactions circular manner proposed approach also allows one handle sequences inherent cyclical patterns process object relationships correct sequential order therefore effectively capture periodic overlapping relationships minimizing information loss extensive experiments aeroeye dataset demonstrate effectiveness proposed cyclo model demonstrating potential perform scene understanding drone videos finally cyclo method consistently achieves stateoftheart sota results two inthewild scene graph generation benchmarks ie pvsg aspire,-1,0.0,-1,0.0
meshbrush painting anatomical mesh neural stylization endoscopy style transfer promising approach close simtoreal gap medical endoscopy rendering synthetic endoscopic videos traversing preoperative scans mri ct generate structurally accurate simulations well ground truth camera poses depth maps although imagetoimage translation models cyclegan imitate realistic endoscopic images simulations unsuitable videotovideo synthesis due lack temporal consistency resulting artifacts frames propose meshbrush neural mesh stylization method synthesize temporally consistent videos differentiable rendering meshbrush uses underlying geometry patient imaging data leveraging existing methods learned pervertex textures stylized mesh guarantees consistency producing highfidelity outputs demonstrate mesh stylization promising approach creating realistic simulations downstream tasks training networks preoperative planning although method tested designed ureteroscopy components transferable general endoscopic laparoscopic procedures code made public github,-1,0.0,-1,0.0
beyond deepfake images detecting aigenerated videos recent advances generative ai led development techniques generate visually realistic synthetic video number techniques developed detect aigenerated synthetic images paper show synthetic image detectors unable detect synthetic videos demonstrate synthetic video generators introduce substantially different traces left image generators despite show synthetic video traces learned used perform reliable synthetic video detection generator source attribution even recompression furthermore demonstrate detecting videos new generators zeroshot transferability challenging accurate detection videos new generator achieved fewshot learning,-1,0.0,-1,0.0
generative ai replace immunofluorescent staining processes comparison study synthetically generated cellpainting images brightfield cell imaging assays utilizing fluorescence stains essential observing subcellular organelles responses perturbations immunofluorescent staining process routinely labs however recent innovations generative ai challenging idea staining required especially true availability cost specific fluorescence dyes problem labs furthermore staining process takes time leads interintra technician hinders downstream image data analysis reusability image data projects recent studies showed use generated synthetic immunofluorescence images brightfield bf images using generative ai algorithms literature therefore study benchmark compare five models three types generation backbones cnn gan diffusion models using publicly available dataset paper serves comparative study determine bestperforming model also proposes comprehensive analysis pipeline evaluating efficacy generators image synthesis highlighted potential deep learningbased generators image synthesis also discussed potential issues future research directions although generative ai shows promise simplifying cell phenotyping using bf images staining research validations needed address key challenges model generalisability batch effects feature relevance computational costs,3,0.7847038064441724,3,0.7847038064441724
infinicube unbounded controllable dynamic driving scene generation worldguided video models present infinicube scalable method generating unbounded dynamic driving scenes high fidelity controllability previous methods scene generation either suffer limited scales lack geometric appearance consistency along generated sequences contrast leverage recent advancements scalable representation video models achieve large dynamic scene generation allows flexible controls hd maps vehicle bounding boxes text descriptions first construct mapconditioned sparsevoxelbased generative model unleash power unbounded voxel world generation repurpose video model ground voxel world set carefully designed pixelaligned guidance buffers synthesizing consistent appearance finally propose fast feedforward approach employs voxel pixel branches lift dynamic videos dynamic gaussians controllable objects method generate controllable realistic driving scenes extensive experiments validate effectiveness superiority model,-1,0.0,-1,0.0
make cheap scaling selfcascade diffusion model higherresolution adaptation diffusion models proven highly effective image video generation however encounter challenges correct composition objects generating images varying sizes due singlescale training data adapting large pretrained diffusion models higher resolution demands substantial computational optimization resources yet achieving generation capabilities comparable lowresolution models remains challenging paper proposes novel selfcascade diffusion model leverages knowledge gained welltrained lowresolution imagevideo generation model enabling rapid adaptation higherresolution generation building employ pivot replacement strategy facilitate tuningfree version progressively leveraging reliable semantic guidance derived lowresolution model propose integrate sequence learnable multiscale upsampler modules tuning version capable efficiently learning structural details new scale small amount newly acquired highresolution training data compared full finetuning approach achieves training speedup requires tuning parameters extensive experiments demonstrate approach quickly adapt higherresolution image video synthesis finetuning steps virtually additional inference time,-1,0.0,-1,0.0
vlogger multimodal diffusion embodied avatar synthesis propose vlogger method audiodriven human video generation single input image person builds success recent generative diffusion models method consists stochastic diffusion model novel diffusionbased architecture augments texttoimage models spatial temporal controls supports generation high quality video variable length easily controllable highlevel representations human faces bodies contrast previous work method require training person rely face detection cropping generates complete image face lips considers broad spectrum scenarios eg visible torso diverse subject identities critical correctly synthesize humans communicate also curate mentor new diverse dataset pose expression annotations one order magnitude larger previous ones identities dynamic gestures train ablate main technical contributions vlogger outperforms stateoftheart methods three public benchmarks considering image quality identity preservation temporal consistency also generating upperbody gestures analyze performance vlogger respect multiple diversity metrics showing architectural choices use mentor benefit training fair unbiased model scale finally show applications video editing personalization,6,0.4516646520007367,6,0.4516646520007367
gaussian splatting modeling dynamic scenes native primitives dynamic scene representation novel view synthesis captured videos crucial enabling immersive experiences required arvr metaverse applications however task challenging due complexity unconstrained realworld scenes temporal dynamics paper frame dynamic scenes spatiotemporal volume learning problem offering native explicit reformulation minimal assumptions motion serves versatile dynamic scene learning framework specifically represent target dynamic scene using collection gaussian primitives explicit geometry appearance features dubbed gaussian splatting approach capture relevant information space time fitting underlying spatiotemporal volume modeling spacetime whole gaussians parameterized anisotropic ellipses rotate arbitrarily space time model naturally learn viewdependent timeevolved appearance spherindrical harmonics notably model first solution supports realtime rendering highresolution photorealistic novel views complex dynamic scenes enhance efficiency derive several compact variants effectively reduce memory footprint mitigate risk overfitting extensive experiments validate superiority terms visual quality efficiency across range dynamic scenerelated tasks eg novel view synthesis generation scene understanding scenarios eg single object indoor scenes driving environments synthetic real data,1,1.0,1,1.0
fast spatialtemporal consistent generation via video diffusion models availability largescale multimodal datasets advancements diffusion models significantly accelerated progress content generation prior approaches rely multiple image video diffusion models utilizing score distillation sampling optimization generating pseudo novel views direct supervision however methods hindered slow optimization speeds multiview inconsistency issues spatial temporal consistency geometry extensively explored respectively diffusion models traditional monocular video diffusion models building foundation propose strategy migrate temporal consistency video diffusion models spatialtemporal consistency required generation specifically present novel framework efficient scalable content generation leveraging meticulously curated dynamic dataset develop video diffusion model capable synthesizing orbital views dynamic assets control dynamic strength assets introduce motion magnitude metric guidance additionally propose novel motion magnitude reconstruction loss classifierfree guidance refine learning generation motion dynamics obtaining orbital views asset perform explicit construction gaussian splatting coarsetofine manner synthesized multiview consistent image set enables us swiftly generate highfidelity diverse assets within several minutes extensive experiments demonstrate method surpasses prior stateoftheart techniques terms generation efficiency geometry consistency across various prompt modalities,-1,0.0,-1,0.0
oneclick upgrade sandwiched rgbd video compression stereoscopic teleconferencing stereoscopic video conferencing still challenging due need compress stereo rgbd video realtime though hardware implementations standard video codecs avc hevc widely available designed stereoscopic videos suffer reduced quality performance specific multiview extensions codecs complex lack efficient implementations paper propose new approach upgrade video codec support stereo rgbd video compression wrapping neural pre postprocessor pair neural networks endtoend trained image codec proxy shown work sophisticated video codec also propose geometryaware loss function improve rendering quality train neural pre postprocessors synthetic people dataset evaluate synthetic realcaptured stereo rgbd videos experimental results show neural networks generalize well unseen data work outofbox various video codecs approach saves bitrate compared conventional video coding scheme mvhevc level rendering quality novel view without need taskspecific hardware upgrade,-1,0.0,-1,0.0
vloggeraugmented graph neural network model microvideo recommendation existing microvideo recommendation models exploit interactions users microvideos andor multimodal information microvideos predict next microvideo user watch ignoring information related vloggers ie producers microvideos however microvideo scenarios vloggers play significant role uservideo interactions since vloggers generally focus specific topics users tend follow vloggers interested therefore paper propose vloggeraugmented graph neural network model vagnn takes effect vloggers consideration specifically construct tripartite graph users microvideos vloggers nodes capturing user preferences different views ie videoview vloggerview moreover conduct crossview contrastive learning keep consistency node embeddings two different views besides predicting next uservideo interaction adaptively combine user preferences video vlogger conduct extensive experiments two realworld datasets experimental results show vagnn outperforms multiple existing gnnbased recommendation models,-1,0.0,-1,0.0
dlkdd duallight knowledge distillation action recognition dark human action recognition dark videos challenging task computer vision recent research focuses applying dark enhancement methods improve visibility video however video processing results loss critical information original unenhanced video conversely traditional twostream methods capable learning information original processed videos lead significant increase computational cost inference phase task video classification address challenges propose novel teacherstudent video classification framework named duallight knowledge distillation action recognition dark dlkdd framework enables model learn original enhanced video without introducing additional computational cost inference specifically dlkdd utilizes strategy knowledge distillation training teacher model trained enhanced video student model trained original video soft target generated teacher model teacherstudent framework allows student model predict action using original input video inference experiments proposed dlkdd framework outperforms stateoftheart methods arid arid datasets achieve best performance dataset improvement using original video inputs thus avoiding use twostream framework enhancement modules inference validate effectiveness distillation strategy ablative experiments results highlight advantages knowledge distillation framework dark human action recognition,7,0.8170518315267903,7,0.8170518315267903
videollmonline online video large language model streaming video recent large language models enhanced vision capabilities enabling comprehend images videos interleaved visionlanguage content however learning methods large multimodal models typically treat videos predetermined clips making less effective efficient handling streaming video inputs paper propose novel learninginvideostream live framework enables temporally aligned longcontext realtime conversation within continuous video stream live framework comprises comprehensive approaches achieve video streaming dialogue encompassing training objective designed perform language modeling continuous streaming inputs data generation scheme converts offline temporal annotations streaming dialogue format optimized inference pipeline speed model responses realworld video streams live framework built videollmonline model upon demonstrate significant advantages processing streaming videos instance average model support streaming dialogue video clip fps gpu moreover also showcases stateoftheart performance public offline video benchmarks recognition captioning forecasting code model data demo made available httpsshowlabgithubiovideollmonline,0,1.0,0,1.0
clipvqavideo quality assessment via clip learning visionlanguage representations webscale data contrastive languageimage pretraining clip mechanism demonstrated remarkable performance many vision tasks however application widely studied video quality assessment vqa task still open issue paper propose efficient effective clipbased transformer method vqa problem clipvqa specifically first design effective video frame perception paradigm goal extracting rich spatiotemporal quality content information among video frames spatiotemporal quality features adequately integrated together using selfattention mechanism yield videolevel quality representation utilize quality language descriptions videos supervision develop clipbased encoder language embedding fully aggregated generated content information via crossattention module producing videolanguage representation finally videolevel quality videolanguage representations fused together final video quality prediction vectorized regression loss employed efficient endtoend optimization comprehensive experiments conducted eight inthewild video datasets diverse resolutions evaluate performance clipvqa experimental results show proposed clipvqa achieves new stateoftheart vqa performance better generalizability existing benchmark vqa methods series ablation studies also performed validate effectiveness module clipvqa,12,0.907156221339356,12,0.907156221339356
learning video context interleaved multimodal sequences narrative videos movies pose significant challenges video understanding due rich contexts characters dialogues storylines diverse demands identify relationship reason paper introduce movieseq multimodal language model developed address wide range challenges understanding video contexts core idea represent videos interleaved multimodal sequences including images plots videos subtitles either linking external knowledge databases using offline models whisper subtitles instructiontuning approach empowers language model interact videos using interleaved multimodal instructions example instead solely relying video input jointly provide character photos alongside names dialogues allowing model associate elements generate comprehensive responses demonstrate effectiveness validate movieseqs performance six datasets lvu mad movienet cmd tvc movieqa across five settings video classification audio description videotext retrieval video captioning video questionanswering code public httpsgithubcomshowlabmovieseq,0,1.0,0,1.0
et bench towards openended eventlevel videolanguage understanding recent advances video large language models videollms demonstrated great potential generalpurpose video understanding verify significance models number benchmarks proposed diagnose capabilities different scenarios however existing benchmarks merely evaluate models videolevel questionanswering lacking finegrained eventlevel assessment task diversity fill gap introduce et bench eventlevel timesensitive video understanding benchmark largescale highquality benchmark openended eventlevel video understanding categorized within task taxonomy et bench encompasses samples tasks videos total length domains providing comprehensive evaluations extensively evaluated imagellms videollms benchmark results reveal stateoftheart models coarselevel videolevel understanding struggle solve finegrained tasks eg grounding eventofinterests within videos largely due short video context length improper time representations lack multievent training data focusing issues propose strong baseline model et chat together instructiontuning dataset et instruct tailored finegrained eventlevel understanding simple effective solution demonstrates superior performance multiple scenarios,0,0.9940346772183354,0,0.9940346772183354
seconds hours reviewing multimodal large language models comprehensive long video understanding integration large language models llms visual encoders recently shown promising performance visual understanding tasks leveraging inherent capability comprehend generate humanlike text visual reasoning given diverse nature visual data multimodal large language models mmllms exhibit variations model designing training understanding images short videos long videos paper focuses substantial differences unique challenges posed long video understanding compared static image short video understanding unlike static images short videos encompass sequential frames spatial withinevent temporal information long videos consist multiple events betweenevent longterm temporal information survey aim trace summarize advancements mmllms image understanding long video understanding review differences among various visual understanding tasks highlight challenges long video understanding including finegrained spatiotemporal details dynamic events longterm dependencies provide detailed summary advancements mmllms terms model design training methodologies understanding long videos finally compare performance existing mmllms video understanding benchmarks various lengths discuss potential future directions mmllms long video understanding,0,0.9950227277837327,0,0.9950227277837327
video seal open efficient video watermarking proliferation aigenerated content sophisticated video editing tools made important challenging moderate digital platforms video watermarking addresses challenges embedding imperceptible signals videos allowing identification however rare open tools methods often fall short efficiency robustness flexibility reduce gaps paper introduces video seal comprehensive framework neural video watermarking competitive opensourced model approach jointly trains embedder extractor ensuring watermark robustness applying transformations inbetween eg video codecs training multistage includes image pretraining hybrid posttraining extractor finetuning also introduce temporal watermark propagation technique convert image watermarking model efficient video watermarking model without need watermark every highresolution frame present experimental results demonstrating effectiveness approach terms speed imperceptibility robustness video seal achieves higher robustness compared strong baselines especially challenging distortions combining geometric transformations video compression additionally provide new insights impact video compression training compare methods operating different payloads contributions work including codebase models public demo opensourced permissive licenses foster research development field,4,1.0,4,1.0
tinq temporal inconsistency guided blind video quality assessment blind video quality assessment bvqa actively researched usergenerated content ugc videos recently superresolution sr techniques widely applied ugc therefore effective bvqa method ugc sr scenarios essential temporal inconsistency referring irregularities consecutive frames relevant video quality current bvqa approaches typically model temporal relationships ugc videos using statistics motion information inconsistencies remain unexplored additionally different temporal inconsistency ugc videos inconsistency sr videos amplified due upscaling algorithms paper introduce temporal inconsistency guided blind video quality assessment tinq metric demonstrating exploring temporal inconsistency crucial effective bvqa since temporal inconsistencies vary ugc sr videos calculated different ways based spatial module highlights inconsistent areas across consecutive frames coarse fine granularities addition temporal module aggregates features time two stages first stage employs visual memory capacity block adaptively segment time dimension based estimated complexity second stage focuses selecting key features stages work together consistencyaware fusion units regress crosstimescale video quality extensive experiments ugc sr video quality datasets show method outperforms existing stateoftheart bvqa methods code available httpsgithubcomlightingyxlitinq,12,1.0,12,1.0
ppllava varied video sequence understanding prompt guidance past year witnessed significant advancement videobased large language models however challenge developing unified model short long video understanding remains unresolved existing video llms handle hourlong videos methods custom long videos tend ineffective shorter videos images paper identify key issue redundant content videos address propose novel pooling strategy simultaneously achieves token compression instructionaware visual feature aggregation model termed promptguided pooling llava ppllava short specifically ppllava consists three core components clipbased visualprompt alignment extracts visual information relevant users instructions promptguided pooling compresses visual sequence arbitrary scales using convolutionstyle pooling clip context extension designed lengthy prompt common visual dialogue moreover codebase also integrates advanced video direct preference optimization dpo visual interleave training extensive experiments validated performance model superior throughput visual context ppllava achieves better results image benchmarks video llm achieving stateoftheart performance across various video benchmarks excelling tasks ranging caption generation multiplechoice questions handling video lengths seconds hours codes available httpsgithubcomfarewellthreeppllava,0,1.0,0,1.0
selfguided trajectory control imagetovideo generation methods imagetovideo generation achieved impressive photorealistic quality however adjusting specific elements generated videos object motion camera movement often tedious process trial error eg involving regenerating videos different random seeds recent techniques address issue finetuning pretrained model follow conditioning signals bounding boxes point trajectories yet finetuning procedure computationally expensive requires datasets annotated object motion difficult procure work introduce framework controllable imagetovideo generation zeroshot control relying solely knowledge present pretrained imagetovideo diffusion model without need finetuning external knowledge zeroshot method outperforms unsupervised baselines significantly narrowing performance gap supervised models terms visual quality motion fidelity additional details video results available project page,-1,0.0,-1,0.0
vonet unsupervised video object learning parallel unet attention objectwise sequential vae unsupervised video object learning seeks decompose video scenes structural object representations without supervision depth optical flow segmentation present vonet innovative approach inspired monet utilizing unet architecture vonet employs efficient effective parallel attention inference process generating attention masks slots simultaneously additionally enhance temporal consistency mask across consecutive video frames vonet develops objectwise sequential vae framework integration innovative encoderside techniques conjunction expressive transformerbased decoder establishes vonet leading unsupervised method object learning across five movi datasets encompassing videos diverse complexities code available httpsgithubcomhnyuvonet,-1,0.0,-1,0.0
fast deep predictive coding networks videos feature extraction without labels braininspired deep predictive coding networks dpcns effectively model capture video features bidirectional information flow even without labels based overcomplete description video scenes one bottlenecks lack effective sparsification techniques find discriminative robust dictionaries fista best alternative paper proposes dpcn fast inference internal model variables states causes achieves high sparsity accuracy feature clustering proposed unsupervised learning procedure inspired adaptive dynamic programming majorizationminimization framework convergence rigorously analyzed experiments data sets super mario bros video game validate approach outperforms previous versions dpcns learning rate sparsity ratio feature clustering accuracy dcpns solid foundation explainability advance opens door general applications object recognition video without labels,-1,0.0,-1,0.0
locomotion learning motionfocused videolanguage representations paper strives motionfocused videolanguage representations existing methods learn videolanguage representations use spatialfocused data identifying objects scene often enough distinguish relevant caption instead propose locomotion learn motionfocused captions describe movement temporal progression local object motions achieve adding synthetic motions videos using parameters motions generate corresponding captions furthermore propose verbvariation paraphrasing increase caption variety learn link primitive motions highlevel verbs able learn motionfocused videolanguage representation experiments demonstrate approach effective variety downstream tasks particularly limited data available finetuning code available httpshazeldoughtygithubiopaperslocomotion,0,0.8750353118424763,0,0.8750353118424763
simple effective temporal grounding pipeline basketball broadcast footage present reliable temporal grounding pipeline videotoanalytic alignment basketball broadcast footage given series frames input method quickly accurately extracts timeremaining quarter values basketball broadcast scenes work intends expedite development large multimodal video datasets train datahungry video models sports action recognition domain method aligns prelabeled corpus playbyplay annotations containing dense event annotations video frames enabling quick retrieval labeled video segments unlike previous methods forgo need localize game clocks finetuning outofthebox object detector find semantic text regions directly endtoend approach improves generality work additionally interpolation parallelization techniques prepare pipeline deployment large computing cluster code made publicly available,-1,0.0,-1,0.0
geometryaware videoinstruction tuning embodied navigation visionandlanguage navigation vln suffers limited diversity scale training data primarily constrained manual curation existing simulators address introduce videoinstruction dataset derived webbased room tour videos capture realworld indoor spaces human walking demonstrations unlike existing vln datasets leverages scale diversity online videos generate openended human walking trajectories openworld navigable instructions compensate lack navigation data online videos perform reconstruction obtain trajectories walking paths augmented additional information room types object locations shape surrounding scenes dataset includes openended descriptionenriched trajectories instructions actionenriched trajectories room tour environments demonstrate experimentally enables significant improvements across multiple vln tasks including cvdn soon reverie moreover facilitates development trainable zeroshot vln agents showcasing potential challenges advancing towards openworld navigation,5,0.36534218166126475,5,0.36534218166126475
investigating memorization video diffusion models diffusion models widely used image video generation face significant limitation risk memorizing reproducing training data inference potentially generating unauthorized copyrighted content prior research focused image diffusion models idms video diffusion models vdms remain underexplored address gap first formally define two types memorization vdms content memorization motion memorization practical way focuses privacy preservation applies generation types introduce new metrics specifically designed separately assess content motion memorization vdms additionally curate dataset text prompts prone triggering memorization used conditioning vdms leveraging prompts generate diverse videos various opensource vdms successfully extracting numerous training videos tested model application proposed metrics systematically analyze memorization across various pretrained vdms including textconditional unconditional models variety datasets comprehensive study reveals memorization widespread across tested vdms indicating vdms also memorize image training data addition video datasets finally propose efficient effective detection strategies content motion memorization offering foundational approach improving privacy vdms,-1,0.0,-1,0.0
viditq efficient accurate quantization diffusion transformers image video generation diffusion transformers demonstrated remarkable performance visual generation tasks generating realistic images videos based textual instructions however larger model sizes multiframe processing video generation lead increased computational memory costs posing challenges practical deployment edge devices posttraining quantization ptq effective method reducing memory costs computational complexity quantizing diffusion transformers find existing quantization methods face challenges applied texttoimage video tasks address challenges begin systematically analyzing source quantization error conclude unique challenges posed dit quantization accordingly design improved quantization scheme viditq video image diffusion transformer quantization tailored specifically dit models validate effectiveness viditq across variety texttoimage video models achieving negligible degradation visual quality metrics additionally implement efficient gpu kernels achieve practical memory saving endtoend latency speedup,20,1.0,20,1.0
good video lmm complex video reasoning robustness evaluation suite videolmms recent advancements large language models llms led development video large multimodal models videolmms handle wide range video understanding tasks models potential deployed realworld applications robotics ai assistants medical surgery autonomous vehicles widespread adoption videolmms daily lives underscores importance ensuring evaluating robust performance mirroring humanlike reasoning interaction capabilities complex realworld contexts however existing benchmarks videolmms primarily focus general video comprehension abilities neglect assessing reasoning capabilities complex videos realworld context robustness models lens user prompts text queries paper present complex video reasoning robustness evaluation suite cvrres novel benchmark comprehensively assesses performance videolmms across diverse realworld video dimensions evaluate recent models including opensource closedsource variants find videolmms especially opensource ones struggle robustness reasoning dealing complex videos based analysis develop trainingfree dualstep contextual prompting dscp technique enhance performance existing videolmms findings provide valuable insights building next generation humancentric ai systems advanced robustness reasoning capabilities dataset code publicly available httpsmbzuaioryxgithubiocvrrevaluationsuite,0,1.0,0,1.0
vript video worth thousands words advancements multimodal learning particularly video understanding generation require highquality videotext datasets improved model performance vript addresses issue meticulously annotated corpus highresolution videos offering detailed dense scriptlike captions clips clip caption words longer videotext datasets unlike captions documenting static content previous datasets enhance video captioning video scripting documenting content also camera operations include shot types medium shot closeup etc camera movements panning tilting etc utilizing vript explore three training paradigms aligning text video modality rather clipcaption pairs results vriptor topperforming video captioning model among opensource models comparable performance vriptor also powerful model capable endtoend generation dense detailed captions long videos moreover introduce vripthard benchmark consisting three video understanding tasks challenging existing benchmarks vripthal first benchmark evaluating action object hallucinations video llms vriptrr combines reasoning retrieval resolving question ambiguity longvideo qas vriptero new task evaluate temporal understanding events long videos rather actions short videos previous works code models datasets available httpsgithubcommutonixvript ps included videotext datasets vript series,0,1.0,0,1.0
vidtldr training free token merging lightweight video transformer video transformers become prevalent solution various video downstream tasks superior expressive power flexibility however video transformers suffer heavy computational costs induced massive number tokens across entire video frames major barrier training model patches irrelevant main contents eg backgrounds degrade generalization performance models tackle issues propose training free token merging lightweight video transformer vidtldr aims enhance efficiency video transformers merging background tokens without additional training vidtldr introduce novel approach capture salient regions videos attention map introduce saliencyaware token merging strategy dropping background tokens sharpening object scores experiments show vidtldr significantly mitigates computational complexity video transformers achieving competitive performance compared base model without vidtldr code available httpsgithubcommlvlabvidtldr,-1,0.0,-1,0.0
understanding long videos multimodal language models large language models llms allowed recent llmbased approaches achieve excellent performance longvideo understanding benchmarks investigate extensive world knowledge strong reasoning skills underlying llms influence strong performance surprisingly discover llmbased approaches yield surprisingly good accuracy longvideo tasks limited video information sometimes even video specific information building explore injecting videospecific information llmbased framework utilize offtheshelf vision tools extract three objectcentric information modalities videos leverage natural language medium fusing information resulting multimodal video understanding mvu framework demonstrates stateoftheart performance across multiple video understanding benchmarks strong performance also robotics domain tasks establish strong generality code httpsgithubcomkahnchanamvu,-1,0.0,-1,0.0
every shot counts using exemplars repetition counting videos video repetition counting infers number repetitions recurring actions motion within video propose exemplarbased approach discovers visual correspondence video exemplars across repetitions within target videos proposed every shot counts escounts model attentionbased encoderdecoder encodes videos varying lengths alongside exemplars different videos training escounts regresses locations high correspondence exemplars within video tandem method learns latent encodes representations general repetitive motions use exemplarfree zeroshot inference extensive experiments commonly used datasets repcount countix ucfrep showcase escounts obtaining stateoftheart performance across three datasets detailed ablations demonstrate effectiveness method,-1,0.0,-1,0.0
auvmae knowledgeguide action units detection via video masked autoencoder current facial action unit fau detection methods generally encounter difficulties due scarcity labeled video training data limited number training face ids renders trained feature extractor insufficient coverage modeling large diversity interperson facial structures movements explicitly address challenges propose novel videolevel pretraining scheme fully exploring multilabel property faus video well temporal label consistency heart design pretrained video feature extractor based videomasked autoencoder together finetuning network jointly completes multilevel video faus analysis tasks emphie integrating videolevel framelevel fau detections thus dramatically expanding supervision set sparse faus annotations video frames including masked ones moreover utilize interframe intraframe au pair state matrices prior knowledge guide network training instead traditional graph neural networks better temporal supervision approach demonstrates substantial enhancement performance compared existing stateoftheart methods used disfa faus datasets,7,0.9862345609730642,7,0.9862345609730642
delving deep engagement prediction short videos understanding modeling popularity user generated content ugc short videos social media platforms presents critical challenge broad implications content creators recommendation systems study delves deep intricacies predicting engagement newly published videos limited user interactions surprisingly findings reveal mean opinion scores previous video quality assessment datasets strongly correlate video engagement levels address introduce substantial dataset comprising realworld ugc short videos snapchat rather relying view count average watch time rate likes propose two metrics normalized average watch percentage nawp engagement continuation rate ecr describe engagement levels short videos comprehensive multimodal features including visual content background music text data investigated enhance engagement prediction proposed dataset two key metrics method demonstrates ability predict engagements short videos purely video content,-1,0.0,-1,0.0
video instruction tuning synthetic data development video large multimodal models lmms hindered difficulty curating large amounts highquality raw data web address propose alternative approach creating highquality synthetic dataset specifically video instructionfollowing namely dataset includes key tasks detailed captioning openended questionanswering qa multiplechoice qa training dataset combination existing visual instruction tuning data introduce llavavideo new video lmm experiments demonstrate llavavideo achieves strong performance across various video benchmarks highlighting effectiveness dataset plan release dataset generation pipeline model checkpoints,0,0.9851258491801653,0,0.9851258491801653
whats video factorized autoregressive decoding online dense video captioning generating automatic dense captions videos accurately describe contents remains challenging area research current models require processing entire video instead propose efficient online approach outputs frequent detailed temporally aligned captions without access future frames model uses novel autoregressive factorized decoding architecture models sequence visual features time segment outputting localized descriptions efficiently leverages context previous video segments allows model output frequent detailed captions comprehensively describe video according actual local content rather mimic training data second propose optimization efficient training inference enables scaling longer videos approach shows excellent performance compared offline online methods uses less compute annotations produced much comprehensive frequent utilized automatic video tagging largescale video data harvesting,-1,0.0,-1,0.0
subjectivealigned dataset metric texttovideo quality assessment rapid development generative models artificial intelligencegenerated contents aigc exponentially increased daily lives among texttovideo generation received widespread attention though many models released generating high perceptual quality videos still lack method evaluate quality videos quantitatively solve issue establish largestscale texttovideo quality assessment database date dataset composed videos generated different models also conduct subjective study obtain videos corresponding mean opinion score based propose novel transformerbased model subjectivealigned texttovideo quality assessment model extracts features textvideo alignment video fidelity perspectives leverages ability large language model give prediction score experimental results show outperforms existing metrics sota video quality assessment models quantitative analysis indicates capable giving subjectivealign predictions validating effectiveness dataset code released,-1,0.0,-1,0.0
directed domain finetuning tailoring separate modalities specific training tasks large language models llms large visual language models lvlms forefront artificial intelligence field particularly tasks like text generation video captioning questionanswering typically applicable train models broader knowledge bases datasets increase generalizability learn relationships topics recognize patterns instead propose provide instructional datasets specific task modality within distinct domain finetune parameters model using lora approach eliminate noise irrelevant given task also ensuring model generates enhanced precision work use videollava generate recipes given cooking videos without transcripts videollavas multimodal architecture allows us provide cooking images image encoder cooking videos video encoder general cooking questions text encoder thus aim remove noise unrelated cooking improving models capabilities generate specific ingredient lists detailed instructions result approach finetuning videollava leads gains baseline videollava dataset may seem like marginal increase model trains image instruction dataset size videollavas video instruction dataset videollavas,0,0.8600581164238855,0,0.8600581164238855
bridging gap analogue circuit discrete frames videotoevents simulator event cameras operate fundamentally differently traditional active pixel sensor aps cameras offering significant advantages recent research developed simulators convert video frames events addressing shortage real event datasets current simulators primarily focus logical behavior event cameras however fundamental analogue properties pixel circuits seldom considered simulator design gap analogue pixel circuit discrete video frames causes degeneration synthetic events particularly highcontrast scenes paper propose novel method generating reliable event data based detailed analysis pixel circuitry event cameras incorporate analogue properties event camera pixel circuits simulator design analogue filtering signals light intensity events cutoff frequency independent video frame rate experimental results two relevant tasks including semantic segmentation image reconstruction validate reliability simulated event data even highcontrast scenes demonstrates deep neural networks exhibit strong generalization simulated real event data confirming synthetic events generated proposed method realistic wellsuited effective training,1,1.0,1,1.0
tivdiffusion towards objectcentric movement textdriven image video generation textdriven image video generation aims generate controllable video given first frame corresponding textual description primary challenges task lie two parts identify target objects ensure consistency movement trajectory textual description ii improve subjective quality generated videos tackle challenges propose new diffusionbased framework termed tivdiffusion via objectcentric textualvisual alignment intending achieve precise control highquality video generation based textualdescribed motion different objects concretely enable tivdiffuion model perceive textualdescribed objects motion trajectory incorporating fused textual visual knowledge scaleoffset modulation moreover mitigate problems object disappearance misaligned objects motion introduce objectcentric textualvisual alignment module reduces risk misaligned objectsmotion decoupling objects reference image aligning textual features object individually based innovations tivdiffusion achieves stateoftheart highquality video generation compared existing methods,-1,0.0,-1,0.0
materialpicker multimodal material generation diffusion transformers highquality material generation key virtual environment authoring inverse rendering propose materialpicker multimodal material generator leveraging diffusion transformer dit architecture improving simplifying creation highquality materials text prompts andor photographs method generate material based image crop material sample even captured surface distorted viewed angle partially occluded often case photographs natural scenes allow user specify text prompt provide additional guidance generation finetune pretrained ditbased video generator material generator material map treated frame video sequence evaluate approach quantitatively qualitatively show enables diverse material generation better distortion correction previous work,-1,0.0,-1,0.0
comprehensive survey human video generation challenges methods insights human video generation dynamic rapidly evolving task aims synthesize human body video sequences generative models given control conditions text audio pose potential wideranging applications film gaming virtual communication ability generate natural realistic human video critical recent advancements generative models laid solid foundation growing interest area despite significant progress task human video generation remains challenging due consistency characters complexity human motion difficulties relationship environment survey provides comprehensive review current state human video generation marking best knowledge first extensive literature review domain start introduction fundamentals human video generation evolution generative models facilitated fields growth examine main methods employed three key subtasks within human video generation textdriven audiodriven posedriven motion generation areas explored concerning conditions guide generation process furthermore offer collection commonly utilized datasets evaluation metrics crucial assessing quality realism generated videos survey concludes discussion current challenges field suggests possible directions future research goal survey offer research community clear holistic view advancements human video generation highlighting milestones achieved challenges lie ahead,10,0.9991130570152621,10,0.9991130570152621
asymrnr video diffusion transformers acceleration asymmetric reduction restoration diffusion transformers dits proven effective generating highquality videos hindered high computational costs existing video dit sampling acceleration methods often rely costly finetuning exhibit limited generalization capabilities propose asymmetric reduction restoration asymrnr trainingfree modelagnostic method accelerate video dits builds observation redundancies feature tokens dits vary significantly across different model blocks denoising steps feature types asymrnr asymmetrically reduces redundant tokens attention operation achieving acceleration negligible degradation output quality cases even improving also tailored reduction schedule distribute reduction across components adaptively accelerate process introduce matching cache efficient reduction backed theoretical foundations extensive experimental validation asymrnr integrates stateoftheart video dits offers substantial speedup,20,1.0,20,1.0
advancing video quality assessment aigc recent years ai generative models made remarkable progress across various domains including text generation image generation video generation however assessing quality texttovideo generation still infancy existing evaluation frameworks fall short compared natural videos current video quality assessment vqa methods primarily focus evaluating overall quality natural videos fail adequately account substantial quality discrepancies frames generated videos address issue propose novel loss function combines mean absolute error crossentropy loss mitigate interframe quality inconsistencies additionally introduce innovative technique retain critical content leveraging adversarial training enhance models generalization capabilities experimental results demonstrate method outperforms existing vqa techniques aigc video dataset surpassing previous stateoftheart terms plcc,12,0.2828572863421619,12,0.2828572863421619
humans calibration pattern dynamic scene reconstruction unsynchronized uncalibrated videos recent works dynamic neural field reconstruction assume input synchronized multiview videos whose poses known input constraints often satisfied realworld setups making approach impractical show unsynchronized videos unknown poses generate dynamic neural fields long videos capture human motion humans one common dynamic subjects captured videos shapes poses estimated using stateoftheart libraries noisy estimated human shape pose parameters provide decent initialization point start highly nonconvex underconstrained problem training consistent dynamic neural representation given shape pose parameters humans individual frames formulate methods calculate time offsets videos followed camera pose estimations analyze joint positions train dynamic neural fields employing multiresolution grids concurrently refine time offsets camera poses setup still involves optimizing many parameters therefore introduce robust progressive learning strategy stabilize process experiments show approach achieves accurate spatiotemporal calibration highquality scene reconstruction challenging conditions,1,1.0,1,1.0
match stereo videos via bidirectional alignment video stereo matching task estimating consistent disparity maps rectified stereo videos considerable scope improvement datasets methods within area recent learningbased methods often focus optimizing performance independent stereo pairs leading temporal inconsistencies videos existing video methods typically employ sliding window operation time dimension result lowfrequency oscillations corresponding window size address challenges propose bidirectional alignment mechanism adjacent frames fundamental operation building introduce novel video processing framework bidastereo plugin stabilizer network bidastabilizer compatible general imagebased methods regarding datasets current synthetic objectbased indoor datasets commonly used training benchmarking lack outdoor nature scenarios bridge gap present realistic synthetic dataset benchmark focused natural scenes along realworld dataset captured stereo camera diverse urban scenes qualitative evaluation extensive experiments indomain outofdomain robustness evaluation demonstrate contribution methods datasets showcasing improvements prediction quality achieving stateoftheart results various commonly used benchmarks project page demos code datasets available urlhttpstomtomtommigithubiobidavideo,-1,0.0,-1,0.0
explainable controllable motion curve guided cardiac ultrasound video generation echocardiography video primary modality diagnosing heart diseases limited data poses challenges clinical teaching machine learning training recently video generative models emerged promising strategy alleviate issue however previous methods often relied holistic conditions generation hindering flexible movement control specific cardiac structures context propose explainable controllable method echocardiography video generation taking initial frame motion curve guidance contributions threefold first extract motion information heart substructure construct motion curves enabling diffusion model synthesize customized echocardiography videos modifying curves second propose structuretomotion alignment module map semantic features onto motion curves across cardiac structures third positionaware attention mechanism designed enhance video consistency utilizing gaussian masks structural position information extensive experiments three echocardiography datasets show method outperforms others regarding fidelity consistency full code released,-1,0.0,-1,0.0
videotoaudio generation finegrained temporal semantics recent advances aigc video generation gained surge research interest academia industry eg sora however remains challenge produce temporally aligned audio synchronize generated video considering complicated semantic information included latter work inspired recent success texttoaudio tta generation first investigate videotoaudio vta generation framework based latent diffusion model ldm similar latest pioneering exploration vta preliminary results also show great potentials ldm vta task still suffers suboptimal temporal alignment end propose enhance temporal alignment vta framelevel semantic information recently popular grounding segment anything model grounding sam extract finegrained semantics video frames enable vta produce betteraligned audio signal extensive experiments demonstrate effectiveness system objective subjective evaluation metrics shows better audio quality finegrained temporal alignment,8,0.7757791171057415,8,0.7757791171057415
sonique video background music generation using unpaired audiovisual data present sonique model generating background music tailored video content unlike traditional videotomusic generation approaches rely heavily paired audiovisual datasets sonique leverages unpaired data combining royaltyfree music independent video sources utilizing large language models llms video understanding converting visual descriptions musical tags alongside unetbased conditional diffusion model sonique enables customizable music generation users control specific aspects music instruments genres tempo melodies ensuring generated output fits creative vision sonique opensource demo available online,8,0.8806217299249822,8,0.8806217299249822
dragentity trajectory guided video generation using entity positional relationships recent years diffusion models achieved tremendous success field video generation controllable video generation receiving significant attention however existing control methods still face two limitations firstly control conditions depth maps mesh difficult ordinary users obtain directly secondly challenging drive multiple objects complex motions multiple trajectories simultaneously paper introduce dragentity video generation model utilizes entity representation controlling motion multiple objects compared previous methods dragentity offers two main advantages method userfriendly interaction allows users drag entities within image rather individual pixels use entity representation represent object image multiple objects maintain relative spatial relationships therefore allow multiple trajectories control multiple objects image different levels complexity simultaneously experiments validate effectiveness dragentity demonstrating excellent performance finegrained control video generation,-1,0.0,-1,0.0
sora review background technology limitations opportunities large vision models sora texttovideo generative ai model released openai february model trained generate videos realistic imaginative scenes text instructions show potential simulating physical world based public technical reports reverse engineering paper presents comprehensive review models background related technologies applications remaining challenges future directions texttovideo ai models first trace soras development investigate underlying technologies used build world simulator describe detail applications potential impact sora multiple industries ranging filmmaking education marketing discuss main challenges limitations need addressed widely deploy sora ensuring safe unbiased video generation lastly discuss future development sora video generation models general advancements field could enable new ways humanai interaction boosting productivity creativity video generation,10,1.0,10,1.0
unipaint unified spacetime video inpainting via mixtureofexperts paper present unipaint unified generative spacetime video inpainting framework enables spatialtemporal inpainting interpolation different existing methods treat video inpainting video interpolation two distinct tasks leverage unified inpainting framework tackle observe two tasks mutually enhance synthesis performance specifically first introduce plugandplay spacetime video inpainting adapter employed various personalized models key insight propose mixture experts moe attention cover various tasks design spatialtemporal masking strategy training stage mutually enhance improve performance unipaint produces highquality aesthetically pleasing results achieving best quantitative results across various tasks scale setups code checkpoints available soon,-1,0.0,-1,0.0
image compression using novel view synthesis priors realtime visual feedback essential tetherless control remotely operated vehicles particularly inspection manipulation tasks though acoustic communication preferred choice mediumrange communication underwater limited bandwidth renders impractical transmit images videos realtime address propose modelbased image compression technique leverages prior mission information approach employs trained machinelearning based novel view synthesis models uses gradient descent optimization refine latent representations help generate compressible differences camera images rendered images evaluate proposed compression technique using dataset artificial ocean basin demonstrating superior compression ratios image quality existing techniques moreover method exhibits robustness introduction new objects within scene highlighting potential advancing tetherless remotely operated vehicle operations,14,1.0,14,1.0
advancing diffusion models aliasfree resampling enhanced rotational equivariance recent advances image generation particularly via diffusion models led impressive improvements image synthesis quality despite diffusion models still challenged modelinduced artifacts limited stability image fidelity work hypothesize primary cause issue improper resampling operation introduces aliasing diffusion model careful aliasfree resampling dictated image processing theory improve models performance image synthesis propose integration aliasfree resampling layers unet architecture diffusion models without adding extra trainable parameters thereby maintaining computational efficiency assess whether theorydriven modifications enhance image quality rotational equivariance experimental results benchmark datasets including mnist mnistm reveal consistent gains image quality particularly terms fid kid scores furthermore propose modified diffusion process enables usercontrolled rotation generated images without requiring additional training findings highlight potential theorydriven enhancements aliasfree resampling generative models improve image quality maintaining model efficiency pioneer future research directions incorporate videogenerating diffusion models enabling deeper exploration applications aliasfree resampling generative modeling,13,1.0,13,1.0
oneshot realistic talking portrait synthesis oneshot talking portrait generation aims reconstruct avatar unseen image animate reference video audio generate talking portrait video existing methods fail simultaneously achieve goals accurate avatar reconstruction stable talking face animation besides existing works mainly focus synthesizing head part also vital generate natural torso background segments obtain realistic talking portrait video address limitations present framework improves oneshot reconstruction power large imagetoplane model distills prior knowledge face generative model facilitates accurate motionconditioned animation efficient motion adapter synthesizes realistic video natural torso movement switchable background using headtorsobackground superresolution model supports oneshot audiodriven talking face generation generalizable audiotomotion model extensive experiments show generalizes well unseen identities generates realistic talking portrait videos compared previous methods video samples source code available,6,0.7802728188074641,6,0.7802728188074641
hoiswap swapping objects videos handobject interaction awareness study problem precisely swapping objects videos focus interacted hands given one userprovided reference object image despite great advancements diffusion models made video editing recently models often fall short handling intricacies handobject interactions hoi failing produce realistic edits especially object swapping results object shape functionality changes bridge gap present hoiswap novel diffusionbased video editing framework trained selfsupervised manner designed two stages first stage focuses object swapping single frame hoi awareness model learns adjust interaction patterns hand grasp based changes objects properties second stage extends singleframe edit across entire sequence achieve controllable motion alignment original video warping new sequence stagei edited frame based sampled motion points conditioning video generation warped sequence comprehensive qualitative quantitative evaluations demonstrate hoiswap significantly outperforms existing methods delivering highquality video edits realistic hois,-1,0.0,-1,0.0
trip temporal residual learning image noise prior imagetovideo diffusion models recent advances texttovideo generation demonstrated utility powerful diffusion models nevertheless problem trivial shaping diffusion models animate static image ie imagetovideo generation difficulty originates aspect diffusion process subsequent animated frames preserve faithful alignment given image also pursue temporal coherence among adjacent frames alleviate present trip new recipe imagetovideo diffusion paradigm pivots image noise prior derived static image jointly trigger interframe relational reasoning ease coherent temporal modeling via temporal residual learning technically image noise prior first attained onestep backward diffusion process based static image noised video latent codes next trip executes residuallike dualpath scheme noise prediction shortcut path directly takes image noise prior reference noise frame amplify alignment first frame subsequent frames residual path employs noised video static image latent codes enable interframe relational reasoning thereby easing learning residual noise frame furthermore reference residual noise frame dynamically merged via attention mechanism final video generation extensive experiments dtdb msrvtt datasets demonstrate effectiveness trip imagetovideo generation please see project page,-1,0.0,-1,0.0
videolavit unified videolanguage pretraining decoupled visualmotional tokenization light recent advances multimodal large language models llms increasing attention scaling imagetext data informative realworld videos compared static images video poses unique challenges effective largescale pretraining due modeling spatiotemporal dynamics paper address limitations videolanguage pretraining efficient video decomposition represents video keyframes temporal motions adapted llm using welldesigned tokenizers discretize visual temporal information tokens thus enabling unified generative pretraining videos images text inference generated tokens llm carefully recovered original continuous pixel space create various video content proposed framework capable comprehending generating image video content demonstrated competitive performance across multimodal benchmarks image video understanding generation code models available httpsvideolavitgithubio,0,0.9689222095399914,0,0.9689222095399914
tuningfree framework videotovideo editing tasks dynamic field digital content creation using generative models stateoftheart video editing models still offer level quality control users desire previous works video editing either extended imagebased generative models zeroshot manner necessitated extensive finetuning hinder production fluid video edits furthermore methods frequently rely textual input editing guidance leading ambiguities limiting types edits perform recognizing challenges introduce novel tuningfree paradigm designed simplify video editing two primary steps employing offtheshelf image editing model modify first frame utilizing existing imagetovideo generation model generate edited video temporal feature injection leverage existing image editing tools support extensive array video editing tasks including promptbased editing referencebased style transfer subjectdriven editing identity manipulation unattainable previous methods also support video length evaluation shows achieved clipscores comparable baseline methods furthermore significantly outperformed baselines human evaluations demonstrating notable improvements visual consistency source video producing highquality edits across editing tasks,-1,0.0,-1,0.0
goldfish visionlanguage understanding arbitrarily long videos current llmbased models video understanding process videos within minutes however struggle lengthy videos due challenges noise redundancy well memory computation constraints paper present goldfish methodology tailored comprehending videos arbitrary lengths also introduce tvqalong benchmark specifically designed evaluate models capabilities understanding long videos questions vision text content goldfish approaches challenges efficient retrieval mechanism initially gathers topk video clips relevant instruction proceeding provide desired response design retrieval mechanism enables goldfish efficiently process arbitrarily long video sequences facilitating application contexts movies television series facilitate retrieval process developed generates detailed descriptions video clips addressing scarcity benchmarks long video evaluation adapted tvqa short video benchmark extended content analysis aggregating questions entire episodes thereby shifting evaluation partial full episode comprehension attained accuracy rate tvqalong benchmark surpassing previous methods also shows exceptional performance short video comprehension exceeding existing stateoftheart methods msvd msrvtt tgif tvqa short video benchmarks respectively results indicate models significant improvements long shortvideo understanding models code made publicly available,0,0.8969902565627508,0,0.8969902565627508
see got learning creation posefree videos scale recent generation models typically rely limitedscale goldlabels diffusion priors content creation however performance upperbounded constrained priors due lack scalable learning paradigms work present visualconditional multiview diffusion model trained largescale internet videos openworld creation model aims get knowledge solely seeing visual contents vast rapidly growing video data see got achieve first scale training data using proposed data curation pipeline automatically filters multiview inconsistencies insufficient observations source videos results highquality richly diverse largescale dataset multiview images termed containing frames video clips nevertheless learning generic priors videos without explicit geometry camera pose annotations nontrivial annotating poses webscale videos prohibitively expensive eliminate need pose conditions introduce innovative visualcondition purely visual signal generated adding timedependent noise masked video data finally introduce novel visualconditional generation framework integrating warpingbased pipeline highfidelity generation numerical visual comparisons single sparse reconstruction benchmarks show trained costeffective scalable video data achieves notable zeroshot openworld generation capabilities markedly outperforming models trained costly constrained datasets please refer project page,-1,0.0,-1,0.0
demamba aigenerated video detection millionscale genvideo benchmark recently video generation techniques advanced rapidly given popularity video content social media platforms models intensify concerns spread fake information therefore growing demand detectors capable distinguishing fake aigenerated videos mitigating potential harm caused fake information however lack largescale datasets advanced video generators poses barrier development detectors address gap introduce first aigenerated video detection dataset genvideo features following characteristics large volume videos including one million aigenerated real videos collected rich diversity generated content methodologies covering broad spectrum video categories generation techniques conducted extensive studies dataset proposed two evaluation methods tailored realworldlike scenarios assess detectors performance crossgenerator video classification task assesses generalizability trained detectors generators degraded video classification task evaluates robustness detectors handle videos degraded quality dissemination moreover introduced plugandplay module named detail mamba demamba designed enhance detectors identifying aigenerated videos analysis inconsistencies temporal spatial dimensions extensive experiments demonstrate demambas superior generalizability robustness genvideo compared existing detectors believe genvideo dataset demamba module significantly advance field aigenerated video detection code dataset aviliable urlhttpsgithubcomchenhaoxingdemamba,4,1.0,4,1.0
storydiffusion consistent selfattention longrange image video generation recent diffusionbased generative models maintaining consistent content across series generated images especially containing subjects complex details presents significant challenge paper propose new way selfattention calculation termed consistent selfattention significantly boosts consistency generated images augments prevalent pretrained diffusionbased texttoimage models zeroshot manner extend method longrange video generation introduce novel semantic space temporal motion prediction module named semantic motion predictor trained estimate motion conditions two provided images semantic spaces module converts generated sequence images videos smooth transitions consistent subjects significantly stable modules based latent spaces especially context long video generation merging two novel components framework referred storydiffusion describe textbased story consistent images videos encompassing rich variety contents proposed storydiffusion encompasses pioneering explorations visual story generation presentation images videos hope could inspire research aspect architectural modifications code made publicly available httpsgithubcomhvisionnkustorydiffusion,11,0.9648614358089842,11,0.9648614358089842
seeing hearing opendomain visualaudio generation diffusion latent aligners video audio content creation serves core technique movie industry professional users recently existing diffusionbased methods tackle video audio generation separately hinders technique transfer academia industry work aim filling gap carefully designed optimizationbased framework crossvisualaudio jointvisualaudio generation observe powerful generation ability offtheshelf video audio generation models thus instead training giant models scratch propose bridge existing strong models shared latent representation space specifically propose multimodality latent aligner pretrained imagebind model latent aligner shares similar core classifier guidance guides diffusion denoising process inference time carefully designed optimization strategy loss functions show superior performance method joint videoaudio generation visualsteered audio generation audiosteered visual generation tasks project website found,8,0.5999510005555262,8,0.5999510005555262
interactive generation laparoscopic videos diffusion models generative ai general synthetic visual data generation specific hold much promise benefiting surgical training providing photorealism simulation environments current training methods primarily rely reading materials observing live surgeries timeconsuming impractical work take significant step towards improving training process specifically use diffusion models combination zeroshot video diffusion method interactively generate realistic laparoscopic images videos specifying surgical action text guiding generation tool positions segmentation masks demonstrate performance approach using publicly available cholec dataset family evaluate fidelity factual correctness generated images using surgical action recognition model well pixelwise spatial control tool generation achieve fid,-1,0.0,-1,0.0
motion prompting controlling video generation motion trajectories motion control crucial generating expressive compelling video content however existing video generation models rely mainly text prompts control struggle capture nuances dynamic actions temporal compositions end train video generation model conditioned spatiotemporally sparse dense motion trajectories contrast prior motion conditioning work flexible representation encode number trajectories objectspecific global scene motion temporally sparse motion due flexibility refer conditioning motion prompts users may directly specify sparse trajectories also show translate highlevel user requests detailed semidense motion prompts process term motion prompt expansion demonstrate versatility approach various applications including camera object motion control interacting image motion transfer image editing results showcase emergent behaviors realistic physics suggesting potential motion prompts probing video models interacting future generative world models finally evaluate quantitatively conduct human study demonstrate strong performance video results available webpage httpsmotionpromptinggithubio,18,1.0,18,1.0
mapping noise data enhanced diffusion models diffusion models established de facto primary paradigm visual generative modeling revolutionizing field remarkable success across various diverse applications ranging highquality image synthesis temporal aware video generation despite advancements three fundamental limitations persist including discrepancy training inference processes progressive information leakage throughout noise corruption procedures inherent constraints preventing effective integration modern optimization criteria like perceptual adversarial loss mitigate critical challenges paper present novel endtoend learning paradigm establishes direct optimization final generated samples initial noises proposed endtoend differentiable diffusion dubbed introduces several key improvements eliminates sequential trainingsampling mismatch intermediate information leakage via conceptualizing training direct transformation isotropic gaussian noise target data distribution additionally training framework enables seamless incorporation adversarial perceptual losses core optimization objective comprehensive evaluation across standard benchmarks including reveals method achieves substantial performance gains terms frechet inception distance fid clip score even fewer sampling steps less findings highlight endtoend mechanism might pave way robust efficient solutions emphie combining diffusion stability ganlike discriminative optimization endtoend manner,13,0.9149390547725647,13,0.9149390547725647
spectrum translation refinement image generation stig based contrastive learning spectral filter profile currently image generation synthesis remarkably progressed generative models despite photorealistic results intrinsic discrepancies still observed frequency domain spectral discrepancy appeared generative adversarial networks diffusion models study propose framework effectively mitigate disparity frequency domain generated images improve generative performance gan diffusion models realized spectrum translation refinement image generation stig based contrastive learning adopt theoretical logic frequency components various generative networks key idea refine spectrum generated image via concept imagetoimage translation contrastive learning terms digital signal processing evaluate framework across eight fake image datasets various cuttingedge models demonstrate effectiveness stig framework outperforms cuttingedges showing significant decreases fid log frequency distance spectrum emphasize stig improves image quality decreasing spectral anomaly additionally validation results present frequencybased deepfake detector confuses case fake spectrums manipulated stig,13,1.0,13,1.0
qvd posttraining quantization video diffusion models recently video diffusion models vdms garnered significant attention due notable advancements generating coherent realistic video content however processing multiple frame features concurrently coupled considerable model size results high latency extensive memory consumption hindering broader application posttraining quantization ptq effective technique reduce memory footprint improve computational efficiency unlike image diffusion observe temporal features integrated frame features exhibit pronounced skewness furthermore investigate significant interchannel disparities asymmetries activation video diffusion models resulting low coverage quantization levels individual channels increasing challenge quantization address issues introduce first ptq strategy tailored video diffusion models dubbed qvd specifically propose high temporal discriminability quantization htdq method designed temporal features retains high discriminability quantized features providing precise temporal guidance video frames addition present scattered channel range integration scri method aims improve coverage quantization levels across individual channels experimental validations across various models datasets bitwidth settings demonstrate effectiveness qvd terms diverse metrics particular achieve nearlossless performance degradation outperforming current methods fvd,20,1.0,20,1.0
safesora towards safety alignment generation via human preference dataset mitigate risk harmful outputs large vision models lvms introduce safesora dataset promote research aligning texttovideo generation human values dataset encompasses human preferences texttovideo generation tasks along two primary dimensions helpfulness harmlessness capture indepth human preferences facilitate structured reasoning crowdworkers subdivide helpfulness subdimensions harmlessness subcategories serving basis pilot annotations safesora dataset includes unique prompts unique videos generated distinct lvms pairs preference annotations labeled humans demonstrate utility safesora dataset several applications including training textvideo moderation model aligning lvms human preference finetuning prompt augmentation module diffusion model applications highlight potential foundation texttovideo alignment research human preference modeling development validation alignment algorithms,-1,0.0,-1,0.0
echopulse ecg controlled echocardiograms video generation echocardiography echo essential cardiac assessments video quality interpretation heavily relies manual expertise leading inconsistent results clinical portable devices echo video generation offers solution improving automated monitoring synthetic data generating highquality videos routine health data however existing models often face high computational costs slow inference rely complex conditional prompts require experts annotations address challenges propose echopulse ecgconditioned echo video generation model echopulse introduces two key advancements accelerates echo video generation leveraging vqvae tokenization masked visual token modeling fast decoding conditions readily accessible ecg signals highly coherent echo videos bypassing complex conditional prompts best knowledge first work use timeseries prompts like ecg signals echo video generation echopulse enables controllable synthetic echo data generation also provides updated cardiac function information disease monitoring prediction beyond ecg alone evaluations three public private datasets demonstrate stateoftheart performance echo video generation across qualitative quantitative measures additionally echopulse easily generalized modality generation tasks cardiac mri fmri ct generation demo seen,-1,0.0,-1,0.0
decoupling degradations recurrent network video restoration underdisplay camera underdisplay camera udc systems foundation fullscreen display devices lens mounts display pixel array lightemitting diodes used display diffracts attenuates incident light causing various degradations light intensity changes unlike general video restoration recovers video treating different degradation factors equally video restoration udc systems challenging concerns removing diverse degradation time preserving temporal consistency paper introduce novel video restoration network called specifically designed udc systems employs set decoupling attention modules dam effectively separate various video degradation factors specifically soft mask generation function proposed formulate frame flare haze based diffraction arising incident light different intensities followed proposed flare haze removal components leverage long shortterm feature learning handle respective degradations design offers targeted effective solution eliminating various types degradation udc systems extend design multiscale overcome scalechanging degradation often occur longrange videos demonstrate superiority propose largescale udc video benchmark gathering hdr videos generating realistically degraded videos using point spread function measured commercial udc system extensive quantitative qualitative evaluations demonstrate superiority compared stateoftheart video restoration udc image restoration methods code available httpsgithubcomchengxuliuddrnetgit,-1,0.0,-1,0.0
omnivid generative framework universal video understanding core video understanding tasks recognition captioning tracking automatically detect objects actions video analyze temporal evolution despite sharing common goal different tasks often rely distinct model architectures annotation formats contrast natural language processing benefits unified output space ie text sequences simplifies training powerful foundational language models extensive training corpora inspired seek unify output space video understanding tasks using languages labels additionally introducing time box tokens way variety video tasks could formulated videogrounded token generation enables us address various types video tasks including classification action recognition captioning covering clip captioning video question answering dense video captioning localization tasks visual object tracking within fully shared encoderdecoder architecture following generative framework comprehensive experiments demonstrate simple straightforward idea quite effective achieve stateoftheart competitive results seven video benchmarks providing novel perspective universal video understanding code available,-1,0.0,-1,0.0
enhancing video summarization context awareness video summarization crucial research area aims efficiently browse retrieve relevant information vast amount video content available today exponential growth multimedia data ability extract meaningful representations videos become essential video summarization techniques automatically generate concise summaries selecting keyframes shots segments capture videos essence process improves efficiency accuracy various applications including video surveillance education entertainment social media despite importance video summarization lack diverse representative datasets hindering comprehensive evaluation benchmarking algorithms existing evaluation metrics also fail fully capture complexities video summarization limiting accurate algorithm assessment hindering fields progress overcome data scarcity challenges improve evaluation propose unsupervised approach leverages video data structure information generating informative summaries moving away fixed annotations framework produce representative summaries effectively moreover introduce innovative evaluation pipeline tailored specifically video summarization human participants involved evaluation comparing generated summaries ground truth summaries assessing informativeness humancentric approach provides valuable insights effectiveness proposed techniques experimental results demonstrate trainingfree framework outperforms existing unsupervised approaches achieves competitive results compared stateoftheart supervised methods,0,1.0,0,1.0
videogigagan towards detailrich video superresolution video superresolution vsr approaches shown impressive temporal consistency upsampled videos however approaches tend generate blurrier results image counterparts limited generative capability raises fundamental question extend success generative image upsampler vsr task preserving temporal consistency introduce videogigagan new generative vsr model produce videos highfrequency details temporal consistency videogigagan builds upon largescale image upsampler gigagan simply inflating gigagan video model adding temporal modules produces severe temporal flickering identify several key issues propose techniques significantly improve temporal consistency upsampled videos experiments show unlike previous vsr methods videogigagan generates temporally consistent videos finegrained appearance details validate effectiveness videogigagan comparing stateoftheart vsr models public datasets showcasing video results superresolution,-1,0.0,-1,0.0
instruct editing scenes scenes using diffusion paper proposes instruct achieves awareness spatialtemporal consistency diffusion models generate highquality instructionguided dynamic scene editing results traditional applications diffusion models dynamic scene editing often result inconsistency primarily due inherent framebyframe editing methodology addressing complexities extending instructionguided editing key insight treat scene scene decoupled two subproblems achieving temporal consistency video editing applying edits scene following first enhance model anchoraware attention module batch processing consistent editing additionally integrate optical flowguided appearance propagation sliding window fashion precise frametoframe editing incorporate depthbased projection manage extensive data scenes followed iterative editing achieve convergence extensively evaluate approach various scenes editing instructions demonstrate achieves spatially temporally consistent editing results significantly enhanced detail sharpness prior art notably instruct general applicable monocular challenging multicamera scenes code results available,-1,0.0,-1,0.0
uniscene unified occupancycentric driving scene generation generating highfidelity controllable annotated training data critical autonomous driving existing methods typically generate single data form directly coarse scene layout fails output rich data forms required diverse downstream tasks also struggles model direct layouttodata distribution paper introduce uniscene first unified framework generating three key data forms semantic occupancy video lidar driving scenes uniscene employs progressive generation process decomposes complex task scene generation two hierarchical steps first generating semantic occupancy customized scene layout meta scene representation rich semantic geometric information b conditioned occupancy generating video lidar data respectively two novel transfer strategies gaussianbased joint rendering priorguided sparse modeling occupancycentric approach reduces generation burden especially intricate scenes providing detailed intermediate representations subsequent generation stages extensive experiments demonstrate uniscene outperforms previous sotas occupancy video lidar generation also indeed benefits downstream driving tasks project page,16,0.7150465824772868,16,0.7150465824772868
tora trajectoryoriented diffusion transformer video generation recent advancements diffusion transformer dit demonstrated remarkable proficiency producing highquality video content nonetheless potential transformerbased diffusion models effectively generating videos controllable motion remains area limited exploration paper introduces tora first trajectoryoriented dit framework concurrently integrates textual visual trajectory conditions thereby enabling scalable video generation effective motion guidance specifically tora consists trajectory extractor te spatialtemporal dit motionguidance fuser mgf te encodes arbitrary trajectories hierarchical spacetime motion patches motion compression network mgf integrates motion patches dit blocks generate consistent videos accurately follow designated trajectories design aligns seamlessly dits scalability allowing precise control video contents dynamics diverse durations aspect ratios resolutions extensive experiments demonstrate tora excels achieving high motion fidelity compared foundational dit model also accurately simulating complex movements physical world code made available httpsgithubcomalibabatora,9,0.7676269057144607,9,0.7676269057144607
photorealistic object insertion diffusionguided inverse rendering correct insertion virtual objects images realworld scenes requires deep understanding scenes lighting geometry materials well image formation process recent largescale diffusion models shown strong generative inpainting capabilities find current models sufficiently understand scene shown single picture generate consistent lighting effects shadows bright reflections etc preserving identity details composited object propose using personalized large diffusion model guidance physically based inverse rendering process method recovers scene lighting tonemapping parameters allowing photorealistic composition arbitrary virtual objects single frames videos indoor outdoor scenes physically based pipeline enables automatic materials tonemapping refinement,1,0.9108645379316634,1,0.9108645379316634
emodiffhead continuously emotional control talking head generation via diffusion task audiodriven portrait animation involves generating talking head video using identity image audio track speech many existing approaches focus lip synchronization video quality tackle challenge generating emotiondriven talking head videos ability control edit emotions essential producing expressive realistic animations response challenge propose emodiffhead novel method emotional talking head video generation enables finegrained control emotion categories intensities also enables oneshot generation given flame models linearity expression modeling utilize deca method extract expression vectors combined audio guide diffusion model generating videos precise lip synchronization rich emotional expressiveness approach enables learning rich facial information emotionirrelevant data also facilitates generation emotional videos effectively overcomes limitations emotional data lack diversity facial background information addresses absence emotional details emotionirrelevant data extensive experiments user studies demonstrate approach achieves stateoftheart performance compared emotion portrait animation methods,6,1.0,6,1.0
dollar fewstep video generation via distillation latent reward optimization diffusion probabilistic models shown significant progress video generation however computational efficiency limited large number sampling steps required reducing sampling steps often compromises video quality generation diversity work introduce distillation method combines variational score distillation consistency distillation achieve fewstep video generation maintaining high quality diversity also propose latent reward model finetuning approach enhance video generation performance according specified reward metric approach reduces memory usage require reward differentiable method demonstrates stateoftheart performance fewstep generation videos frames fps distilled student model achieves score vbench surpassing teacher model well baseline models kling onestep distillation accelerates teacher models diffusion sampling times enabling near realtime generation human evaluations validate superior performance student models compared teacher model using ddim sampling,-1,0.0,-1,0.0
cinepregen camera controllable video previsualization via enginepowered diffusion advancements video generative ai models eg sora creators increasingly using techniques enhance video previsualization however face challenges incomplete mismatched ai workflows existing methods mainly rely text descriptions struggle camera placement key component previsualization address issues introduce cinepregen visual previsualization system enhanced enginepowered diffusion features novel camera storyboard interface offers dynamic control global local camera adjustments combined userfriendly ai rendering workflow aims achieve consistent results multimasked ipadapter engine simulation guidelines comprehensive evaluation study demonstrate system reduces development viscosity ie complexity challenges development process meets users needs extensive control iteration design process outperforms ai video production workflows cinematic camera movement shown experiments withinsubjects user study intuitive camera controls realistic rendering camera motion cinepregen shows great potential improving video production individual creators industry professionals,-1,0.0,-1,0.0
learning generalizable photorealistic video diffusion propose novel framework generating videos organized grid video frames time viewpoint axes grid row contains frames sharing timestep column contains frames viewpoint propose novel twostream architecture one stream performs viewpoint updates columns stream performs temporal updates rows diffusion transformer layer synchronization layer exchanges information two token streams propose two implementations synchronization layer using either hard soft synchronization feedforward architecture improves upon previous work three ways higher inference speed enhanced visual quality measured fvd clip videoscore improved temporal viewpoint consistency measured videoscore,-1,0.0,-1,0.0
technical report competition solution modelscopesora report presents approach adopted modelscopesora challenge focuses finetuning data video generation models challenge evaluates participants ability analyze clean generate highquality datasets videobased texttovideo tasks specific computational constraints provided methodology involves data processing techniques video description generation filtering acceleration report outlines procedures tools utilized enhance quality training data ensuring improved performance texttovideo generation models,-1,0.0,-1,0.0
customcrafter customized video generation preserving motion concept composition abilities customized video generation aims generate highquality videos guided text prompts subjects reference images however since trained static images finetuning process subject learning disrupts abilities video diffusion models vdms combine concepts generate motions restore abilities methods use additional video similar prompt finetune guide model requires frequent changes guiding videos even retuning model generating different motions inconvenient users paper propose customcrafter novel framework preserves models motion generation conceptual combination abilities without additional video finetuning recovery preserving conceptual combination ability design plugandplay module update parameters vdms enhancing models ability capture appearance details ability concept combinations new subjects motion generation observed vdms tend restore motion video early stage denoising focusing recovery subject details later stage therefore propose dynamic weighted video sampling strategy using pluggability subject learning modules reduce impact module motion generation early stage denoising preserving ability generate motion vdms later stage denoising restore module repair appearance details specified subject thereby ensuring fidelity subjects appearance experimental results show method significant improvement compared previous methods code available httpsgithubcomwutaocscustomcrafter,2,0.6860564220557998,2,0.6860564220557998
ganfusion feedforward diffusion gan space train feedforward diffusion generator human characters using singleview data supervision existing generative models yet match fidelity image video generative models stateoftheart generators either trained explicit supervision thus limited volume diversity existing data meanwhile generators trained data supervision typically produce coarser results textconditioned must revert testtime optimization observe gan diffusionbased generators complementary qualities gans trained efficiently supervision produce highquality objects hard condition text contrast denoising diffusion models conditioned efficiently tend hard train supervision introduce ganfusion starts generating unconditional triplane features data using gan architecture trained singleview data generate random samples gan caption train textconditioned diffusion model directly learns sample space good triplane features decoded objects,-1,0.0,-1,0.0
simgen simulatorconditioned driving scene generation controllable synthetic data generation substantially lower annotation cost training data prior works use diffusion models generate driving images conditioned object layout however models trained smallscale datasets like nuscenes lack appearance layout diversity moreover overfitting often happens trained models generate images based layout data validation set dataset work introduce simulatorconditioned scene generation framework called simgen learn generate diverse driving scenes mixing data simulator real world uses novel cascade diffusion pipeline address challenging simtoreal gaps multicondition conflicts driving video dataset diva collected enhance generative diversity simgen contains hours realworld driving videos locations worldwide simulated driving data metadrive simulator simgen achieves superior generation quality diversity preserving controllability based text prompt layout pulled simulator demonstrate improvements brought simgen synthetic data augmentation bev detection segmentation task showcase capability safetycritical data generation,-1,0.0,-1,0.0
scalable indoor novelview synthesis using dronecaptured imagery gaussian splatting scene reconstruction novelview synthesis large complex multistory indoor scenes challenging timeconsuming task prior methods utilized drones data capture radiance fields scene reconstruction present certain challenges first order capture diverse viewpoints drones frontfacing camera approaches fly drone unstable zigzag fashion hinders dronepiloting generates motion blur captured data secondly radiance field methods easily scale arbitrarily large number images paper proposes efficient scalable pipeline indoor novelview synthesis dronecaptured videos using gaussian splatting cameras capture wide set viewpoints allowing comprehensive scene capture simple straightforward drone trajectory scale method large scenes devise divideandconquer strategy automatically split scene smaller blocks reconstructed individually parallel also propose coarsetofine alignment strategy seamlessly match blocks together compose entire scene experiments demonstrate marked improvement reconstruction quality ie psnr ssim computation time compared prior approaches,1,1.0,1,1.0
tunnel tryon excavating spatialtemporal tunnels highquality virtual tryon videos video tryon challenging task well tackled previous works main obstacle lies preserving details clothing modeling coherent motions simultaneously faced difficulties address video tryon proposing diffusionbased framework named tunnel tryon core idea excavating focus tunnel input video gives closeup shots around clothing regions zoom region tunnel better preserve fine details clothing generate coherent motions first leverage kalman filter construct smooth crops focus tunnel inject position embedding tunnel attention layers improve continuity generated videos addition develop environment encoder extract context information outside tunnels supplementary cues equipped techniques tunnel tryon keeps fine details clothing synthesizes stable smooth videos demonstrating significant advancements tunnel tryon could regarded first attempt toward commerciallevel application virtual tryon videos,-1,0.0,-1,0.0
dawn video generation preliminary explorations soralike models highquality video generation encompassing texttovideo imagetovideo videotovideo generation holds considerable significance content creation benefit anyone express inherent creativity new ways world simulation modeling understanding world models like sora advanced generating videos higher resolution natural motion better visionlanguage alignment increased controllability particularly long video sequences improvements driven evolution model architectures shifting unet scalable parameterrich dit models along largescale data expansion refined training strategies however despite emergence ditbased closedsource opensource models comprehensive investigation capabilities limitations remains lacking furthermore rapid development made challenging recent benchmarks fully cover soralike models recognize significant advancements additionally evaluation metrics often fail align human preferences,-1,0.0,-1,0.0
towards realistic landmarkguided facial video inpainting based gans facial video inpainting plays crucial role wide range applications including limited removal obstructions video conferencing telemedicine enhancement facial expression analysis privacy protection integration graphical overlays virtual makeup domain presents serious challenges due intricate nature facial features inherent human familiarity faces heightening need accurate persuasive completions addressing challenges specifically related occlusion removal context focus progressive task generating complete images facial data covered masks ensuring spatial temporal coherence study introduces network designed expressionbased video inpainting employing generative adversarial networks gans handle static moving occlusions across frames utilizing facial landmarks occlusionfree reference image model maintains users identity consistently across frames enhance emotional preservation customized facial expression recognition fer loss function ensuring detailed inpainted outputs proposed framework exhibits proficiency eliminating occlusions facial videos adaptive form whether appearing static dynamic frames providing realistic coherent results,6,0.4407734650155672,6,0.4407734650155672
simple strong baseline sounding video generation effective adaptation audio video diffusion models joint generation work build simple strong baseline sounding video generation given base diffusion models audio video integrate additional modules single model train make model jointly generate audio video enhance alignment audiovideo pairs introduce two novel mechanisms model first one timestep adjustment provides different timestep information base model designed align samples generated along timesteps across modalities second one new design additional modules termed crossmodal conditioning positional encoding cmcpe cmcpe crossmodal information embedded represents temporal position information embeddings fed model like positional encoding compared popular crossattention mechanism cmcpe provides better inductive bias temporal alignment generated data experimental results validate effectiveness two newly introduced mechanisms also demonstrate method outperforms existing methods,8,0.5841394262124738,8,0.5841394262124738
vires video instance repainting via sketch text guided generation introduce vires video instance repainting method sketch text guidance enabling video instance repainting replacement generation removal existing approaches struggle temporal consistency accurate alignment provided sketch sequence vires leverages generative priors texttovideo models maintain temporal consistency produce visually pleasing results propose sequential controlnet standardized selfscaling effectively extracts structure layouts adaptively captures highcontrast sketch details augment diffusion transformer backbone sketch attention interpret inject finegrained sketch semantics sketchaware encoder ensures repainted results aligned provided sketch sequence additionally contribute vireset dataset detailed annotations tailored training evaluating video instance editing methods experimental results demonstrate effectiveness vires outperforms stateoftheart methods visual quality temporal consistency condition alignment human ratings project pagehttpssuimucgithubiosuimugithubioprojectsvires,-1,0.0,-1,0.0
tarsier recipes training evaluating large video description models generating finegrained video descriptions fundamental challenge video understanding work introduce tarsier family largescale videolanguage models designed generate highquality video descriptions tarsier employs clipvit encode frames separately uses llm model temporal relationships despite simple architecture demonstrate meticulously designed twostage training procedure tarsier models exhibit substantially stronger video description capabilities existing opensource model showing advantage human sidebyside evaluation strongest model additionally comparable stateoftheart proprietary models advantage disadvantage gemini pro upgraded building upon siglip improves significantly advantage besides video description tarsier proves versatile generalist model achieving new stateoftheart results across nine public benchmarks including multichoice vqa openended vqa zeroshot video captioning second contribution introduction new benchmark httpstarsiervlmgithubio evaluating video description models consisting new challenging dataset featuring videos diverse sources varying complexity along automatic method specifically designed assess quality finegrained video descriptions make models evaluation benchmark publicly available httpsgithubcombytedancetarsier,-1,0.0,-1,0.0
physgame uncovering physical commonsense violations gameplay videos recent advancements videobased large language models video llms witnessed emergence diverse capabilities reason interpret dynamic visual content among gameplay videos stand distinctive data source often containing glitches defy physics commonsense characteristic renders effective benchmark assessing underexplored capability physical commonsense understanding video llms paper propose physgame pioneering benchmark evaluate physical commonsense violations gameplay videos physgame comprises videos associated glitches spanning four fundamental domains ie mechanics kinematics optics material properties across distinct physical commonsense extensively evaluating various stateoftheart video llms findings reveal performance current opensource video llms significantly lags behind proprietary counterparts bridge gap curate instruction tuning dataset physinstruct questionanswering pairs facilitate physical commonsense learning addition also propose preference optimization dataset physdpo training pairs dispreferred responses generated conditioned misleading titles ie meta information hacking fewer frames ie temporal hacking lower spatial resolutions ie spatial hacking based suite datasets propose physvlm physical knowledgeenhanced video llm extensive experiments physicaloriented benchmark physgame general video understanding benchmarks demonstrate stateoftheart performance physvlm,0,1.0,0,1.0
diffusion attack leveraging stable diffusion naturalistic image attacking virtual reality vr adversarial attack remains significant security threat deep learningbased methods physical digital adversarial attacks focus enhancing attack performance crafting adversarial examples contain large printable distortions easy human observers identify however attackers rarely impose limitations naturalness comfort appearance generated attack image resulting noticeable unnatural attack address challenge propose framework incorporate style transfer craft adversarial inputs natural styles exhibit minimal detectability maximum natural appearance maintaining superior attack capabilities,4,1.0,4,1.0
simvs simulating world inconsistencies robust view synthesis novelview synthesis techniques achieve impressive results static scenes struggle faced inconsistencies inherent casual capture settings varying illumination scene motion unintended effects difficult model explicitly present approach leveraging generative video models simulate inconsistencies world occur capture use process along existing multiview datasets create synthetic data training multiview harmonization network able reconcile inconsistent observations consistent scene demonstrate worldsimulation strategy significantly outperforms traditional augmentation methods handling realworld scene variations thereby enabling highly accurate static reconstructions presence variety challenging inconsistencies project page httpsalextrevithickgithubiosimvs,1,1.0,1,1.0
star benchmark situated reasoning realworld videos reasoning real world divorced situations capture present knowledge surrounding situations perform reasoning accordingly crucial challenging machine intelligence paper introduces new benchmark evaluates situated reasoning ability via situation abstraction logicgrounded question answering realworld videos called situated reasoning realworld videos star benchmark benchmark built upon realworld videos associated human actions interactions naturally dynamic compositional logical dataset includes four types questions including interaction sequence prediction feasibility represent situations realworld videos hypergraphs connecting extracted atomic entities relations eg actions persons objects relationships besides visual perception situated reasoning also requires structured situation comprehension logical reasoning questions answers procedurally generated answering logic question represented functional program based situation hypergraph compare various existing video reasoning models find struggle challenging situated reasoning task propose diagnostic neurosymbolic model disentangle visual perception situation abstraction language understanding functional reasoning understand challenges benchmark,0,1.0,0,1.0
aigcbench comprehensive evaluation imagetovideo content generated ai burgeoning field artificial intelligence generated content aigc witnessing rapid advancements particularly video generation paper introduces aigcbench pioneering comprehensive scalable benchmark designed evaluate variety video generation tasks primary focus imagetovideo generation aigcbench tackles limitations existing benchmarks suffer lack diverse datasets including varied opendomain imagetext dataset evaluates different stateoftheart algorithms equivalent conditions employ novel text combiner create rich text prompts used generate images via advanced texttoimage models establish unified evaluation framework video generation tasks benchmark includes metrics spanning four dimensions assess algorithm performance dimensions controlvideo alignment motion effects temporal consistency video quality metrics reference videodependent videofree ensuring comprehensive evaluation strategy evaluation standard proposed correlates well human judgment providing insights strengths weaknesses current algorithms findings extensive experiments aim stimulate research development field aigcbench represents significant step toward creating standardized benchmarks broader aigc landscape proposing adaptable equitable framework future assessments video generation tasks opensourced dataset evaluation code project website httpswwwbenchcouncilorgaigcbench,-1,0.0,-1,0.0
distinguish fake videos unleashing power largescale data motion features development aigenerated content aigc empowered creation remarkably realistic aigenerated videos involving sora however widespread adoption models raises concerns regarding potential misuse including face video scams copyright disputes addressing concerns requires development robust tools capable accurately determining video authenticity main challenges lie dataset neural classifier training current datasets lack varied comprehensive repository real generated content effective discrimination paper first introduce extensive video dataset designed specifically aigenerated video detection genviddet includes instances real generated videos varying categories frames per second resolutions lengths comprehensiveness genviddet enables training generalizable video detector also present dualbranch transformer innovative effective method distinguishing real generated videos enhanced incorporating motion information alongside visual appearance utilizes dualbranch architecture adaptively leverages fuses raw spatiotemporal data optical flow systematically explore critical factors affecting detection performance achieving optimal configuration trained genviddet distinguish real generated video content accuracy strong generalization capability even unseen types,4,0.8857008220990839,4,0.8857008220990839
vidmusician videotomusic generation semanticrhythmic alignment via hierarchical visual features videotomusic generation presents significant potential video production requiring generated music semantically rhythmically aligned video achieving alignment demands advanced music generation capabilities sophisticated video understanding efficient mechanism learn correspondence two modalities paper propose vidmusician parameterefficient videotomusic generation framework built upon texttomusic models vidmusician leverages hierarchical visual features ensure semantic rhythmic alignment video music specifically approach utilizes global visual features semantic conditions local visual features rhythmic cues features integrated generative backbone via crossattention inattention mechanisms respectively twostage training process incrementally incorporate semantic rhythmic features utilizing zero initialization identity initialization maintain inherent musicgenerative capabilities backbone additionally construct diverse videomusic dataset dvmset encompassing various scenarios promo videos commercials compilations experiments demonstrate vidmusician outperforms stateoftheart methods across multiple evaluation metrics exhibits robust performance aigenerated videos samples available,-1,0.0,-1,0.0
motionclone trainingfree motion cloning controllable video generation motionbased controllable video generation offers potential creating captivating visual content existing methods typically necessitate model training encode particular motion cues incorporate finetuning inject certain motion patterns resulting limited flexibility generalization work propose motionclone trainingfree framework enables motion cloning reference videos versatile motioncontrolled video generation including texttovideo imagetovideo based observation dominant components temporalattention maps drive motion synthesis rest mainly capture noisy subtle motions motionclone utilizes sparse temporal attention weights motion representations motion guidance facilitating diverse motion transfer across varying scenarios meanwhile motionclone allows direct extraction motion representation single denoising step bypassing cumbersome inversion processes thus promoting efficiency flexibility extensive experiments demonstrate motionclone exhibits proficiency global camera motion local object motion notable superiority terms motion fidelity textual alignment temporal consistency,9,0.7383553838449082,9,0.7383553838449082
technical report soccernet dense video captioning task dense video captioning soccernet dataset propose generate video caption soccer action locate timestamp caption firstly apply blip video caption framework generate video captions locate timestamp using multisize sliding windows temporal proposal generation proposal classification,0,1.0,0,1.0
cyberhost taming audiodriven avatar diffusion model region codebook attention diffusionbased video generation technology advanced significantly catalyzing proliferation research human animation however majority studies confined samemodality driving settings crossmodality human body animation remaining relatively underexplored paper introduce endtoend audiodriven human animation framework ensures hand integrity identity consistency natural motion key design cyberhost region codebook attention mechanism improves generation quality facial hand animations integrating finegrained local features learned motion pattern priors furthermore developed suite humanpriorguided training strategies including body movement map hand clarity score posealigned reference feature local enhancement supervision improve synthesis results knowledge cyberhost first endtoend audiodriven human diffusion model capable facilitating zeroshot video generation within scope human body extensive experiments demonstrate cyberhost surpasses previous works quantitative qualitative aspects,-1,0.0,-1,0.0
accelerating diffusion transformers tokenwise feature caching diffusion transformers shown significant effectiveness image video synthesis expense huge computation costs address problem feature caching methods introduced accelerate diffusion transformers caching features previous timesteps reusing following timesteps however previous caching methods ignore different tokens exhibit different sensitivities feature caching feature caching tokens may lead destruction overall generation quality compared tokens paper introduce tokenwise feature caching allowing us adaptively select suitable tokens caching enable us apply different caching ratios neural layers different types depths extensive experiments pixartalpha opensora dit demonstrate effectiveness image video generation requirements training instance acceleration achieved opensora pixartalpha almost drop generation quality,-1,0.0,-1,0.0
smoothcache universal inference acceleration technique diffusion transformers diffusion transformers dit emerged powerful generative models various tasks including image video speech synthesis however inference process remains computationally expensive due repeated evaluation resourceintensive attention feedforward modules address introduce smoothcache modelagnostic inference acceleration technique dit architectures smoothcache leverages observed high similarity layer outputs across adjacent diffusion timesteps analyzing layerwise representation errors small calibration set smoothcache adaptively caches reuses key features inference experiments demonstrate smoothcache achieves speed maintaining even improving generation quality across diverse modalities showcase effectiveness ditxl image generation opensora texttovideo stable audio open texttoaudio highlighting potential enable realtime applications broaden accessibility powerful dit models,20,1.0,20,1.0
importancebased token merging diffusion models diffusion models excel highquality image video generation however major drawback high latency simple yet powerful way speed merging similar tokens faster computation though result quality loss paper demonstrate preserving important tokens merging significantly improves sample quality notably importance token reliably determined using classifierfree guidance magnitude measure strongly correlated conditioning input corresponds output fidelity since classifierfree guidance incurs additional computational cost requires extra modules method easily integrated diffusionbased frameworks experiments show approach significantly outperforms baseline across various applications including texttoimage synthesis multiview image generation video generation,20,1.0,20,1.0
highquality video generation event camera via theoryinspired modelaided deep learning bioinspired event cameras dynamic vision sensors capable asynchronously capturing perpixel brightness changes called eventstreams high temporal resolution high dynamic range however nonstructural spatialtemporal eventstreams make challenging providing intuitive visualization rich semantic information human vision calls eventstovideo solutions take eventstreams input generate high quality video frames intuitive visualization however current solutions predominantly datadriven without considering prior knowledge underlying statistics relating eventstreams video frames highly relies nonlinearity generalization capability deep neural networks thus struggling reconstructing detailed textures scenes complex work propose novel paradigm designed produce highquality video frames events approach leverages modelaided deep learning framework underpinned theoryinspired model meticulously derived fundamental imaging principles event cameras deal issue statereset recurrent components also design temporal shift embedding module improve quality video frames comprehensive evaluations real world event camera datasets validate approach notably outperforming stateoftheart approaches eg surpassing second best evaluation metrics,-1,0.0,-1,0.0
domainadaptive video deblurring via testtime blurring dynamic scene video deblurring aims remove undesirable blurry artifacts captured exposure process although previous video deblurring methods achieved impressive results suffer significant performance drops due domain gap training testing videos especially captured realworld scenarios address issue propose domain adaptation scheme based blurring model achieve testtime finetuning deblurring models unseen domains since blurred sharp pairs unavailable finetuning inference scheme generate domainadaptive training pairs calibrate deblurring model target domain first relative sharpness detection module proposed identify relatively sharp regions blurry input images regard pseudosharp images next utilize blurring model produce blurred images based pseudosharp images extracted testing synthesize blurred images compliance target data distribution propose domainadaptive blur condition generation module create domainspecific blur conditions blurring model finally generated pseudosharp blurred pairs used finetune deblurring model better performance extensive experimental results demonstrate approach significantly improve stateoftheart video deblurring methods providing performance gains various realworld video deblurring datasets source code available httpsgithubcomjintinghedadeblur,-1,0.0,-1,0.0
irag advancing rag videos incremental approach retrievalaugmented generation rag systems combine strengths language generation information retrieval power many realworld applications like chatbots use rag understanding videos appealing two critical limitations onetime upfront conversion content large corpus videos text descriptions entails high processing times also information rich video data typically captured text descriptions since user queries known apriori developing system video text conversion interactive querying video data challenging address limitations propose incremental rag system called irag augments rag novel incremental workflow enable interactive querying large corpus videos unlike traditional rag irag quickly indexes large repositories videos incremental workflow uses index opportunistically extract details select portions videos retrieve context relevant interactive user query incremental workflow avoids long video text conversion times overcomes information loss issues due conversion video text ondemand queryspecific extraction details video data ensures high quality responses interactive user queries often known apriori best knowledge irag first system augment rag incremental workflow support efficient interactive querying large corpus videos experimental results realworld datasets demonstrate faster video text ingestion ensuring latency quality responses interactive user queries comparable responses traditional rag video data converted text upfront user querying,-1,0.0,-1,0.0
dance beat blending beats visuals dance video generation generating dance music crucial advancing automated choreography current methods typically produce skeleton keypoint sequences instead dance videos lack capability make specific individuals dance reduces realworld applicability methods also require precise keypoint annotations complicating data collection limiting use selfcollected video datasets overcome challenges introduce novel task generating dance videos directly images individuals guided music task enables dance generation specific individuals without requiring keypoint annotations making versatile applicable various situations solution dance beat diffusion model dabfusion utilizes reference image music piece generate dance videos featuring various dance types choreographies music analyzed specially designed music encoder identifies essential features including dance style movement rhythm dabfusion excels generating dance videos individuals training dataset also previously unseen person versatility stems approach generating latent optical flow contains necessary motion information animate person image evaluate dabfusions performance using aist dataset focusing video quality audiovideo synchronization motionmusic alignment propose motionmusic alignment score align builds beat alignment score effectively evaluate motionmusic alignment new task experiments show dabfusion establishes solid baseline innovative task video results found project page httpsdabfusiongithubio,8,0.5058523326771341,8,0.5058523326771341
stiv scalable text image conditioned video generation field video generation made remarkable advancements yet remains pressing need clear systematic recipe guide development robust scalable models work present comprehensive study systematically explores interplay model architectures training recipes data curation strategies culminating simple scalable textimageconditioned video generation method named stiv framework integrates image condition diffusion transformer dit frame replacement incorporating text conditioning via joint imagetext conditional classifierfree guidance design enables stiv perform texttovideo textimagetovideo tasks simultaneously additionally stiv easily extended various applications video prediction frame interpolation multiview generation long video generation etc comprehensive ablation studies stiv demonstrate strong performance despite simple design model resolution achieves vbench surpassing leading open closedsource models like pika kling samesized model also achieves stateoftheart result vbench task resolution providing transparent extensible recipe building cuttingedge video generation models aim empower future research accelerate progress toward versatile reliable video generation solutions,-1,0.0,-1,0.0
latentartifusion effective efficient histological artifacts restoration framework histological artifacts pose challenges pathologists computeraided diagnosis cad systems leading errors analysis current approaches histological artifact restoration based generative adversarial networks gans pixellevel diffusion models suffer performance limitations computational inefficiencies paper propose novel framework latentartifusion leverages latent diffusion model ldm reconstruct histological artifacts high performance computational efficiency unlike traditional pixellevel diffusion frameworks latentartifusion executes restoration process lowerdimensional latent space significantly improving computational efficiency moreover introduce novel regional artifact reconstruction algorithm latent space prevent mistransfer nonartifact regions distinguishing approach ganbased methods extensive experiments realworld histology datasets latentartifusion demonstrates remarkable speed outperforming stateoftheart pixellevel diffusion frameworks also consistently surpasses ganbased methods least across multiple evaluation metrics furthermore evaluate effectiveness proposed framework downstream tissue classification tasks showcasing practical utility code available httpsgithubcombugscreatorlatentartifusion,-1,0.0,-1,0.0
unveiling contextrelated anomalies knowledge graph empowered decoupling scene action humanrelated video anomaly detection detecting anomalies humanrelated videos crucial surveillance applications current methods primarily include appearancebased actionbased techniques appearancebased methods rely lowlevel visual features color texture shape learn large number pixel patterns features related known scenes training making effective detecting anomalies within familiar contexts however encountering new significantly changed scenes ie unknown scenes often fail existing sota methods effectively capture relationship actions surrounding scenes resulting low generalization contrast actionbased methods focus detecting anomalies human actions usually less informative tend overlook relationship actions scenes leading incorrect detection instance normal event running beach abnormal event running street might considered normal due lack scene information short current methods struggle integrate lowlevel visual highlevel action features leading poor anomaly detection varied complex scenes address challenge propose novel decouplingbased architecture humanrelated video anomaly detection decoad decoad significantly improves integration visual action features decoupling interweaving scenes actions thereby enabling intuitive accurate understanding complex behaviors scenes decoad supports fully supervised weakly supervised unsupervised settings,-1,0.0,-1,0.0
survey visual signal coding processing generative models technologies standards optimization paper provides survey latest developments visual signal coding processing generative models specifically focus presenting advancement generative models influence research domain visual signal coding processing survey study begins brief introduction wellestablished generative models including variational autoencoder vae models generative adversarial network gan models autoregressive ar models normalizing flows diffusion models subsequent section paper explores advancements visual signal coding based generative models well ongoing international standardization activities realm visual signal processing focus lies application development various generative models research visual signal restoration also present latest developments generative visual signal synthesis editing along visual signal quality assessment using generative models quality assessment generative models practical implementation studies closely linked investigation fast optimization paper additionally presents latest advancements fast optimization visual signal coding processing generative models hope advance field providing researchers practitioners comprehensive literature review topic visual signal coding processing generative models,-1,0.0,-1,0.0
explorative inbetweening time space introduce bounded generation generalized task control video generation synthesize arbitrary camera subject motion based given start end frame objective fully leverage inherent generalization capability imagetovideo model without additional training finetuning original model achieved proposed new sampling strategy call time reversal fusion fuses temporally forward backward denoising paths conditioned start end frame respectively fused path results video smoothly connects two frames generating inbetweening faithful subject motion novel views static scenes seamless video looping two bounding frames identical curate diverse evaluation dataset image pairs compare closest existing methods find time reversal fusion outperforms related work subtasks exhibiting ability generate complex motions views guided bounded frames see project page httpstimereversalgithubio,-1,0.0,-1,0.0
texttoaudio generation synchronized videos recent times focus texttoaudio tta generation intensified researchers strive synthesize audio textual descriptions however existing methods though leveraging latent diffusion models learn correlation audio text embeddings fall short comes maintaining seamless synchronization produced audio video often results discernible audiovisual mismatches bridge gap introduce groundbreaking benchmark texttoaudio generation aligns videos named benchmark distinguishes three novel metrics dedicated evaluating visual alignment temporal consistency complement also present simple yet effective videoaligned tta generation model namely moving beyond traditional methods refines latent diffusion approach integrating visualaligned text embeddings conditional foundation employs temporal multihead attention transformer extract understand temporal nuances video data feat amplified audiovisual controlnet adeptly merges temporal visual representations text embeddings enhancing integration weave contrastive learning objective designed ensure visualaligned text embeddings resonate closely audio features extensive evaluations audiocaps demonstrate sets new standard videoaligned tta generation ensuring visual alignment temporal consistency,8,0.5568697814006139,8,0.5568697814006139
cospeech gesture video generation via motiondecoupled diffusion model cospeech gestures presented lively form videos achieve superior visual effects humanmachine interaction previous works mostly generate structural human skeletons resulting omission appearance information focus direct generation audiodriven cospeech gesture videos work two main challenges suitable motion feature needed describe complex human movements crucial appearance information gestures speech exhibit inherent dependencies temporally aligned even arbitrary length solve problems present novel motiondecoupled framework generate cospeech gesture videos specifically first introduce welldesigned nonlinear tps transformation obtain latent motion features preserving essential appearance information transformerbased diffusion model proposed learn temporal correlation gestures speech performs generation latent motion space followed optimal motion selection module produce longterm coherent consistent gesture videos better visual perception design refinement network focusing missing details certain areas extensive experimental results show proposed framework significantly outperforms existing approaches motion videorelated evaluations code demos resources available,6,0.3842467233307934,6,0.3842467233307934
controllable generation anyview rendering street scenes controllable generative models images videos achieved remarkable success highquality models scenes particularly unbounded scenarios like autonomous driving remain underdeveloped due high data acquisition costs paper introduce novel pipeline controllable street scene generation supports multicondition control including bev maps objects text descriptions unlike previous methods reconstruct training generative models first trains video generation model reconstructs generated data innovative approach enables easily controllable generation static scene acquisition resulting highquality scene reconstruction address minor errors generated content propose deformable gaussian splatting monocular depth initialization appearance modeling manage exposure discrepancies across viewpoints validated nuscenes dataset generates diverse highquality driving scenes support anyview rendering enhance downstream tasks like bev segmentation results demonstrate frameworks superior performance showcasing potential autonomous driving simulation beyond,-1,0.0,-1,0.0
sugar subjectdriven video customization zeroshot manner present sugar zeroshot method subjectdriven video customization given input image sugar capable generating videos subject contained image aligning generation arbitrary visual attributes style motion specified userinput text unlike previous methods require testtime finetuning fail generate textaligned videos sugar achieves superior results without need extra cost testtime enable zeroshot capability introduce scalable pipeline construct synthetic dataset specifically designed subjectdriven customization leading millions imagevideotext triplets additionally propose several methods enhance model including special attention designs improved training strategies refined sampling algorithm extensive experiments conducted compared previous methods sugar achieves stateoftheart results identity preservation video dynamics videotext alignment subjectdriven video customization demonstrating effectiveness proposed method,11,0.9235240508025361,11,0.9235240508025361
one image reeditable dynamic model video generation one image editable dynamic model video generation novel direction change research area single image representation reconstruction image gaussian splatting demonstrated advantages implicit reconstruction compared original neural radiance fields rapid development technologies principles people tried used stable diffusion models generate targeted models text instructions however using normal implicit machine learning methods hard gain precise motions actions control difficult generate long content semantic continuous video address issue propose method theory used one single image generate editable model generate targeted semantic continuous timeunlimited video used normal basic gaussian splatting model generate model single image requires less volume video memory computer calculation ability subsequently designed automatic generation selfadaptive binding mechanism object armature combined reeditable motions actions analyzing controlling algorithm proposed achieve better performance sota projects area building model precise motions actions control generating stable semantic continuous timeunlimited video input text instructions analyze detailed implementation methods theories analyses relative comparisons conclusions presented project code open source,-1,0.0,-1,0.0
trainingfree adaptive diffusion bounded difference approximation strategy diffusion models recently achieved great success synthesis highquality images videos however existing denoising techniques diffusion models commonly based stepbystep noise predictions suffers high computation cost resulting prohibitive latency interactive applications paper propose adaptivediffusion relieve bottleneck adaptively reducing noise prediction steps denoising process method considers potential skipping many noise prediction steps possible keeping final denoised results identical original fullstep ones specifically skipping strategy guided thirdorder latent difference indicates stability timesteps denoising process benefits reusing previous noise prediction results extensive experiments image video diffusion models demonstrate method significantly speed denoising process generating identical results original process achieving average speedup without quality degradation,-1,0.0,-1,0.0
neural fields tracking anatomy surgical instruments monocular laparoscopic video clips laparoscopic video tracking primarily focuses two target types surgical instruments anatomy former could used skill assessment latter necessary projection virtual overlays instrument anatomy tracking often considered two separate problems paper propose method joint tracking structures simultaneously based single monocular video clip train neural field represent continuous spatiotemporal scene used create tracks surfaces visible least one frame due small size instruments generally cover small part image resulting decreased tracking accuracy therefore propose enhanced class weighting improve instrument tracks evaluate tracking video clips laparoscopic cholecystectomies find mean tracking accuracies anatomical structures instruments additionally assess quality depth maps obtained methods scene reconstructions show pseudodepths comparable quality stateoftheart pretrained depth estimator laparoscopic videos scared dataset method predicts depth mae mm relative error results show feasibility using neural fields monocular reconstruction laparoscopic scenes,-1,0.0,-1,0.0
selfsupervised monocular scene reconstruction egocentric videos egocentric videos provide valuable insights human interactions physical world sparked growing interest computer vision robotics communities critical challenge fully understanding geometry dynamics egocentric videos dense scene reconstruction however lack highquality labeled datasets field hindered effectiveness current supervised learning methods work aim address issue exploring selfsupervised dynamic scene reconstruction approach introduce novel model unifies estimation multiple variables necessary egocentric monocular reconstruction including camera intrinsic camera poses video depth within fast feedforward framework starting pretrained singleframe depth intrinsic estimation model extend camera poses estimation align multiframe results largescale unlabeled egocentric videos evaluate indomain zeroshot generalization settings achieving superior performance dense pointclouds sequence reconstruction compared baselines represents first attempt apply selfsupervised learning pointclouds sequence reconstruction labelscarce egocentric field enabling fast dense generalizable reconstruction interactable visualization code trained models released,1,1.0,1,1.0
videoshop localized semantic video editing noiseextrapolated diffusion inversion introduce videoshop trainingfree video editing algorithm localized semantic edits videoshop allows users use editing software including photoshop generative inpainting modify first frame automatically propagates changes semantic spatial temporally consistent motion remaining frames unlike existing methods enable edits imprecise textual instructions videoshop allows users add remove objects semantically change objects insert stock photos videos etc finegrained control locations appearance achieve imagebased video editing inverting latents noise extrapolation generate videos conditioned edited image videoshop produces higher quality edits baselines editing benchmarks using evaluation metrics,9,0.9241005990203617,9,0.9241005990203617
genmm geometrically temporally consistent multimodal data generation video lidar multimodal synthetic data generation crucial domains autonomous driving robotics augmentedvirtual reality retail propose novel approach genmm jointly editing rgb videos lidar scans inserting temporally geometrically consistent objects method uses reference image bounding boxes seamlessly insert blend new objects target videos inpaint regions interest consistent boxes using diffusionbased video inpainting model compute semantic boundaries object estimate surface depth using stateoftheart semantic segmentation monocular depth estimation techniques subsequently employ geometrybased optimization algorithm recover shape objects surface ensuring fits precisely within bounding box finally lidar rays intersecting new object surface updated reflect consistent depths geometry experiments demonstrate effectiveness genmm inserting various objects across video lidar modalities,1,1.0,1,1.0
ffa sora video generation fundus fluorescein angiography simulator fundus fluorescein angiography ffa critical diagnosing retinal vascular diseases beginners often struggle image interpretation study develops ffa sora texttovideo model converts ffa reports dynamic videos via waveletflow variational autoencoder wfvae diffusion transformer dit trained anonymized dataset ffa sora accurately simulates disease features input text confirmed objective metrics frechet video distance fvd learned perceptual image patch similarity lpips visualquestionanswering score vqascore specific evaluations showed acceptable alignment generated videos textual prompts bertscore additionally model demonstrated strong privacypreserving performance retrieval evaluations achieving average recallk human assessments indicated satisfactory visual quality average score best worst model addresses privacy concerns associated sharing largescale ffa data enhances medical education,19,1.0,19,1.0
kvq kwai video quality assessment shortform videos shortform ugc video platforms like kwai tiktok emerging irreplaceable mainstream media form thriving userfriendly engagement kaleidoscope creation etc however advancing contentgeneration modes eg special effects sophisticated processing workflows eg deartifacts introduced significant challenges recent ugc video quality assessment ambiguous contents hinder identification qualitydetermined regions ii diverse complicated hybrid distortions hard distinguish tackle challenges assist development shortform videos establish first largescale kaleidoscope short video database quality assessment termed kvq comprises useruploaded short videos processed videos diverse practical processing workflows including preprocessing transcoding enhancement among absolute quality score video partial ranking score among indistinguishable samples provided team professional researchers specializing image processing based database propose first shortform video quality evaluator ie ksvqe enables quality evaluator identify qualitydetermined semantics content understanding large vision language models ie clip distinguish distortions distortion understanding module experimental results shown effectiveness ksvqe kvq database popular vqa databases,12,1.0,12,1.0
videoprism foundational visual encoder video understanding introduce videoprism generalpurpose video encoder tackles diverse video understanding tasks single frozen model pretrain videoprism heterogeneous corpus containing highquality videocaption pairs video clips noisy parallel text eg asr transcripts pretraining approach improves upon masked autoencoding globallocal distillation semantic video embeddings token shuffling scheme enabling videoprism focus primarily video modality leveraging invaluable text associated videos extensively test videoprism four broad groups video understanding tasks web video question answering cv science achieving stateoftheart performance video understanding benchmarks,0,1.0,0,1.0
improving video corpus moment retrieval partial relevance enhancement video corpus moment retrieval vcmr new video retrieval task aimed retrieving relevant moment large corpus untrimmed videos using text query relevance video query partial mainly evident two untrimmed video contains many frames relevant query strong relevance typically observed within relevant relevance query varies different modalities action descriptions align visual elements character conversations related textual informationexisting methods often treat video contents equally leading suboptimal moment retrieval argue effectively capturing partial relevance query video essential vcmr task end propose partial relevance enhanced modelprem improve vcmr vcmr involves two subtasks video retrieval moment localization align distinct objectives implement specialized partial relevance enhancement strategies video retrieval introduce multimodal collaborative video retriever generating different query representations two modalities modalityspecific pooling ensuring effective match moment localization propose focusthenfuse moment localizer utilizing modalityspecific gates capture essential content also introduce relevant contentenhanced training methods retriever localizer enhance ability model capture relevant content experimental results tvr didemo datasets show proposed model outperforms baselines achieving new stateoftheart vcmr code available,0,1.0,0,1.0
shorts vs regular videos youtube comparative analysis user engagement content creation trends youtube introduced shorts video format allowing users upload short videos prominently displayed website app despite large visual footprint studies date looked impact shorts introduction production consumption content youtube paper presents first comparative analysis youtube shorts versus regular videos respect user engagement ie views likes comments content creation frequency video categories collected dataset containing information channels posted least one short analyzed metadata videos shorts regular videos uploaded january december spanning twoyear period including introduction shorts longitudinal analysis shows content creators consistently increased frequency shorts production period especially newlycreated channels surpassed regular videos also observe shorts target mostly entertainment categories regular videos cover wide variety categories general shorts attract views likes per view regular videos attract less comments per view however shorts outperform regular videos education political categories much categories study contributes understanding social media dynamics quantifying spread shortform content motivating future research impact society,-1,0.0,-1,0.0
tempcompass video llms really understand videos recently surge interest surrounding video large language models video llms however existing benchmarks fail provide comprehensive feedback temporal perception ability video llms one hand unable distinguish different temporal aspects eg speed direction thus reflect nuanced performance specific aspects hand limited diversity task formats eg multichoice qa hinders understanding temporal perception performance may vary across different types tasks motivated two problems propose textbftempcompass benchmark introduces diversity temporal aspects task formats collect highquality test data devise two novel strategies video collection construct conflicting videos share static content differ specific temporal aspect prevents video llms leveraging singleframe bias language priors collect task instructions propose paradigm humans first annotate metainformation video llm generates instruction also design llmbased approach automatically accurately evaluate responses video llms based tempcompass comprehensively evaluate stateoftheart sota video llms image llms reveal discerning fact models exhibit notably poor temporal perception ability data available,0,1.0,0,1.0
koala key frameconditioned long videollm long video question answering challenging task involves recognizing shortterm activities reasoning finegrained relationships stateoftheart video large language models vllms hold promise viable solution due demonstrated emergent capabilities new tasks however despite trained millions short secondslong videos vllms unable understand minuteslong videos accurately answer questions address limitation propose lightweight selfsupervised approach key frameconditioned long videollm koala introduces learnable spatiotemporal queries adapt pretrained vllms generalizing longer videos approach introduces two new tokenizers condition visual tokens computed sparse video key frames understanding short long video moments train proposed approach demonstrate effectiveness zeroshot long video understanding benchmarks outperforms stateoftheart large models absolute accuracy across tasks surprisingly also empirically show approach helps pretrained vllm understand long videos also improves accuracy shortterm action recognition,0,0.9616836977366661,0,0.9616836977366661
objectattributerelation representation based video semantic communication rapid growth multimedia data volume increasing need efficient video transmission applications virtual reality future video streaming services semantic communication emerging vital technique ensuring efficient reliable transmission lowbandwidth highnoise settings however current approaches focus joint sourcechannel coding jscc depends endtoend training methods often lack interpretable semantic representation struggle adaptability various downstream tasks paper introduce use objectattributerelation oar semantic framework videos facilitate low bitrate coding enhance jscc process effective video transmission utilize oar sequences low bitrate representation generative video reconstruction additionally incorporate oar image jscc model prioritize communication resources areas critical downstream tasks experiments traffic surveillance video datasets assess effectiveness approach terms video transmission performance empirical findings demonstrate oarbased video coding method outperforms coding lower bitrates also synergizes jscc deliver robust efficient video transmission,2,0.8703634389417582,2,0.8703634389417582
standard compliant video coding using low complexity switchable neural wrappers proliferation high resolution videos posts great storage bandwidth pressure cloud video services driving development nextgeneration video codecs despite great progress made neural video coding existing approaches still far economical deployment considering complexity ratedistortion performance tradeoff clear roadblocks neural video coding paper propose new framework featuring standard compatibility high performance low decoding complexity employ set jointly optimized neural pre postprocessors wrapping standard video codec encode videos different resolutions ratedistorion optimal downsampling ratio signaled decoder persequence level target rate design low complexity neural postprocessor architecture handle different upsampling ratios change resolution exploits spatial redundancy highresolution videos neural wrapper achieves ratedistortion performance improvement endtoend optimization codec proxy lightweight postprocessor architecture complexity macs pixel achieves bdrate reduction vvc uvg dataset aom ctc class approach potential advance performance latest video coding standards using neural processing minimal added complexity,2,1.0,2,1.0
video dataflywheel resolving impossible data trinity videolanguage understanding recently videolanguage understanding achieved great success largescale pretraining however data scarcity remains prevailing challenge study quantitatively reveals impossible trinity among data quantity diversity quality pretraining datasets recent efforts seek refine largescale diverse asr datasets compromised low quality synthetic annotations methods successfully leverage useful information multimodal video content frames tags asr transcripts etc refine original annotations nevertheless struggle mitigate noise within synthetic annotations lack scalability dataset size expands address issues introduce video dataflywheel framework iteratively refines video annotations improved noise control methods iterative refinement first leverage videolanguage model generate synthetic annotations resulting refined dataset pretrain finetune human refinement examples stronger model processes repeated continuous improvement noise control present adatailr novel noise control method requires weaker assumptions noise distribution thereby proving effective large datasets theoretical guarantees combination iterative refinement adatailr achieve better scalability videolanguage understanding extensive experiments show framework outperforms existing data refinement baselines delivering performance boost improving dataset quality minimal diversity loss furthermore refined dataset facilitates significant improvements various videolanguage understanding tasks including video question answering textvideo retrieval,-1,0.0,-1,0.0
trace temporal grounding video llm via causal event modeling video temporal grounding vtg crucial capability video understanding models plays vital role downstream tasks video browsing editing effectively handle various tasks simultaneously enable zeroshot prediction growing trend employing video llms vtg tasks however current video llmbased methods rely exclusively natural language generation lacking ability model clear structure inherent videos restricts effectiveness tackling vtg tasks address issue paper first formally introduces causal event modeling framework represents video llm outputs sequences events predict current event using previous events video inputs textural instructions event consists three components timestamps salient scores textual captions propose novel taskinterleaved video llm called trace effectively implement causal event modeling framework practice trace process visual frames timestamps salient scores text distinct tasks employing various encoders decoding heads task tokens arranged interleaved sequence according causal event modeling frameworks formulation extensive experiments various vtg tasks datasets demonstrate superior performance trace compared stateoftheart video llms model code available httpsgithubcomgyxxygtrace,0,0.9517774944640763,0,0.9517774944640763
pvvtt privacycentric dataset missionspecific anomaly detection natural language interpretation video crime detection significant application computer vision artificial intelligence however existing datasets primarily focus detecting severe crimes analyzing entire video clips often neglecting precursor activities ie privacy violations could potentially prevent crimes address limitation present pvvtt privacy violation video text unique multimodal dataset aimed identifying privacy violations pvvtt provides detailed annotations video text scenarios ensure privacy individuals videos provide video feature vectors avoiding release raw video data privacyfocused approach allows researchers use dataset protecting participant confidentiality recognizing privacy violations often ambiguous contextdependent propose graph neural network gnnbased video description model model generates gnnbased prompt image large language model llm deliver costeffective highquality video descriptions leveraging single video frame along relevant text method reduces number input tokens required maintaining descriptive quality optimizing llm apiusage extensive experiments validate effectiveness interpretability approach video description tasks flexibility pvvtt dataset,4,0.7697690183881342,4,0.7697690183881342
large motion video autoencoding crossmodal video vae learning robust video variational autoencoder vae essential reducing video redundancy facilitating efficient video generation directly applying image vaes individual frames isolation result temporal inconsistencies suboptimal compression rates due lack temporal compression existing video vaes begun address temporal compression however often suffer inadequate reconstruction performance paper present novel powerful video autoencoder capable highfidelity video encoding first observe entangling spatial temporal compression merely extending image vae vae introduce motion blur detail distortion artifacts thus propose temporalaware spatial compression better encode decode spatial information additionally integrate lightweight motion compression model temporal compression second propose leverage textual information inherent texttovideo datasets incorporate text guidance model significantly enhances reconstruction quality particularly terms detail preservation temporal stability third improve versatility model joint training images videos enhances reconstruction quality also enables model perform image video autoencoding extensive evaluations strong recent baselines demonstrate superior performance method project website found,2,1.0,2,1.0
megactor harness power raw video vivid portrait animation despite raw driving videos contain richer information facial expressions intermediate representations landmarks field portrait animation seldom subject research due two challenges inherent portrait animation driven raw videos significant identity leakage irrelevant background facial details wrinkles degrade performance harnesses power raw videos vivid portrait animation proposed pioneering conditional diffusion model named megactor first introduced synthetic data generation framework creating videos consistent motion expressions inconsistent ids mitigate issue id leakage second segmented foreground background reference image employed clip encode background details encoded information integrated network via text embedding module thereby ensuring stability background finally style transfer appearance reference image driving video eliminate influence facial details driving videos final model trained solely public datasets achieving results comparable commercial models hope help opensource communitythe code available httpsgithubcommegviiresearchmegfaceanimate,11,0.9705565649061898,11,0.9705565649061898
grounded video caption generation propose new task dataset model grounded video caption generation task unifies captioning object grounding video objects caption grounded video via temporally consistent bounding boxes introduce following contributions first present task definition manually annotated test dataset task referred grounded video caption generation groc second introduce largescale automatic annotation method leveraging existing model grounded still image captioning together llm summarising framelevel captions temporally consistent captions video furthermore prompt llm track language classifying noun phrases framelevel captions noun phrases videolevel generated caption apply approach videos dataset results new largescale training dataset called howtoground automatically annotated captions spatiotemporally consistent bounding boxes coherent natural language labels third introduce new grounded video caption generation model called videoground train model new automatically annotated howtoground dataset finally results videoground model set state art new task grounded video caption generation perform extensive ablations demonstrate importance key technical contributions model,0,1.0,0,1.0
video new language realworld decision making text video data abundant internet support largescale selfsupervised learning next token frame prediction however equally leveraged language models significant realworld impact whereas video generation remained largely limited media entertainment yet video data captures important information physical world difficult express language address gap discuss underappreciated opportunity extend video generation solve tasks real world observe akin language video serve unified interface absorb internet knowledge represent diverse tasks moreover demonstrate like language models video generation serve planners agents compute engines environment simulators techniques incontext learning planning reinforcement learning identify major impact opportunities domains robotics selfdriving science supported recent work demonstrates advanced capabilities video generation plausibly within reach lastly identify key challenges video generation mitigate progress addressing challenges enable video generation models demonstrate unique value alongside language models wider array ai applications,-1,0.0,-1,0.0
survey aigenerated video evaluation growing capabilities ai generating video content brought forward significant challenges effectively evaluating videos unlike static images text video content involves complex spatial temporal dynamics may require comprehensive systematic evaluation contents aspects like video presentation quality semantic information delivery alignment human intentions virtualreality consistency physical world survey identifies emerging field aigenerated video evaluation aigve highlighting importance assessing well aigenerated videos align human perception meet specific instructions provide structured analysis existing methodologies could potentially used evaluate aigenerated videos outlining strengths gaps current approaches advocate development robust nuanced evaluation frameworks handle complexities video content include conventional metricbased evaluations also current humaninvolved evaluations future modelcentered evaluations survey aims establish foundational knowledge base researchers academia practitioners industry facilitating future advancement evaluation methods aigenerated video content,10,0.6574876925698866,10,0.6574876925698866
tsvg textdriven stereoscopic video generation advent stereoscopic videos opened new horizons multimedia particularly extended reality xr virtual reality vr applications immersive content captivates audiences across various platforms despite growing popularity producing stereoscopic videos remains challenging due technical complexities involved generating stereo parallax refers positional differences objects viewed two distinct perspectives crucial creating depth perception complex process poses significant challenges creators aiming deliver convincing engaging presentations address challenges paper introduces textdriven stereoscopic video generation tsvg system innovative modelagnostic zeroshot approach streamlines video generation using text prompts create reference videos videos transformed point cloud sequences rendered two perspectives subtle parallax differences achieving natural stereoscopic effect tsvg represents significant advancement stereoscopic content creation integrating stateoftheart trainingfree techniques texttovideo generation depth estimation video inpainting flexible architecture ensures high efficiency userfriendliness allowing seamless updates newer models without retraining simplifying production pipeline tsvg makes stereoscopic video generation accessible broader audience demonstrating potential revolutionize field,1,1.0,1,1.0
textvideo retrieval via variational multimodal hypergraph networks textvideo retrieval challenging task aims identify relevant videos given textual queries compared conventional textual retrieval main obstacle textvideo retrieval semantic gap textual nature queries visual richness video content previous works primarily focus aligning query video finely aggregating wordframe matching signals inspired human cognitive process modularly judging relevance text video judgment needs highorder matching signal due consecutive complex nature video contents paper propose chunklevel textvideo matching query chunks extracted describe specific retrieval unit video chunks segmented distinct clips videos formulate chunklevel matching nary correlations modeling words query frames video introduce multimodal hypergraph nary correlation modeling representing textual units video frames nodes using hyperedges depict relationships multimodal hypergraph constructed way query video aligned highorder semantic space addition enhance models generalization ability extracted features fed variational inference component computation obtaining variational representation gaussian distribution incorporation hypergraphs variational inference allows model capture complex nary interactions among textual visual contents experimental results demonstrate proposed method achieves stateoftheart performance textvideo retrieval task,0,1.0,0,1.0
trusted video inpainting localization via deep attentive noise learning digital video inpainting techniques substantially improved deep learning recent years although inpainting originally designed repair damaged areas also used malicious manipulation remove important objects creating false scenes facts significant identify inpainted regions blindly paper present trusted video inpainting localization network truvil excellent robustness generalization ability observing highfrequency noise effectively unveil inpainted regions design deep attentive noise learning multiple stages capture inpainting traces firstly multiscale noise extraction module based high pass layers used create noise modality input rgb frames correlation two complementary modalities explored crossmodality attentive fusion module facilitate mutual feature learning lastly spatial details selectively enhanced attentive noise decoding module boost localization performance network prepare enough training samples also build framelevel video object segmentation dataset videos pixellevel annotation frames extensive experimental results validate superiority truvil compared stateofthearts particular quantitative qualitative evaluations various inpainted videos verify remarkable robustness generalization ability proposed truvil code dataset available httpsgithubcommultimediafortruvil,-1,0.0,-1,0.0
adult learners recall recognition performance affective feedback learning aigenerated synthetic video widespread use generative ai led multiple applications aigenerated text media potentially enhance learning outcomes however limited number welldesigned experimental studies investigating impact learning gains affective feedback aigenerated media compared traditional media eg text documents human recordings video current study recruited participants investigate adult learners recall recognition performances well affective feedback aigenerated synthetic video using mixedmethods approach preand posttest design specifically four learning conditions aigenerated framing human instructorgenerated text aigenerated synthetic videos human instructorgenerated text human instructorgenerated videos human instructorgenerated text frame baseline considered results indicated statistically significant difference amongst conditions recall recognition performance addition participants affective feedback statistically significantly different two video conditions however adult learners preferred learn video formats rather text materials,-1,0.0,-1,0.0
multimodal emotion recognition visionlanguage prompting modality dropout paper present solution second multimodal emotion recognition challenge track enhance accuracy generalization performance emotion recognition propose several methods multimodal emotion recognition firstly introduce emovclip model finetuned based clip using visionlanguage prompt learning designed videobased emotion recognition tasks leveraging prompt learning clip emovclip improves performance pretrained clip emotional videos additionally address issue modality dependence multimodal fusion employ modality dropout robust information fusion furthermore aid baichuan better extracting emotional information suggest using prompt baichuan lastly utilize selftraining strategy leverage unlabeled videos process use unlabeled videos highconfidence pseudolabels generated model incorporate training set experimental results demonstrate model ranks track achieving accuracy test set,-1,0.0,-1,0.0
rhythmic foley framework seamless audiovisual alignment videotoaudio synthesis research introduces innovative framework videotoaudio synthesis solves problems audiovideo desynchronization semantic loss audio incorporating semantic alignment adapter temporal synchronization adapter method significantly improves semantic integrity precision beat point synchronization particularly fastpaced action sequences utilizing contrastive audiovisual pretrained encoder model trained video highquality audio data improving quality generated audio dualadapter approach empowers users enhanced control audio semantics beat effects allowing adjustment controller achieve better results extensive experiments substantiate effectiveness framework achieving seamless audiovisual alignment,8,0.7074682926071666,8,0.7074682926071666
synthetic thermal rgb videos automatic pain assessment utilizing visionmlp architecture pain assessment essential developing optimal pain management protocols alleviate suffering prevent functional decline patients consequently reliable accurate automatic pain assessment systems essential continuous effective patient monitoring study presents synthetic thermal videos generated generative adversarial networks integrated pain recognition pipeline evaluates efficacy framework consisting visionmlp transformerbased module utilized employing rgb synthetic thermal videos unimodal multimodal settings experiments conducted facial videos biovid database demonstrate effectiveness synthetic thermal videos underline potential advantages,-1,0.0,-1,0.0
vimts unified video image text spotter enhancing crossdomain generalization text spotting task involving extraction textual information image video sequences faces challenges crossdomain adaption imagetoimage imagetovideo generalization paper introduce new method termed vimts enhances generalization ability model achieving better synergy among different tasks typically propose prompt queries generation module tasksaware adapter effectively convert original singletask model multitask model suitable image video scenarios minimal additional parameters prompt queries generation module facilitates explicit interaction different tasks tasksaware adapter helps model dynamically learn suitable features task additionally enable model learn temporal information lower cost propose synthetic video text dataset leveraging content deformation fields codef algorithm notably method outperforms stateoftheart method average six crossdomain benchmarks videolevel crossdomain adaption method even surpasses previous endtoend video spotting method video dstext average mota metric using imagelevel data demonstrate existing large multimodal models exhibit limitations generating crossdomain scene text spotting contrast vimts model requires significantly fewer parameters data code datasets made available httpsvimtextspottergithubio,7,0.8043767369646646,7,0.8043767369646646
human video generation diffusion transformer present novel approach generating highquality spatiotemporally coherent human videos single image framework combines strengths diffusion transformers capturing global correlations across viewpoints time cnns accurate condition injection core hierarchical transformer architecture factorizes selfattention across views time steps spatial dimensions enabling efficient modeling space precise conditioning achieved injecting human identity camera parameters temporal signals respective transformers train model collect multidimensional dataset spanning images videos multiview data limited footage along tailored multidimensional training strategy approach overcomes limitations previous methods based generative adversarial networks vanilla diffusion models struggle complex motions viewpoint changes generalization extensive experiments demonstrate methods ability synthesize realistic coherent human motion videos paving way advanced multimedia applications areas virtual reality animation,-1,0.0,-1,0.0
uniedit unified tuningfree framework video motion appearance editing recent advances textguided video editing showcased promising results appearance editing eg stylization however video motion editing temporal dimension eg eating waving distinguishes video editing image editing underexplored work present uniedit tuningfree framework supports video motion appearance editing harnessing power pretrained texttovideo generator within inversionthengeneration framework realize motion editing preserving source video content based insights temporal spatial selfattention layers encode interframe intraframe dependency respectively introduce auxiliary motionreference reconstruction branches produce textguided motion source features respectively obtained features injected main editing path via temporal spatial selfattention layers extensive experiments demonstrate uniedit covers video motion editing various appearance editing scenarios surpasses stateoftheart methods code publicly available,-1,0.0,-1,0.0
intelligent director automatic framework dynamic visual composition using chatgpt rise short video platforms represented tiktok trend users expressing creativity photos videos increased dramatically however ordinary users lack professional skills produce highquality videos using professional creation software meet demand intelligent userfriendly video creation tools propose dynamic visual composition dvc task interesting challenging task aims automatically integrate various media elements based user requirements create storytelling videos propose intelligent director framework utilizing lens generate descriptions images video frames combining chatgpt generate coherent captions recommending appropriate music names bestmatched music obtained music retrieval materials captions images videos music integrated seamlessly synthesize video finally apply style transfer construct personal album datasets verified effectiveness framework solving dvc qualitative quantitative comparisons along user studies demonstrating substantial potential,-1,0.0,-1,0.0
place solution mevis track cvpr pvuw workshop motion expression guided video segmentation referring video object segmentation rvos relies natural language expressions segment target objects video emphasizing modeling dense textvideo relations current rvos methods typically use independently pretrained vision language models backbones resulting significant domain gap video text crossmodal feature interaction text features used query initialization fully utilize important information text work propose using frozen pretrained visionlanguage models vlm backbones specific emphasis enhancing crossmodal feature interaction firstly use frozen convolutional clip backbone generate featurealigned vision text features alleviating issue domain gap reducing training costs secondly add crossmodal feature fusion pipeline enhance utilization multimodal information furthermore propose novel video query initialization method generate higher quality video queries without bells whistles method achieved jf mevis test set ranked place mevis track cvpr pvuw workshop motion expression guided video segmentation,-1,0.0,-1,0.0
sitar semisupervised image transformer action recognition recognizing actions limited set labeled videos remains challenge annotating visual data tedious also expensive due classified nature moreover handling spatiotemporal data using deep transformers introduce significant computational complexity paper objective address video action recognition semisupervised setting leveraging handful labeled videos along collection unlabeled videos compute efficient manner specifically rearrange multiple frames input videos rowcolumn form construct super images subsequently capitalize vast pool unlabeled samples employ contrastive learning encoded super images proposed approach employs two pathways generate representations temporally augmented super images originating video specifically utilize imagetransformer generate representations apply contrastive loss function minimize similarity representations different videos maximizing representations identical videos method demonstrates superior performance compared existing stateoftheart approaches semisupervised action recognition across various benchmark datasets significantly reducing computational costs,-1,0.0,-1,0.0
storynavi ondemand narrativedriven reconstruction video play generative ai manually navigating lengthy videos seek information answer questions tedious timeconsuming task users introduce storynavi novel system powered vllms generating customised video play experiences retrieving materials original videos directly answers users query constructing nonlinear sequence identified relevant clips form cohesive narrative storynavi offers two modes playback constructed video plays videocentric plays original audio skips irrelevant segments narrativecentric narration guides experience original audio muted technical evaluation showed adequate retrieval performance compared human retrieval user evaluation shows maintaining narrative coherence significantly enhances user engagement viewing disjointed video segments however factors like video genre content query may lead varying user preferences playback mode,-1,0.0,-1,0.0
variational quantum circuits enhanced generative adversarial network generative adversarial network gan one widelyadopted machinelearning frameworks wide range applications generating highquality images video audio contents however training gan could become computationally expensive large neural networks work propose hybrid quantumclassical architecture improving gan denoted qcgan performance examed numerically benchmarking classical gan using mindspore quantum task handwritten image generation generator qcgan consists quantum variational circuit together onelayer neural network discriminator consists traditional neural network leveraging entangling expressive power quantum circuits hybrid architecture achieved better performance frechet inception distance classical gan much fewer training parameters number iterations convergence also demonstrated superiority qcgan alternative quantum gan namely pathgan could hardly generate larger images work demonstrates value combining ideas quantum computing machine learning areas quantumforai aiforquantum,-1,0.0,-1,0.0
fast highresolution image synthesis latent adversarial diffusion distillation diffusion models main driver progress image video synthesis suffer slow inference speed distillation methods like recently introduced adversarial diffusion distillation add aim shift model manyshot singlestep inference albeit cost expensive difficult optimization due reliance fixed pretrained discriminator introduce latent adversarial diffusion distillation ladd novel distillation approach overcoming limitations add contrast pixelbased add ladd utilizes generative features pretrained latent diffusion models approach simplifies training enhances performance enabling highresolution multiaspect ratio image synthesis apply ladd stable diffusion obtain fast model matches performance stateoftheart texttoimage generators using four unguided sampling steps moreover systematically investigate scaling behavior demonstrate ladds effectiveness various applications image editing inpainting,-1,0.0,-1,0.0
disenstudio customized multisubject texttovideo generation disentangled spatial control generating customized content videos received increasing attention recently however existing works primarily focus customized texttovideo generation single subject suffering subjectmissing attributebinding problems video expected contain multiple subjects furthermore existing models struggle assign desired actions corresponding subjects actionbinding problem failing achieve satisfactory multisubject generation performance tackle problems paper propose disenstudio novel framework generate textguided videos customized multiple subjects given images subject specifically disenstudio enhances pretrained diffusionbased texttovideo model proposed spatialdisentangled crossattention mechanism associate subject desired action model customized multiple subjects proposed motionpreserved disentangled finetuning involves three tuning strategies multisubject cooccurrence tuning masked singlesubject tuning multisubject motionpreserved tuning first two strategies guarantee subject occurrence preserve visual attributes third strategy helps model maintain temporal motiongeneration ability finetuning static images conduct extensive experiments demonstrate proposed disenstudio significantly outperforms existing methods various metrics additionally show disenstudio used powerful tool various controllable generation applications,-1,0.0,-1,0.0
largescale visionlanguage sticker dataset multiframe animated sticker generation common form communication social mediastickers win users love internet scenarios ability convey emotions vivid cute interesting way people prefer get appropriate sticker retrieval rather creation reason creating sticker timeconsuming relies rulebased creative tools limited capabilities nowadays advanced texttovideo algorithms spawned numerous general video generation systems allow users customize highquality photorealistic videos providing simple text prompts however creating customized animated stickers lower frame rates abstract semantics videos greatly hindered difficulties data acquisition incomplete benchmarks facilitate exploration researchers animated sticker generation asg field firstly construct currently largest visionlanguage sticker dataset named twomillion scale contains static animated stickers secondly improve performance traditional video generation methods asg tasks discrete characteristics propose spatial temporal interaction sti layer utilizes semantic interaction detail preservation address issue insufficient information utilization moreover train baselines several video generation methods eg transformerbased diffusionbased methods conduct detailed analysis establish systemic supervision asg task best knowledge comprehensive largescale benchmark multiframe animated sticker generation hope work provide valuable inspiration scholars intelligent creation,-1,0.0,-1,0.0
arlon boosting diffusion transformers autoregressive models long video generation texttovideo models recently undergone rapid substantial advancements nevertheless due limitations data computational resources achieving efficient generation long videos rich motion dynamics remains significant challenge generate highquality dynamic temporally consistent long videos paper presents arlon novel framework boosts diffusion transformers autoregressive models long video generation integrating coarse spatial longrange temporal information provided ar model guide dit model specifically arlon incorporates several key innovations latent vector quantized variational autoencoder vqvae compresses input latent space dit model compact visual tokens bridging ar dit models balancing learning complexity information density adaptive normbased semantic injection module integrates coarse discrete visual units ar model dit model ensuring effective guidance video generation enhance tolerance capability noise introduced ar inference dit model trained coarser visual latent tokens incorporated uncertainty sampling module experimental results demonstrate arlon significantly outperforms baseline eight eleven metrics selected vbench notable improvements dynamic degree aesthetic quality delivering competitive results remaining three simultaneously accelerating generation process addition arlon achieves stateoftheart performance long video generation detailed analyses improvements inference efficiency presented alongside practical application demonstrates generation long videos using progressive text prompts see demos arlon httpakamsarlon,-1,0.0,-1,0.0
sparsecontrolled generation motion transfer recent advances generative models enable generation dynamic objects singleview video existing approaches utilize score distillation sampling form dynamic scene dynamic nerf dense gaussians however methods struggle strike balance among reference view alignment spatiotemporal consistency motion fidelity singleview conditions due implicit nature nerf intricate dense gaussian motion prediction address issues paper proposes efficient sparsecontrolled framework named decouples motion appearance achieve superior generation moreover introduce adaptive gaussian ag initialization gaussian alignment ga loss mitigate shape degeneration issue ensuring fidelity learned motion shape comprehensive experimental results demonstrate method surpasses existing methods quality efficiency addition facilitated disentangled modeling motion appearance devise novel application seamlessly transfers learned motion onto diverse array entities according textual descriptions,-1,0.0,-1,0.0
panacea panoramic controllable video generation autonomous driving field autonomous driving increasingly demands highquality annotated video training data paper propose panacea powerful universally applicable framework generating video data driving scenes built upon foundation previous work panacea panacea adopts multiview appearance noise prior mechanism superresolution module enhanced consistency increased resolution extensive experiments show generated video samples panacea greatly benefit wide range tasks different datasets including object tracking object detection lane detection tasks nuscenes argoverse dataset results strongly prove panacea valuable data generation framework autonomous driving,16,0.8573618307168253,16,0.8573618307168253
dartcontrol diffusionbased autoregressive motion model realtime textdriven motion control textconditioned human motion generation allows user interaction natural language become increasingly popular existing methods typically generate short isolated motions based single input sentence however human motions continuous extend long periods carrying rich semantics creating long complex motions precisely respond streams text descriptions particularly online realtime setting remains significant challenge furthermore incorporating spatial constraints textconditioned motion generation presents additional challenges requires aligning motion semantics specified text descriptions geometric information goal locations scene geometry address limitations propose dartcontrol short dart diffusionbased autoregressive motion primitive model realtime textdriven motion control model effectively learns compact motion primitive space jointly conditioned motion history text inputs using latent diffusion models autoregressively generating motion primitives based preceding history current text input dart enables realtime sequential motion generation driven natural language descriptions additionally learned motion primitive space allows precise spatial motion control formulate either latent noise optimization problem markov decision process addressed reinforcement learning present effective algorithms approaches demonstrating models versatility superior performance various motion synthesis tasks experiments show method outperforms existing baselines motion realism efficiency controllability video results available project page,-1,0.0,-1,0.0
vbench comprehensive versatile benchmark suite video generative models video generation witnessed significant advancements yet evaluating models remains challenge comprehensive evaluation benchmark video generation indispensable two reasons existing metrics fully align human perceptions ideal evaluation system provide insights inform future developments video generation end present vbench comprehensive benchmark suite dissects video generation quality specific hierarchical disentangled dimensions tailored prompts evaluation methods vbench several appealing properties comprehensive dimensions vbench comprises dimensions video generation eg subject identity inconsistency motion smoothness temporal flickering spatial relationship etc evaluation metrics finegrained levels reveal individual models strengths weaknesses human alignment also provide dataset human preference annotations validate benchmarks alignment human perception evaluation dimension respectively valuable insights look current models ability across various evaluation dimensions various content types also investigate gaps video image generation models versatile benchmarking vbench supports evaluating texttovideo imagetovideo introduce highquality image suite adaptive aspect ratio enable fair evaluations across different imagetovideo generation settings beyond assessing technical quality vbench evaluates trustworthiness video generative models providing holistic view model performance full opensourcing fully opensource vbench continually add new video generation models leaderboard drive forward field video generation,10,0.6353933965770848,10,0.6353933965770848
freedygs cameraposefree scene reconstruction based gaussian splatting dynamic surgical videos reconstructing endoscopic videos crucial highfidelity visualization efficiency surgical operations despite importance existing reconstruction methods encounter several challenges including stringent demands accuracy imprecise camera positioning intricate dynamic scenes necessity rapid reconstruction addressing issues paper presents first cameraposefree scene reconstruction framework freedygs tailored dynamic surgical videos leveraging gaussian splatting technology approach employs framebyframe reconstruction strategy delineated four distinct phases scene initialization joint learning scene expansion retrospective learning introduce generalizable gaussians parameterization module within scene initialization expansion phases proficiently generate gaussian attributes pixel rgbd frames joint learning phase crafted concurrently deduce scene deformation camera pose facilitated innovative flexible deformation module scene expansion stage gaussian points gradually grow camera moves retrospective learning phase dedicated enhancing precision scene deformation reassessment prior frames efficacy proposed freedygs substantiated experiments two datasets stereomis hamlyn datasets experimental outcomes underscore freedygs surpasses conventional baseline models rendering fidelity computational efficiency,1,1.0,1,1.0
movie gen cast media foundation models present movie gen cast foundation models generates highquality hd videos different aspect ratios synchronized audio also show additional capabilities precise instructionbased video editing generation personalized videos based users image models set new stateoftheart multiple tasks texttovideo synthesis video personalization video editing videotoaudio generation texttoaudio generation largest video generation model parameter transformer trained maximum context length video tokens corresponding generated video seconds framespersecond show multiple technical innovations simplifications architecture latent spaces training objectives recipes data curation evaluation protocols parallelization techniques inference optimizations allow us reap benefits scaling pretraining data model size training compute training large scale media generation models hope paper helps research community accelerate progress innovation media generation models videos paper available httpsgofbmemoviegenresearchvideos,-1,0.0,-1,0.0
multiview video diffusion model generation current generation methods achieved noteworthy efficacy aid advanced diffusion generative models however methods lack multiview spatialtemporal modeling encounter challenges integrating diverse prior knowledge multiple diffusion models resulting inconsistent temporal appearance flickers paper propose novel generation pipeline namely aimed generating spatialtemporally consistent content monocular video first design unified diffusion model tailored multiview video generation incorporating learnable motion module frozen diffusion model capture multiview spatialtemporal correlations training curated dataset diffusion model acquires reasonable temporal consistency inherently preserves generalizability spatial consistency diffusion model subsequently propose score distillation sampling loss based multiview video diffusion model optimize representation parameterized dynamic nerf aims eliminate discrepancies arising multiple diffusion models allowing generating spatialtemporally consistent content moreover devise anchor loss enhance appearance details facilitate learning dynamic nerf extensive qualitative quantitative experiments demonstrate method achieves superior performance compared previous methods,2,0.6962709329283329,2,0.6962709329283329
miradata largescale video dataset long durations structured captions soras highmotion intensity long consistent videos significantly impacted field video generation attracting unprecedented attention however existing publicly available datasets inadequate generating soralike videos mainly contain short videos low motion intensity brief captions address issues propose miradata highquality video dataset surpasses previous ones video duration caption detail motion strength visual quality curate miradata diverse manually selected sources meticulously process data obtain semantically consistent clips employed annotate structured captions providing detailed descriptions four different perspectives along summarized dense caption better assess temporal consistency motion intensity video generation introduce mirabench enhances existing benchmarks adding consistency trackingbased motion strength metrics mirabench includes evaluation prompts metrics covering temporal consistency motion strength consistency visual quality textvideo alignment distribution similarity demonstrate utility effectiveness miradata conduct experiments using ditbased video generation model miradit experimental results mirabench demonstrate superiority miradata especially motion strength,-1,0.0,-1,0.0
generative video indexer efficient textvideo retrieval current textvideo retrieval methods mainly rely crossmodal matching queries videos calculate similarity scores sorted obtain retrieval results method considers matching candidate video query incurs significant time cost increase notably increase candidates generative models common natural language processing computer vision successfully applied document retrieval application multimodal retrieval remains unexplored enhance retrieval efficiency paper introduce modelbased video indexer named sequencetosequence generative model directly generating video identifiers retrieving candidate videos constant time complexity aims reduce retrieval time maintaining high accuracy achieve goal propose video identifier encoding queryidentifier augmentation approaches represent videos short sequences preserving semantic information method consistently enhances retrieval efficiency current stateoftheart models four standard datasets enables baselines original retrieval time achieve better retrieval performance msrvtt msvd activitynet didemo code available,0,0.91958174223171,0,0.91958174223171
millionscale real text image prompt dataset imagetovideo generation video generation models revolutionizing content creation imagetovideo models drawing increasing attention due enhanced controllability visual consistency practical applications however despite popularity models rely userprovided text image prompts currently dedicated dataset studying prompts paper introduce first largescale dataset million unique userprovided text image prompts specifically imagetovideo generation additionally provide corresponding generated videos five stateoftheart imagetovideo models begin outlining timeconsuming costly process curating largescale dataset next compare two popular prompt datasets vidprom texttovideo diffusiondb texttoimage highlighting differences basic semantic information dataset enables advancements imagetovideo research instance develop better models researchers use prompts analyze user preferences evaluate multidimensional performance trained models enhance model safety may focus addressing misinformation issue caused imagetovideo models new research inspired differences existing datasets emphasize importance specialized imagetovideo prompt dataset project publicly available,-1,0.0,-1,0.0
efficient autoregressive video diffusion model causal generation cache sharing advance diffusion models todays video generation achieved impressive quality extend generation length facilitate realworld applications majority video diffusion models vdms generate videos autoregressive manner ie generating subsequent clips conditioned last frames previous clip however existing autoregressive vdms highly inefficient redundant model must recompute conditional frames overlapped adjacent clips issue exacerbated conditional frames extended autoregressively provide model longterm context cases computational demands increase significantly ie quadratic complexity wrt autoregression step paper propose efficient autoregressive vdm causal generation cache sharing causal generation introduces unidirectional feature computation ensures cache conditional frames precomputed previous autoregression steps reused every subsequent step eliminating redundant computations cache sharing shares cache across denoising steps avoid huge cache storage cost extensive experiments demonstrated achieves stateoftheart quantitative qualitative video generation results significantly improves generation speed code available httpsgithubcomdawnlxcausalcachevdm,-1,0.0,-1,0.0
harnessing large language models trainingfree video anomaly detection video anomaly detection vad aims temporally locate abnormal events video existing works mostly rely training deep models learn distribution normality either videolevel supervision oneclass supervision unsupervised setting trainingbased methods prone domainspecific thus costly practical deployment domain change involve data collection model training paper radically depart previous efforts propose languagebased vad lavad method tackling vad novel trainingfree paradigm exploiting capabilities pretrained large language models llms existing visionlanguage models vlms leverage vlmbased captioning models generate textual descriptions frame test video textual scene description devise prompting mechanism unlock capability llms terms temporal aggregation anomaly score estimation turning llms effective video anomaly detector leverage modalityaligned vlms propose effective techniques based crossmodal similarity cleaning noisy captions refining llmbased anomaly scores evaluate lavad two large datasets featuring realworld surveillance scenarios ucfcrime xdviolence showing outperforms unsupervised oneclass methods without requiring training data collection,7,0.8481455247201086,7,0.8481455247201086
cinepile long video question answering dataset benchmark current datasets longform video understanding often fall short providing genuine longform comprehension challenges many tasks derived datasets successfully tackled analyzing one random frames video address issue present novel dataset benchmark cinepile specifically designed authentic longform video understanding paper details innovative approach creating questionanswer dataset utilizing advanced llms humanintheloop building upon humangenerated raw data comprehensive dataset comprises multiplechoice questions mcqs covering various visual multimodal aspects including temporal comprehension understanding humanobject interactions reasoning events actions within scene additionally finetuned opensource videollms training split evaluated opensource proprietary videocentric llms test split dataset findings indicate although current models underperform compared humans finetuning models lead significant improvements performance,0,1.0,0,1.0
learning semantic traversability egocentric video automated annotation strategy reliable autonomous robot navigation urban settings robot must ability identify semantically traversable terrains image based semantic understanding scene reasoning ability based semantic traversability frequently achieved using semantic segmentation models finetuned testing domain finetuning process often involves manual data collection target robot annotation human labelers prohibitively expensive unscalable work present effective methodology training semantic traversability estimator using egocentric videos automated annotation process egocentric videos collected camera mounted pedestrians chest dataset training semantic traversability estimator automatically generated extracting semantically traversable regions video frame using recent foundation model image segmentation prompting technique extensive experiments videos taken across several countries cities covering diverse urban scenarios demonstrate high scalability generalizability proposed annotation method furthermore performance analysis realworld deployment autonomous robot navigation showcase trained semantic traversability estimator highly accurate able handle diverse camera viewpoints computationally light realworld applicable summary video available httpsyoutubeeuvohwala,-1,0.0,-1,0.0
ophnet largescale video benchmark ophthalmic surgical workflow understanding surgical scene perception via videos critical advancing robotic surgery telesurgery aiassisted surgery particularly ophthalmology however scarcity diverse richly annotated video datasets hindered development intelligent systems surgical workflow analysis existing datasets face challenges small scale lack diversity surgery phase categories absence timelocalized annotations limitations impede action understanding model generalization validation complex diverse realworld surgical scenarios address gap introduce ophnet largescale expertannotated video benchmark ophthalmic surgical workflow understanding ophnet features diverse collection surgical videos spanning types cataract glaucoma corneal surgeries detailed annotations unique surgical phases finegrained operations sequential hierarchical annotations surgery phase operation enabling comprehensive understanding improved interpretability timelocalized annotations facilitating temporal localization prediction tasks within surgical workflows approximately hours surgical videos ophnet times larger largest existing surgical workflow analysis benchmark code dataset available,-1,0.0,-1,0.0
fmital fewshot multiple instances temporal action localization probability distribution learning interval cluster refinement present fewshot temporal action localization model cant handle situation videos contain multiple action instances purpose paper achieve manifold action instances localization lengthy untrimmed query video using limited trimmed support videos address challenging problem effectively proposed novel solution involving spatialchannel relation transformer probability learning cluster refinement method accurately identify start end boundaries actions query video utilizing limited number labeled videos proposed method adept capturing temporal spatial contexts effectively classify precisely locate actions videos enabling comprehensive utilization crucial details selective cosine penalization algorithm designed suppress temporal boundaries include action scene switches probability learning combined label generation algorithm alleviates problem action duration diversity enhances models ability handle fuzzy action boundaries interval cluster help us get final results multiple instances situations fewshot temporal action localization model achieves competitive performance meticulous experimentation utilizing benchmark datasets code readily available httpsgithubcomycwfsfmital,-1,0.0,-1,0.0
authentication integrity smartphone videos multimedia container structure analysis nowadays mobile devices become natural substitute digital camera capture everyday situations easily quickly encouraging users express images videos videos shared across different platforms exposing kind intentional manipulation criminals aware weaknesses forensic techniques accuse innocent person exonerate guilty person judicial process commonly manufacturers comply specifications standards creation videos also videos shared social networks instant messaging applications go filtering compression processes reduce size facilitate transfer optimize storage platforms omission specifications results transformations carried platforms embed features pattern multimedia container videos patterns make possible distinguish brand device generated video social network instant messaging application used transfer research recent years focused analysis avi containers tiny video datasets work presents novel technique detect possible attacks mov format videos affect integrity authenticity method based analysis structure video containers generated mobile devices behavior shared social networks instant messaging applications manipulated editing programs objectives proposal verify integrity videos identify source acquisition distinguish original manipulated videos,4,0.7607410889068473,4,0.7607410889068473
conditional brownian bridge diffusion model vhr sar optical image translation synthetic aperture radar sar imaging technology provides unique advantage able collect data regardless weather conditions time however sar images exhibit complex backscatter patterns speckle noise necessitate expertise interpretation research translating sar images opticallike representations conducted aid interpretation sar data nevertheless existing studies predominantly utilized lowresolution satellite imagery datasets largely based generative adversarial network gan known training instability low fidelity overcome limitations lowresolution data usage ganbased approaches paper introduces conditional imagetoimage translation approach based brownian bridge diffusion model bbdm conducted comprehensive experiments msaw dataset paired sar optical images collection veryhighresolution vhr experimental results indicate method surpasses conditional diffusion models cdms ganbased models diverse perceptual quality metrics,-1,0.0,-1,0.0
camvig camera aware imagetovideo generation multimodal transformers extend multimodal transformers include camera motion conditioning signal task video generation generative video models becoming increasingly powerful thus focusing research efforts methods controlling output models propose add virtual camera controls generative video methods conditioning generated video encoding threedimensional camera movement course generated video results demonstrate able successfully control camera video generation starting single frame camera signal demonstrate accuracy generated camera paths using traditional computer vision methods,-1,0.0,-1,0.0
vintage joint video text conditioning holistic audio generation recent advances audio generation focused texttoaudio videotoaudio tasks however methods generate holistic sounds onscreen offscreen generate sounds aligning onscreen objects generate semantically complete offscreen sounds missing work address task holistic audio generation given video text prompt aim generate onscreen offscreen sounds temporally synchronized video semantically aligned text video previous approaches joint text videotoaudio generation often suffer modality bias favoring one modality overcome limitation introduce vintage flowbased transformer model jointly considers text video guide audio generation framework comprises two key components visualtext encoder joint vtsit model reduce modality bias improve generation quality employ pretrained unimodal texttoaudio videotoaudio generation models additional guidance due lack appropriate benchmarks also introduce vintagebench dataset videotextaudio pairs containing onscreen offscreen sounds comprehensive experiments vintagebench demonstrate joint text visual interaction necessary holistic audio generation furthermore vintage achieves stateoftheart results vggsound benchmark source code pretrained models released demo available httpswwwyoutubecomwatchvqmqwhujpkji,8,0.8633785048502406,8,0.8633785048502406
learning watching review videobased learning approaches robot manipulation robot learning manipulation skills hindered scarcity diverse unbiased datasets curated datasets help challenges remain generalizability realworld transfer meanwhile largescale inthewild video datasets driven progress computer vision selfsupervised techniques translating robotics recent works explored learning manipulation skills passively watching abundant videos sourced online showing promising results videobased learning paradigms provide scalable supervision reducing dataset bias survey reviews foundations video feature representation learning techniques object affordance understanding handbody modeling largescale robot resources well emerging techniques acquiring robot manipulation skills uncontrolled video demonstrations discuss learning observing largescale human videos enhance generalization sample efficiency robotic manipulation survey summarizes videobased learning approaches analyses benefits standard datasets survey metrics benchmarks discusses open challenges future directions nascent domain intersection computer vision natural language processing robot learning,5,0.42449882125689176,5,0.42449882125689176
place anything video controllable video editing demonstrated remarkable potential across diverse applications particularly scenarios capturing recapturing realworld videos either impractical costly paper introduces novel efficient system named placeanything facilitates insertion object video solely based picture text description target object element system comprises three modules generation video reconstruction target insertion integrated approach offers efficient effective solution producing editing highquality videos seamlessly inserting realistic objects user study demonstrate system effortlessly place object video using photograph object demo video found httpsyoutubeafxqgllrnte please also visit project page httpsplaceanythinggithubio get access,-1,0.0,-1,0.0
enhancing bandwidth efficiency video motion transfer applications using deep learning based keypoint prediction propose deep learning based novel prediction framework enhanced bandwidth reduction motion transfer enabled video applications video conferencing virtual reality gaming privacy preservation patient health monitoring model complex motion use first order motion model fomm represents dynamic objects using learned keypoints along local affine transformations keypoints extracted selfsupervised keypoint detector organized time series corresponding video frames prediction keypoints enable transmission using lower frames per second source device performed using variational recurrent neural network vrnn predicted keypoints synthesized video frames using optical flow estimator generator network efficacy leveraging keypoint based representations conjunction vrnn based prediction video animation reconstruction demonstrated three diverse datasets realtime applications results show effectiveness proposed architecture enabling additional bandwidth reduction existing keypoint based video motion transfer frameworks without significantly compromising video quality,-1,0.0,-1,0.0
enhancing traffic safety parallel dense video captioning endtoend event analysis paper introduces solution track ai city challenge task aims solve traffic safety description analysis dataset woven traffic safety wts realworld pedestriancentric traffic video dataset finegrained spatialtemporal understanding solution mainly focuses following points solve dense video captioning leverage framework dense video captioning parallel decoding pdvc model visuallanguage sequences generate dense caption chapters video work leverages clip extract visual features efficiently perform crossmodality training visual textual representations conduct domainspecific model adaptation mitigate domain shift problem poses recognition challenge video understanding moreover leverage captioned videos conduct knowledge transfer better understanding wts videos accurate captioning solution yielded test set achieving place competition open source code available,-1,0.0,-1,0.0
siavc semisupervised framework industrial accident video classification semisupervised learning suffers imbalance labeled unlabeled training data video surveillance scenario paper propose new semisupervised learning method called siavc industrial accident video classification specifically design video augmentation module called super augmentation block sab sab adds gaussian noise randomly masks video frames according historical loss unlabeled data model optimization propose video crossset augmentation module vcam generate diverse pseudolabel samples highconfidence unlabeled samples alleviates mismatch sampling experience provides highquality training data additionally construct new industrial accident surveillance video dataset framelevel annotation namely evaluate proposed method compared stateoftheart semisupervised learning based methods siavc demonstrates outstanding video classification performance achieving accuracy fire detection datasets respectively source code constructed dataset released urlhttpsgithubcomalchemyemperorsiavc,-1,0.0,-1,0.0
visionbased manipulation single human video openworld object graphs present objectcentric approach empower robots learn visionbased manipulation skills human videos investigate problem imitating robot manipulation single human video openworld setting robot must learn manipulate novel objects one video demonstration introduce orion algorithm tackles problem extracting objectcentric manipulation plan single rgbd video deriving policy conditions extracted plan method enables robot learn videos captured daily mobile devices ipad generalize policies deployment environments varying visual backgrounds camera angles spatial layouts novel object instances systematically evaluate method shorthorizon longhorizon tasks demonstrating efficacy orion learning single human video open world videos found project website httpsutaustinrplgithubioorionrelease,5,0.9760160781239179,5,0.9760160781239179
live video captioning dense video captioning task involves detection description events within video sequences traditional approaches focus offline solutions entire video analysis available captioning model work introduce paradigm shift towards live video captioning lvc lvc dense video captioning models must generate captions video streams online manner facing important constraints work partial observations video need temporal anticipation course ensuring ideally realtime response work formally introduce novel problem lvc propose new evaluation metrics tailored online scenario demonstrating superiority traditional metrics also propose lvc model integrating deformable transformers temporal filtering address lvc new challenges experimental evaluations activitynet captions dataset validate effectiveness approach highlighting performance lvc compared stateoftheart offline methods results model well evaluation kit novel metrics integrated made publicly available encourage research lvc,-1,0.0,-1,0.0
video incontext learning autoregressive transformers zeroshot video imitators people interact realworld largely dependent visual signal ubiquitous illustrate detailed demonstrations paper explore utilizing visual signals new interface models interact environment specifically choose videos representative visual signal training autoregressive transformers video datasets selfsupervised objective find model emerges zeroshot capability infer semantics demonstration video imitate semantics unseen scenario allows models perform unseen tasks watching demonstration video incontext manner without finetuning validate imitation capacity design various evaluation metrics including objective subjective measures results show models generate highquality video clips accurately align semantic guidance provided demonstration videos also show imitation capacity follows scaling law code models opensourced,-1,0.0,-1,0.0
slvideo sign language video moment retrieval framework slvideo video moment retrieval system sign language videos incorporates facial expressions addressing gap existing technology system extracts embedding representations hand face signs video frames capture signs entirety enabling users search specific sign language video segment text queries collection eight hours annotated portuguese sign language videos used dataset clip model used generate embeddings initial results promising zeroshot setting addition slvideo incorporates thesaurus enables users search similar signs retrieved using video segment embeddings also supports edition creation video sign language annotations project web page httpsnovasearchgithubioslvideo,-1,0.0,-1,0.0
aim challenge efficient video superresolution compressed content video superresolution vsr critical task enhancing lowbitrate lowresolution videos particularly streaming applications numerous solutions developed often suffer high computational demands resulting low frame rates fps poor power efficiency especially mobile platforms work compile different methods address challenges solutions endtoend realtime video superresolution frameworks optimized high performance low runtime also introduce new test set highquality videos validate approaches proposed solutions tackle video upscaling two applications general case tailored towards mobile devices tracks solutions reduced number parameters operations macs allow high fps improve vmaf psnr interpolation baselines report gauges efficient video superresolution methods date,2,1.0,2,1.0
neural video representation redundancy reduction consistency preservation implicit neural representation inr embed various signals neural networks gained attention recent years versatility handling diverse signal types context video inr achieves video compression embedding video signals directly networks compressing conventional methods either use index expresses time frame features extracted individual frames network inputs latter method provides greater expressive capability input specific video however features extracted frames often contain redundancy contradicts purpose video compression additionally redundancies make challenging accurately reconstruct highfrequency components frames address problems focus separating highfrequency lowfrequency components reconstructed frame propose video representation method generates highfrequency lowfrequency components frame using features extracted highfrequency components temporal information respectively experimental results demonstrate method outperforms existing hnerv method achieving superior results percent videos,2,0.9094934620351659,2,0.9094934620351659
ncst neuralbased color style transfer video retouching video color style transfer aims transform color style original video using reference style image existing methods employ neural networks come challenges like opaque transfer processes limited user control outcomes typically users finetune resulting images videos tackle issue introduce method predicts specific parameters color style transfer using two images initially train neural network learn corresponding color adjustment parameters applying style transfer video finetune network key frames video chosen style image generating precise transformation parameters applied convert color style images videos experimental results demonstrate algorithm surpasses current methods color style transfer quality moreover parameter method specific interpretable meaning enabling users understand color style transfer process allowing perform manual finetuning desired,-1,0.0,-1,0.0
ikea manuals work grounding assembly instructions internet videos shape assembly ubiquitous task daily life integral constructing complex structures like ikea furniture significant progress made developing autonomous agents shape assembly existing datasets yet tackled grounding assembly instructions videos essential holistic understanding assembly space time introduce ikea video manuals dataset features models furniture parts instructional manuals assembly videos internet importantly annotations dense spatiotemporal alignments data modalities demonstrate utility ikea video manuals present five applications essential shape assembly assembly plan generation partconditioned segmentation partconditioned pose estimation video object segmentation furniture assembly based instructional video manuals application provide evaluation metrics baseline methods experiments annotated data highlight many challenges grounding assembly instructions videos improve shape assembly including handling occlusions varying viewpoints extended assembly sequences,1,1.0,1,1.0
optimal transcoding preset selection live video streaming todays digital landscape video content dominates internet traffic underscoring need efficient video processing support seamless live streaming experiences platforms like youtube live twitch facebook live paper introduces comprehensive framework designed optimize video transcoding parameters specific focus preset bitrate selection minimize distortion respecting constraints bitrate transcoding time framework comprises three main steps feature extraction prediction optimization leverages extracted features predict transcoding time ratedistortion employing supervised unsupervised methods utilizing integer linear programming identifies optimal sequence presets bitrates video segments ensuring realtime application feasibility set constraints results demonstrate frameworks effectiveness enhancing video quality live streaming maintaining high standards video delivery managing computational resources efficiently optimization approach meets evolving demands video delivery offering solution realtime transcoding optimization evaluation using user generated content dataset showed average psnr improvement db default twitch configuration highlighting significant psnr gains additionally subsequent experiments demonstrated bdrate reduction reinforcing frameworks superior performance twitchs default configuration,2,1.0,2,1.0
playable game generation recent years artificial intelligence generated content aigc advanced texttoimage generation texttovideo multimodal video synthesis however generating playable games presents significant challenges due stringent requirements realtime interaction high visual quality accurate simulation game mechanics existing approaches often fall short either lacking realtime capabilities failing accurately simulate interactive mechanics tackle playability issue propose novel method called emphplaygen encompasses game data generation autoregressive ditbased diffusion model comprehensive playabilitybased evaluation framework validated wellknown games playgen achieves realtime interaction ensures sufficient visual quality provides accurate interactive mechanics simulation notably results sustained even frames gameplay nvidia rtx gpu code publicly available playable demo generated ai,-1,0.0,-1,0.0
exocentric egocentric transfer action recognition short survey egocentric vision captures scene point view camera wearer exocentric vision captures overall scene context jointly modeling ego exo views crucial developing nextgeneration ai agents community regained interest field egocentric vision thirdperson view firstperson thoroughly investigated works aim study synchronously exocentric videos contain many relevant signals transferrable egocentric videos paper provide broad overview works combining egocentric exocentric visions,-1,0.0,-1,0.0
temporally consistent dynamic scene graphs endtoend approach action tracklet generation understanding video content pivotal advancing realworld applications like activity recognition autonomous systems humancomputer interaction scene graphs adept capturing spatial relationships objects individual frames extending representations capture dynamic interactions across video sequences remains significant challenge address present tcdsg temporally consistent dynamic scene graphs innovative endtoend framework detects tracks links subjectobject relationships across time generating action tracklets temporally consistent sequences entities interactions approach leverages novel bipartite matching mechanism enhanced adaptive decoder queries feedback loops ensuring temporal coherence robust tracking extended sequences method establishes new benchmark achieving improvement temporal recallk action genome openpvsg meva datasets also pioneers augmentation meva persistent object id annotations comprehensive tracklet generation seamlessly integrating spatial temporal dynamics work sets new standard multiframe video analysis opening new avenues highimpact applications surveillance autonomous navigation beyond,-1,0.0,-1,0.0
motionaware contrastive learning temporal panoptic scene graph generation equip artificial intelligence comprehensive understanding towards temporal world video panoptic scene graph generation abstracts visual data nodes represent entities edges capture temporal relations existing methods encode entity masks tracked across temporal dimensions mask tubes predict relations temporal pooling operation fully utilize motion indicative entities relation overcome limitation introduce contrastive representation learning framework focuses motion pattern temporal scene graph generation firstly framework encourages model learn close representations mask tubes similar subjectrelationobject triplets secondly seek push apart mask tubes temporally shuffled versions moreover also learn distant representations mask tubes belonging video different triplets extensive experiments show motionaware contrastive framework significantly improves stateoftheart methods video datasets,-1,0.0,-1,0.0
spatiotemporal skip guidance enhanced video diffusion sampling diffusion models emerged powerful tool generating highquality images videos content sampling guidance techniques like cfg improve quality reduce diversity motion autoguidance mitigates issues demands extra weak model training limiting practicality largescale models work introduce spatiotemporal skip guidance stg simple trainingfree sampling guidance method enhancing transformerbased video diffusion models stg employs implicit weak model via selfperturbation avoiding need external models additional training selectively skipping spatiotemporal layers stg produces aligned degraded version original model boost sample quality without compromising diversity dynamic degree contributions include introducing stg efficient highperforming guidance technique video diffusion models eliminating need auxiliary models simulating weak model layer skipping ensuring qualityenhanced guidance without compromising sample diversity dynamics unlike cfg additional results visit httpsjunhahyunggithubiostguidance,-1,0.0,-1,0.0
recent advances digital image video forensics antiforensics counter antiforensics image video forensics recently gained increasing attention due proliferation manipulated images videos especially social media platforms twitter instagram spread disinformation fake news survey explores image video identification forgery detection covering manipulated digital media generative media however media forgery detection techniques susceptible antiforensics hand antiforensics techniques detected therefore cover antiforensics counter antiforensics techniques image video finally conclude survey highlighting open problems domain,4,0.9108417604365809,4,0.9108417604365809
presenting sense effort vibration based force estimated inverse dynamics videos present sense effort vibration help video viewer understand person video moves body suppose sense effort related force generate vibration based force present sense effort vibration use perceived intensity make sense effort proportional vibration demonstration experience vibration watching video create vibration spot experience vibration made video taken spot,-1,0.0,-1,0.0
video representation learning jointembedding predictive architectures video representation learning increasingly important topic machine learning research present video jepa variancecovariance regularization vjvcr jointembedding predictive architecture selfsupervised video representation learning employs variance covariance regularization avoid representation collapse show hidden representations vjvcr contain abstract highlevel information input data specifically outperform representations obtained generative baseline downstream tasks require understanding underlying dynamics moving objects videos additionally explore different ways incorporate latent variables vjvcr framework capture information uncertainty future nondeterministic settings,-1,0.0,-1,0.0
using physics informed generative adversarial networks model porous media microct scanning rocks significantly enhances understanding porescale physics porous media advancements porescale simulation methods pore network models possible accurately simulate multiphase flow properties including relative permeability ctscanned rock samples however limited number ctscanned samples challenge connecting porescale networks fieldscale rock properties often make difficult use porescale simulated properties realistic fieldscale reservoir simulations deep learning approaches create synthetic rock structures allow us simulate variations ct rock structures used compute representative rock properties flow functions however current deep learning methods rock structure synthesis dont consider rock properties derived well observations lacking direct link porescale structures fieldscale data present method construct rock structures constrained observed rock properties using generative adversarial networks gans conditioning accomplished gradual gaussian deformation process begin pretraining wasserstein gan reconstruct rock structures subsequently use pore network model simulator compute rock properties latent vectors image generation gan progressively altered using gaussian deformation approach produce rock structures constrained wellderived conditioning data gan gaussian deformation approach enables highresolution synthetic image generation reproduces userdefined rock properties porosity permeability pore size distribution research provides novel way link gangenerated models fieldderived quantities,-1,0.0,-1,0.0
scenellm implicit language reasoning llm dynamic scene graph generation dynamic scenes contain intricate spatiotemporal information crucial mobile robots uavs autonomous driving systems make informed decisions parsing scenes semantic triplets subjectpredicateobject accurate scene graph generation sgg highly challenging due fluctuating spatiotemporal complexity inspired reasoning capabilities large language models llms propose scenellm novel framework leverages llms powerful scene analyzers dynamic sgg framework introduces videotolanguage mapping module transforms video frames linguistic signals scene tokens making input comprehensible llms better encode spatial information devise spatial information aggregation sia scheme inspired structure chinese characters encodes spatial data tokens using optimal transport ot generate implicit language signal framelevel token sequence captures videos spatiotemporal information improve llms ability process implicit linguistic input apply lowrank adaptation lora finetune model finally use transformerbased sgg predictor decode llms reasoning predict semantic triplets method achieves stateoftheart results action genome ag benchmark extensive experiments show effectiveness scenellm understanding generating accurate dynamic scene graphs,0,0.9696319138145872,0,0.9696319138145872
subjective objective analysis indian social media video quality conducted largescale subjective study perceptual quality usergenerated mobile video content set mobileoriginated videos obtained indian social media platform sharechat content viewed volunteer human subjects controlled laboratory conditions benefit culturally diversifying existing corpus usergenerated content ugc video quality datasets great need large diverse ugcvqa datasets given explosive global growth visual internet social media platforms particularly true regard videos obtained smartphones especially rapidly emerging economies like india sharechat provides safe cultural community oriented space users generate share content preferred indian languages dialects subjective quality study based data offers boost cultural visual language diversification video quality research community expect new data resource also allow development systems predict perceived visual quality indian social media videos control scaling compression protocols streaming provide better user recommendations guide content analysis processing demonstrate value new data resource conducting study leading blind video quality models including new model called moeva deploys mixture experts predict video quality new livesharechat dataset sample source code moeva made freely available research community httpsgithubcomsandeepsmlivesc,12,0.4384761383606359,12,0.4384761383606359
saliency detection educational videos analyzing performance current models identifying limitations advancement directions identifying regions learning resource learner pays attention crucial assessing materials impact improving design related support systems saliency detection videos addresses automatic recognition attentiondrawing regions single frames educational settings recognition pertinent regions videos visual stream enhance content accessibility information retrieval tasks video segmentation navigation summarization advancements pave way development advanced aiassisted technologies support learning greater efficacy however task becomes particularly challenging educational videos due combination unique characteristics text voice illustrations animations best knowledge currently study evaluates saliency detection approaches educational videos paper address gap evaluating four stateoftheart saliency detection approaches educational videos reproduce original studies explore replication capabilities generalpurpose noneducational datasets investigate generalization capabilities models evaluate performance educational videos conduct comprehensive analysis identify common failure scenarios possible areas improvement experimental results show educational videos remain challenging context generic video saliency detection models,10,1.0,10,1.0
puppetmaster scaling interactive video generation motion prior partlevel dynamics present puppetmaster interactive video generative model serve motion prior partlevel dynamics test time given single image sparse set motion trajectories ie drags puppetmaster synthesize video depicting realistic partlevel motion faithful given drag interactions achieved finetuning largescale pretrained video diffusion model propose new conditioning architecture inject dragging control effectively importantly introduce alltofirst attention mechanism dropin replacement widely adopted spatial attention modules significantly improves generation quality addressing appearance background issues existing models unlike motionconditioned video generators trained inthewild videos mostly move entire object puppetmaster learned objaverseanimationhq new dataset curated partlevel motion clips propose strategy automatically filter suboptimal animations augment synthetic renderings meaningful motion trajectories puppetmaster generalizes well real images across various categories outperforms existing methods zeroshot manner realworld benchmark see project page results vggpuppetmastergithubio,-1,0.0,-1,0.0
pose guided human motion copy human motion copy intriguing yet challenging task artificial intelligence computer vision strives generate fake video target person performing motion source person problem inherently challenging due subtle humanbody texture details generated temporal consistency considered existing approaches typically adopt conventional gan loss produce target fake video intrinsically necessitates large number training samples challenging acquire meanwhile current methods still difficulties attaining realistic image details temporal consistency unfortunately easily perceived human observers motivated try tackle issues three aspects constrain posetoappearance generation perceptual loss theoretically motivated gromovwasserstein loss bridge gap pose appearance present episodic memory module posetoappearance generation propel continuous learning helps model learn past poor generations also utilize geometrical cues face optimize facial details refine key body part dedicated local gan advocate generating foreground sequencetosequence manner rather singleframe manner explicitly enforcing temporal inconsistency empirical results five datasets iper complexmotion solodance fish mouse datasets demonstrate method capable generating realistic target videos precisely copying motion source video method significantly outperforms stateoftheart approaches gains improvements psnr fid respectively,6,0.4268945506810246,6,0.4268945506810246
enhancing multimodal llm detailed accurate video captioning using multiround preference optimization videos contain wealth information generating detailed accurate descriptions natural language key aspect video understanding paper present videosalmonn advanced audiovisual large language model llm lowrank adaptation lora designed enhanced video paired audio captioning directed preference optimization dpo propose new metrics evaluate completeness accuracy video descriptions optimized using dpo improve training introduce novel multiround dpo mrdpo approach involves periodically updating dpo reference model merging reinitializing lora module proxy parameter updates training round steps incorporating guidance groundtruth video captions stabilize process address potential catastrophic forgetting noncaptioning abilities due mrdpo propose rebirth tuning finetunes predpo llm using captions generated mrdpotrained model supervised labels experiments show mrdpo significantly enhances videosalmonn captioning accuracy reducing global local error rates respectively decreasing repetition rate final videosalmonn model billion parameters surpasses leading models video captioning tasks maintaining competitive performance stateoftheart widely used video questionanswering benchmark among models similar size upon acceptance release code model checkpoints training test data demos available,-1,0.0,-1,0.0
mimicmotion highquality human motion video generation confidenceaware pose guidance recent years generative artificial intelligence achieved significant advancements field image generation spawning variety applications however video generation still faces considerable challenges various aspects controllability video length richness details hinder application popularization technology work propose controllable video generation framework dubbed mimicmotion generate highquality videos arbitrary length mimicking specific motion guidance compared previous methods approach several highlights firstly introduce confidenceaware pose guidance ensures high frame quality temporal smoothness secondly introduce regional loss amplification based pose confidence significantly reduces image distortion lastly generating long smooth videos propose progressive latent fusion strategy means produce videos arbitrary length acceptable resource consumption extensive experiments user studies mimicmotion demonstrates significant improvements previous approaches various aspects detailed results comparisons available project page httpstencentgithubiomimicmotion,-1,0.0,-1,0.0
dragtraffic interactive controllable traffic scene generation autonomous driving evaluating training autonomous driving systems require diverse scalable corner cases however existing scene generation methods lack controllability accuracy versatility resulting unsatisfactory generation results inspired draggan image generation propose dragtraffic generalized interactive controllable traffic scene generation framework based conditional diffusion dragtraffic enables nonexperts generate variety realistic driving scenarios different types traffic agents adaptive mixture expert architecture employ regression model provide general initial solution refinement process based conditional diffusion model ensure diversity usercustomized context introduced crossattention ensure high controllability experiments realworld driving dataset show dragtraffic outperforms existing methods terms authenticity diversity freedom demo videos code available httpschantsssgithubiodragtraffic,11,1.0,11,1.0
fleximo towards flexible texttohuman motion video generation current methods generating human motion videos rely extracting pose sequences reference videos restricts flexibility control additionally due limitations pose detection techniques extracted pose sequences sometimes inaccurate leading lowquality video outputs introduce novel task aimed generating human motion videos solely reference images natural language approach offers greater flexibility ease use text accessible desired guidance videos however training endtoend model task requires millions highquality text human motion video pairs challenging obtain address propose new framework called fleximo leverages largescale pretrained motion models approach straightforward textgenerated skeletons may consistently match scale reference image may lack detailed information overcome challenges introduce anchor point based rescale method design skeleton adapter fill missing details bridge gap texttomotion motiontovideo generation also propose video refinement process enhance video quality large language model llm employed decompose natural language discrete motion sequences enabling generation motion videos desired length assess performance fleximo introduce new benchmark called motionbench includes videos across identities motions also propose new metric motionscore evaluate accuracy motion following qualitative quantitative results demonstrate method outperforms existing textconditioned imagetovideo generation methods code model weights made publicly available,-1,0.0,-1,0.0
radiance field learners uav firstperson viewers firstpersonview fpv holds immense potential revolutionizing trajectory unmanned aerial vehicles uavs offering exhilarating avenue navigating complex building structures yet traditional neural radiance field nerf methods face challenges sampling single points per iteration requiring extensive array views supervision uav videos exacerbate issues limited viewpoints significant spatial scale variations resulting inadequate detail rendering across diverse scales response introduce fpvnerf addressing challenges three key facets temporal consistency leveraging spatiotemporal continuity ensures seamless coherence frames global structure incorporating various global features point sampling preserves space integrity local granularity employing comprehensive framework multiresolution supervision multiscale scene feature representation tackles intricacies uav video spatial scales additionally due scarcity publicly available fpv videos introduce innovative view synthesis method using nerf generate fpv perspectives uav footage enhancing spatial perception drones novel dataset spans diverse trajectories outdoor indoor environments uav domain differing significantly traditional nerf scenarios extensive experiments encompassing interior exterior building structures fpvnerf demonstrates superior understanding uav flying space outperforming stateoftheart methods curated uav dataset explore project page insights httpsfpvnerfgithubio,-1,0.0,-1,0.0
flip flowcentric generative planning generalpurpose manipulation world model aim develop modelbased planning framework world models scaled increasing model data budgets generalpurpose manipulation tasks language vision inputs end present flowcentric generative planning flip modelbased planning algorithm visual space features three key modules multimodal flow generation model generalpurpose action proposal module flowconditioned video generation model dynamics module visionlanguage representation learning model value module given initial image language instruction goal flip progressively search longhorizon flow video plans maximize discounted return accomplish task flip able synthesize longhorizon plans across objects robots tasks image flows general action representation dense flow information also provides rich guidance longhorizon video generation addition synthesized flow video plans guide training lowlevel control policies robot execution experiments diverse benchmarks demonstrate flip improve success rates quality longhorizon video plan synthesis interactive world model property opening wider applications future worksvideo demos website httpsnuslinslabgithubioflipweb,5,0.3355147800097197,5,0.3355147800097197
visatronic multimodal decoderonly model speech synthesis paper propose new task generating speech videos people transcripts vtts motivate new techniques multimodal speech generation task generalizes task generating speech cropped lip videos also complicated task generating generic audio clips eg dog barking videos text multilingual versions task could lead new techniques crosslingual dubbing also present decoderonly multimodal model task call visatronic model embeds vision text speech directly common subspace transformer model uses autoregressive loss learn generative model discretized melspectrograms conditioned speaker videos transcripts speech embedding modalities common subspace visatronic achieve improved results models use text video input presents much simpler approach multimodal speech generation compared prevailing approaches rely lipdetectors complicated architectures fuse modalities producing better results since model flexible enough accommodate different ways ordering inputs sequence carefully explore different strategies better understand best way propagate information generative steps facilitate research vtts release code ii clean transcriptions largescale dataset iii standardized evaluation protocol vtts incorporating objective subjective metrics,-1,0.0,-1,0.0
movie gen swot analysis metas generative ai foundation model transforming media generation advertising entertainment industries generative ai reshaping media landscape enabling unprecedented capabilities video creation personalization scalability paper presents comprehensive swot analysis metas movie gen cuttingedge generative ai foundation model designed produce hd videos synchronized audio simple text prompts explore strengths including highresolution video generation precise editing seamless audio integration make transformative tool across industries filmmaking advertising education however analysis also addresses limitations constraints video length potential biases generated content pose challenges broader adoption addition examine evolving regulatory ethical considerations surrounding generative ai focusing issues like content authenticity cultural representation responsible use comparative insights leading models like dalle google imagen paper highlights movie gens unique features video personalization multimodal synthesis identifying opportunities innovation areas requiring research findings provide actionable insights stakeholders emphasizing opportunities challenges deploying generative ai media production work aims guide future advancements generative ai ensuring scalability quality ethical integrity rapidly evolving field,10,1.0,10,1.0
creating controllable portraits casual monocular videos creating controllable human portraits casual smartphone videos highly desirable due immense value arvr applications recent development gaussian splatting shown improvements rendering quality training efficiency however still remains challenge accurately model disentangle head movements facial expressions singleview capture achieve highquality renderings paper introduce address challenge represent entire scene including dynamic subject using set gaussians canonical space using set control signals head pose expressions transform space learned deformations generate desired rendering key innovation carefully designed deformation method guided learnable prior derived morphable model approach highly efficient training effective controlling facial expressions head positions view synthesis across various captures demonstrate effectiveness learned deformation extensive quantitative qualitative experiments project page found,-1,0.0,-1,0.0
gan skip patch discriminator biological electron microscopy image generation generating realistic electron microscopy em images challenging problem due complex global local structures isola et al proposed conditional generative adversarial network gan general purpose imagetoimage translation fails generate realistic em images propose new architecture discriminator gan providing access multiple patch sizes using skip patches generating realistic em images,17,1.0,17,1.0
generalized grounded temporal reasoning robot instruction following combining large pretrained models consider scenario human cleans table robot observing scene instructed task remove cloth using wiped table instruction following temporal reasoning requires robot identify relevant past object interaction ground object interest present scene execute task according humans instruction directly grounding utterances referencing past interactions grounded objects challenging due multihop nature references past interactions large space object groundings video stream observing robots workspace key insight factor temporal reasoning task estimating video interval associated event reference ii performing spatial reasoning interaction frames infer intended object iii semantically track objects location till current scene enable future robot interactions approach leverages existing large pretrained models possess inherent generalization capabilities combines appropriately temporal grounding tasks evaluation videolanguage corpus acquired robot manipulator displaying rich temporal interactions spatiallycomplex scenes displays average accuracy dataset code videos available httpsreailiitdelhigithubiotemporalreasoninggithubio,5,0.3886133099811755,5,0.3886133099811755
taming rectified flow inversion editing rectifiedflowbased diffusion transformers like flux opensora demonstrated outstanding performance field image video generation despite robust generative capabilities models often struggle inversion inaccuracies could limit effectiveness downstream tasks image video editing address issue propose rfsolver novel trainingfree sampler effectively enhances inversion precision mitigating errors odesolving process rectified flow specifically derive exact formulation rectified flow ode apply highorder taylor expansion estimate nonlinear components significantly enhancing precision ode solutions timestep building upon rfsolver propose rfedit general featuresharingbased framework image video editing incorporating selfattention features inversion process editing process rfedit effectively preserves structural information source image video achieving highquality editing results approach compatible pretrained rectifiedflowbased models image video tasks requiring additional training optimization extensive experiments across generation inversion editing tasks image video modalities demonstrate superiority versatility method source code available,-1,0.0,-1,0.0
motion queries identitymotion tradeoffs texttovideo generation texttovideo diffusion models shown remarkable progress generating coherent video clips textual descriptions however interplay motion structure identity representations models remains underexplored investigate selfattention query features aka q features simultaneously govern motion structure identity examine challenges arising representations interact analysis reveals q affects layout denoising q also strong effect subject identity making hard transfer motion without sideeffect transferring identity understanding dual role enabled us control query feature injection q injection demonstrate two applications zeroshot motion transfer method times efficient existing approaches trainingfree technique consistent multishot video generation characters maintain identity across multiple video shots q injection enhances motion fidelity,9,0.7676269057144607,9,0.7676269057144607
lmagic language model assisted generation images coherence current era generative ai breakthroughs generating panoramic scenes single input image remains key challenge existing methods use diffusionbased iterative simultaneous multiview inpainting however lack global scene layout priors leads subpar outputs duplicated objects eg multiple beds bedroom requires timeconsuming human text inputs view propose lmagic novel method leveraging large language models guidance diffusing multiple coherent views degree panoramic scenes lmagic harnesses pretrained diffusion language models without finetuning ensuring zeroshot performance output quality enhanced superresolution multiview fusion techniques extensive experiments demonstrate resulting panoramic scenes feature better scene layouts perspective view rendering quality compared related works preference human evaluations combined conditional diffusion models lmagic accept various input modalities including limited text depth maps sketches colored scripts applying depth estimation enables point cloud generation dynamic scene exploration fluid camera motion code available httpsgithubcomintellabsmmpano video presentation available,-1,0.0,-1,0.0
facenhance facial expression enhancing recurrent ddpms facial expressions vital nonverbal human communication found applications various computer vision fields like virtual reality gaming emotional ai assistants despite advancements many facial expression generation models encounter challenges low resolution eg pixels poor quality absence background details paper introduce facenhance novel diffusionbased approach addressing constraints existing lowresolution facial expression generation models facenhance enhances lowresolution facial expression videos pixels higher resolutions pixels incorporating background details improving overall quality leveraging conditional denoising within diffusion framework guided backgroundfree lowresolution video single neutral expression highresolution image facenhance generates video incorporating facial expression lowresolution video performed individual background neutral image complementing lightweight lowresolution models facenhance strikes balance computational efficiency desirable image resolution quality extensive experiments mug facial expression database demonstrate efficacy facenhance enhancing lowresolution model outputs stateoftheart quality preserving content identity consistency facenhance represents significant progress towards resourceefficient highfidelity facial expression generation renewing outdated lowresolution methods uptodate standards,6,0.5606411090652511,6,0.5606411090652511
pom efficient image video generation polynomial mixer diffusion models based multihead attention mha become ubiquitous generate high quality images videos however encoding image video sequence patches results costly attention patterns requirements terms memory compute grow quadratically alleviate problem propose dropin replacement mha called polynomial mixer pom benefit encoding entire sequence explicit state pom linear complexity respect number tokens explicit state also allows us generate frames sequential fashion minimizing memory compute requirement still able train parallel show polynomial mixer universal sequencetosequence approximator like regular mha adapt several diffusion transformers dit generating images videos pom replacing mha obtain high quality samples using less computational resources code available httpsgithubcomdavidpicardhomm,-1,0.0,-1,0.0
cpa cameraposeawareness diffusion transformer video generation despite significant advancements made diffusion transformer ditbased methods video generation remains notable gap controllable camera pose perspectives existing works opensora adhere precisely anticipated trajectories physical interactions thereby limiting flexibility downstream applications alleviate issue introduce cpa unified cameraposeawareness texttovideo generation approach elaborates camera movement integrates textual visual spatial conditions specifically deploy sparse motion encoding sme module transform camera pose information spatialtemporal embedding activate temporal attention injection tai module inject motion patches stdit block plugin architecture accommodates original dit parameters facilitating diverse types camera poses flexible object movement extensive qualitative quantitative experiments demonstrate method outperforms ldmbased methods long video generation achieving optimal performance trajectory consistency object consistency,9,0.7676269057144607,9,0.7676269057144607
thisthat languagegesture controlled video generation robot planning propose robot learning method communicating planning executing wide range tasks dubbed thisthat achieve robot planning general tasks leveraging power video generative models trained internetscale data containing rich physical semantic context work tackle three fundamental challenges videobased planning unambiguous task communication simple human instructions controllable video generation respects user intents translating visual planning robot actions propose languagegesture conditioning generate videos simpler clearer existing languageonly methods especially complex uncertain environments suggest behavioral cloning design seamlessly incorporates video plans thisthat demonstrates stateoftheart effectiveness addressing three challenges justifies use video generation intermediate representation generalizable task planning execution project website,5,0.5011948820020167,5,0.5011948820020167
lightvqa video quality assessment model exposure correction visionlanguage guidance recently usergenerated content ugc videos gained popularity daily lives however ugc videos often suffer poor exposure due limitations photographic equipment techniques therefore video exposure correction vec algorithms proposed lowlight video enhancement llve overexposed video recovery oevr included equally important vec video quality assessment vqa unfortunately almost existing vqa models built generally measuring quality video comprehensive perspective result lightvqa trained llveqa proposed assessing llve extend work lightvqa expanding llveqa dataset video exposure correction quality assessment vecqa dataset overexposed videos corresponding corrected versions addition propose lightvqa vqa model specialized assessing vec lightvqa differs lightvqa mainly usage clip model visionlanguage guidance feature extraction followed new module referring human visual system hvs accurate assessment extensive experimental results show model achieves best performance current stateoftheart sota vqa models vecqa dataset public datasets,12,1.0,12,1.0
auroracap efficient performant video detailed captioning new benchmark video detailed captioning key task aims generate comprehensive coherent textual descriptions video content benefiting video understanding generation paper propose auroracap video captioner based large multimodal model follow simplest architecture design without additional parameters temporal modeling address overhead caused lengthy video sequences implement token merging strategy reducing number input visual tokens surprisingly found strategy results little performance loss auroracap shows superior performance various video image captioning benchmarks example obtaining cider beating pro however existing video caption benchmarks include simple descriptions consisting dozen words limits research field therefore develop vdc video detailed captioning benchmark one thousand carefully annotated structured captions addition propose new llmassisted metric vdcscore bettering evaluation adopts divideandconquer strategy transform long caption evaluation multiple short questionanswer pairs help human elo ranking experiments show benchmark better correlates human judgments video detailed captioning quality,0,0.9174347293360907,0,0.9174347293360907
safewatch efficient safetypolicy following video guardrail model transparent explanations rise generative ai rapid growth highquality video generation video guardrails become crucial ever ensure safety security across platforms current video guardrails however either overly simplistic relying pure classification models trained simple policies limited unsafe categories lack detailed explanations prompting multimodal large language models mllms long safety guidelines inefficient impractical guardrailing realworld content bridge gap propose safewatch efficient mllmbased video guardrail model designed follow customized safety policies provide multilabel video guardrail outputs contentspecific explanations zeroshot manner particular unlike traditional mllmbased guardrails encode safety policies autoregressively causing inefficiency bias safewatch uniquely encodes policy chunk parallel eliminates position bias policies attended simultaneously equal importance addition improve efficiency accuracy safewatch incorporates policyaware visual token pruning algorithm adaptively selects relevant video tokens policy discarding noisy irrelevant information allows focused policycompliant guardrail significantly reduced computational overhead considering limitations existing video guardrail benchmarks propose safewatchbench largescale video guardrail benchmark comprising videos spanning six safety categories covers tasks ensure comprehensive coverage potential safety scenarios safewatch outperforms sota safewatchbench benchmarks cuts costs delivers toptier explanations validated llm human reviews,-1,0.0,-1,0.0
video anomaly detection motion appearance guided patch diffusion model recent endeavor one class video anomaly detection leverage diffusion models posit task generation problem diffusion model trained recover normal patterns exclusively thus reporting abnormal patterns outliers yet existing attempts neglect various formations anomaly predict normal samples feature level regardless abnormal objects surveillance videos often relatively small address novel patchbased diffusion model proposed specifically engineered capture finegrained local information observe anomalies videos manifest deviations appearance motion therefore argue comprehensive solution must consider aspects simultaneously achieve accurate frame prediction address introduce innovative motion appearance conditions seamlessly integrated patch diffusion model conditions designed guide model generating coherent contextually appropriate predictions semantic content motion relations experimental results four challenging video anomaly detection datasets empirically substantiate efficacy proposed approach demonstrating consistently outperforms existing methods detecting abnormal behaviors,-1,0.0,-1,0.0
generation hybrid priors due fascinating generative performance texttoimage diffusion models growing generation works explore distilling generative priors using score distillation sampling sds loss bypass data scarcity problem existing methods achieved promising results realism consistency generation still faces challenges including lack realism insufficient dynamic motions paper propose novel method generation ensures dynamic amplitude authenticity direct supervision provided video prior specifically adopt texttovideo diffusion model generate reference video divide generation two stages static generation dynamic generation static generation achieved guidance input text first frame reference video dynamic generation stage introduce customized sds loss ensure multiview consistency videobased sds loss improve temporal consistency importantly direct priors reference video ensure quality geometry texture moreover design priorswitching training strategy avoid conflicts different priors fully leverage benefits prior addition enrich generated motion introduce dynamic modeling representation composed deformation network topology network ensures dynamic continuity modeling topological changes method supports generation also enables generation monocular videos comparison experiments demonstrate superiority method compared existing methods,2,0.6962709329283329,2,0.6962709329283329
autovfx physically realistic video editing natural language instructions modern visual effects vfx software made possible skilled artists create imagery virtually anything however creation process remains laborious complex largely inaccessible everyday users work present autovfx framework automatically creates realistic dynamic vfx videos single video natural language instructions carefully integrating neural scene modeling llmbased code generation physical simulation autovfx able provide physicallygrounded photorealistic editing effects controlled directly using natural language instructions conduct extensive experiments validate autovfxs efficacy across diverse spectrum videos instructions quantitative qualitative results suggest autovfx outperforms competing methods large margin generative quality instruction alignment editing versatility physical plausibility,-1,0.0,-1,0.0
storyagent customized storytelling video generation via multiagent collaboration advent aigenerated content aigc spurred research automated video generation streamline conventional processes however automating storytelling video production particularly customized narratives remains challenging due complexity maintaining subject consistency across shots existing approaches like mora aesopagent integrate multiple agents storytovideo generation fall short preserving protagonist consistency supporting customized storytelling video generation csvg address limitations propose storyagent multiagent framework designed csvg storyagent decomposes csvg distinct subtasks assigned specialized agents mirroring professional production process notably framework includes agents story design storyboard generation video creation agent coordination result evaluation leveraging strengths different models storyagent enhances control generation process significantly improving character consistency specifically introduce customized imagetovideo method lorabe enhance intrashot temporal consistency novel storyboard generation pipeline proposed maintain subject consistency across shots extensive experiments demonstrate effectiveness approach synthesizing highly consistent storytelling videos outperforming stateoftheart methods contributions include introduction storyagent versatile framework video generation tasks novel techniques preserving protagonist consistency,15,1.0,15,1.0
anisora exploring frontiers animation video generation sora era animation gained significant interest recent film tv industry despite success advanced video generation models like sora kling cogvideox generating natural videos lack effectiveness handling animation videos evaluating animation video generation also great challenge due unique artist styles violating laws physics exaggerated motions paper present comprehensive system anisora designed animation video generation includes data processing pipeline controllable generation model evaluation dataset supported data processing pipeline highquality data generation model incorporates spatiotemporal mask module facilitate key animation production functions imagetovideo generation frame interpolation localized imageguided animation also collect evaluation benchmark various animation videos evaluation vbench human doubleblind test demonstrates consistency character motion achieving stateoftheart results animation video generation evaluation benchmark publicly available httpsgithubcombilibiliindexanisora,-1,0.0,-1,0.0
longduration highresolution audiodriven portrait image animation recent advances latent diffusionbased generative models portrait image animation hallo achieved impressive results shortduration video synthesis paper present updates hallo introducing several design enhancements extend capabilities first extend method produce longduration videos address substantial challenges appearance drift temporal artifacts investigate augmentation strategies within image space conditional motion frames specifically introduce patchdrop technique augmented gaussian noise enhance visual consistency temporal coherence long duration second achieve resolution portrait video generation accomplish implement vector quantization latent codes apply temporal alignment techniques maintain coherence across temporal dimension integrating highquality decoder realize visual synthesis resolution third incorporate adjustable semantic textual labels portrait expressions conditional inputs extends beyond traditional audio cues improve controllability increase diversity generated content best knowledge proposed paper first method achieve resolution generate hourlong audiodriven portrait image animations enhanced textual prompts conducted extensive experiments evaluate method publicly available datasets including hdtf celebv introduced wild dataset experimental results demonstrate approach achieves stateoftheart performance longduration portrait video animation successfully generating rich controllable content resolution duration extending tens minutes project page,-1,0.0,-1,0.0
unified framework intra interframe video compression video compression aims reconstruct seamless frames encoding motion residual information existing frames previous neural video compression methods necessitate distinct codecs three types frames iframe pframe bframe hinders unified approach generalization across different video contexts intracodec techniques lack advanced motion estimation motion compensation memc found intercodec leading fragmented frameworks lacking uniformity proposed intra interframe video compression framework employs single spatiotemporal codec guides feature compression rates according content importance unified codec transforms dependence across frames conditional coding scheme thus integrating intra interframe compression one cohesive strategy given absence explicit motion data achieving competent interframe compression conditional codec poses challenge resolve approach includes implicit interframe alignment mechanism pretrained diffusion denoising process utilization diffusioninverted reference feature rather random noise supports initial compression state process allows selective denoising motionrich regions based decoded features facilitating accurate alignment without need memc experimental findings across various compression configurations ai ld ra frame types prove outperforms stateoftheart perceptual learned codecs impressively exhibits enhancement perceptual reconstruction performance benchmarked standard vtm official implementation found,2,1.0,2,1.0
dtsgan learning dynamic textures via spatiotemporal generative adversarial network dynamic texture synthesis aims generate sequences visually similar reference video texture exhibit specific stationary properties time paper introduce spatiotemporal generative adversarial network dtsgan learn single dynamic texture capturing motion content distribution pipeline dtsgan new video sequence generated coarsest scale finest one avoid mode collapse propose novel strategy data updates helps improve diversity generated results qualitative quantitative experiments show model able generate high quality dynamic textures natural motion,-1,0.0,-1,0.0
accelerated imageaware generative diffusion modeling propose paper analytically new construct diffusion model whose drift diffusion parameters yield exponentially timedecaying signal noise ratio forward process reverse construct cleverly carries learning diffusion coefficients structure clean images using autoencoder proposed methodology significantly accelerates diffusion process reducing required diffusion time steps around seen conventional models without compromising image quality reversetime diffusion departure conventional models typically use timeconsuming multiple runs introduce parallel datadriven model generate reversetime diffusion trajectory single run model resulting collective blocksequential generative model eliminates need mcmcbased subsampling correction safeguarding improving image quality improve acceleration image generation collectively advancements yield generative model order magnitude faster conventional approaches maintaining high fidelity diversity generated images hence promising widespread applicability rapid image synthesis tasks,13,1.0,13,1.0
literature review fetus brain motion correction mri paper provides comprehensive review latest advancements fetal motion correction mri delve various contemporary methodologies technological advancements aimed overcoming challenges includes traditional fetal mri correction methods like slice volume registration svr deep learningbased techniques convolutional neural networks cnns long shortterm memory lstm networks transformers generative adversarial networks gans recent advancements diffusion models insights derived literature review reflect thorough understanding technical intricacies practical implications fetal motion mri studies offering reasoned perspective potential solutions future improvements field,-1,0.0,-1,0.0
disrupting style mimicry attacks video imagery generative ai models often used perform mimicry attacks pretrained model finetuned small sample images learn mimic specific artist interest researchers introduced multiple antimimicry protection tools mist glaze antidreambooth recent evidence points growing trend mimicry models using videos sources training data paper presents experiences exploring techniques disrupt style mimicry video imagery first validate mimicry attacks succeed training individual frames extracted videos show antimimicry tools offer protection applied individual frames approach vulnerable adaptive countermeasure removes protection exploiting randomness optimization results consecutive nearlyidentical frames develop new toolagnostic framework segments videos short scenes based framelevel similarity use perscene optimization baseline remove interframe randomization reducing computational cost show via image level metrics endtoend user study resulting protection restores protection mimicry including countermeasure finally develop another adaptive countermeasure find falls short framework,4,1.0,4,1.0
global motion understanding largescale video object segmentation paper show transferring knowledge domains video understanding combined largescale learning improve robustness video object segmentation vos complex circumstances namely focus integrating scene global motion knowledge improve largescale semisupervised video object segmentation prior works vos mostly rely direct comparison semantic contextual features perform dense matching current past frames passing actual motion structure hand optical flow estimation task aims approximate scene motion field exposing global motion patterns typically undiscoverable pairs similarity search present warpformer architecture semisupervised video object segmentation exploits existing knowledge motion understanding conduct smoother propagation accurate matching framework employs generic pretrained optical flow estimation network whose prediction used warp past frames instance segmentation masks current frame domain consequently warped segmentation masks refined fused together aiming inpaint occluded regions eliminate artifacts caused flow field imperfects additionally employ novel largescale mose dataset train model various complex scenarios method demonstrates strong performance davis validation davis testdev youtubevos validation competitive alternative stateoftheart methods using much simpler memory mechanism instance understanding logic,-1,0.0,-1,0.0
comprehensive benchmark compositional texttovideo generation texttovideo generative models advanced significantly yet ability compose different objects attributes actions motions video remains unexplored previous texttovideo benchmarks also neglect important ability evaluation work conduct first systematic study compositional texttovideo generation propose first benchmark tailored compositional texttovideo generation encompasses diverse aspects compositionality including consistent attribute binding dynamic attribute binding spatial relationships motion binding action binding object interactions generative numeracy carefully design evaluation metrics multimodal large language model mllmbased detectionbased trackingbased metrics better reflect compositional texttovideo generation quality seven proposed categories text prompts effectiveness proposed metrics verified correlation human evaluations also benchmark various texttovideo generative models conduct indepth analysis across different models various compositional categories find compositional texttovideo generation highly challenging current models hope attempt could shed light future research direction,-1,0.0,-1,0.0
synthesizing audio silent video using sequence sequence modeling generating audio videos visual context multiple practical applications improving interact audiovisual media example enhancing cctv footage analysis restoring historical videos eg silent movies improving video generation models propose novel method generate audio video using sequencetosequence model improving prior work used cnns wavenet faced sound diversity generalization challenges approach employs vector quantized variational autoencoder vqvae capture videos spatial temporal structures decoding custom audio decoder broader range sounds trained dataset segment focusing specific domains model aims enhance applications like cctv footage analysis silent movie restoration video generation models,8,0.4972001295940391,8,0.4972001295940391
method software tool generating artificial databases biomedical images based deep neural networks wide variety biomedical image data well methods generating training images using basic deep neural networks analyzed additionally platforms creating images analyzed considering characteristics article develops method generating artificial biomedical images based gan gan architecture developed biomedical image synthesis data foundation module generating training images designed implemented software system comparison generated image database known databases made,-1,0.0,-1,0.0
gvdiff grounded texttovideo generation diffusion models texttovideo generation significant attention directed toward development yet unifying discrete continuous grounding conditions generation remains underexplored paper proposes grounded texttovideo generation framework termed gvdiff first inject grounding condition selfattention uncertaintybased representation explicitly guide focus network second introduce spatialtemporal grounding layer connects grounding condition target objects enables model grounded generation capacity spatialtemporal domain third dynamic gate network adaptively skips redundant grounding process selectively extract grounding information semantics improving efficiency extensively evaluate grounded generation capacity gvdiff demonstrate versatility applications including longrange video generation sequential prompts objectspecific editing,-1,0.0,-1,0.0
audiosynchronized visual animation current visual generation methods produce high quality videos guided texts however effectively controlling object dynamics remains challenge work explores audio cue generate temporally synchronized image animations introduce audio synchronized visual animation asva task animating static image demonstrate motion dynamics temporally guided audio clips across multiple classes end present dataset curated vggsound videos featuring synchronized audio visual events across categories also present diffusion model avsyncd capable generating dynamic animations guided audios extensive evaluations validate reliable benchmark synchronized generation demonstrate models superior performance explore avsyncds potential variety audio synchronized generation tasks generating full videos without base image controlling object motions various sounds hope established benchmark open new avenues controllable visual generation videos project webpage httpslzhangbjgithubioprojectsasvaasvahtml,-1,0.0,-1,0.0
discrete continuous generating smooth transition poses sign language observation generating continuous sign language videos discrete segments challenging due need smooth transitions preserve natural flow meaning traditional approaches simply concatenate isolated signs often result abrupt transitions disrupting video coherence address propose novel framework employs conditional diffusion model synthesize contextually smooth transition frames enabling seamless construction continuous sign language sequences approach transforms unsupervised problem transition frame generation supervised training task simulating absence transition frames random masking segments longduration sign videos model learns predict masked frames denoising gaussian noise conditioned surrounding sign observations allowing handle complex unstructured transitions inference apply linearly interpolating padding strategy initializes missing frames interpolation boundary frames providing stable foundation iterative refinement diffusion model extensive experiments datasets demonstrate effectiveness method producing continuous natural sign language videos,9,0.8125780980350269,9,0.8125780980350269
dual encoder gan inversion highfidelity head reconstruction single images gan inversion aims project single image latent space generative adversarial network gan thereby achieving geometry reconstruction exist encoders achieve good results gan inversion predominantly built specializes synthesizing nearfrontal views limiting synthesizing comprehensive scenes diverse viewpoints contrast existing approaches propose novel framework built panohead excels synthesizing images perspective achieve realistic modeling input image introduce dual encoder system tailored highfidelity reconstruction realistic generation different viewpoints accompanying propose stitching framework triplane domain get best predictions achieve seamless stitching encoders must output consistent results despite specialized different tasks reason carefully train encoders using specialized losses including adversarial loss based novel occlusionaware triplane discriminator experiments reveal approach surpasses existing encoder training methods qualitatively quantitatively please visit project page,1,1.0,1,1.0
stair spatialtemporal reasoning auditable intermediate results video question answering recently witnessed rapid development video question answering models however models handle simple videos terms temporal reasoning performance tends drop answering temporalreasoning questions long informative videos tackle problem propose stair spatialtemporal reasoning model auditable intermediate results video question answering stair neural module network contains program generator decompose given question hierarchical combination several subtasks set lightweight neural modules complete subtasks though neural module networks already widely studied imagetext tasks applying videos nontrivial task reasoning videos requires different abilities paper define set basic videotext subtasks video question answering design set lightweight modules complete different prior works modules stair return intermediate outputs specific intentions instead always returning attention maps makes easier interpret collaborate pretrained models also introduce intermediate supervision make intermediate outputs accurate conduct extensive experiments several video question answering datasets various settings show stairs performance explainability compatibility pretrained models applicability program annotations available code httpsgithubcomyellowbinarytreestair,0,1.0,0,1.0
nerv enhanced implicit neural video representation neural fields also known implicit neural representations inrs shown remarkable capability representing generating manipulating various data types allowing continuous data reconstruction low memory footprint though promising inrs applied video compression still need improve ratedistortion performance large margin require huge number parameters long training iterations capture highfrequency details limiting wider applicability resolving problem remains quite challenging task would make inrs accessible compression tasks take step towards resolving shortcomings introducing neural representations videos nerv enhanced implicit neural video representation straightforward yet effective enhancement original nerv decoder architecture featuring separable residual blocks scrbs sandwiches upsampling block ub bilinear interpolation skip layer improved feature representation nerv allows videos directly represented function approximated neural network significantly enhance representation capacity beyond current inrbased video codecs evaluate method uvg mcl jvc bunny datasets achieving competitive results video compression inrs achievement narrows gap autoencoderbased video coding marking significant stride inrbased video compression research,2,0.8365512887725767,2,0.8365512887725767
vqnerv vector quantized neural representation videos implicit neural representations inr excel encoding videos within neural networks showcasing promise computer vision tasks like video compression denoising inrbased approaches reconstruct video frames contentagnostic embeddings hampers efficacy video frame regression restricts generalization ability video interpolation address deficiencies hybrid neural representation videos hnerv introduced contentadaptive embeddings nevertheless hnervs compression ratios remain relatively low attributable oversight leveraging networks shallow features interframe residual information work introduce advanced ushaped architecture vector quantizednerv vqnerv integrates novel componentthe vqnerv block block incorporates codebook mechanism discretize networks shallow residual features interframe residual information effectively approach proves particularly advantageous video compression results smaller size compared quantized features furthermore introduce original codebook optimization technique termed shallow codebook optimization designed refine utility efficiency codebook experimental evaluations indicate vqnerv outperforms hnerv video regression tasks delivering superior reconstruction quality increase db peak signaltonoise ratio psnr better bit per pixel bpp efficiency improved video inpainting outcomes,2,1.0,2,1.0
versatile deep visualaudio watermarking manipulation localization copyright protection aigenerated video revolutionized short video production filmmaking personalized media making video local editing essential tool however progress also blurs line reality fiction posing challenges multimedia forensics solve urgent issue proposed address limitations current video tampering forensics poor generalizability singular function single modality focus combining fragility videointovideo steganography deep robust watermarking method embed invisible visualaudio localization watermarks copyright watermarks original video frames audio enabling precise manipulation localization copyright protection also design temporal alignment fusion module degradation prompt learning enhance localization accuracy decoding robustness meanwhile introduce samplelevel audio localization method crossmodal copyright extraction mechanism couple information audio video frames effectiveness verified visualaudio tampering dataset emphasizing superiority localization precision copyright accuracy crucial sustainable development video editing aigc video era,4,1.0,4,1.0
mllm video narrator mitigating modality imbalance video moment retrieval video moment retrieval vmr aims localize specific temporal segment within untrimmed long video given natural language query existing methods often suffer inadequate training annotations ie sentence typically matches fraction prominent video content foreground limited wording diversity intrinsic modality imbalance leaves considerable portion visual information remaining unaligned text confines crossmodal alignment knowledge within scope limited text corpus thereby leading suboptimal visualtextual modeling poor generalizability leveraging visualtextual understanding capability multimodal large language models mllm work take mllm video narrator generate plausible textual descriptions video thereby mitigating modality imbalance boosting temporal localization effectively maintain temporal sensibility localization design get text narratives certain video timestamp construct structured text paragraph time information temporally aligned visual content perform crossmodal feature merging temporalaware narratives corresponding video temporal features produce semanticenhanced video representation sequences query localization subsequently introduce unimodal narrativequery matching mechanism encourages model extract complementary information contextual cohesive descriptions improved retrieval extensive experiments two benchmarks show effectiveness generalizability proposed method,0,1.0,0,1.0
cuboidnet multibranch convolutional neural network joint spacetime video super resolution demand highresolution videos consistently rising across various domains propelled continuous advancements science technology societal nonetheless challenges arising limitations imaging equipment capabilities imaging conditions well economic temporal factors often result obtaining lowresolution images particular situations spacetime video superresolution aims enhance spatial temporal resolutions lowresolution lowframerate videos currently available spacetime video superresolution methods often fail fully exploit abundant information existing within spatiotemporal domain address problem tackle issue conceptualizing input lowresolution video cuboid structure drawing perspective introduce innovative methodology called cuboidnet incorporates multibranch convolutional neural network cuboidnet designed collectively enhance spatial temporal resolutions videos enabling extraction rich meaningful information across spatial temporal dimensions specifically take input video cuboid generate different directional slices input different branches network proposed network contains four modules ie multibranchbased hybrid feature extraction mbfe module multibranchbased reconstruction mbr module first stage quality enhancement qe module second stage cross frame quality enhancement cfqe module interpolated frames experimental results demonstrate proposed method effective spatial temporal superresolution video also spatial angular superresolution light field,-1,0.0,-1,0.0
videollamb longcontext video understanding recurrent memory bridges recent advancements largescale videolanguage models shown significant potential realtime planning detailed interactions however high computational demands scarcity annotated datasets limit practicality academic researchers work introduce videollamb novel framework utilizes temporal memory tokens within bridge layers allow encoding entire video sequences alongside historical visual data effectively preserving semantic continuity enhancing model performance across various tasks approach includes recurrent memory tokens scenetilling algorithm segments videos independent semantic units preserve semantic integrity empirically videollamb significantly outstrips existing videolanguage models demonstrating points improvement competitors across three videoqa benchmarks points egocentric planning comprehensive results mvbench show achieves markedly better results previous models llm remarkably maintains robust performance pllava even video length increases times besides frame retrieval results specialized needle video haystack niavh benchmark validate videollambs prowess accurately identifying specific frames within lengthy videos scenetilling algorithm also enables generation streaming video captions directly without necessitating additional training terms efficiency videollamb trained frames supports frames single nvidia gpu linear gpu memory scaling ensuring high performance costeffectiveness thereby setting new foundation longform videolanguage models academic practical applications,-1,0.0,-1,0.0
lowcomputational video synopsis framework standard dataset video synopsis efficient method condensing surveillance videos technique begins detection tracking objects followed creation object tubes tubes consist sequences containing chronologically ordered bounding boxes unique object generate condensed video first step involves rearranging object tubes maximize number nonoverlapping objects frame tubes stitched background image extracted source video lack standard dataset video synopsis task hinders comparison different video synopsis models paper addresses issue introducing standard dataset called synoclip designed specifically video synopsis task synoclip includes necessary features needed evaluate various models directly effectively additionally work introduces video synopsis model called fgs low computational cost model includes emptyframe object detector identify frames empty objects facilitating efficient utilization deep object detector moreover tube grouping algorithm proposed maintain relationships among tubes synthesized video followed greedy tube rearrangement algorithm efficiently determines start time tube finally proposed model evaluated using proposed dataset source code finetuned object detection model tutorials available httpsgithubcomramtinmavideosynopsisfgs,-1,0.0,-1,0.0
deepfake detection videos multiple faces using geometricfakeness features due development facial manipulation techniques recent years deepfake detection video stream became important problem face biometrics brand monitoring online video conferencing solutions case biometric authentication replace real datastream deepfake bypass liveness detection system using deepfake video conference penetrate private meeting deepfakes victims public figures also used fraudsters blackmailing extorsion financial fraud therefore task detecting deepfakes relevant ensuring privacy security existing approaches deepfake detection performance deteriorates multiple faces present video simultaneously objects erroneously classified faces research propose use geometricfakeness features gff characterize dynamic degree face presence video perframe deepfake scores analyze temporal inconsistencies gffs frames train complex deep learning model outputs final deepfake prediction employ approach analyze videos multiple faces simultaneously present video videos often occur practice eg online video conference case real faces appearing frame together deepfake face significantly affect deepfake detection approach allows counter problem extensive experiments demonstrate approach outperforms current stateoftheart methods popular benchmark datasets faceforensics dfdc celebdf wilddeepfake proposed approach remains accurate trained detect multiple different deepfake generation techniques,4,1.0,4,1.0
free videollm promptguided visual perception efficient trainingfree video llms visionlanguage large models achieved remarkable success various multimodal tasks yet applying video understanding remains challenging due inherent complexity computational demands video data trainingbased videollms deliver high performance often require substantial resources training inference conversely trainingfree approaches offer efficient alternative adapting pretrained imagellms models video tasks without additional training face inference efficiency bottlenecks due large number visual tokens generated video frames work present novel promptguided visual perception framework abbreviated free videollm efficient inference trainingfree video llms proposed framework decouples spatialtemporal dimension performs temporal frame sampling spatial roi cropping respectively based taskspecific prompts method effectively reduces number visual tokens maintaining high performance across multiple video questionanswering benchmarks extensive experiments demonstrate approach achieves competitive results significantly fewer tokens offering optimal tradeoff accuracy computational efficiency compared stateoftheart video llms code available httpsgithubcomcontrastivefreevideollm,-1,0.0,-1,0.0
videorag visuallyaligned retrievalaugmented long video comprehension existing large videolanguage models lvlms struggle comprehend long videos correctly due limited context address problem finetuning longcontext lvlms employing gptbased agents emerged promising solutions however finetuning lvlms would require extensive highquality data substantial gpu resources gptbased agents would rely proprietary models eg paper propose video retrievalaugmented generation videorag trainingfree costeffective pipeline employs visuallyaligned auxiliary texts help facilitate crossmodality alignment providing additional information beyond visual content specifically leverage opensource external tools extract visuallyaligned information pure video data eg audio optical character object detection incorporate extracted information existing lvlm auxiliary texts alongside video frames queries plugandplay manner videorag offers several key advantages lightweight low computing overhead due singleturn retrieval ii easy implementation compatibility lvlm iii significant consistent performance gains across long video understanding benchmarks including videomme mlvu longvideobench notably model demonstrates superior performance proprietary models like utilized model,0,0.8330323834187976,0,0.8330323834187976
neptune long orbit benchmarking long video understanding introduce neptune benchmark long video understanding requires reasoning long time horizons across different modalities many existing video datasets models focused short clips long video datasets exist often solved powerful image models applied per frame often frames video usually manually annotated high cost order mitigate problems propose scalable dataset creation pipeline leverages large models vlms llms automatically generate dense timealigned video captions well tough question answer decoy sets video segments minutes length dataset neptune covers broad range long video reasoning abilities consists subset emphasizes multimodal reasoning since existing metrics openended question answering either rulebased may rely proprietary models provide new open source modelbased metric gem score openended responses neptune benchmark evaluations reveal current opensource long video models perform poorly neptune particularly questions testing temporal ordering counting state changes neptune aim spur development advanced models capable understanding long videos dataset available httpsgithubcomgoogledeepmindneptune,0,0.8993598327368896,0,0.8993598327368896
sampleefficient unsupervised policy cloning ensemble selfsupervised labeled videos current advanced policy learning methodologies demonstrated ability develop expertlevel strategies provided enough information however requirements including taskspecific rewards expertlabeled trajectories huge environmental interactions expensive even unavailable many scenarios contrast humans efficiently acquire skills within trials errors imitating easily accessible internet video absence supervision paper try let machines replicate efficient watchingandlearning process unsupervised policy ensemble selfsupervised labeled videos upesv novel framework efficiently learn policies videos without expert supervision upesv trains video labeling model infer expert actions expert videos several organically combined selfsupervised tasks task performs duties together enable model make full use expert videos rewardfree interactions advanced dynamics understanding robust prediction simultaneously upesv clones policy labeled expert videos turn collecting environmental interactions selfsupervised tasks sampleefficient unsupervised ie rewardfree training process advanced videoimitated policy obtained extensive experiments sixteen challenging procedurallygenerated environments demonstrate proposed upesv achieves stateoftheart fewshot policy learning outperforming five current advanced baselines tasks without exposure supervision except videos detailed analysis also provided verifying necessity selfsupervised task employed upesv,5,0.29641232130506584,5,0.29641232130506584
onthefly training gaussians efficient streaming photorealistic freeviewpoint videos constructing photorealistic freeviewpoint videos fvvs dynamic scenes multiview videos remains challenging endeavor despite remarkable advancements achieved current neural rendering techniques methods generally require complete video sequences offline training capable realtime rendering address constraints introduce method designed efficient fvv streaming realworld dynamic scenes method achieves fast onthefly perframe reconstruction within seconds realtime rendering fps specifically utilize gaussians represent scene instead naive approach directly optimizing perframe employ compact neural transformation cache ntc model translations rotations markedly reducing training time storage required fvv frame furthermore propose adaptive addition strategy handle emerging objects dynamic scenes experiments demonstrate achieves competitive performance terms rendering speed image quality training time model storage compared stateoftheart methods,1,0.8954936027957391,1,0.8954936027957391
videogenofthought stepbystep generating multishot video minimal manual intervention current video generation models excel short clips fail produce cohesive multishot narratives due disjointed visual dynamics fractured storylines existing solutions either rely extensive manual scriptingediting prioritize singleshot fidelity crossscene continuity limiting practicality movielike content introduce videogenofthought vgot stepbystep framework automates multishot video synthesis single sentence systematically addressing three core challenges narrative fragmentation existing methods lack structured storytelling propose dynamic storyline modeling first converts user prompt concise shot descriptions elaborates detailed cinematic specifications across five domains character dynamics background continuity relationship evolution camera movements hdr lighting ensuring logical narrative progression selfvalidation visual inconsistency existing approaches struggle maintaining visual consistency across shots identityaware crossshot propagation generates identitypreserving portrait ipp tokens maintain character fidelity allowing trait variations expressions aging dictated storyline transition artifacts abrupt shot changes disrupt immersion adjacent latent transition mechanisms implement boundaryaware reset strategies process adjacent shots features transition points enabling seamless visual flow preserving narrative continuity vgot generates multishot videos outperform stateoftheart baselines withinshot face consistency style consistency achieving better crossshot consistency fewer manual adjustments alternatives,-1,0.0,-1,0.0
motionbank largescale video motion benchmark disentangled rulebased annotations paper tackle problem build benchmark large motion model lmm ultimate goal lmm serve foundation model versatile motionrelated tasks eg human motion generation interpretability generalizability though advanced recent lmmrelated works still limited smallscale motion data costly text descriptions besides previous motion benchmarks primarily focus pure body movements neglecting ubiquitous motions context ie humans interacting humans objects scenes address limitations consolidate largescale video action datasets knowledge banks build motionbank comprises video action datasets motion sequences frames natural diverse human motions different laboratorycaptured motions inthewild humancentric videos contain abundant motions context facilitate better motion text alignment also meticulously devise motion caption generation algorithm automatically produce rulebased unbiased disentangled text descriptions via kinematic characteristics motion extensive experiments show motionbank beneficial general motionrelated tasks human motion generation motion incontext generation motion understanding video motions together rulebased text annotations could serve efficient alternative larger lmms dataset codes benchmark publicly available httpsgithubcomliangxuymotionbank,0,1.0,0,1.0
pemfvto pointenhanced video virtual tryon via maskfree paradigm video virtual tryon aims seamlessly transfer reference garment onto target person video preserving visual fidelity temporal coherence existing methods typically rely inpainting masks define tryon area enabling accurate garment transfer simple scenes eg inshop videos however maskbased approaches struggle complex realworld scenarios overly large inconsistent masks often destroy spatialtemporal information leading distorted results maskfree methods alleviate issue face challenges accurately determining tryon area especially videos dynamic body movements address limitations propose pemfvto novel pointenhanced maskfree video virtual tryon framework leverages sparse point alignments explicitly guide garment transfer key innovation introduction pointenhanced guidance provides flexible reliable control spatiallevel garment transfer temporallevel video coherence specifically design pointenhanced transformer pet two core components pointenhanced spatial attention psa uses framecloth point alignments precisely guide garment transfer pointenhanced temporal attention pta leverages frameframe point correspondences enhance temporal coherence ensure smooth transitions across frames extensive experiments demonstrate pemfvto outperforms stateoftheart methods generating natural coherent visually appealing tryon videos particularly challenging inthewild scenarios link papers homepage httpspemfvtogithubio,11,1.0,11,1.0
contextualized diffusion models textguided image video generation conditional diffusion models exhibited superior performance highfidelity textguided visual generation editing nevertheless prevailing textguided visual diffusion models primarily focus incorporating textvisual relationships exclusively reverse process often disregarding relevance forward process inconsistency forward reverse processes may limit precise conveyance textual semantics visual synthesis results address issue propose novel general contextualized diffusion model contextdiff incorporating crossmodal context encompassing interactions alignments text condition visual sample forward reverse processes propagate context timesteps two processes adapt trajectories thereby facilitating crossmodal conditional modeling generalize contextualized diffusion ddpms ddims theoretical derivations demonstrate effectiveness model evaluations two challenging tasks texttoimage generation texttovideo editing task contextdiff achieves new stateoftheart performance significantly enhancing semantic alignment text condition generated samples evidenced quantitative qualitative evaluations code available,-1,0.0,-1,0.0
fast physicsdriven content generation single image content generation focuses creating dynamic objects change time existing methods primarily rely pretrained video diffusion models utilizing sampling processes reference videos however approaches face significant challenges firstly generated content often fails adhere realworld physics since video diffusion models incorporate physical priors secondly extensive sampling process large number parameters diffusion models result exceedingly timeconsuming generation processes address issues introduce novel fast physicsdriven method controllable content generation single image integrates physical simulation directly generation process ensuring resulting content adheres natural physical laws also eliminates use diffusion models dynamics generation phase significantly speeding process allows control dynamics including movement speed direction manipulating external forces extensive experiments demonstrate generates highfidelity content significantly reduced inference times achieving stateoftheart performance code generated content available provided link,-1,0.0,-1,0.0
dynamic content generation via score composition video multiview diffusion models recent advancements generation predominantly propelled improvements image diffusion models models pretrained internetscale image data finetuned massive data offering capability producing highly consistent multiview images however due scarcity synchronized multiview video data remains challenging adapt paradigm generation directly despite available video data adequate training video multiview diffusion models separately provide satisfactory dynamic geometric priors respectively take advantage paper presents novel framework dynamic content creation reconciles knowledge geometric consistency temporal smoothness models directly sample dense multiview multiframe images employed optimize continuous representation specifically design simple yet effective denoising strategy via score composition pretrained video multiview diffusion models based probability structure target image array alleviate potential conflicts two heterogeneous scores introduce variancereducing sampling via interpolated steps facilitating smooth stable generation owing high parallelism proposed image generation process efficiency modern reconstruction pipeline framework generate content within minutes notably method circumvents reliance expensive hardtoscale data thereby potential benefit scaling foundation video multiview diffusion models extensive experiments demonstrate efficacy proposed framework generating highly seamless consistent assets various types conditions,-1,0.0,-1,0.0
spatialtemporal anchored generative gaussians recent progress pretrained diffusion models generation spurred interest content creation however achieving highfidelity generation spatialtemporal consistency remains challenge work propose novel framework combines pretrained diffusion models dynamic gaussian splatting highfidelity generation drawing inspiration generation techniques utilize multiview diffusion model initialize multiview images anchoring input video frames video either realworld captured generated video diffusion model ensure temporal consistency multiview sequence initialization introduce simple yet effective fusion strategy leverage first frame temporal anchor selfattention computation almost consistent multiview sequences apply score distillation sampling optimize gaussian point cloud gaussian spatting specially crafted generation task adaptive densification strategy proposed mitigate unstable gaussian gradient robust optimization notably proposed pipeline require pretraining finetuning diffusion networks offering accessible practical solution generation task extensive experiments demonstrate method outperforms prior generation works rendering quality spatialtemporal consistency generation robustness setting new stateoftheart generation diverse inputs including text image video,2,0.6868764847684388,2,0.6868764847684388
stylemaster stylize video artistic generation translation style control popular video generation models existing methods often generate videos far given style cause content leakage struggle transfer one video desired style first observation style extraction stage matters whereas existing methods emphasize global style ignore local textures order bring texture features preventing content leakage filter contentrelated patches retaining style ones based promptpatch similarity global style extraction generate paired style dataset model illusion facilitate contrastive learning greatly enhances absolute style consistency moreover fill imagetovideo gap train lightweight motion adapter still videos implicitly enhances stylization extent enables imagetrained model seamlessly applied videos benefited efforts approach stylemaster achieves significant improvement style resemblance temporal coherence also easily generalize video style transfer gray tile controlnet extensive experiments visualizations demonstrate stylemaster significantly outperforms competitors effectively generating highquality stylized videos align textual content closely resemble style reference images project page httpszixuanyegithubiostylemaster,-1,0.0,-1,0.0
spherical worldlocking audiovisual localization egocentric videos egocentric videos provide comprehensive contexts user scene understanding spanning multisensory perception behavioral interaction propose spherical worldlocking swl general framework egocentric scene representation implicitly transforms multisensory streams respect measurements head orientation compared conventional headlocked egocentric representations planar fieldofview swl effectively offsets challenges posed selfmotion allowing improved spatial synchronization input modalities using set multisensory embeddings worldlocked sphere design unified encoderdecoder transformer architecture preserves spherical structure scene representation without requiring expensive projections image world coordinate systems evaluate effectiveness proposed framework multiple benchmark tasks egocentric video understanding including audiovisual active speaker localization auditory spherical source localization behavior anticipation everyday activities,1,1.0,1,1.0
learning generate diverse pedestrian movements web videos noisy labels understanding modeling pedestrian movements real world crucial applications like motion forecasting scene simulation many factors influence pedestrian movements scene context individual characteristics goals often ignored existing human generation methods web videos contain natural pedestrian behavior rich motion context annotating pretrained predictors leads noisy labels work propose learning diverse pedestrian movements web videos first curate largescale dataset called citywalkers captures diverse realworld pedestrian movements urban scenes based citywalkers propose generative model called pedgen diverse pedestrian movement generation pedgen introduces automatic label filtering remove lowquality labels mask embedding train partial labels also contains novel context encoder lifts scene context incorporate various context factors generating realistic pedestrian movements urban scenes experiments show pedgen outperforms existing baseline methods pedestrian movement generation learning noisy labels incorporating context factors addition pedgen achieves zeroshot generalization realworld simulated environments code model data made publicly available httpsgenforcegithubiopedgen,-1,0.0,-1,0.0
motion inversion video customization work present novel approach motion customization video generation addressing widespread gap exploration motion representation within video generative models recognizing unique challenges posed spatiotemporal nature video method introduces motion embeddings set explicit temporally coherent embeddings derived given video embeddings designed integrate seamlessly temporal transformer modules video diffusion models modulating selfattention computations across frames without compromising spatial integrity approach provides compact efficient solution motion representation utilizing two types embeddings motion querykey embedding modulate temporal attention map motion value embedding modulate attention values additionally introduce inference strategy excludes spatial dimensions motion querykey embedding applies differential operation motion value embedding designed debias appearance ensure embeddings focus solely motion contributions include introduction tailored motion embedding customization tasks demonstration practical advantages effectiveness method extensive experiments,9,0.7287290573356382,9,0.7287290573356382
videodirector precise video editing via texttovideo models despite typical inversionthenediting paradigm using texttoimage models demonstrated promising results directly extending texttovideo models still suffers severe artifacts color flickering content distortion consequently current video editing methods primarily rely models inherently lack temporalcoherence generative ability often resulting inferior editing results paper attribute failure typical editing paradigm tightly spatialtemporal coupling vanilla pivotalbased inversion strategy struggles disentangle spatialtemporal information video diffusion model complicated spatialtemporal layout vanilla crossattention control deficient preserving unedited content address limitations propose spatialtemporal decoupled guidance stdg multiframe nulltext optimization strategy provide pivotal temporal cues precise pivotal inversion furthermore introduce selfattention control strategy maintain higher fidelity precise partial content editing experimental results demonstrate method termed videodirector effectively harnesses powerful temporal generation capabilities models producing edited videos stateoftheart performance accuracy motion smoothness realism fidelity unedited content,-1,0.0,-1,0.0
enhancing medical imaging gans synthesizing realistic images limited data research introduce innovative method synthesizing medical images using generative adversarial networks gans proposed gans method demonstrates capability produce realistic synthetic images even trained limited quantity real medical image data showcasing commendable generalization prowess achieve devised generator discriminator network architecture founded deep convolutional neural networks cnns leveraging adversarial training paradigm model optimization extensive experimentation across diverse medical image datasets method exhibits robust performance consistently generating synthetic images closely emulate structural textural attributes authentic medical images,17,1.0,17,1.0
diffusion based multidomain neuroimaging harmonization method preservation anatomical details multicenter neuroimaging studies face technical variability due batch differences across sites potentially hinders data aggregation impacts study reliabilityrecent efforts neuroimaging harmonization aimed minimize technical gaps reduce technical variability across batches generative adversarial networks gan prominent method addressing image harmonization tasks ganharmonized images suffer artifacts anatomical distortions given advancements denoising diffusion probabilistic model produces highfidelity images assessed efficacy diffusion model neuroimaging harmonization demonstrated diffusion models superior capability harmonizing images multiple domains ganbased methods limited harmonizing images two domains per model experiments highlight learned domain invariant anatomical condition reinforces model accurately preserve anatomical details differentiating batch differences diffusion step proposed method tested two public neuroimaging dataset abide ii yielding harmonization results consistent anatomy preservation superior fid score compared ganbased methods conducted multiple analysis including extensive quantitative qualitative evaluations baseline models ablation study showcasing benefits learned conditions improvements consistency perivascular spaces pvs segmentation harmonization,3,0.8762183835044279,3,0.8762183835044279
casgan contrastfree angiography synthesis iodinated contrast agents widely utilized numerous interventional procedures yet posing substantial health risks patients paper presents casgan novel gan framework serves virtual contrast agent synthesize xray angiographies via disentanglement representation learning vessel semantic guidance thereby reducing reliance iodinated contrast agents interventional procedures specifically approach disentangles xray angiographies background vessel components leveraging medical prior knowledge specialized predictor learns map interrelationships components additionally vessel semanticguided generator corresponding loss function introduced enhance visual fidelity generated images experimental results xcad dataset demonstrate stateoftheart performance casgan achieving fid mmd promising results highlight casgans potential clinical applications,3,0.5984880695464675,3,0.5984880695464675
phydiff physicsguided hourglass diffusion model diffusion mri synthesis diffusion mri dmri important neuroimaging technique high acquisition costs deep learning approaches used enhance dmri predict diffusion biomarkers undersampled dmri generate comprehensive raw dmri generative adversarial network based methods proposed include bvalues bvectors conditions limited unstable training less desirable diversity emerging diffusion model dm promises improve generative performance however remains challenging include essential information conditioning dm relevant generation ie physical principles dmri white matter tract structures study propose physicsguided diffusion model generate highquality dmri model introduces physical principles dmri noise evolution diffusion process introduce querybased conditional mapping within difussion model addition enhance anatomical fine detials generation introduce xtract atlas prior white matter tracts adopting adapter technique experiment results show method outperforms stateoftheart methods potential advance dmri enhancement,3,1.0,3,1.0
rolling diffusion models diffusion models recently increasingly applied temporal data video fluid mechanics simulations climate data methods generally treat subsequent frames equally regarding amount noise diffusion process paper explores rolling diffusion new approach uses sliding window denoising process ensures diffusion process progressively corrupts time assigning noise frames appear later sequence reflecting greater uncertainty future generation process unfolds empirically show temporal dynamics complex rolling diffusion superior standard diffusion particular result demonstrated video prediction task using video dataset chaotic fluid dynamics forecasting experiment,-1,0.0,-1,0.0
diffvps video polyp segmentation via multitask diffusion network adversarial temporal reasoning diffusion probabilistic models recently attracted significant attention community computer vision due outstanding performance however substantial amount diffusionbased research focused generative tasks work introduces diffusion models advance results polyp segmentation videos frequently challenged polyps high camouflage redundant temporal cuesin paper present novel diffusionbased network video polyp segmentation task dubbed diffvps incorporate multitask supervision diffusion models promote discrimination diffusion models pixelbypixel segmentation integrates contextual highlevel information achieved joint classification detection tasks explore temporal dependency temporal reasoning module trm devised via reasoning reconstructing target frame previous frames equip trm generative adversarial selfsupervised strategy produce realistic frames thus capture better dynamic cues extensive experiments conducted sunseg results indicate proposed diffvps significantly achieves stateoftheart performance code available httpsgithubcomlydiaylludiffvps,-1,0.0,-1,0.0
videoscore building automatic metrics simulate finegrained human feedback video generation recent years witnessed great advances video generation however development automatic video metrics lagging significantly behind none existing metric able provide reliable scores generated videos main barrier lack largescale humanannotated dataset paper release videofeedback first largescale dataset containing humanprovided multiaspect score synthesized videos existing video generative models train videoscore initialized mantis based videofeedback enable automatic video quality assessment experiments show spearman correlation videoscore humans reach videofeedbacktest beating prior best metrics points result heldout evalcrafter genaibench vbench show videoscore consistently much higher correlation human judges metrics due results believe videoscore serve great proxy human raters rate different video models track progress simulate finegrained human feedback reinforcement learning human feedback rlhf improve current video generation models,-1,0.0,-1,0.0
text prompting multiconcept video customization autoregressive generation present method multiconcept customization pretrained texttovideo models intuitively multiconcept customized video derived nonlinear intersection video manifolds individual concepts straightforward find hypothesize sequential controlled walking towards intersection video manifolds directed text prompting leads solution generate various concepts corresponding interactions sequentially autoregressive manner method generate videos multiple custom concepts subjects action background teddy bear running towards brown teapot dog playing violin teddy bear swimming ocean quantitatively evaluate method using videoclip dino scores addition human evaluation videos results presented paper found,-1,0.0,-1,0.0
expertaf expert actionable feedback video feedback essential learning new skill improving ones current skilllevel however current methods skillassessment video provide scores compare demonstrations leaving burden knowing differently user introduce novel method generate actionable feedback video person physical activity basketball soccer method takes video demonstration accompanying body pose generates freeform expert commentary describing person well could improve visual expert demonstration incorporates required corrections show leverage videos skilled activity expert commentary together strong language model create weaklysupervised training dataset task devise multimodal videolanguage model infer coaching feedback method able reason across multimodal input combinations output fullspectrum actionable coaching expert commentary expert video retrieval expert pose generation outperforming strong visionlanguage models established metrics human preference studies code data publicly released,0,0.8480887640005381,0,0.8480887640005381
hotvcom generating buzzworthy comments videos era social media video platforms popular hotcomments play crucial role attracting user impressions shortform videos making vital marketing branding purpose however existing research predominantly focuses generating descriptive comments danmaku english offering immediate reactions specific video moments addressing gap study introduces textschotvcom largest chinese video hotcomment dataset comprising diverse videos million comments also present textttcomheat framework synergistically integrates visual auditory textual data generate influential hotcomments chinese video dataset empirical evaluations highlight effectiveness framework demonstrating excellence newly constructed existing datasets,-1,0.0,-1,0.0
exploring temporal event cues dense video captioning cyclic colearning dense video captioning aims detect describe events untrimmed videos paper presents dense video captioning network called multiconcept cyclic learning mccl aims detect multiple concepts frame level using concepts enhance video features provide temporal event cues design cyclic colearning generator localizer within captioning network promote semantic perception event localization specifically perform weakly supervised concept detection frame detected concept embeddings integrated video features provide event cues additionally videolevel concept contrastive learning introduced obtain discriminative concept embeddings captioning network establish cyclic colearning strategy generator guides localizer event localization semantic matching localizer enhances generators event semantic perception location matching making semantic perception event localization mutually beneficial mccl achieves stateoftheart performance activitynet captions datasets extensive experiments demonstrate effectiveness interpretability,-1,0.0,-1,0.0
vidtwin video vae decoupled structure dynamics recent advancements video autoencoders video aes significantly improved quality efficiency video generation paper propose novel compact video autoencoder vidtwin decouples video two distinct latent spaces structure latent vectors capture overall content global movement dynamics latent vectors represent finegrained details rapid movements specifically approach leverages encoderdecoder backbone augmented two submodules extracting latent spaces respectively first submodule employs qformer extract lowfrequency motion trends followed downsampling blocks remove redundant content details second averages latent vectors along spatial dimension capture rapid motion extensive experiments show vidtwin achieves high compression rate high reconstruction quality psnr mcljcv dataset performs efficiently effectively downstream generative tasks moreover model demonstrates explainability scalability paving way future research video latent representation generation code released httpsgithubcommicrosoftvidtoktreemainvidtwin,2,0.7331985466476586,2,0.7331985466476586
content bias frchet video distance frechet video distance fvd prominent metric evaluating video generation models known conflict human perception occasionally paper aim explore extent fvds bias toward perframe quality temporal realism identify sources first quantify fvds sensitivity temporal axis decoupling frame motion quality find fvd increases slightly large temporal corruption analyze generated videos show via careful sampling large set generated videos contain motions one drastically decrease fvd without improving temporal quality studies suggest fvds bias towards quality individual frames observe bias attributed features extracted supervised video classifier trained contentbiased dataset show fvd features extracted recent largescale selfsupervised video models less biased toward image quality finally revisit realworld examples validate hypothesis,-1,0.0,-1,0.0
scaling diffusion mamba bidirectional ssms efficient image video generation recent developments mamba architecture known selective state space approach shown potential efficient modeling long sequences however application image generation remains underexplored traditional diffusion transformers dit utilize selfattention blocks effective computational complexity scales quadratically input length limiting use highresolution images address challenge introduce novel diffusion architecture diffusion mamba dim foregoes traditional attention mechanisms favor scalable alternative harnessing inherent efficiency mamba architecture dim achieves rapid inference times reduced computational load maintaining linear complexity respect sequence length architecture scales effectively also outperforms existing diffusion transformers image video generation tasks results affirm scalability efficiency dim establishing new benchmark image video generation techniques work advances field generative models paves way applications scalable architectures,13,0.9235744575662452,13,0.9235744575662452
video emotion openvocabulary recognition based multimodal large language model multimodal emotion recognition task great concern however traditional data sets based fixed labels resulting models often focus main emotions ignore detailed emotional changes complex scenes report introduces solution using mllms technology generate openvocabulary emotion labels video solution includes use framework data generation processing training methods results generation multimodel cojudgment merov openword emotion recognition challenge method achieved significant advantages leading superior capabilities complex emotion computation,-1,0.0,-1,0.0
enhancing film grain coding vvc improving encoding quality efficiency paper presents indepth analysis film grain handling opensource implementations versatile video coding vvc standard focus two key components film grain analysis fga module implemented vvenc film grain synthesis fgs module implemented vvdec describe methodologies used implement modules discuss generation supplementary enhancement information sei parameters signal film grain characteristics encoded video sequences additionally conduct subjective objective evaluations across full hd videos assess effectiveness film grain handling results demonstrate capability fga fgs techniques accurately analyze synthesize film grain thereby improving visual quality encoded video content overall study contributes advancing understanding implementation film grain handling techniques vvc opensource implementations implications enhancing viewing experience multimedia applications,2,1.0,2,1.0
timerewind rewinding time imageandevents video diffusion paper addresses novel challenge rewinding time single captured image recover fleeting moments missed shutter button pressed problem poses significant challenge computer vision computational photography requires predicting plausible precapture motion single static frame inherently illposed task due high degree freedom potential pixel movements overcome challenge leveraging emerging technology neuromorphic event cameras capture motion information high temporal resolution integrating data advanced imagetovideo diffusion models proposed framework introduces event motion adaptor conditioned event camera data guiding diffusion model generate videos visually coherent physically grounded captured events extensive experimentation demonstrate capability approach synthesize highquality videos effectively rewind time showcasing potential combining event camera technology generative models work opens new avenues research intersection computer vision computational photography generative modeling offering forwardthinking solution capturing missed moments enhancing future consumer cameras smartphones please see project page httpstimerewindgithubio video results code release,-1,0.0,-1,0.0
yingsound videoguided sound effects generation multimodal chainofthought controls generating sound effects productlevel videos small amount labeled data available diverse scenes requires production highquality sounds fewshot settings tackle challenge limited labeled data realworld scenes introduce yingsound foundation model designed videoguided sound generation supports highquality audio generation fewshot settings specifically yingsound consists two major modules first module uses conditional flow matching transformer achieve effective semantic alignment sound generation across audio visual modalities module aims build learnable audiovisual aggregator ava integrates highresolution visual features corresponding audio features multiple stages second module developed proposed multimodal visualaudio chainofthought cot approach generate finer sound effects fewshot settings finally industrystandard videotoaudio dataset encompasses various realworld scenarios presented show yingsound effectively generates highquality synchronized sounds across diverse conditional inputs automated evaluations human studies project page urlhttpsgiantailabgithubioyingsound,8,0.6662516942260219,8,0.6662516942260219
twostage omnidirectional image synthesis geometric distortion correction omnidirectional images increasingly used various applications including virtual reality sns social networking services however availability comparatively limited contrast normal field view nfov images since specialized cameras required take omnidirectional images consequently several methods proposed based generative adversarial networks gan synthesize omnidirectional images approaches shown difficulties training models due instability andor significant time consumption training address problems paper proposes novel omnidirectional image synthesis method twostage omnidirectional image synthesis generated highquality omnidirectional images drastically reduced training time realized utilizing vqgan vector quantized gan model pretrained largescale nfov image database imagenet without finetuning since pretrained model represent distortions omnidirectional images equirectangular projection erp applied directly omnidirectional image synthesis erp therefore twostage structure adopted first create global coarse image erp refine image integrating multiple local nfov images higher resolution compensate distortions erp based pretrained vqgan model result proposed method achieved reduction training time days omnidreamer four days higher image quality,-1,0.0,-1,0.0
videosavi selfaligned video language models without human supervision recent advances visionlanguage models vlms significantly enhanced video understanding tasks instruction tuning ie finetuning models datasets instructions paired desired outputs key improving model performance however creating diverse instructiontuning datasets challenging due high annotation costs complexity capturing temporal information videos existing approaches often rely large language models generate instructionoutput pairs limit diversity lead responses lack grounding video content address propose videosavi selfaligned video language model novel selftraining pipeline enables vlms generate training data without extensive manual annotation process involves three stages generating diverse videospecific questions producing multiple candidate answers evaluating responses alignment video content selfgenerated data used direct preference optimization dpo allowing model refine highquality outputs improve alignment video content experiments demonstrate even smaller models parameters effectively use selftraining approach outperforming previous methods achieving results comparable trained proprietary preference data videosavi shows significant improvements across multiple benchmarks multichoice qa zeroshot openended qa temporal reasoning benchmarks results demonstrate effectiveness selftraining approach enhancing video understanding reducing dependence proprietary models,-1,0.0,-1,0.0
geometryguided gan face animation animating human face images aims synthesize desired source identity naturallooking way mimicking driving videos facial movements context generative adversarial networks demonstrated remarkable potential realtime face reenactment using single source image yet constrained limited geometry consistency compared graphicbased approaches paper introduce geometryguided gan face animation tackle limitation novel approach empowers face animation model incorporate information using images improving image generation capabilities talking head synthesis model integrate inverse rendering techniques extract facial geometry properties improving feedback loop generator weighted average ensemble discriminators face reenactment model leverage motion warping capture motion dynamics along orthogonal ray sampling volume rendering techniques produce ultimate visual output evaluate performance conducted comprehensive experiments using various evaluation protocols talkinghead benchmarks demonstrate effectiveness proposed framework compared stateoftheart realtime face animation methods,6,0.5969143224037134,6,0.5969143224037134
worldsimbench towards video generation models world simulators recent advancements predictive models demonstrated exceptional capabilities predicting future state objects scenes however lack categorization based inherent characteristics continues hinder progress predictive model development additionally existing benchmarks unable effectively evaluate highercapability highly embodied predictive models embodied perspective work classify functionalities predictive models hierarchy take first step evaluating world simulators proposing dual evaluation framework called worldsimbench worldsimbench includes explicit perceptual evaluation implicit manipulative evaluation encompassing human preference assessments visual perspective actionlevel evaluations embodied tasks covering three representative embodied scenarios openended embodied environment autonomous driving robot manipulation explicit perceptual evaluation introduce hfembodied dataset video assessment dataset based finegrained human feedback use train human preference evaluator aligns human perception explicitly assesses visual fidelity world simulators implicit manipulative evaluation assess videoaction consistency world simulators evaluating whether generated situationaware video accurately translated correct control signals dynamic environments comprehensive evaluation offers key insights drive innovation video generation models positioning world simulators pivotal advancement toward embodied artificial intelligence,10,0.6431727539269747,10,0.6431727539269747
realtime video generation pyramid attention broadcast present pyramid attention broadcast pab realtime high quality trainingfree approach ditbased video generation method founded observation attention difference diffusion process exhibits ushaped pattern indicating significant redundancy mitigate broadcasting attention outputs subsequent steps pyramid style applies different broadcast strategies attention based variance best efficiency introduce broadcast sequence parallel efficient distributed inference pab demonstrates speedup across three models compared baselines achieving realtime generation videos anticipate simple yet effective method serve robust baseline facilitate future research application video generation,-1,0.0,-1,0.0
shorts rise assessing effects youtube shorts longform video content short form content permeated video creator space past years led industry leading products tiktok youtube shorts instagram reels youtube particular previously synonymous main hub long form video content consumption monetization long form videos easier allowed multiple advertisement placements course video model also facilitated thematic brand partnerships however since introduction short form content creators found difficult generate revenue advertisement placements decreased leads unique situation people spending time watching shorter videos yet generate less revenue creators paper perform study creators significant audiences see introduction short form content affected view counts engagement long form content findings reveal noteworthy trend since advent shortform content significant decrease view counts engagement longform videos channels,-1,0.0,-1,0.0
moviellm enhancing long video understanding aigenerated movies development multimodal models marked significant step forward machines understand videos models shown promise analyzing short video clips however comes longer formats like movies often fall short main hurdles lack highquality diverse video data intensive work required collect annotate data face challenges propose moviellm novel framework designed synthesize consistent highquality video data instruction tuning pipeline carefully designed control style videos improving textual inversion technique powerful text generation capability first framework thing approach stands flexibility scalability empowering users create customized movies one description makes superior alternative traditional data collection methods extensive experiments validate data produced moviellm significantly improves performance multimodal models understanding complex video narratives overcoming limitations existing datasets regarding scarcity bias,-1,0.0,-1,0.0
videodriven animation neural head avatars present new approach videodriven animation highquality neural head models addressing challenge personindependent animation video input typically highquality generative models learned specific individuals multiview video footage resulting personspecific latent representations drive generation process order achieve personindependent animation video input introduce lstmbased animation network capable translating personindependent expression features personalized animation parameters personspecific head models approach combines advantages personalized head models high quality realism convenience videodriven animation employing multiperson facial performance capture demonstrate effectiveness approach synthesized animations high quality based different source videos well ablation study,6,0.7863721519400273,6,0.7863721519400273
aqua automated questionanswering software tutorial videos visual anchors tutorial videos popular help source learning featurerich software however getting quick answers questions tutorial videos difficult present automated approach responding tutorial questions analyzing questions found video comments identified different question types observed users frequently described parts video questions asked participants watch tutorial videos ask questions annotating video relevant visual anchors visual anchors referred ui elements application workspace based insights built aqua pipeline generates useful answers questions visual anchors demonstrate fusion showing recognize ui elements visual anchors generate answers using augmented visual information software documentation evaluation study demonstrates approach provides better answers baseline methods,-1,0.0,-1,0.0
unveiling invisible captioning videos metaphors metaphors common communication tool used daytoday life detection generation metaphors textual form studied extensively metaphors forms underexplored recent studies shown visionlanguage vl models understand visual metaphors memes adverts probing studies done involve complex language phenomena like metaphors videos hence introduce new vl task describing metaphors present videos work facilitate novel task construct release manually created dataset videos humanwritten captions along new metric called average concept distance acd automatically evaluate creativity metaphors generated also propose novel lowresource video metaphor captioning system gitllava obtains comparable performance sota video language models proposed task perform comprehensive analysis existing video language models task publish dataset models benchmark results enable research,-1,0.0,-1,0.0
uvis unsupervised video instance segmentation video instance segmentation requires classifying segmenting tracking every object across video frames unlike existing approaches rely masks boxes category labels propose uvis novel unsupervised video instance segmentation uvis framework perform video instance segmentation without video annotations dense labelbased pretraining key insight comes leveraging dense shape prior selfsupervised vision foundation model dino openset recognition ability imagecaption supervised visionlanguage model clip uvis framework consists three essential steps framelevel pseudolabel generation transformerbased vis model training querybased tracking improve quality vis predictions unsupervised setup introduce dualmemory design design includes semantic memory bank generating accurate pseudolabels tracking memory bank maintaining temporal consistency object tracks evaluate approach three standard vis benchmarks namely occluded vis uvis achieves ap without video annotations dense pretraining demonstrating potential unsupervised vis framework,7,0.888604450309385,7,0.888604450309385
generative outpainting enhance memorability shortform videos expanding use shortform video format advertising social media entertainment education need media captivate remembered video memorability indicates us likely video remembered viewer emotional personal connection content paper presents results using generative outpainting expand screen size shortform video view improving memorability advances machine learning deep learning compared leveraged understand extending borders video screensizes affect memorability viewers using quantitative evaluation determine bestperforming model outpainting impact outpainting based image saliency video memorability scores,-1,0.0,-1,0.0
foundation models adaptive feature selection synergistic approach video question answering paper tackles intricate challenge video questionanswering videoqa despite notable progress current methods fall short effectively integrating questions video frames semantic objectlevel abstractions create questionaware video representations introduce localglobal question aware video embedding lgqave incorporates three major innovations integrate multimodal knowledge better emphasize semantic visual concepts relevant specific questions lgqave moves beyond traditional adhoc frame sampling utilizing crossattention mechanism precisely identifies relevant frames concerning questions captures dynamics objects within frames using distinct graphs grounding question semantics minigpt model graphs processed questionaware dynamic graph transformer qdgt refines outputs develop nuanced global local video representations additional crossattention module integrates local global embeddings generate final video embeddings language model uses generate answers extensive evaluations across multiple benchmarks demonstrate lgqave significantly outperforms existing models delivering accurate multichoice openended answers,0,0.8459798921779021,0,0.8459798921779021
oneshot posedriving face animation platform objective face animation generate dynamic expressive talking head videos single reference face utilizing driving conditions derived either video audio inputs current approaches often require finetuning specific identities frequently fail produce expressive videos due limited effectiveness modules facilitate generation oneshot consecutive talking head videos refine existing model integrating face locator motion frame mechanism subsequently optimize model using extensive human face video datasets significantly enhancing ability produce highquality expressive talking head videos additionally develop demo platform using gradio framework streamlines process enabling users quickly create customized talking head videos,6,0.7120003803411707,6,0.7120003803411707
videomme firstever comprehensive evaluation benchmark multimodal llms video analysis quest artificial general intelligence multimodal large language models mllms emerged focal point recent advancements however predominant focus remains developing capabilities static image understanding potential mllms processing sequential visual data still insufficiently explored highlighting absence comprehensive highquality assessment performance paper introduce videomme firstever fullspectrum multimodal evaluation benchmark mllms video analysis work distinguishes existing benchmarks four key features diversity video types spanning primary visual domains subfields ensure broad scenario generalizability duration temporal dimension encompassing short medium longterm videos ranging seconds hour robust contextual dynamics breadth data modalities integrating multimodal inputs besides video frames including subtitles audios unveil allround capabilities mllms quality annotations utilizing rigorous manual labeling expert annotators facilitate precise reliable model assessment videos total hours manually selected annotated repeatedly viewing video content resulting questionanswer pairs videomme extensively evaluate various stateoftheart mllms including series gemini pro well opensource image models like video models like llavanextvideo experiments reveal gemini pro bestperforming commercial model significantly outperforming opensource models dataset along findings underscores need improvements handling longer sequences multimodal data project page httpsvideommegithubio,0,1.0,0,1.0
omegance single parameter various granularities diffusionbased synthesis work introduce single parameter omega effectively control granularity diffusionbased synthesis parameter incorporated denoising steps diffusion models reverse process approach require model retraining architectural modifications additional computational overhead inference yet enables precise control level details generated outputs moreover spatial masks denoising schedules varying omega values applied achieve regionspecific timestepspecific granularity control prior knowledge image composition control signals reference images facilitates creation precise omega masks granularity control specific objects highlight parameters role controlling subtle detail variations technique named omegance combining omega nuance method demonstrates impressive performance across various image video synthesis tasks adaptable advanced diffusion models code available,9,1.0,9,1.0
solving masked jigsaw puzzles diffusion vision transformers solving image video jigsaw puzzles poses challenging task rearranging image fragments video frames unordered sequences restore meaningful images video sequences existing approaches often hinge discriminative models tasked predicting either absolute positions puzzle elements permutation actions applied original data unfortunately methods face limitations effectively solving puzzles large number elements paper propose jpdvt innovative approach harnesses diffusion transformers address challenge specifically generate positional information image patches video frames conditioned underlying visual content information employed accurately assemble puzzle pieces correct positions even scenarios involving missing pieces method achieves stateoftheart performance several datasets,-1,0.0,-1,0.0
video motion transfer diffusion transformers propose ditflow method transferring motion reference video newly synthesized one designed specifically diffusion transformers dit first process reference video pretrained dit analyze crossframe attention maps extract patchwise motion signal called attention motion flow amf guide latent denoising process optimizationbased trainingfree manner optimizing latents amf loss generate videos reproducing motion reference one also apply optimization strategy transformer positional embeddings granting us boost zeroshot motion transfer capabilities evaluate ditflow recently published methods outperforming across multiple metrics human evaluation,9,0.7676269057144607,9,0.7676269057144607
single trajectory distillation accelerating image video style transfer diffusionbased stylization methods typically denoise specific partial noise state imagetoimage videotovideo tasks multistep diffusion process computationally expensive hinders realworld application promising solution speed process obtain fewstep consistency models trajectory distillation however current consistency models force initialstep alignment probability flow ode pfode trajectories student imperfect teacher models training strategy ensure consistency whole trajectories address issue propose single trajectory distillation std starting specific partial noise state introduce trajectory bank store teacher models trajectory states mitigating time cost training besides use asymmetric adversarial loss enhance style quality generated images extensive experiments image video stylization demonstrate method surpasses existing acceleration models terms style similarity aesthetic evaluations code results available project page httpssingletrajectorydistillationgithubio,-1,0.0,-1,0.0
vividpose advancing stable video diffusion realistic human image animation human image animation involves generating video static image following specified pose sequence current approaches typically adopt multistage pipeline separately learns appearance motion often leads appearance degradation temporal inconsistencies address issues propose vividpose innovative endtoend pipeline based stable video diffusion svd ensures superior temporal stability enhance retention human identity propose identityaware appearance controller integrates additional facial information without compromising appearance details clothing texture background approach ensures generated videos maintain high fidelity identity human subject preserving key facial features across various poses accommodate diverse human body shapes hand movements introduce geometryaware pose controller utilizes dense rendering maps smplx sparse skeleton maps enables accurate alignment pose shape generated videos providing robust framework capable handling wide range body shapes dynamic hand movements extensive qualitative quantitative experiments ubcfashion tiktok benchmarks demonstrate method achieves stateoftheart performance furthermore vividpose exhibits superior generalization capabilities proposed inthewild dataset codes models available,6,0.47744190546049264,6,0.47744190546049264
animated stickers bringing stickers life video diffusion introduce animated stickers video diffusion model generates animation conditioned text prompt static sticker image model built top stateoftheart emu texttoimage model addition temporal layers model motion due domain gap ie differences visual motion style model performed well generating natural videos longer generate vivid videos applied stickers bridge gap employ twostage finetuning pipeline first weakly indomain data followed humanintheloop hitl strategy term ensembleofteachers distills best qualities multiple teachers smaller student model show strategy allows us specifically target improvements motion quality maintaining style static image inference optimizations model able generate eightframe video highquality interesting relevant motion one second,-1,0.0,-1,0.0
hrinr continuous spacetime video superresolution via event camera continuous spacetime video superresolution cstvsr aims simultaneously enhance video resolution frame rate arbitrary scale recently implicit neural representation inr applied video restoration representing videos implicit fields decoded arbitrary scale however highly illposed nature cstvsr limits effectiveness current inrbased methods assume linear motion frames use interpolation feature warping generate features arbitrary spatiotemporal positions two consecutive frames restrains cstvsr capturing rapid nonlinear motion longterm dependencies involving two frames complex dynamic scenes paper propose novel cstvsr framework called hrinr captures holistic dependencies regional motions based inr assisted event camera novel sensor renowned high temporal resolution low latency fully utilize rich temporal information events design feature extraction consisting regional event feature extractor taking events inputs via proposed event temporal pyramid representation capture regional nonlinear motion holistic eventframe feature extractor longterm dependence continuity motion propose novel inrbased decoder spatiotemporal embeddings capture longterm dependencies larger temporal perception field validate effectiveness generalization method four datasets simulated real data showing superiority method,-1,0.0,-1,0.0
global spatialtemporal informationbased residual convlstm video spacetime superresolution converting lowframerate lowresolution videos highframerate highresolution ones spacetime video superresolution techniques enhance visual experiences facilitate efficient information dissemination propose convolutional neural network cnn spacetime video superresolution namely girnet generate highly accurate features thus improve performance proposed network integrates featurelevel temporal interpolation module deformable convolutions global spatialtemporal informationbased residual convolutional long shortterm memory convlstm module featurelevel temporal interpolation module leverage deformable convolution adapts deformations scale variations objects across different scene locations presents efficient solution conventional convolution extracting features moving objects network effectively uses forward backward feature information determine interframe offsets leading direct generation interpolated frame features global spatialtemporal informationbased residual convlstm module first convlstm used derive global spatialtemporal information input features second convlstm uses previously computed global spatialtemporal information feature initial cell state second convlstm adopts residual connections preserve spatial information thereby enhancing output features experiments dataset show proposed method outperforms stateoftheart techniques peak signaltonoiseratio db db db starnet tmnet respectively structural similarity indexby starnet tmnet respectively visually,-1,0.0,-1,0.0
one shot one talk wholebody talking avatar single image building realistic animatable avatars still requires minutes multiview monocular selfrotating videos methods lack precise control gestures expressions push boundary address challenge constructing wholebody talking avatar single image propose novel pipeline tackles two critical issues complex dynamic modeling generalization novel gestures expressions achieve seamless generalization leverage recent poseguided imagetovideo diffusion models generate imperfect video frames pseudolabels overcome dynamic modeling challenge posed inconsistent noisy pseudovideos introduce tightly coupled hybrid avatar representation apply several key regularizations mitigate inconsistencies caused imperfect labels extensive experiments diverse subjects demonstrate method enables creation photorealistic precisely animatable expressive wholebody talking avatar single image,-1,0.0,-1,0.0
landmarkguided diffusion model highfidelity temporally coherent talking head generation audiodriven talking head generation significant challenging task applicable various fields virtual avatars film production online conferences however existing ganbased models emphasize generating wellsynchronized lip shapes overlook visual quality generated frames diffusionbased models prioritize generating highquality frames neglect lip shape matching resulting jittery mouth movements address aforementioned problems introduce twostage diffusionbased model first stage involves generating synchronized facial landmarks based given speech second stage generated landmarks serve condition denoising process aiming optimize mouth jitter issues generate highfidelity wellsynchronized temporally coherent talking head videos extensive experiments demonstrate model yields best performance,6,1.0,6,1.0
biasconflict sample synthesis adversarial removal debias strategy temporal sentence grounding video temporal sentence grounding video tsgv troubled dataset bias issue caused uneven temporal distribution target moments samples similar semantic components input videos query texts existing methods resort utilizing prior knowledge bias artificially break uneven distribution removes limited amount significant language biases work propose biasconflict sample synthesis adversarial removal debias strategy bssard dynamically generates biasconflict samples explicitly leveraging potentially spurious correlations singlemodality features temporal position target moments adversarial training bias generators continuously introduce biases generate biasconflict samples deceive grounding model meanwhile grounding model continuously eliminates introduced biases requires model multimodality alignment information bssard cover kinds coupling relationships disrupt language visual biases simultaneously extensive experiments charadescd activitynetcd demonstrate promising debiasing capability bssard source codes available httpsgithubcomqzhbbssard,-1,0.0,-1,0.0
followyourmultipose tuningfree multicharacter texttovideo generation via pose guidance texteditable posecontrollable character video generation challenging prevailing topic practical applications however existing approaches mainly focus singleobject video generation pose guidance ignoring realistic situation multicharacter appear concurrently scenario tackle propose novel multicharacter video generation framework tuningfree manner based separated text pose guidance specifically first extract character masks pose sequence identify spatial position generating character single prompts character obtained llms precise text guidance moreover spatialaligned cross attention multibranch control module proposed generate fine grained controllable multicharacter video visualized results generating video demonstrate precise controllability method multicharacter generation also verify generality method applying various personalized models moreover quantitative results show approach achieves superior performance compared previous works,-1,0.0,-1,0.0
modular blind video quality assessment blind video quality assessment bvqa plays pivotal role evaluating improving viewing experience endusers across wide range videobased platforms services contemporary deep learningbased models primarily analyze video content aggressively subsampled format blind impact actual spatial resolution frame rate video quality paper propose modular bvqa model method training improve modularity model comprises base quality predictor spatial rectifier temporal rectifier responding visual content distortion spatial resolution frame rate changes video quality respectively training spatial temporal rectifiers dropped probabilities render base quality predictor standalone bvqa model work better rectifiers extensive experiments professionallygenerated content usergenerated content video databases show quality model achieves superior comparable performance current methods additionally modularity model offers opportunity analyze existing video quality databases terms spatial temporal complexity,12,0.9683270469515687,12,0.9683270469515687
exploring explainability video action recognition image classification video action recognition perhaps two foundational tasks computer vision consequently explaining inner workings trained deep neural networks prime importance numerous efforts focus explaining decisions trained deep neural networks image classification exploration domain temporal version video action recognition scant work take deeper look problem begin revisiting gradcam one popular feature attribution methods image classification extension video action recognition tasks examine methods limitations address introduce videotcav building tcav image classification tasks aims quantify importance specific concepts decisionmaking process video action recognition models scalable generation concepts still open problem propose machineassisted approach generate spatial spatiotemporal concepts relevant video action recognition testing videotcav establish importance temporallyvarying concepts demonstrating superiority dynamic spatiotemporal concepts trivial spatial concepts conclusion introduce framework investigating hypotheses action recognition quantitatively testing thus advancing research explainability deep neural networks used video action recognition,-1,0.0,-1,0.0
guide guidelineguided dataset instructional video comprehension substantial instructional videos internet provide us tutorials completing various tasks existing instructional video datasets focus specific steps video level lacking experiential guidelines task level lead beginners struggling learn new tasks due lack relevant experience moreover specific steps without guidelines trivial unsystematic making difficult provide clear tutorial address problems present guide guidelineguided dataset contains videos instructional tasks domains related daily life specifically annotate instructional task guideline representing common pattern shared taskrelated videos basis annotate systematic specific steps including associated guideline steps specific step descriptions timestamps proposed benchmark consists three subtasks evaluate comprehension ability models step captioning models generate captions specific steps videos guideline summarization models mine common pattern taskrelated videos summarize guideline guidelineguided captioning models generate captions specific steps guide guideline evaluate plenty foundation models guide perform indepth analysis given diversity practicality guide believe used better benchmark instructional video comprehension,0,0.8580448205442166,0,0.8580448205442166
multimodal language models domainspecific procedural video summarization videos serve powerful medium convey ideas tell stories provide detailed instructions especially longformat tutorials tutorials valuable learning new skills ones pace yet overwhelming due length dense content viewers often seek specific information like precise measurements stepbystep execution details making essential extract summarize key segments efficiently intelligent timesensitive video assistant capable summarizing detecting highlights long videos highly sought recent advancements multimodal large language models offer promising solutions develop assistant research explores use multimodal models enhance video summarization stepbystep instruction generation within specific domains models need understand temporal events relationships among actions across video frames approach focuses finetuning timechat improve performance specific domains cooking medical procedures training model domainspecific datasets like tasty cooking medvidqa medical procedures aim enhance ability generate concise accurate summaries instructional videos curate restructure datasets create highquality videocentric instruction data findings indicate finetuned domainspecific procedural data timechat significantly improve extraction summarization key instructional steps longformat videos research demonstrates potential specialized multimodal models assist practical tasks providing personalized stepbystep guidance tailored unique aspects domain,-1,0.0,-1,0.0
intertrack tracking human object interaction without object templates tracking human object interaction videos important understand human behavior rapidly growing stream video data previous videobased methods require predefined object templates singleimagebased methods templatefree lack temporal consistency paper present method track human object interaction without object shape templates decompose tracking problem perframe pose tracking canonical shape optimization first apply singleview reconstruction method obtain temporallyinconsistent perframe interaction reconstructions human propose efficient autoencoder predict smpl vertices directly perframe reconstructions introducing temporally consistent correspondence object introduce pose estimator leverages temporal information predict smooth object rotations occlusions train model propose method generate synthetic interaction videos synthesize total hour videos sequences full ground truth experiments behave intercap show method significantly outperforms previous templatebased video tracking singleframe reconstruction methods proposed synthetic video dataset also allows training videobased methods generalize realworld videos code dataset publicly released,1,1.0,1,1.0
videoglamm large multimodal model pixellevel visual grounding videos finegrained alignment videos text challenging due complex spatial temporal dynamics videos existing videobased large multimodal models lmms handle basic conversations struggle precise pixellevel grounding videos address introduce videoglamm lmm designed finegrained pixellevel grounding videos based userprovided textual inputs design seamlessly connects three key components large language model dual vision encoder emphasizes spatial temporal details spatiotemporal decoder accurate mask generation connection facilitated via tunable vl lv adapters enable close visionlanguage vl alignment architecture trained synchronize spatial temporal elements video content textual instructions enable finegrained grounding curate multimodal dataset featuring detailed visuallygrounded conversations using semiautomatic annotation pipeline resulting diverse set videoqa triplets along objects masks evaluate videoglamm three challenging tasks grounded conversation generation visual grounding referring video segmentation experimental results show model consistently outperforms existing approaches across three tasks,0,0.8767767404114003,0,0.8767767404114003
storyteller improving long video description global audiovisual character identification existing large visionlanguage models lvlms largely limited processing short secondslong videos struggle generating coherent descriptions extended video spanning minutes long video description introduces new challenges consistent character identification plotlevel descriptions incorporating visual audio information address figure audiovisual character identification matching character names dialogue key factor propose storyteller system generating dense descriptions long videos incorporating lowlevel visual concepts highlevel plot information storyteller uses multimodal large language model integrates visual audio text modalities perform audiovisual character identification minutelong video clips results fed lvlm enhance consistency video description validate approach movie description tasks introduce dataset dense descriptions threeminute movie clips evaluate long video descriptions create storyqa large set multiplechoice questions test set assess descriptions inputting answer questions using accuracy automatic evaluation metric experiments show storyteller outperforms open closedsource baselines storyqa achieving higher accuracy strongest baseline demonstrating advantage human sidebyside evaluations additionally incorporating audiovisual character identification storyteller improves performance video description models showing relative improvement respectively accuracy storyqa,0,1.0,0,1.0
grounding video models actions goal conditioned exploration large video models pretrained massive amounts internet video provide rich source physical knowledge dynamics motions objects tasks however video models grounded embodiment agent describe actuate world reach visual states depicted video tackle problem current methods use separate visionbased inverse dynamic model trained embodimentspecific data map image states actions gathering data train model often expensive challenging model limited visual settings similar ones data available paper investigate directly ground video models continuous actions selfexploration embodied environment using generated video states visual goals exploration propose framework uses trajectory level action generation combination video guidance enable agent solve complex tasks without external supervision eg rewards action labels segmentation masks validate proposed approach tasks libero tasks metaworld tasks calvin tasks ithor visual navigation show approach par even surpasses multiple behavior cloning baselines trained expert demonstrations without requiring action annotations,5,0.30667023232951074,5,0.30667023232951074
videoicl confidencebased iterative incontext learning outofdistribution video understanding recent advancements video large multimodal models lmms significantly improved video understanding reasoning capabilities however performance drops outofdistribution ood tasks underrepresented training data traditional methods like finetuning ood datasets impractical due high computational costs incontext learning icl demonstration examples shown promising generalization performance language tasks imagelanguage tasks without finetuning applying icl videolanguage tasks faces challenges due limited context length video lmms videos require longer token lengths address issues propose videoicl novel video incontext learning framework ood tasks introduces similaritybased relevant example selection strategy confidencebased iterative inference approach allows select relevant examples rank based similarity used inference generated response low confidence framework selects new examples performs inference iteratively refining results highconfidence response obtained approach improves ood video understanding performance extending effective context length without incurring high costs experimental results multiple benchmarks demonstrate significant performance gains especially domainspecific scenarios laying groundwork broader video comprehension applications code released,-1,0.0,-1,0.0
tuning large multimodal models videos using reinforcement learning ai feedback recent advancements large language models influenced development video large multimodal models vlmms previous approaches vlmms involved supervised finetuning sft instructiontuned datasets integrating llm visual encoders adding additional learnable modules video text multimodal alignment remains challenging primarily due deficient volume quality multimodal instructiontune data compared textonly data present novel alignment strategy employs multimodal ai system oversee called reinforcement learning ai feedback rlaif providing selfpreference feedback refine facilitating alignment video text modalities specific propose contextaware reward modeling providing detailed video descriptions context generation preference feedback order enrich understanding video content demonstrating enhanced performance across diverse video benchmarks multimodal rlaif approach vlmrlaif outperforms existing approaches including sft model commit opensourcing code models datasets foster research area,0,0.838758613496029,0,0.838758613496029
momentor advancing video large language model finegrained temporal reasoning large language models llms demonstrate remarkable proficiency comprehending handling textbased tasks many efforts made transfer attributes video modality termed videollms however existing videollms capture coarsegrained semantics unable effectively handle tasks related comprehension localization specific video segments light challenges propose momentor videollm capable accomplishing finegrained temporal understanding tasks support training momentor design automatic data generation engine construct largescale video instruction dataset segmentlevel instruction data train momentor enabling perform segmentlevel reasoning localization zeroshot evaluations several tasks demonstrate momentor excels finegrained temporally grounded comprehension localization,0,0.8958296665955048,0,0.8958296665955048
fmmattack flowbased multimodal adversarial attack videobased llms despite remarkable performance videobased large language models llms adversarial threat remains unexplored fill gap propose first adversarial attack tailored videobased llms crafting flowbased multimodal adversarial perturbations small fraction frames within video dubbed fmmattack extensive experiments show attack effectively induce videobased llms generate incorrect answers videos added imperceptible adversarial perturbations intriguingly fmmattack also induce garbling model output prompting videobased llms hallucinate overall observations inspire understanding multimodal robustness safetyrelated feature alignment across different modalities great importance various large multimodal models code available httpsgithubcomthukingminfmmattack,7,0.9058575577608108,7,0.9058575577608108
gazeguided graph neural network action anticipation conditioned intention humans utilize gaze concentrate essential information perceiving interpreting intentions videos incorporating human gaze computational algorithms significantly enhance model performance video understanding tasks work address challenging innovative task video understanding predicting actions agent video based partial video introduce gazeguided action anticipation algorithm establishes visualsemantic graph video input method utilizes graph neural network recognize agents intention predict action sequence fulfill intention assess efficiency approach collect dataset containing household activities generated virtualhome environment accompanied human gaze data viewing videos method outperforms stateoftheart techniques achieving improvement accuracy intention recognition highlights efficiency method learning important features human gaze data,5,0.2595583804133858,5,0.2595583804133858
place solution pvuw challenge video panoptic segmentation video panoptic segmentation vps challenging task extends image panoptic segmentationvps aims simultaneously classify track segment objects video including things stuff due wide application many downstream tasks video understanding video editing autonomous driving order deal task video panoptic segmentation wild propose robust integrated video panoptic segmentation solution use dvis framework baseline generate initial masks thenwe add additional image semantic segmentation model improve performance semantic classesfinally method achieves stateoftheart performance vpq score development test phases respectively ultimately ranked vps track pvuw challenge,-1,0.0,-1,0.0
trainingfree robust interactive video object segmentation interactive video object segmentation crucial video task various applications video editing data annotating however current approaches struggle accurately segment objects across diverse domains recently segment anything model sam introduces interactive visual prompts demonstrates impressive performance across different domains paper propose trainingfree prompt tracking framework interactive video object segmentation ipt leveraging powerful generalization sam although point tracking efficiently captures pixelwise information objects video points tend unstable tracked long period resulting incorrect segmentation towards fast robust interaction jointly adopt sparse points boxes tracking filtering unstable points capturing objectwise information better integrate reference information multiple interactions introduce crossround spacetime module crstm adaptively aggregates mask features previous rounds frames enhancing segmentation stability framework demonstrated robust zeroshot video segmentation results popular vos datasets interaction types including davis youtubevos mose maintaining good tradeoff performance interaction time,-1,0.0,-1,0.0
vcllm video codecs secretly tensor codecs parameter size large language models llms continues expand need large memory footprint high communication bandwidth become significant bottlenecks training inference llms mitigate bottlenecks various tensor compression techniques proposed reduce data size thereby alleviating memory requirements communication pressure research found video codecs despite originally designed compressing videos show excellent efficiency compressing various types tensors demonstrate video codecs versatile generalpurpose tensor codecs achieving stateoftheart compression efficiency various tasks make use hardware video encoding decoding module available gpus create framework capable inference training video codecs repurposed tensor codecs greatly reduces requirement memory capacity communication bandwidth enabling training inference large models consumergrade gpus,2,0.7815283117209206,2,0.7815283117209206
videocot video chainofthought dataset active annotation tool multimodal large language models mllms flourishing mainly focus images less attention videos especially subfields prompt engineering video chainofthought cot instruction tuning videos therefore try explore collection cot datasets videos lead video openqa improve reasoning ability mllms unfortunately making video cot datasets easy task given human annotation cumbersome expensive machinegenerated reliable due hallucination issue develop automatic annotation tool combines machine human experts active learning paradigm active learning interactive strategy model human experts way workload human labeling reduced quality dataset guaranteed help automatic annotation tool strive contribute three datasets namely videocot topicqa topiccot furthermore propose simple effective benchmark based collected datasets exploits cot maximize complex reasoning capabilities mllms extensive experiments demonstrate effectiveness solution,-1,0.0,-1,0.0
bridging information asymmetry textvideo retrieval datacentric approach online video content rapidly grows task textvideo retrieval tvr becomes increasingly important key challenge tvr information asymmetry video text videos inherently richer information textual descriptions often capture fragments complexity paper introduces novel datacentric framework bridge gap enriching textual representations better match richness video content training videos segmented eventlevel clips captioned ensure comprehensive coverage retrieval large language model llm generates semantically diverse queries capture broader range possible matches enhance retrieval efficiency propose query selection mechanism identifies relevant diverse queries reducing computational cost improving accuracy method achieves stateoftheart results across multiple benchmarks demonstrating power datacentric approaches addressing information asymmetry tvr work paves way new research focused leveraging data improve crossmodal retrieval,0,1.0,0,1.0
flying bird object detection dataset surveillance video flying bird dataset surveillance videos introduced tailored development performance evaluation flying bird detection algorithms surveillance videos dataset comprises video clips amounting frames total among frames contain instances flying birds proposed dataset flying birds surveillance videos collected realistic surveillance scenarios birds exhibit characteristics inconspicuous features single frames instances generally small sizes shape variability flight attributes pose challenges need addressed developing flying bird detection methods surveillance videos finally advanced video object detection algorithms selected experimentation proposed dataset results demonstrated dataset remains challenging algorithms publicly available please visit dataset download link related processing scripts,4,0.8029022334127444,4,0.8029022334127444
airletters open video dataset characters drawn air introduce airletters new video dataset consisting realworld videos humangenerated articulated motions specifically dataset requires vision model predict letters humans draw air unlike existing video datasets accurate classification predictions airletters rely critically discerning motion patterns integrating longrange information video time extensive evaluation stateoftheart image video understanding models airletters shows methods perform poorly fall far behind human baseline work shows despite recent progress endtoend video understanding accurate representations complex articulated motions task trivial humans remains open problem endtoend learning,-1,0.0,-1,0.0
video summarization techniques comprehensive review rapid expansion video content across variety industries including social media education entertainment surveillance made video summarization essential field study current work survey explores various approaches methods created video summarizing emphasizing abstractive extractive strategies process extractive summarization involves identification key frames segments source video utilizing methods shot boundary recognition clustering hand abstractive summarization creates new content getting essential content video using machine learning models like deep neural networks natural language processing reinforcement learning attention mechanisms generative adversarial networks multimodal learning also include approaches incorporate two methodologies along discussing uses difficulties encountered realworld implementations paper also covers datasets used benchmark techniques review attempts provide stateoftheart thorough knowledge current state future directions video summarization research,10,0.8315210136597967,10,0.8315210136597967
chatvtg video temporal grounding via chat video dialogue large language models video temporal grounding vtg aims ground specific segments within untrimmed video corresponding given natural language query existing vtg methods largely depend supervised learning extensive annotated data laborintensive prone human biases address challenges present chatvtg novel approach utilizes video dialogue large language models llms zeroshot video temporal grounding chatvtg leverages video dialogue llms generate multigranularity segment captions matches captions given query coarse temporal grounding circumventing need paired annotation data furthermore obtain precise temporal grounding results employ moment refinement finegrained caption proposals extensive experiments three mainstream vtg datasets including charadessta activitynetcaptions tacos demonstrate effectiveness chatvtg chatvtg surpasses performance current zeroshot methods,-1,0.0,-1,0.0
stablizing shape consistency videotovideo editing recent advancements generative ai significantly promoted content creation editing prevailing studies extend exciting progress video editing studies mainly transfer inherent motion patterns source videos edited ones results inferior consistency user prompts often observed due lack particular alignments delivered motions edited contents address limitation present shapeconsistent video editing method namely paper method decomposes entire editing pipeline several sequential procedures edits first video frame establishes alignment delivered motions user prompts eventually propagates edited contents frames based alignment furthermore curate testing benchmark namely davisedit comprehensive evaluation video editing considering various types prompts difficulties experimental results analyses illustrate outperforming performance visual consistency inference efficiency method compared existing stateoftheart studies,9,0.9971449297111065,9,0.9971449297111065
motion free bframe coding neural video compression typical deep neural video compression networks usually follow hybrid approach classical video coding contains two separate modules motion coding residual coding addition symmetric autoencoder often used normal architecture motion residual coding paper propose novel approach handles drawbacks two typical abovementioned architectures call kernelbased motionfree video coding advantages motionfree approach twofold improves coding efficiency network significantly reduces computational complexity thanks eliminating motion estimation motion compensation motion coding timeconsuming engines addition kernelbased autoencoder alleviates blur artifacts usually occur conventional symmetric autoencoder consequently improves visual quality reconstructed frames experimental results show proposed framework outperforms sota deep neural video compression networks hevcclass b dataset competitive uvg mcljcv datasets addition generates highquality reconstructed frames comparison conventional motion codingbased symmetric autoencoder meanwhile model size much smaller motionbased networks around three four times,2,0.7170593794026083,2,0.7170593794026083
mcucoder adaptive bitrate learned video compression iot devices rapid growth camerabased iot devices demands need efficient video compression particularly edge applications devices face hardware constraints often mb ram unstable internet connections traditional deep video compression methods designed highend hardware exceeding capabilities constrained devices consequently video compression scenarios often limited mjpeg due high hardware efficiency low complexity paper introduces opensource adaptive bitrate video compression model tailored resourcelimited iot settings mcucoder features ultralightweight encoder parameters minimal memory footprint making wellsuited edge devices mcus mcucoder uses similar amount energy mjpeg reduces bitrate mcljcv dataset uvg dataset measured msssim moreover mcucoder supports adaptive bitrate streaming generating latent representation sorted importance allowing transmission based available bandwidth ensures smooth realtime video transmission even fluctuating network conditions lowresource devices source code available httpsgithubcomdskielmcucoder,2,1.0,2,1.0
progressaware video frame captioning image captioning provides isolated descriptions individual images video captioning offers one single narrative entire video clip work explores important middle ground progressaware video captioning frame level novel task aims generate temporally finegrained captions accurately describe frame also capture subtle progression actions throughout video sequence despite strong capabilities existing leading vision language models often struggle discern nuances framewise differences address propose progresscaptioner captioning model designed capture finegrained temporal dynamics within action sequence alongside develop framecap dataset support training framecapeval benchmark assess caption quality results demonstrate progresscaptioner significantly surpasses leading captioning models producing precise captions accurately capture action progression set new standard temporal precision video captioning finally showcase practical applications approach specifically aiding keyframe selection advancing video understanding highlighting broad utility,-1,0.0,-1,0.0
towards openvocabulary video semantic segmentation semantic segmentation videos focal point recent research however existing models encounter challenges faced unfamiliar categories address introduce open vocabulary video semantic segmentation ovvss task designed accurately segment every pixel across wide range openvocabulary categories including novel previously unexplored enhance ovvss performance propose robust baseline integrates spatialtemporal fusion module allowing model utilize temporal relationships across consecutive frames additionally incorporate random frame enhancement module broadening models understanding semantic context throughout entire video sequence approach also includes video text encoding strengthens models capability interpret textual information within video context comprehensive evaluations benchmark datasets vspw cityscapes highlight ovvsss zeroshot generalization capabilities especially handling novel categories results validate effectiveness demonstrating improved performance semantic segmentation tasks across diverse video datasets,-1,0.0,-1,0.0
vidtok versatile opensource video tokenizer encoding video content compact latent tokens become fundamental step video generation understanding driven need address inherent redundancy pixellevel representations consequently growing demand highperformance opensource video tokenizers videocentric research gains prominence introduce vidtok versatile video tokenizer delivers stateoftheart performance continuous discrete tokenizations vidtok incorporates several key advancements existing approaches model architecture convolutional layers updownsampling modules address training instability codebook collapse commonly associated conventional vector quantization vq integrate finite scalar quantization fsq discrete video tokenization improved training strategies including twostage training process use reduced frame rates integrating advancements vidtok achieves substantial improvements existing methods demonstrating superior performance across multiple metrics including psnr ssim lpips fvd standardized evaluation settings,20,1.0,20,1.0
ghop generative handobject prior interaction reconstruction grasp synthesis propose ghop denoising diffusion based generative prior handobject interactions allows modeling object human hand conditioned object category learn spatial diffusion model capture joint distribution represent human hand via skeletal distance field obtain representation aligned latent signed distance field object show handobject prior serve generic guidance facilitate tasks like reconstruction interaction clip human grasp synthesis believe model trained aggregating seven diverse realworld interaction datasets spanning across categories represents first approach allows jointly generating hand object empirical evaluations demonstrate benefit joint prior videobased reconstruction human grasp synthesis outperforming current taskspecific baselines project website httpsjudyyegithubioghopwww,-1,0.0,-1,0.0
audioagent leveraging llms audio generation editing composition introduce audioagent multimodal framework audio generation editing composition based text video inputs conventional approaches texttoaudio tta tasks often make singlepass inferences text descriptions straightforward design struggles produce highquality audio given complex text conditions method utilize pretrained tta diffusion network audio generation agent work tandem decomposes text condition atomic specific instructions calls agent audio generation audioagent generate highquality audio closely aligned provided text video exhibiting complex multiple events supporting variablelength variablevolume generation videotoaudio vta tasks existing methods require training timestamp detector synchronize video events generated audio process tedious timeconsuming instead propose simpler approach finetuning pretrained large language model llm eg obtain semantic temporal conditions bridge video audio modality consequently framework contributes comprehensive solution tta vta tasks without substantial computational overhead training,8,0.5414271281728795,8,0.5414271281728795
faster generation closer look clip image embeddings impact spatiotemporal crossattentions paper investigates role clip image embeddings within stable video diffusion svd framework focusing impact video generation quality computational efficiency findings indicate clip embeddings crucial aesthetic quality significantly contribute towards subject background consistency video outputs moreover computationally expensive crossattention mechanism effectively replaced simpler linear layer layer computed first diffusion inference step output cached reused throughout inference process thereby enhancing efficiency maintaining highquality outputs building insights introduce vcut trainingfree approach optimized efficiency within svd architecture vcut eliminates temporal crossattention replaces spatial crossattention onetime computed linear layer significantly reducing computational load implementation vcut leads reduction multipleaccumulate operations macs per video decrease model parameters achieving reduction latency compared baseline approach demonstrates conditioning semantic binding stage sufficient eliminating need continuous computation across inference steps setting new standard efficient video generation,-1,0.0,-1,0.0
domain translation framework adversarial denoising diffusion model generate synthetic datasets echocardiography images currently medical image domain translation operations show high demand researchers clinicians amongst capabilities task allows generation new medical images sufficiently high image quality making clinically relevant deep learning dl architectures specifically deep generative models widely used generate translate images one domain another proposed framework relies adversarial denoising diffusion model ddm synthesize echocardiography images perform domain translation contrary generative adversarial networks gans ddms able generate high quality image samples large diversity ddm combined gan ability generate new data completed even faster sampling time work trained adversarial ddm combined gan learn reverse denoising process relying guide image making sure relevant anatomical structures echocardiography image kept represented generated image samples several domain translation operations results verified generative model able synthesize high quality image samples mse psnr db ssim proposed method showed high generalization ability introducing framework create echocardiography images suitable used clinical research purposes,-1,0.0,-1,0.0
vidman exploiting implicit dynamics video diffusion model effective robot manipulation recent advancements utilizing largescale video data learning video generation models demonstrate significant potential understanding complex physical dynamics suggests feasibility leveraging diverse robot trajectory data develop unified dynamicsaware model enhance robot manipulation however given relatively small amount available robot data directly fitting data without considering relationship visual observations actions could lead suboptimal data utilization end propose vidman video diffusion robot manipulation novel framework employs twostage training mechanism inspired dualprocess theory neuroscience enhance stability improve data utilization efficiency specifically first stage vidman pretrained open xembodiment dataset oxe predicting future visual trajectories video denoising diffusion manner enabling model develop long horizontal awareness environments dynamics second stage flexible yet effective layerwise selfattention adapter introduced transform vidman efficient inverse dynamics model predicts action modulated implicit dynamics knowledge via parameter sharing vidman framework outperforms stateoftheart baseline model calvin benchmark achieving relative improvement demonstrates precision gains oxe smallscale dataset results provide compelling evidence world models significantly enhance precision robot action prediction codes models public,-1,0.0,-1,0.0
deformationrecovery diffusion model drdm instance deformation image manipulation synthesis medical imaging diffusion models shown great potential synthetic image generation tasks however models often struggle interpretable connections generated existing images could create illusions address challenges research proposes novel diffusionbased generative model based deformation diffusion recovery model named deformationrecovery diffusion model drdm diverges traditional scoreintensity latent featurebased approaches emphasizing morphological changes deformation fields rather direct image synthesis achieved introducing topologicalpreserving deformation field generation method randomly samples integrates set multiscale deformation vector fields dvf drdm trained learn recover unreasonable deformation components thereby restoring randomly deformed image realistic distribution innovations facilitate generation diverse anatomically plausible deformations enhancing data augmentation synthesis analysis downstream tasks fewshot learning image registration experimental results cardiac mri pulmonary ct show drdm capable creating diverse large image size deformation scale highquality negative rate jacobian matrixs determinant lower deformation fields experimental results downstream tasks image segmentation image registration indicate significant improvements resulting drdm showcasing potential model advance image manipulation synthesis medical imaging beyond project page,-1,0.0,-1,0.0
videophy evaluating physical commonsense video generation recent advances internetscale video data pretraining led development texttovideo generative models create highquality videos across broad range visual concepts synthesize realistic motions render complex objects hence generative models potential become generalpurpose simulators physical world however unclear far goal existing texttovideo generative models end present videophy benchmark designed assess whether generated videos follow physical commonsense realworld activities eg marbles roll placed slanted surface specifically curate diverse prompts involve interactions various material types physical world eg solidsolid solidfluid fluidfluid generate videos conditioned captions diverse stateoftheart texttovideo generative models including open models eg cogvideox closed models eg lumiere dream machine human evaluation reveals existing models severely lack ability generate videos adhering given text prompts also lack physical commonsense specifically best performing model generates videos adhere caption physical laws instances videophy thus highlights video generative models far accurately simulating physical world finally propose autoevaluator videoconphysics assess performance reliably newly released models,-1,0.0,-1,0.0
tiled diffusion image tiling seamless connection disparate images create coherent visual field crucial applications texture creation video game asset development digital art traditionally tiles constructed manually method poses significant limitations scalability flexibility recent research attempted automate process using generative models however current approaches primarily focus tiling textures manipulating models singleimage generation without inherently supporting creation multiple interconnected tiles across diverse domains paper presents tiled diffusion novel approach extends capabilities diffusion models accommodate generation cohesive tiling patterns across various domains image synthesis require tiling method supports wide range tiling scenarios selftiling complex manytomany connections enabling seamless integration multiple images tiled diffusion automates tiling process eliminating need manual intervention enhancing creative possibilities various applications seamlessly tiling existing images tiled texture creation synthesis,-1,0.0,-1,0.0
baking gaussian splatting diffusion denoiser fast scalable singlestage generation reconstruction existing feedforward methods mainly rely multiview diffusion models guarantee consistency methods easily collapse changing prompt view direction mainly handle objectcentric cases paper propose novel singlestage diffusion model diffusiongs object generation scene reconstruction single view diffusiongs directly outputs gaussian point clouds timestep enforce view consistency allow model generate robustly given prompt views directions beyond objectcentric inputs plus improve capability generality diffusiongs scale training data developing sceneobject mixed training strategy experiments show diffusiongs yields improvements psnrfid objects scenes stateoftheart methods without depth estimator plus method enjoys faster speed gpu project page shows video interactive results,1,1.0,1,1.0
sangria surgical video scene graph optimization surgical workflow prediction graphbased holistic scene representations facilitate surgical workflow understanding recently demonstrated significant success however task often hindered limited availability densely annotated surgical scene data work introduce endtoend framework generation optimization surgical scene graphs downstream task approach leverages flexibility graphbased spectral clustering generalization capability foundation models generate unsupervised scene graphs learnable properties reinforce initial spatial graph sparse temporal connections using local matches consecutive frames predict temporally consistent clusters across temporal neighborhood jointly optimizing spatiotemporal relations node features dynamic scene graph downstream task phase segmentation address costly annotationburdensome task semantic scene comprehension scene graph generation surgical videos using weak surgical phase labels incorporating effective intermediate scene representation disentanglement steps within pipeline solution outperforms sota cataracts dataset accuracy score surgical workflow recognition,7,0.8307766540344504,7,0.8307766540344504
wolf dense video captioning world summarization framework propose wolf world summarization framework accurate video captioning wolf automated captioning framework adopts mixtureofexperts approach leveraging complementary strengths vision language models vlms utilizing image video models framework captures different levels information summarizes efficiently approach applied enhance video understanding autolabeling captioning evaluate caption quality introduce capscore llmbased metric assess similarity quality generated captions compared ground truth captions build four humanannotated datasets three domains autonomous driving general scenes robotics facilitate comprehensive comparisons show wolf achieves superior captioning performance compared stateoftheart approaches research community cogagent commercial solutions instance comparison wolf improves capscore qualitywise similaritywise challenging driving videos finally establish benchmark video captioning introduce leaderboard aiming accelerate advancements video understanding captioning data alignment webpage,0,1.0,0,1.0
snapcap efficient snapshot compressive video captioning video captioning vc challenging multimodal task since requires describing scene language understanding various complex videos machines traditional vc follows imagingcompressiondecodingandthencaptioning pipeline compression pivot storage transmission however pipeline potential shortcomings inevitable ie information redundancy resulting low efficiency information loss sampling process captioning address problems paper propose novel vc pipeline generate captions directly compressed measurement captured snapshot compressive sensing camera dub model snapcap specific benefiting signal simulation access obtain abundant measurementvideoannotation data pairs model besides better extract languagerelated visual representations compressed measurement propose distill knowledge videos via pretrained clip plentiful languagevision associations guide learning snapcap demonstrate effectiveness snapcap conduct experiments two widelyused vc datasets qualitative quantitative results verify superiority pipeline conventional vc pipelines particular compared captionafterreconstruction methods snapcap run least faster achieve better caption results,-1,0.0,-1,0.0
eventhdr event highspeed hdr videos beyond event cameras innovative neuromorphic sensors asynchronously capture scene dynamics due eventtriggering mechanism cameras record event streams much shorter response latency higher intensity sensitivity compared conventional cameras basis features previous works attempted reconstruct high dynamic range hdr videos events either suffered unrealistic artifacts failed provide sufficiently high frame rates paper present recurrent convolutional neural network reconstruct highspeed hdr videos event sequences key frame guidance prevent potential error accumulation caused sparse event data additionally address problem severely limited real dataset develop new optical system collect realworld dataset paired highspeed hdr videos event streams facilitating future research field dataset provides first real paired dataset eventtohdr reconstruction avoiding potential inaccuracies simulation strategies experimental results demonstrate method generate highquality highspeed hdr videos explore potential work crosscamera reconstruction downstream computer vision tasks including object detection panoramic segmentation optical flow estimation monocular depth estimation hdr scenarios,1,1.0,1,1.0
learning camera movement control realworld drone videos study seeks automate camera movement control filming existing subjects attractive videos contrasting creation nonexistent content directly generating pixels select drone videos test case due rich challenging motion patterns distinctive viewing angles precise controls existing ai videography methods struggle limited appearance diversity simulation training high costs recording expert operations difficulties designing heuristicbased goals cover scenarios avoid issues propose scalable method involves collecting realworld training data improve diversity extracting camera trajectories automatically minimize annotation costs training effective architecture rely heuristics specifically collect highquality trajectories running reconstruction online videos connecting camera poses consecutive frames formulate camera paths using kalman filter identify remove lowquality data moreover introduce dvgformer autoregressive transformer leverages camera path images past frames predict camera movement next frame evaluate system across synthetic natural scenes real city scans show system effectively learns perform challenging camera movements navigating obstacles maintaining low altitude increase perceived speed orbiting towers buildings useful recording highquality videos data code available dvgformergithubio,-1,0.0,-1,0.0
pandora towards general world model natural language actions video states world models simulate future states world response different actions facilitate interactive content creation provides foundation grounded longhorizon reasoning current foundation models fully meet capabilities general world models large language models llms constrained reliance language modality limited understanding physical world video models lack interactive action control world simulations paper makes step towards building general world model introducing pandora hybrid autoregressivediffusion model simulates world states generating videos allows realtime control freetext actions pandora achieves domain generality video consistency controllability largescale pretraining instruction tuning crucially pandora bypasses cost trainingfromscratch integrating pretrained llm pretrained video model requiring additional lightweight finetuning illustrate extensive outputs pandora across diverse domains indooroutdoor naturalurban humanrobot etc results indicate great potential building stronger general world models largerscale training,5,0.24448747826462794,5,0.24448747826462794
unsupervised video summarization via reinforcement learning trained evaluator paper presents novel approach unsupervised video summarization using reinforcement learning aims address existing limitations current unsupervised methods including unstable training adversarial generatordiscriminator architectures reliance handcrafted reward functions quality evaluation proposed method based concept concise informative summary result reconstructed video closely resembles original summarizer model assigns importance score frame generates video summary proposed scheme reinforcement learning coupled unique reward generation pipeline employed train summarizer model reward generation pipeline trains summarizer create summaries lead improved reconstructions comprises generator model capable reconstructing masked frames partially masked video along reward mechanism compares reconstructed video summary original video generator trained selfsupervised manner reconstruct randomly masked frames enhancing ability generate accurate summaries training pipeline results summarizer model better mimics humangenerated video summaries compared methods relying handcrafted rewards training process consists two stable isolated training steps unlike adversarial architectures experimental results demonstrate promising performance fscores tvsum summe datasets respectively additionally inference stage times faster previously reported stateoftheart method,0,1.0,0,1.0
world simulator good story presenter consecutive eventsbased benchmark future long video generation current stateoftheart video generative models produce commercialgrade videos highly realistic details however still struggle coherently present multiple sequential events stories specified prompts foreseeable essential capability future long video generation scenarios example top generative models still fail generate video short simple story put elephant refrigerator existing detailoriented benchmarks primarily focus finegrained metrics like aesthetic quality spatialtemporal consistency fall short evaluating models abilities handle eventlevel story presentation address gap introduce storyeval storyoriented benchmark specifically designed assess texttovideo models storycompletion capabilities storyeval features prompts spanning classes representing short stories composed consecutive events employ advanced visionlanguage models verify completion event generated videos applying unanimous voting method enhance reliability methods ensure high alignment human evaluations evaluation models reveals challenge none exceeding average storycompletion rate storyeval provides new benchmark advancing models highlights challenges opportunities developing nextgeneration solutions coherent storydriven video generation,-1,0.0,-1,0.0
identitypreserving texttovideo generation frequency decomposition identitypreserving texttovideo generation aims create highfidelity videos consistent human identity important task video generation remains open problem generative models paper pushes technical frontier two directions resolved literature tuningfree pipeline without tedious casebycase finetuning frequencyaware heuristic identitypreserving ditbased control scheme propose consisid tuningfree ditbased controllable model keep human identity consistent generated video inspired prior findings frequency analysis diffusion transformers employs identitycontrol signals frequency domain facial features decomposed lowfrequency global features highfrequency intrinsic features first lowfrequency perspective introduce global facial extractor encodes reference images facial key points latent space generating features enriched lowfrequency information features integrated shallow layers network alleviate training challenges associated dit second highfrequency perspective design local facial extractor capture highfrequency details inject transformer blocks enhancing models ability preserve finegrained features propose hierarchical training strategy leverage frequency information identity preservation transforming vanilla pretrained video generation model model extensive experiments demonstrate frequencyaware heuristic scheme provides optimal control solution ditbased models thanks scheme consisid generates highquality identitypreserving videos making strides towards effective code httpsgithubcompkuyuangroupconsisid,-1,0.0,-1,0.0
unified static dynamic network efficient temporal filtering video grounding inspired activitysilent persistent activity mechanisms human visual perception biology design unified static dynamic network unisdnet learn semantic association video textaudio queries crossmodal environment efficient video grounding static modeling devise novel residual structure resmlp boost global comprehensive interaction video segments queries achieving effective semantic enhancementsupplement dynamic modeling effectively exploit three characteristics persistent activity mechanism network design better video context comprehension specifically construct diffusely connected video clip graph basis sparse temporal masking reflect shortterm effect relationship innovatively consider temporal distance relevance joint auxiliary evidence clues design multikernel temporal gaussian filter expand context clue highdimensional space simulating complex visual perception conduct element level filtering convolution operations neighbour clip nodes message passing stage finally generating ranking candidate proposals unisdnet applicable natural language video grounding nlvg spoken language video grounding slvg tasks unisdnet achieves sota performance three widely used datasets nlvg well three datasets slvg eg reporting new records activitynet captions tacos facilitate field collect two new datasets charadessta speech tacos speech slvg task meanwhile inference speed unisdnet faster strong multiquery benchmark code available httpsgithubcomxianshunisdnet,-1,0.0,-1,0.0
turning text imagery captivating visual video ability visualize structure multiple perspectives crucial comprehensive planning presentation paper introduces advanced application generative models akin stable video diffusion tailored architectural visualization explore potential models create consistent multiperspective videos buildings single images generate design videos directly textual descriptions proposed method enhances design process offering rapid prototyping cost time efficiency enriched creative space architects designers harnessing power ai approach accelerates visualization architectural concepts also enables interactive immersive experience clients stakeholders advancement architectural visualization represents significant leap forward allowing deeper exploration design possibilities effective communication complex architectural ideas,10,0.9193262775949379,10,0.9193262775949379
refereverything towards segmenting everything speak videos present rem framework segmenting wide range concepts video described natural language method capitalizes visuallanguage representations learned video diffusion models internetscale datasets key insight approach preserving much generative models original representation possible finetuning narrowdomain referral object segmentation datasets result framework accurately segment track rare unseen objects despite trained object masks limited set categories additionally generalize nonobject dynamic concepts waves crashing ocean demonstrated newly introduced benchmark referral video process segmentation refvps experiments show rem performs par stateoftheart approaches indomain datasets like refdavis outperforming twelve points terms region similarity outofdomain data leveraging power internetscale pretraining,-1,0.0,-1,0.0
pathways image manifold image editing via video generation recent advances image editing driven image diffusion models shown remarkable progress however significant challenges remain models often struggle follow complex edit instructions accurately frequently compromise fidelity altering key elements original image simultaneously video generation made remarkable strides models effectively function consistent continuous world simulators paper propose merging two fields utilizing imagetovideo models image editing reformulate image editing temporal process using pretrained video models create smooth transitions original image desired edit approach traverses image manifold continuously ensuring consistent edits preserving original images key aspects approach achieves stateoftheart results textbased image editing demonstrating significant improvements edit accuracy image preservation visit project page,9,0.7676269057144607,9,0.7676269057144607
anatomical featureprioritized loss enhanced mr ct translation medical image synthesis precision localized structural details crucial particularly addressing specific clinical requirements identification measurement fine structures traditional methods image translation synthesis generally optimized global image reconstruction often fall short providing finesse required detailed local analysis study represents step toward addressing challenge introducing novel anatomical featureprioritized afp loss function synthesis process method enhances reconstruction focusing clinically significant structures utilizing features pretrained model designed specific downstream task segmentation particular anatomical regions afp loss function replace complement global reconstruction methods ensuring balanced emphasis global image fidelity local structural details various implementations loss function explored including integration different synthesis networks ganbased cnnbased models approach applied evaluated two contexts lung mr ct translation focusing highquality reconstruction bronchial structures using private dataset pelvis mr ct synthesis targeting accurate representation organs muscles utilizing public dataset challenge leverage embeddings pretrained segmentation models specific anatomical regions demonstrate capability afp loss prioritize accurately reconstruct essential features tailored approach shows promising potential enhancing specificity practicality medical image synthesis clinical applications,3,1.0,3,1.0
leveraging usergenerated metadata online videos cover song identification youtube rich source cover songs since platform organized terms videos rather songs retrieval covers trivial field cover song identification addresses problem provides approaches usually rely audio content however including usergenerated video metadata available youtube promises improved identification results paper propose multimodal approach cover song identification online video platforms combine entity resolution models audiobased approaches using ranking model findings implicate leveraging usergenerated metadata stabilize cover song identification performance youtube,-1,0.0,-1,0.0
spatial decomposition temporal fusion based inter prediction learned video compression video compression performance closely related accuracy inter prediction tends difficult obtain accurate inter prediction local video regions inconsistent motion occlusion traditional video coding standards propose various technologies handle motion inconsistency occlusion recursive partitions geometric partitions longterm references however existing learned video compression schemes focus obtaining overall minimized prediction error averaged regions ignoring motion inconsistency occlusion local regions paper propose spatial decomposition temporal fusion based inter prediction learned video compression handle motion inconsistency propose decompose video structure detail sdd components first perform sddbased motion estimation sddbased temporal context mining structure detail components generate shortterm temporal contexts handle occlusion propose propagate longterm temporal contexts recurrently accumulating temporal information historical reference feature fuse shortterm temporal contexts sddbased motion model long shortterm temporal contexts fusion proposed learned video codec obtain accurate inter prediction comprehensive experimental results demonstrate codec outperforms reference software common test datasets psnr msssim,-1,0.0,-1,0.0
analysis neural video compression networks video coding increasing efforts bringing highquality virtual reality technologies market efficient video compression gains importance stateoftheart video coding standard integrates dedicated tools video considerable efforts put designing projection formats improved compression efficiency fastevolving field neural video compression networks nvcs effects different projection formats overall compression performance yet investigated thus unclear whether resampling conventional equirectangular projection erp projection formats yields similar gains nvcs hybrid video codecs formats perform best paper analyze several generations nvcs extensive set projection formats respect compression performance video based analysis find projection format resampling yields significant improvements compression performance also nvcs adjusted cubemap projection acp equatorial cylindrical projection ecp show perform best achieve rate savings compared erp based wspsnr recent nvc remarkably observed rate savings higher emphasizing importance projection format resampling nvcs,2,1.0,2,1.0
universal representation learning video editing components paper focuses understanding predominant video creation pipeline ie compositional video editing six main types editing components including video effects animation transition filter sticker text contrast existing visual representation learning visual materials ie imagesvideos aim learn visual representations editing actionscomponents generally applied raw materials start proposing first largescale dataset editing components video creation covers editing components videos video dataset rendered various imagevideo materials single editing component supports atomic visual understanding different editing components also benefit several downstream tasks eg editing component recommendation editing component recognitionretrieval etc existing visual representation methods perform poorly difficult disentangle visual appearance editing components raw materials end benchmark popular alternative solutions propose novel method learns attend appearance editing components regardless raw materials method achieves favorable results editing component retrievalrecognition compared alternative solutions user study also conducted show representations cluster visually similar editing components better alternatives furthermore learned representations used transition recommendation tasks achieve stateoftheart results autotransition dataset code dataset available,0,0.8713932561578327,0,0.8713932561578327
elysium exploring objectlevel perception videos via mllm multimodal large language models mllms demonstrated ability perceive objects still images application videorelated tasks object tracking remains understudied lack exploration primarily due two key challenges firstly extensive pretraining largescale video datasets required equip mllms capability perceive objects across multiple frames understand interframe relationships secondly processing large number frames within context window large language models llms impose significant computational burden address first challenge introduce largescale video dataset supported three tasks single object tracking sot referring single object tracking rsot video referring expression generation videoreg contains million annotated video frames corresponding object boxes descriptions leveraging dataset conduct training mllms propose tokencompression model tselector tackle second challenge proposed approach elysium exploring objectlevel perception videos via mllm endtoend trainable mllm attempts conduct objectlevel tasks videos without requiring additional plugin expert models codes datasets available httpsgithubcomhonwongelysium,-1,0.0,-1,0.0
text mass modeling stochastic embedding textvideo retrieval increasing prevalence video clips sparked growing interest textvideo retrieval recent advances focus establishing joint embedding space text video relying consistent embedding representations compute similarity however text content existing datasets generally short concise making hard fully describe redundant semantics video correspondingly single text embedding may less expressive capture video embedding empower retrieval study propose new stochastic text modeling method tmass ie text modeled stochastic embedding enrich text embedding flexible resilient semantic range yielding text mass specific introduce similarityaware radius module adapt scale text mass upon given textvideo pairs plus design develop support text regularization control text mass training inference pipeline also tailored fully exploit text mass accurate retrieval empirical evidence suggests tmass effectively attracts relevant textvideo pairs distancing irrelevant ones also enables determination precise text embeddings relevant pairs experimental results show substantial improvement tmass baseline also tmass achieves stateoftheart performance five benchmark datasets including msrvtt lsmdc didemo vatex charades,0,1.0,0,1.0
stllm large language models effective temporal learners large language models llms showcased impressive capabilities text comprehension generation prompting research efforts towards video llms facilitate humanai interaction video level however effectively encode understand videos videobased dialogue systems remains solved paper investigate straightforward yet unexplored question feed spatialtemporal tokens llm thus delegating task video sequence modeling llms surprisingly simple approach yields significant improvements video understanding based upon propose stllm effective videollm baseline spatialtemporal sequence modeling inside llm furthermore address overhead stability issues introduced uncompressed video tokens within llms develop dynamic masking strategy tailormade training objectives particularly long videos also designed globallocal input module balance efficiency effectiveness consequently harness llm proficient spatialtemporal modeling upholding efficiency stability extensive experimental results attest effectiveness method concise model training pipeline stllm establishes new stateoftheart result videochatgptbench mvbench codes available httpsgithubcomtencentarcstllm,0,1.0,0,1.0
parameterefficient instanceadaptive neural video compression learningbased neural video codecs nvcs emerged compelling alternative standard video codecs demonstrating promising performance simple easily maintainable pipelines however nvcs often fall short compression performance occasionally exhibit poor generalization capability due inferenceonly compression scheme dependence training data instanceadaptive video compression techniques recently suggested viable solution finetuning encoder decoder networks particular test instance video however finetuning model parameters incurs high computational costs increases bitrates often leads unstable training work propose parameterefficient instanceadaptive video compression framework inspired remarkable success parameterefficient finetuning largescale neural network models propose use lightweight adapter module easily attached pretrained nvcs finetuned test video sequences resulting algorithm significantly improves compression performance reduces encoding time compared existing instantadaptive video compression algorithms furthermore suggested finetuning method enhances robustness training process allowing proposed method widely used many practical settings conducted extensive experiments various standard benchmark datasets including uvg mcljvc hevc sequences experimental results shown significant improvement ratedistortion rd curves db psnr bd rates compared baselines nvc code available httpsgithubcomohsngjunpevc,2,0.9172949137271535,2,0.9172949137271535
rmtbvqa recurrent memory transformerbased blind video quality assessment enhanced video content recent advances deep learning numerous algorithms developed enhance video quality reduce visual artifacts improve perceptual quality however little research reported quality assessment enhanced content evaluation enhancement methods often based quality metrics designed compression applications paper propose novel blind deep video quality assessment vqa method specifically enhanced video content employs new recurrent memory transformer rmt based network architecture obtain video quality representations optimized novel contentqualityaware contrastive learning strategy based new database containing training patches enhanced content extracted quality representations combined linear regression generate videolevel quality indices proposed method rmtbvqa evaluated vdpve vqa dataset perceptual video enhancement database fivefold cross validation results show superior correlation performance compared ten existing noreference quality metrics,12,1.0,12,1.0
ubiss unified framework bimodal semantic summarization videos surge amount video data video summarization techniques including visualmodalvm textualmodaltm summarization attracting attention however unimodal summarization inevitably loses rich semantics video paper focus comprehensive video summarization task named bimodal semantic summarization videos bissv specifically first construct largescale dataset bids video vmsummary tmsummary triplet format unlike traditional processing methods construction procedure contains vmsummary extraction algorithm aiming preserve salient content within long videos based bids propose unified framework ubiss bissv task models saliency information video generates tmsummary vmsummary simultaneously optimize model listwise rankingbased objective improve capacity capture highlights lastly propose metric provide joint evaluation bimodal summary experiments show unified framework achieves better performance multistage summarization pipelines code data available httpsgithubcommeiyutinggubiss,-1,0.0,-1,0.0
mama metaoptimized angular margin contrastive framework videolanguage representation learning data quality stands forefront deciding effectiveness videolanguage representation learning however videotext pairs previous data typically align perfectly might lead videolanguage representations accurately reflect crossmodal semantics moreover previous data also possess uneven distribution concepts thereby hampering downstream performance across unpopular subjects address problems propose mama new approach learning videolanguage representations utilizing contrastive objective subtractive angular margin regularize crossmodal representations effort reach perfect similarity furthermore adapt nonuniform concept distribution mama utilizes multilayer perceptron mlpparameterized weighting function maps loss values sample weights enable dynamic adjustment models focus throughout training training guided small amount unbiased metadata augmented videotext data generated large visionlanguage model mama improves videolanguage representations achieve superior performances commonly used video question answering textvideo retrieval datasets code model data made available httpsnguyentthonggithubiomama,-1,0.0,-1,0.0
visa reasoning video object segmentation via large language models existing video object segmentation vos relies explicit user instructions categories masks short phrases restricting ability perform complex video segmentation requiring reasoning world knowledge paper introduce new task reasoning video object segmentation reasonvos task aims generate sequence segmentation masks response implicit text queries require complex reasoning abilities based world knowledge video contexts crucial structured environment understanding objectcentric interactions pivotal development embodied ai tackle reasonvos introduce visa videobased large language instructed segmentation assistant leverage world knowledge reasoning capabilities multimodal llms possessing ability segment track objects videos mask decoder moreover establish comprehensive benchmark consisting instructionmask sequence pairs diverse videos incorporates complex world knowledge reasoning segmentation tasks instructiontuning evaluation purposes reasonvos models experiments conducted datasets demonstrate effectiveness visa tackling complex reasoning segmentation vanilla referring segmentation video image domains code dataset available httpsgithubcomcilinyanvisa,0,0.9312525591991255,0,0.9312525591991255
disentangling spatiotemporal knowledge weakly supervised object detection segmentation surgical video weakly supervised video object segmentation wsvos enables identification segmentation maps without requiring extensive training dataset object masks relying instead coarse video labels indicating object presence current stateoftheart methods either require multiple independent stages processing employ motion cues case endtoend trainable networks lack segmentation accuracy part due difficulty learning segmentation maps videos transient object presence limits application wsvos semantic annotation surgical videos multiple surgical tools frequently move field view problem difficult typically encountered wsvos paper introduces video spatiotemporal disentanglement networks vdstnet framework disentangle spatiotemporal information using semidecoupled knowledge distillation predict highquality class activation maps cams teacher network designed resolve temporal conflicts specifics object location timing video provided works student network integrates information time leveraging temporal dependencies demonstrate efficacy framework public reference dataset challenging surgical video dataset objects average present less annotated frames method outperforms stateoftheart techniques generates superior segmentation masks videolevel weak supervision,7,0.8420605945844708,7,0.8420605945844708
unlocking exocentric videolanguage data egocentric video representation learning present embed egocentric models built exocentric data method designed transform exocentric videolanguage data egocentric video representation learning largescale exocentric data covers diverse activities significant potential egocentric learning inherent disparities egocentric exocentric data pose challenges utilizing one view seamlessly egocentric videos predominantly feature closeup handobject interactions whereas exocentric videos offer broader perspective human activities additionally narratives egocentric datasets typically actioncentric closely linked visual content contrast narrative styles found exocentric datasets address challenges employ data transformation framework adapt exocentric data egocentric training focusing identifying specific video clips emphasize handobject interactions transforming narration styles align egocentric perspectives applying vision language style transfer framework creates new egocentric dataset derived exocentric videolanguage data extensive evaluations demonstrate effectiveness embed achieving stateoftheart results across various egocentric downstream tasks including absolute improvement multiinstance retrieval egtea classification benchmarks zeroshot settings furthermore embed enables egocentric videolanguage models perform competitively exocentric tasks finally showcase embeds application across various exocentric datasets exhibiting strong generalization capabilities applied different exocentric datasets,-1,0.0,-1,0.0
temporal divideandconquer anomaly actions localization semisupervised videos hierarchical transformer anomaly action detection localization play essential role security advanced surveillance systems however due tremendous amount surveillance videos available data task unlabeled semilabeled video class known location anomaly event unknown work target anomaly localization semisupervised videos mainstream direction addressing task focused segmentlevel multiinstance learning generation pseudo labels aim explore promising yet unfulfilled direction solve problem learning temporal relations within videos order locate anomaly events end propose hierarchical transformer model designed evaluate significance observed actions anomalous videos divideandconquer strategy along temporal axis approach segments parent video hierarchically multiple temporal children instances measures influence children nodes classifying abnormality parent video evaluating model two wellknown anomaly detection datasets ucfcrime shanghaitech proves ability interpret observed actions within videos localize anomalous ones proposed approach outperforms previous works relying segmentlevel multipleinstance learning approaches reaching promising performance compared recent pseudolabelingbased approaches,7,1.0,7,1.0
prompting videolanguage foundation models domainspecific finegrained heuristics video question answering video question answering videoqa represents crucial intersection video understanding language processing requiring discriminative unimodal comprehension sophisticated crossmodal interaction accurate inference despite advancements multimodal pretrained models videolanguage foundation models systems often struggle domainspecific videoqa due generalized pretraining objectives addressing gap necessitates bridging divide broad crossmodal knowledge specific inference demands videoqa tasks end introduce heurvidqa framework leverages domainspecific entityaction heuristics refine pretrained videolanguage foundation models approach treats models implicit knowledge engines employing domainspecific entityaction prompters direct models focus toward precise cues enhance reasoning delivering finegrained heuristics improve models ability identify interpret key entities actions thereby enhancing reasoning capabilities extensive evaluations across multiple videoqa datasets demonstrate method significantly outperforms existing models underscoring importance integrating domainspecific knowledge videolanguage models accurate contextaware videoqa,-1,0.0,-1,0.0
pseudolabeling keyword refining fewsupervised video captioning video captioning generate sentence describes video content existing methods always require number captions eg per video train model quite costly work explore possibility using one groundtruth sentences introduce new task named fewsupervised video captioning specifically propose fewsupervised video captioning framework consists lexically constrained pseudolabeling module keywordrefined captioning module unlike random sampling natural language processing may cause invalid modifications ie edit words former module guides model edit words using actions eg copy replace insert delete pretrained tokenlevel classifier finetunes candidate sentences pretrained language model meanwhile former employs repetition penalized sampling encourage model yield concise pseudolabeled sentences less repetition selects relevant sentences upon pretrained videotext model moreover keep semantic consistency pseudolabeled sentences video content develop transformerbased keyword refiner videokeyword gated fusion strategy emphasize relevant words extensive experiments several benchmarks demonstrate advantages proposed approach fewsupervised fullysupervised scenarios code implementation available,-1,0.0,-1,0.0
vidmorp video moment retrieval pretraining unlabeled videos wild given natural language query video moment retrieval aims localize described temporal moment untrimmed video major challenge task heavy dependence laborintensive annotations training unlike existing works directly train models manually curated data propose novel paradigm reduce annotation costs pretraining model unlabeled realworld videos support introduce video moment retrieval pretraining vidmorp largescale dataset collected minimal human intervention consisting videos captured wild pseudo annotations direct pretraining imperfect pseudo annotations however presents significant challenges including mismatched sentencevideo pairs imprecise temporal boundaries address issues propose recorrect algorithm comprises two main phases semanticsguided refinement memoryconsensus correction semanticsguided refinement enhances pseudo labels leveraging semantic similarity video frames clean unpaired data make initial adjustments temporal boundaries following memoryconsensus correction phase memory bank tracks model predictions progressively correcting temporal boundaries based consensus within memory comprehensive experiments demonstrate recorrects strong generalization abilities across multiple downstream settings zeroshot recorrect achieves best fullysupervised performance two benchmarks unsupervised recorrect reaches code dataset pretrained models available httpsgithubcombaopjvidmorp,7,0.9243588794223389,7,0.9243588794223389
semisupervised contrastive learning controllable videotomusic retrieval content creators often use music enhance videos soundtracks movies background music video blogs social media content however identifying best music video difficult timeconsuming task address challenge propose novel framework automatically retrieving matching music clip given video vice versa approach leverages annotated music labels well inherent artistic correspondence visual music elements distinct previous crossmodal music retrieval works method combines selfsupervised supervised training objectives use selfsupervised labelsupervised contrastive learning train joint embedding space music video show effectiveness approach using music genre labels supervised training component framework generalized music annotations eg emotion instrument etc furthermore method enables finegrained control much retrieval process focuses selfsupervised vs label information inference time evaluate learned embeddings variety videotomusic musictovideo retrieval tasks experiments show proposed approach successfully combines selfsupervised supervised objectives effective controllable musicvideo retrieval,8,0.44524913657388193,8,0.44524913657388193
stitch contrast human action segmentation model using trimmed skeleton videos existing skeletonbased human action classification models rely welltrimmed actionspecific skeleton videos training testing precluding scalability realworld applications untrimmed videos exhibiting concatenated actions predominant overcome limitation recently introduced skeleton action segmentation models involve untrimmed skeleton videos endtoend training model optimized provide framewise predictions length testing videos simultaneously realizing action localization classification yet achieving improvement imposes framewise annotated skeleton videos remains timeconsuming practice paper features novel framework skeletonbased action segmentation trained short trimmed skeleton videos run longer untrimmed videos approach implemented three steps stitch contrast segment first stitch proposes temporal skeleton stitching scheme treats trimmed skeleton videos elementary human motions compose semantic space sampled generate multiaction stitched sequences contrast learns contrastive representations stitched sequences novel discrimination pretext task enables skeleton encoder learn meaningful actiontemporal contexts improve action segmentation finally segment relates proposed method action segmentation learning segmentation layer handling particular data availability experiments involve trimmed source dataset untrimmed target dataset adaptation formulation realworld skeletonbased human action segmentation evaluate effectiveness proposed method,7,1.0,7,1.0
friendsqa new largescale deep video understanding dataset finegrained topic categorization story videos video question answering videoqa aims answer natural language questions according given videos although existing models perform well factoid videoqa task still face challenges deep video understanding dvu task focuses story videos compared factoid videos significant feature story videos storylines composed complex interactions longrange evolvement core story topics including characters actions locations understanding topics requires models possess dvu capability however existing dvu datasets rarely organize questions according story topics making difficult comprehensively assess videoqa models dvu capability complex storylines additionally question quantity video length dataset limited high labor costs handcrafted dataset building method paper devise large language model based multiagent collaboration framework storymind automatically generate new largescale dvu dataset dataset friendsqa derived renowned sitcom friends average episode length seconds contains questions evenly distributed across finegrained topics finally conduct comprehensive experiments stateoftheart videoqa models using friendsqa dataset,0,0.9238696206967387,0,0.9238696206967387
diffusionact controllable diffusion autoencoder oneshot face reenactment videodriven neural face reenactment aims synthesize realistic facial images successfully preserve identity appearance source face transferring target head pose facial expressions existing ganbased methods suffer either distortions visual artifacts poor reconstruction quality ie background several important appearance details hair stylecolor glasses accessories faithfully reconstructed recent advances diffusion probabilistic models dpms enable generation highquality realistic images end paper present diffusionact novel method leverages photorealistic image generation diffusion models perform neural face reenactment specifically propose control semantic space diffusion autoencoder diffae order edit facial pose input images defined head pose orientation facial expressions method allows oneshot self crosssubject reenactment without requiring subjectspecific finetuning compare stateoftheart gan diffusionbased methods showing better onpar reenactment performance,6,0.8034074728787646,6,0.8034074728787646
towards generalizable tumor synthesis tumor synthesis enables creation artificial tumors medical images facilitating training ai models tumor detection segmentation however success tumor synthesis hinges creating visually realistic tumors generalizable across multiple organs furthermore resulting ai models capable detecting real tumors images sourced different domains eg hospitals paper made progressive stride toward generalizable tumor synthesis leveraging critical observation earlystage tumors tend similar imaging characteristics computed tomography ct whether originate liver pancreas kidneys ascertained generative ai models eg diffusion models create realistic tumors generalized range organs even trained limited number tumor examples one organ moreover shown ai models trained synthetic tumors generalized detect segment real tumors ct volumes encompassing broad spectrum patient demographics imaging protocols healthcare facilities,3,0.6280135331868956,3,0.6280135331868956
generalizing deepfake video detection plugandplay videolevel blending spatiotemporal adapter tuning three key challenges hinder development current deepfake video detection temporal features complex diverse identify general temporal artifacts enhance model generalization spatiotemporal models often lean heavily one type artifact ignore ensure balanced learning videos naturally resourceintensive tackle efficiency without compromising accuracy paper attempts tackle three challenges jointly first inspired notable generality using imagelevel blending data image forgery detection investigate whether videolevel blending effective video perform thorough analysis identify previously underexplored temporal forgery artifact facial feature drift ffd commonly exists across different forgeries reproduce ffd propose novel videolevel blending data vb vb implemented blending original image warped version framebyframe serving hard negative sample mine general artifacts second carefully design lightweight spatiotemporal adapter sta equip pretrained image model vits cnns ability capture spatial temporal features jointly efficiently sta designed twostream varying kernel sizes allowing process spatial temporal features separately extensive experiments validate effectiveness proposed methods show approach generalize well previously unseen forgery videos even latest generation methods,-1,0.0,-1,0.0
bytheway boost texttovideo generation model higher quality trainingfree way texttovideo generation models offering convenient visual creation recently garnered increasing attention despite substantial potential generated videos may present artifacts including structural implausibility temporal inconsistency lack motion often resulting nearstatic video work identified correlation disparity temporal attention maps across different blocks occurrence temporal inconsistencies additionally observed energy contained within temporal attention maps directly related magnitude motion amplitude generated videos based observations present bytheway trainingfree method improve quality texttovideo generation without introducing additional parameters augmenting memory sampling time specifically bytheway composed two principal components temporal selfguidance improves structural plausibility temporal consistency generated videos reducing disparity temporal attention maps across various decoder blocks fourierbased motion enhancement enhances magnitude richness motion amplifying energy map extensive experiments demonstrate bytheway significantly improves quality texttovideo generation negligible additional cost,-1,0.0,-1,0.0
pyramidal flow matching efficient video generative modeling video generation requires modeling vast spatiotemporal space demands significant computational resources data usage reduce complexity prevailing approaches employ cascaded architecture avoid direct training full resolution latent despite reducing computational demands separate optimization substage hinders knowledge sharing sacrifices flexibility work introduces unified pyramidal flow matching algorithm reinterprets original denoising trajectory series pyramid stages final stage operates full resolution thereby enabling efficient video generative modeling sophisticated design flows different pyramid stages interlinked maintain continuity moreover craft autoregressive video generation temporal pyramid compress fullresolution history entire framework optimized endtoend manner single unified diffusion transformer dit extensive experiments demonstrate method supports generating highquality videos resolution fps within gpu training hours code models opensourced httpspyramidflowgithubio,-1,0.0,-1,0.0
anchorcrafter animate cyberanchors saling products via humanobject interacting video generation automatic generation anchorstyle product promotion videos presents promising opportunities online commerce advertising consumer engagement however remains challenging task despite significant advancements poseguided human video generation addressing challenge identify integration humanobject interactions hoi poseguided human video generation core issue end introduce anchorcrafter novel diffusionbased system designed generate videos featuring target human customized object achieving high visual fidelity controllable interactions specifically propose two key innovations hoiappearance perception enhances object appearance recognition arbitrary multiview perspectives disentangles object human appearance hoimotion injection enables complex humanobject interactions overcoming challenges object trajectory conditioning interocclusion management additionally introduce hoiregion reweighting loss training objective enhances learning object details extensive experiments demonstrate proposed system outperforms existing methods preserving object appearance shape awareness simultaneously maintaining consistency human appearance motion project page httpscangczgithubioanchorcrafter,11,0.9285146339446815,11,0.9285146339446815
dispose disentangling pose guidance controllable human image animation controllable human image animation aims generate videos reference images using driving videos due limited control signals provided sparse guidance eg skeleton pose recent works attempted introduce additional dense conditions eg depth map ensure motion alignment however strict dense guidance impairs quality generated video body shape reference character differs significantly driving video paper present dispose mine generalizable effective control signals without additional dense input disentangles sparse skeleton pose human image animation motion field guidance keypoint correspondence specifically generate dense motion field sparse motion field reference image provides regionlevel dense guidance maintaining generalization sparse pose control also extract diffusion features corresponding pose keypoints reference image point features transferred target pose provide distinct identity information seamlessly integrate existing models propose plugandplay hybrid controlnet improves quality consistency generated videos freezing existing model parameters extensive qualitative quantitative experiments demonstrate superiority dispose compared current methods project page hrefhttpsgithubcomlihxxxdisposehttpsgithubcomlihxxxdispose,-1,0.0,-1,0.0
spatiotemporal style transfer algorithm dynamic visual stimulus generation understanding visual information encoded biological artificial systems often requires vision scientists generate appropriate stimuli test specific hypotheses although deep neural network models revolutionized field image generation methods image style transfer available methods video generation scarce introduce spatiotemporal style transfer stst algorithm dynamic visual stimulus generation framework allows powerful manipulation synthesis video stimuli vision research based twostream deep neural network model factorizes spatial temporal features generate dynamic visual stimuli whose model layer activations matched input videos example show algorithm enables generation model metamers dynamic stimuli whose layer activations within twostream model matched natural videos show generated stimuli match lowlevel spatiotemporal features natural counterparts lack highlevel semantic features making powerful paradigm study object recognition late layer activations deep vision models exhibited lower similarity natural metameric stimuli compared early layers confirming lack highlevel information generated stimuli finally use generated stimuli probe representational capabilities predictive coding deep networks results showcase potential applications algorithm versatile tool dynamic stimulus generation vision science,-1,0.0,-1,0.0
exploiting style latent flows generalizing deepfake video detection paper presents new approach detection fake videos based analysis style latent vectors abnormal behavior temporal changes generated videos discovered generated facial videos suffer temporal distinctiveness temporal changes style latent vectors inevitable generation temporally stable videos various facial expressions geometric transformations framework utilizes stylegru module trained contrastive learning represent dynamic properties style latent vectors additionally introduce style attention module integrates stylegrugenerated features contentbased features enabling detection visual temporal artifacts demonstrate approach across various benchmark scenarios deepfake detection showing superiority crossdataset crossmanipulation scenarios analysis also validate importance using temporal changes style latent vectors improve generality deepfake video detection,-1,0.0,-1,0.0
survey long video generation challenges methods prospects video generation rapidly advancing research area garnering significant attention due broad range applications one critical aspect field generation longduration videos presents unique challenges opportunities paper presents first survey recent advancements long video generation summarises two key paradigms divide conquer temporal autoregressive delve common models employed paradigm including aspects network design conditioning techniques furthermore offer comprehensive overview classification datasets evaluation metrics crucial advancing long video generation research concluding summary existing studies also discuss emerging challenges future directions dynamic field hope survey serve essential reference researchers practitioners realm long video generation,10,0.969542803196847,10,0.969542803196847
animating past reconstruct trilobite via video generation paleontology study past life fundamentally relies fossils reconstruct ancient ecosystems understand evolutionary dynamics trilobites important group extinct marine arthropods offer valuable insights paleozoic environments wellpreserved fossil records reconstructing trilobite behaviour static fossils set new standards dynamic reconstructions scientific research education despite potential current computational methods purpose like texttovideo face significant challenges maintaining visual realism consistency hinder application science contexts overcome obstacles introduce automatic prompt learning method within framework prompts finetuned video generation model generated large language model trained using rewards quantify visual realism smoothness generated video finetuning video generation model along reward calculations make use collected dataset eoredlichia intermedia fossil images provides common representative visual details class trilobites qualitative quantitative experiments show method generate trilobite videos significantly higher visual realism compared powerful baselines promising boost scientific understanding public engagement,-1,0.0,-1,0.0
retta retrievalenhanced testtime adaptation zeroshot video captioning despite significant progress fullysupervised video captioning zeroshot methods remain much less explored paper propose novel zeroshot video captioning framework named retrievalenhanced testtime adaptation retta takes advantage existing pretrained largescale vision language models directly generate captions testtime adaptation specifically bridge video text using four key models general videotext retrieval model xclip general imagetext matching model clip text alignment model angle text generation model due sourcecode availability main challenge enable text generation model sufficiently aware content given video generate corresponding captions address problem propose using learnable tokens communication medium among four frozen models xclip clip angle different conventional way trains tokens training data propose learn tokens soft targets inference data several carefully crafted loss functions enable tokens absorb video information catered procedure efficiently done iterations use iterations experiments require ground truth data extensive experimental results three widely used datasets msrvtt msvd vatex show absolute improvements terms main metric cider compared several stateoftheart zeroshot video captioning methods,-1,0.0,-1,0.0
llavasurg towards multimodal surgical assistant via structured surgical video learning multimodal large language models llms achieved notable success across various domains research medical field largely focused unimodal images meanwhile current generaldomain multimodal models videos still lack capabilities understand engage conversations surgical videos one major contributing factor absence datasets surgical field paper create new dataset surgqa consisting surgical videoinstruction pairs largest kind far build dataset propose novel twostage questionanswer generation pipeline llm learn surgical knowledge structured manner publicly available surgical lecture videos pipeline breaks generation process two stages significantly reduce task complexity allowing us use affordable locally deployed opensource llm premium paid llm services also mitigates risk llm hallucinations questionanswer generation thereby enhancing overall quality generated data train llavasurg novel visionlanguage conversational assistant capable answering openended questions surgical videos surgqa dataset conduct comprehensive evaluations zeroshot surgical video questionanswering tasks show llavasurg significantly outperforms previous generaldomain models demonstrating exceptional multimodal conversational skills answering openended questions surgical videos release code model instructiontuning dataset,-1,0.0,-1,0.0
ecisvqg generation entitycentric informationseeking questions videos previous studies question generation videos mostly focused generating questions common objects attributes hence entitycentric work focus generation entitycentric informationseeking questions videos system could useful videobased learning recommending people also ask questions videobased chatbots factchecking work addresses three key challenges identifying questionworthy information linking entities effectively utilizing multimodal signals best knowledge exist largescale dataset task video question generation datasets tv shows movies human activities lack entitycentric informationseeking questions hence contribute diverse dataset youtube videos videoquestions consisting videos manually annotated questions propose model architecture combining transformers rich context signals titles transcripts captions embeddings combination crossentropy contrastive loss function encourage entitycentric question generation best method yields bleu rouge cider meteor scores respectively demonstrating practical usability make code dataset publicly available httpsgithubcomthephukanecisvqg,0,0.9742376648321287,0,0.9742376648321287
personalvideo high idfidelity video customization without dynamic semantic degradation current texttovideo generation made significant progress synthesizing realistic general videos still underexplored identityspecific human video generation customized id images key challenge lies maintaining high id fidelity consistently preserving original motion dynamic semantic following identity injection current video identity customization methods mainly rely reconstructing given identity images texttoimage models divergent distribution model process introduces tuninginference gap leading dynamic semantic degradation tackle problem propose novel framework dubbed textbfpersonalvideo applies mixture reward supervision synthesized videos instead simple reconstruction objective images specifically first incorporate identity consistency reward effectively inject references identity without tuninginference gap propose novel semantic consistency reward align semantic distribution generated videos original model preserves dynamic semantic following capability identity injection nonreconstructive reward training employ simulated prompt augmentation reduce overfitting supervising generated results semantic scenarios gaining good robustness even single reference image extensive experiments demonstrate methods superiority delivering high identity faithfulness preserving inherent video generation qualities original model outshining prior methods,-1,0.0,-1,0.0
virtual avatar generation models world navigators introduce sabrclimb novel video model simulating human movement rock climbing environments using virtual avatar diffusion transformer predicts sample instead noise diffusion step ingests entire videos output complete motion sequences leveraging large proprietary dataset substantial computational resources showcase proof concept system train generalpurpose virtual avatars complex tasks robotics sports healthcare,-1,0.0,-1,0.0
chronomagicbench benchmark metamorphic evaluation texttotimelapse video generation propose novel texttovideo generation benchmark chronomagicbench evaluate temporal metamorphic capabilities models eg sora lumiere timelapse video generation contrast existing benchmarks focus visual quality textual relevance generated videos chronomagicbench focuses models ability generate timelapse videos significant metamorphic amplitude temporal coherence benchmark probes models physics biology chemistry capabilities freeform text query purposes chronomagicbench introduces prompts realworld videos references categorized four major types timelapse videos biological humancreated meteorological physical phenomena divided subcategories categorization comprehensively evaluates models capacity handle diverse complex transformations accurately align human preference benchmark introduce two new automatic metrics mtscore chscore evaluate videos metamorphic attributes temporal coherence mtscore measures metamorphic amplitude reflecting degree change time chscore assesses temporal coherence ensuring generated videos maintain logical progression continuity based chronomagicbench conduct comprehensive manual evaluations ten representative models revealing strengths weaknesses across different categories prompts providing thorough evaluation framework addresses current gaps video generation research moreover create largescale chronomagicpro dataset containing highquality pairs timelapse videos detailed captions ensuring high physical pertinence large metamorphic amplitude homepagehttpspkuyuangroupgithubiochronomagicbench,-1,0.0,-1,0.0
towards scene graph anticipation spatiotemporal scene graphs represent interactions video decomposing scenes individual objects pairwise temporal relationships longterm anticipation finegrained pairwise relationships objects challenging problem end introduce task scene graph anticipation sga adapt stateoftheart scene graph generation methods baselines anticipate future pairwise relationships objects propose novel approach scenesayer scenesayer leverage objectcentric representations relationships reason observed video frames model evolution relationships objects take continuous time perspective model latent dynamics evolution object interactions using concepts neuralode neuralsde respectively infer representations future relationships solving ordinary differential equation stochastic differential equation respectively extensive experimentation action genome dataset validates efficacy proposed methods,-1,0.0,-1,0.0
adversarial diffusion compression realworld image superresolution realworld image superresolution realisr aims reconstruct highresolution images lowresolution inputs degraded complex unknown processes many stable diffusion sdbased realisr methods achieved remarkable success slow multistep inference hinders practical deployment recent sdbased onestep networks like osediff alleviate issue still incur high computational costs due reliance large pretrained sd models paper proposes novel realisr method adcsr distilling onestep diffusion network osediff streamlined diffusiongan model adversarial diffusion compression adc framework meticulously examine modules osediff categorizing two types removable vae encoder prompt extractor text encoder etc prunable denoising unet vae decoder since direct removal pruning degrade models generation capability pretrain pruned vae decoder restore ability decode images employ adversarial distillation compensate performance loss adcbased diffusiongan hybrid design effectively reduces complexity inference time computation parameters preserving models generation capability experiments manifest proposed adcsr achieves competitive recovery quality synthetic realworld datasets offering speedup previous onestep diffusionbased methods code models available,-1,0.0,-1,0.0
breaking quality bottleneck video consistency model mixed reward feedback diffusionbased texttovideo models achieved significant success continue hampered slow sampling speed iterative sampling processes address challenge consistency models proposed facilitate fast inference albeit cost sample quality work aim break quality bottleneck video consistency model vcm achieve textbfboth fast highquality video generation introduce integrates feedback mixture differentiable reward models consistency distillation cd process pretrained model notably directly optimize rewards associated singlestep generations arise naturally computing cd loss effectively bypassing memory constraints imposed backpropagating gradients iterative sampling process remarkably generations achieve highest total score vbench even surpassing pika conduct human evaluations corroborate results validating generations preferred ddim samples teacher models representing tenfold acceleration improving video generation quality,-1,0.0,-1,0.0
lefusion controllable pathology synthesis via lesionfocused diffusion models patient data realworld clinical practice often suffers data scarcity longtail imbalances leading biased outcomes algorithmic unfairness study addresses challenges generating lesioncontaining imagesegmentation pairs lesionfree images previous efforts medical imaging synthesis struggled separating lesion information background resulting lowquality backgrounds limited control synthetic output inspired diffusionbased image inpainting propose lefusion lesionfocused diffusion model redesigning diffusion learning objectives focus lesion areas simplify learning process improve control output preserving highfidelity backgrounds integrating forwarddiffused background contexts reverse diffusion process additionally tackle two major challenges lesion texture synthesis multipeak multiclass lesions introduce two effective strategies histogrambased texture control multichannel decomposition enabling controlled generation highquality lesions difficult scenarios furthermore incorporate lesion mask diffusion allowing control lesion size location boundary thus increasing lesion diversity validated cardiac lesion mri lung nodule ct datasets lefusiongenerated data significantly improves performance stateoftheart segmentation models including nnunet swinunetr code model available,3,0.5893304712259849,3,0.5893304712259849
synchronous synthesis cospeech affective face body expressions affordable inputs present multimodal learningbased method simultaneously synthesize cospeech facial expressions upperbody gestures digital characters using rgb video data captured using commodity cameras approach learns sparse face landmarks upperbody joints estimated directly video data generate plausible emotive character motions given speech audio waveform token sequence speakers face landmark motion bodyjoint motion computed video method synthesizes motion sequences speakers face landmarks body joints match content affect speech design generator consisting set encoders transform inputs multimodal embedding space capturing correlations followed pair decoders synthesize desired face pose motions enhance plausibility synthesis use adversarial discriminator learns differentiate face pose motions computed original videos synthesized motions based affective expressions evaluate approach extend ted gesture dataset include viewnormalized cospeech face landmarks addition body gestures demonstrate performance method thorough quantitative qualitative experiments multiple evaluation metrics via user study observe method results low reconstruction error produces synthesized samples diverse facial expressions body gestures digital characters,6,0.9350883106845811,6,0.9350883106845811
mmmrs multimodal multigsd multiscene remote sensing dataset benchmark texttoimage generation recently diffusionbased generative paradigm achieved impressive general image generation capabilities text prompts due accurate distribution modeling stable training process however generating diverse remote sensing rs images tremendously different general images terms scale perspective remains formidable challenge due lack comprehensive remote sensing image generation dataset various modalities ground sample distances gsd scenes paper propose multimodal multigsd multiscene remote sensing mmmrs dataset benchmark texttoimage generation diverse remote sensing scenarios specifically first collect nine publicly available rs datasets conduct standardization samples bridge rs images textual semantic information utilize largescale pretrained visionlanguage model automatically output text prompts perform handcrafted rectification resulting informationrich textimage pairs including multimodal images particular design methods obtain images different gsd various environments eg lowlight foggy single sample extensive manual screening refining annotations ultimately obtain mmmrs dataset comprises approximately million textimage pairs extensive experimental results verify proposed mmmrs dataset allows offtheshelf diffusion models generate diverse rs images across various modalities scenes weather conditions gsd dataset available,-1,0.0,-1,0.0
learning temporally consistent video depth video diffusion priors work addresses challenge streamed video depth estimation expects perframe accuracy importantly crossframe consistency argue sharing contextual information frames clips pivotal fostering temporal consistency thus instead directly developing depth estimator scratch reformulate predictive task conditional generation problem provide contextual information within clip across clips specifically propose consistent contextaware training inference strategy arbitrarily long videos provide crossclip context sample independent noise levels frame within clip training using sliding window strategy initializing overlapping frames previously predicted frames without adding noise moreover design effective training strategy provide context within clip extensive experimental results validate design choices demonstrate superiority approach dubbed chronodepth project page httpsxdimlabgithubiochronodepth,-1,0.0,-1,0.0
sora generates videos stunning geometrical consistency recently developed sora model exhibited remarkable capabilities video generation sparking intense discussions regarding ability simulate realworld phenomena despite growing popularity lack established metrics evaluate fidelity realworld physics quantitatively paper introduce new benchmark assesses quality generated videos based adherence realworld physics principles employ method transforms generated videos models leveraging premise accuracy reconstruction heavily contingent video quality perspective reconstruction use fidelity geometric constraints satisfied constructed models proxy gauge extent generated videos conform realworld physics rules project page httpssorageometricalconsistencygithubio,-1,0.0,-1,0.0
drivescape towards highresolution controllable multiview driving video generation recent advancements generative models provided promising solutions synthesizing realistic driving videos crucial training autonomous driving perception models however existing approaches often struggle multiview video generation due challenges integrating information maintaining spatialtemporal consistency effectively learning unified model propose drivescape endtoend framework multiview conditionguided video generation capable producing x highresolution videos unlike methods limited due box annotation frame rate drivescape overcomes ability operate sparse conditions bidirectional modulated transformer bimot ensures precise alignment structural information maintaining spatialtemporal consistency drivescape excels video generation performance achieving stateoftheart results nuscenes dataset fid score fvd score project homepage,16,1.0,16,1.0
stillmoving customized video generation without customized video data customizing texttoimage models seen tremendous progress recently particularly areas personalization stylization conditional generation however expanding progress video generation still infancy primarily due lack customized video data work introduce stillmoving novel generic framework customizing texttovideo model without requiring customized video data framework applies prominent design video model built texttoimage model eg via inflation assume access customized version model trained still image data eg using dreambooth styledrop naively plugging weights customized model model often leads significant artifacts insufficient adherence customization data overcome issue train lightweight textitspatial adapters adjust features produced injected layers importantly adapters trained textitfrozen videos ie repeated images constructed image samples generated customized model training facilitated novel textitmotion adapter module allows us train static videos preserving motion prior video model test time remove motion adapter modules leave trained spatial adapters restores motion prior model adhering spatial prior customized model demonstrate effectiveness approach diverse tasks including personalized stylized conditional generation evaluated scenarios method seamlessly integrates spatial prior customized model motion prior supplied model,-1,0.0,-1,0.0
explainable deepfake video detection using convolutional neural network capsulenet deepfake technology derived deep learning seamlessly inserts individuals digital media irrespective actual participation foundation lies machine learning artificial intelligence ai initially deepfakes served research industry entertainment concept existed decades recent advancements render deepfakes nearly indistinguishable reality accessibility soared empowering even novices create convincing deepfakes however accessibility raises security concernsthe primary deepfake creation algorithm gan generative adversarial network employs machine learning craft realistic images videos objective utilize cnn convolutional neural network capsulenet lstm differentiate deepfakegenerated frames originals furthermore aim elucidate models decisionmaking process explainable ai fostering transparent humanai relationships offering practical examples reallife scenarios,-1,0.0,-1,0.0
unsupervised sign language translation generation motivated success unsupervised neural machine translation unmt introduce unsupervised sign language translation generation network uslnet learns abundant singlemodality text video data without parallel sign language data uslnet comprises two main components singlemodality reconstruction modules text video rebuild input noisy version modality crossmodality backtranslation modules textvideotext videotextvideo reconstruct input noisy version different modality using backtranslation procedureunlike singlemodality backtranslation procedure textbased unmt uslnet faces crossmodality discrepancy feature representation length feature dimension mismatch text video sequences propose sliding window method address issues aligning variablelength text video sequences knowledge uslnet first unsupervised sign language translation generation model capable generating natural language text sign language video unified manner experimental results bbcoxford sign language dataset bobsl opendomain american sign language dataset openasl reveal uslnet achieves competitive results compared supervised baseline models indicating effectiveness sign language translation generation,-1,0.0,-1,0.0
vurf generalpurpose reasoning selfrefinement framework video understanding recent studies demonstrated effectiveness large language models llms reasoning modules deconstruct complex tasks manageable subtasks particularly applied visual reasoning tasks images contrast paper introduces video understanding reasoning framework vurf based reasoning power llms novel approach extend utility llms context video tasks leveraging capacity generalize minimal input output demonstrations within contextual framework harness contextual learning capabilities presenting llms pairs instructions corresponding highlevel programs generate executable visual programs video understanding enhance programs accuracy robustness implement two important strategies emphfirstly employ feedbackgeneration approach powered rectify errors programs utilizing unsupported functions emphsecondly taking motivation recent works selfrefinement llm outputs introduce iterative procedure improving quality incontext examples aligning initial outputs outputs would generated llm bound structure incontext examples results several videospecific tasks including visual qa video anticipation pose estimation multivideo qa illustrate enhancements efficacy improving performance visual programming approaches video tasks,0,0.8682276118581431,0,0.8682276118581431
configurable embodied data generation classagnostic rgbd video segmentation paper presents method generating largescale datasets improve classagnostic video segmentation across robots different form factors specifically consider question whether video segmentation models trained generic segmentation data could effective particular robot platforms robot embodiment factored data generation process answer question pipeline formulated using reconstructions eg generate segmented videos configurable based robots embodiment eg sensor type sensor placement illumination source resulting massive rgbd video panoptic segmentation dataset mvpd introduced extensive benchmarking foundation video segmentation models well support embodimentfocused research video segmentation experimental findings demonstrate using mvpd finetuning lead performance improvements transferring foundation models certain robot embodiments specific camera placements experiments also show using modalities depth images camera pose lead improvements video segmentation accuracy consistency project webpage available httpstopiparicomprojectsmvpd,-1,0.0,-1,0.0
videoguided foley sound generation multimodal controls generating sound effects videos often requires creating artistic sound effects diverge significantly reallife sources flexible control sound design address problem introduce multifoley model designed videoguided sound generation supports multimodal conditioning text audio video given silent video text prompt multifoley allows users create clean sounds eg skateboard wheels spinning without wind noise whimsical sounds eg making lions roar sound like cats meow multifoley also allows users choose reference audio sound effects sfx libraries partial videos conditioning key novelty model lies joint training internet video datasets lowquality audio professional sfx recordings enabling highquality fullbandwidth audio generation automated evaluations human studies demonstrate multifoley successfully generates synchronized highquality sounds across varied conditional inputs outperforms existing methods please see project page video results httpsificlgithubiomultifoley,8,1.0,8,1.0
eva zeroshot accurate attributes multiobject video editing current diffusionbased video editing primarily focuses local editing textiteg objectbackground editing global style editing utilizing various dense correspondences however methods often fail accurately edit foreground background simultaneously preserving original layout find crux issue stems imprecise distribution attention weights across designated regions including inaccurate texttoattribute control attention leakage tackle issue introduce eva textbfzeroshot textbfmultiattribute video editing framework tailored humancentric videos complex motions incorporate spatialtemporal layoutguided attention mechanism leverages intrinsic positive negative correspondences crossframe diffusion features avoid attention leakage utilize correspondences boost attention scores tokens within attribute across video frames limiting interactions tokens different attributes selfattention layer precise texttoattribute manipulation use discrete text embeddings focused specific layout areas within crossattention layer benefiting precise attention weight distribution eva easily generalized multiobject editing scenarios achieves accurate identity mapping extensive experiments demonstrate eva achieves stateoftheart results realworld scenarios full results provided httpsknightyxpgithubioeva,9,0.7066540206839943,9,0.7066540206839943
hiding faces plain sight defending deepfakes disrupting face detection paper investigates feasibility proactive deepfake defense framework em faceposion prevent individuals becoming victims deepfake videos sabotaging face detection motivation stems reliance deepfake methods face detectors automatically extract victim faces videos training synthesis testing face detectors malfunction extracted faces distorted incorrect subsequently disrupting training synthesis deepfake model achieve adapt various adversarial attacks dedicated design purpose thoroughly analyze feasibility based facepoison introduce em videofacepoison strategy propagates facepoison across video frames rather applying individually frame strategy largely reduce computational overhead retaining favorable attack performance method validated five face detectors extensive experiments eleven different deepfake models demonstrate effectiveness disrupting face detectors hinder deepfake generation,4,0.9468933886239709,4,0.9468933886239709
ildiff generate transparent animated stickers implicit layout distillation highquality animated stickers usually contain transparent channels often ignored current video generation models generate finegrained animated transparency channels existing methods roughly divided video matting algorithms diffusionbased algorithms methods based video matting poor performance dealing semiopen areas stickers diffusionbased methods often used model single image lead local flicker modeling animated stickers paper firstly propose ildiff method generate animated transparent channels implicit layout distillation solves problems semiopen area collapse consideration temporal information existing methods secondly create transparent animated sticker dataset tasd contains highquality samples transparent channel provide data support related fields extensive experiments demonstrate ildiff produce finer smoother transparent channels compared methods matting anything layer diffusion code dataset released link,-1,0.0,-1,0.0
motionaware latent diffusion models video frame interpolation advancement aigc video frame interpolation vfi become crucial component existing video generation frameworks attracting widespread research interest vfi task motion estimation neighboring frames plays crucial role avoiding motion ambiguity however existing vfi methods always struggle accurately predict motion information consecutive frames imprecise estimation leads blurred visually incoherent interpolated frames paper propose novel diffusion framework motionaware latent diffusion models madiff specifically designed vfi task incorporating motion priors conditional neighboring frames target interpolated frame predicted throughout diffusion sampling procedure madiff progressively refines intermediate outcomes culminating generating visually smooth realistic results extensive experiments conducted benchmark datasets demonstrate method achieves stateoftheart performance significantly outperforming existing approaches especially challenging scenarios involving dynamic textures complex motion,2,0.6962709329283329,2,0.6962709329283329
style transfer stylealigned multiview images propose simple yet effective pipeline stylizing scene harnessing power image diffusion models given nerf model reconstructed set multiview images perform style transfer refining source nerf model using stylized images generated stylealigned imagetoimage diffusion model given target style prompt first generate perceptually similar multiview images leveraging depthconditioned diffusion model attentionsharing mechanism next based stylized multiview images propose guide style transfer process sliced wasserstein loss based feature maps extracted pretrained cnn model pipeline consists decoupled steps allowing users test various prompt ideas preview stylized result proceeding nerf finetuning stage demonstrate method transfer diverse artistic styles realworld scenes competitive quality result videos also available project page,-1,0.0,-1,0.0
music consistency models consistency models exhibited remarkable capabilities facilitating efficient imagevideo generation enabling synthesis minimal sampling steps proven advantageous mitigating computational burdens associated diffusion models nevertheless application consistency models music generation remains largely unexplored address gap present music consistency models textttmusiccm leverages concept consistency models efficiently synthesize melspectrogram music clips maintaining high quality minimizing number sampling steps building upon existing texttomusic diffusion models textttmusiccm model incorporates consistency distillation adversarial discriminator training moreover find beneficial generate extended coherent music incorporating multiple diffusion processes shared constraints experimental results reveal effectiveness model terms computational efficiency fidelity naturalness notable textttmusiccm achieves seamless music synthesis mere four sampling steps eg one second per minute music clip showcasing potential realtime application,-1,0.0,-1,0.0
comprehensive taxonomy analysis talking head synthesis techniques portrait generation driving mechanisms editing talking head synthesis advanced method generating portrait videos still image driven specific content garnered widespread attention virtual reality augmented reality game production recently significant breakthroughs made introduction novel models transformer diffusion model current methods generate new content also edit generated material survey systematically reviews technology categorizing three pivotal domains portrait generation driven mechanisms editing techniques summarize milestone studies critically analyze innovations shortcomings within domain additionally organize extensive collection datasets provide thorough performance analysis current methodologies based various evaluation metrics aiming furnish clear framework robust data support future research finally explore application scenarios talking head synthesis illustrate specific cases examine potential future directions,10,0.9658208182757342,10,0.9658208182757342
diffusion implicit policy unpaired sceneaware motion synthesis human motion generation longstanding problem sceneaware motion synthesis widely researched recently due numerous applications prevailing methods rely heavily paired motionscene data whose quantity limited meanwhile difficult generalize diverse scenes trained specific ones thus propose unified framework termed diffusion implicit policy dip sceneaware motion synthesis paired motionscene data longer necessary framework disentangle humanscene interaction motion synthesis training introduce interactionbased implicit policy motion diffusion inference synthesized motion derived iterative diffusion denoising implicit policy optimization thus motion naturalness interaction plausibility maintained simultaneously proposed implicit policy optimizes intermediate noised motion gan inversion manner maintain motion continuity control keyframe poses though controlnet branch motion inpainting longterm motion synthesis introduce motion blending stable transitions multiple subtasks motions fused rotation power space translation linear space proposed method evaluated synthesized scenes shapenet furniture real scenes prox replica results show framework presents better motion naturalness interaction plausibility cuttingedge methods also indicates feasibility utilizing dip motion synthesis general tasks versatile scenes httpsjingyugonggithubiodiffusionimplicitpolicy,-1,0.0,-1,0.0
animatezoo zeroshot video generation crossspecies animation via subject alignment recent video editing advancements rely accurate pose sequences animate subjects however efforts suitable crossspecies animation due pose misalignment species example poses cat differs greatly pig due differences body structure paper present animatezoo zeroshot diffusionbased video generator address challenging crossspecies animation issue aiming accurately produce animal animations preserving background key technique used animatezoo subject alignment includes two steps first improve appearance feature extraction integrating laplacian detail booster prompttuning identity extractor components specifically designed capture essential appearance information including identity fine details second align shape features address conflicts differing subjects introducing scaleinformation remover ensures accurate crossspecies animation moreover introduce two highquality animal video datasets featuring wide variety species trained extensive datasets model capable generating videos characterized accurate movements consistent appearance highfidelity frames without need preinference finetuning prior arts required extensive experiments showcase outstanding performance method crossspecies action following tasks demonstrating exceptional shape adaptation capability project page available,-1,0.0,-1,0.0
baboonland dataset tracking primates wild automating behaviour recognition drone videos using drones track multiple individuals simultaneously natural environment powerful approach better understanding group primate behavior previous studies demonstrated possible automate classification primate behavior video data studies carried captivity groundbased cameras understand group behavior selforganization collective whole troop needs seen scale behavior seen relation natural environment ecological decisions made study presents novel dataset drone videos baboon detection tracking behavior recognition baboon detection dataset created manually annotating baboons drone videos bounding boxes tiling method subsequently applied create pyramid images various scales original resolution images resulting approximately images used baboon detection tracking dataset derived detection dataset bounding boxes assigned id throughout video process resulted half hour dense tracking data behavior recognition dataset generated converting tracks miniscenes video subregion centered animal miniscene manually annotated distinct behavior types resulting hours data benchmark results show mean average precision map detection model multiple object tracking precision mota botsort tracking algorithm micro accuracy behavior recognition model using deep learning classify wildlife behavior drone footage facilitates noninvasive insight collective behavior entire group,-1,0.0,-1,0.0
histodiffusion diffusion superresolution method digital pathology comprehensive quality assessment digital pathology advanced significantly last decade whole slide images wsis encompassing vast amounts data essential accurate disease diagnosis highresolution wsis essential precise diagnosis technical limitations scanning equipment variablity slide preparation hinder obtaining images superresolution techniques enhance lowresolution images generative adversarial networks gans effective natural image superresolution tasks often struggle histopathology due overfitting mode collapse traditional evaluation metrics fall short assessing complex characteristics histopathology images necessitating robust histologyspecific evaluation methods introduce histodiffusion novel diffusionbased method specially designed generating evaluating superresolution images digital pathology includes restoration module histopathology prior controllable diffusion module generating highquality images curated two histopathology datasets proposed comprehensive evaluation strategy incorporates fullreference noreference metrics thoroughly assess quality digital pathology images comparative analyses multiple datasets stateoftheart methods reveal histodiffusion outperforms gans method offers versatile solution histopathology image superresolution capable handling multiresolution generation varied input sizes providing valuable support diagnostic processes,-1,0.0,-1,0.0
gettok genaienriched multimodal tiktok dataset documenting attempted coup peru tiktok one largest fastestgrowing social media sites world tiktok features however voice transcripts often missing important features ocr video descriptions exist introduce generative ai enriched tiktok gettok data pipeline collecting tiktok videos enriched data augmenting tiktok research api generative ai models case study collect videos attempted coup peru initiated former president pedro castillo accompanying protests data includes information videos published november march days generative ai augments collected data via transcripts tiktok videos text descriptions shown videos text displayed within video stances expressed video overall pipeline contribute better understanding online discussion multimodal setting applications generative ai especially outlining utility pipeline nonenglishlanguage social media code used produce pipeline public github repository httpsgithubcomgabbypintogettokperu,0,0.8898202405051708,0,0.8898202405051708
empirical study general video transformer adaptation remote physiological measurement remote physiological measurement rpm essential tool healthcare monitoring enables measurement physiological signs eg heart rate remote setting via physical wearables recently facial videos seen rapid advancements videobased rpms however adopting facial videos rpm clinical setting largely depends accuracy robustness work across patient populations fortunately capability stateoftheart transformer architecture general natural video understanding resulted marked improvements translated facial understanding including rpm however existing rpm methods usually need rpmspecific modules eg temporal difference convolution handcrafted feature maps although customized modules increase accuracy demonstrated robustness across datasets due customization transformer architecture use advancements made general video transformers gvt study interrogate gvt architecture empirically analyze training designs ie data preprocessing network configurations affect model performance applied rpm based structure video transformers propose configure spatiotemporal hierarchy align dense temporal information needed rpm signal feature extraction define several practical guidelines gradually adapt gvts rpm without introducing rpmspecific modules experiments demonstrate favorable results existing rpmspecific module counterparts conducted extensive experiments five datasets using intradataset crossdataset settings highlight proposed guidelines generalized video transformers robust various datasets,-1,0.0,-1,0.0
vcome verbal video composition multimodal editing effects verbal videos featuring voiceovers text overlays provide valuable content present significant challenges composition especially incorporating editing effects enhance clarity visual appeal paper introduce novel task verbal video composition editing effects task aims generate coherent visually appealing verbal videos integrating multimodal editing effects across textual visual audio categories achieve curate largescale dataset video effects compositions publicly available sources formulate task generative problem involving identification appropriate positions verbal content recommendation editing effects positions address task propose vcome general framework employs large multimodal model generate editing effects video composition specifically vcome takes multimodal video context autoregressively outputs apply effects within verbal content effects appropriate position vcome also supports promptbased control composition density style providing substantial flexibility diverse applications extensive quantitative qualitative evaluations clearly demonstrate effectiveness vcome comprehensive user study shows method produces videos professional quality efficient professional editors,15,1.0,15,1.0
percept chat adapt multimodal knowledge transfer foundation models openworld video recognition openworld video recognition challenging since traditional networks generalized well complex environment variations alternatively foundation models rich knowledge recently shown generalization power however apply knowledge fully explored openworld video recognition end propose generic knowledge transfer pipeline progressively exploits integrates external multimodal knowledge foundation models boost openworld video recognition name pca based three stages percept chat adapt first perform percept process reduce video domain gap obtain external visual knowledge second generate rich linguistic semantics external textual knowledge chat stage finally blend external multimodal knowledge adapt stage inserting multimodal knowledge adaptation modules networks conduct extensive experiments three challenging openworld video benchmarks ie tinyvirat arid qvpipe approach achieves stateoftheart performance three datasets,-1,0.0,-1,0.0
general surgery vision transformer video pretrained foundation model general surgery absence openly accessible data specialized foundation models major barrier computational research surgery toward opensource largest dataset general surgery videos todate consisting hours surgical videos including data robotic laparoscopic techniques across procedures ii propose technique video pretraining general surgery vision transformer gsvit surgical videos based forward video prediction run realtime surgical applications toward opensource code weights gsvit iii also release code weights procedurespecific finetuned versions gsvit across procedures iv demonstrate performance gsvit phase annotation task displaying improved performance stateoftheart single frame predictors,-1,0.0,-1,0.0
aigvassessor benchmarking evaluating perceptual quality texttovideo generation lmm rapid advancement large multimodal models lmms led rapid expansion artificial intelligence generated videos aigvs highlights pressing need effective video quality assessment vqa models designed specifically aigvs current vqa models generally fall short accurately assessing perceptual quality aigvs due presence unique distortions unrealistic objects unnatural movements inconsistent visual elements address challenge first present aigvqadb largescale dataset comprising aigvs generated advanced texttovideo models using diverse prompts aigvs systematic annotation pipeline including scoring ranking processes devised collects expert ratings date based aigvqadb introduce aigvassessor novel vqa model leverages spatiotemporal features lmm frameworks capture intricate quality attributes aigvs thereby accurately predicting precise video quality scores video pair preferences comprehensive experiments aigvqadb existing aigv databases aigvassessor demonstrates stateoftheart performance significantly surpassing existing scoring evaluation methods terms multiple perceptual quality dimensions,12,0.6002213088713652,12,0.6002213088713652
unveiling deep shadows survey benchmark image video shadow detection removal generation deep learning era shadows created light encounters obstacles resulting regions reduced illumination computer vision detecting removing generating shadows critical tasks improving scene understanding enhancing image quality ensuring visual consistency video editing optimizing virtual environments paper offers comprehensive survey evaluation benchmark shadow detection removal generation images videos focusing deep learning approaches past decade covers key aspects tasks deep models datasets evaluation metrics comparative results consistent experimental settings main contributions include thorough survey shadow analysis standardization experimental comparisons exploration relationships model size speed performance crossdataset generalization study identification open challenges future research directions provision publicly available resources support research field,-1,0.0,-1,0.0
generalized deepfake attribution landscape fake media creation changed introduction generative adversarial networks gan fake media creation rise rapid advances generation technology leading new challenges detecting fake media fundamental characteristic gan sensitivity parameter initialization known seeds distinct seed utilized training leads creation unique model instances resulting divergent image outputs despite employing architecture means even one gan architecture produce countless variations gan models depending seed used existing methods attributing deepfakes work well seen specific gan model training gan architectures retrained different seed methods struggle attribute fakes seed dependency issue made difficult attribute deepfakes existing methods proposed generalized deepfake attribution network gdan et attribute fake images respective gan architectures even generated retrained version gan architecture different seed crossseed finetuned version existing gan model extensive experiments crossseed finetuned data gan models show method highly effective compared existing methods provided source code validate results,-1,0.0,-1,0.0
kandinsky texttoimage synthesis multifunctional generative framework texttoimage diffusion models popular introducing image manipulation methods editing image fusion inpainting etc time imagetovideo texttovideo models also built top models present kandinsky novel model based latent diffusion achieving high level quality photorealism key feature new architecture simplicity efficiency adaptation many types generation tasks extend base model various applications create multifunctional generation system includes textguided inpaintingoutpainting image fusion textimage fusion image variations generation generation also present distilled version model evaluating inference steps reverse process without reducing image quality times faster base model deployed userfriendly demo system features tested public domain additionally released source code checkpoints kandinsky extended models human evaluations show kandinsky demonstrates one highest quality scores among open source generation systems,-1,0.0,-1,0.0
mitigating analytical variability fmri results style transfer propose novel approach improve reproducibility neuroimaging results converting statistic maps across different functional mri pipelines make assumption pipelines used compute fmri statistic maps considered style component propose use different generative models among generative adversarial networks gan diffusion models dm convert statistic maps across different pipelines explore performance multiple gan frameworks design new dm framework unsupervised multidomain styletransfer constrain generation fmri statistic maps using latent space auxiliary classifier distinguishes statistic maps different pipelines extend traditional sampling techniques used dm improve transition performance experiments demonstrate proposed methods aresuccessful pipelines indeed transferred style component providing animportant source data augmentation future medical studies,3,0.6882106458063791,3,0.6882106458063791
emag egomotion aware generalizable hand forecasting egocentric videos predicting future human behavior egocentric videos challenging critical task human intention understanding existing methods forecasting hand positions rely visual representations mainly focus handobject interactions paper investigate hand forecasting task tackle two significant issues persist existing methods hand positions future frames severely affected egomotions egocentric videos prediction based visual information tends overfit background scene textures posing challenge generalization novel scenes human behaviors solve aforementioned problems propose emag egomotionaware generalizable hand forecasting method response first problem propose method considers egomotion represented sequence homography matrices two consecutive frames leverage modalities optical flow trajectories hands interacting objects egomotions thereby alleviating second issue extensive experiments two largescale egocentric video datasets epickitchens verify effectiveness proposed method particular model outperforms prior methods intra crossdataset evaluations respectively project page httpsmasashihatanogithubioemag,-1,0.0,-1,0.0
gui action narrator action take place advent multimodal llms significantly enhanced image ocr recognition capabilities making gui automation viable reality increasing efficiency digital tasks one fundamental aspect developing gui automation system understanding primitive gui actions comprehension crucial enables agents learn user demonstrations essential element automation rigorously evaluate capabilities developed video captioning benchmark gui actions comprising diverse video captioning samples task presents unique challenges compared natural scene video captioning gui screenshots typically contain denser information natural scenes events within guis subtler occur rapidly requiring precise attention appropriate time span spatial region accurate understanding address challenges introduce gui action dataset well simple yet effective framework textbfgui narrator gui video captioning utilizes cursor visual prompt enhance interpretation highresolution screenshots specifically cursor detector trained dataset multimodal llm model mechanisms selecting keyframes key regions generates captions experimental results indicate even todays advanced multimodal models task remains highly challenging additionally evaluations show strategy effectively enhances model performance whether integrated finetuning opensource models employed prompting strategy closedsource models,-1,0.0,-1,0.0
dyblurf dynamic neural radiance fields blurry monocular video recent advancements dynamic neural radiance field methods yielded remarkable outcomes however approaches rely assumption sharp input images faced motion blur existing dynamic nerf methods often struggle generate highquality novel views paper propose dyblurf dynamic radiance field approach synthesizes sharp novel views monocular video affected motion blur account motion blur input images simultaneously capture camera trajectory object discrete cosine transform dct trajectories within scene additionally employ global crosstime rendering approach ensure consistent temporal coherence across entire scene curate dataset comprising diverse dynamic scenes specifically tailored task experimental results dataset demonstrate method outperforms existing approaches generating sharp novel views motionblurred inputs maintaining spatialtemporal consistency scene,1,1.0,1,1.0
gaspct gaussian splatting novel ct projection view synthesis present gaspct novel view synthesis scene representation method used generate novel projection views computer tomography ct scans adapt gaussian splatting framework enable novel view synthesis ct based limited sets image projections without need structure motion sfm methodologies therefore reduce total scanning duration amount radiation dose patient receives scan adapted loss function usecase encouraging stronger background foreground distinction using two sparsity promoting regularizers beta loss total variation tv loss finally initialize gaussian locations across space using uniform prior distribution brains positioning would expected within field view evaluate performance model using brain ct scans parkinsons progression markers initiative ppmi dataset demonstrate rendered novel views closely match original projection views simulated scan better performance implicit scene representations methodologies furthermore empirically observe reduced training time compared neural network based image synthesis sparseview ct image reconstruction finally memory requirements gaussian splatting representations reduced compared equivalent voxel grid image representations,-1,0.0,-1,0.0
tfoley controllable waveformdomain diffusion model temporaleventguided foley sound synthesis foley sound audio content inserted synchronously videos plays critical role user experience multimedia content recently active research foley sound synthesis leveraging advancements deep generative models however works mainly focus replicating single sound class textual sound description neglecting temporal information crucial practical applications foley sound present tfoley temporaleventguided waveform generation model foley sound synthesis tfoley generates highquality audio using two conditions sound class temporal event feature temporal conditioning devise temporal event feature novel conditioning technique named blockfilm tfoley achieves superior performance objective subjective evaluation metrics generates foley sound wellsynchronized temporal events additionally showcase tfoleys practical applications particularly scenarios involving vocal mimicry temporal event control show demo companion website,8,1.0,8,1.0
motion dreamer boundary conditional motion reasoning physically coherent video generation recent advances video generation shown promise generating future scenarios critical planning control autonomous driving embodied intelligence however realworld applications demand visually plausible predictions require reasoning object motions based explicitly defined boundary conditions initial scene image partial object motion term capability boundary conditional motion reasoning current approaches either neglect explicit userdefined motion constraints producing physically inconsistent motions conversely demand complete motion inputs rarely available practice introduce motion dreamer twostage framework explicitly separates motion reasoning visual synthesis addressing limitations approach introduces instance flow sparsetodense motion representation enabling effective integration partial userdefined motions motion inpainting strategy robustly enable reasoning motions objects extensive experiments demonstrate motion dreamer significantly outperforms existing methods achieving superior motion plausibility visual realism thus bridging gap towards practical boundary conditional motion reasoning webpage available httpsenvisionresearchgithubiomotiondreamer,18,1.0,18,1.0
lifelong learning using dynamically growing tree subnetworks domain generalization video object segmentation current stateoftheart video object segmentation models achieved great success using supervised learning massive labeled training datasets however models trained using single source domain evaluated using videos sampled source domain models evaluated using videos sampled different target domain performance degrades significantly due poor domain generalization ie inability learn multidomain sources simultaneously using traditional supervised learning paper propose dynamically growing tree subnetworks dgt learn effectively multidomain sources dgt uses novel lifelong learning technique allows model continuously effectively learn new domains without forgetting previously learned domains hence model generalize outofdomain videos proposed work evaluated using singlesource indomain traditional video object segmentation multisource indomain multisource outofdomain video object segmentation results dgt show single source indomain performance gain datasets respectively however dgt evaluated using indomain multisources results show superior performance compared stateoftheart video object segmentation lifelong learning techniques average performance increase fscore minimal catastrophic forgetting finally outofdomain experiment performance dgt better stateoftheart respectively,-1,0.0,-1,0.0
evcmf endtoend video captioning network multiscale features conventional approaches video captioning leverage variety offlineextracted features generate captions despite availability various offlinefeatureextractors offer diverse information different perspectives several limitations due fixed parameters concretely extractors solely pretrained imagevideo comprehension tasks making less adaptable video caption datasets additionally extractors capture features prior classifier pretraining task ignoring significant amount valuable shallow information furthermore employing multiple offlinefeatures may introduce redundant information address issues propose endtoend encoderdecoderbased network evcmf video captioning efficiently utilizes multiscale visual textual features generate video descriptions specifically evcmf consists three modules firstly instead relying multiple feature extractors directly feed video frames transformerbased network obtain multiscale visual features update feature extractor parameters secondly fuse multiscale features input masked encoder reduce redundancy encourage learning useful features finally utilize enhanced transformerbased decoder efficiently leverage shallow textual information generate video descriptions evaluate proposed model conduct extensive experiments benchmark datasets results demonstrate evcmf yields competitive performance compared stateoftheart methods,-1,0.0,-1,0.0
vibe texttovideo benchmark evaluating hallucination large multimodal models recent advances large multimodal models lmms expanded capabilities video understanding texttovideo models excelling generating videos textual prompts however still frequently produce hallucinated content revealing aigenerated inconsistencies introduce vibe largescale dataset hallucinated videos opensource models identify five major hallucination types vanishing subject omission error numeric variability subject dysmorphia visual incongruity using ten models generated manually annotated videos diverse ms coco captions proposed benchmark includes dataset hallucinated videos classification framework using video embeddings vibe serves critical resource evaluating reliability advancing hallucination detection establish classification baseline timesformer cnn ensemble achieving best performance accuracy score initial baselines proposed achieve modest accuracy highlights difficulty automated hallucination detection need improved methods research aims drive development robust models evaluate outputs based user preferences,-1,0.0,-1,0.0
vidmuse simple videotomusic generation framework longshortterm modeling work systematically study music generation conditioned solely video first present largescale dataset comprising videomusic pairs including various genres movie trailers advertisements documentaries furthermore propose vidmuse simple framework generating music aligned video inputs vidmuse stands producing highfidelity music acoustically semantically aligned video incorporating local global visual cues vidmuse enables creation musically coherent audio tracks consistently match video content longshortterm modeling extensive experiments vidmuse outperforms existing models terms audio quality diversity audiovisual alignment code datasets available httpsgithubcomzeyuetvidmuse,8,0.566635266169594,8,0.566635266169594
synthetic human memories aiedited images videos implant false memories distort recollection ai increasingly used enhance images videos intentionally unintentionally ai editing tools become integrated smartphones users modify animate photos realistic videos study examines impact aialtered visuals false memoriesrecollections events didnt occur deviate reality preregistered study participants divided four conditions participants viewed original images completed filler task saw stimuli corresponding assigned condition unedited images aiedited images aigenerated videos aigenerated videos aiedited images aiedited visuals significantly increased false recollections aigenerated videos aiedited images strongest effect compared control confidence false memories also highest condition compared control discuss potential applications hci therapeutic memory reframing challenges ethical legal political societal domains,-1,0.0,-1,0.0
unified framework temporal video understanding tasks development video understanding proliferation tasks cliplevel temporal video analysis including temporal action detection tad temporal action segmentation tas generic event boundary detection gebd taskspecific video understanding models exhibited outstanding performance task remains dearth unified framework capable simultaneously addressing multiple tasks promising direction next generation ai end paper propose single unified framework coined formulate output temporal video understanding tasks sequence discrete tokens unified token representation train generalist model within single architecture different video understanding tasks absence multitask learning mtl benchmarks compile comprehensive cotraining dataset borrowing datasets tad tas gebd tasks evaluate generalist model corresponding test sets three tasks demonstrating produce reasonable results various tasks achieve advantages compared singletask training framework also investigate generalization performance generalist model new datasets different tasks yields superior performance specific model,-1,0.0,-1,0.0
animateanything consistent controllable animation video generation present unified controllable video generation approach animateanything facilitates precise consistent video manipulation across various conditions including camera trajectories text prompts user motion annotations specifically carefully design multiscale control feature fusion network construct common motion representation different conditions explicitly converts control information framebyframe optical flows incorporate optical flows motion priors guide final video generation addition reduce flickering issues caused largescale motion propose frequencybased stabilization module enhance temporal coherence ensuring videos frequency domain consistency experiments demonstrate method outperforms stateoftheart approaches details videos please refer webpage,-1,0.0,-1,0.0
posttraining quantization efficient text conditional audio diffusion models denoising diffusion models emerged stateoftheart generative tasks across image audio video domains producing highquality diverse contextually relevant data however broader adoption limited high computational costs large memory footprints posttraining quantization ptq offers promising approach mitigate challenges reducing model complexity lowbandwidth parameters yet direct application ptq diffusion models degrade synthesis quality due accumulated quantization noise across multiple denoising steps particularly conditional tasks like texttoaudio synthesis work introduces novel framework quantizing audio diffusion modelsadms key contributions include coveragedriven prompt augmentation method activationaware calibration set generation algorithm textconditional adms techniques ensure comprehensive coverage audio aspects modalities preserving synthesis fidelity validate approach tango makeanaudio audioldm models textconditional audio generation extensive experiments demonstrate capability reduce model size achieving synthesis quality metrics comparable fullprecision increase fd scores show specific layers backbone network quantized weights activations without significant quality loss work paves way efficient deployment adms resourceconstrained environments,-1,0.0,-1,0.0
controllable talking face generation implicit facial keypoints editing audiodriven talking face generation garnered significant interest within domain digital human research existing methods encumbered intricate model architectures intricately dependent complicating process reediting image video inputs work present controltalk talking face generation method control face expression deformation based driven audio construct head pose facial expression including lip motion single image sequential video inputs unified manner utilizing pretrained video synthesis renderer proposing lightweight adaptation controltalk achieves precise naturalistic lip synchronization enabling quantitative control mouth opening shape experiments show method superior stateoftheart performance widely used benchmarks including hdtf mead parameterized adaptation demonstrates remarkable generalization capabilities effectively handling expression deformation across sameid crossid scenarios extending utility outofdomain portraits regardless languages code available httpsgithubcomneteasemediacontroltalk,6,0.9149043681771051,6,0.9149043681771051
stylepreserving lip sync via audioaware style reference audiodriven lip sync recently drawn significant attention due widespread application multimedia domain individuals exhibit distinct lip shapes speaking utterance attributed unique speaking styles individuals posing notable challenge audiodriven lip sync earlier methods task often bypassed modeling personalized speaking styles resulting suboptimal lip sync conforming general styles recent lip sync techniques attempt guide lip sync arbitrary audio aggregating information style reference video yet preserve speaking styles well due inaccuracy style aggregation work proposes innovative audioaware style reference scheme effectively leverages relationships input audio reference audio style reference video address stylepreserving audiodriven lip sync specifically first develop advanced transformerbased model adept predicting lip motion corresponding input audio augmented style information aggregated crossattention layers style reference video afterwards better render lip motion realistic talking face video devise conditional latent diffusion model integrating lip motion modulated convolutional layers fusing reference facial images via spatial crossattention layers extensive experiments validate efficacy proposed approach achieving precise lip sync preserving speaking styles generating highfidelity realistic talking face videos,6,1.0,6,1.0
directavideo customized video generation userdirected camera movement object motion recent texttovideo diffusion models achieved impressive progress practice users often desire ability control object motion camera movement independently customized video creation however current methods lack focus separately controlling object motion camera movement decoupled manner limits controllability flexibility texttovideo models paper introduce directavideo system allows users independently specify motions multiple objects well cameras pan zoom movements directing video propose simple yet effective strategy decoupled control object motion camera movement object motion controlled spatial crossattention modulation using models inherent priors requiring additional optimization camera movement introduce new temporal crossattention layers interpret quantitative camera movement parameters employ augmentationbased approach train layers selfsupervised manner smallscale dataset eliminating need explicit motion annotation components operate independently allowing individual combined control generalize opendomain scenarios extensive experiments demonstrate superiority effectiveness method project page code available httpsdirectavideogithubio,-1,0.0,-1,0.0
zeroshot subjectdriven video customization precise motion control recent advances customized video generation enabled users create videos tailored specific subjects motion trajectories however existing methods often require complicated testtime finetuning struggle balancing subject learning motion control limiting realworld applications paper present zeroshot video customization framework capable generating videos specific subject motion trajectory guided single image bounding box sequence respectively without need testtime finetuning specifically introduce reference attention leverages models inherent capabilities subject learning devise maskguided motion module achieve precise motion control fully utilizing robust motion signal box masks derived bounding boxes two components achieve intended functions empirically observe motion control tends dominate subject learning address propose two key designs masked reference attention integrates blended latent mask modeling scheme reference attention enhance subject representations desired positions reweighted diffusion loss differentiates contributions regions inside outside bounding boxes ensure balance subject motion control extensive experimental results newly curated dataset demonstrate outperforms stateoftheart methods subject customization motion control dataset code models made publicly available,-1,0.0,-1,0.0
mri synthesis slicebased latent diffusion models improving tumor segmentation tasks datascarce regimes despite increasing use deep learning medical image segmentation limited availability annotated training data remains major challenge due timeconsuming data acquisition privacy regulations context segmentation tasks providing medical images corresponding target masks essential however conventional data augmentation approaches mainly focus image synthesis study propose novel slicebased latent diffusion architecture designed address complexities volumetric data generation slicebyslice fashion approach extends joint distribution modeling medical images associated masks allowing simultaneous generation datascarce regimes approach mitigates computational complexity memory expensiveness typically associated diffusion models furthermore architecture conditioned tumor characteristics including size shape relative position thereby providing diverse range tumor variations experiments segmentation task using confirm effectiveness synthesized volumes masks data augmentation,3,0.8750483883489834,3,0.8750483883489834
diffsr learning radar reflectivity synthesis via diffusion model satellite observations weather radar data synthesis fill data areas ground observations missing existing methods often employ reconstructionbased approaches mse loss reconstruct radar data satellite observation however methods lead oversmoothing hinders generation highfrequency details highvalue observation areas associated convective weather address issue propose twostage diffusionbased method called diffsr first pretrain reconstruction model globalscale data obtain radar estimation synthesize radar reflectivity combining radar estimation results satellite data conditions diffusion model extensive experiments show method achieves stateoftheart sota results demonstrating ability generate highfrequency details highvalue areas,14,1.0,14,1.0
multitask sar image processing via ganbased unsupervised manipulation generative adversarial networks gans shown tremendous potential synthesizing large number realistic sar images learning patterns data distribution gans achieve image editing introducing latent codes demonstrating significant promise sar image processing compared traditional sar image processing methods editing based gan latent space control entirely unsupervised allowing image processing conducted without labeled data additionally information extracted data interpretable paper proposes novel sar image processing framework called ganbased unsupervised editing gue aiming address following two issues disentangling semantic directions gan latent space finding meaningful directions establishing comprehensive sar image processing framework achieving multiple image processing functions implementation gue decompose entangled semantic directions gan latent space training carefully designed network moreover accomplish multiple sar image processing tasks including despeckling localization auxiliary identification rotation editing single training process without form supervision extensive experiments validate effectiveness proposed method,3,0.7464754802456705,3,0.7464754802456705
gtautoact automatic datasets generation framework based game engine redevelopment action recognition current datasets action recognition tasks face limitations stemming traditional collection generation methods including constrained range action classes absence multiviewpoint recordings limited diversity poor video quality laborintensive manually collection address challenges introduce gtautoact innovative dataset generation framework leveraging game engine technology facilitate advancements action recognition gtautoact excels automatically creating largescale wellannotated datasets extensive action classes superior video quality frameworks distinctive contributions encompass innovatively transforms readily available coordinatebased human motion rotationorientated representation enhanced suitability multiple viewpoints employs dynamic segmentation interpolation rotation sequences create smooth realistic animations action offers extensively customizable animation scenes implements autonomous video capture processing pipeline featuring randomly navigating camera autotrimming labeling functionalities experimental results underscore frameworks robustness highlights potential significantly improve action recognition model training,-1,0.0,-1,0.0
tcpdm temporally consistent patch diffusion models infraredtovisible video translation infrared imaging offers resilience changing lighting conditions capturing object temperatures yet scenarios lack visual details compared daytime visible images poses significant challenge human machine interpretation paper proposes novel diffusion method dubbed temporally consistent patch diffusion models tcdpm infraredtovisible video translation method extending patch diffusion model consists two key components firstly propose semanticguided denoising leveraging strong representations foundational models method faithfully preserves semantic structure generated visible images secondly propose novel temporal blending module guide denoising trajectory ensuring temporal consistency consecutive frames experiment shows tcpdm outperforms stateoftheart methods fvd infraredtovisible video translation daytonight object detection code publicly available,-1,0.0,-1,0.0
diffsign aiassisted generation customizable sign language videos enhanced realism proliferation several streaming services recent years made possible diverse audience across world view media content movies tv shows translation dubbing services added make content accessible local audience support making content accessible people different abilities deaf hard hearing dhh community still lagging goal make media content accessible dhh community generating sign language videos synthetic signers realistic expressive using signer given media content viewed globally may limited appeal hence approach combines parametric modeling generative modeling generate realisticlooking synthetic signers customize appearance based user preferences first retarget human sign language poses sign language avatars optimizing parametric model highfidelity poses rendered avatars used condition poses synthetic signers generated using diffusionbased generative model appearance synthetic signer controlled image prompt supplied visual adapter results show sign language videos generated using approach better temporal consistency realism signing videos generated diffusion model conditioned text prompts also support multimodal prompts allow users customize appearance signer accommodate diversity eg skin tone gender approach also useful signer anonymization,-1,0.0,-1,0.0
utilizing generative adversarial networks image data augmentation classification semiconductor wafer dicing induced defects semiconductor manufacturing wafer dicing process central yet vulnerable defects significantly impair yield proportion defectfree chips deep neural networks current state art semiautomated visual inspection however notoriously known require particularly large amount data model training address challenges explore application generative adversarial networks gan image data augmentation classification semiconductor wafer dicing induced defects enhance variety balance training data visual inspection systems approach synthetic yet realistic images generated mimic realworld dicing defects employ three different gan variants highresolution image synthesis deep convolutional gan dcgan cyclegan workinprogress results demonstrate improved classification accuracies obtained showing average improvement baseline experiment dcgan experiment balanced accuracy may enable yield optimization production,14,1.0,14,1.0
mvinpainter learning multiview consistent inpainting bridge editing novel view synthesis nvs generation recently achieved prominent improvements however works mainly focus confined categories synthetic assets discouraged generalizing challenging inthewild scenes fail employed synthesis directly moreover methods heavily depended camera poses limiting realworld applications overcome issues propose mvinpainter reformulating editing multiview inpainting task specifically mvinpainter partially inpaints multiview images reference guidance rather intractably generating entirely novel view scratch largely simplifies difficulty inthewild nvs leverages unmasked clues instead explicit pose conditions ensure crossview consistency mvinpainter enhanced video priors motion components appearance guidance concatenated reference keyvalue attention furthermore mvinpainter incorporates slot attention aggregate highlevel optical flow features unmasked regions control camera movement posefree training inference sufficient scenelevel experiments objectcentric forwardfacing datasets verify effectiveness mvinpainter including diverse tasks multiview object removal synthesis insertion replacement project page httpsewrfcasgithubiomvinpainter,-1,0.0,-1,0.0
keypoint guided deformable image manipulation using diffusion model paper introduce keypointguided diffusion probabilistic model kdm gains precise control images manipulating objects keypoint propose twostage generative model incorporating optical flow map intermediate output dense pixelwise understanding semantic relation image sparse key point configured leading realistic image generation additionally integration optical flow helps regulate interframe variance sequential images demonstrating authentic sequential image generation kdm evaluated diverse keypoint conditioned image synthesis tasks including facial image generation human pose synthesis echocardiography video prediction demonstrating kdm proving consistency enhanced photorealistic images compared stateoftheart models,-1,0.0,-1,0.0
identifying solving conditional image leakage imagetovideo diffusion model diffusion models obtained substantial progress imagetovideo generation however paper find models tend generate videos less motion expected attribute issue called conditional image leakage imagetovideo diffusion models tend overrely conditional image large time steps address challenge inference training aspects first propose start generation process earlier time step avoid unreliable largetime steps well initial noise distribution optimal analytic expressions analyticinit minimizing kl divergence actual marginal distribution bridge traininginference gap second design timedependent noise distribution timenoise conditional image training applying higher noise levels larger time steps disrupt reduce models dependency validate general strategies various collected opendomain image benchmark dataset extensive results show methods outperform baselines producing higher motion scores lower errors maintaining image alignment temporal consistency thereby yielding superior overall performance enabling accurate motion control project page urlhttpscondimageleakgithubio,-1,0.0,-1,0.0
svp styleenhanced vivid portrait talking head diffusion model talking head generation thg typically driven audio important challenging task broad application prospects various fields digital humans film production virtual reality diffusion modelbased thg methods present high quality stable content generation often overlook intrinsic style encompasses personalized features speaking habits facial expressions video consequence generated video content lacks diversity vividness thus limited real life scenarios address issues propose novel framework named styleenhanced vivid portrait svp fully leverages stylerelated information thg specifically first introduce novel probabilistic style prior learning model intrinsic style gaussian distribution using facial expressions audio embedding distribution learned bespoked contrastive objective effectively capturing dynamic style information video finetune pretrained stable diffusion sd model inject learned intrinsic style controlling signal via cross attention experiments show model generates diverse vivid highquality videos flexible control intrinsic styles outperforming existing stateoftheart methods,6,0.5327433366955362,6,0.5327433366955362
towards precise scaling laws video diffusion transformers achieving optimal performance video diffusion transformers within given data compute budget crucial due high training costs necessitates precisely determining optimal model size training hyperparameters largescale training scaling laws employed language models predict performance existence accurate derivation visual generation models remain underexplored paper systematically analyze scaling laws video diffusion transformers confirm presence moreover discover unlike language models video diffusion models sensitive learning rate batch size two hyperparameters often precisely modeled address propose new scaling law predicts optimal hyperparameters model size compute budget optimal settings achieve comparable performance reduce inference costs compared conventional scaling methods within compute budget tflops furthermore establish generalized precise relationship among validation loss model size compute budget enables performance prediction nonoptimal model sizes may also appealed practical inference cost constraints achieving better tradeoff,-1,0.0,-1,0.0
instructionguided editing controls images multimedia survey llm era rapid advancement large language models llms multimodal learning transformed digital content creation manipulation traditional visual editing tools require significant expertise limiting accessibility recent strides instructionbased editing enabled intuitive interaction visual content using natural language bridge user intent complex editing operations survey provides overview techniques focusing llms multimodal models empower users achieve precise visual modifications without deep technical knowledge synthesizing publications explore methods generative adversarial networks diffusion models examining multimodal integration finegrained content control discuss practical applications across domains fashion scene manipulation video synthesis highlighting increased accessibility alignment human intuition survey compares existing literature emphasizing llmempowered editing identifies key challenges stimulate research aim democratize powerful visual editing across various industries entertainment education interested readers encouraged access repository httpsgithubcomtamlhpawesomeinstructionediting,-1,0.0,-1,0.0
dgl dynamic globallocal prompt tuning textvideo retrieval textvideo retrieval critical multimodal task find relevant video text query although pretrained models like clip demonstrated impressive potential area rising cost fully finetuning models due increasing model size continues pose problem address challenge prompt tuning emerged alternative however existing works still face two problems adapting pretrained imagetext models downstream videotext tasks visual encoder could encode framelevel features failed extract globallevel general video information equipping visual text encoder separated prompts failed mitigate visualtext modality gap end propose dgl crossmodal dynamic prompt tuning method globallocal video attention contrast previous prompt tuning methods employ shared latent space generate locallevel text frame prompts encourage intermodal interaction furthermore propose modeling video globallocal attention mechanism capture global video information perspective prompt tuning extensive experiments reveal parameters tuned crossmodal prompt tuning strategy dgl outperforms comparable fully finetuning methods msrvtt vatex lsmdc activitynet datasets code available httpsgithubcomknightyxpdgl,7,0.8449819004824173,7,0.8449819004824173
vlnvideo utilizing driving videos outdoor visionandlanguage navigation outdoor visionandlanguage navigation vln requires agent navigate realistic outdoor environments based natural language instructions performance existing vln methods limited insufficient diversity navigation environments limited training data address issues propose vlnvideo utilizes diverse outdoor environments present driving videos multiple cities us augmented automatically generated navigation instructions actions improve outdoor vln performance vlnvideo combines best intuitive classical approaches modern deep learning techniques using template infilling generate grounded navigation instructions combined image rotation similaritybased navigation action predictor obtain vln style data driving videos pretraining deep learning vln models pretrain model touchdown dataset videoaugmented dataset created driving videos three proxy tasks masked language modeling instruction trajectory matching next action prediction learn temporallyaware visuallyaligned instruction representations learned instruction representation adapted stateoftheart navigator finetuning touchdown dataset empirical results demonstrate vlnvideo significantly outperforms previous stateoftheart models task completion rate achieving new stateoftheart touchdown dataset,5,0.30302606556252437,5,0.30302606556252437
orientationconditioned facial texture mapping videobased facial remote photoplethysmography estimation camerabased remote photoplethysmography rppg enables contactless measurement important physiological signals pulse rate pr however dynamic unconstrained subject motion introduces significant variability facial appearance video confounding ability videobased methods accurately extract rppg signal study leverage facial surface construct novel orientationconditioned facial texture video representation improves motion robustness existing videobased facial rppg estimation methods proposed method achieves significant performance improvement crossdataset testing mmpd baseline using physnet model trained pure highlighting efficacy generalization benefits designed video representation demonstrate significant performance improvements tested motion scenarios crossdataset testing mmpd even presence dynamic unconstrained subject motion emphasizing benefits disentangling motion modeling facial surface motion robust facial rppg estimation validate efficacy design decisions impact different video processing steps ablation study findings illustrate potential strengths exploiting facial surface general strategy addressing dynamic unconstrained subject motion videos code available httpssamcantrillgithubioorientationuvrppg,6,0.8778918253629717,6,0.8778918253629717
contrast imitate adapt learning robotic skills raw human videos learning robotic skills raw human videos remains nontrivial challenge previous works tackled problem leveraging behavior cloning learning reward functions videos despite remarkable performances may introduce several issues necessity robot actions requirements consistent viewpoints similar layouts human robot videos well low sample efficiency end key insight learn task priors contrasting videos learn action priors imitating trajectories videos utilize task priors guide trajectories adapt novel scenarios propose threestage skill learning framework denoted contrastimitateadapt cia interactionaware alignment transformer proposed learn task priors temporally aligning video pairs trajectory generation model used learn action priors adapt novel scenarios different human videos inversioninteraction method designed initialize coarse trajectories refine limited interaction addition cia introduces optimization method based semantic directions trajectories interaction security sample efficiency alignment distances computed iaaformer used rewards evaluate cia six realworld everyday tasks empirically demonstrate cia significantly outperforms previous stateoftheart works terms task success rate generalization diverse novel scenarios layouts object instances,5,0.34928820993670573,5,0.34928820993670573
one token seg language instructed reasoning segmentation videos introduce videolisa videobased multimodal large language model designed tackle problem languageinstructed reasoning segmentation videos leveraging reasoning capabilities world knowledge large language models augmented segment anything model videolisa generates temporally consistent segmentation masks videos based language instructions existing imagebased methods lisa struggle video tasks due additional temporal dimension requires temporal dynamic understanding consistent segmentation across frames videolisa addresses challenges integrating sparse dense sampling strategy videollm balances temporal context spatial detail within computational constraints additionally propose onetokensegall approach using specially designed trk token enabling model segment track objects across multiple frames extensive evaluations diverse benchmarks including newly introduced reasonvos benchmark demonstrate videolisas superior performance video object segmentation tasks involving complex reasoning temporal understanding object tracking optimized videos videolisa also shows promising generalization image segmentation revealing potential unified foundation model languageinstructed object segmentation code model available httpsgithubcomshowlabvideolisa,-1,0.0,-1,0.0
grounding need dual temporal grounding video dialog realm video dialog response generation understanding video content temporal nuances conversation history paramount segment current research leans heavily largescale pretrained visuallanguage models often overlooks temporal dynamics another delves deep spatialtemporal relationships within videos demands intricate object trajectory preextractions sidelines dialog temporal dynamics paper introduces dual temporal groundingenhanced video dialog model dtgvd strategically designed merge strengths dominant approaches emphasizes dual temporal relationships predicting dialog turnspecific temporal regions filtering video content accordingly grounding responses video dialog contexts one standout feature dtgvd heightened attention chronological interplay recognizing acting upon dependencies different dialog turns captures nuanced conversational dynamics bolster alignment video dialog temporal dynamics weve implemented listwise contrastive learning strategy within framework accurately grounded turnclip pairings designated positive samples less precise pairings categorized negative refined classification funneled holistic endtoend response generation mechanism evaluations using datasets underscore superiority methodology,0,0.9217786694310612,0,0.9217786694310612
multipair temporal sentence grounding via multithread knowledge transfer network given videoquery pairs untrimmed videos sentence queries temporal sentence grounding tsg aims locate queryrelevant segments videos although previous respectable tsg methods achieved remarkable success train videoquery pair separately ignore relationship different pairs observe similar videoquery content helps tsg model better understand generalize crossmodal representation also assists model locating complex videoquery pairs previous methods follow singlethread framework cotrain different pairs usually spends much time reobtaining redundant knowledge limiting realworld applications end paper pose brandnew setting multipair tsg aims cotrain pairs particular propose novel videoquery cotraining approach multithread knowledge transfer network locate variety videoquery pairs effectively efficiently firstly mine spatial temporal semantics across different queries cooperate learn intra intermodal representations simultaneously design crossmodal contrast module explore semantic consistency selfsupervised strategy fully align visual textual representations different pairs design prototype alignment strategy match object prototypes phrase prototypes spatial alignment align activity prototypes sentence prototypes temporal alignment finally develop adaptive negative selection module adaptively generate threshold crossmodal matching extensive experiments show effectiveness efficiency proposed method,-1,0.0,-1,0.0
hierarchical banzhaf interaction general videolanguage representation learning multimodal representation learning contrastive learning plays important role artificial intelligence domain important subfield videolanguage representation learning focuses learning representations using global semantic interactions predefined videotext pairs however enhance refine coarsegrained global interactions detailed interactions necessary finegrained multimodal learning study introduce new approach models videotext game players using multivariate cooperative game theory handle uncertainty finegrained semantic interactions diverse granularity flexible combination vague intensity specifically design hierarchical banzhaf interaction simulate finegrained correspondence video clips textual words hierarchical perspectives furthermore mitigate bias calculations within banzhaf interaction propose reconstructing representation fusion singlemodal crossmodal components reconstructed representation ensures fine granularity comparable singlemodal representation also preserving adaptive encoding characteristics crossmodal representation additionally extend original structure flexible encoderdecoder framework enabling model adapt various downstream tasks extensive experiments commonly used textvideo retrieval videoquestion answering video captioning benchmarks superior performance validate effectiveness generalization method,-1,0.0,-1,0.0
explicit generation object without score distillation recent years increasing demand dynamic assets design gaming applications given rise powerful generative pipelines capable synthesizing highquality objects previous methods generally rely score distillation sampling sds algorithm infer unseen views motion objects thus leading unsatisfactory results defects like oversaturation janus problem therefore inspired recent progress video diffusion models propose optimize representation explicitly generating multiview videos one input image however far trivial handle practical challenges faced pipeline including dramatic temporal inconsistency interframe geometry texture diversity semantic defects brought video generation results address issues propose novel multistage framework generates highquality consistent assets without score distillation specifically collaborative techniques solutions developed including attention injection strategy synthesize temporalconsistent multiview videos robust efficient dynamic reconstruction method based gaussian splatting refinement stage diffusion prior semantic restoration qualitative results user preference study demonstrate framework outperforms baselines generation quality considerable margin code released,-1,0.0,-1,0.0
improving deep learning segmentation biophysically motivated cell synthesis biomedical research increasingly relies cell culture models aibased analysis potentially facilitate detailed accurate feature extraction singlecell level however requires precise segmentation cell datasets turn demands highquality ground truth training manual annotation gold standard ground truth data timeconsuming thus feasible generation large training datasets address present novel framework generating training data integrates biophysical modeling realistic cell shape alignment approach allows silico generation coherent membrane nuclei signals enable training segmentation models utilizing channels improved performance furthermore present new gan training scheme generates image data also matching labels quantitative evaluation shows superior performance biophysical motivated synthetic training data even outperforming manual annotation pretrained models underscores potential incorporating biophysical modeling enhancing synthetic training data quality,3,0.8440581771232976,3,0.8440581771232976
perceptual eventstovideo reconstruction using diffusion priors event cameras mimicking human retina capture brightness changes unparalleled temporal resolution dynamic range integrating events intensities poses highly illposed challenge marred initial condition ambiguities traditional regressionbased deep learning methods fall short perceptual quality offering deterministic often unrealistic reconstructions paper introduce diffusion models eventstovideo reconstruction achieving colorful realistic perceptually superior video generation achromatic events powered image generation ability knowledge pretrained diffusion models proposed method achieve better tradeoff perception distortion reconstructed frame compared previous solutions extensive experiments benchmark datasets demonstrate approach produce diverse realistic frames faithfulness given events,1,1.0,1,1.0
dip diffusion learning inconsistency pattern general deepfake detection advancement deepfake generation techniques importance deepfake detection protecting multimedia content integrity become increasingly obvious recently temporal inconsistency clues explored improve generalizability deepfake video detection according observation temporal artifacts forged videos terms motion information usually exhibits quite distinct inconsistency patterns along horizontal vertical directions could leveraged improve generalizability detectors paper transformerbased framework diffusion learning inconsistency pattern dip proposed exploits directional inconsistencies deepfake video detection specifically dip begins spatiotemporal encoder represent spatiotemporal information directional inconsistency decoder adopted accordingly directionaware attention inconsistency diffusion incorporated explore potential inconsistency patterns jointly learn inherent relationships addition spatiotemporal invariant loss sti loss introduced contrast spatiotemporally augmented sample pairs prevent model overfitting nonessential forgery artifacts extensive experiments several public datasets demonstrate method could effectively identify directional forgery clues achieve stateoftheart performance,-1,0.0,-1,0.0
physicsinformed latent diffusion multimodal brain mri synthesis recent advances generative models medical imaging shown promise representing multiple modalities however variability modality availability across datasets limits general applicability synthetic data produce address present novel physicsinformed generative model capable synthesizing variable number brain mri modalities including present original dataset approach utilizes latent diffusion models twostep generative process first unobserved physical tissue property maps synthesized using latent diffusion model maps combined physical signal model generate final mri scan experiments demonstrate efficacy approach generating unseen mr contrasts preserving physical plausibility furthermore validate distributions generated tissue properties comparing measured real brain tissue,3,0.861286002840848,3,0.861286002840848
trainingfree video temporal grounding using largescale pretrained models video temporal grounding aims identify video segments within untrimmed videos relevant given natural language query existing video temporal localization models rely specific datasets training high data collection costs exhibit poor generalization capability acrossdataset outofdistribution ood settings paper propose trainingfree video temporal grounding tfvtg approach leverages ability pretrained large models naive baseline enumerate proposals video use pretrained visual language models vlms select best proposal according visionlanguage alignment however existing vlms trained imagetext pairs trimmed video cliptext pairs making struggle grasp relationship distinguish temporal boundaries multiple events within video comprehend sensitive dynamic transition events transition one event another video address issues propose leveraging large language models llms analyze multiple subevents contained query text analyze temporal order relationships events secondly split subevent dynamic transition static status parts propose dynamic static scoring functions using vlms better evaluate relevance event description finally subevent description use vlms locate topk proposals leverage order relationships subevents provided llms filter integrate proposals method achieves best performance zeroshot video temporal grounding charadessta activitynet captions datasets without training demonstrates better generalization capabilities crossdataset ood settings,0,1.0,0,1.0
olvit multimodal state tracking via attentionbased embeddings videogrounded dialog present object language video transformer olvit novel model video dialog operating multimodal attentionbased dialog state tracker existing video dialog models struggle questions requiring spatial temporal localization within videos longterm temporal reasoning accurate object tracking across multiple dialog turns olvit addresses challenges maintaining global dialog state based output object state tracker ost language state tracker lst ost attends important objects within video lst keeps track important linguistic coreferences previous dialog turns stark contrast previous works approach generic nature therefore capable learning continuous multimodal dialog state representations relevant objects rounds result seamlessly integrated large language models llms offer high flexibility dealing different datasets tasks evaluations challenging dvd response classification simmc response generation datasets show olvit achieves new stateoftheart performance across datasets,0,0.8221414801387104,0,0.8221414801387104
cococo improving textguided video inpainting better consistency controllability compatibility recent advancements video generation remarkable yet many existing methods struggle issues consistency poor textvideo alignment moreover field lacks effective techniques textguided video inpainting stark contrast wellexplored domain textguided image inpainting end paper proposes novel textguided video inpainting model achieves better consistency controllability compatibility specifically introduce simple efficient motion capture module preserve motion consistency design instanceaware region selection instead random region selection obtain better textual controllability utilize novel strategy inject personalized models cococo model thus obtain better model compatibility extensive experiments show model generate highquality video clips meanwhile model shows better motion consistency textual controllability model compatibility details shown cococozibojiagithubiocococozibojiagithubio,-1,0.0,-1,0.0
learned scanpaths aid blind panoramic video quality assessment panoramic videos advantage providing immersive interactive viewing experience nevertheless spherical nature gives rise various uncertain user viewing behaviors poses significant challenges panoramic video quality assessment pvqa work propose endtoend optimized blind pvqa method explicit modeling user viewing patterns visual scanpaths method consists two modules scanpath generator quality assessor scanpath generator initially trained predict future scanpaths minimizing expected code length jointly optimized quality assessor quality prediction blind pvqa method enables direct quality assessment panoramic images treating videos composed identical frames experiments three public panoramic image video quality datasets encompassing synthetic authentic distortions validate superiority blind pvqa model existing methods,12,0.7871546825074391,12,0.7871546825074391
dibs enhancing dense video captioning unlabeled videos via pseudo boundary enrichment online refinement present dive boundaries dibs novel pretraining framework dense video captioning dvc elaborates improving quality generated event captions associated pseudo event boundaries unlabeled videos leveraging capabilities diverse large language models llms generate rich dvcoriented caption candidates optimize corresponding pseudo boundaries several meticulously designed objectives considering diversity eventcentricity temporal ordering coherence moreover introduce novel online boundary refinement strategy iteratively improves quality pseudo boundaries training comprehensive experiments conducted examine effectiveness proposed technique components leveraging substantial amount unlabeled video data achieve remarkable advancement standard dvc datasets like activitynet outperform previous stateoftheart across majority metrics achieving unlabeled video data used pretraining,7,0.8172818363526487,7,0.8172818363526487
video enriched retrieval augmented generation using aligned video captions work propose use aligned visual captions mechanism integrating information contained within videos retrieval augmented generation rag based chat assistant systems captions able describe visual audio content videos large corpus advantage textual format easy reason incorporate large language model llm prompts also typically require less multimedia content inserted multimodal llm context window typical configurations aggressively fill context window sampling video frames source video furthermore visual captions adapted specific use cases prompting original foundational model captioner particular visual details fine tuning hopes helping advancing progress area curate dataset describe automatic evaluation procedures common rag tasks,-1,0.0,-1,0.0
youtube sfvhdr quality dataset popularity short form videos sfv grown dramatically past years become phenomenal video category billions viewers meanwhile high dynamic range hdr advanced feature also becomes popular video sharing platforms hot topic huge impact sfv hdr bring new questions video quality research sfvhdr quality assessment significantly different traditional user generated content ugc quality assessment objective quality metrics designed traditional ugc still work well sfvhdr answer questions created first large scale sfvhdr dataset reliable subjective quality scores covering popular content categories also introduce general sampling framework maximize representativeness dataset provided comprehensive analysis subjective quality scores short form sdr hdr videos discuss reliability stateoftheart ugc quality metrics potential improvements,12,0.9794436612525277,12,0.9794436612525277
contextaware temporal embedding objects video data video analysis understanding temporal context crucial recognizing object interactions event patterns contextual changes time proposed model leverages adjacency semantic similarities objects neighboring video frames construct contextaware temporal object embeddings unlike traditional methods rely solely visual appearance temporal embedding model considers contextual relationships objects creating meaningful embedding space temporally connected objects vectors positioned proximity empirical studies demonstrate contextaware temporal embeddings used conjunction conventional visual embeddings enhance effectiveness downstream applications moreover embeddings used narrate video using large language model llm paper describes intricate details proposed objective function generate contextaware temporal object embeddings video data showcases potential applications generated embeddings video analysis object classification tasks,-1,0.0,-1,0.0
portrait video editing empowered multimodal generative priors introduce portraitgen powerful portrait video editing method achieves consistent expressive stylization multimodal prompts traditional portrait video editing methods often struggle temporal consistency typically lack rendering quality efficiency address issues lift portrait video frames unified dynamic gaussian field ensures structural temporal coherence across frames furthermore design novel neural gaussian texture mechanism enables sophisticated style editing also achieves rendering speed approach incorporates multimodal inputs knowledge distilled largescale generative models system also incorporates expression similarity guidance faceaware portrait editing module effectively mitigating degradation issues associated iterative dataset updates extensive experiments demonstrate temporal consistency editing efficiency superior rendering quality method broad applicability proposed approach demonstrated various applications including textdriven editing imagedriven editing relighting highlighting great potential advance field video editing demo videos released code provided project page,-1,0.0,-1,0.0
bridging vision language modeling causality temporality video narratives video captioning critical task field multimodal machine learning aiming generate descriptive coherent textual narratives video content large visionlanguage models lvlms shown significant progress often struggle capture causal temporal dynamics inherent complex video sequences address limitation propose enhanced framework integrates causaltemporal reasoning module ctrm stateoftheart lvlms ctrm comprises two key components causal dynamics encoder cde temporal relational learner trl collectively encode causal dependencies temporal consistency video frames design multistage learning strategy optimize model combining pretraining largescale videotext datasets finetuning causally annotated data contrastive alignment better embedding coherence experimental results standard benchmarks msvd msrvtt demonstrate method outperforms existing approaches automatic metrics cider rougel human evaluations achieving fluent coherent relevant captions results validate effectiveness approach generating captions enriched causaltemporal narratives,0,0.8698174924892307,0,0.8698174924892307
teasergen generating teasers long documentaries teasers effective tool promoting content entertainment commercial educational fields however creating effective teaser long videos challenging requires longrange multimodal modeling input videos necessitating maintaining audiovisual alignments managing scene changes preserving factual accuracy output teasers due lack publiclyavailable dataset progress along research direction hindered work present documentarynet collection documentaries paired teasers featuring multimodal data streams video speech music sound effects narrations documentarynet propose new twostage system generating teasers long documentaries proposed teasergen system first generates teaser narration transcribed narration documentary using pretrained large language model selects relevant visual content accompany generated narration languagevision models narrationvideo matching explore two approaches pretrainingbased model using pretrained contrastive languagevision models deep sequential model learns mapping narrations visuals experimental results show pretrainingbased approach effective identifying relevant visual content directly trained deep autoregressive models,0,0.9787872295190563,0,0.9787872295190563
fast dynamic object generation singleview video generating dynamic object singleview video challenging due lack labeled data intuitive approach extend previous pipelines transferring offtheshelf image generation models score distillation samplinghowever approach would slow expensive scale due need backpropagating informationlimited supervision signals large pretrained model address propose efficient object generation framework called generates highquality spacetimeconsistent images different camera views uses labeled data directly reconstruct content gaussian splatting model importantly method achieve realtime rendering continuous camera trajectories enable robust reconstruction sparse views introduce inconsistencyaware confidenceweighted loss design along lightly weighted score distillation loss extensive experiments synthetic real videos show offers remarkable increase speed compared prior art alternatives preserving quality novel view synthesis example takes minutes model dynamic object vs minutes previous art model,1,1.0,1,1.0
eprecon efficient framework realtime panoptic reconstruction monocular video panoptic reconstruction monocular video fundamental perceptual task robotic scene understanding however existing efforts suffer inefficiency terms inference speed accuracy limiting practical applicability present eprecon efficient realtime panoptic reconstruction framework current volumetricbased reconstruction methods usually utilize multiview depth map fusion obtain scene depth priors timeconsuming poses challenges realtime scene reconstruction address issue propose lightweight module directly estimate scene depth priors volume reconstruction quality improvement generating occupancy probabilities voxels addition compared existing panoptic segmentation methods eprecon extracts panoptic features voxel features corresponding image features obtaining detailed comprehensive instancelevel semantic information achieving accurate segmentation results experimental results dataset demonstrate superiority eprecon current stateoftheart methods terms panoptic reconstruction quality realtime inference code available,1,0.9448439634016286,1,0.9448439634016286
simple early exiting framework accelerated sampling diffusion models diffusion models shown remarkable performance generation problems various domains including images videos text audio practical bottleneck diffusion models sampling speed due repeated evaluation score estimation networks inference work propose novel framework capable adaptively allocating compute required score estimation thereby reducing overall sampling time diffusion models observe amount computation required score estimation may vary along time step score estimated based observation propose earlyexiting scheme skip subset parameters score estimation network inference based timedependent exit schedule using diffusion models image synthesis show method could significantly improve sampling throughput diffusion models without compromising image quality furthermore also demonstrate method seamlessly integrates various types solvers faster sampling capitalizing compatibility enhance overall efficiency source code experiments available urlhttpsgithubcomtaehongmooneediffusion,-1,0.0,-1,0.0
imageconditioned diffusion model qspace upsampling dwi data propose imageconditioned diffusion model estimate high angular resolution diffusion weighted imaging dwi low angular resolution acquisition model call takes input set low angular resolution dwi data uses information estimate dwi data associated target gradient direction leverage unet architecture crossattention preserve positional information reference images guiding target image generation train evaluate singleshell dwi samples curated human connectome project hcp dataset specifically subsample hcp gradient directions produce low angular resolution dwi data train reconstruct missing high angular resolution samples compare two stateoftheart gan models results demonstrate achieves higherquality generated images consistently outperforms gan models downstream tensor estimation across multiple metrics taken together study highlights potential diffusion models particular qspace upsampling thus offering promising toolkit clinical research applications,3,1.0,3,1.0
garfield reinforced gaussian radiance fields largescale scene reconstruction paper proposes novel framework largescale scene reconstruction based gaussian splatting aims address scalability accuracy challenges faced existing methods tackling scalability issue split large scene multiple cells candidate pointcloud camera views cell correlated visibilitybased camera selection progressive pointcloud extension reinforce rendering quality three highlighted improvements made comparison vanilla strategy raygaussian intersection novel gaussians density control learning efficiency appearance decoupling module based convkan network solve uneven lighting conditions largescale scenes refined final loss color loss depth distortion loss normal consistency loss finally seamless stitching procedure executed merge individual gaussian radiance field novel view synthesis across different cells evaluation matrixcity datasets shows method consistently generates highfidelity rendering results stateoftheart methods largescale scene reconstruction validate generalizability proposed approach rendering selfcollected video clips recorded commercial drone,1,1.0,1,1.0
instructionbased image manipulation watching things move paper introduces novel dataset construction pipeline samples pairs frames videos uses multimodal large language models mllms generate editing instructions training instructionbased image manipulation models video frames inherently preserve identity subjects scenes ensuring consistent content preservation editing additionally video data captures diverse natural dynamicssuch nonrigid subject motion complex camera movementsthat difficult model otherwise making ideal source scalable dataset construction using approach create new dataset train instructmove model capable instructionbased complex manipulations difficult achieve synthetically generated datasets model demonstrates stateoftheart performance tasks adjusting subject poses rearranging elements altering camera perspectives,-1,0.0,-1,0.0
video prediction policy generalist robot policy predictive visual representations recent advancements robotics focused developing generalist policies capable performing multiple tasks typically policies utilize pretrained vision encoders capture crucial information current observations however previous vision encoders trained twoimage contrastive learning singleimage reconstruction perfectly capture sequential information essential embodied tasks recently video diffusion models vdms demonstrated capability accurately predict future image sequences exhibiting good understanding physical dynamics motivated strong visual prediction capabilities vdms hypothesize inherently possess visual representations reflect evolution physical world term predictive visual representations building hypothesis propose video prediction policy vpp generalist robotic policy conditioned predictive visual representations vdms enhance representations incorporate diverse human robotic manipulation datasets employing unified videogeneration training objectives vpp consistently outperforms existing methods across two simulated two realworld benchmarks notably achieves relative improvement calvin abcd benchmark compared previous stateoftheart delivers increase success rates complex realworld dexterous manipulation tasks,5,0.40509671000283853,5,0.40509671000283853
generative inbetweening framewise conditionsdriven video generation generative inbetweening aims generate intermediate frame sequences utilizing two key frames input although remarkable progress made video generation models generative inbetweening still faces challenges maintaining temporal stability due ambiguous interpolation path two key frames issue becomes particularly severe large motion gap input frames paper propose straightforward yet highly effective framewise conditionsdriven video generation fcvg method significantly enhances temporal stability interpolated video frames specifically fcvg provides explicit condition frame making much easier identify interpolation path two input frames thus ensuring temporally stable production visually plausible video frames achieve suggest extracting matched lines two input frames easily interpolated frame frame serving framewise conditions seamlessly integrated existing video generation models extensive evaluations covering diverse scenarios natural landscapes complex human poses camera movements animations existing methods often exhibit incoherent transitions across frames contrast fcvg demonstrates capability generate temporally stable videos using linear nonlinear interpolation curves project page code available urlhttpsfcvginbetweengithubio,-1,0.0,-1,0.0
mitsgan safeguarding medical imaging tampering generative adversarial networks progress generative models particularly generative adversarial networks gans opened new possibilities image generation raised concerns potential malicious uses especially sensitive areas like medical imaging study introduces mitsgan novel approach prevent tampering medical images specific focus ct scans approach disrupts output attackers ctgan architecture introducing finely tuned perturbations imperceptible human eye specifically proposed approach involves introduction appropriate gaussian noise input protective measure various attacks method aims enhance tamper resistance comparing favorably existing techniques experimental results ct scan demonstrate mitsgans superior performance emphasizing ability generate tamperresistant images negligible artifacts image tampering medical domains poses lifethreatening risks proactive approach contributes responsible ethical use generative models work provides foundation future research countering cyber threats medical imaging models codes publicly available,4,1.0,4,1.0
multiscale conditional generative modeling microscopic image restoration advance diffusionbased generative models recent years revolutionized stateoftheart sota techniques wide variety image analysis synthesis tasks whereas adaptation image restoration particularly within computational microscopy remains theoretically empirically underexplored research introduce multiscale generative model enhances conditional image restoration novel exploitation brownian bridge process within wavelet domain initiating brownian bridge diffusion process specifically lowestfrequency subband applying generative adversarial networks subsequent multiscale highfrequency subbands wavelet domain method provides significant acceleration training sampling sustaining high image generation quality diversity par sota diffusion models experimental results various computational microscopy imaging tasks confirm methods robust performance considerable reduction sampling steps time pioneering technique offers efficient image restoration framework harmonizes efficiency quality signifying major stride incorporating cuttingedge generative models computational microscopy workflows,-1,0.0,-1,0.0
scaling laws diffusion transformers diffusion transformers dit already achieved appealing synthesis scaling properties content recreation eg image video generation however scaling laws dit less explored usually offer precise predictions regarding optimal model size data requirements given specific compute budget therefore experiments across broad range compute budgets flops conducted confirm existence scaling laws dit first time concretely loss pretraining dit also follows powerlaw relationship involved compute based scaling law determine optimal model size required data also accurately predict texttoimage generation loss given model parameters compute budget flops additionally also demonstrate trend pretraining loss matches generation performances eg fid even across various datasets complements mapping compute synthesis quality thus provides predictable benchmark assesses model performance data quality reduced cost,13,0.8636053815530663,13,0.8636053815530663
accelerating diffusion sartooptical image translation via adversarial consistency distillation synthetic aperture radar sar provides allweather highresolution imaging capabilities unique imaging mechanism often requires expert interpretation limiting widespread applicability translating sar images easily recognizable optical images using diffusion models helps address challenge however diffusion models suffer high latency due numerous iterative inferences generative adversarial networks gans achieve image translation single iteration often cost image quality overcome issues propose new training framework sartooptical image translation combines strengths approaches method employs consistency distillation reduce iterative inference steps integrates adversarial learning ensure image clarity minimize color shifts additionally approach allows tradeoff quality speed providing flexibility based application requirements conducted experiments datasets performing quantitative evaluations using peak signaltonoise ratio psnr structural similarity index ssim frechet inception distance fid well calculating inference latency results demonstrate approach significantly improves inference speed times maintaining visual quality generated images thus offering robust efficient solution sartooptical image translation,-1,0.0,-1,0.0
loopgaussian creating cinemagraph multiview images via eulerian motion field cinemagraph unique form visual media combines elements still photography subtle motion create captivating experience however majority videos generated recent works lack depth information confined constraints image space paper inspired significant progress field novel view synthesis nvs achieved gaussian splatting propose loopgaussian elevate cinemagraph image space space using gaussian modeling achieve first employ method reconstruct gaussian point clouds multiview images static scenesincorporating shape regularization terms prevent blurring artifacts caused object deformation adopt autoencoder tailored gaussian project feature space maintain local continuity scene devise supergaussian clustering based acquired features calculating similarity clusters employing twostage estimation method derive eulerian motion field describe velocities across entire scene gaussian points move within estimated eulerian motion field bidirectional animation techniques ultimately generate cinemagraph exhibits natural seamlessly loopable dynamics experiment results validate effectiveness approach demonstrating highquality visually appealing scene generation project available httpspokerlishaogithubioloopgaussian,-1,0.0,-1,0.0
advanced multimodal liver tumor segmentation via diffusionbased image synthesis alignment multimodal learning demonstrated enhance performance across various clinical tasks owing diverse perspectives offered different modalities data however existing multimodal segmentation methods rely wellregistered multimodal data unrealistic realworld clinical images particularly indistinct diffuse regions liver tumors paper introduce fourstage multimodal liver tumor segmentation pipeline preregistration target organs multimodal cts dilation annotated modalitys mask followed use inpainting obtain multimodal normal cts without tumors synthesis strictly aligned multimodal cts tumors using latent diffusion model based multimodal ct features randomly generated tumor masks finally training segmentation model thus eliminating need strictly aligned multimodal data extensive experiments public internal datasets demonstrate superiority stateoftheart multimodal segmentation methods,3,1.0,3,1.0
towards accurate liptospeech synthesis inthewild paper introduce novel approach address task synthesizing speech silent videos inthewild speaker solely based lip movements traditional approach directly generating speech lip videos faces challenge able learn robust language model speech alone resulting unsatisfactory outcomes overcome issue propose incorporating noisy text supervision using stateoftheart liptotext network instills language information model noisy text generated using pretrained liptotext model enabling approach work without text annotations inference design visual texttospeech network utilizes visual stream generate accurate speech insync silent input video perform extensive experiments ablation studies demonstrating approachs superiority current stateoftheart methods various benchmark datasets demonstrate essential practical application method assistive technology generating speech als patient lost voice make mouth movements demo video code additional details found,-1,0.0,-1,0.0
highfidelity lipsynced talking face synthesis via landmarkbased diffusion model audiodriven talking face video generation attracted increasing attention due huge industrial potential previous methods focus learning direct mapping audio visual content despite progress often struggle ambiguity mapping process leading flawed results alternative strategy involves facial structural representations eg facial landmarks intermediaries multistage approach better preserves appearance details suffers error accumulation due independent optimization different stages moreover previous methods rely generative adversarial networks prone training instability mode collapse address challenges study proposes novel landmarkbased diffusion model talking face generation leverages facial landmarks intermediate representations enabling endtoend optimization specifically first establish less ambiguous mapping audio landmark motion lip jaw introduce innovative conditioning module called talkformer align synthesized motion motion represented landmarks via differentiable crossattention enables endtoend optimization improved lip synchronization besides talkformer employs implicit feature warping align reference image features target motion preserving appearance details extensive experiments demonstrate approach synthesize highfidelity lipsynced talking face videos preserving subject appearance details reference image,6,1.0,6,1.0
timesuite improving mllms long video understanding via grounded tuning multimodal large language models mllms demonstrated impressive performance short video understanding however understanding longform videos still remains challenging mllms paper proposes timesuite collection new designs adapt existing shortform video mllms long video understanding including simple yet efficient framework process long video sequence highquality video dataset grounded tuning mllms carefullydesigned instruction tuning task explicitly incorporate grounding supervision traditional qa format specifically based videochat propose longvideo mllm coined videochatt implementing token shuffling compress long video tokens introducing temporal adaptive position encoding tape enhance temporal awareness visual representation meanwhile introduce timepro comprehensive groundingcentric instruction tuning dataset composed tasks highquality grounded annotations notably design new instruction tuning task type called temporal grounded caption peform detailed video descriptions corresponding time stamps prediction explicit temporal location prediction guide mllm correctly attend visual content generating description thus reduce hallucination risk caused llms experimental results demonstrate timesuite provides successful solution enhance long video understanding capability shortform mllm achieving improvement benchmarks egoschema videomme respectively addition videochatt exhibits robust zeroshot temporal grounding capabilities significantly outperforming existing stateoftheart mllms finetuning performs par traditional supervised expert models,0,0.8903486744309248,0,0.8903486744309248
knowledge guided entityaware video captioning basketball benchmark despite recent emergence video captioning models generate text description specific entity names finegrained actions far solved however great applications basketball live text broadcast paper new multimodal knowledge graph supported basketball benchmark video captioning proposed specifically construct multimodal basketball game knowledge graph provide additional knowledge beyond videos multimodal basketball game video captioning dataset contains types finegrained shooting events players knowledge ie images names constructed based develop knowledge guided entityaware video captioning network keanet based candidate player list encoderdecoder form basketball live text broadcast temporal contextual information video encoded introducing bidirectional gru bigru module entityaware module designed model relationships among players highlight key players extensive experiments multiple sports benchmarks demonstrate keanet effectively leverages extera knowledge outperforms advanced video captioning models proposed dataset corresponding codes publicly available soon,0,0.8910470884561363,0,0.8910470884561363
leveraging irregular repetition priors improving video action counting video action counting vac crucial analyzing sports fitness everyday activities quantifying repetitive actions videos however traditional vac methods overlooked complexity action repetitions interruptions variability cycle duration research addresses shortfall introducing novel approach vac called irregular video action counting ivac ivac prioritizes modeling irregular repetition patterns videos define two primary aspects intercycle consistency cycleinterval inconsistency intercycle consistency ensures homogeneity spatialtemporal representations cycle segments signifying action uniformity within cycles cycleinterval inconsistency highlights importance distinguishing cycle segments intervals based inherent content differences encapsulate principles propose new methodology includes consistency inconsistency modules supported unique pullpush loss mechanism model applies pull loss promote coherence among cycle segment features push loss clearly distinguish features cycle segments interval segments empirical evaluations conducted repcount dataset demonstrate model sets new benchmark vac task performance furthermore model demonstrates exceptional adaptability generalization across various video contents outperforming existing models two additional datasets ucfrep countix without need datasetspecific optimization results confirm efficacy approach addressing irregular repetitions videos pave way advancements video analysis understanding,-1,0.0,-1,0.0
fast encoderbased casual videos via point track processing paper addresses longstanding challenge reconstructing structures videos dynamic content current approaches problem designed operate casual videos recorded standard cameras require long optimization time aiming significantly improve efficiency previous approaches present learningbased approach enables inferring structure camera positions dynamic content originating casual videos using single efficient feedforward pass achieve propose operating directly point tracks input designing architecture tailored processing point tracks proposed architecture designed two key principles mind takes account inherent symmetries present input point tracks data assumes movement patterns effectively represented using lowrank approximation trained unsupervised way dataset casual videos utilizing point tracks extracted videos without supervision experiments show reconstruct temporal point cloud camera positions underlying video accuracy comparable stateoftheart methods drastically reducing runtime show generalizes well unseen videos unseen semantic categories inference time,-1,0.0,-1,0.0
beyond alignment blind video face restoration via parsingguided temporalcoherent transformer multiple complex degradations coupled lowquality video faces real world therefore blind video face restoration highly challenging illposed problem requiring hallucinating highfidelity details also enhancing temporal coherence across diverse pose variations restoring frame independently naive manner inevitably introduces temporal incoherence artifacts pose changes keypoint localization errors address propose first blind video face restoration approach novel parsingguided temporalcoherent transformer pgtformer without prealignment pgtformer leverages semantic parsing guidance select optimal face priors generating temporally coherent artifactfree results specifically pretrain temporalspatial vector quantized autoencoder highquality video face datasets extract expressive contextrich priors temporal parseguided codebook predictor tpcp restores faces different poses based face parsing context cues without performing face prealignment strategy reduces artifacts mitigates jitter caused cumulative errors face prealignment finally temporal fidelity regulator tfr enhances fidelity temporal feature interaction improves video temporal consistency extensive experiments face videos show method outperforms previous face restoration baselines code released hrefhttpsgithubcomkepengxupgtformerhttpsgithubcomkepengxupgtformer,-1,0.0,-1,0.0
unveiling potential harnessing deep metric learning circumvent video streaming encryption encryption internet shift https important step improve privacy internet users however increasing body work extracting information encrypted internet traffic without decrypt attacks bypass security guarantees assumed given https thus need understood prior works showed variable bitrates video streams sufficient identify video someone watching works generally make tradeoffs aspects accuracy scalability robustness etc tradeoffs complicate practical use attacks end propose deep metric learning framework based triplet loss method framework achieve robust generalisable scalable transferable encrypted video stream detection first triplet loss better able deal video streams seen training second approach accurately classify videos seen training third show method scales well dataset videos finally show model trained video streams chrome also classify streams firefox results suggest sidechannel attack broadly applicable originally thought provide code alongside diverse uptodate dataset future research,4,1.0,4,1.0
videosalmonn speechenhanced audiovisual large language models speech understanding element generic video understanding using audiovisual large language models avllms crucial yet understudied aspect paper proposes videosalmonn single endtoend avllm video processing understand visual frame sequences audio events music speech well obtain finegrained temporal information required speech understanding keeping efficient video elements paper proposes novel multiresolution causal qformer mrc qformer structure connect pretrained audiovisual encoders backbone large language model moreover dedicated training approaches including diversity loss unpaired audiovisual mixed training scheme proposed avoid frames modality dominance introduced speechaudiovisual evaluation benchmark videosalmonn achieves absolute accuracy improvements videoqa task absolute accuracy improvements audiovisual qa tasks human speech addition videosalmonn demonstrates remarkable video comprehension reasoning abilities tasks unprecedented avllms training code model checkpoints available texttturlhttpsgithubcombytedancesalmonn,0,1.0,0,1.0
hypergraph multimodal large language model exploiting eeg eyetracking modalities evaluate heterogeneous responses video understanding understanding video creativity content often varies among individuals differences focal points cognitive levels across different ages experiences genders currently lack research area existing benchmarks suffer several drawbacks limited number modalities answers restrictive length content scenarios within videos excessively monotonous transmitting allegories emotions overly simplistic bridge gap realworld applications introduce largescale subjective response indicators advertisement videos dataset namely sriadv specifically collected real changes electroencephalographic eeg eyetracking regions different demographics viewed identical video content utilizing multimodal dataset developed tasks protocols analyze evaluate extent cognitive understanding video content among different users along dataset designed hypergraph multimodal large language model hmllm explore associations among different demographics video elements eeg eyetracking indicators hmllm could bridge semantic gaps across rich modalities integrate information beyond different modalities perform logical reasoning extensive experimental evaluations sriadv additional videobased generative performance benchmarks demonstrate effectiveness method codes dataset released httpsgithubcommininglampmllmhmllm,0,0.9986189224600447,0,0.9986189224600447
sigma sinkhornguided masked video modeling videobased pretraining offers immense potential learning strong visual representations unprecedented scale recently masked video modeling methods shown promising scalability yet fall short capturing higherlevel semantics due reconstructing predefined lowlevel targets pixels tackle present sinkhornguided masked video modelling sigma novel video pretraining method jointly learns video model addition target feature space using projection network however simple modification means regular reconstruction loss lead trivial solutions networks jointly optimized solution distribute features spacetime tubes evenly across limited number learnable clusters posing optimal transport problem enforce high entropy generated features across batch infusing semantic temporal meaning feature space resulting cluster assignments used targets symmetric prediction task video model predicts cluster assignment projection network vice versa experimental results ten datasets across three benchmarks validate effectiveness sigma learning performant temporallyaware robust video representations improving upon stateoftheart methods project website code available httpsquvalabgithubiosigma,-1,0.0,-1,0.0
explore crosscodec qualityrate convex hulls relation adaptive streaming ongoing advancement video technology emergence new video platforms suppliers video contents striving ensure video quality meets desire consumers accessing limited amount channel bandwidth often looking novel approach decrease use data thus required energy cost study evaluates quality rate performance codecs across resolutions optimize video quality minimizing bitrate crucial energy cost efficiency approach original videos native resolutions encoded decoded rescaled using ffmpeg resolution encoding decoding performed various quantization levels quality rate qr curves generated using psnr vmaf metric bitrate convex hull curves derived mathematically modelled resolution procedure systematically applied codecs results indicate increasing crf values reduce bitrate psnr vmaf psnr ranging db logarithmic polynomial modelling convex hulls demonstrated high accuracy low rmse high rsquared values findings suggest convex hull one codec predict performance others aiding future contentdriven prediction methodologies enhancing adaptive streaming efficiency keywords video codecs adaptive streaming compression bitrate psnr vmaf,2,1.0,2,1.0
evaluation large pretrained models gesture recognition using synthetic videos work explore possibility using synthetically generated data videobased gesture recognition large pretrained models consider whether models sufficiently robust expressive representation spaces enable trainingfree classification specifically utilize various stateoftheart video encoders extract features use knearest neighbors classification training data points derived synthetic videos compare results another trainingfree approach zeroshot classification using text descriptions gesture experiments dataset find using synthetic training videos yields significantly lower classification accuracy real test videos compared using relatively small number real training videos also observe video backbones finetuned classification tasks serve superior feature extractors choice finetuning data substantial impact knearest neighbors performance lastly find zeroshot textbased classification performs poorly gesture recognition task gestures easily described natural language,-1,0.0,-1,0.0
data playwright authoring data videos annotated narration creating data videos effectively narrate stories animated visuals requires substantial effort expertise promising research trend leveraging easytouse natural language nl interaction automatically synthesize data video components narrative content like text narrations nl commands specify userrequired designs nevertheless previous research overlooked integration narrative content specific design authoring commands leading generated results lack customization fail seamlessly fit narrative context address issues introduce novel paradigm creating data videos seamlessly integrates users authoring narrative intents unified format called annotated narration allowing users incorporate nl commands design authoring inline annotations within narration text informed formative study users preference annotated narration develop prototype system named data playwright embodies paradigm effective creation data videos within data playwright users write annotated narration based uploaded visualizations systems interpreter automatically understands users inputs synthesizes data videos narrationanimation interplay powered large language models finally users preview finetune video user study demonstrated participants effectively create data videos data playwright effortlessly articulating desired outcomes annotated narration,15,1.0,15,1.0
video quality assessment comprehensive survey video quality assessment vqa important processing task aiming predicting quality videos manner highly consistent human judgments perceived quality traditional vqa models based natural image andor video statistics inspired models projected images real world dual models human visual system deliver limited prediction performances realworld usergenerated content ugc exemplified recent largescale vqa databases containing large numbers diverse video contents crawled web fortunately recent advances deep neural networks large multimodality models lmms enabled significant progress solving problem yielding better results prior handcrafted models numerous deep learningbased vqa models developed progress direction driven creation contentdiverse largescale humanlabeled databases supply ground truth psychometric video quality data present comprehensive survey recent progress development vqa algorithms benchmarking studies databases make possible also analyze open research directions study design vqa algorithm architectures github link httpsgithubcomtacogroupvideoqualityassessmentacomprehensivesurvey,12,0.3545353409230002,12,0.3545353409230002
bvicr multiview human dataset volumetric video compression advances immersive technologies reconstruction enabled creation digital replicas realworld objects environments fine details processes generate vast amounts data requiring efficient compression methods satisfy memory bandwidth constraints associated data storage transmission however development validation efficient data compression methods constrained lack comprehensive highquality volumetric video datasets typically require much effort acquire consume increased resources compared image video databases bridge gap present open multiview volumetric human dataset denoted bvicr contains multiview rgbd captures corresponding textured polygonal meshes depicting range diverse human actions video sequence contains views resolution durations seconds using bvicr benchmarked three conventional neural coordinatebased multiview video compression methods following mpeg miv common test conditions reported rate quality performance based various quality metrics results show great potential neural representation based methods volumetric video compression compared conventional video coding methods average coding gain psnr dataset provides development validation platform variety tasks including volumetric reconstruction compression quality assessment database shared publicly urlhttpsgithubcomfanaaronzhangbvicr,-1,0.0,-1,0.0
lift leveraging human feedback texttovideo model alignment recent advances texttovideo generative models shown impressive capabilities however models still inadequate aligning synthesized videos human preferences eg accurately reflecting text descriptions particularly difficult address human preferences subjective challenging formalize objective functions existing studies train video quality assessment models rely humanannotated ratings video evaluation overlook reasoning behind evaluations limiting ability capture nuanced human criteria moreover aligning model using videobased human feedback remains unexplored therefore paper proposes lift first method designed leverage human feedback model alignment specifically first construct human rating annotation dataset lifthra consisting approximately human annotations including score corresponding rationale based train reward model liftcritic learn reward function effectively serves proxy human judgment measuring alignment given videos human expectations lastly leverage learned reward function align model maximizing rewardweighted likelihood case study apply pipeline showing finetuned model outperforms across metrics highlighting potential human feedback improving alignment quality synthesized videos,-1,0.0,-1,0.0
deblurgs gaussian splatting camera motion blur although significant progress made reconstructing sharp scenes motionblurred images transition realworld applications remains challenging primary obstacle stems severe blur leads inaccuracies acquisition initial camera poses structurefrommotion critical aspect often overlooked previous approaches address challenge propose deblurgs method optimize sharp gaussian splatting motionblurred images even noisy camera pose initialization restore finegrained sharp scene leveraging remarkable reconstruction capability gaussian splatting approach estimates camera motion blurry observation synthesizes corresponding blurry renderings optimization process furthermore propose gaussian densification annealing strategy prevent generation inaccurate gaussians erroneous locations early training stages camera motion still imprecise comprehensive experiments demonstrate deblurgs achieves stateoftheart performance deblurring novel view synthesis realworld synthetic benchmark datasets well fieldcaptured blurry smartphone videos,1,1.0,1,1.0
pose estimation camera images underwater inspection highprecision localization pivotal underwater reinspection missions traditional localization methods like inertial navigation systems doppler velocity loggers acoustic positioning face significant challenges costeffective applications visual localization costeffective alternative cases leveraging cameras already equipped inspection vehicles estimate poses images surrounding scene amongst machine learningbased pose estimation images shows promise underwater environments performing efficient relocalization using models trained based previously mapped scenes explore efficacy learningbased pose estimators clear turbid water inspection missions assessing impact image formats model architectures training data diversity innovate employing novel view synthesis models generate augmented training data significantly enhancing pose estimation unexplored regions moreover enhance localization accuracy integrating pose estimator outputs sensor data via extended kalman filter demonstrating improved trajectory smoothness accuracy,14,1.0,14,1.0
videoguide improving video diffusion models without training teachers guide texttoimage diffusion models revolutionized visual content creation extending capabilities texttovideo generation remains challenge particularly preserving temporal consistency existing methods aim improve consistency often cause tradeoffs reduced imaging quality impractical computational time address issues introduce videoguide novel framework enhances temporal consistency pretrained models without need additional training finetuning instead videoguide leverages pretrained video diffusion model vdm guide early stages inference improving temporal quality interpolating guiding models denoised samples sampling models denoising process proposed method brings significant improvement temporal consistency image fidelity providing costeffective practical solution synergizes strengths various video diffusion models furthermore demonstrate prior distillation revealing base models achieve enhanced text coherence utilizing superior data prior guiding model proposed method project page,-1,0.0,-1,0.0
ctnerf crosstime transformer dynamic neural radiance field monocular video goal work generate highquality novel views monocular videos complex dynamic scenes prior methods dynamicnerf shown impressive performance leveraging timevarying dynamic radiation fields however methods limitations comes accurately modeling motion complex objects lead inaccurate blurry renderings details address limitation propose novel approach builds upon recent generalization nerf aggregates nearby views onto new viewpoints however methods typically effective static scenes overcome challenge introduce module operates time frequency domains aggregate features object motion allows us learn relationship frames generate higherquality images experiments demonstrate significant improvements stateoftheart methods dynamic scene datasets specifically approach outperforms existing methods terms accuracy visual quality synthesized views code available,1,1.0,1,1.0
medical video generation disease progression simulation modeling disease progression crucial improving quality efficacy clinical diagnosis prognosis often hindered lack longitudinal medical image monitoring individual patients address challenge propose first medical video generation mvg framework enables controlled manipulation diseaserelated image video features allowing precise realistic personalized simulations disease progression approach begins leveraging large language models llms recaption prompt disease trajectory next controllable multiround diffusion model simulates disease progression state patient creating realistic intermediate disease state sequence finally diffusionbased video transition generation model interpolates disease progression states validate framework across three medical imaging domains chest xray fundus photography skin image results demonstrate mvg significantly outperforms baseline models generating coherent clinically plausible disease trajectories two user studies veteran physicians provide validation insights clinical utility generated sequences mvg potential assist healthcare providers modeling disease trajectories interpolating missing medical image data enhancing medical education realistic dynamic visualizations disease progression,-1,0.0,-1,0.0
liveportrait efficient portrait animation stitching retargeting control portrait animation aims synthesize lifelike video single source image using appearance reference motion ie facial expressions head pose derived driving video audio text generation instead following mainstream diffusionbased methods explore extend potential implicitkeypointbased framework effectively balances computational efficiency controllability building upon develop videodriven portrait animation framework named liveportrait focus better generalization controllability efficiency practical usage enhance generation quality generalization ability scale training data million highquality frames adopt mixed imagevideo training strategy upgrade network architecture design better motion transformation optimization objectives additionally discover compact implicit keypoints effectively represent kind blendshapes meticulously propose stitching two retargeting modules utilize small mlp negligible computational overhead enhance controllability experimental results demonstrate efficacy framework even compared diffusionbased methods generation speed remarkably reaches rtx gpu pytorch inference code models available httpsgithubcomkwaivgiliveportrait,11,1.0,11,1.0
portraittalk towards customizable oneshot audiototalking face generation audiodriven talking face generation challenging task digital communication despite significant progress area existing methods concentrate audiolip synchronization often overlooking aspects visual quality customization generalization crucial producing realistic talking faces address limitations introduce novel customizable oneshot audiodriven talking face generation framework named portraittalk proposed method utilizes latent diffusion framework consisting two main components identitynet animatenet identitynet designed preserve identity features consistently across generated video frames animatenet aims enhance temporal coherence motion consistency framework also integrates audio input reference images thereby reducing reliance referencestyle videos prevalent existing approaches key innovation portraittalk incorporation text prompts decoupled crossattention mechanisms significantly expands creative control generated videos extensive experiments including newly developed evaluation metric model demonstrates superior performance stateoftheart methods setting new standard generation customizable realistic talking faces suitable realworld applications,6,0.9750712521331176,6,0.9750712521331176
neuralnetworkenhanced metalens camera highdefinition dynamic imaging longwave infrared spectrum provide lightweight costeffective solution longwave infrared imaging using singlet develop camera integrating highfrequencyenhancing cyclegan neural network metalens imaging system highfrequencyenhancing cyclegan improves quality original metalens images addressing inherent frequency loss introduced metalens addition bidirectional cyclic generative adversarial network incorporates highfrequency adversarial learning module module utilizes wavelet transform extract highfrequency components establishes highfrequency feedback loop enables generator enhance camera outputs integrating adversarial feedback highfrequency discriminator ensures generator adheres constraints imposed highfrequency adversarial loss thereby effectively recovering cameras frequency loss recovery guarantees highfidelity image output camera facilitating smooth video production camera capable achieving dynamic imaging frames per second end point error value also achieve frechet inception distance peak signal noise ratio structural similarity recorded videos,-1,0.0,-1,0.0
pointvos pointing video object segmentation current stateoftheart video object segmentation vos methods rely dense perobject mask annotations training testing requires timeconsuming costly video annotation mechanisms propose novel pointvos task spatiotemporally sparse pointwise annotation scheme substantially reduces annotation effort apply annotation scheme two largescale video datasets text descriptions annotate points across objects videos based annotations propose new pointvos benchmark corresponding pointbased training mechanism use establish strong baseline results show existing vos methods easily adapted leverage point annotations training achieve results close fullysupervised performance trained pseudomasks generated points addition show data used improve models connect vision language evaluating video narrative grounding vng task make code annotations available httpspointvosgithubio,7,0.9129948122571027,7,0.9129948122571027
dont judge look towards motion coherent video representation current training pipelines object recognition neglect hue jittering data augmentation brings appearance changes detrimental classification also implementation inefficient practice study investigate effect hue variance context video understanding find variance beneficial since static appearances less important videos contain motion information based observation propose data augmentation method video understanding named motion coherent augmentation mca introduces appearance variation videos implicitly encourages model prioritize motion patterns rather static appearances concretely propose operation swapmix efficiently modify appearance video samples introduce variation alignment va resolve distribution shift caused swapmix enforcing model learn appearance invariant representations comprehensive empirical evaluation across various architectures different datasets solidly validates effectiveness generalization ability mca application va augmentation methods code available httpsgithubcombespontaneousmcapytorch,-1,0.0,-1,0.0
videoagent memoryaugmented multimodal agent video understanding explore reconciling several foundation models large language models visionlanguage models novel unified memory mechanism could tackle challenging video understanding problem especially capturing longterm temporal relations lengthy videos particular proposed multimodal agent videoagent constructs structured memory store generic temporal event descriptions objectcentric tracking states video given input task query employs tools including video segment localization object memory querying along visual foundation models interactively solve task utilizing zeroshot tooluse ability llms videoagent demonstrates impressive performances several longhorizon video understanding benchmarks average increase nextqa egoschema baselines closing gap opensourced models private counterparts including gemini pro,-1,0.0,-1,0.0
adaptive cooperative streaming holographic video wireless networks proximal policy optimization solution adapting holographic video streaming fluctuating wireless channels essential maintain consistent satisfactory quality experience qoe users however challenging task due dynamic uncertain characteristics wireless networks address issue propose holographic video cooperative streaming framework designed generic wireless network multiple access points cooperatively transmit video different bitrates multiple users additionally model novel qoe metric tailored specifically holographic video streaming effectively encapsulate nuances holographic video quality quality fluctuations rebuffering occurrences simultaneously furthermore formulate formidable qoe maximization problem nonconvex mixed integer nonlinear programming problem using proximal policy optimization ppo new class reinforcement learning algorithms devise joint beamforming bitrate control scheme wisely adapted fluctuations wireless channel numerical results demonstrate superiority proposed scheme representative baselines,2,1.0,2,1.0
microemo timesensitive multimodal emotion recognition microexpression dynamics video dialogues multimodal large language models mllms demonstrated remarkable multimodal emotion recognition capabilities integrating multimodal cues visual acoustic linguistic contexts video recognize human emotional states however existing methods ignore capturing local facial features temporal dynamics microexpressions leverage contextual dependencies utteranceaware temporal segments video thereby limiting expected effectiveness certain extent work propose microemo timesensitive mllm aimed directing attention local facial microexpression dynamics contextual dependencies utteranceaware video clips model incorporates two key architectural contributions globallocal attention visual encoder integrates global framelevel timestampbound image features local facial features temporal dynamics microexpressions utteranceaware video qformer captures multiscale contextual dependencies generating visual token sequences utterance segment entire video combining preliminary qualitative experiments demonstrate new explainable multimodal emotion recognition emer task exploits multimodal multifaceted clues predict emotions openvocabulary ov manner microemo demonstrates effectiveness compared latest methods,-1,0.0,-1,0.0
kalmaninspired feature propagation video face superresolution despite promising progress face image superresolution video face superresolution remains relatively underexplored existing approaches either adapt general video superresolution networks face datasets apply established face image superresolution models independently individual video frames paradigms encounter challenges either reconstructing facial details maintaining temporal consistency address issues introduce novel framework called kalmaninspired feature propagation keep designed maintain stable face prior time kalman filtering principles offer method recurrent ability use information previously restored frames guide regulate restoration process current frame extensive experiments demonstrate effectiveness method capturing facial details consistently across video frames code video demo available httpsjnjabygithubioprojectskeep,-1,0.0,-1,0.0
momentcross nextgeneration realtime crossdomain ctr prediction livestreaming recommendation kuaishou kuaishou one largest shortvideo livestreaming platform compared shortvideo recommendations livestreaming recommendation complex temporarilyalive distribution user may watch long time feedback delay content unpredictable changes time actually even user interested livestreaming author still may negative watching eg shortview since realtime content attractive enough therefore livestreaming recommendation exists challenging task recommend livestreaming right moment users additionally platforms major exposure content short shortvideo amount exposed shortvideo exposed livestreaming thus users leave behaviors shortvideos leads serious data imbalance problem making livestreaming data could fully reflect user interests case raises another challenging task utilize users shortvideo behaviors make livestreaming recommendation better,-1,0.0,-1,0.0
omniclip adapting clip video recognition spatialtemporal omniscale feature learning recent visionlanguage models vlms textiteg clip made great progress video recognition despite improvement brought strong visual backbone extracting spatial features clip still falls short capturing integrating spatialtemporal features essential video recognition paper propose omniclip framework adapts clip video recognition focusing learning comprehensive features encompassing spatial temporal dynamic spatialtemporal scales refer omniscale features achieved design spatialtemporal blocks include parallel temporal adapters pta enabling efficient temporal modeling additionally introduce selfprompt generator spg module capture dynamic object spatial features synergy pta spg allows omniclip discern varying spatial information across frames assess object scales time conducted extensive experiments supervised video recognition fewshot video recognition zeroshot recognition tasks results demonstrate effectiveness method especially omniclip achieving accuracy setting surpassing recent motionprompt approach even full training data code available urlhttpsgithubcomxiaobulomniclip,-1,0.0,-1,0.0
pite pixeltemporal alignment large videolanguage model fueled large language models llms wave large visuallanguage models lvlms emerged pivotal advancement bridging gap image text however video making challenging lvlms perform adequately due complexity relationship language spatialtemporal data structure recent large videolanguage models lvidlms align feature static visual data like image latent space language feature general multimodal tasks leverage abilities llms sufficiently paper explore finegrained alignment approach via object trajectory different modalities across spatial temporal dimensions simultaneously thus propose novel lvidlm trajectoryguided pixeltemporal alignment dubbed pite exhibits promising applicable model property achieve finegrained videolanguage alignment curate multimodal pretraining dataset dataset provision moving trajectories pixel level individual objects appear mention video caption automatic annotation pipeline meanwhile pite demonstrates astounding capabilities myriad videorelated multimodal tasks beat stateoftheart methods large margin,0,0.829421400603186,0,0.829421400603186
dgns deformable gaussian splatting dynamic neural surface monocular dynamic reconstruction dynamic scene reconstruction monocular video critical realworld applications paper tackles dual challenges dynamic novelview synthesis geometry reconstruction introducing hybrid framework deformable gaussian splatting dynamic neural surfaces dgns modules leverage tasks training depth maps generated deformable gaussian splatting module guide ray sampling faster processing provide depth supervision within dynamic neural surface module improve geometry reconstruction simultaneously dynamic neural surface directs distribution gaussian primitives around surface enhancing rendering quality refine depth supervision introduce depthfiltering process depth maps derived gaussian rasterization extensive experiments public datasets demonstrate dgns achieves stateoftheart performance novelview synthesis reconstruction,1,1.0,1,1.0
instantdrag improving interactivity dragbased image editing dragbased image editing recently gained popularity interactivity precision however despite ability texttoimage models generate samples within second drag editing still lags behind due challenge accurately reflecting user interaction maintaining image content existing approaches rely computationally intensive perimage optimization intricate guidancebased methods requiring additional inputs masks movable regions text prompts thereby compromising interactivity editing process introduce instantdrag optimizationfree pipeline enhances interactivity speed requiring image drag instruction input instantdrag consists two carefully designed networks dragconditioned optical flow generator flowgen optical flowconditioned diffusion model flowdiffusion instantdrag learns motion dynamics dragbased image editing realworld video datasets decomposing task motion generation motionconditioned image generation demonstrate instantdrags capability perform fast photorealistic edits without masks text prompts experiments facial video datasets general scenes results highlight efficiency approach handling dragbased image editing making promising solution interactive realtime applications,-1,0.0,-1,0.0
generative object insertion gaussian splatting multiview diffusion model generating inserting new objects content compelling approach achieving versatile scene recreation existing methods rely sds optimization singleview inpainting often struggle produce highquality results address propose novel method object insertion content represented gaussian splatting approach introduces multiview diffusion model dubbed mvinpainter built upon pretrained stable video diffusion model facilitate viewconsistent object inpainting within mvinpainter incorporate controlnetbased conditional injection module enable controlled predictable multiview generation generating multiview inpainted results propose maskaware reconstruction technique refine gaussian splatting reconstruction sparse inpainted views leveraging fabricate techniques approach yields diverse results ensures viewconsistent harmonious insertions produces better object quality extensive experiments demonstrate approach outperforms existing methods,1,0.9690655127253915,1,0.9690655127253915
highfidelity talking portrait synthesis via personalized generative prior recent methods audiodriven talking head synthesis often optimize neural radiance fields nerf monocular talking portrait video leveraging capability render highfidelity novelview frames however often struggle reconstruct complete face geometry due absence comprehensive information input monocular videos paper introduce novel audiodriven talking head synthesis framework called faithfully reconstruct plausible facial geometries effectively adopting pretrained generative prior given personalized generative model present novel audioguided attention unet architecture predicts dynamic face variations nerf space driven audio furthermore model modulated audiounrelated conditioning tokens effectively disentangle variations unrelated audio features compared existing methods method excels generating realistic facial geometries even extreme head poses also conduct extensive experiments showing approach surpasses stateoftheart benchmarks terms quantitative qualitative evaluations,6,0.9180426179672315,6,0.9180426179672315
direct mesh supervision neural radiance field representation generation present approach derive groundtruth radiance fields textured meshes generation tasks many generative approaches represent scenes radiance fields training groundtruth radiance fields usually fitted multiview renderings largescale synthetic dataset often results artifacts due occlusions underfitting issues propose analytic solution directly obtain groundtruth radiance fields meshes characterizing density field occupancy function featuring defined surface thickness determining viewdependent color reflection function considering mesh environment lighting extracts accurate radiance fields provides direct supervision training generative nerfs single scene representation validate effectiveness across various tasks achieving noteworthy improvement psnr view synthesis single scene representation abo dataset psnr enhancement singleview conditional generation shapenet cars notably improved mesh extraction nerf unconditional generation objaverse mugs,1,1.0,1,1.0
panoramic generation resolution blooming virtual reality augmented reality vrar technologies driven increasing demand creation highquality immersive dynamic environments however existing generative techniques either focus solely dynamic objects perform outpainting single perspective image failing meet requirements vrar applications need freeviewpoint virtual views users move directions work tackle challenging task elevating single panorama immersive experience first time demonstrate capability generate omnidirectional dynamic scenes views times resolution thereby providing immersive user experience method introduces pipeline facilitates natural scene animations optimizes set dynamic gaussians using efficient splatting techniques realtime exploration overcome lack scenescale annotated data models especially panoramic formats propose novel textbfpanoramic denoiser adapts generic diffusion priors animate consistently images transforming panoramic videos dynamic scenes targeted regions subsequently propose textbfdynamic panoramic lifting elevate panoramic video immersive environment preserving spatial temporal consistency transferring prior knowledge models perspective domain panoramic domain lifting spatial appearance geometry regularization achieve highquality generation resolution first time,1,1.0,1,1.0
exploiting topological priors boosting point cloud generation paper presents innovative enhancement sphere prior generative adversarial network spgan model stateoftheart gan designed point cloud generation novel method introduced point cloud generation elevates structural integrity overall quality generated point clouds incorporating topological priors training process generator specifically work utilizes kmeans algorithm segment point cloud repository clusters extract centroids used priors generation process spgan furthermore discriminator component spgan utilizes identical point cloud contributed centroids ensuring coherent consistent learning environment strategic use centroids intuitive guides boosts efficiency global feature learning also substantially improves structural coherence fidelity generated point clouds applying kmeans algorithm generate centroids prior work intuitively experimentally demonstrates prior enhances quality generated point clouds,-1,0.0,-1,0.0
motionadaptive inference flexible learned bframe compression performance recent learned intra sequential video compression models exceed respective traditional codecs performance learned bframe compression models generally lag behind traditional bframe coding performance gap bigger complex scenes large motions related fact distance past future references vary hierarchical bframe compression depending level hierarchy causes motion range vary inability single bframe compression model adapt various motion ranges causes loss performance remedy propose controlling motion range flow prediction inference approximately match range motions training data downsampling video frames adaptively according amount motion level hierarchy order compress bframes using single flexiblerate model present stateoftheart bd rate results demonstrate superiority proposed singlemodel motionadaptive inference approach existing learned bframe compression models,2,1.0,2,1.0
fastcad realtime cad retrieval alignment scans videos digitising world clean cad modelbased representation important applications augmented reality robotics current stateoftheart methods computationally intensive individually encode detected object optimise cad alignments second stage work propose fastcad realtime method simultaneously retrieves aligns cad models objects given scene contrast previous works directly predict alignment parameters shape embeddings achieve highquality shape retrievals learning cad embeddings contrastive learning framework distilling fastcad singlestage method accelerates inference time factor compared methods operating rgbd scans outperforming challenging alignment benchmark approach collaborates seamlessly online reconstruction techniques enables realtime generation precise cad modelbased reconstructions videos fps significantly improve alignment accuracy video setting reconstruction accuracy,1,1.0,1,1.0
deep understanding soccer match videos soccer one popular sport worldwide live broadcasts frequently available major matches however extracting detailed framebyframe information player actions videos remains challenge utilizing stateoftheart computer vision technologies system detect key objects soccer balls players referees also tracks movements players ball recognizes player numbers classifies scenes identifies highlights goal kicks analyzing live tv streams soccer matches system generate highlight gifs tactical illustrations diverse summary graphs ongoing games visual recognition techniques deliver comprehensive understanding soccer game videos enriching viewers experience detailed insightful analysis,-1,0.0,-1,0.0
umotion learned point cloud video compression ustructured temporal context generation point cloud video pcv versatile representation dynamic scenes emerging applications paper introduces umotion learningbased compression scheme pcv geometry attributes propose ustructured interframe prediction framework uinter performs explicit motion estimation compensation memc different scales varying levels detail integrates topdown finetocoarse motion propagation bottomup motion predictive coding multiscale group motion compensation enable accurate motion estimation efficient motion compression scale addition design multiscale spatialtemporal predictive coding module capture crossscale spatial redundancy remaining uinter prediction conduct experiments following mpeg common test condition dense dynamic point clouds demonstrate umotion achieve significant gains mpeg gpccgestm recently published learningbased methods geometry attribute compression,2,1.0,2,1.0
relocate simple trainingfree baseline visual query localization using regionbased representations present relocate simple trainingfree baseline designed perform challenging task visual query localization long videos eliminate need taskspecific training efficiently handle long videos relocate leverages regionbased representation derived pretrained vision models high level follows classic object localization approach identify objects video frame compare objects given query select similar ones perform bidirectional tracking get spatiotemporal response however propose key enhancements handle small objects cluttered scenes partial visibility varying appearances notably refine selected objects accurate localization generate additional visual queries capture visual variations evaluate relocate challenging visual query localization dataset establishing new baseline outperforms prior taskspecific methods relative improvement spatiotemporal average precision,-1,0.0,-1,0.0
learning selfsupervised audiovisual representations sound recommendations propose novel selfsupervised approach learning audio visual representations unlabeled videos based correspondence approach uses attention mechanism learn relative importance convolutional features extracted different resolutions audio visual streams uses attention features encode audio visual input based correspondence evaluated representations learned model classify audiovisual correlation well recommend sound effects visual scenes results show representations generated attention model improves correlation accuracy compared baseline recommendation accuracy vggsound public video dataset additionally audiovisual representations learned training attention model crossmodal contrastive learning improves recommendation performance based evaluation using vggsound challenging dataset consisting gameplay video recordings,-1,0.0,-1,0.0
stylerfvolvis style transfer neural radiance fields expressive volume visualization volume visualization visualization synthesis attracted much attention due ability generate novel visualizations without following conventional rendering pipeline however existing solutions based generative adversarial networks often require many training images take significant training time still issues low quality consistency flexibility persist paper introduces stylerfvolvis innovative style transfer framework expressive volume visualization volvis via neural radiance field nerf expressiveness stylerfvolvis upheld ability accurately separate underlying scene geometry ie content color appearance ie style conveniently modify color opacity lighting original rendering maintaining visual content consistency across views effectively transfer arbitrary styles reference images reconstructed scene achieve design base nerf model scene geometry extraction palette color network classify regions radiance field photorealistic editing unrestricted color network lift color palette constraint via knowledge distillation nonphotorealistic editing demonstrate superior quality consistency flexibility stylerfvolvis experimenting various volume rendering scenes reference images comparing stylerfvolvis imagebased adain videobased rerevst nerfbased arf snerf style rendering solutions,1,1.0,1,1.0
float generative motion latent flow matching audiodriven talking portrait rapid advancement diffusionbased generative models portrait image animation achieved remarkable results however still faces challenges temporally consistent video generation fast sampling due iterative sampling nature paper presents float audiodriven talking portrait video generation method based flow matching generative model shift generative modeling pixelbased latent space learned motion latent space enabling efficient design temporally consistent motion achieve introduce transformerbased vector field predictor simple yet effective framewise conditioning mechanism additionally method supports speechdriven emotion enhancement enabling natural incorporation expressive motions extensive experiments demonstrate method outperforms stateoftheart audiodriven talking portrait methods terms visual quality motion fidelity efficiency,6,0.3973742181217773,6,0.3973742181217773
gim learning generalizable image matcher internet videos image matching fundamental computer vision problem learningbased methods achieve stateoftheart performance existing benchmarks generalize poorly inthewild images methods typically need train separate models different scene types impractical scene type unknown advance one underlying problems limited scalability existing data construction pipelines limits diversity standard image matching datasets address problem propose gim selftraining framework learning single generalizable model based image matching architecture using internet videos abundant diverse data source given architecture gim first trains standard domainspecific datasets combines complementary matching methods create dense labels nearby frames novel videos labels filtered robust fitting enhanced propagating distant frames final model trained propagated data strong augmentations also propose zeb first zeroshot evaluation benchmark image matching mixing data diverse domains zeb thoroughly assess crossdomain generalization performance different methods applying gim consistently improves zeroshot performance stateoftheart image matching architectures hours youtube videos relative zeroshot performance improves gim also enables generalization extreme crossdomain data bird eye view bev images projected point clouds fig importantly single zeroshot model consistently outperforms domainspecific baselines evaluated downstream tasks inherent respective domains video presentation available,-1,0.0,-1,0.0
animdirector large multimodal model powered agent controllable animation video generation traditional animation generation methods depend training generative models humanlabelled data entailing sophisticated multistage pipeline demands substantial human effort incurs high training costs due limited prompting plans methods typically produce brief informationpoor contextincoherent animations overcome limitations automate animation process pioneer introduction large multimodal models lmms core processor build autonomous animationmaking agent named animdirector agent mainly harnesses advanced understanding reasoning capabilities lmms generative ai tools create animated videos concise narratives simple instructions specifically operates three main stages firstly animdirector generates coherent storyline user inputs followed detailed directors script encompasses settings character profiles interiorexterior descriptions contextcoherent scene descriptions include appearing characters interiors exteriors scene events secondly employ lmms image generation tool produce visual images settings scenes images designed maintain visual consistency across different scenes using visuallanguage prompting method combines scene descriptions images appearing character setting thirdly scene images serve foundation producing animated videos lmms generating prompts guide process whole process notably autonomous without manual intervention lmms interact seamlessly generative tools generate prompts evaluate visual quality select best one optimize final output,15,0.9998452021022955,15,0.9998452021022955
selfsupervised video desmoking laparoscopic surgery due difficulty collecting real paired data existing desmoking methods train models synthesizing smoke generalizing poorly real surgical scenarios although works explored singleimage realworld desmoking unpaired learning manners still encounter challenges handling dense smoke work address issues together introducing selfsupervised surgery video desmoking selfsvd one hand observe frame captured activation highenergy devices generally clear named presmoke frame ps frame thus serve supervision smoky frames making realworld selfsupervised video desmoking practically feasible hand order enhance desmoking performance feed valuable information ps frame models masking strategy regularization term presented avoid trivial solutions addition construct real surgery video dataset desmoking covers variety smoky scenes extensive experiments dataset show selfsvd remove smoke effectively efficiently recovering photorealistic details stateoftheart methods dataset codes pretrained models available urlhttpsgithubcomzcsrenlongzselfsvd,-1,0.0,-1,0.0
vrdone onestage video visual relation detection video visual relation detection vidvrd focuses understanding entities interact time space videos key step gaining deeper insights video scenes beyond basic visual tasks traditional methods vidvrd challenged complexity typically split task two parts one identifying relation categories present another determining temporal boundaries split overlooks inherent connection elements addressing need recognize entity pairs spatiotemporal interactions across range durations propose vrdone streamlined yet efficacious onestage model vrdone combines features subjects objects turning predicate detection instance segmentation combined representations setup allows relation category identification binary mask generation one go eliminating need extra steps like proposal generation postprocessing vrdone facilitates interaction features across various frames adeptly capturing shortlived enduring relations additionally introduce subjectobject synergy sos module enhancing subjects objects perceive combining vrdone achieves stateoftheart performances vidor benchmark imagenetvidvrd showcasing superior capability discerning relations across different temporal scales code available,-1,0.0,-1,0.0
precise video camera control adjustable motion strength video generation technologies developing rapidly broad potential applications among technologies camera control crucial generating professionalquality videos accurately meet user expectations however existing camera control methods still suffer several limitations including control precision neglect control subject motion dynamics work propose novel camera control method significantly enhances controllability providing adjustability strength subject motion improve control precision employ point trajectory camera coordinate system instead extrinsic matrix information control signal accurately control adjust strength subject motion explicitly model higherorder components video trajectory expansion merely linear terms design operator effectively represents motion strength use adapter architecture independent base model structure experiments static dynamic scenes show framework outperformances previous methods quantitatively qualitatively project page,-1,0.0,-1,0.0
jointmotion mutual learning pose estimation videos human pose estimation videos long compelling yet challenging task within realm computer vision nevertheless task remains difficult complex video scenes video defocus selfocclusion recent methods strive integrate multiframe visual features generated backbone network pose estimation however often ignore useful joint information encoded initial heatmap byproduct backbone generation comparatively methods attempt refine initial heatmap fail consider spatiotemporal motion features result performance existing methods pose estimation falls short due lack ability leverage local joint heatmap information global motion feature dynamics address problem propose novel jointmotion mutual learning framework pose estimation effectively concentrates local joint dependency global pixellevel motion dynamics specifically introduce contextaware joint learner adaptively leverages initial heatmaps motion flow retrieve robust local joint feature given local joint feature global motion flow complementary propose progressive jointmotion mutual learning synergistically exchanges information interactively learns joint feature motion flow improve capability model importantly capture diverse joint motion cues theoretically analyze propose information orthogonality objective avoid learning redundant information multicues empirical experiments show method outperforms prior arts three challenging benchmarks,6,0.42493842198834664,6,0.42493842198834664
comprehensive generative replay taskincremental segmentation concurrent appearance semantic forgetting generalist segmentation models increasingly favored diverse tasks involving various objects different image sources taskincremental learning til offers privacypreserving training paradigm using tasks arriving sequentially instead gathering due strict data sharing policies however task evolution span wide scope involves shifts image appearance segmentation semantics intricate correlation causing concurrent appearance semantic forgetting solve issue propose comprehensive generative replay cgr framework restores appearance semantic knowledge synthesizing imagemask pairs mimic past task data focuses two aspects modeling imagemask correspondence promoting scalability diverse tasks specifically introduce novel bayesian joint diffusion bjd model highquality synthesis imagemask pairs correspondence explicitly preserved conditional denoising furthermore develop taskoriented adapter toa recalibrates prompt embeddings modulate diffusion model making data synthesis compatible different tasks experiments incremental tasks cardiac fundus prostate segmentation show clear advantage alleviating concurrent appearance semantic forgetting code available httpsgithubcomjingyzhangcgr,-1,0.0,-1,0.0
lyrics musicdriven dance synthesis dance typically involves professional choreography complex movements follow musical rhythm also influenced lyrical content integration lyrics addition auditory dimension enriches foundational tone makes motion generation amenable semantic meanings however existing dance synthesis methods tend model motions conditioned audio signals work make two contributions bridge gap first propose novel probabilistic architecture incorporates multimodal diffusion model consistency distillation designed create dance conditioned music lyrics one diffusion generation step second introduce first dancemotion dataset encompasses music lyrics obtained pose estimation technologies evaluate model musiconly baseline models objective metrics human evaluations including dancers choreographers results demonstrate able produce realistic diverse dance matching lyrics music video summary accessed,8,0.5028337766144633,8,0.5028337766144633
align steps optimizing sampling schedules diffusion models diffusion models dms established stateoftheart generative modeling approach visual domain beyond crucial drawback dms slow sampling speed relying many sequential function evaluations large neural networks sampling dms seen solving differential equation discretized set noise levels known sampling schedule past works primarily focused deriving efficient solvers little attention given finding optimal sampling schedules entire literature relies handcrafted heuristics work first time propose general principled approach optimizing sampling schedules dms highquality outputs called textitalign steps leverage methods stochastic calculus find optimal schedules specific different solvers trained dms datasets evaluate novel approach several image video well toy data synthesis benchmarks using variety different samplers observe optimized schedules outperform previous handcrafted schedules almost experiments method demonstrates untapped potential sampling schedule optimization especially fewstep synthesis regime,-1,0.0,-1,0.0
lmgaussian boost sparseview gaussian splatting large model priors aim address sparseview reconstruction scene leveraging priors largescale vision models recent advancements gaussian splatting demonstrated remarkable successes reconstruction methods typically necessitate hundreds input images densely capture underlying scene making timeconsuming impractical realworld applications however sparseview reconstruction inherently illposed underconstrained often resulting inferior incomplete outcomes due issues failed initialization overfitting input images lack details mitigate challenges introduce lmgaussian method capable generating highquality reconstructions limited number images specifically propose robust initialization module leverages stereo priors aid recovery camera poses reliable point clouds additionally diffusionbased refinement iteratively applied incorporate image diffusion priors gaussian optimization process preserve intricate scene details finally utilize video diffusion priors enhance rendered images realistic visual effects overall approach significantly reduces data acquisition requirements compared previous methods validate effectiveness framework experiments various public datasets demonstrating potential highquality scene reconstruction visual results website,1,1.0,1,1.0
incontext ensemble learning pseudo labels improves videolanguage models lowlevel workflow understanding standard operating procedure sop defines lowlevel stepbystep written guide business software workflow sop generation crucial step towards automating endtoend software workflows manually creating sops timeconsuming recent advancements large videolanguage models offer potential automating sop generation analyzing recordings human demonstrations however current large videolanguage models face challenges zeroshot sop generation work first explore incontext learning videolanguage models sop generation propose explorationfocused strategy called incontext ensemble learning aggregate pseudo labels multiple possible paths sops proposed incontext ensemble learning well enables models learn beyond context window limit implicit consistency regularisation report incontext learning helps videolanguage models generate temporally accurate sop proposed incontext ensemble learning consistently enhance capabilities videolanguage models sop generation,-1,0.0,-1,0.0
drivingworld constructing world model autonomous driving via video gpt recent successes autoregressive ar generation models gpt series natural language processing motivated efforts replicate success visual tasks works attempt extend approach autonomous driving building videobased world models capable generating realistic future video sequences predicting ego states however prior works tend produce unsatisfactory results classic gpt framework designed handle contextual information text lacks inherent ability model spatial temporal dynamics essential video generation paper present drivingworld gptstyle world model autonomous driving featuring several spatialtemporal fusion mechanisms design enables effective modeling spatial temporal dynamics facilitating highfidelity longduration video generation specifically propose nextstate prediction strategy model temporal coherence consecutive frames apply nexttoken prediction strategy capture spatial information within frame enhance generalization ability propose novel masking strategy reweighting strategy token prediction mitigate longterm drifting issues enable precise control work demonstrates ability produce highfidelity consistent video clips seconds duration times longer stateoftheart driving world models experiments show contrast prior works method achieves superior visual quality significantly accurate controllable future video generation code available httpsgithubcomyvanyindrivingworld,-1,0.0,-1,0.0
new lightweight hybrid graph convolutional neural network cnn scheme scene classification using object detection inference scene understanding plays important role several highlevel computer vision applications autonomous vehicles intelligent video surveillance robotics however solutions proposed indooroutdoor scene classification ensure scene context adaptability computer vision frameworks propose first lightweight hybrid graph convolutional neural network lhgcnncnn framework addon object detection models proposed approach uses output cnn object detection model predict observed scene type generating coherent gcnn representing semantic geometric content observed scene new method applied natural scenes achieves efficiency scene classification cocoderived dataset containing large number different scenes requiring fewer parameters traditional cnn methods benefit scientific community make source code publicly available httpsgithubcomaymanbeghhybridgcnncnn,-1,0.0,-1,0.0
pancreatic tumor segmentation anomaly detection ct images using denoising diffusion models despite advances medicine cancer remained formidable challenge particularly case pancreatic tumors characterized diversity late diagnosis early detection poses significant challenge crucial effective treatment advancement deep learning techniques particularly supervised algorithms significantly propelled pancreatic tumor detection medical field however supervised deep learning approaches necessitate extensive labeled medical images training yet acquiring annotations limited costly conversely weakly supervised anomaly detection methods requiring imagelevel annotations garnered interest existing methodologies predominantly hinge generative adversarial networks gans autoencoder models pose complexity training models may face difficulties accurately preserving fine image details research presents novel approach pancreatic tumor detection employing weak supervision anomaly detection denoising diffusion algorithms incorporating deterministic iterative process adding removing noise along classifier guidance method enables seamless translation images diseased healthy subjects resulting detailed anomaly maps without requiring complex training protocols segmentation masks study explores denoising diffusion models recent advancement traditional generative models like gans contributing field pancreatic tumor detection recognizing low survival rates pancreatic cancer study emphasizes need continued research leverage diffusion models efficiency medical segmentation tasks,3,0.5960110310137081,3,0.5960110310137081
general framework jersey number recognition sports video jersey number recognition important task sports video analysis partly due importance longterm player tracking viewed variant scene text recognition however lack published attempts apply scene text recognition models jersey number data introduce novel public jersey number recognition dataset hockey study scene text recognition methods adapted problem address issues occlusions assess degree training one sport hockey generalized another soccer latter also consider jersey number recognition singleimage level aggregated across frames yield trackletlevel jersey number labels demonstrate high performance image trackletlevel tasks achieving accuracy hockey images soccer tracklets code models data available httpsgithubcommkoshkinajerseynumberpipeline,-1,0.0,-1,0.0
learning things move internet stereo videos learning understand dynamic scenes imagery crucial applications ranging robotics scene reconstruction yet unlike problems largescale supervised training enabled rapid progress directly supervising methods recovering motion remains challenging due fundamental difficulty obtaining ground truth annotations present system mining highquality reconstructions internet stereoscopic wideangle videos system fuses filters outputs camera pose estimation stereo depth estimation temporal tracking methods highquality dynamic reconstructions use method generate largescale data form worldconsistent pseudometric point clouds longterm motion trajectories demonstrate utility data training variant predict structure motion realworld image pairs showing training reconstructed data enables generalization diverse realworld scenes project page,-1,0.0,-1,0.0
nonadversarial learning vectorquantized common latent space multisequence mri adversarial learning helps generative models translate mri source target sequence lacking paired samples however implementing mri synthesis adversarial learning clinical settings challenging due training instability mode collapse address issue leverage intermediate sequences estimate common latent space among multisequence mri enabling reconstruction distinct sequences common latent space propose generative model compresses discrete representations sequence estimate gaussian distribution vectorquantized common vqc latent space multiple sequences moreover improve latent space consistency contrastive learning increase model stability domain augmentation experiments using dataset show nonadversarial model outperforms ganbased methods vqc latent space aids model achieve antiinterference ability eliminate effects noise bias fields artifacts solid semantic representation ability potential oneshot segmentation code publicly available,-1,0.0,-1,0.0
cwdm conditional wavelet diffusion models crossmodality medical image synthesis paper contributes brats brain mr image synthesis challenge presents conditional wavelet diffusion model cwdm directly solving paired imagetoimage translation task highresolution volumes deep learningbased brain tumor segmentation models demonstrated clear clinical utility typically require mr scans various modalities flair input however due time constraints imaging artifacts modalities may missing hindering application wellperforming segmentation algorithms clinical routine address issue propose method synthesizes one missing modality image conditioned three available images enabling application downstream segmentation models treat paired imagetoimage translation task conditional generation problem solve combining wavelet diffusion model highresolution image synthesis simple conditioning strategy approach allows us directly apply model fullresolution volumes avoiding artifacts caused slice patchwise data processing work focuses specific application presented method applied kinds paired imagetoimage translation problems ct leftrightarrow mr mr leftrightarrow pet translation maskconditioned anatomically guided image generation,3,1.0,3,1.0
geometric generative models based morphological equivariant pdes gans content image generation consist creating generating data noisy information extracting specific features texture edges thin image structures interested generative models two main problems addressed firstly improvements specific feature extraction accounting multiscale levels intrinsic geometric features secondly equivariance network reduce complexity provide geometric interpretability proceed propose geometric generative model based equivariant partial differential equation pde group convolution neural networks gcnns called pdegcnns built morphology operators generative adversarial networks gans equivariant morphological pde layers composed multiscale dilations erosions formulated riemannian manifolds group symmetries defined lie group take advantage lie group structure properly integrate equivariance layers able use riemannian metric solve multiscale morphological operations point lie group associated unique point manifold helps us derive metric riemannian manifold tensor field invariant lie group induced metric symmetries proposed geometric morphological gan gmgan obtained using proposed morphological equivariant convolutions pdegcnns bring nonlinearity classical cnns gmgan evaluated mnist data compared gans preliminary results show gmgan model outperforms classical gan,-1,0.0,-1,0.0
garmentdreamer guided garment synthesis diverse geometry texture details traditional garment creation laborintensive involving sketching modeling uv mapping texturing timeconsuming costly recent advances diffusionbased generative models enabled new possibilities garment generation text prompts images videos however existing methods either suffer inconsistencies among multiview images require additional processes separate cloth underlying human model paper propose garmentdreamer novel method leverages gaussian splatting gs guidance generate wearable simulationready garment meshes text prompts contrast using multiview images directly predicted generative models guidance guidance ensures consistent optimization garment deformation texture synthesis method introduces novel garment augmentation module guided normal rgba information employs implicit neural texture fields netf combined score distillation sampling sds generate diverse geometric texture details validate effectiveness approach comprehensive qualitative quantitative experiments showcasing superior performance garmentdreamer stateoftheart alternatives project page available httpsxuanligithubiogarmentdreamerdemo,-1,0.0,-1,0.0
smoothfoley creating continuous sound videotoaudio generation semantic guidance videotoaudio generation task drawn attention field multimedia due practicality producing foley sound semantic temporal conditions fed generation model indicate sound events temporal occurrence recent studies synthesizing immersive synchronized audio faced challenges videos moving visual presence temporal condition accurate enough lowresolution semantic condition exacerbates problem tackle challenges propose smoothfoley generative model taking semantic guidance textual label across generation enhance semantic temporal alignment audio two adapters trained leverage pretrained texttoaudio generation models frame adapter integrates highresolution framewise video features temporal adapter integrates temporal conditions obtained similarities visual frames textual labels incorporation semantic guidance textual labels achieves precise audiovideo alignment conduct extensive quantitative qualitative experiments results show smoothfoley performs better existing models continuous sound scenarios general scenarios semantic guidance audio generated smoothfoley exhibits higher quality better adherence physical laws,8,0.8469519048321539,8,0.8469519048321539
make actor talk generalizable highfidelity lip sync motion appearance disentanglement aim edit lip movements talking video according given speech preserving personal identity visual details task decomposed two subproblems speechdriven lip motion generation visual appearance synthesis current solutions handle two subproblems within single generative model resulting challenging tradeoff lipsync quality visual details preservation instead propose disentangle motion appearance generate one one speechtomotion diffusion model motionconditioned appearance generation model however still remain challenges stage motionaware identity preservation visual details preservation therefore preserve personal identity adopt landmarks represent motion employ landmarkbased identity loss capture motionagnostic visual details use separate encoders encode lip nonlip appearance motion integrate learned fusion module train mytalk largescale diverse dataset experiments show method generalizes well unknown even outofdomain person terms lip sync visual detail preservation encourage readers watch videos project page,6,0.7133337425794238,6,0.7133337425794238
motionguided diffusion gif generation present motionguided diffusion model imagetogif video generation tackle problem differently formulating task image translation problem steered text motion magnitude prompts shown teaser fig ensure model adheres motion guidance propose new motionguided warping module spatially transform features source image conditioned two types prompts furthermore introduce perceptual loss ensure transformed feature map remains within space target image ensuring content consistency coherence preparation model training meticulously curated data extracting coherent image frames tgif videocaption dataset provides rich information temporal changes subjects pretraining apply model zeroshot manner number video datasets extensive qualitative quantitative experiments demonstrate effectiveness model captures semantic prompt text also spatial ones motion guidance train models using single node gpus code dataset models made public,-1,0.0,-1,0.0
legopet hierarchical feature guided conditional diffusion pet image reconstruction positron emission tomography pet widely utilized cancer detection due ability visualize functional biological processes vivo pet images usually reconstructed histogrammed raw data sinograms using traditional iterative techniques eg osem mlem recently deep learning dl methods shown promise directly mapping raw sinogram data pet images however dl approaches regressionbased ganbased often produce overly smoothed images introduce various artifacts respectively imageconditioned diffusion probabilistic models cdpms another class likelihoodbased dl techniques capable generating highly realistic controllable images cdpms notable strengths still face challenges maintain correspondence consistency input output images different domains eg sinogram vs image domain well slow convergence rates address limitations introduce legopet hierarchical feature guided conditional diffusion model highperceptual quality pet image reconstruction sinograms conducted several experiments demonstrating legopet improves performance cdpms also surpasses recent dlbased pet image reconstruction techniques terms visual quality pixellevel psnrssim metrics code available httpsgithubcomyransunlegopet,17,1.0,17,1.0
generation detection sign language deepfakes linguistic visual analysis research explores positive application deepfake technology upper body generation specifically sign language deaf hard hearing dhoh community given complexity sign language scarcity experts generated videos vetted sign language expert accuracy construct reliable deepfake dataset evaluating technical visual credibility using computer vision natural language processing models dataset consisting videos featuring seen unseen individuals also used detect deepfake videos targeting vulnerable individuals expert annotations confirm generated videos comparable real sign language content linguistic analysis using textual similarity scores interpreter evaluations shows interpretation generated videos least similar authentic sign language visual analysis demonstrates convincingly realistic deepfakes produced even new subjects using posestyle transfer model pay close attention detail ensuring hand movements accurate align driving video also apply machine learning algorithms establish baseline deepfake detection dataset contributing detection fraudulent sign language videos,4,0.9771865719151808,4,0.9771865719151808
text prompt normality guidance weakly supervised video anomaly detection weakly supervised video anomaly detection wsvad challenging task generating finegrained pseudolabels based weaklabel selftraining classifier currently promising solution however since existing methods use rgb visual modality utilization category text information neglected thus limiting generation accurate pseudolabels affecting performance selftraining inspired manual labeling process based event description paper propose novel pseudolabel generation selftraining framework based text prompt normality guidance tpwng wsvad idea transfer rich languagevisual knowledge contrastive languageimage pretraining clip model aligning video event description text corresponding video frames generate pseudolabels specifically first finetune clip domain adaptation designing two ranking losses distributional inconsistency loss propose learnable text prompt mechanism assist normality visual prompt improve matching accuracy video event description text video frames design pseudolabel generation module based normality guidance infer reliable framelevel pseudolabels finally introduce temporal context selfadaptive learning module learn temporal dependencies different video events flexibly accurately extensive experiments show method achieves stateoftheart performance two benchmark datasets ucfcrime xdviole,7,0.9324357128973825,7,0.9324357128973825
narrativebridge enhancing video captioning causaltemporal narrative existing video captioning benchmarks models lack causaltemporal narrative sequences events linked cause effect unfolding time driven characters agents lack narrative restricts models ability generate text descriptions capture causal temporal dynamics inherent video content address gap propose narrativebridge approach comprising novel causaltemporal narrative ctn captions benchmark generated using large language model fewshot prompting explicitly encoding causeeffect temporal relationships video descriptions causeeffect network cen separate encoders capturing cause effect dynamics enabling effective learning generation captions causaltemporal narrative extensive experiments demonstrate cen significantly outperforms stateoftheart models articulating causal temporal aspects video content cider msvdctn msrvttctn datasets respectively crossdataset evaluations showcase cens strong generalization capabilities proposed framework understands generates nuanced text descriptions intricate causaltemporal narrative structures present videos addressing critical limitation video captioning project details visit httpsnarrativebridgegithubio,0,0.8659570751975788,0,0.8659570751975788
implicit locationcaption alignment via complementary masking weaklysupervised dense video captioning weaklysupervised dense video captioning wsdvc aims localize describe events interest video without requiring annotations event boundaries setting poses great challenge accurately locating temporal location event relevant supervision unavailable existing methods rely explicit alignment constraints event locations captions involve complex event proposal procedures training inference tackle problem propose novel implicit locationcaption alignment paradigm complementary masking simplifies complex event proposal localization process maintaining effectiveness specifically model comprises two components dualmode video captioning module mask generation module dualmode video captioning module captures global event information generates descriptive captions mask generation module generates differentiable positive negative masks localizing events masks enable implicit alignment event locations captions ensuring captions generated positively negatively masked videos complementary thereby forming complete video description way even weak supervision event location event caption aligned implicitly extensive experiments public datasets demonstrate method outperforms existing weaklysupervised methods achieves competitive results compared fullysupervised methods,-1,0.0,-1,0.0
semantics guided disentangled gan chest xray image rib segmentation label annotations chest xray image rib segmentation time consuming laborious labeling quality heavily relies medical knowledge annotators reduce dependency annotated data existing works often utilize generative adversarial network gan generate training data however ganbased methods overlook nuanced information specific individual organs degrades generation quality chest xray image hence propose novel semantics guided disentangled gan sdgan generate highquality training data fully utilizing semantic information different organs chest xray image rib segmentation particular use three branches disentangle features different organs use decoder combine features generate corresponding images ensure generated images correspond input organ labels semantics tags employ semantics guidance module perform semantic guidance generated images evaluate efficacy sdgan generating highquality samples introduce modified transunetmtunet specialized segmentation network designed multiscale contextual information extracting multibranch decoding effectively tackling challenge organ overlap also propose new chest xray image dataset cxrs includes samples various medical institutions lungs clavicles ribs simultaneously annotated chest xray image visualization quantitative results demonstrate efficacy sdgan generating highquality chest xray imagemask pairs using generated data trained mtunet overcomes limitations data scale outperforms segmentation networks,3,0.6682958831946795,3,0.6682958831946795
instructavatar textguided emotion motion control avatar generation recent talking avatar generation models made strides achieving realistic accurate lip synchronization audio often fall short controlling conveying detailed expressions emotions avatar making generated video less vivid controllable paper propose novel textguided approach generating emotionally expressive avatars offering finegrained control improved interactivity generalizability resulting video framework named instructavatar leverages natural language interface control emotion well facial motion avatars technically design automatic annotation pipeline construct instructionvideo paired training dataset equipped novel twobranch diffusionbased generator predict avatars audio text instructions time experimental results demonstrate instructavatar produces results align well conditions outperforms existing methods finegrained emotion control lipsync quality naturalness project page,6,1.0,6,1.0
stable video portraits rapid advances field generative ai texttoimage methods particular transformed way interact perceive computergenerated imagery today parallel much progress made face reconstruction using morphable models paper present svp novel hybrid generation method outputs photorealistic videos talking faces leveraging large pretrained texttoimage prior controlled via specifically introduce personspecific finetuning general stable diffusion model lift video model providing temporal sequences conditioning introducing temporal denoising procedure output model generates temporally smooth imagery person controls ie personspecific avatar facial appearance personspecific avatar edited morphed textdefined celebrities without finetuning test time method analyzed quantitatively qualitatively show method outperforms stateoftheart monocular head avatar methods,-1,0.0,-1,0.0
emotivetalk expressive talking head generation audio information decoupling emotional video diffusion diffusion models revolutionized field talking head generation yet still face challenges expressiveness controllability stability longtime generation research propose emotivetalk framework address issues firstly realize better control generation lip movement facial expression visionguided audio information decoupling vaid approach designed generate audiobased decoupled representations aligned lip movements expression specifically achieve alignment audio facial expression representation spaces present diffusionbased cospeech temporal expansion dicte module within vaid generate expressionrelated representations multisource emotion condition constraints propose welldesigned emotional talking head diffusion ethd backbone efficiently generate highly expressive talking head videos contains expression decoupling injection edi module automatically decouple expressions reference portraits integrating target expression information achieving expressive generation performance experimental results show emotivetalk generate expressive talking head videos ensuring promised controllability emotions stability longtime generation yielding stateoftheart performance compared existing methods,6,0.8851978167201433,6,0.8851978167201433
diffdef diffusiongenerated deformation fields conditional atlases anatomical atlases widely used population analysis conditional atlases target particular subpopulation defined via certain conditions eg demographics pathologies allow investigation finegrained anatomical differences morphological changes correlated age existing approaches use either registrationbased methods unable handle large anatomical variations generative models suffer training instabilities hallucinations overcome limitations use latent diffusion models generate deformation fields transform general population atlas one representing specific subpopulation generating deformation field registering conditional atlas neighbourhood images ensure structural plausibility avoid hallucinations occur direct image synthesis compare method several stateoftheart atlas generation methods experiments using brain well wholebody mr images uk biobank method generates highly realistic atlases smooth transformations high anatomical fidelity outperforming baselines,3,0.5691766666153065,3,0.5691766666153065
sds see sorted quadruped skill synthesis single video demonstration paper present sds see sorted novel pipeline intuitive quadrupedal skill learning single demonstration video leveraging visual capabilities sds processes input videos novel chainofthought promoting technique sus generates executable reward functions rfs drive imitation locomotion skills learning proximal policy optimization ppobased reinforcement learning rl policy using environment information nvidia isaacgym simulator sds autonomously evaluates rfs monitoring individual reward components supplying training footage fitness metrics back prompted evolve rfs achieve higher task fitness iteration validate method unitree robot demonstrating ability execute variable skills trotting bounding pacing hopping achieving high imitation fidelity locomotion stability sds shows improvements sota methods task adaptability reduced dependence domainspecific knowledge bypassing need laborintensive reward engineering largescale training datasets additional information opensourced code found httpsrplcsuclgithubiosdsweb,5,0.9450197913644605,5,0.9450197913644605
graphjigsaw conditioned diffusion model skeletonbased video anomaly detection skeletonbased video anomaly detection svad crucial task computer vision accurately identifying abnormal patterns events enables operators promptly detect suspicious activities thereby enhancing safety achieving demands comprehensive understanding human motions body region levels also accounting wide variations performing single action however existing studies fail simultaneously address crucial properties paper introduces novel practical lightweight framework namely graphjigsaw conditioned diffusion model skeletonbased video anomaly detection gicisad overcome challenges associated svad gicisad consists three novel modules graph attentionbased forecasting module capture spatiotemporal dependencies inherent data graphlevel jigsaw puzzle maker module distinguish subtle regionlevel discrepancies normal abnormal motions graphbased conditional diffusion model generate wide spectrum human motions extensive experiments four widely used skeletonbased video datasets show gicisad outperforms existing methods significantly fewer training parameters establishing new stateoftheart,-1,0.0,-1,0.0
muvi videotomusic generation semantic alignment rhythmic synchronization generating music aligns visual content video challenging task requires deep understanding visual semantics involves generating music whose melody rhythm dynamics harmonize visual narratives paper presents muvi novel framework effectively addresses challenges enhance cohesion immersive experience audiovisual content muvi analyzes video content specially designed visual adaptor extract contextually temporally relevant features features used generate music matches videos mood theme also rhythm pacing also introduce contrastive musicvisual pretraining scheme ensure synchronization based periodicity nature music phrases addition demonstrate flowmatchingbased music generator incontext learning ability allowing us control style genre generated music experimental results show muvi demonstrates superior performance audio quality temporal synchronization generated music video samples available,8,0.46880602375501523,8,0.46880602375501523
chinese continuous sign language dataset based complex environments current bottleneck continuous sign language recognition cslr research lies fact publicly available datasets limited laboratory environments television program recordings resulting single background environment uniform lighting significantly deviates diversity complexity found reallife scenarios address challenge constructed new largescale dataset chinese continuous sign language csl based complex environments termed complex environment chinese sign language dataset cecsl dataset encompasses continuous csl video clips collected daily life scenes featuring different complex backgrounds ensure representativeness generalization capability tackle impact complex backgrounds cslr performance propose timefrequency network tfnet model continuous sign language recognition model extracts framelevel features utilizes temporal spectral information separately derive sequence features fusion aiming achieve efficient accurate cslr experimental results demonstrate approach achieves significant performance improvements cecsl validating effectiveness complex background conditions additionally proposed method also yielded highly competitive results applied three publicly available csl datasets,4,0.7256336901212068,4,0.7256336901212068
daefuse adaptive discriminative autoencoder multimodality image fusion extreme scenarios nighttime lowvisibility environments achieving reliable perception critical applications like autonomous driving robotics surveillance multimodality image fusion particularly integrating infrared imaging offers robust solution combining complementary information different modalities enhance scene understanding decisionmaking however current methods face significant limitations ganbased approaches often produce blurry images lack finegrained details aebased methods may introduce bias toward specific modalities leading unnatural fusion results address challenges propose daefuse novel twophase discriminative autoencoder framework generates sharp natural fused images furthermore pioneer extension image fusion techniques static images video domain preserving temporal consistency across frames thus advancing perceptual capabilities required autonomous navigation extensive experiments public datasets demonstrate daefuse achieves stateoftheart performance multiple benchmarks superior generalizability tasks like medical image fusion,-1,0.0,-1,0.0
vmddpm vision mamba diffusion medical image synthesis realm smart healthcare researchers enhance scale diversity medical datasets medical image synthesis however existing methods limited cnn local perception transformer quadratic complexity making difficult balance structural texture consistency end propose vision mamba ddpm vmddpm based state space model ssm fully combining cnn local perception ssm global modeling capabilities maintaining linear computational complexity specifically designed multilevel feature extraction module called multilevel state space block mssblock basic unit encoderdecoder structure called state space layer sslayer medical pathological images besides designed simple plugandplay zeroparameter sequence regeneration strategy crossscan module csm enabled module fully perceive spatial features image stimulate generalization potential model best knowledge first medical image synthesis model based ssmcnn hybrid architecture experimental evaluation three datasets different scales ie acdc chestxray well qualitative evaluation radiologists demonstrate vmddpm achieves stateoftheart performance,-1,0.0,-1,0.0
inverse painting reconstructing painting process given input painting reconstruct timelapse video may painted formulate autoregressive image generation problem initially blank canvas iteratively updated model learns real artists training many painting videos approach incorporates text region understanding define set painting instructions updates canvas novel diffusionbased renderer method extrapolates beyond limited acrylic style paintings trained showing plausible results wide range artistic styles genres,-1,0.0,-1,0.0
infinigen indoors photorealistic indoor scenes using procedural generation introduce infinigen indoors blenderbased procedural generator photorealistic indoor scenes builds upon existing infinigen system focuses natural scenes expands coverage indoor scenes introducing diverse library procedural indoor assets including furniture architecture elements appliances daytoday objects also introduces constraintbased arrangement system consists domainspecific language expressing diverse constraints scene composition solver generates scene compositions maximally satisfy constraints provide export tool allows generated objects scenes directly used training embodied agents realtime simulators omniverse unreal infinigen indoors opensourced bsd license please visit httpsinfinigenorg code videos,-1,0.0,-1,0.0
diffusion models vision survey recent years vision become crucial field within computer vision powering wide range applications autonomous driving robotics augmented reality ar medical imaging field relies accurate perception understanding reconstruction scenes data sources like images videos diffusion models originally designed generative tasks offer potential flexible probabilistic approaches better capture variability uncertainty present realworld data however traditional methods often struggle efficiency scalability paper review stateoftheart approaches leverage diffusion models visual tasks including limited object generation shape completion point cloud reconstruction scene understanding provide indepth discussion underlying mathematical principles diffusion models outlining forward reverse processes well various architectural advancements enable models work datasets also discuss key challenges applying diffusion models vision handling occlusions varying point densities computational demands highdimensional data finally discuss potential solutions including improving computational efficiency enhancing multimodal fusion exploring use largescale pretraining better generalization across tasks paper serves foundation future exploration development rapidly evolving field,-1,0.0,-1,0.0
nickel diming gan dualmethod approach enhancing gan efficiency via knowledge distillation paper address challenge compressing generative adversarial networks gans deployment resourceconstrained environments proposing two novel methodologies distribution matching efficient compression dime network interactive compression via knowledge exchange learning nickel dime employs foundation models embedding kernels efficient distribution matching leveraging maximum mean discrepancy facilitate effective knowledge distillation simultaneously nickel employs interactive compression method enhances communication student generator discriminator achieving balanced stable compression process comprehensive evaluation architecture ffhq dataset shows effectiveness approach nickel dime achieving fid scores compression rates respectively remarkably methods sustain generative quality even extreme compression rate surpassing previous stateoftheart performance large margin findings demonstrate methodologies capacity significantly lower gans computational demands also pave way deploying highquality gan models settings limited resources code released soon,17,1.0,17,1.0
enhancing alzheimers disease prediction novel approach leveraging ganaugmented data improved cnn model accuracy alzheimers disease ad neurodegenerative disease affecting millions individuals across globe prevalence disease continues rise early diagnosis crucial improve clinical outcomes neural networks specifically convolutional neural networks cnns promising tools diagnosing individuals alzheimers however neural networks anns cnns typically yield lower validation accuracies fed lower quantities data hence generative adversarial networks gans utilized synthesize data augment existing mri datasets potentially yielding higher validation accuracies study use principle examining novel application ssmi metric selecting highquality synthetic data generated gan compare accuracies shuffled data generated gan observed incorporating gans ssmi metric returned highest accuracies compared traditional dataset,3,0.5260576955629406,3,0.5260576955629406
value aigenerated metadata ugc platforms evidence largescale field experiment aigenerated content aigc advertisement copy product descriptions social media posts becoming ubiquitous business practices however value aigenerated metadata titles remains unclear usergenerated content ugc platforms address gap conducted largescale field experiment leading shortvideo platform asia provide million users access aigenerated titles uploaded videos findings show provision aigenerated titles significantly boosted content consumption increasing valid watches watch duration producers adopted titles increases jumped respectively viewershipboost effect largely attributed use generative ai gai tool increasing likelihood videos title effect pronounced groups affected metadata sparsity mechanism analysis revealed aigenerated metadata improved uservideo matching accuracy platforms recommender system interestingly video producer would posted title anyway adopting aigenerated title decreased viewership average implying aigenerated titles may lower quality humangenerated ones however producers chose cocreate gai significantly revised aigenerated titles videos outperformed counterparts either fully aigenerated humangenerated titles showcasing benefits humanai cocreation study highlights value aigenerated metadata humanai metadata cocreation enhancing usercontent matching content consumption ugc platforms,10,0.6417013497466647,10,0.6417013497466647
learning poseconditioned denoiser realistic gaussian avatar modeling advancements neural implicit representations differentiable rendering markedly improved ability learn animatable avatars sparse multiview rgb videos however current methods map observation space canonical space often face challenges capturing posedependent details generalizing novel poses diffusion models demonstrated remarkable zeroshot capabilities image generation potential creating animatable avatars inputs remains underexplored work introduce novel approach featuring poseconditioned human modeling pipeline integrates iterative denoising rectifying steps denoiser guided pose cues generates detailed multiview images provide rich feature set necessary highfidelity reconstruction pose rendering complementing gaussianbased rectifier renders images enhanced consistency twostage projection strategy novel local coordinate representation additionally propose innovative sampling strategy ensure smooth temporal continuity across frames video synthesis method effectively addresses limitations traditional numerical solutions handling illposed mappings producing realistic animatable human avatars experimental results demonstrate excels highfidelity avatar modeling robustly generalizes novel poses code available httpsgithubcomsilencetanggaussianactor,-1,0.0,-1,0.0
towards generalist robot learning internet video survey scaling deep learning massive diverse internet data yielded remarkably general capabilities visual natural language understanding generation however data remained scarce challenging collect robotics seeing robot learning struggle obtain similarly general capabilities promising learning videos lfv methods aim address robotics data bottleneck augmenting traditional robot data largescale internet video data video data offers broad foundational information regarding physical behaviour underlying physics world thus highly informative generalist robot survey present thorough overview emerging field lfv outline fundamental concepts including benefits challenges lfv provide comprehensive review current methods extracting knowledge largescale internet video addressing key challenges lfv boosting downstream robot reinforcement learning via use video data survey concludes critical discussion challenges opportunities lfv advocate scalable foundation model approaches leverage full range available internet video improve learning robot policies dynamics models hope survey inform catalyse lfv research driving progress towards development generalpurpose robots,5,0.21982615505809286,5,0.21982615505809286
foleycrafter bring silent videos life lifelike synchronized sounds study neural foley automatic generation highquality sound effects synchronizing videos enabling immersive audiovisual experience despite wide range applications existing approaches encounter limitations comes simultaneously synthesizing highquality videoaligned ie semantic relevant temporal synchronized sounds overcome limitations propose foleycrafter novel framework leverages pretrained texttoaudio model ensure highquality audio generation foleycrafter comprises two key components semantic adapter semantic alignment temporal controller precise audiovideo synchronization semantic adapter utilizes parallel crossattention layers condition audio generation video features producing realistic sound effects semantically relevant visual content meanwhile temporal controller incorporates onset detector timestampbased adapter achieve precise audiovideo alignment one notable advantage foleycrafter compatibility text prompts enabling use text descriptions achieve controllable diverse videotoaudio generation according user intents conduct extensive quantitative qualitative experiments standard benchmarks verify effectiveness foleycrafter models codes available httpsgithubcomopenmmlabfoleycrafter,8,0.6453684788919536,8,0.6453684788919536
standardizing generative face video compression using supplemental enhancement information paper proposes generative face video compression gfvc approach using supplemental enhancement information sei series compact spatial temporal representations face video signal ie keypoints facial semantics compact features coded using sei message inserted coded video bitstream time writing proposed gfvc approach using sei messages adopted official working draft versatile supplemental enhancement information vsei standard joint video experts team jvet isoiec jtc itut standardized new version itut isoiec best authors knowledge jvet work proposed seibased gfvc approach first standardization activity generative video compression proposed sei approach advanced reconstruction quality earlyday modelbased coding mbc via stateoftheart generative technique also established new sei definition future gfvc applications deployment experimental results illustrate proposed seibased gfvc approach achieve remarkable ratedistortion performance compared latest versatile video coding vvc standard whilst also potentially enabling wide variety functionalities including userspecified animationfiltering metaverserelated applications,2,0.8846426902391603,2,0.8846426902391603
motiongrounded video reasoning understanding perceiving motion pixel level paper introduce motiongrounded video reasoning new motion understanding task requires generating visual answers video segmentation masks according input question hence needs implicit spatiotemporal reasoning grounding task extends existing spatiotemporal grounding work focusing explicit actionmotion grounding general format enabling implicit reasoning via questions facilitate development new task collect largescale dataset called groundmore comprises video clips object masks deliberately designed question types causal sequential counterfactual descriptive benchmarking deep comprehensive motion reasoning abilities groundmore uniquely requires models generate visual answers providing concrete visually interpretable response plain texts evaluates models spatiotemporal grounding reasoning fostering address complex challenges motionrelated video reasoning temporal perception pixellevel understanding furthermore introduce novel baseline model named motiongrounded video reasoning assistant mora mora incorporates multimodal reasoning ability multimodal llm pixellevel perception capability grounding model sam temporal perception ability lightweight localization head mora achieves respectable performance groundmore outperforming best existing visual grounding baseline model average relatively hope novel challenging task pave way future advancements robust general motion understanding via video reasoning segmentation,0,0.8807203664905882,0,0.8807203664905882
textdriven traffic anomaly detection temporal highfrequency modeling driving videos traffic anomaly detection tad driving videos critical ensuring safety autonomous driving advanced driver assistance systems previous singlestage tad methods primarily rely frame prediction making vulnerable interference dynamic backgrounds induced rapid movement dashboard camera twostage tad methods appear natural solution mitigate interference preextracting backgroundindependent features bounding boxes optical flow using perceptual algorithms susceptible performance firststage perceptual algorithms may result error propagation paper introduce tthf novel singlestage method aligning video clips text prompts offering new perspective traffic anomaly detection unlike previous approaches supervised signal method derived languages rather orthogonal onehot vectors providing comprehensive representation concerning visual representation propose model high frequency driving videos temporal domain modeling captures dynamic changes driving scenes enhances perception driving behavior significantly improves detection traffic anomalies addition better perceive various types traffic anomalies carefully design attentive anomaly focusing mechanism visually linguistically guides model adaptively focus visual context interest thereby facilitating detection traffic anomalies shown proposed tthf achieves promising performance outperforming stateoftheart competitors auc dota dataset achieving high generalization dada dataset,4,0.7791289004365893,4,0.7791289004365893
matching anything segmenting anything robust association objects across video frames complex scenes crucial many applications especially multiple object tracking mot current methods predominantly rely labeled domainspecific video datasets limits crossdomain generalization learned similarity embeddings propose masa novel method robust instance association learning capable matching objects within videos across diverse domains without tracking labels leveraging rich object segmentation segment anything model sam masa learns instancelevel correspondence exhaustive data transformations treat sam outputs dense object region proposals learn match regions vast image collection design universal masa adapter work tandem foundational segmentation detection models enable track detected objects combinations present strong zeroshot tracking ability complex domains extensive tests multiple challenging mot mots benchmarks indicate proposed method using unlabeled static images achieves even better performance stateoftheart methods trained fully annotated indomain video sequences zeroshot association project page httpsmatchinganythinggithubio,7,0.9096294412879784,7,0.9096294412879784
combining pre postdemosaicking noise removal raw video denoising one fundamental steps processing pipeline converts data captured camera sensor displayready image video generally performed early pipeline usually demosaicking although studies swapping order even conducting jointly proposed advent deep learning quality denoising algorithms steadily increased even modern neural networks still hard time adapting new noise levels scenes indispensable realworld applications mind propose selfsimilaritybased denoising scheme weights pre postdemosaicking denoiser bayerpatterned cfa video data show balance two leads better image quality empirically find higher noise levels benefit higher influence predemosaicking also integrate temporal trajectory prefiltering steps denoiser improve texture reconstruction proposed method requires estimation noise model sensor accurately adapts noise level competitive state art making suitable realworld videography,-1,0.0,-1,0.0
egomimic scaling imitation learning via egocentric video scale diversity demonstration data required imitation learning significant challenge present egomimic fullstack framework scales manipulation via human embodiment data specifically egocentric human videos paired hand tracking egomimic achieves system capture human embodiment data using ergonomic project aria glasses lowcost bimanual manipulator minimizes kinematic gap human data crossdomain data alignment techniques imitation learning architecture cotrains human robot data compared prior works extract highlevel intent human videos approach treats human robot data equally embodied demonstration data learns unified policy data sources egomimic achieves significant improvement diverse set longhorizon singlearm bimanual manipulation tasks stateoftheart imitation learning methods enables generalization entirely new scenes finally show favorable scaling trend egomimic adding hour additional hand data significantly valuable hour additional robot data videos additional information found httpsegomimicgithubio,5,1.0,5,1.0
video guided controllable dynamics physicsbased generation work introduce novel approach creating controllable dynamics gaussians using casually captured reference videos method transfers motion objects reference videos variety generated gaussians across different categories ensuring precise customizable motion transfer achieve employing blend skinningbased nonparametric shape reconstruction extract shape motion reference objects process involves segmenting reference objects motionrelated parts based skinning weights establishing shape correspondences generated target shapes address shape temporal inconsistencies prevalent existing methods integrate physical simulation driving target shapes matched motion integration optimized displacement loss ensure reliable genuine dynamics approach supports diverse reference inputs including humans quadrupeds articulated objects generate dynamics arbitrary length providing enhanced fidelity applicability unlike methods heavily reliant diffusion video generation models technique offers specific highquality motion transfer maintaining shape integrity temporal consistency,-1,0.0,-1,0.0
frieren efficient videotoaudio generation network rectified flow matching videotoaudio generation aims synthesize contentmatching audio silent video remains challenging build models high generation quality efficiency visualaudio temporal synchrony propose frieren model based rectified flow matching frieren regresses conditional transport vector field noise spectrogram latent straight paths conducts sampling solving ode outperforming autoregressive scorebased models terms audio quality employing nonautoregressive vector field estimator based feedforward transformer channellevel crossmodal feature fusion strong temporal alignment model generates audio highly synchronized input video furthermore reflow onestep distillation guided vector field model generate decent audio even one sampling step experiments indicate frieren achieves stateoftheart performance generation quality temporal alignment vggsound alignment accuracy reaching improvement inception score strong diffusionbased baseline audio samples available,-1,0.0,-1,0.0
synthetic brain images bridging gap brain mapping generative adversarial model magnetic resonance imaging mri vital modality gaining precise anatomical information plays significant role medical imaging diagnosis therapy planning image synthesis problems seen revolution recent years due introduction deep learning techniques specifically generative adversarial networks gans work investigates use deep convolutional generative adversarial networks dcgan producing highfidelity realistic mri image slices suggested approach uses dataset variety brain mri scans train dcgan architecture discriminator network discerns created real slices generator network learns synthesise realistic mri image slices generator refines capacity generate slices closely mimic real mri data adversarial training approach outcomes demonstrate dcgan promise range uses medical imaging research since show effectively produce mri image slices train consequent number epochs work adds expanding corpus research application deep learning techniques medical image synthesis slices could produced possess capability enhance datasets provide data augmentation training deep learning models well number functions made available make mri data cleaning easier three ready use clean dataset major anatomical plans,3,0.688078104604028,3,0.688078104604028
motion modes could happen next predicting diverse object motions single static image remains challenging current video generation models often entangle object movement camera motion scene changes recent methods predict specific motions motion arrow input rely synthetic data predefined motions limiting application complex scenes introduce motion modes trainingfree approach explores pretrained imagetovideo generators latent distribution discover various distinct plausible motions focused selected objects static images achieve employing flow generator guided energy functions designed disentangle object camera motion additionally use energy inspired particle guidance diversify generated motions without requiring explicit training data experimental results demonstrate motion modes generates realistic varied object animations surpassing previous methods even human predictions regarding plausibility diversity project webpage httpsmotionmodesgithubio,18,1.0,18,1.0
electrooptical image synthesis sar imagery using generative adversarial networks utility synthetic aperture radar sar imagery remote sensing satellite image analysis well established offering robustness various weather lighting conditions however sar images characterized unique structural texture characteristics often pose interpretability challenges analysts accustomed electrooptical eo imagery application compares stateoftheart generative adversarial networks gans including cyclegan scyclegan novel dualgenerator gan utilizing partial convolutions novel dualgenerator architecture utilizing transformers models designed progressively refine realism translated optical images thereby enhancing visual interpretability sar data demonstrate efficacy approach qualitative quantitative evaluations comparing synthesized eo images actual eo images terms visual fidelity feature preservation results show significant improvements interpretability making sar data accessible analysts familiar eo imagery furthermore explore potential technology various applications including environmental monitoring urban planning military reconnaissance rapid accurate interpretation sar data crucial research contributes field remote sensing bridging gap sar eo imagery offering novel tool enhanced data interpretation broader application sar technology various domains,14,0.9006821509711094,14,0.9006821509711094
selfsupervised learning deviation latent representation cospeech gesture video generation gestures pivotal enhancing cospeech communication recent works mostly focused pointlevel motion transformation fully supervised motion representations datadriven approaches explore representation gestures cospeech focus selfsupervised representation pixellevel motion deviation utilizing diffusion model incorporates latent motion features approach leverages selfsupervised deviation latent representation facilitate hand gestures generation crucial generating realistic gesture videos results first experiment demonstrate method enhances quality generated videos improvement fgd div fvd psnr ssim current stateoftheart methods,-1,0.0,-1,0.0
atomovideo high fidelity imagetovideo generation recently video generation achieved significant rapid development based superior texttoimage generation techniques work propose high fidelity framework imagetovideo generation named atomovideo based multigranularity image injection achieve higher fidelity generated video given image addition thanks high quality datasets training strategies achieve greater motion intensity maintaining superior temporal consistency stability architecture extends flexibly video frame prediction task enabling long sequence prediction iterative generation furthermore due design adapter training approach well combined existing personalized models controllable modules quantitatively qualitatively evaluation atomovideo achieves superior results compared popular methods examples found project website httpsatomovideogithubio,-1,0.0,-1,0.0
semantically consistent videotoaudio generation using multimodal language large model existing works made strides video generation lack sound effects sfx background music bgm hinders complete immersive viewer experience introduce novel semantically consistent v ideotoaudio generation framework namely sva automatically generates audio semantically consistent given video content framework harnesses power multimodal large language model mllm understand video semantics key frame generate creative audio schemes utilized prompts texttoaudio models resulting videotoaudio generation natural language interface show satisfactory performance sva case study discuss limitations along future research direction project page available,15,0.9164996508256604,15,0.9164996508256604
elevating flowguided video inpainting reference generation video inpainting vi challenging task requires effective propagation observable content across frames simultaneously generating new content present original video study propose robust practical vi framework leverages large generative model reference generation combination advanced pixel propagation algorithm powered strong generative model method significantly enhances framelevel quality object removal also synthesizes new content missing areas based userprovided text prompts pixel propagation introduce oneshot pixel pulling method effectively avoids error accumulation repeated sampling maintaining subpixel precision evaluate various vi methods realistic scenarios also propose highquality vi benchmark hqvi comprising carefully generated videos using alpha matte composition public benchmarks hqvi dataset method demonstrates significantly higher visual quality metric scores compared existing solutions furthermore process highresolution videos exceeding resolution ease underscoring superiority realworld applications,-1,0.0,-1,0.0
rdpm solve diffusion probabilistic models via recurrent token prediction diffusion probabilistic models dpms emerged de facto approach highfidelity image synthesis operating diffusion processes continuous vae latent significantly differ text generation methods employed large language models llms paper introduce novel generative framework recurrent diffusion probabilistic model rdpm enhances diffusion process recurrent token prediction mechanism thereby pioneering field discrete diffusion progressively introducing gaussian noise latent representations images encoding vectorquantized tokens recurrent manner rdpm facilitates unique diffusion process discretevalue domains process iteratively predicts token codes subsequent timesteps transforming initial standard gaussian noise source data distribution aligning gptstyle models terms loss function rdpm demonstrates superior performance benefiting speed advantage requiring inference steps model leverages diffusion process ensure highquality generation also converts continuous signals series highfidelity discrete tokens thereby maintaining unified optimization strategy discrete tokens text anticipate work contribute development unified model multimodal generation specifically integrating continuous signal domains images videos audio text release code model weights opensource community,-1,0.0,-1,0.0
controlling rate distortion realism towards single comprehensive neural image compression model recent years neural networkdriven image compression nic gained significant attention works adopt deep generative models gans diffusion models enhance perceptual quality realism critical obstacle generative nic methods model optimized single bit rate consequently multiple models required compress images different bit rates impractical realworld applications tackle issue propose variablerate generative nic model specifically explore several discriminator designs tailored variablerate approach introduce novel adversarial loss moreover incorporating newly proposed multirealism technique method allows users adjust bit rate distortion realism single model achieving ultracontrollability unlike existing variablerate generative nic models method matches surpasses performance stateoftheart singlerate generative nic models covering wide range bit rates using one model code available httpsgithubcomiwashicrdr,-1,0.0,-1,0.0
vase objectcentric appearance shape manipulation real videos recently several works tackled video editing task fostered success largescale texttoimage generative models however methods holistically edit frame using text exploiting prior given foundation diffusion models focusing improving temporal consistency across frames work introduce framework objectcentric designed control objects appearance notably execute precise explicit structural modifications object build framework pretrained imageconditioned diffusion model integrate layers handle temporal dimension propose training strategies architectural modifications enable shape control evaluate method imagedriven video editing task showing similar performance stateoftheart showcasing novel shapeediting capabilities details code examples available project page,-1,0.0,-1,0.0
stylecinegan landscape cinemagraph generation using pretrained stylegan propose method generate cinemagraphs automatically still landscape image using pretrained stylegan inspired success recent unconditional video generation leverage powerful pretrained image generator synthesize highquality cinemagraphs unlike previous approaches mainly utilize latent space pretrained stylegan approach utilizes deep feature space gan inversion cinemagraph generation specifically propose multiscale deep feature warping msdfw warps intermediate features pretrained stylegan different resolutions using msdfw generated cinemagraphs high resolution exhibit plausible looping animation demonstrate superiority method user studies quantitative comparisons stateoftheart cinemagraph generation methods video generation method uses pretrained stylegan,-1,0.0,-1,0.0
lidardm generative lidar simulation generated world present lidardm novel lidar generative model capable producing realistic layoutaware physically plausible temporally coherent lidar videos lidardm stands two unprecedented capabilities lidar generative modeling lidar generation guided driving scenarios offering significant potential autonomous driving simulations ii lidar point cloud generation enabling creation realistic temporally coherent sequences heart model novel integrated world generation framework specifically employ latent diffusion models generate scene combine dynamic actors form underlying world subsequently produce realistic sensory observations within virtual environment experiments indicate approach outperforms competing algorithms realism temporal coherency layout consistency additionally show lidardm used generative world model simulator training testing perception models,-1,0.0,-1,0.0
crossmodal magnetic resonance imaging synthesis leveraging multiscale brain structures spanning multiple scalesfrom macroscopic anatomy intricate microscopic architecturethe human brain exemplifies complex system demands integrated approaches fully understand complexity yet mapping nonlinear relationships scales remains challenging due technical limitations high cost multimodal magnetic resonance imaging mri acquisition introduce deep learning framework predicts brain microstructure macrostructure using generative adversarial network gan grounded scalefree selfsimilar nature brain organizationwhere microscale information inferred macroscale explicitly encodes multiscale brain representations distinct processing branches enhance image fidelity suppress artifacts propose simple yet effective auxiliary discriminator learning objective results show faithfully translates mris corresponding fractional anisotropy fa images achieving improvement structural similarity index measure ssim compared previous methods preserving individual neurobiological characteristics,-1,0.0,-1,0.0
learning longform video prior via generative pretraining concepts involved longform videos people objects interactions viewed following implicit prior notably complex continue pose challenges comprehensively learned recent years generative pretraining gpt exhibited versatile capacities modeling kind text content even visual locations manner work learning longform video prior instead operating pixel space efficient employ visual locations like bounding boxes keypoints represent key information videos simply discretized tokenized consumption gpt due scarcity suitable data create new dataset called movies serve representative includes synopses shotbyshot keyframes finegrained annotations film sets characters consistent ids bounding boxes whole body keypoints way longform videos represented set tokens learned via generative pretraining experimental results validate approach great potential learning longform video prior code data released urlhttpsgithubcomshowlablongformvideoprior,0,0.9201956118358338,0,0.9201956118358338
bviugc video quality database usergenerated content transcoding recent years usergenerated content ugc become one major video types consumed via streaming networks numerous research contributions focused assessing visual quality subjective tests objective modeling cases objective assessments based noreference scenario corresponding reference content assumed available however fullreference video quality assessment also important ugc delivery pipeline particularly associated video transcoding process context present new ugc video quality database bviugc usergenerated content transcoding contains nonpristine reference videos test sequences work simulated creation nonpristine reference sequences wide range compression distortions typical content uploaded ugc platforms transcoding comprehensive crowdsourced subjective study conducted involving human participants based collected subjective data benchmarked performance fullreference noreference quality metrics results demonstrate poor performance srocc values lower metrics predicting perceptual quality ugc two different scenarios without reference,12,1.0,12,1.0
towards universal synthetic video detector face background manipulations fully aigenerated content existing deepfake detection techniques primarily focus facial manipulations faceswapping lipsyncing however advancements texttovideo imagetovideo generative models allow fully aigenerated synthetic content seamless background alterations challenging facecentric detection methods demanding versatile approaches address introduce underlineuniversal underlinenetwork underlineidentifying underlinetampered synthunderlineetic videos textttunite model unlike traditional detectors captures fullframe manipulations textttunite extends detection capabilities scenarios without faces nonhuman subjects complex background modifications leverages transformerbased architecture processes domainagnostic features extracted videos via foundation model given limited datasets encompassing facialbackground alterations content integrate taskirrelevant data alongside standard deepfake datasets training mitigate models tendency overfocus faces incorporating attentiondiversity ad loss promotes diverse spatial attention across video frames combining ad loss crossentropy improves detection performance across varied contexts comparative evaluations demonstrate textttunite outperforms stateoftheart detectors datasets crossdata settings featuring facebackground manipulations fully synthetic videos showcasing adaptability generalizable detection capabilities,-1,0.0,-1,0.0
multimodal multigenre multipurpose audiovisual academic lecture dataset publishing opensource academic video recordings emergent prevalent approach sharing knowledge online videos carry rich multimodal information including speech facial body movements speakers well texts pictures slides possibly even papers although multiple academic video datasets constructed released support multimodal content recognition understanding tasks partially due lack highquality human annotations paper propose novel multimodal multigenre multipurpose audiovisual academic lecture dataset almost hours videos five sources covering computer science mathematics medical biology topics highquality human annotations slide text spoken words particular highvalued name entities dataset used multiple audiovisual recognition understanding tasks evaluations performed contextual speech recognition speech synthesis slide script generation tasks demonstrate diversity makes challenging dataset,0,0.8809464332192091,0,0.8809464332192091
dintr tracking via diffusionbased interpolation object tracking fundamental task computer vision requiring localization objects interest across video frames diffusion models shown remarkable capabilities visual generation making wellsuited addressing several requirements tracking problem work proposes novel diffusionbased methodology formulate tracking task firstly conditional process allows injecting indications target object generation process secondly diffusion mechanics developed inherently model temporal correspondences enabling reconstruction actual frames video however existing diffusion models rely extensive unnecessary mapping gaussian noise domain replaced efficient stable interpolation process proposed interpolation mechanism draws inspiration classic imageprocessing techniques offering interpretable stable faster approach tailored specifically object tracking task leveraging strengths diffusion models circumventing limitations diffusionbased interpolation tracker dintr presents promising new paradigm achieves superior multiplicity seven benchmarks across five indicator representations,-1,0.0,-1,0.0
aigenerated video detection via spatiotemporal anomaly learning advancement generation models led emergence highly realistic artificial intelligence aigenerated videos malicious users easily create nonexistent videos spread false information letter proposes effective aigenerated video detection aigvdet scheme capturing forensic traces twobranch spatiotemporal convolutional neural network cnn specifically two resnet subdetectors learned separately identifying anomalies spatical optical flow domains respectively results subdetectors fused enhance discrimination ability largescale generated video dataset gvd constructed benchmark model training evaluation extensive experimental results verify high generalization robustness aigvdet scheme code dataset available httpsgithubcommultimediaforaigvdet,4,1.0,4,1.0
consistent multiview diffusion enhancement despite advances neural rendering due scarcity highquality datasets inherent limitations multiview diffusion models view synthesis model generation restricted low resolutions suboptimal multiview consistency study present novel enhancement pipeline dubbed employs multiview latent diffusion model enhance coarse inputs preserving multiview consistency method includes poseaware encoder diffusionbased denoiser refine lowquality multiview images along data augmentation multiview attention module epipolar aggregation maintain consistent highquality outputs across views unlike existing videobased approaches model supports seamless multiview enhancement improved coherence across diverse viewing angles extensive evaluations show significantly outperforms existing methods boosting multiview enhancement perinstance optimization tasks,1,1.0,1,1.0
fast accurate cooperative radio map estimation enabled gan era realtime radio resource monitoring management urged support diverse wirelessempowered applications calls fast accurate estimation distribution radio resources usually represented spatial signal power strength geographical environment known radio map paper present cooperative radio map estimation crme approach enabled generative adversarial network gan called gancrme features fast accurate radio map estimation without transmitters information radio map inferred exploiting interaction distributed received signal strength rss measurements mobile users geographical map using deep neural network estimator resulting low dataacquisition cost computational complexity moreover ganbased learning algorithm proposed boost inference capability deep neural network estimator exploiting power generative ai simulation results showcase proposed gancrme even capable coarse errorcorrection geographical map information inaccurate,-1,0.0,-1,0.0
joyvasa portrait animal image animation diffusionbased audiodriven facial dynamics head motion generation audiodriven portrait animation made significant advances diffusionbased models improving video quality lipsync accuracy however increasing complexity models led inefficiencies training inference well constraints video length interframe continuity paper propose joyvasa diffusionbased method generating facial dynamics head motion audiodriven facial animation specifically first stage introduce decoupled facial representation framework separates dynamic facial expressions static facial representations decoupling allows system generate longer videos combining static facial representation dynamic motion sequences second stage diffusion transformer trained generate motion sequences directly audio cues independent character identity finally generator trained first stage uses facial representation generated motion sequences inputs render highquality animations decoupled facial representation identityindependent motion generation process joyvasa extends beyond human portraits animate animal faces seamlessly model trained hybrid dataset private chinese public english data enabling multilingual support experimental results validate effectiveness approach future work focus improving realtime performance refining expression control expanding applications portrait animation code available httpsgithubcomjdhalgojoyvasa,6,0.8544355674947859,6,0.8544355674947859
hallo hierarchical audiodriven visual synthesis portrait image animation field portrait image animation driven speech audio input experienced significant advancements generation realistic dynamic portraits research delves complexities synchronizing facial movements creating visually appealing temporally consistent animations within framework diffusionbased methodologies moving away traditional paradigms rely parametric models intermediate facial representations innovative approach embraces endtoend diffusion paradigm introduces hierarchical audiodriven visual synthesis module enhance precision alignment audio inputs visual outputs encompassing lip expression pose motion proposed network architecture seamlessly integrates diffusionbased generative models unetbased denoiser temporal alignment techniques reference network proposed hierarchical audiodriven visual synthesis offers adaptive control expression pose diversity enabling effective personalization tailored different identities comprehensive evaluation incorporates qualitative quantitative analyses approach demonstrates obvious enhancements image video quality lip synchronization precision motion diversity visualization access source code found httpsfudangenerativevisiongithubiohallo,6,0.42146101658932117,6,0.42146101658932117
got ta hear sound source aware vision audio generation visiontoaudio synthesis broad applications multimedia recent advancements methods made possible generate relevant audios inputs videos still images however immersiveness expressiveness generation limited one possible problem existing methods solely rely global scene overlook details local sounding objects ie sound sources address issue propose sound sourceaware generator able locally perceive multimodal sound sources scene visual detection crossmodality translation contrastively learns crossmodal sound source cmss manifold semantically disambiguate source finally attentively mix cmss semantics rich audio representation pretrained audio generator outputs sound model cmss manifold curate novel singlesoundsource visualaudio dataset vggsound also design sound source matching score measure localized audio relevance addressing generation soundsource level surpasses stateoftheart methods generation fidelity relevance evidenced extensive experiments demonstrate ability achieve intuitive control compositing vision text audio conditions generation tried heard,8,1.0,8,1.0
llmguided iterative selfrefinement physicsgrounded texttovideo generation texttovideo generation recently enabled transformerbased diffusion models current models lack capabilities adhering realworld common knowledge physical rules due limited understanding physical realism deficiency temporal modeling existing solutions either datadriven require extra model inputs generalizable outofdistribution domains paper present new dataindependent technique expands current models capability video generation outofdistribution domains enabling chainofthought stepback reasoning prompting experiments show improves existing models adherence realworld physical rules achieves improvement compared prompt enhancers source codes available,-1,0.0,-1,0.0
quark realtime highresolution general neural view synthesis present novel neural algorithm performing highquality highresolution realtime novel view synthesis sparse set input rgb images videos streams network reconstructs scene renders novel views resolution nvidia feedforward network generalizes across wide variety datasets scenes produces stateoftheart quality realtime method quality approaches cases surpasses quality top offline methods order achieve results use novel combination several key concepts tie together cohesive effective algorithm build previous works represent scene using semitransparent layers use iterative learned renderandrefine approach improve layers instead flat layers method reconstructs layered depth maps ldms efficiently represent scenes complex depth occlusions iterative update steps embedded multiscale unetstyle architecture perform much compute possible reduced resolution within update step better aggregate information multiple input views use specialized transformerbased network component allows majority perinput image processing performed input image space opposed layer space increasing efficiency finally due realtime nature reconstruction rendering dynamically create discard internal geometry frame generating ldm view taken together produces novel effective algorithm view synthesis extensive evaluation demonstrate achieve stateoftheart quality realtime rates project page,1,1.0,1,1.0
lightit illumination modeling control diffusion models introduce lightit method explicit illumination control image generation recent generative methods lack lighting control crucial numerous artistic aspects image generation setting overall mood cinematic appearance overcome limitations propose condition generation shading normal maps model lighting single bounce shading includes cast shadows first train shading estimation module generate dataset realworld images shading pairs train control network using estimated shading normals input method demonstrates highquality image generation lighting control numerous scenes additionally use generated dataset train identitypreserving relighting model conditioned image target shading method first enables generation images controllable consistent lighting performs par specialized relighting stateoftheart methods,-1,0.0,-1,0.0
shotit computeefficient imagetovideo search engine cloud rapid growth information technology users exposed massive amount data online including image music video led strong needs provide effective corresponsive search services image music video search services operated based keywords namely using keywords find related image music video additionally imagetoimage search services enable users find similar images using one input image given videos essentially composed image frames similar videos searched one input image screenshot want target scenario provide efficient method implementation paper present shotit cloudnative imagetovideo search engine tailors search scenario computeefficient approach one main limitation faced scenario scale dataset typical imagetoimage search engine handles onetoone relationships colloquially one image corresponds another single image imagetovideo proliferates take length video example generate roughly image frames number videos grows scale dataset explodes exponentially case computeefficient approach ought considered system design cater cloudnative trend choosing emerging technology vector database backbone shotit fits two metrics performantly experiments two different datasets thousandscale blender open movie dataset millionscale proprietary tv genre dataset core ram intel xeon gold cloud machine object storage reveal effectiveness shotit demo regarding blender open movie dataset illustrated within paper,-1,0.0,-1,0.0
human action clips detecting aigenerated human motion fullblown aigenerated video generation continues journey uncanny valley produce content perceptually indistinguishable reality intermixed many exciting creative applications malicious applications harm individuals organizations democracies describe effective robust technique distinguishing real aigenerated human motion technique leverages multimodal semantic embedding making robust types laundering typically confound low midlevel approaches method evaluated custombuilt dataset video clips human actions generated seven texttovideo ai models matching real footage,-1,0.0,-1,0.0
fight scene detection movie highlight generation system paper research based project using bidirectional long shortterm memory bilstm networks provide novel fight scene detection fsd model used movie highlight generation systems mhgs based deep learning neural networks movies usually fight scenes keep audience amazed trailer generation application highlight generation tidious first identify scenes manually compile generate highlight serving purpose proposed fsd system utilises temporal characteristics movie scenes thus capable automatically identify fight scenes thereby helping effective production captivating movie highlights observe proposed solution features accuracy higher cnn hough forests accurate significantly higher cnn features accuracy,4,0.7791289004365893,4,0.7791289004365893
deepfake generation detection benchmark survey deepfake technology dedicated creating highly realistic facial images videos specific conditions significant application potential fields entertainment movie production digital human creation name advancements deep learning techniques primarily represented variational autoencoders generative adversarial networks achieved impressive generation results recently emergence diffusion models powerful generation capabilities sparked renewed wave research addition deepfake generation corresponding detection technologies continuously evolve regulate potential misuse deepfakes privacy invasion phishing attacks survey comprehensively reviews latest developments deepfake generation detection summarizing analyzing current stateofthearts rapidly evolving field first unify task definitions comprehensively introduce datasets metrics discuss developing technologies discuss development several related subfields focus researching four representative deepfake fields face swapping face reenactment talking face generation facial attribute editing well forgery detection subsequently comprehensively benchmark representative methods popular datasets field fully evaluating latest influential published works finally analyze challenges future research directions discussed fields,4,0.7518011170638895,4,0.7518011170638895
instance brownian bridge texts openvocabulary video instance segmentation temporally locating objects arbitrary class texts primary pursuit openvocabulary video instance segmentation vis insufficient vocabulary video data previous methods leverage imagetext pretraining model recognizing object instances separately aligning frame class texts ignoring correlation frames result separation breaks instance movement context videos causing inferior alignment video text tackle issue propose link framelevel instance representations brownian bridge model instance dynamics align bridgelevel instance representation class texts precisely openvocabulary vis brivis specifically build system upon frozen video segmentor generate framelevel instance queries design temporal instance resampler tir generate queries temporal context frame queries mold instance queries follow brownian bridge accomplish alignment class texts design bridgetext alignment bta learn discriminative bridgelevel representations instances via contrastive objectives setting minvis basic video segmentor brivis surpasses openvocabulary sota clear margin example challenging largevocabulary vis dataset burst brivis achieves map exhibits improvement compared map,7,1.0,7,1.0
spikemba multimodal spiking saliency mamba temporal video grounding temporal video grounding tvg critical task video content understanding requiring precise alignment video content natural language instructions despite significant advancements existing methods face challenges managing confidence bias towards salient objects capturing longterm dependencies video sequences address issues introduce spikemba multimodal spiking saliency mamba temporal video grounding approach integrates spiking neural networks snns state space models ssms leverage unique advantages handling different aspects task specifically use snns develop spiking saliency detector generates proposal set detector emits spike signals input signal exceeds predefined threshold resulting dynamic binary saliency proposal set enhance models capability retain infer contextual information introduce relevant slots learnable tensors encode prior knowledge slots work contextual moment reasoner maintain balance preserving contextual information exploring semantic relevance dynamically ssms facilitate selective information propagation addressing challenge longterm dependency video content combining snns proposal generation ssms effective contextual reasoning spikemba addresses confidence bias longterm dependencies thereby significantly enhancing finegrained multimodal relationship capture experiments demonstrate effectiveness spikemba consistently outperforms stateoftheart methods across mainstream benchmarks,7,0.9201342459570006,7,0.9201342459570006
holmesvad towards unbiased explainable video anomaly detection via multimodal llm towards openended video anomaly detection vad existing methods often exhibit biased detection faced challenging unseen events lack interpretability address drawbacks propose holmesvad novel framework leverages precise temporal supervision rich multimodal instructions enable accurate anomaly localization comprehensive explanations firstly towards unbiased explainable vad system construct first largescale multimodal vad instructiontuning benchmark ie dataset created using carefully designed semiautomatic labeling paradigm efficient singleframe annotations applied collected untrimmed videos synthesized highquality analyses abnormal normal video clips using robust offtheshelf video captioner large language model llm building upon dataset develop customized solution interpretable video anomaly detection train lightweight temporal sampler select frames high anomaly response finetune multimodal large language model llm generate explanatory content extensive experimental results validate generality interpretability proposed holmesvad establishing novel interpretable technique realworld video anomaly analysis support community benchmark model publicly available httpsholmesvadgithubio,0,0.8898202405051708,0,0.8898202405051708
missiongnn hierarchical multimodal gnnbased weakly supervised video anomaly recognition missionspecific knowledge graph generation context escalating safety concerns across various domains tasks video anomaly detection vad video anomaly recognition var emerged critically important applications intelligent surveillance evidence investigation violence alerting etc tasks aimed identifying classifying deviations normal behavior video data face significant challenges due rarity anomalies leads extremely imbalanced data impracticality extensive framelevel data annotation supervised learning paper introduces novel hierarchical graph neural network gnn based model missiongnn addresses challenges leveraging stateoftheart large language model comprehensive knowledge graph efficient weakly supervised learning var approach circumvents limitations previous methods avoiding heavy gradient computations large multimodal models enabling fully framelevel training without fixed video segmentation utilizing automated missionspecific knowledge graph generation model provides practical efficient solution realtime video analysis without constraints previous segmentationbased multimodal approaches experimental validation benchmark datasets demonstrates models performance vad var highlighting potential redefine landscape anomaly detection recognition video surveillance systems,4,0.7791289004365893,4,0.7791289004365893
videoeval comprehensive benchmark suite lowcost evaluation video foundation model growth highquality data advancement visual pretraining paradigms video foundation models vfms made significant progress recently demonstrating remarkable performance traditional video understanding benchmarks however existing benchmarks eg kinetics evaluation protocols often limited relatively poor diversity high evaluation costs saturated performance metrics paper build comprehensive benchmark suite address issues namely videoeval specifically establish video task adaption benchmark vidtab video embedding benchmark videb two perspectives evaluating task adaptability vfms fewshot conditions assessing representation power directly applying downstream tasks videoeval conduct largescale study popular opensource vision foundation models study reveals insightful findings vfms overall current vfms exhibit weak generalization across diverse tasks increasing video data whether labeled weaklylabeled videotext pairs necessarily improve task performance effectiveness pretraining paradigms may fully validated previous benchmarks combining different pretraining paradigms help improve generalization capabilities believe study serves important complement current evaluation vfms offers valuable insights future research,10,0.6431727539269747,10,0.6431727539269747
learning natural consistency representation face forgery video detection face forgery videos elicited critical social public concerns various detectors proposed however fullysupervised detectors may lead easily overfitting specific forgery methods videos existing selfsupervised detectors strict auxiliary tasks requiring audio multimodalities leading limited generalization robustness paper examine whether address issue leveraging visualonly real face videos end propose learn natural consistency representation naco real face videos selfsupervised manner inspired observation fake videos struggle maintain natural spatiotemporal consistency even unknown forgery methods different perturbations naco first extracts spatial features frame cnns integrates transformer learn longrange spatiotemporal representation leveraging advantages cnns transformer local spatial receptive field longterm memory respectively furthermore spatial predictive modulespm temporal contrastive moduletcm introduced enhance natural consistency representation learning spm aims predict random masked spatial features spatiotemporal representation tcm regularizes latent distance spatiotemporal representation shuffling natural order disturb consistency could force naco sensitive natural spatiotemporal consistency representation learning stage mlp head finetuned perform usual forgery video classification task extensive experiments show method outperforms stateoftheart competitors impressive generalization robustness,-1,0.0,-1,0.0
queryefficient video adversarial attack stylized logo video classification systems based deep neural networks dnns demonstrated excellent performance accurately verifying video content however recent studies shown dnns highly vulnerable adversarial examples therefore deep understanding adversarial attacks better respond emergency situations order improve attack performance many styletransferbased attacks patchbased attacks proposed however global perturbation former bring unnatural global color latter difficult achieve success targeted attacks due limited perturbation space moreover compared plethora methods targeting image classifiers video adversarial attacks still popular therefore generate adversarial examples low budget provide higher verisimilitude propose novel blackbox video attack framework called stylized logo attack sla sla conducted three steps first step involves building style references set logos make generated examples natural also carry target class features targeted attacks reinforcement learning rl employed determine style reference position parameters logo within video ensures stylized logo placed video optimal attributes finally perturbation optimization designed optimize perturbations improve fooling rate stepbystep manner sufficient experimental results indicate sla achieve better performance stateoftheart methods still maintain good deception effects facing various defense methods,4,1.0,4,1.0
personalized video summarization using textbased queries conditional modeling proliferation video content platforms like youtube vimeo presents significant challenges efficiently locating relevant information automatic video summarization aims address extracting presenting key content condensed form thesis explores enhancing video summarization integrating textbased queries conditional modeling tailor summaries user needs traditional methods often produce fixed summaries may align individual requirements overcome propose multimodal deep learning approach incorporates textual queries visual information fusing different levels model architecture evaluation metrics accuracy assess quality generated summaries thesis also investigates improving textbased query representations using contextualized word embeddings specialized attention networks enhances semantic understanding queries leading better video summaries emulate humanlike summarization accounts visual coherence abstract factors like storyline consistency introduce conditional modeling approach method uses multiple random variables joint distributions capture key summarization components resulting humanlike explainable summaries addressing data scarcity fully supervised learning thesis proposes segmentlevel pseudolabeling approach selfsupervised method generates additional data improving model performance even limited humanlabeled datasets summary research aims enhance automatic video summarization incorporating textbased queries improving query representations introducing conditional modeling addressing data scarcity thereby creating effective personalized video summaries,0,1.0,0,1.0
realizing video summarization path languagebased semantic understanding recent development videobased large language models videollms significantly advanced video summarization aligning video features cases audio features large language models llms videollms possesses unique strengths weaknesses many recent methods required extensive finetuning overcome limitations models resourceintensive work observe strengths one videollm complement weaknesses another leveraging insight propose novel video summarization framework inspired mixture experts moe paradigm operates inferencetime algorithm without requiring form finetuning approach integrates multiple videollms generate comprehensive coherent textual summaries effectively combines visual audio content provides detailed background descriptions excels identifying keyframes enables semantically meaningful retrieval compared traditional computer vision approaches rely solely visual information without need additional finetuning moreover resulting summaries enhance performance downstream tasks summary video generation either keyframe selection combination texttoimage models languagedriven approach offers semantically rich alternative conventional methods provides flexibility incorporate newer videollms enhancing adaptability performance video summarization tasks,0,0.9661712583953785,0,0.9661712583953785
verified video corpus moment retrieval benchmark finegrained video understanding existing video corpus moment retrieval vcmr limited coarsegrained understanding hinders precise video moment localization given finegrained queries paper propose challenging finegrained vcmr benchmark requiring methods localize bestmatched moment corpus partially matched candidates improve dataset construction efficiency guarantee highquality data annotations propose verified automatic underlinevidunderlineeotext annotation pipeline generate captions underlinerelunderlineiable underlinefinunderlineegrained statics underlinedynamics specifically resort large language models llm large multimodal models lmm proposed statics dynamics enhanced captioning modules generate diverse finegrained captions video filter inaccurate annotations caused llm hallucination propose finegranularity aware noise evaluator finetune video foundation model disturbed hardnegatives augmented contrastive matching losses verified construct challenging finegrained vcmr benchmark containing charadesfig didemofig activitynetfig demonstrate high level annotation quality evaluate several stateoftheart vcmr models proposed dataset revealing still significant scope finegrained video understanding vcmr code datasets,0,0.8909962344321367,0,0.8909962344321367
dashcam videos driving simulations stress testing automated vehicles rare events testing automated driving systems ads simulation realistic driving scenarios important verifying performance however converting realworld driving videos simulation scenarios significant challenge due complexity interpreting highdimensional video data timeconsuming nature precise manual scenario reconstruction work propose novel framework automates conversion realworld car crash videos detailed simulation scenarios ads testing approach leverages promptengineered video language modelsvlm transform dashcam footage scenic scripts define environment driving behaviors carla simulator enabling generation realistic simulation scenarios importantly rather solely aiming onetoone scenario reconstruction framework focuses capturing essential driving behaviors original video offering flexibility parameters weather road conditions facilitate searchbased testing additionally introduce similarity metric helps iteratively refine generated scenario feedback comparing key features driving behaviors real simulated videos preliminary results demonstrate substantial time efficiency finishing realtosim conversion minutes full automation human intervention maintaining high fidelity original driving events,16,1.0,16,1.0
overview trec medical video question answering medvidqa track one key goals artificial intelligence ai development multimodal system facilitates communication visual world image video using natural language query earlier works medical question answering primarily focused textual visual image modalities may inefficient answering questions requiring demonstration recent years significant progress achieved due introduction largescale languagevision datasets development efficient deep neural techniques bridge gap language visual understanding improvements made numerous visionandlanguage tasks visual captioning visual question answering natural language video localization existing work language vision focused creating datasets developing solutions opendomain applications believe medical videos may provide best possible answers many first aid medical emergency medical education questions increasing interest ai support clinical decisionmaking improve patient engagement need explore challenges develop efficient algorithms medical languagevideo understanding generation toward introduced new tasks foster research toward designing systems understand medical videos provide visual answers natural language questions equipped multimodal capability generate instruction steps medical video tasks potential support development sophisticated downstream applications benefit public medical professionals,0,1.0,0,1.0
youtube datadriven analysis sentiment toxicity content recommendations study presents datadriven analysis discourse youtube examining sentiment toxicity thematic patterns video content published january october analysis involved applying advanced natural language processing nlp techniques sentiment analysis vader toxicity detection detoxify topic modeling using latent dirichlet allocation lda sentiment analysis revealed video descriptions positive neutral negative indicating generally informative supportive tone pandemicrelated content toxicity analysis identified content toxic suggesting minimal exposure toxic content topic modeling revealed two main themes videos covering general health information pandemicrelated impacts focused news realtime updates highlighting dual informational role youtube recommendation system also developed using tfidf vectorization cosine similarity refined sentiment toxicity topic filters ensure relevant contextaligned video recommendations system achieved aggregate coverage monthly coverage rates consistently demonstrating robust performance adaptability time evaluation across recommendation sizes showed coverage reaching five video recommendations ten video recommendations per video summary work presents framework understanding discourse youtube recommendation system supports user engagement promoting responsible relevant content related,-1,0.0,-1,0.0
xmic crossmodal instance conditioning egocentric action generalization lately growing interest adapting visionlanguage models vlms image thirdperson video classification due success zeroshot recognition however adaptation models egocentric videos largely unexplored address gap propose simple yet effective crossmodal adaptation framework call xmic using video adapter pipeline learns align frozen text embeddings egocentric video directly shared embedding space novel adapter architecture retains improves generalization pretrained vlms disentangling learnable temporal modeling frozen visual encoder results enhanced alignment text embeddings egocentric video leading significant improvement crossdataset generalization evaluate approach epickitchens egtea datasets finegrained crossdataset action generalization demonstrating effectiveness method code available httpsgithubcomannushaxmic,7,0.8620847842216571,7,0.8620847842216571
evaluation texttovideo generation models dynamics perspective comprehensive constructive evaluation protocols play important role development sophisticated texttovideo generation models existing evaluation protocols primarily focus temporal consistency content continuity yet largely ignore dynamics video content dynamics essential dimension measuring visual vividness honesty video content text prompts study propose effective evaluation protocol termed devil centers dynamics dimension evaluate models purpose establish new benchmark comprising text prompts fully reflect multiple dynamics grades define set dynamics scores corresponding various temporal granularities comprehensively evaluate dynamics generated video based new benchmark dynamics scores assess models design three metrics dynamics range dynamics controllability dynamicsbased quality experiments show devil achieves pearson correlation exceeding human ratings demonstrating potential advance generation models code available httpsgithubcommingxiangldevil,10,0.6431727539269747,10,0.6431727539269747
general taskoriented video segmentation present gvseg general video segmentation framework addressing four different video segmentation tasks ie instance semantic panoptic exemplarguided maintaining identical architectural design currently trend towards developing general video segmentation solutions applied across multiple tasks streamlines research endeavors simplifies deployment however highly homogenized framework current design element maintains uniformity could overlook inherent diversity among different tasks lead suboptimal performance tackle gvseg provides holistic disentanglement modeling segment targets thoroughly examining perspective appearance position shape basis ii reformulates query initialization matching sampling strategies alignment taskspecific requirement architectureagnostic innovations empower gvseg effectively address unique task accommodating specific properties characterize extensive experiments seven goldstandard benchmark datasets demonstrate gvseg surpasses existing specializedgeneral solutions significant margin four different video segmentation tasks,-1,0.0,-1,0.0
attentionbased generative adversarial network joint spacetime video superresolution many applications including surveillance entertainment restoration need increase spatial resolution frame rate video sequence aim improve visual quality refine details create realistic viewing experience existing spacetime video superresolution methods effectively use spatiotemporal information address limitation propose generative adversarial network joint spacetime video superresolution generative network consists three operations shallow feature extraction deep feature extraction reconstruction uses threedimensional convolutions process temporal spatial information simultaneously includes novel attention mechanism extract important channel spatial information discriminative network uses twobranch structure handle details motion information making generated results accurate experimental results reds datasets demonstrate effectiveness proposed method source code publicly available,-1,0.0,-1,0.0
dormant defending posedriven human image animation posedriven human image animation achieved tremendous progress enabling generation vivid realistic human videos one single photo however conversely exacerbates risk image misuse attackers may use one available image create videos involving politics violence illegal content counter threat propose dormant novel protection approach tailored defend posedriven human image animation techniques dormant applies protective perturbation one human image preserving visual similarity original resulting poorquality video generation protective perturbation optimized induce misextraction appearance features image create incoherence among generated video frames extensive evaluation across animation methods datasets demonstrates superiority dormant baseline protection methods leading misaligned identities visual distortions noticeable artifacts inconsistent frames generated videos moreover dormant shows effectiveness realworld commercial services even fully blackbox access,4,1.0,4,1.0
simtube generating simulated video comments multimodal ai user personas audience feedback crucial refining video content yet typically comes publication limiting creators ability make timely adjustments bridge gap introduce simtube generative ai system designed simulate audience feedback form video comments videos release simtube features computational pipeline integrates multimodal data videosuch visuals audio metadatawith user personas derived broad diverse corpus audience demographics generating varied contextually relevant feedback furthermore systems ui allows creators explore customize simulated comments comprehensive evaluationcomprising quantitative analysis crowdsourced assessments qualitative user studieswe show simtubes generated comments relevant believable diverse often detailed informative actual audience comments highlighting potential help creators refine content release,15,0.9448348372489216,15,0.9448348372489216
arcon advancing autoregressive continuation driving videos recent advancements autoregressive large language models llms led application video generation paper explores use large vision models lvms video continuation task essential building world models predicting future frames introduce arcon scheme alternates generating semantic rgb tokens allowing lvm explicitly learn highlevel structural video information find high consistency rgb images semantic maps generated without special design moreover employ optical flowbased texture stitching method enhance visual quality experiments autonomous driving scenarios show model consistently generate long videos,-1,0.0,-1,0.0
manivideo generating handobject manipulation video dexterous generalizable grasping paper introduce manivideo novel method generating consistent temporally coherent bimanual handobject manipulation videos given motion sequences hands objects core idea manivideo construction multilayer occlusion mlo representation learns occlusion relationships occlusionfree normal maps occlusion confidence maps embedding mlo structure unet two forms model enhances consistency dexterous handobject manipulation achieve generalizable grasping objects integrate objaverse largescale object dataset address scarcity video data thereby facilitating learning extensive object consistency additionally propose innovative training strategy effectively integrates multiple datasets supporting downstream tasks humancentric handobject manipulation video generation extensive experiments demonstrate approach achieves video generation plausible handobject interaction generalizable objects also outperforms existing sota methods,5,0.42145643697344,5,0.42145643697344
gender bias texttovideo generation models case study sora advent texttovideo generation models revolutionized content creation produces highquality videos textual prompts however concerns regarding inherent biases models prompted scrutiny particularly regarding gender representation study investigates presence gender bias openais sora stateoftheart texttovideo generation model uncover significant evidence bias analyzing generated videos diverse set genderneutral stereotypical prompts results indicate sora disproportionately associates specific genders stereotypical behaviors professions reflects societal prejudices embedded training data,10,0.6310985614712533,10,0.6310985614712533
jpeglm llms image generators canonical codec representations recent work image video generation adopting autoregressive llm architecture due generality potentially easy integration multimodal systems crux applying autoregressive training language generation visual generation discretization representing continuous data like images videos discrete tokens common methods discretizing images videos include modeling raw pixel values prohibitively lengthy vector quantization requires convoluted prehoc training work propose directly model images videos compressed files saved computers via canonical codecs eg jpeg using default llama architecture without visionspecific modifications pretrain jpeglm scratch generate images avclm generate videos proof concept directly outputting compressed file bytes jpeg avc formats evaluation image generation shows simple straightforward approach effective pixelbased modeling sophisticated vector quantization baselines method yields reduction fid analysis shows jpeglm especial advantage vector quantization models generating longtail visual elements overall show using canonical codec representations help lower barriers language generation visual generation facilitating future research multimodal languageimagevideo llms,-1,0.0,-1,0.0
quality prediction ai generated images videos emerging trends opportunities advent ai influenced many aspects human life selfdriving cars intelligent chatbots textbased image video generation models capable creating realistic images videos based user prompts texttoimage imagetoimage imagetovideo aibased methods image video super resolution video frame interpolation denoising compression already gathered significant attention interest industry solutions already implemented realworld products services however achieve widespread integration acceptance aigenerated enhanced content must visually accurate adhere intended use maintain high visual quality avoid degrading end users quality experience qoe one way monitor control visual quality aigenerated enhanced content deploying image quality assessment iqa video quality assessment vqa models however existing iqa vqa models measure visual fidelity terms reconstruction quality pristine reference content designed assess quality generative artifacts address newer metrics models recently proposed performance evaluation overall efficacy limited datasets small otherwise lack representative content andor distortion capacity performance measures accurately report success iqavqa model genai paper examines current shortcomings possibilities presented aigenerated enhanced image video content particular focus enduser perceived quality finally discuss open questions make recommendations future work genai quality assessment problems towards progressing interesting relevant field research,-1,0.0,-1,0.0
generative enhancement medical images limited availability medical image datasets due privacy concerns high collection annotation costs poses significant challenges field medical imaging promising alternative use synthesized medical data solutions realistic medical image synthesis due difficulties backbone design fewer training samples compared counterparts paper propose novel generative approach synthesis medical images enhancement existing datasets using conditional diffusion models method begins slice noted informed slice serve patient prior propagates generation process using segmentation mask decomposing medical images masks patient prior information offers flexible yet effective solution generating versatile images existing datasets enable dataset enhancement combining informed slice selection generation random positions along editable mask volumes introduce large variations diffusion sampling moreover informed slice contains patientwise information also facilitate counterfactual image synthesis datasetlevel deenhancement desired control experiments brain mri abdomen ct images demonstrate capable synthesizing highquality medical images volumetric consistency offering straightforward solution dataset enhancement inference code available,3,0.8794881923138937,3,0.8794881923138937
convofusion multimodal conversational diffusion cospeech gesture synthesis gestures play key role human communication recent methods cospeech gesture generation managing generate beataligned motions struggle generating gestures semantically aligned utterance compared beat gestures align naturally audio signal semantically coherent gestures require modeling complex interactions language human motion controlled focusing certain words therefore present convofusion diffusionbased approach multimodal gesture synthesis generate gestures based multimodal speech inputs also facilitate controllability gesture synthesis method proposes two guidance objectives allow users modulate impact different conditioning modalities eg audio vs text well choose certain words emphasized gesturing method versatile trained either generating monologue gestures even conversational gestures advance research multiparty interactive gestures dnd group gesture dataset released contains hours gesture data showing people interacting one another compare method several recent works demonstrate effectiveness method variety tasks urge reader watch supplementary video website,-1,0.0,-1,0.0
towards languageguided semanticaware eventtovideo reconstruction event cameras harness advantages low latency high temporal resolution high dynamic range hdr compared standard cameras due distinct imaging paradigm shift dominant line research focuses eventtovideo reconstruction bridge eventbased standard computer vision however task remains challenging due inherently illposed nature event cameras detect edge motion information locally consequently reconstructed videos often plagued artifacts regional blur primarily caused ambiguous semantics event data paper find language naturally conveys abundant semantic information rendering stunningly superior ensuring semantic consistency reconstruction accordingly propose novel framework called achieve semanticaware highquality reconstruction languageguided perspective buttressed textconditional diffusion models however due diffusion models inherent diversity randomness hardly possible directly apply achieve spatial temporal consistency reconstruction thus first propose eventguided spatiotemporal attention esa module condition event data denoising pipeline effectively introduce eventaware mask loss ensure temporal coherence noise initialization strategy enhance spatial consistency given absence eventtextvideo paired data aggregate existing datasets generate textual descriptions using tagging models training evaluation extensive experiments three datasets covering diverse challenging scenarios eg fast motion low light demonstrate superiority method,-1,0.0,-1,0.0
noiseenabled static scene recovery event cameras event cameras also known dynamic vision sensors emerging modality measuring fast dynamics asynchronously event cameras capture changes logintensity time stream events generally measure intensity hence used imaging dynamic scenes however fluctuations due random photon arrival inevitably trigger noise events even static scenes previous efforts focused filtering undesirable noise events improve signal quality find photonnoise regime noise events correlated static scene intensity analyze noise event generation model relationship illuminance based understanding propose method called leverage illuminancedependent noise characteristics recover static parts scene otherwise invisible event cameras experimentally collect dataset noise events static scenes train validate results provide novel approach capturing static scenes event cameras solely noise events without additional hardware,1,1.0,1,1.0
simtoreal toward general eventbased lowlight frame interpolation perscene optimization video frame interpolation vfi important video enhancement frame rate upconversion slowmotion generation introduction event cameras capture perpixel brightness changes asynchronously significantly enhanced vfi capabilities particularly highspeed nonlinear motions however eventbased methods encounter challenges lowlight conditions notably trailing artifacts signal latency hinder direct applicability generalization addressing issues propose novel perscene optimization strategy tailored lowlight conditions approach utilizes internal statistics sequence handle degraded event data lowlight conditions improving generalizability different lighting camera settings evaluate robustness lowlight condition introduce evfill unique rgbevent dataset captured lowlight conditions results demonstrate stateoftheart performance lowlight environments project page,1,1.0,1,1.0
degstalk decomposed perembedding gaussian fields hairpreserving talking face synthesis accurately synthesizing talking face videos capturing fine facial features individuals long hair presents significant challenge tackle challenges existing methods propose decomposed perembedding gaussian fields degstalk gaussian splatting talking face synthesis method generating realistic talking faces long hairs degstalk employs deformable preembedding gaussian fields dynamically adjust preembedding gaussian primitives using implicit expression coefficients enables precise capture dynamic facial regions subtle expressions additionally propose dynamic hairpreserving portrait rendering technique enhance realism long hair motions synthesized videos results show degstalk achieves improved realism synthesis quality compared existing approaches particularly handling complex facial dynamics hair preservation code publicly available httpsgithubcomcviszudegstalk,6,0.630002726058148,6,0.630002726058148
cascaded multipath shortcut diffusion model medical image translation imagetoimage translation vital component medical imaging processing many uses wide range imaging modalities clinical scenarios previous methods include generative adversarial networks gans diffusion models dms offer realism suffer instability lack uncertainty estimation even though gan dm methods individually exhibited capability medical image translation tasks potential combining gan dm improve translation performance enable uncertainty estimation remains largely unexplored work address challenges proposing cascade multipath shortcut diffusion model cmdm highquality medical image translation uncertainty estimation reduce required number iterations ensure robust performance method first obtains conditional gangenerated prior image used efficient reverse translation dm subsequent step additionally multipath shortcut diffusion strategy employed refine translation results estimate uncertainty cascaded pipeline enhances translation quality incorporating residual averaging cascades collected three different medical image datasets two subtasks dataset test generalizability approach experimental results found cmdm produce highquality translations comparable stateoftheart methods providing reasonable uncertainty estimations correlate well translation error,-1,0.0,-1,0.0
conditional controllable image fusion image fusion aims integrate complementary information multiple input images acquired various sources synthesize new fused image existing methods usually employ distinct constraint designs tailored specific scenes forming fixed fusion paradigms however datadriven fusion approach challenging deploy varying scenarios especially rapidly changing environments address issue propose conditional controllable fusion ccf framework general image fusion tasks without specific training due dynamic differences different samples ccf employs specific fusion constraints individual practice given powerful generative capabilities denoising diffusion model first inject specific constraints pretrained ddpm adaptive fusion conditions appropriate conditions dynamically selected ensure fusion process remains responsive specific requirements reverse diffusion stage thus ccf enables conditionally calibrating fused images step step extensive experiments validate effectiveness general fusion tasks across diverse scenarios competing methods without additional training,-1,0.0,-1,0.0
codecnerf toward fast encoding decoding compact highquality novelview synthesis neural radiance fields nerf achieved huge success effectively capturing representing objects scenes however establish ubiquitous presence everyday media formats images videos need fulfill three key objectives fast encoding decoding time compact model sizes highquality renderings despite recent advancements comprehensive algorithm adequately addresses objectives yet fully realized work present codecnerf neural codec nerf representations consisting encoder decoder architecture generate nerf representation single forward pass furthermore inspired recent parameterefficient finetuning approaches propose finetuning method efficiently adapt generated nerf representations new test instance leading highquality image renderings compact code sizes proposed codecnerf newly suggested encodingdecodingfinetuning pipeline nerf achieved unprecedented compression performance remarkable reduction encoding time maintaining improving image quality widely used object datasets,2,1.0,2,1.0
rdsinger referencebased diffusion network singing voice synthesis singing voice synthesis svs aims produce highfidelity singing audio music scores requiring detailed understanding notes pitch duration unlike texttospeech tasks although diffusion models shown exceptional performance various generative tasks like image video creation application svs hindered time complexity challenge capturing acoustic features particularly pitch transitions networks learn prior distribution use compressed latent state better start diffusion model denoising step doesnt consistently improve quality entire duration introduce rdsinger referencebased denoising diffusion network generates highquality audio svs tasks approach inspired animate anyone diffusion image network maintains intricate appearance features reference images rdsinger utilizes melspectrogram reference mitigate denoising step artifacts additionally existing models could influenced misleading information compressed latent state pitch transitions address issue applying gaussian blur partial reference melspectrogram adjusting loss weights regions extensive ablation studies demonstrate efficiency method evaluations opencpop chinese singing dataset show rdsinger outperforms current stateoftheart svs methods performance,-1,0.0,-1,0.0
gameir largescale synthesized groundtruth dataset image restoration gaming content image restoration methods like superresolution image synthesis successfully used commercial cloud gaming products like nvidias dlss however restoration gaming content well studied general public discrepancy mainly caused lack groundtruth gaming training data match test cases due unique characteristics gaming content common approach generating pseudo training data degrading original hr images results inferior restoration performance work develop gameir largescale highquality computersynthesized groundtruth dataset fill blanks targeting two different applications first superresolution deferred rendering support gaming solution rendering transferring lr images restoring hr images client side provide lrhr paired groundtruth frames coming videos rendered task second novel view synthesis nvs support multiview gaming solution rendering transferring part multiview frames generating remaining frames client side task hr frames videos scenes camera views addition rgb frames gbuffers deferred rendering stage also provided used help restoration furthermore evaluate several sota superresolution algorithms nerfbased nvs algorithms dataset demonstrates effectiveness groundtruth gameir data improving restoration performance gaming content also test method incorporating gbuffers additional input information helping superresolution nvs release dataset models general public facilitate research restoration methods gaming content,1,1.0,1,1.0
network bending diffusion models audiovisual generation paper present first steps towards creation tool enables artists create music visualizations using pretrained generative machine learning models first investigate application network bending process applying transforms within layers generative network image generation diffusion models utilizing range pointwise tensorwise morphological operators identify number visual effects result various operators including easily recreated standard image editing tools find process allows continuous finegrain control image generation helpful creative applications next generate musicreactive videos using stable diffusion passing audio features parameters network bending operators finally comment certain transforms radically shift image possibilities learning latent space stable diffusion based transforms,-1,0.0,-1,0.0
disentangled multimodal brain mr image translation via transformerbased modality infuser multimodal magnetic resonance mr imaging plays crucial role disease diagnosis due ability provide complementary information analyzing relationship multimodal images subject acquiring mr modalities however expensive scanning session certain mr images may missed depending study protocol typical solution would synthesize missing modalities acquired images using generative adversarial networks gans yet gans constructed convolutional neural networks cnns likely suffer lack global relationships mechanisms condition desired modality address work propose transformerbased modality infuser designed synthesize multimodal brain mr images method extract modalityagnostic features encoder transform modalityspecific features using modality infuser furthermore modality infuser captures longrange relationships among brain structures leading generation realistic images carried experiments brats dataset translating four mr modalities experimental results demonstrate superiority proposed method terms synthesis quality addition conducted experiments brain tumor segmentation task different conditioning methods,3,0.8537457581452778,3,0.8537457581452778
gaussianflow splatting gaussian dynamics content creation creating fields gaussian splatting images videos challenging task due underconstrained nature optimization draw photometric reference input videos regulated generative models directly supervising gaussian motions remains underexplored paper introduce novel concept gaussian flow connects dynamics gaussians pixel velocities consecutive frames gaussian flow efficiently obtained splatting gaussian dynamics image space differentiable process enables direct dynamic supervision optical flow method significantly benefits dynamic content generation novel view synthesis gaussian splatting especially contents rich motions hard handled existing methods common color drifting issue happens generation also resolved improved guassian dynamics superior visual quality extensive experiments demonstrates methods effectiveness quantitative qualitative evaluations show method achieves stateoftheart results tasks generation novel view synthesis project page httpszergovermindgithubiogaussianflowgithubio,-1,0.0,-1,0.0
novel hybrid integrated wgan model gradient penalty binary images denoising paper introduces novel approach image denoising leverages advantages generative adversarial networks gans specifically propose model combines elements model wasserstein gan wgan gradient penalty wgangp hybrid framework seeks capitalize denoising capabilities conditional gans demonstrated model mitigating need exhaustive search optimal hyperparameters could potentially ruin stability learning process proposed method gans generator employed produce denoised images harnessing power conditional gan noise reduction simultaneously implementation lipschitz continuity constraint updates featured wgangp aids reducing susceptibility mode collapse innovative design allows proposed model benefit strong points wgangp generating superior denoising results ensuring training stability drawing previous work imagetoimage translation gan stabilization techniques proposed research highlights potential gans generalpurpose solution denoising paper details development testing model showcasing effectiveness numerical experiments dataset created adding synthetic noise clean images numerical results based realworld dataset validation underscore efficacy approach imagedenoising tasks exhibiting significant enhancements traditional techniques notably proposed model demonstrates strong generalization capabilities performing effectively even trained synthetic noise,13,0.8660022637503455,13,0.8660022637503455
advancing weaklysupervised audiovisual video parsing via segmentwise pseudo labeling audiovisual video parsing task aims identify temporally localize events occur either audio visual streams audible videos often performs weaklysupervised manner video event labels provided ie modalities timestamps labels unknown due lack densely annotated labels recent work attempts leverage pseudo labels enrich supervision commonly used strategy generate pseudo labels categorizing known video event labels modality however labels still confined video level temporal boundaries events remain unlabeled paper propose new pseudo label generation strategy explicitly assign labels video segment utilizing prior knowledge learned open world specifically exploit largescale pretrained models namely clip clap estimate events video segment generate segmentlevel visual audio pseudo labels respectively propose new loss function exploit pseudo labels taking account categoryrichness segmentrichness label denoising strategy also adopted improve visual pseudo labels flipping whenever abnormally large forward losses occur perform extensive experiments llp dataset demonstrate effectiveness proposed design achieve stateoftheart video parsing performance types event parsing ie audio event visual event audiovisual event also examine proposed pseudo label generation strategy relevant weaklysupervised audiovisual event localization task experimental results verify benefits generalization method,7,1.0,7,1.0
ganbased architecture lowdose computed tomography imaging denoising generative adversarial networks gans surfaced revolutionary element within domain lowdose computed tomography ldct imaging providing advanced resolution enduring issue reconciling radiation exposure image quality comprehensive review synthesizes rapid advancements ganbased ldct denoising techniques examining evolution foundational architectures stateoftheart models incorporating advanced features anatomical priors perceptual loss functions innovative regularization strategies critically analyze various gan architectures including conditional gans cgans cyclegans superresolution gans srgans elucidating unique strengths limitations context ldct denoising evaluation provides qualitative quantitative results related improvements performance benchmark clinical datasets metrics psnr ssim lpips highlighting positive results discuss challenges preventing wider clinical use including interpretability images generated gans synthetic artifacts need clinically relevant metrics review concludes highlighting essential significance ganbased methodologies progression precision medicine via tailored ldct denoising models underlining transformative possibilities presented artificial intelligence within contemporary radiological practice,-1,0.0,-1,0.0
vdma video question answering dynamically generated multiagents technical report provides detailed description approach egoschema challenge egoschema challenge aims identify appropriate responses questions regarding given video clip paper propose video question answering dynamically generated multiagents vdma method complementary approach existing response generation systems employing multiagent system dynamically generated expert agents method aims provide accurate contextually appropriate responses report details stages approach tools employed results experiments,-1,0.0,-1,0.0
embodimentagnostic action planning via objectpart scene flow observing key robotic action planning understand targetobject motion associated part manipulated end effector propose generate objectpart scene flow extract transformations solve action trajectories diverse embodiments advantage approach derives robot action explicitly object motion prediction yielding robust policy understanding object motions also beyond policies trained embodimentcentric data method embodimentagnostic generalizable across diverse embodiments able learn human demonstrations method comprises three components objectpart predictor locate part end effector manipulate rgbd video generator predict future rgbd videos trajectory planner extract embodimentagnostic transformation sequences solve trajectory diverse embodiments trained videos even without trajectory data method still outperforms existing works significantly prevailing virtual environments metaworld frankakitchen respectively furthermore conducted realworld experiments showing policy trained human demonstration deployed various embodiments,5,0.7203529936347713,5,0.7203529936347713
enhancing video transformers action understanding vlmaided training owing ability extract relevant spatiotemporal video embeddings vision transformers vits currently best performing models video action understanding however generalization domains datasets somewhat limited contrast visual language models vlms demonstrated exceptional generalization performance currently unable process videos consequently extract spatiotemporal patterns crucial action understanding paper propose fourtiered prompts ftp framework takes advantage complementary strengths vits vlms retain vits strong spatiotemporal representation ability improve visual encodings comprehensive general aligning vlm outputs ftp framework adds four feature processors focus specific aspects human action videos action category action components action description context information vlms employed training inference incurs minimal computation cost approach consistently yields stateoftheart performance instance achieve remarkable accuracy somethingsomething surpassing respectively,-1,0.0,-1,0.0
investigating eventbased cameras video frame interpolation sports slowmotion replays provide thrilling perspective pivotal moments within sports games offering fresh captivating visual experience however capturing slowmotion footage typically demands hightech expensive cameras infrastructures deep learning video frame interpolation vfi techniques emerged promising avenue capable generating highspeed footage regular camera feeds moreover utilization eventbased cameras recently gathered attention provide valuable motion information frames enhancing vfi performances work present first investigation eventbased vfi models generating sports slowmotion videos particularly design implement bicamera recording setup including rgb eventbased camera capture sports videos temporally align spatially register cameras experimental validation demonstrates timelens offtheshelf eventbased vfi model effectively generate slowmotion footage sports videos first investigation underscores practical utility eventbased cameras producing sports slowmotion content lays groundwork future research endeavors domain,-1,0.0,-1,0.0
headsetoff enabling photorealistic video conferencing economical vr headsets virtual reality vr become increasingly popular remote collaboration video conferencing poses challenges users face covered headset existing solutions limitations terms accessibility paper propose headsetoff novel system achieves photorealistic video conferencing economical vr headsets leveraging voicedriven face reconstruction headsetoff consists three main components multimodal predictor generator adaptive controller predictor effectively predicts user future behavior based different modalities generator employs voice head motion eye blink animate human face adaptive controller dynamically selects appropriate generator model based tradeoff video quality delay experimental results demonstrate effectiveness headsetoff achieving highquality lowlatency video conferencing economical vr headsets,6,0.9140743624188302,6,0.9140743624188302
selfenhancing video data management system compositional events large language models technical report complex video queries answered decomposing modular subtasks however existing video data management systems assume existence predefined modules subtask introduce vocaludf novel selfenhancing system supports compositional queries videos without need predefined modules vocaludf automatically identifies constructs missing modules encapsulates userdefined functions udfs thus expanding querying capabilities achieve formulate unified udf model leverages large language models llms aid new udf generation vocaludf handles wide range concepts supporting programbased udfs ie python functions generated llms distilledmodel udfs lightweight vision models distilled strong pretrained models resolve inherent ambiguity user intent vocaludf generates multiple candidate udfs uses active learning efficiently select best one selfenhancing capability vocaludf significantly improves query performance across three video datasets,0,0.9479224797312717,0,0.9479224797312717
physical informed driving world model autonomous driving requires robust perception models trained highquality largescale multiview driving videos tasks like object detection segmentation trajectory prediction world models provide costeffective solution generating realistic driving videos challenges remain ensuring videos adhere fundamental physical principles relative absolute motion spatial relationship like occlusion spatial consistency temporal consistency address propose drivephysica innovative model designed generate realistic multiview driving videos accurately adhere essential physical principles three key advancements coordinate system aligner module integrates relative absolute motion features enhance motion interpretation instance flow guidance module ensures precise temporal consistency via efficient flow extraction box coordinate guidance module improves spatial relationship understanding accurately resolves occlusion hierarchies grounded physical principles achieve stateoftheart performance driving video generation quality fid fvd nuscenes dataset downstream perception tasks project homepage,16,1.0,16,1.0
future research avenues artificial intelligence digital gaming exploratory report video games natural synergistic application domain artificial intelligence ai systems offering potential enhance player experience immersion well providing valuable benchmarks virtual environments advance ai technologies general report presents highlevel overview five promising research pathways applying stateoftheart ai methods particularly deep learning digital gaming within context current research landscape objective work outline curated nonexhaustive list encouraging research directions intersection ai video games may serve inspire rigorous comprehensive research efforts future discuss investigating large language models core engines game agent modelling ii using neural cellular automata procedural game content generation iii accelerating computationally expensive ingame simulations via deep surrogate modelling iv leveraging selfsupervised learning obtain useful video game state embeddings v training generative models interactive worlds using unlabelled video data also briefly address current technical challenges associated integration advanced deep learning systems video game development indicate key areas progress likely beneficial,10,1.0,10,1.0
paragan scalable distributed training framework generative adversarial networks recent advances generative artificial intelligence fueled numerous applications particularly involving generative adversarial networks gans essential synthesizing realistic photos videos however efficiently training gans remains critical challenge due computationally intensive numerically unstable nature existing methods often require days even weeks training posing significant resource time constraints work introduce paragan scalable distributed gan training framework leverages asynchronous training asymmetric optimization policy accelerate gan training paragan employs congestionaware data pipeline hardwareaware layout transformation enhance accelerator utilization resulting improvements throughput paragan reduce training time biggan days hours achieving scaling efficiency additionally paragan enables unprecedented highresolution image generation using biggan,17,1.0,17,1.0
evaluating effectiveness attackagnostic features morphing attack detection morphing attacks diversified significantly past years new methods based generative adversarial networks gans diffusion models posing substantial threats face recognition systems recent research demonstrated effectiveness features extracted large vision models pretrained bonafide data attackagnostic features detecting deep generative images building investigate potential image representations morphing attack detection mad develop supervised detectors training simple binary linear svm extracted features oneclass detectors modeling distribution bonafide features gaussian mixture model gmm method evaluated across comprehensive set attacks various scenarios including generalization unseen attacks different source datasets printscan data results indicate attackagnostic features effectively detect morphing attacks outperforming traditional supervised oneclass detectors literature scenarios additionally provide insights strengths limitations considered representation discuss potential future research directions enhance robustness generalizability approach,4,0.8991771060384347,4,0.8991771060384347
dynamicavatars accurate dynamic facial avatars reconstruction precise editing diffusion models generating editing dynamic head avatars crucial tasks virtual reality film production however existing methods often suffer facial distortions inaccurate head movements limited finegrained editing capabilities address challenges present dynamicavatars dynamic model generates photorealistic moving head avatars video clips parameters associated facial positions expressions approach enables precise editing novel promptbased editing model integrates userprovided prompts guiding parameters derived large language models llms achieve propose dualtracking framework based gaussian splatting introduce prompt preprocessing module enhance editing stability incorporating specialized gan algorithm connecting control module generates precise guiding parameters llms successfully address limitations existing methods additionally develop dynamic editing strategy selectively utilizes specific training datasets improve efficiency adaptability model dynamic editing tasks,-1,0.0,-1,0.0
videotoaudio generation hidden alignment generating semantically temporally aligned audio content accordance video input become focal point researchers particularly following remarkable breakthrough texttovideo generation work aim offer insights videotoaudio generation paradigm focusing three crucial aspects vision encoders auxiliary embeddings data augmentation techniques beginning foundational model built simple yet surprisingly effective intuition explore various vision encoders auxiliary embeddings ablation studies employing comprehensive evaluation pipeline emphasizes generation quality videoaudio synchronization alignment demonstrate model exhibits stateoftheart videotoaudio generation capabilities furthermore provide critical insights impact different data augmentation methods enhancing generation frameworks overall capacity showcase possibilities advance challenge generating synchronized audio semantic temporal perspectives hope insights serve stepping stone toward developing realistic accurate audiovisual generation models,-1,0.0,-1,0.0
nerf view synthesis subjective quality assessment objective metrics evaluation neural radiance fields nerf groundbreaking computer vision technology enables generation highquality immersive visual content multiple viewpoints capability significant advantages applications virtualaugmented reality modelling content creation film entertainment industry however evaluation nerf methods poses several challenges including lack comprehensive datasets reliable assessment methodologies objective quality metrics paper addresses problem nerf view synthesis nvs quality assessment thoroughly conducting rigorous subjective quality assessment test considers several scene classes recently proposed nvs methods additionally performance wide range stateoftheart conventional learningbased fullreference image video quality assessment metrics evaluated subjective scores subjective study study found errors camera pose estimation result spatial misalignments synthesized reference images need corrected applying objective quality metric experimental results analyzed depth providing comparative evaluation several nvs methods objective quality metrics across different classes visual scenes including real synthetic content frontface camera trajectories,12,0.7084624860687027,12,0.7084624860687027
choroidal thinning assessment facial video analysis different features skin associated various medical conditions provide opportunities evaluate monitor body health study created strategy assess choroidal thinning video analysis facial skin videos capturing entire facial skin collected participants agerelated macular degeneration amd healthy individuals facial videos analyzed using videobased transangiosomes imaging photoplethysmography taippg generate facial imaging biomarkers correlated choroidal thickness ct measurements ct patients determined using sweptsource optical coherence tomography ssoct results revealed relationship relative blood pulsation amplitude bpa three typical facial angiosomes cheek sideforehead midforehead average macular ct r p r p r p considering diagnostic threshold newly developed facial video analysis tool effectively distinguished cases choroidal thinning normal cases yielding areas curve findings shed light connection choroidal blood flow facial skin hemodynamics suggests potential predicting vascular diseases widely accessible skin imaging data,19,1.0,19,1.0
slotvlm slowfast slots videolanguage modeling videolanguage models vlms powered advancements large language models llms charting new frontiers video understanding pivotal challenge development efficient method encapsulate video content set representative tokens align llms work introduce slotvlm novel framework designed generate semantically decomposed video tokens terms objectwise eventwise visual representations facilitate llm inference particularly design slowfast slots module ie sfslots adaptively aggregates dense video tokens clip vision encoder set representative slots order take account spatial object details varied temporal dynamics sfslots built dualbranch structure slowslots branch focuses extracting objectcentric slots features high spatial resolution low slow frame sample rate emphasizing detailed object information conversely fastslots branch engineered learn eventcentric slots high temporal sample rate low spatial resolution features complementary slots combined form vision context serving input llm efficient question answering experimental results demonstrate effectiveness slotvlm achieves stateoftheart performance video questionanswering,0,1.0,0,1.0
boosting neural representations videos conditional decoder implicit neural representations inrs emerged promising approach video storage processing showing remarkable versatility across various video tasks however existing methods often fail fully leverage representation capabilities primarily due inadequate alignment intermediate features target frame decoding paper introduces universal boosting framework current implicit video representation approaches specifically utilize conditional decoder temporalaware affine transform module uses frame index prior condition effectively align intermediate features target frames besides introduce sinusoidal nervlike block generate diverse intermediate features achieve balanced parameter distribution thereby enhancing models capacity highfrequency informationpreserving reconstruction loss approach successfully boosts multiple baseline inrs reconstruction quality convergence speed video regression exhibits superior inpainting interpolation results integrate consistent entropy minimization technique develop video codecs based boosted inrs experiments uvg dataset confirm enhanced codecs significantly outperform baseline inrs offer competitive ratedistortion performance compared traditional learningbased codecs code available httpsgithubcomxinjieqboostingnerv,-1,0.0,-1,0.0
lita language instructed temporallocalization assistant tremendous progress multimodal large language models llms recent works extended models video input promising instruction following capabilities however important missing piece temporal localization models accurately answer questions identify three key aspects limit temporal localization capabilities time representation ii architecture iii data address shortcomings proposing language instructed temporallocalization assistant lita following features introduce time tokens encode timestamps relative video length better represent time videos introduce slowfast tokens architecture capture temporal information fine temporal resolution emphasize temporal localization data lita addition leveraging existing video datasets timestamps propose new task reasoning temporal localization rtl along dataset activitynetrtl learning evaluating task reasoning temporal localization requires reasoning temporal localization video llms lita demonstrates strong performance challenging task nearly doubling temporal mean intersectionoverunion miou baselines addition show emphasis temporal localization also substantially improves videobased text generation compared existing video llms including relative improvement temporal understanding code available httpsgithubcomnvlabslita,0,1.0,0,1.0
arena patchofinterest vit inference acceleration system edgeassisted video analytics advent edge computing made realtime intelligent video analytics feasible previous works based traditional model architecture eg cnn rnn etc employ various strategies filter nonregionofinterest content minimize bandwidth computation consumption show inferior performance adverse environments recently visual foundation models based transformers shown great performance adverse environments due amazing generalization capability however require large amount computation power limits applications realtime intelligent video analytics paper find visual foundation models like vision transformer vit also dedicated acceleration mechanism video analytics end introduce arena endtoend edgeassisted video inference acceleration system based vit leverage capability vit accelerated token pruning offloading feeding patchesofinterest downstream models additionally design adaptive keyframe inference switching algorithm tailored different videos capable adapting current video content jointly optimize accuracy bandwidth extensive experiments findings reveal arena boost inference speeds average consuming bandwidth respectively high inference accuracy,2,0.776074881639217,2,0.776074881639217
groupaware parameterefficient updating contentadaptive neural video compression contentadaptive compression crucial enhancing adaptability pretrained neural codec various contents although methods practical neural image compression nic application neural video compression nvc still limited due two main aspects video compression relies heavily temporal redundancy therefore updating one frames lead significant errors accumulating time nvc frameworks generally complex many large components easy update quickly encoding address previously mentioned challenges developed contentadaptive nvc technique called groupaware parameterefficient updating gpu initially minimize error accumulation adopt groupaware approach updating encoder parameters involves adopting patchbased group pictures gop training strategy segment video patchbased gops updated facilitate globally optimized domaintransferable solution subsequently introduce parameterefficient deltatuning strategy achieved integrating several lightweight adapters coding component encoding process serial parallel configuration architectureagnostic modules stimulate components large parameters thereby reducing update cost encoding time incorporate gpu latest nvc framework conduct comprehensive experiments whose results showcase outstanding video compression efficiency across four video benchmarks adaptability one medical image benchmark,2,1.0,2,1.0
toxvidlm multimodal framework toxicity detection codemixed videos era rapidly evolving internet technology surge multimodal content including videos expanded horizons online communication however detection toxic content diverse landscape particularly lowresource codemixed languages remains critical challenge substantial research addressed toxic content detection textual data realm video content especially nonenglish languages relatively underexplored paper addresses research gap introducing benchmark dataset first kind consisting videos codemixed hindienglish utterances collected youtube utterance within dataset meticulously annotated toxicity severity sentiment labels developed advanced multimodal multitask framework built toxicity detection video content leveraging language models lms crafted primary objective along additional tasks conducting sentiment severity analysis toxvidlm incorporates three key modules encoder module crossmodal synchronization module multitask module crafting generic multimodal lm customized intricate video classification tasks experiments reveal incorporating multiple modalities videos substantially enhances performance toxic content detection achieving accuracy weighted score respectively,-1,0.0,-1,0.0
shotluck holmes family efficient smallscale large language vision models video captioning summarization video increasingly prominent informationdense medium yet poses substantial challenges language models typical video consists sequence shorter segments shots collectively form coherent narrative shot analogous word sentence multiple data streams information visual auditory data must processed simultaneously comprehension entire video requires understanding visualaudio information shot also requires model links ideas shot generate larger allencompassing story despite significant progress field current works often overlook videos granular shotbyshot semantic information project propose family efficient large language vision models llvms boost video summarization captioning called shotluck holmes leveraging better pretraining data collection strategies extend abilities existing small llvms able understand picture able understand sequence frames specifically show shotluck holmes achieves better performance stateoftheart results video captioning summary task significantly smaller computationally efficient models,0,1.0,0,1.0
thoracic surgery video analysis surgical phase recognition paper presents approach surgical phase recognition using video data aiming provide comprehensive understanding surgical procedures automated workflow analysis advent robotic surgery digitized operating rooms generation vast amounts data opened doors application machine learning computer vision analysis surgical videos among advancements surgical phase recognitionspr stands emerging technology potential recognize assess ongoing surgical scenario summarize surgery evaluate surgical skills offer surgical decision support facilitate medical training paper analyse evaluate framebased video clippingbased phase recognition thoracic surgery dataset consisting classes phases specifically utilize imagenet vit imagebased classification videomae baseline model videobased classification show masked video distillationmvd exhibits superior performance achieving accuracy compared achieved imagenet vit findings underscore efficacy videobased classifiers imagebased counterparts surgical phase recognition tasks,-1,0.0,-1,0.0
sstfb leveraging selfsupervised pretext learning temporal selfattention feature branching realtime video polyp segmentation polyps early cancer indicators assessing occurrences polyps removal critical observed colonoscopy screening procedure generates stream video frames segmenting polyps natural video screening procedure several challenges coexistence imaging artefacts motion blur floating debris existing polyp segmentation algorithms developed curated still image datasets represent realworld colonoscopy performance often degrades video data propose video polyp segmentation method performs selfsupervised learning auxiliary task spatialtemporal selfattention mechanism improved representation learning endtoend configuration joint optimisation losses enable network learn discriminative contextual features videos experimental results demonstrate improvement respect several stateoftheart sota methods ablation study also confirms choice proposed joint endtoend training improves network accuracy nearly dice similarity coefficient intersectionoverunion compared recently proposed method pns polyppvt respectively results previously unseen video data indicate proposed method generalises,19,1.0,19,1.0
alanavlm multimodal embodied ai foundation model egocentric video understanding ai personal assistants deployed via robots wearables require embodied understanding collaborate humans effectively however current visionlanguage models vlms primarily focus thirdperson view videos neglecting richness egocentric perceptual experience address gap propose three key contributions first introduce egocentric video understanding dataset evud training vlms video captioning question answering tasks specific egocentric videos second present alanavlm parameter vlm trained using parameterefficient methods evud finally evaluate alanavlms capabilities openeqa challenging benchmark embodied video question answering model achieves stateoftheart performance outperforming opensource models including strong socratic models using planner additionally outperform claude gemini pro vision showcase competitive results compared gemini pro even surpassing latter spatial reasoning research paves way building efficient vlms deployed robots wearables leveraging embodied video understanding collaborate seamlessly humans everyday tasks contributing next generation embodied ai,-1,0.0,-1,0.0
harnessing llms automated video content analysis exploratory workflow short videos depression despite growing interest leveraging large language models llms content analysis current studies primarily focused textbased content present work explored potential llms assisting video content analysis conducting case study followed new workflow llmassisted multimodal content analysis workflow encompasses codebook design prompt engineering llm processing human evaluation strategically crafted annotation prompts get llm annotations structured form explanation prompts generate llm explanations better understanding llm reasoning transparency test llms video annotation capabilities analyzed keyframes extracted youtube short videos depression compared llm annotations two human coders found llm higher accuracy object activity annotations emotion genre annotations moreover identified potential limitations llms capabilities annotating videos based findings explore opportunities challenges future research improvements workflow also discuss ethical concerns surrounding future studies based llmassisted video analysis,-1,0.0,-1,0.0
open vocabulary multilabel video classification pretrained visionlanguage models vlms enabled significant progress open vocabulary computer vision tasks image classification object detection image segmentation recent works focused extending vlms open vocabulary single label action classification videos however previous methods fall short holistic video understanding requires ability simultaneously recognize multiple actions entities eg objects video open vocabulary setting formulate problem open vocabulary multilabel video classification propose method adapt pretrained vlm clip solve task leverage large language models llms provide semantic guidance vlm class labels improve open vocabulary performance two key contributions first propose endtoend trainable architecture learns prompt llm generate soft attributes clip textencoder enable recognize novel classes second integrate temporal modeling module clips vision encoder effectively model spatiotemporal dynamics video concepts well propose novel regularized finetuning technique ensure strong open vocabulary classification performance video domain extensive experimentation showcases efficacy approach multiple benchmark datasets,-1,0.0,-1,0.0
longvideobench benchmark longcontext interleaved videolanguage understanding large multimodal models lmms processing increasingly longer richer inputs albeit progress public benchmark available measure development mitigate gap introduce longvideobench questionanswering benchmark features videolanguage interleaved inputs hour long benchmark includes varyinglength webcollected videos subtitles across diverse themes designed comprehensively evaluate lmms longterm multimodal understanding achieve interpret primary challenge accurately retrieve reason detailed multimodal information long inputs formulate novel video questionanswering task termed referring reasoning specifically part question contains referring query references related video contexts called referred context model required reason relevant video details referred context following paradigm referring reasoning curate humanannotated multiplechoice questions finegrained categories establishing one comprehensive benchmarks longform video understanding evaluations suggest longvideobench presents significant challenges even advanced proprietary models eg opensource counterparts show even larger performance gap addition results indicate model performance benchmark improves capable processing frames positioning longvideobench valuable benchmark evaluating futuregeneration longcontext lmms,0,0.940377225397076,0,0.940377225397076
accelerating learned video compression via lowresolution representation learning recent years field learned video compression witnessed rapid advancement exemplified latest neural video codecs dcvcdc outperformed upcoming nextgeneration codec ecm terms compression ratio despite learned video compression frameworks often exhibit low encoding decoding speeds primarily due increased computational complexity unnecessary highresolution spatial operations hugely hinder applications reality work introduce efficiencyoptimized framework learned video compression focuses lowresolution representation learning aiming significantly enhance encoding decoding speeds firstly diminish computational load reducing resolution interframe propagated features obtained reused features decoded frames including iframes implement joint training strategy iframe pframe models improving compression ratio secondly approach efficiently leverages multiframe priors parameter prediction minimizing computation decoding end thirdly revisit application online encoder update oeu strategy highresolution sequences achieving notable improvements compression ratio without compromising decoding efficiency efficiencyoptimized framework significantly improved balance compression ratio speed learned video compression comparison traditional codecs method achieves performance levels par lowdecay p configuration reference software vtm furthermore contrasted dcvchem approach delivers comparable compression ratio boosting encoding decoding speeds factor respectively rtx method decode frame,2,1.0,2,1.0
performance nonadversarial robustness segment anything model surgical video segmentation fully supervised deep learning dl models surgical video segmentation shown struggle nonadversarial realworld corruptions image quality including smoke bleeding low illumination foundation models image segmentation segment anything model sam focuses interactive promptbased segmentation move away semantic classes thus trained larger diverse data offers outstanding zeroshot generalization appropriate user prompts recently building upon success proposed extend zeroshot interactive segmentation capabilities independent framebyframe video segmentation paper present first experimental study evaluating performance surgical video data leveraging segstrongc miccai endovis subchallenge dataset assess effectiveness uncorrupted endoscopic sequences evaluate nonadversarial robustness videos corrupted image quality simulating smoke bleeding low brightness conditions various prompt strategies experiments demonstrate zeroshot manner achieve competitive even superior performance compared fullysupervised deep learning models surgical video data including nonadversarial corruptions image quality additionally consistently outperforms original sam medical variants across conditions finally framesparse prompting consistently outperform framewise prompting suggesting allowing leverage temporal modeling capabilities leads coherent accurate segmentation compared frequent prompting,-1,0.0,-1,0.0
highefficiency neural video compression via hierarchical predictive learning enhanced deep hierarchical video compressiondhvc introduced singlemodel neural video codec operates across broad range bitrates delivering superior compression performance representative methods also impressive complexity efficiency enabling realtime processing significantly smaller memory footprint standard gpus remarkable advancements stem use hierarchical predictive coding video frame uniformly transformed multiscale representations hierarchical variational autoencoders specific scales feature representation frame corresponding latent residual variables generated referencing lowerscale spatial features frame conditionally entropyencoded using probabilistic model whose parameters predicted using samescale temporal reference previous frames lowerscale spatial reference current frame featurespace processing operates lowest highest scale frame completely eliminating need complexityintensive motion estimation compensation techniques standard video codecs decades hierarchical approach facilitates parallel processing accelerating encoding decoding supports transmissionfriendly progressive decoding making particularly advantageous networked video applications presence packet loss source codes made available,2,1.0,2,1.0
multivent massive multilingual benchmark eventcentric video retrieval efficiently retrieving synthesizing information largescale multimodal collections become critical challenge however existing video retrieval datasets suffer scope limitations primarily focusing matching descriptive vague queries small collections professionally edited englishcentric videos address gap introduce textbfmultivent largescale multilingual eventcentric video retrieval benchmark featuring collection news videos queries targeting specific world events queries specifically target information found visual content audio embedded text text metadata videos requiring systems leverage sources succeed task preliminary results show stateoftheart visionlanguage models struggle significantly task alternative approaches show promise still insufficient adequately address problem findings underscore need robust multimodal retrieval systems effective video retrieval crucial step towards multimodal content understanding generation,0,1.0,0,1.0
spatial meets temporal action recognition video action recognition made significant strides challenges remain effectively using spatial temporal information existing methods often focus either spatial features eg object appearance temporal dynamics eg motion rarely address need comprehensive integration capturing rich temporal evolution video frames preserving spatial details crucial improving accuracy paper introduce temporal integration motion enhancement time layer novel preprocessing technique designed incorporate temporal information time layer generates new video frames rearranging original sequence preserving temporal order embedding temporally evolving frames single spatial grid size n times n transformation creates new frames balance spatial temporal information making compatible existing video models layer captures rich spatial details similar existing methods n increases temporal information becomes prominent spatial information decreases ensure compatibility model inputs demonstrate effectiveness time layer integrating popular action recognition models vision transformer video masked autoencoders rgb depth video data experiments show time layer enhances recognition accuracy offering valuable insights video processing tasks,-1,0.0,-1,0.0
ophclip hierarchical retrievalaugmented learning ophthalmic surgical videolanguage pretraining surgical practice involves complex visual interpretation procedural skills advanced medical knowledge making surgical visionlanguage pretraining vlp particularly challenging due complexity limited availability annotated data address gap propose ophclip hierarchical retrievalaugmented visionlanguage pretraining framework specifically designed ophthalmic surgical workflow understanding ophclip leverages ophvl dataset constructed largescale comprehensive collection hierarchically structured videotext pairs tens thousands different combinations attributes surgeries phasesoperationsactions instruments medications well advanced aspects like causes eye diseases surgical objectives postoperative recovery recommendations etc hierarchical videotext correspondences enable ophclip learn finegrained longterm visual representations aligning short video clips detailed narrative descriptions full videos structured titles capturing intricate surgical details highlevel procedural insights respectively ophclip also designs retrievalaugmented pretraining framework leverage underexplored largescale silent surgical procedure videos automatically retrieving semantically relevant content enhance representation learning narrative videos evaluation across datasets phase recognition multiinstrument identification shows ophclips robust generalization superior performance,-1,0.0,-1,0.0
unleashing generalization endtoend autonomous driving controllable long video generation using generative models synthesize new data become defacto standard autonomous driving address data scarcity issue though existing approaches able boost perception models discover approaches fail improve performance planning endtoend autonomous driving models generated videos usually less frames spatial temporal inconsistencies negligible end propose delphi novel diffusionbased long video generation method shared noise modeling mechanism across multiviews increase spatial consistency featurealigned module achieves precise controllability temporal consistency method generate frames video without loss consistency times longer compared stateoftheart methods instead randomly generating new data design sampling policy let delphi generate new data similar failure cases improve sample efficiency achieved building failurecase driven framework help pretrained visual language models extensive experiment demonstrates delphi generates higher quality long videos surpassing previous stateoftheart methods consequentially generating training dataset size framework able go beyond perception prediction tasks first time best knowledge boost planning performance endtoend autonomous driving model margin,-1,0.0,-1,0.0
bidirectional ms lesion filling synthesis using denoising diffusion implicit modelbased lesion repainting automatic magnetic resonance mr image processing pipelines widely used study people multiple sclerosis pwms encompassing tasks lesion segmentation brain parcellation however presence lesion often complicates analysis particularly brain parcellation lesion filling commonly used mitigate issue existing lesion filling algorithms often fall short accurately reconstructing realistic lesionfree images vital consistent downstream analysis additionally performance lesion segmentation algorithms often limited insufficient data lesion delineation training labels paper propose novel approach leveraging denoising diffusion implicit models ddims ms lesion filling synthesis based image inpainting modified ddim architecture trained enables ms lesion filing synthesis specifically generate lesionfree flair images containing lesions add lesions flair images healthy subjects former essential downstream analyses require lesionfree images latter valuable augmenting training datasets lesion segmentation tasks validate approach initial experiments paper demonstrate promising results lesion filling synthesis paving way future work,3,1.0,3,1.0
comparative study generative adversarial networks image recognition algorithms based deep learning traditional methods paper image recognition algorithm based combination deep learning generative adversarial network gan studied compared traditional image recognition methods purpose study evaluate advantages application prospects deep learning technology especially gan field image recognition firstly paper reviews basic principles techniques traditional image recognition methods including classical algorithms based feature extraction sift hog combination support vector machine svm random forest classifiers working principle network structure unique advantages gan image generation recognition introduced order verify effectiveness gan image recognition series experiments designed carried using multiple public image data sets training testing experimental results show compared traditional methods gan excellent performance processing complex images recognition accuracy antinoise ability specifically gans better able capture highdimensional features details images significantly improving recognition performance addition gans shows unique advantages dealing image noise partial missing information generating highquality images,-1,0.0,-1,0.0
gaussian splatting physicsgrounded motion generation generation valuable technology virtual reality digital content creation recent works pushed boundaries generation producing highfidelity objects inefficient prompts simulating physicsgrounded motion accurately still remain unsolved challenges address challenges present innovative framework utilizes large language model llmrefined prompts diffusion priorsguided gaussian splatting gs generating models accurate appearances geometric structures also incorporate continuum mechanicsbased deformation map color regularization synthesize vivid physicsgrounded motion generated gaussians adhering conservation mass momentum integrating generation physicsgrounded motion synthesis framework renders photorealistic objects exhibit physicsaware motion accurately reflecting behaviors objects various forces constraints across different materials extensive experiments demonstrate approach achieves highquality generations realistic physicsgrounded motion,-1,0.0,-1,0.0
beyond generation unlocking universal editing via selfsupervised finetuning recent advances video generation outpaced progress video editing remains constrained several limiting factors namely tasks dependency supervision severely limits generality b unnecessary artificial separation generation editing task c high computational costs training video model work propose ues unlocking universal editing via selfsupervision lightweight selfsupervised finetuning strategy transforms generation models unified generationediting systems selfsupervised semantic alignment approach establishes dualconditioning mechanism original videotext pairs jointly provide visual textual semantics enabling structured learning intrinsic spatiotemporal correspondences key advantages include universality supervisionfree adaptation diverse editing tasks ii unification generation editing applicable textimagetovideo model iii efficiency via lightweight finetune reduces tunable parameters enable systematic evaluation introduce comprehensive benchmark spanning videos across humansanimals environments objects comprising editing types scenarios extensive experiments show ues enables models without inherent editing capability perform powerful universal editing preserving even enhancing original generation performance,-1,0.0,-1,0.0
bikvil keypointsbased visual imitation learning bimanual manipulation tasks visual imitation learning achieved impressive progress learning unimanual manipulation tasks small set visual observations thanks latest advances computer vision however learning bimanual coordination strategies complex object relations bimanual visual demonstrations well generalizing categorical objects novel cluttered scenes remain unsolved challenges paper extend previous work keypointsbased visual imitation learning bimanual manipulation tasks proposed bikvil jointly extracts socalled emphhybrid masterslave relationships hmsr among objects hands bimanual coordination strategies subsymbolic task representations bimanual task representation objectcentric embodimentindependent viewpointinvariant thus generalizing well categorical objects novel scenes evaluate approach various realworld applications showcasing ability learn finegrained bimanual manipulation tasks small number human demonstration videos videos source code available httpssitesgooglecomviewbikvil,5,1.0,5,1.0
lngen rectal lymph nodes generation via anatomical features accurate segmentation rectal lymph nodes crucial staging treatment planning rectal cancer however complexity surrounding anatomical structures scarcity annotated data pose significant challenges study introduces novel lymph node synthesis technique aimed generating diverse realistic synthetic rectal lymph node samples mitigate reliance manual annotation unlike direct diffusion methods often produce masks discontinuous suboptimal quality approach leverages implicit sdfbased method mask generation ensuring production continuous stable morphologically diverse masks experimental results demonstrate synthetic data significantly improves segmentation performance work highlights potential diffusion model accurately synthesizing structurally complex lesions lymph nodes rectal cancer alleviating challenge limited annotated data field aiding advancements rectal cancer diagnosis treatment,3,1.0,3,1.0
drdm disentangled representations diffusion model synthesizing realistic person images person image synthesis controllable body poses appearances essential task owing practical needs context virtual tryon image editing video production however existing methods face significant challenges details missing limbs distortion garment style deviation address issues propose disentangled representations diffusion model drdm generate photorealistic images source portraits specific desired poses appearances first pose encoder responsible encoding pose features highdimensional space guide generation person images second bodypart subspace decoupling block bsdb disentangles features different body parts source figure feeds various layers noise prediction block thereby supplying network rich disentangled features generating realistic target image moreover inference develop parsing mapbased disentangled classifierfree guided sampling method amplifies conditional signals texture pose extensive experimental results deepfashion dataset demonstrate effectiveness approach achieving pose transfer appearance control,-1,0.0,-1,0.0
generating rich finegrained mmwave radar data videos generalized gesture recognition millimeter wave radar gaining traction recently promising modality enabling pervasive privacypreserving gesture recognition however lack rich finegrained radar datasets hinders progress developing generalized deep learning models gesture recognition across various user postures eg standing sitting positions scenes remedy resort designing software pipeline exploits wealthy videos generate realistic radar data needs address challenge simulating diversified finegrained reflection properties user gestures end design three key components gesture reflection point generator expands arms skeleton points form human reflection points ii signal simulation model simulates multipath reflection attenuation radar signals output human intensity map iii encoderdecoder model combines sampling module fitting module address differences number distribution points generated realworld radar data generating realistic radar data implement evaluate using videos public data sources selfcollected realworld radar data demonstrating superiority stateoftheart approaches gesture recognition,-1,0.0,-1,0.0
editable scene simulation autonomous driving via collaborative llmagents scene simulation autonomous driving gained significant attention huge potential generating customized data however existing editable scene simulation approaches face limitations terms user interaction efficiency multicamera photorealistic rendering external digital assets integration address challenges paper introduces chatsim first system enables editable photorealistic driving scene simulations via natural language commands external digital assets enable editing high command flexibilitychatsim leverages large language model llm agent collaboration framework generate photorealistic outcomes chatsim employs novel multicamera neural radiance field method furthermore unleash potential extensive highquality digital assets chatsim employs novel multicamera lighting estimation method achieve sceneconsistent assets rendering experiments waymo open dataset demonstrate chatsim handle complex language commands generate corresponding photorealistic scene videos,-1,0.0,-1,0.0
enhancing video generation model posttraining data reward conditional guidance design paper focus enhancing diffusionbased texttovideo model posttraining phase distilling highly capable consistency model pretrained model proposed method introduces significant advancement integrating various supervision signals including highquality training data reward model feedback conditional guidance consistency distillation process comprehensive ablation studies highlight crucial importance tailoring datasets specific learning objectives effectiveness learning diverse reward models enhancing visual quality textvideo alignment additionally highlight vast design space conditional guidance strategies centers designing effective energy function augment teacher ode solver demonstrate potential approach extracting motion guidance training datasets incorporating ode solver showcasing effectiveness improving motion quality generated videos improved motionrelated metrics vbench empirically establishes new stateoftheart result vbench total score surpassing proprietary systems kling,-1,0.0,-1,0.0
automating video thumbnails selection generation multimodal multistage analysis thesis presents innovative approach automate video thumbnail selection traditional broadcast content methodology establishes stringent criteria diverse representative aesthetically pleasing thumbnails considering factors like logo placement space incorporation vertical aspect ratios accurate recognition facial identities emotions introduce sophisticated multistage pipeline select candidate frames generate novel images blending video elements using diffusion models pipeline incorporates stateoftheart models various tasks including downsampling redundancy reduction automated cropping face recognition closedeye emotion detection shot scale aesthetic prediction segmentation matting harmonization also leverages large language models visual transformers semantic consistency gui tool facilitates rapid navigation pipelines output evaluate method conducted comprehensive experiments study videos proposed sets included thumbnails chosen professional designers containing similar images survey participants showed preference method compared manually chosen thumbnails alternative method professional designers reported increase valid candidates compared alternative method confirming approach meets established criteria conclusion findings affirm proposed method accelerates thumbnail creation maintaining highquality standards fostering greater user engagement,-1,0.0,-1,0.0
textcraftor text encoder image quality controller diffusionbased texttoimage generative models eg stable diffusion revolutionized field content generation enabling significant advancements areas like image editing video synthesis despite formidable capabilities models without limitations still challenging synthesize image aligns well input text multiple runs carefully crafted prompts required achieve satisfactory results mitigate limitations numerous studies endeavored finetune pretrained diffusion models ie unet utilizing various technologies yet amidst efforts pivotal question texttoimage diffusion model training remained largely unexplored possible feasible finetune text encoder improve performance texttoimage diffusion models findings reveal instead replacing clip text encoder used stable diffusion large language models enhance proposed finetuning approach textcraftor leading substantial improvements quantitative benchmarks human assessments interestingly technique also empowers controllable image generation interpolation different text encoders finetuned various rewards also demonstrate textcraftor orthogonal unet finetuning combined improve generative quality,-1,0.0,-1,0.0
optical generative models generative models cover various application areas including image video music synthesis natural language processing molecular design among many others digital generative models become larger scalable inference fast energyefficient manner becomes challenge present optical generative models inspired diffusion models shallow fast digital encoder first maps random noise phase patterns serve optical generative seeds desired data distribution jointlytrained freespacebased reconfigurable decoder alloptically processes generative seeds create novel images never seen following target data distribution except illumination power random seed generation shallow encoder optical generative models consume computing power synthesis novel images report optical generation monochrome multicolor novel images handwritten digits fashion products butterflies human faces following data distributions mnist fashion mnist celeba datasets respectively achieving overall performance comparable digital neural networkbased generative models experimentally demonstrate optical generative models used visible light generate snapshot novel images handwritten digits fashion products optical generative models might pave way energyefficient scalable rapid inference tasks exploiting potentials optics photonics artificial intelligencegenerated content,-1,0.0,-1,0.0
nonrigid relative placement dense diffusion task relative placement predict placement one object relation another eg placing mug onto mug rack explicit objectcentric geometric reasoning recent methods relative placement made tremendous progress towards dataefficient learning robot manipulation generalizing unseen task variations however yet represent deformable transformations despite ubiquity nonrigid bodies real world settings first step towards bridging gap propose crossdisplacement extension principles relative placement geometric relationships deformable objects present novel visionbased method learn crossdisplacement dense diffusion end demonstrate methods ability generalize unseen object instances outofdistribution scene configurations multimodal goals multiple highly deformable tasks simulation real world beyond scope prior works supplementary information videos found,5,0.2327289835135419,5,0.2327289835135419
syntheocc synthesize geometriccontrolled street view images semantic mpis advancement autonomous driving increasingly reliant highquality annotated datasets especially task occupancy prediction occupancy labels require dense annotation significant human effort paper propose syntheocc denotes diffusion model synthesize photorealistic geometriccontrolled images conditioning occupancy labels driving scenarios yields unlimited amount diverse annotated controllable datasets applications like training perception models simulation syntheocc addresses critical challenge efficiently encode geometric information conditional input diffusion model approach innovatively incorporates semantic multiplane images mpis provide comprehensive spatially aligned scene descriptions conditioning result syntheocc generate photorealistic multiview images videos faithfully align given geometric labels semantics voxel space extensive qualitative quantitative evaluations syntheocc nuscenes dataset prove effectiveness generating controllable occupancy datasets serve effective data augmentation perception models,-1,0.0,-1,0.0
head eye tracking dataset panoramic video rapid development widespread application vrar technology maximizing quality immersive panoramic video services match users personal preferences habits become longstanding challenge understanding saliency region users focus based data collected hmds promote multimedia encoding transmission quality assessment time largescale datasets essential researchers developers explore shortlongterm user behavior patterns train ai models related panoramic videos however existing panoramic video datasets often include lowfrequency user head eye movement data shortterm videos lacking sufficient data analyzing users field view fov generating video saliency regions driven practical factors paper present head eye tracking dataset involving users males females watching panoramic videos dataset provides details viewport gaze attention locations users besides present statistics samples extracted dataset example deviation head eye movements challenges widely held assumption gaze attention decreases center fov following gaussian distribution analysis reveals consistent downward offset gaze fixations relative fov experimental settings involving multiple users videos thats name dataset panonut saliency weighting shaped like donut finally also provide script generates saliency distributions based given head eye coordinates pregenerated saliency distribution map sets video collected eye tracking data dataset available website,-1,0.0,-1,0.0
vtggpt tuningfree zeroshot video temporal grounding gpt video temporal grounding vtg aims locate specific temporal segments untrimmed video based linguistic query existing vtg models trained extensive annotated videotext pairs process introduces human biases queries also incurs significant computational costs tackle challenges propose vtggpt gptbased method zeroshot vtg without training finetuning reduce prejudice original query employ generate debiased queries lessen redundant information videos apply transform visual content precise captions finally devise proposal generator postprocessing produce accurate segments debiased queries image captions extensive experiments demonstrate vtggpt significantly outperforms sota methods zeroshot settings surpasses unsupervised approaches notably achieves competitive performance comparable supervised methods code available httpsgithubcomyoucanbabyvtggpt,7,0.9034730474083802,7,0.9034730474083802
patch spatiotemporal relation prediction video anomaly detection video anomaly detection vad aiming identify abnormalities within specific context timeframe crucial intelligent video surveillance systems recent deep learningbased vad models shown promising results generating highresolution frames often lack competence preserving detailed spatial temporal coherence video frames tackle issue propose selfsupervised learning approach vad interpatch relationship prediction task specifically introduce twobranch vision transformer network designed capture deep visual features video frames addressing spatial temporal dimensions responsible modeling appearance motion patterns respectively interpatch relationship dimension decoupled interpatch similarity order information patch mitigate memory consumption convert order information prediction task multilabel learning problem interpatch similarity prediction task distance matrix regression problem comprehensive experiments demonstrate effectiveness method surpassing pixelgenerationbased methods significant margin across three public benchmarks additionally approach outperforms selfsupervised learningbased methods,-1,0.0,-1,0.0
owviscaptor abstractors openworld video instance segmentation captioning propose new task openworld video instance segmentation captioning requires detect segment track describe rich captions never seen objects challenging task addressed developing abstractors connect vision model language foundation model concretely connect multiscale visual feature extractor large language model llm developing object abstractor objecttotext abstractor object abstractor consisting prompt encoder transformer blocks introduces spatiallydiverse openworld object queries discover never seen objects videos interquery contrastive loss encourages diversity object queries objecttotext abstractor augmented masked crossattention acts bridge object queries frozen llm generate rich descriptive objectcentric captions detected object generalized approach surpasses baseline jointly addresses tasks openworld video instance segmentation dense video object captioning never seen objects objectcentric captions,-1,0.0,-1,0.0
thqa perceptual quality assessment database talking heads realm media technology digital humans gained prominence due rapid advancements computer technology however manual modeling control required majority digital humans pose significant obstacles efficient development speechdriven methods offer novel avenue manipulating mouth shape expressions digital humans despite proliferation driving methods quality many generated talking head th videos remains concern impacting user visual experiences tackle issue paper introduces talking head quality assessment thqa database featuring th videos generated diverse speechdriven methods extensive experiments affirm thqa databases richness character speech features subsequent subjective quality assessment experiments analyze correlations scoring results speechdriven methods ages genders addition experimental results show mainstream image video quality assessment methods limitations thqa database underscoring imperative research enhance th video quality assessment thqa database publicly accessible,-1,0.0,-1,0.0
learning human motion monocular videos via crossmodal manifold alignment learning human motion inputs fundamental task realms computer vision computer graphics many previous methods grapple inherently ambiguous task introducing motion priors learning process however approaches face difficulties defining complete configurations priors training robust model paper present videotomotion generator vtm leverages motion priors crossmodal latent feature space alignment human motion inputs namely videos keypoints reduce complexity modeling motion priors model motion data separately upper lower body parts additionally align motion data scaleinvariant virtual skeleton mitigate interference human skeleton variations motion priors evaluated aist vtm showcases stateoftheart performance reconstructing human motion monocular videos notably vtm exhibits capabilities generalization unseen view angles inthewild videos,1,0.9193361563759279,1,0.9193361563759279
enhancing inertial hand based har joint representation language pose synthetic imus due scarcity labeled sensor data har prior research turned video data synthesize inertial measurement units imu data capitalizing rich activity annotations however generating imu data videos presents challenges har realworld settings attributed poor quality synthetic imu data limited efficacy subtle finegrained motions paper propose novel multimodal multitask contrastivebased framework approach address issue limited data pretraining procedure uses videos online repositories aiming learn joint representations text pose imu simultaneously employing video data contrastive learning method seeks enhance wearable har performance especially recognizing subtle activitiesour experimental findings validate effectiveness approach improving har performance imu data demonstrate models trained synthetic imu data generated videos using method surpass existing approaches recognizing finegrained activities,-1,0.0,-1,0.0
officialnv llmgenerated news video dataset multimodal fake news detection news media especially video news media penetrated every aspect daily life also brings risk fake news therefore multimodal fake news detection recently garnered increased attention however existing datasets comprised useruploaded videos contain excess amounts superfluous data introduces noise model training process address issue construct dataset named officialnv comprising officially published news videos crawl officially published videos augmented use llmsbased generation manual verification thereby expanding dataset also propose new baseline model called ofnvd captures key information multimodal features glu attention mechanism performs feature enhancement modal aggregation via crossmodal transformer benchmarking dataset baselines demonstrates effectiveness model multimodal news detection,4,0.6685393940893511,4,0.6685393940893511
sportoonizer augmenting sports highlights narration visual impact via automatic manga broll generation sports highlights becoming increasingly popular videosharing platforms yet crafting sport highlight videos challenging requires producing engaging narratives different angles conforming different platform affordances constantly changing audiences many content creators therefore create derivative work original sports video manga styles enhance expressiveness manually creating inserting tailored mangastyle content still timeconsuming introduce sportoonizer system embedding pipeline automatic generation mangastyle animations highlights sports videos insertion original videos seamlessly merges dynamic manga sequences liveaction footage enriching visual tapestry deepening narrative scope leveraging genais sportoonizer crafts compelling storylines encapsulating intensity sports moments athletes personal journeys evaluation study demonstrates integrating manga brolls significantly enhances viewer engagement visual interest emotional connection towards athletes stories viewing experience,-1,0.0,-1,0.0
gemvpc dual graphenhanced multimodal integration video paragraph captioning video paragraph captioning vpc aims generate paragraph captions summarises key events within video despite recent advancements challenges persist notably effectively utilising multimodal signals inherent videos addressing longtail distribution words paper introduces novel multimodal integrated caption generation framework vpc leverages information various modalities external knowledge bases framework constructs two graphs videospecific temporal graph capturing major events interactions multimodal information commonsense knowledge theme graph representing correlations words specific theme graphs serve input transformer network shared encoderdecoder architecture also introduce node selection module enhance decoding efficiency selecting relevant nodes graphs results demonstrate superior performance across benchmark datasets,-1,0.0,-1,0.0
okami teaching humanoid robots manipulation skills single video imitation study problem teaching humanoid robots manipulation skills imitating single video demonstrations introduce okami method generates manipulation plan single rgbd video derives policy execution heart approach objectaware retargeting enables humanoid robot mimic human motions rgbd video adjusting different object locations deployment okami uses openworld vision models identify taskrelevant objects retarget body motions hand poses separately experiments show okami achieves strong generalizations across varying visual spatial conditions outperforming stateoftheart baseline openworld imitation observation furthermore okami rollout trajectories leveraged train closedloop visuomotor policies achieve average success rate without need laborintensive teleoperation videos found website httpsutaustinrplgithubiookami,5,1.0,5,1.0
vidhal benchmarking temporal hallucinations vision llms vision large language models vllms widely acknowledged prone hallucinations existing research addressing problem primarily confined image inputs limited exploration videobased hallucinations furthermore current evaluation methods fail capture nuanced errors generated responses often exacerbated rich spatiotemporal dynamics videos address introduce vidhal benchmark specially designed evaluate videobased hallucinations vllms vidhal constructed bootstrapping video instances across wide range common temporal aspects defining feature benchmark lies careful creation captions represent varying levels hallucination associated video enable finegrained evaluation propose novel caption ordering task requiring vllms rank captions hallucinatory extent conduct extensive experiments vidhal comprehensively evaluate broad selection models results uncover significant limitations existing vllms regarding hallucination generation benchmark aim inspire research holistic understanding vllm capabilities particularly regarding hallucination extensive development advanced vllms alleviate problem,0,0.8932977789800487,0,0.8932977789800487
temporal contrastive learning video temporal reasoning large visionlanguage models temporal reasoning critical challenge videolanguage understanding requires models align semantic concepts consistently across time existing large visionlanguage models lvlms large language models llms excel static tasks struggle capture dynamic interactions temporal dependencies video sequences work propose temporal semantic alignment via dynamic prompting tsadp novel framework enhances temporal reasoning capabilities dynamic taskspecific prompts temporal contrastive learning tsadp leverages dynamic prompt generator dpg encode finegrained temporal relationships temporal contrastive loss tcl align visual textual embeddings across time evaluate method vidsitu dataset augmented enriched temporal annotations demonstrate significant improvements stateoftheart models tasks intravideo entity association temporal relationship understanding chronology prediction human evaluations confirm tsadps ability generate coherent semantically accurate descriptions analysis highlights robustness efficiency practical utility tsadp making step forward field videolanguage understanding,0,0.9140892490927379,0,0.9140892490927379
ensemble approach shortform video quality assessment using multimodal llm rise shortform videos characterized diverse content editing styles artifacts poses substantial challenges learningbased blind video quality assessment bvqa models multimodal large language models mllms renowned superior generalization capabilities present promising solution paper focuses effectively leveraging pretrained mllm shortform video quality assessment regarding impacts preprocessing response variability insights combining mllm bvqa models first investigated frame preprocessing sampling techniques influence mllms performance introduced lightweight learningbased ensemble method adaptively integrates predictions mllm stateoftheart bvqa models results demonstrated superior generalization performance proposed ensemble approach furthermore analysis contentaware ensemble weights highlighted video characteristics fully represented existing bvqa models revealing potential directions improve bvqa models,-1,0.0,-1,0.0
scaling rectified flow transformers highresolution image synthesis diffusion models create data noise inverting forward paths data towards noise emerged powerful generative modeling technique highdimensional perceptual data images videos rectified flow recent generative model formulation connects data noise straight line despite better theoretical properties conceptual simplicity yet decisively established standard practice work improve existing noise sampling techniques training rectified flow models biasing towards perceptually relevant scales largescale study demonstrate superior performance approach compared established diffusion formulations highresolution texttoimage synthesis additionally present novel transformerbased architecture texttoimage generation uses separate weights two modalities enables bidirectional flow information image text tokens improving text comprehension typography human preference ratings demonstrate architecture follows predictable scaling trends correlates lower validation loss improved texttoimage synthesis measured various metrics human evaluations largest models outperform stateoftheart models make experimental data code model weights publicly available,13,0.9136379219237998,13,0.9136379219237998
ambientaware generation action sounds egocentric videos generating realistic audio human actions important many applications creating sound effects films virtual reality games existing approaches implicitly assume total correspondence video audio training yet many sounds happen offscreen weak correspondence visuals resulting uncontrolled ambient sounds hallucinations test time propose novel ambientaware audio generation model avldm devise novel audioconditioning mechanism learn disentangle foreground action sounds ambient background sounds inthewild training videos given novel silent video model uses retrievalaugmented generation create audio matches visual content semantically temporally train evaluate model two inthewild egocentric video datasets epickitchens introduce curated clips actionaudio correspondence model outperforms array existing methods allows controllable generation ambient sound even shows promise generalizing computer graphics game clips overall approach first focus videotoaudio generation faithfully observed visual content despite training uncurated clips natural background sounds,8,0.7373335785983263,8,0.7373335785983263
synergizing motion appearance multiscale compensatory codebooks talking head video generation talking head video generation aims generate realistic talking head video preserves persons identity source image motion driving video despite promising progress made field remains challenging critical problem generate videos accurate poses finegrained facial details simultaneously essentially facial motion often highly complex model precisely oneshot source face image provide sufficient appearance guidance generation due dynamic pose changes tackle problem propose jointly learn motion appearance codebooks perform multiscale codebook compensation effectively refine facial motion conditions appearance features talking face image decoding specifically designed multiscale motion appearance codebooks learned simultaneously unified framework store representative global facial motion flow appearance patterns present novel multiscale motion appearance compensation module utilizes transformerbased codebook retrieval strategy query complementary information two codebooks joint motion appearance compensation entire process produces motion flows greater flexibility appearance features fewer distortions across different scales resulting highquality talking head video generation framework extensive experiments various benchmarks validate effectiveness approach demonstrate superior generation results qualitative quantitative perspectives compared stateoftheart competitors,6,0.47955308932137003,6,0.47955308932137003
comprehensive review eegtooutput research decoding neural signals images videos audio electroencephalography eeg invaluable tool neuroscience offering insights brain activity high temporal resolution recent advancements machine learning generative modeling catalyzed application eeg reconstructing perceptual experiences including images videos audio paper systematically reviews eegtooutput research focusing stateoftheart generative methods evaluation metrics data challenges using prisma guidelines analyze studies identify key trends challenges opportunities field findings emphasize potential advanced models generative adversarial networks gans variational autoencoders vaes transformers highlighting pressing need standardized datasets crosssubject generalization roadmap future research proposed aims improve decoding accuracy broadening realworld applications,-1,0.0,-1,0.0
inloop filtering via trained lookup tables inloop filtering ilf key technology removing artifacts imagevideo coding standards recently neural networkbased inloop filtering methods achieve remarkable coding gains beyond capability advanced video coding standards becomes powerful coding tool candidate future video coding standards however utilization deep neural networks brings heavy time computational complexity high demands highperformance hardware challenging apply general uses coding scene address limitation inspired explorations image restoration propose efficient practical inloop filtering scheme adopting lookup table lut train dnn inloop filtering within fixed filtering reference range cache output values dnn lut via traversing possible inputs testing time coding process filtered pixel generated locating input pixels tobefiltered pixel reference pixels interpolating cached filtered pixel values enable large filtering reference range limited storage cost lut introduce enhanced indexing mechanism filtering process clippingfinetuning mechanism training proposed method implemented versatile video coding vvc reference software experimental results show ultrafast fast fast mode proposed method achieves average bdrate reduction intra ai random access ra configurations especially method friendly time computational complexity time increase kmacspixel kb storage cost single model solution may shed light journey practical neural networkbased coding tool evolution,2,0.9462960355157072,2,0.9462960355157072
temporal evolution knee osteoarthritis diffusionbased morphing model xray medical image synthesis knee osteoarthritis koa common musculoskeletal disorder significantly affects mobility older adults medical domain images containing temporal data frequently utilized study temporal dynamics statistically monitor disease progression deep learningbased generative models natural images widely researched comparatively methods available synthesizing temporal knee xrays work introduce novel deeplearning model designed synthesize intermediate xray images specific patients healthy knee severe koa stages testing phase based healthy knee xray proposed model produce continuous effective sequence koa xray images varying degrees severity specifically introduce diffusionbased morphing model modifying denoising diffusion probabilistic model approach integrates diffusion morphing modules enabling model capture spatial morphing details source target knee xray images synthesize intermediate frames along geodesic path hybrid loss consisting diffusion loss morphing loss supervision loss employed demonstrate proposed approach achieves highest temporal frame synthesis performance effectively augmenting data classification models simulating progression koa,-1,0.0,-1,0.0
predicting point tracks internet videos enables generalizable robot manipulation seek learn generalizable goalconditioned policy enables zeroshot robot manipulation interacting unseen objects novel scenes without testtime adaptation typical approaches rely large amount demonstration data generalization propose approach leverages web videos predict plausible interaction plans learns taskagnostic transformation obtain robot actions real world predicts tracks points image move future timesteps based goal trained diverse videos web including humans robots manipulating everyday objects use track predictions infer sequence rigid transforms object manipulated obtain robot endeffector poses executed openloop manner refine openloop plan predicting residual actions closed loop policy trained embodimentspecific demonstrations show approach combining scalably learned track prediction residual policy requiring minimal indomain robotspecific data enables diverse generalizable robot manipulation present wide array realworld robot manipulation results across unseen tasks objects scenes,5,0.925247544354015,5,0.925247544354015
evaluation strategies efficient ratedistortion nerf streaming neural radiance fields nerf revolutionized field visual representation enabling highly realistic detailed scene reconstructions sparse set images nerf uses volumetric functional representation maps points corresponding colors opacities allowing photorealistic view synthesis arbitrary viewpoints despite advancements efficient streaming nerf content remains significant challenge due large amount data involved paper investigates ratedistortion performance two nerf streaming strategies pixelbased neural network nn parameterbased streaming former images coded transmitted throughout network latter respective nerf model parameters coded transmitted instead work also highlights tradeoffs complexity performance demonstrating nn parameterbased strategy generally offers superior efficiency making suitable onetomany streaming scenarios,-1,0.0,-1,0.0
coronetgan controlled pruning gans via hypernetworks generative adversarial networks gans proven exhibit remarkable performance widely used across many generative computer vision applications however unprecedented demand deployment gans resourceconstrained edge devices still poses challenge due huge number parameters involved generation process led focused attention area compressing gans existing works use knowledge distillation overhead teacher dependency moreover ability control degree compression methods hence propose coronetgan compressing gan using combined strength differentiable pruning method via hypernetworks proposed method provides advantage performing controllable compression training along reducing training time substantial factor experiments done various conditional gan architectures cyclegan signify effectiveness approach multiple benchmark datasets edgestoshoes horsetozebra summertowinter results obtained illustrate approach succeeds outperform baselines zebratohorse summertowinter achieving best fid score respectively yielding highfidelity images across datasets additionally approach also outperforms stateoftheart methods achieving better inference time various smartphone chipsets datatypes making feasible solution deployment edge devices,17,1.0,17,1.0
neurocine decoding vivid video sequences human brain activties pursuit understand intricacies human brains visual processing reconstructing dynamic visual experiences brain activities emerges challenging yet fascinating endeavor recent advancements achieved success reconstructing static images noninvasive brain recordings domain translating continuous brain activities video format remains underexplored work introduce neurocine novel dualphase framework targeting inherent challenges decoding fmri data noises spatial redundancy temporal lags framework proposes spatial masking temporal interpolationbased augmentation contrastive learning fmri representations diffusion model enhanced dependent prior noise video generation tested publicly available fmri dataset method shows promising results outperforming previous stateoftheart models notable margin respectively decoding brain activities three subjects fmri dataset measured ssim additionally attention analysis suggests model aligns existing brain structures functions indicating biological plausibility interpretability,-1,0.0,-1,0.0
efficient digital twin data processing lowlatency multicast short video streaming paper propose novel efficient digital twin dt data processing scheme reduce service latency multicast short video streaming particularly dt constructed emulate analyze user status multicast group update swipe feature abstraction precise measurement model dt data processing developed characterize relationship among dt model size user dynamics user clustering accuracy service latency model consisting dt data processing delay video transcoding delay multicast transmission delay constructed incorporating impact user clustering accuracy finally joint optimization problem dt model size selection bandwidth allocation formulated minimize service latency efficiently solve problem diffusionbased resource management algorithm proposed utilizes denoising technique improve actiongeneration process deep reinforcement learning algorithm simulation results based realworld dataset demonstrate proposed dt data processing scheme outperforms benchmark schemes terms service latency,2,1.0,2,1.0
hr human modeling human avatars triangular mesh highresolution textures videos recently implicit neural representation widely used generate animatable human avatars however materials geometry representations coupled neural network hard edit hinders application traditional graphics engines present framework acquiring human avatars attached highresolution physicallybased material textures triangular mesh monocular video method introduces novel information fusion strategy combine information monocular video synthesize virtual multiview images tackle sparsity input view reconstruct humans deformable neural implicit surfaces extract triangle mesh wellbehaved pose initial mesh next stage addition introduce approach correct bias boundary size coarse mesh extracted finally adapt prior knowledge latent diffusion model superresolution multiview distill decomposed texture experiments show approach outperforms previous representations terms high fidelity explicit result supports deployment common renderers,1,0.984741731695719,1,0.984741731695719
frequencyguided diffusion model perturbation training skeletonbased video anomaly detection video anomaly detection essential yet challenging openset task computer vision often addressed leveraging reconstruction proxy task however existing reconstructionbased methods encounter challenges two main aspects limited model robustness openset scenarios overemphasis restricted capacity detailed motion reconstruction end propose novel frequencyguided diffusion model perturbation training enhances model robustness perturbation training emphasizes principal motion components guided motion frequencies specifically first use trainable generator produce perturbative samples perturbation training diffusion model perturbation training phase model robustness enhanced domain reconstructed model broadened training generator subsequently perturbative samples introduced inference impacts reconstruction normal abnormal motions differentially thereby enhancing separability considering motion details originate highfrequency information propose masking method based discrete cosine transform separate highfrequency information lowfrequency information guided highfrequency information observed motion diffusion model focus generating lowfrequency information thus reconstructing motion accurately experimental results five video anomaly detection datasets including humanrelated openset benchmarks demonstrate effectiveness proposed method code available httpsgithubcomxiaofengtanfgdmadcode,-1,0.0,-1,0.0
mededit counterfactual diffusionbased image editing brain mri denoising diffusion probabilistic models enable highfidelity image synthesis editing biomedicine models facilitate counterfactual image editing producing pairs images one edited simulate hypothetical conditions example model progression specific diseases stroke lesions however current image editing techniques often fail generate realistic biomedical counterfactuals either inadequately modeling indirect pathological effects like brain atrophy excessively altering scan disrupts correspondence original images propose mededit conditional diffusion model medical image editing mededit induces pathology specific areas balancing modeling disease effects preserving integrity original scan evaluated mededit atlas stroke dataset using frechet inception distance dice scores outperforming stateoftheart diffusionbased methods palette sdedit additionally clinical evaluations boardcertified neuroradiologist confirmed mededit generated realistic stroke scans indistinguishable real ones believe work enable counterfactual image editing research advance development realistic clinically useful imaging tools,3,0.7258807550503464,3,0.7258807550503464
tutorial diffusion models imaging vision astonishing growth generative tools recent years empowered many exciting applications texttoimage generation texttovideo generation underlying principle behind generative tools concept diffusion particular sampling mechanism overcome shortcomings deemed difficult previous approaches goal tutorial discuss essential ideas underlying diffusion models target audience tutorial includes undergraduate graduate students interested research diffusion models applying models solve problems,-1,0.0,-1,0.0
gaia rethinking action quality assessment aigenerated videos assessing action quality imperative challenging due significant impact quality aigenerated videos complicated inherently ambiguous nature actions within aigenerated video aigv current action quality assessment aqa algorithms predominantly focus actions real specific scenarios pretrained normative action features thus rendering inapplicable aigvs address problems construct gaia generic aigenerated action dataset conducting largescale subjective evaluation novel causal reasoningbased perspective resulting ratings among videoaction pairs based gaia evaluate suite popular texttovideo models ability generate visually rational actions revealing pros cons different categories actions also extend gaia testbed benchmark aqa capacity existing automatic evaluation methods results show traditional aqa methods actionrelated metrics recent benchmarks mainstream video quality methods perform poorly average srcc respectively indicating sizable gap current models human action perception patterns aigvs findings underscore significance action quality unique perspective studying aigvs catalyze progress towards methods enhanced capacities aqa aigvs,-1,0.0,-1,0.0
larp tokenizing videos learned autoregressive generative prior present larp novel video tokenizer designed overcome limitations current video tokenization methods autoregressive ar generative models unlike traditional patchwise tokenizers directly encode local visual patches discrete tokens larp introduces holistic tokenization scheme gathers information visual content using set learned holistic queries design allows larp capture global semantic representations rather limited local patchlevel information furthermore offers flexibility supporting arbitrary number discrete tokens enabling adaptive efficient tokenization based specific requirements task align discrete token space downstream ar generation tasks larp integrates lightweight ar transformer trainingtime prior model predicts next token discrete latent space incorporating prior model training larp learns latent space optimized video reconstruction also structured way conducive autoregressive generation moreover process defines sequential order discrete tokens progressively pushing toward optimal configuration training ensuring smoother accurate ar generation inference time comprehensive experiments demonstrate larps strong performance achieving stateoftheart fvd classconditional video generation benchmark larp enhances compatibility ar models videos opens potential build unified highfidelity multimodal large language models mllms,-1,0.0,-1,0.0
opensora plan opensource large video generation model introduce opensora plan opensource project aims contribute large generation model generating desired highresolution videos long durations based various user inputs project comprises multiple components entire video generation process including waveletflow variational autoencoder joint imagevideo skiparse denoiser various condition controllers moreover many assistant strategies efficient training inference designed multidimensional data curation pipeline proposed obtaining desired highquality data benefiting efficient thoughts opensora plan achieves impressive video generation results qualitative quantitative evaluations hope careful design practical experience inspire video generation research community codes model weights publicly available urlhttpsgithubcompkuyuangroupopensoraplan,-1,0.0,-1,0.0
dsplats generation denoising splatsbased multiview diffusion models generating highquality content requires models capable learning robust distributions complex scenes realworld objects within recent gaussianbased reconstruction techniques achieved impressive results recovering highfidelity assets sparse input images predicting gaussians feedforward manner however techniques often lack extensive priors expressiveness offered diffusion models hand diffusion models successfully applied denoise multiview images show potential generating wide range photorealistic outputs still fall short explicit priors consistency work aim bridge two approaches introducing dsplats novel method directly denoises multiview images using gaussian splatbased reconstructors produce diverse array realistic assets harness extensive priors diffusion models incorporate pretrained latent diffusion model reconstructor backbone predict set gaussians additionally explicit representation embedded denoising network provides strong inductive bias ensuring geometrically consistent novel view generation qualitative quantitative experiments demonstrate dsplats produces highquality spatially consistent outputs also sets new standard singleimage reconstruction evaluated google scanned objects dataset dsplats achieves psnr ssim lpips,-1,0.0,-1,0.0
temporalbench benchmarking finegrained temporal understanding multimodal video models understanding finegrained temporal dynamics crucial multimodal video comprehension generation due lack finegrained temporal annotations existing video benchmarks mostly resemble static image benchmarks incompetent evaluating models temporal understanding paper introduce temporalbench new benchmark dedicated evaluating finegrained temporal understanding videos temporalbench consists video questionanswer pairs derived highquality human annotations detailing temporal dynamics video clips result benchmark provides unique testbed evaluating various temporal understanding reasoning abilities action frequency motion magnitude event order etc moreover enables evaluations various tasks like video question answering captioning short long video understanding well different models multimodal video embedding models text generation models results show stateoftheart models like achieve question answering accuracy temporalbench demonstrating significant gap humans ai temporal understanding furthermore notice critical pitfall multichoice qa llms detect subtle changes negative captions find centralized description cue prediction propose multiple binary accuracy mba correct bias hope temporalbench foster research improving models temporal reasoning capabilities dataset evaluation code made available,0,0.9111184530139986,0,0.9111184530139986
world models effective data machines driving scene representation closedloop simulation essential advancing endtoend autonomous driving systems contemporary sensor simulation methods nerf rely predominantly conditions closely aligned training data distributions largely confined forwarddriving scenarios consequently methods face limitations rendering complex maneuvers eg lane change acceleration deceleration recent advancements autonomousdriving world models demonstrated potential generate diverse driving videos however approaches remain constrained video generation inherently lacking spatiotemporal coherence required capture intricacies dynamic driving environments paper introduce enhances driving scene representation leveraging world model priors specifically utilize world model data machine synthesize novel trajectory videos structured conditions explicitly leveraged control spatialtemporal consistency traffic elements besides cousin data training strategy proposed facilitate merging real synthetic data optimizing knowledge first utilize video generation models improving reconstruction driving scenarios experimental results reveal significantly enhances generation quality novel trajectory views achieving relative improvement fid compared pvg deformablegs moreover markedly enhances spatiotemporal coherence driving agents verified comprehensive user study relative increases ntaiou metric,16,1.0,16,1.0
volumetric saliency guided image summarization rgbd indoor scene classification image summary abridged version original visual content used represent scene thus tasks scene classification identification indexing etc performed efficiently using unique summary saliency commonly used technique generating relevant image summary however definition saliency subjective nature depends upon application existing saliency detection methods using rgbd data mainly focus color texture depth features consequently generated summary contains either foreground objects nonstationary objects however applications scene identification require stationary characteristics scene unlike stateoftheart methods paper proposes novel volumetric saliencyguided framework indoor scene classification results highlight efficacy proposed method,-1,0.0,-1,0.0
towards unbiased robust spatiotemporal scene graph generation anticipation spatiotemporal scene graphs stsgs provide concise expressive representation dynamic scenes modeling objects evolving relationships time however realworld visual relationships often exhibit longtailed distribution causing existing methods tasks like video scene graph generation vidsgg scene graph anticipation sga produce biased scene graphs end propose impartail novel training framework leverages loss masking curriculum learning mitigate bias generation anticipation spatiotemporal scene graphs unlike prior methods add extra architectural components learn unbiased estimators propose impartial training objective reduces dominance head classes learning focuses underrepresented tail relationships curriculumdriven mask generation strategy empowers model adaptively adjust bias mitigation strategy time enabling balanced robust estimations thoroughly assess performance various distribution shifts also introduce two new tasks robust spatiotemporal scene graph generation robust scene graph anticipation offering challenging benchmark evaluating resilience stsg models extensive experiments action genome dataset demonstrate superior unbiased performance robustness method compared existing baselines,-1,0.0,-1,0.0
mechanisms generative imagetoimage translation networks generative adversarial networks gans class neural networks widely used field imagetoimage translation paper propose streamlined imagetoimage translation network simpler architecture compared existing models investigate relationship gans autoencoders provide explanation efficacy employing gan component tasks involving image translation show adversarial gan models yields results comparable existing methods without additional complex loss penalties subsequently elucidate rationale behind phenomenon also incorporate experimental results demonstrate validity findings,-1,0.0,-1,0.0
dream improving videotext retrieval relevancebased augmentation using large foundation models recent progress videotext retrieval driven largely advancements model architectures training strategies however representation learning capabilities videotext retrieval models remain constrained lowquality limited training data annotations address issue present novel videotext retrieval paradigm relevancebased augmentation namely dream enhances video text data using large foundation models learn generalized features specifically first adopt simple augmentation method generates selfsimilar data randomly duplicating dropping subwords frames addition inspired recent advancement visual language generative models propose robust augmentation method textual paraphrasing video stylization using large language models llms visual generative models vgms enrich video text information propose relevancebased augmentation method llms vgms generate integrate new relevant information original data leveraging enriched data extensive experiments several videotext retrieval benchmarks demonstrate superiority dream existing methods,0,1.0,0,1.0
learning online scale transformation talking head video generation oneshot talking head video generation uses source image driving video create synthetic video source persons facial movements imitate driving video however differences scale source driving images remain challenge face reenactment existing methods attempt locate frame driving video aligns best source image imprecise alignment result suboptimal outcomes end introduce scale transformation module automatically adjust scale driving image fit source image using information scale difference maintained detected keypoints source image driving frame furthermore keep perceiving scale information faces generation process incorporate scale information learned scale transformation module layer generation process produce final result accurate scale method perform accurate motion transfer two images without anchor frame achieved contributions proposed online scale transformation facial reenactment network extensive experiments demonstrated proposed method adjusts scale driving face automatically according source face generates highquality faces accurate scale crossidentity facial reenactment,6,0.7630552886431554,6,0.7630552886431554
towards open domain textdriven synthesis multiperson motions work aims generate natural diverse group motions multiple humans textual descriptions singleperson texttomotion generation extensively studied remains challenging synthesize motions one two subjects inthewild prompts mainly due lack available datasets work curate human pose motion datasets estimating pose information largescale image video datasets models use transformerbased diffusion framework accommodates multiple datasets number subjects frames experiments explore generation multiperson static poses generation multiperson motion sequences knowledge method first generate multisubject motion sequences high diversity fidelity large variety textual prompts,18,1.0,18,1.0
upsample guidance scale diffusion models without training diffusion models demonstrated superior performance across various generative tasks including images videos audio however encounter difficulties directly generating highresolution samples previously proposed solutions issue involve modifying architecture training partitioning sampling process multiple stages methods limitation able directly utilize pretrained models asis requiring additional work paper introduce upsample guidance technique adapts pretrained diffusion model eg generate higherresolution images eg adding single term sampling process remarkably technique necessitate additional training relying external models demonstrate upsample guidance applied various models pixelspace latent space video diffusion models also observed proper selection guidance scale improve image quality fidelity prompt alignment,-1,0.0,-1,0.0
pseudomriguided pet image reconstruction method based diffusion probabilistic model anatomically guided pet reconstruction using mri information shown potential improve pet image quality however improvements limited pet scans paired mri information work employed diffusion probabilistic model dpm infer deepmri images fdgpet brain images use dpmgenerated guide pet reconstruction model trained brain fdg scans tested datasets containing multiple levels counts deepmri images appeared somewhat degraded acquired mri images regarding pet image quality volume interest analysis different brain regions showed pet reconstructed images using acquired deepmri images improved image quality compared osem conclusions found analysing decimated datasets subjective evaluation performed two physicians confirmed osem scored consistently worse mriguided pet images significant differences observed mriguided pet images proof concept shows possible infer dpmbased mri imagery guide pet reconstruction enabling possibility changing reconstruction parameters strength prior anatomically guided pet reconstruction absence mri,3,1.0,3,1.0
reading frames multimodal depression detection videos nonverbal cues depression prominent contributor global disability affects substantial portion population efforts detect depression social media texts prevalent yet works explored depression detection usergenerated video content work address research gap proposing simple flexible multimodal temporal model capable discerning nonverbal depression cues diverse modalities noisy realworld videos show inthewild videos using additional highlevel nonverbal cues crucial achieving good performance extracted processed audio speech embeddings face emotion embeddings face body hand landmarks gaze blinking information extensive experiments show model achieves stateoftheart results three key benchmark datasets depression detection video substantial margin code publicly available github,-1,0.0,-1,0.0
network structural dense displacement based deformable mesh model optical flow study proposes network recognize displacement rc frame structure video monocular camera proposed network consists two modules pofrnnet used generate dense optical flow well pofrnnet extract pose parameter h convert two video frames dense optical flow pofrnnet inputted dense optical flow output pose parameter h displacement points structure calculated parameter h fast fourier transform fft applied obtain frequency domain signal corresponding displacement signal furthermore comparison truth displacement first floor first video shown study finally predicted displacements four floors rc frame structure given three videos exhibited last study,-1,0.0,-1,0.0
depthaware testtime training zeroshot video object segmentation zeroshot video object segmentation zsvos aims segmenting primary moving object without human annotations mainstream solutions mainly focus learning single model largescale video datasets struggle generalize unseen videos work introduce testtime training ttt strategy address problem key insight enforce model predict consistent depth ttt process detail first train single network perform segmentation depth prediction tasks effectively learned specifically designed depth modulation layer ttt process model updated predicting consistent depth maps frame different data augmentations addition explore different ttt weight updating strategies empirical results suggest momentumbased weight initialization loopingbased training scheme lead stable improvements experiments show proposed method achieves clear improvements zsvos proposed video ttt strategy provides significant superiority stateoftheart ttt methods code available httpsnifangbaagegithubiodattt,-1,0.0,-1,0.0
temporally consistent unbalanced optimal transport unsupervised action segmentation propose novel approach action segmentation task long untrimmed videos based solving optimal transport problem encoding temporal consistency prior gromovwasserstein problem able decode temporally consistent segmentation noisy affinitymatching cost matrix video frames action classes unlike previous approaches method require knowing action order video attain temporal consistency furthermore resulting fused gromovwasserstein problem efficiently solved gpus using iterations projected mirror descent demonstrate effectiveness method unsupervised learning setting method used generate pseudolabels selftraining evaluate segmentation approach unsupervised learning pipeline breakfast youtube instructions desktop assembly datasets yielding stateoftheart results unsupervised video action segmentation task,7,0.9109369990739105,7,0.9109369990739105
transformerbased model prediction human gaze behavior videos eyetracking applications utilize human gaze video understanding tasks become increasingly important effectively automate process video analysis based eyetracking data important accurately replicate human gaze behavior however task presents significant challenges due inherent complexity ambiguity human gaze patterns work introduce novel method simulating human gaze behavior approach uses transformerbased reinforcement learning algorithm train agent acts human observer primary role watching videos simulating human gaze behavior employed eyetracking dataset gathered videos generated virtualhome simulator primary focus activity recognition experimental results demonstrate effectiveness gaze prediction method highlighting capability replicate human gaze behavior applicability downstream tasks real humangaze used input,5,0.2557034098603377,5,0.2557034098603377
leveraging temporal contextualization video action recognition propose novel framework video understanding called temporally contextualized clip tcclip leverages essential temporal information global interactions spatiotemporal domain within video specific introduce temporal contextualization tc layerwise temporal information infusion mechanism videos extracts core information frame connects relevant information across frames summarization context tokens leverages context tokens feature encoding furthermore videoconditional prompting vp module processes context tokens generate informative prompts text modality extensive experiments zeroshot fewshot basetonovel fullysupervised action recognition validate effectiveness model ablation studies tc vp support design choices project page source code available httpsgithubcomnaveraitcclip,-1,0.0,-1,0.0
schvppnet spatial channel hybridattention video postprocessing network cnn transformer convolutional neural network cnn transformer attracted much attention recently video postprocessing vpp however interaction cnn transformer existing vpp methods fully explored leading inefficient communication local global extracted features paper explore interaction cnn transformer task vpp propose novel spatial channel hybridattention video postprocessing network schvppnet cooperatively exploit image priors spatial channel domains specifically spatial domain novel spatial attention fusion module designed two attention weights generated fuse local global representations collaboratively channel domain novel channel attention fusion module developed blend deep representations channel dimension dynamically extensive experiments show schvppnet notably boosts video restoration quality average bitrate savings u v components ra configuration,-1,0.0,-1,0.0
csta cnnbased spatiotemporal attention video summarization video summarization aims generate concise representation video capturing essential content key moments reducing overall length although several methods employ attention mechanisms handle longterm dependencies often fail capture visual significance inherent frames address limitation propose cnnbased spatiotemporal attention csta method stacks feature frames single video form imagelike frame representations applies cnn frame features methodology relies cnn comprehend inter intraframe relations find crucial attributes videos exploiting ability learn absolute positions within images contrast previous work compromising efficiency designing additional modules focus spatial importance csta requires minimal computational overhead uses cnn sliding window extensive experiments two benchmark datasets summe tvsum demonstrate proposed approach achieves stateoftheart performance fewer macs compared previous methods codes available,-1,0.0,-1,0.0
contextenhanced video moment retrieval large language models current methods video moment retrieval vmr struggle align complex situations involving specific environmental details character descriptions action narratives tackle issue propose large language modelguided moment retrieval lmr approach employs extensive knowledge large language models llms improve video context representation well crossmodal alignment facilitating accurate localization target moments specifically lmr introduces context enhancement technique llms generate crucial targetrelated context semantics semantics integrated visual features producing discriminative video representations finally languageconditioned transformer designed decode freeform language queries fly using aligned video representations moment retrieval extensive experiments demonstrate lmr achieves stateoftheart results outperforming nearest competitor challenging qvhighlights charadessta benchmarks respectively importantly performance gains significantly higher localization complex queries,0,1.0,0,1.0
compressed video quality enhancement temporal group alignment fusion paper propose temporal group alignment fusion network enhance quality compressed videos using longshort term correlations frames proposed model consists intragroup feature alignment intragfa module intergroup feature fusion intergff module feature enhancement fe module form group pictures gop selecting frames video according temporal distances target enhanced frame grouping composed gop contain either long shortterm correlated information neighboring frames design intragfa module align features frames gop eliminate motion existing frames construct intergff module fuse features belonging different gops finally enhance fused features fe module generate highquality video frames experimental results show proposed method achieves gain lower complexity compared stateoftheart method,-1,0.0,-1,0.0
computational thinking design patterns video games prior research explored potential applications video games programming education elicit computational thinking skills however existing approaches often either general taking account diversity genres mechanisms video games narrow selecting tools specifically designed educational purposes paper propose fundamental approach defining beneficial connections individual design patterns present video games computational thinking skills argue video games capacity elicit skills even potentially train could effective method solidify conceptual base would make programming education effective,10,1.0,10,1.0
audiodriven highresolution seamless talking head video editing via stylegan existing methods audiodriven talking head video editing limitations poor visual effects paper tries tackle problem editing talking face images seamless different emotions based two modules audiotolandmark module consisting crossreconstructed emotion disentanglement alignment network module bridges gap speech facial motions predicting corresponding emotional landmarks speech landmarkbased editing module edits face videos via stylegan aims generate seamless edited video consisting emotion content components input audio extensive experiments confirm compared stateofthearts methods method provides highresolution videos high visual quality,6,0.8516056833924918,6,0.8516056833924918
orthogonal hypercategory guided multiinterest elicitation microvideo matching watching microvideos becoming part public daily life usually user watching behaviors thought rooted multiple different interests paper propose model named opal microvideo matching elicits users multiple heterogeneous interests disentangling multiple soft hard interest embeddings user interactions moreover opal employs twostage training strategy pretrain generate soft interests historical interactions guidance orthogonal hypercategories microvideos finetune reinforce degree disentanglement among interests learn temporal evolution interest user conduct extensive experiments two realworld datasets results show opal returns diversified microvideos also outperforms six stateoftheart models terms recall hit rate,-1,0.0,-1,0.0
mtfl multitimescale feature learning weaklysupervised anomaly detection surveillance videos detection anomaly events relevant public safety requires combination finegrained motion information contextual events variable timescales end propose multitimescale feature learning mtfl method enhance representation anomaly features short medium long temporal tubelets employed extract spatiotemporal video features using video swin transformer experimental results demonstrate mtfl outperforms stateoftheart methods ucfcrime dataset achieving anomaly detection performance auc moreover performs complementary sota auc shanghaitech ap xdviolence dataset furthermore generate extended dataset ucfcrime development evaluation wider range anomalies namely video anomaly detection dataset vadd involving videos classes extensive coverage realistic anomalies,-1,0.0,-1,0.0
elastictok adaptive tokenization image video efficient video tokenization remains key bottleneck learning general purpose vision models capable processing long video sequences prevailing approaches restricted encoding videos fixed number tokens tokens result overly lossy encodings many tokens result prohibitively long sequence lengths work introduce elastictok method conditions prior frames adaptively encode frame variable number tokens enable computationally scalable way propose masking technique drops random number tokens end framess token encoding inference elastictok dynamically allocate tokens needed complex data leverage tokens simpler data needs tokens empirical evaluations images video demonstrate effectiveness approach efficient token usage paving way future development powerful multimodal models world models agents,-1,0.0,-1,0.0
agenttosim learning interactive behavior models casual longitudinal videos present agenttosim ats framework learning interactive behavior models agents casual longitudinal video collections different prior works rely markerbased tracking multiview cameras ats learns natural behaviors animal human agents noninvasively video observations recorded long timespan eg month single environment modeling behavior agent requires persistent tracking eg knowing point corresponds long time period obtain data develop coarsetofine registration method tracks agent camera time canonical space resulting complete persistent spacetime representation train generative model agent behaviors using paired data perception motion agent queried reconstruction ats enables realtosim transfer video recordings agent interactive behavior simulator demonstrate results pets eg cat dog bunny human given monocular rgbd videos captured smartphone,5,0.25183063196789757,5,0.25183063196789757
enhancing multimodal affective analysis learned live comment features live comments also known danmaku usergenerated messages synchronized video content comments overlay directly onto streaming videos capturing viewer emotions reactions realtime prior work leveraged live comments affective analysis use limited due relative rarity live comments across different video platforms address first construct live comment affective analysis lcaffect dataset contains live comments english chinese videos spanning diverse genres elicit wide spectrum emotions using dataset use contrastive learning train video encoder produce synthetic live comment features enhanced multimodal affective content analysis comprehensive experimentation wide range affective analysis tasks sentiment emotion recognition sarcasm detection english chinese demonstrate synthetic live comment features significantly improve performance stateoftheart methods,-1,0.0,-1,0.0
eventguided lowlight video semantic segmentation recent video semantic segmentation vss methods demonstrated promising results welllit environments however performance significantly drops lowlight scenarios due limited visibility reduced contextual details addition unfavorable lowlight conditions make harder incorporate temporal consistency across video frames thus lead video flickering effects compared conventional cameras event cameras capture motion dynamics filter temporalredundant information robust lighting conditions end propose evsnet lightweight framework leverages event modality guide learning unified illuminationinvariant representation specifically leverage motion extraction module extract shortterm longterm temporal motions event modality motion fusion module integrate image features motion features adaptively furthermore use temporal decoder exploit video contexts generate segmentation predictions designs evsnet result lightweight architecture achieving sota performance experimental results largescale datasets demonstrate proposed evsnet outperforms sota methods higher parameter efficiency,-1,0.0,-1,0.0
referring video object segmentation via languagealigned track selection referring video object segmentation rvos requires tracking segmenting object throughout video according given natural language expression demanding complex motion understanding alignment visual representations language descriptions given challenges recently proposed segment anything model emerges potential candidate due ability generate coherent segmentation mask tracks across video frames provide inherent spatiotemporal objectness object token representations paper introduce sola selection object language alignment novel framework leverages object tokens compact videolevel object representations aligned language features lightweight track selection module effectively facilitate alignment propose ioubased pseudolabeling strategy bridges modality gap representations language features extensive experiments show sola achieves stateoftheart performance mevis dataset demonstrate sola offers effective solution rvos project page available httpscvlabkaistgithubiosola,-1,0.0,-1,0.0
experimental study lowlatency video streaming oran setup generative ai video streaming services depend underlying communication infrastructure available network resources offer ultralow latency highquality content delivery open radio access network oran provides dynamic programmable flexible ran architecture configured support requirements timecritical applications work considers setup constrained network resources supplemented glsgai glsmec techniques order reach satisfactory video quality specifically implement novel semantic control channel enables glsmec support lowlatency applications tight coupling among oran xapp glsmec control channel proposed concepts experimentally verified actual oran setup supports video streaming performance evaluation includes glspsnr metric endtoend latency findings reveal latency adjustments yield gains image glspsnr underscoring tradeoff potential optimized video quality resourcelimited environments,2,1.0,2,1.0
esvqa perceptual quality assessment egocentric spatial videos rapid development extended reality xr egocentric spatial shooting display technologies enhanced immersion engagement users assessing quality experience qoe egocentric spatial videos crucial ensure highquality viewing experience however corresponding research still lacking paper use embodied experience highlight immersive experience study new problem ie embodied perceptual quality assessment egocentric spatial videos specifically introduce first egocentric spatial video quality assessment database esvqad comprises egocentric spatial videos mean opinion scores moss furthermore propose novel multidimensional binocular feature fusion model termed esvqanet integrates binocular spatial motion semantic features predict perceptual quality experimental results demonstrate esvqanet outperforms stateoftheart vqa models embodied perceptual quality assessment task exhibits strong generalization capability traditional vqa tasks database codes released upon publication,12,1.0,12,1.0
genxd generating scenes recent developments visual generation remarkably successful however generation remain challenging realworld applications due lack largescale data effective model design paper propose jointly investigate general generation leveraging camera object movements commonly observed daily life due lack realworld data community first propose data curation pipeline obtain camera poses object motion strength videos based pipeline introduce largescale realworld scene dataset leveraging data develop framework genxd allows us produce scene propose multiviewtemporal modules disentangle camera object movements seamlessly learn data additionally genxd employs masked latent conditions support variety conditioning views genxd generate videos follow camera trajectory well consistent views lifted representations perform extensive evaluations across various realworld synthetic datasets demonstrating genxds effectiveness versatility compared previous methods generation,1,1.0,1,1.0
nonlinear inverse design mechanical multimaterial metamaterials enabled video denoising diffusion structure identifier metamaterials synthetic materials customized properties emerged promising field due advancements additive manufacturing materials derive unique mechanical properties internal lattice structures often composed multiple materials repeat geometric patterns traditional inverse design approaches shown potential struggle map nonlinear material behavior multiple possible structural configurations paper presents novel framework leveraging video diffusion models type generative artificial intelligence ai inverse multimaterial design based nonlinear stressstrain responses approach consists two key components fields generator using video diffusion model create solution fields based target nonlinear stressstrain responses structure identifier employing two unet models determine corresponding multimaterial design incorporating multiple materials plasticity large deformation innovative design method allows enhanced control highly nonlinear mechanical behavior metamaterials commonly seen realworld applications offers promising solution generating nextgeneration metamaterials finely tuned mechanical characteristics,-1,0.0,-1,0.0
nexttoken prediction need nexttoken prediction considered promising path towards artificial general intelligence struggled excel multimodal tasks still dominated diffusion models eg stable diffusion compositional approaches eg clip combined llms paper introduce new suite stateoftheart multimodal models trained solely nexttoken prediction tokenizing images text videos discrete space train single transformer scratch mixture multimodal sequences outperforms several wellestablished taskspecific models generation perception tasks surpassing flagship models sdxl eliminating need diffusion compositional architectures also capable generating highfidelity video via predicting next token video sequence simplify complex multimodal model designs converging singular focus tokens unlocking great potential scaling training inference results demonstrate nexttoken prediction promising path towards building general multimodal intelligence beyond language opensource key techniques models support research direction,-1,0.0,-1,0.0
ct synthesis conditional diffusion models abdominal lymph node segmentation despite significant success achieved deep learning methods medical image segmentation researchers still struggle computeraided diagnosis abdominal lymph nodes due complex abdominal environment small indistinguishable lesions limited annotated data address problems present pipeline integrates conditional diffusion model lymph node generation nnunet model lymph node segmentation improve segmentation performance abdominal lymph nodes synthesizing diversity realistic abdominal lymph node data propose lnddpm conditional denoising diffusion probabilistic model ddpm lymph node ln generation lnddpm utilizes lymph node masks anatomical structure masks model conditions conditions work two conditioning mechanisms global structure conditioning local detail conditioning distinguish lymph nodes surroundings better capture lymph node characteristics obtained paired abdominal lymph node images masks used downstream segmentation task experimental results abdominal lymph node datasets demonstrate lnddpm outperforms generative methods abdominal lymph node image synthesis better assists downstream abdominal lymph node segmentation task,3,0.8787792186270161,3,0.8787792186270161
pegasus personalized generative avatars composable attributes present pegasus method constructing personalized generative face avatar monocular video sources generative avatar enables disentangled controls selectively alter facial attributes eg hair nose preserving identity approach consists two stages synthetic database generation constructing personalized generative avatar generate synthetic video collection target identity varying facial attributes videos synthesized borrowing attributes monocular videos diverse identities build personspecific generative avatar modify attributes continuously preserving identity extensive experiments demonstrate method generating synthetic database creating generative avatar effective preserving identity achieving high realism subsequently introduce zeroshot approach achieve goal generative modeling efficiently leveraging previously constructed personalized generative model,-1,0.0,-1,0.0
refdrop controllable consistency image video generation via reference feature guidance rapidly growing interest controlling consistency across multiple generated images using diffusion models among various methods recent works found simply manipulating attention modules concatenating features multiple reference images provides efficient approach enhancing consistency without finetuning despite popularity success studies elucidated underlying mechanisms contribute effectiveness work reveal popular approach linear interpolation image selfattention crossattention synthesized content reference features constant coefficient motivated observation find coefficient necessary simplifies controllable generation mechanism resulting algorithm coin refdrop allows users control influence reference context direct precise manner besides enhancing consistency singlesubject image generation method also enables interesting applications consistent generation multiple subjects suppressing specific features encourage diverse content highquality personalized video generation boosting temporal consistency even compared stateoftheart imagepromptbased generators ipadapter refdrop competitive terms controllability quality avoiding need train separate image encoder feature injection reference images making versatile plugandplay solution image video diffusion model,2,0.6962709329283329,2,0.6962709329283329
improving textconditioned latent diffusion cancer pathology development generative models past decade allowed hyperrealistic data synthesis potentially beneficial synthetic data generation process relatively underexplored cancer histopathology one algorithm synthesising realistic image diffusion iteratively converts image noise learns recovery process noise wang vastola effective highly computationally expensive highresolution images rendering infeasible histopathology development variational autoencoders vaes allowed us learn representation complex highresolution images latent space vital byproduct ability compress highresolution images space recover lossless marriage diffusion vaes allows us carry diffusion latent space autoencoder enabling us leverage realistic generative capabilities diffusion maintaining reasonable computational requirements rombach et al yellapragada et al build foundational models task paving way generate realistic histopathology images paper discuss pitfalls current methods namely yellapragada et al resolve critical errors proposing improvements along way methods achieve fid score beating sota counterparts yellapragada et al fid presenting traintime gpu memory usage reduction,-1,0.0,-1,0.0
aesopagent agentdriven evolutionary system storytovideo production agent aigc artificial intelligence generated content technologies recently made significant progress propose aesopagent agentdriven evolutionary system storytovideo production aesopagent practical application agent technology multimodal content generation system integrates multiple generative capabilities within unified framework individual users leverage modules easily innovative system would convert user story proposals scripts images audio integrate multimodal contents videos additionally animating units eg sora could make videos infectious aesopagent system could orchestrate task workflow video generation ensuring generated video rich content coherent system mainly contains two layers ie horizontal layer utility layer horizontal layer introduce novel ragbased evolutionary system optimizes whole video generation workflow steps within workflow continuously evolves iteratively optimizes workflow accumulating expert experience professional knowledge including optimizing llm prompts utilities usage utility layer provides multiple utilities leading consistent image generation visually coherent terms composition characters style meanwhile provides audio special effects integrating expressive logically arranged videos overall aesopagent achieves stateoftheart performance compared many previous works visual storytelling aesopagent designed convenient service individual users available following page httpsaesopaigithubio,-1,0.0,-1,0.0
nes videomusic database dataset symbolic video game music paired gameplay videos neural models one popular approaches music generation yet arent standard large datasets tailored learning music directly game data address research gap introduce novel dataset named nesvmdb containing gameplay videos nes games paired original soundtrack symbolic format midi nesvmdb built upon nintendo entertainment system music database nesmdb encompassing music pieces nes games approach involves collecting longplay videos games original dataset slicing clips extracting audio clip subsequently apply audio fingerprinting algorithm similar shazam automatically identify corresponding piece nesmdb dataset additionally introduce baseline method based controllable music transformer generate nes music conditioned gameplay clips evaluated approach objective metrics results showed conditional cmt improves musical structural quality compared unconditional counterpart moreover used neural classifier predict game genre generated pieces results showed cmt generator learn correlations gameplay videos game genres research conducted achieve humanlevel performance,8,0.4349018732502582,8,0.4349018732502582
aniclipart clipart animation texttovideo priors clipart premade art form offers convenient efficient way creating visual content however traditional workflows animating static clipart laborious timeconsuming involving steps like rigging keyframing inbetweening recent advancements texttovideo generation hold great potential resolving challenge nevertheless direct application texttovideo models often struggles preserve visual identity clipart generate cartoonstyle motion resulting subpar animation outcomes paper introduce aniclipart computational system converts static clipart highquality animations guided texttovideo priors generate natural smooth coherent motion first parameterize motion trajectories keypoints defined initial clipart image cubic bezier curves align motion trajectories given text prompt optimizing video score distillation sampling sds loss skeleton fidelity loss incorporating differentiable asrigidaspossible arap shape deformation differentiable rendering aniclipart endtoend optimized maintaining deformation rigidity extensive experimental results show proposed aniclipart consistently outperforms competing methods terms textvideo alignment visual identity preservation temporal consistency additionally showcase versatility aniclipart adapting generate layered animations allow topological changes,-1,0.0,-1,0.0
sentimentoriented transformerbased variational autoencoder network live video commenting automatic live video commenting increasing attention due significance narration generation topic explanation etc however diverse sentiment consideration generated comments missing current methods sentimental factors critical interactive commenting lack research far thus paper propose sentimentoriented transformerbased variational autoencoder sotvae network consists sentimentoriented diversity encoder module batch attention module achieve diverse video commenting multiple sentiments multiple semantics specifically sentimentoriented diversity encoder elegantly combines vae random mask mechanism achieve semantic diversity sentiment guidance fused crossmodal features generate live video comments furthermore batch attention module also proposed paper alleviate problem missing sentimental samples caused data imbalance common live videos popularity videos varies extensive experiments livebot videoic datasets demonstrate proposed sotvae outperforms stateoftheart methods terms quality diversity generated comments related code available,-1,0.0,-1,0.0
fancyvideo towards dynamic consistent video generation via crossframe textual guidance synthesizing motionrich temporally consistent videos remains challenge artificial intelligence especially dealing extended durations existing texttovideo models commonly employ spatial crossattention text control equivalently guiding different frame generations without framespecific textual guidance thus models capacity comprehend temporal logic conveyed prompts generate videos coherent motion restricted tackle limitation introduce fancyvideo innovative video generator improves existing textcontrol mechanism welldesigned crossframe textual guidance module ctgm specifically ctgm incorporates temporal information injector tii temporal affinity refiner tar temporal feature booster tfb beginning middle end crossattention respectively achieve framespecific textual guidance firstly tii injects framespecific information latent features text conditions thereby obtaining crossframe textual conditions tar refines correlation matrix crossframe textual conditions latent features along time dimension lastly tfb boosts temporal consistency latent features extensive experiments comprising quantitative qualitative evaluations demonstrate effectiveness fancyvideo video demo code model available,9,0.6845118564374378,9,0.6845118564374378
lips lying spotting temporal inconsistency audio visual lipsyncing deepfakes recent years deepfake technology achieved unprecedented success highquality video synthesis methods also pose potential severe security threats humanity deepfake bifurcated entertainment applications like face swapping illicit uses lipsyncing fraud however lipforgery videos neither change identity discernible visual artifacts present formidable challenge existing deepfake detection methods preliminary experiments shown effectiveness existing methods often drastically decrease even fail tackling lipsyncing videos paper first time propose novel approach dedicated lipforgery identification exploits inconsistency lip movements audio signals also mimic human natural cognition capturing subtle biological links lips head regions boost accuracy better illustrate effectiveness advances proposed method create highquality lipsync dataset avlips employing stateoftheart lip generators hope highquality diverse dataset could well served research challenging interesting field experimental results show approach gives average accuracy spotting lipsyncing videos significantly outperforming baselines extensive experiments demonstrate capability tackle deepfakes robustness surviving diverse input transformations method achieves accuracy realworld scenarios eg wechat video call shows powerful capabilities real scenario deployment facilitate progress research community release resources httpsgithubcomaaroncomolipfd,4,0.9244845646077513,4,0.9244845646077513
sequential posterior sampling diffusion models diffusion models quickly risen popularity ability model complex distributions perform effective posterior sampling unfortunately iterative nature generative models makes computationally expensive unsuitable realtime sequential inverse problems ultrasound imaging considering strong temporal structure across sequences frames propose novel approach models transition dynamics improve efficiency sequential diffusion posterior sampling conditional image synthesis modeling sequence data using video vision transformer vivit transition model based previous diffusion outputs initialize reverse diffusion trajectory lower noise scale greatly reducing number iterations required convergence demonstrate effectiveness approach realworld dataset high frame rate cardiac ultrasound images show achieves performance full diffusion trajectory accelerating inference enabling realtime posterior sampling furthermore show addition transition model improves psnr cases severe motion method opens new possibilities realtime applications diffusion models imaging domains requiring realtime inference,13,0.9779357180832958,13,0.9779357180832958
stochastic deep restoration priors imaging inverse problems deep neural networks trained image denoisers widely used priors solving imaging inverse problems gaussian denoising thought sufficient learning image priors show priors deep models pretrained general restoration operators perform better introduce stochastic deep restoration priors sharp novel method leverages ensemble restoration models regularize inverse problems sharp improves upon methods using gaussian denoiser priors better handling structured artifacts enabling selfsupervised training even without fully sampled data prove sharp minimizes objective function involving regularizer derived score functions minimum mean square error mmse restoration operators theoretically analyze convergence empirically sharp achieves stateoftheart performance tasks magnetic resonance imaging reconstruction singleimage superresolution surpassing denoiserand diffusionmodelbased methods without requiring retraining,-1,0.0,-1,0.0
cyclic perceptual loss crossmodal image synthesis mri taupet alzheimers disease ad common form dementia characterised cognitive decline biomarkers tauproteins taupositron emission tomography taupet employs radiotracer selectively bind detect visualise tau protein aggregates within brain valuable early ad diagnosis less accessible due high costs limited availability invasive nature image synthesis neural networks enables generation taupet images accessible magnetic resonance imaging mri images ensure highquality image synthesis propose cyclic perceptual loss combined mean squared error structural similarity index measure ssim losses cyclic perceptual loss sequentially calculates axial average perceptual loss specified number epochs followed coronal sagittal planes number epochs sequence cyclically performed intervals reducing cycles repeat conduct supervised synthesis taupet images mri images using paired mri taupet images adni database collected data perform preprocessing including intensity standardisation taupet images manufacturer proposed loss applied generative unet variants outperformed perceptual losses ssim peak signaltonoise ratio psnr addition including cyclic perceptual loss original losses ganbased image synthesis models cyclegan improves ssim psnr least furthermore bymanufacturer pet standardisation helps models synthesising highquality images minmax pet normalisation,19,0.8313489201150798,19,0.8313489201150798
accelerating vision diffusion transformers skip branches diffusion transformers dit emerging image video generation model architecture demonstrated great potential high generation quality scalability properties despite impressive performance practical deployment constrained computational complexity redundancy sequential denoising process feature caching across timesteps proven effective accelerating diffusion models application dit limited fundamental architectural differences unetbased approaches empirical analysis dit feature dynamics identify significant feature variation dit blocks presents key challenge feature reusability address convert standard dit skipdit skip branches enhance feature smoothness introduce skipcache utilizes skip branches cache dit features across timesteps inference time validated effectiveness proposal different dit backbones video image generation showcasing skip branches help preserve generation quality achieve higher speedup experimental results indicate skipdit achieves speedup almost free speedup minor reduction quantitative metrics code available httpsgithubcomopensparsellmsskipditgit,-1,0.0,-1,0.0
physicsbased scene layout generation human motion creating scenes captured motions achieve realistic humanscene interaction crucial animation movies video games character motion often captured bluescreened studio without real furniture objects place may discrepancy planned motion captured one gives rise need automatic scene layout generation relieve burdens selecting positioning furniture objects previous approaches avoid artifacts like penetration floating due lack physical constraints furthermore heavily rely specific data learn contact affordances restricting generalization ability different motions work present physicsbased approach simultaneously optimizes scene layout generator simulates moving human physics simulator attain plausible realistic interaction motions method explicitly introduces physical constraints automatically recover generate scene layout minimize motion tracking errors identify objects afford interaction use reinforcement learning perform dualoptimization character motion imitation controller scene layout generator facilitate optimization reshape tracking rewards devise pose prior guidance obtained estimated pseudocontact labels evaluate method using motions samp prox demonstrate physically plausible scene layout reconstruction compared previous kinematicsbased method,18,1.0,18,1.0
expressionaware video inpainting hmd removal xr applications headmounted displays hmds serve indispensable devices observing extended reality xr environments virtual content however hmds present obstacle external recording techniques block upper face user limitation significantly affects social xr applications specifically teleconferencing facial features eye gaze information play vital role creating immersive user experience study propose new network expressionaware video inpainting hmd removal evihrnet based generative adversarial networks gans model effectively fills missing information regard facial landmarks single occlusionfree reference image user framework components ensure preservation users identity across frames using reference frame improve level realism inpainted output introduce novel facial expression recognition fer loss function emotion preservation results demonstrate remarkable capability proposed framework remove hmds facial videos maintaining subjects facial expression identity moreover outputs exhibit temporal consistency along inpainted frames lightweight framework presents practical approach hmd occlusion removal potential enhance various collaborative xr applications without need additional hardware,6,0.40833895386691055,6,0.40833895386691055
functional imaging constrained diffusion brain pet synthesis structural mri magnetic resonance imaging mri positron emission tomography pet increasingly used multimodal analysis neurodegenerative disorders mri broadly utilized clinical settings pet less accessible many studies attempted use deep generative models synthesize pet mri scans however often suffer unstable training inadequately preserve brain functional information conveyed pet end propose functional imaging constrained diffusion ficd framework brain pet image synthesis paired structural mri input condition new constrained diffusion model cdm ficd introduces noise pet progressively removes cdm ensuring high output fidelity throughout stable training phase cdm learns predict denoised pet functional imaging constraint introduced ensure voxelwise alignment denoised pet ground truth quantitative qualitative analyses conducted subjects paired mri fdgpet scans suggest ficd achieves superior performance generating fdgpet data compared stateoftheart methods validate effectiveness proposed ficd data total subjects three downstream tasks experimental results suggesting utility generalizability,3,0.9818733983711829,3,0.9818733983711829
opflowtalker realistic natural talking face generation via optical flow guidance creating realistic natural lipreadable talking face videos remains formidable challenge previous research primarily concentrated generating aligning singleframe images overlooking smoothness frametoframe transitions temporal dependencies often compromised visual quality effects practical settings particularly handling complex facial data audio content frequently led semantically incongruent visual illusions specifically synthesized videos commonly featured disorganized lip movements making difficult understand recognize overcome limitations paper introduces application optical flow guide facial image generation enhancing interframe continuity semantic consistency propose opflowtalker novel approach utilizes predicted optical flow changes audio inputs rather direct image predictions method smooths image transitions aligns changes semantic content moreover employs sequence fusion technique replace independent generation single frames thus preserving contextual information maintaining temporal coherence also developed optical flow synchronization module regulates fullface lip movements optimizing visual synthesis balancing regional dynamics furthermore introduce visual text consistency score vtcs accurately measures lipreadability synthesized videos extensive empirical evidence validates effectiveness approach,6,0.5324623999889403,6,0.5324623999889403
mcgan enhancing gan training regressionbased generator loss generative adversarial networks gans emerged powerful tool generating highfidelity data however main bottleneck existing approaches lack supervision generator training often results undamped oscillation unsatisfactory performance address issue propose algorithm called monte carlo gan mcgan approach utilizing innovative generative loss function termly regression loss reformulates generator training regression task enables generator training minimizing mean squared error discriminators output real data expected discriminator fake data demonstrate desirable analytic properties regression loss including discriminability optimality show method requires weaker condition discriminator effective generator training properties justify strength approach improve training stability retaining optimality gan leveraging strong supervision regression loss extensive experiments diverse datasets including image data imagenet lsun bedroom time series data var stock data video data conducted demonstrate flexibility effectiveness proposed mcgan numerical results show proposed mcgan versatile enhancing variety backbone gan models achieves consistent significant improvement terms quality accuracy training stability learned latent space,17,1.0,17,1.0
selfsupervised learning video representations childs perspective children learn powerful internal models world around years egocentric visual experience internal models learned childs visual experience highly generic learning algorithms require strong inductive biases recent advances collecting largescale longitudinal developmentally realistic video datasets generic selfsupervised learning ssl algorithms allowing us begin tackle nature vs nurture question however existing work typically focuses imagebased ssl algorithms visual capabilities learned static images eg object recognition thus ignoring temporal aspects world close gap train selfsupervised video models longitudinal egocentric headcam recordings collected child two year period early development months resulting models highly effective facilitating learning action concepts small number labeled examples favorable data size scaling properties display emergent video interpolation capabilities video models also learn accurate robust object representations imagebased models trained exact data results suggest important temporal aspects childs internal model world may learnable visual experience using highly generic learning algorithms without strong inductive biases,-1,0.0,-1,0.0
sentimentenhanced graphbased sarcasm explanation dialogue sarcasm explanation dialogue sed new yet challenging task aims generate natural language explanation given sarcastic dialogue involves multiple modalities ie utterance video audio although existing studies achieved great success based generative pretrained language model bart overlook exploiting sentiments residing utterance video audio play important roles reflecting sarcasm essentially involves subtle sentiment contrasts nevertheless nontrivial incorporate sentiments boosting sed performance due three main challenges diverse effects utterance tokens sentiments gap videoaudio sentiment signals embedding space bart various relations among utterances utterance sentiments videoaudio sentiments tackle challenges propose novel sentimentenhanced graphbased multimodal sarcasm explanation framework named edge particular first propose lexiconguided utterance sentiment inference module heuristic utterance sentiment refinement strategy devised develop module named joint cross attentionbased sentiment inference jcasi extending multimodal sentiment analysis model jca derive joint sentiment label videoaudio clip thereafter devise contextsentiment graph comprehensively model semantic relations among utterances utterance sentiments videoaudio sentiments facilitate sarcasm explanation generation extensive experiments publicly released dataset wits verify superiority model cuttingedge methods,-1,0.0,-1,0.0
picture worth thousand words exploring diagram videobased oop exercises counter llm overreliance much research highlighted impressive capabilities large language models llms like gpt bard solving introductory programming exercises recent work shown llms effectively solve range complex objectoriented programming oop exercises textbased specifications raises concerns academic integrity students might use models complete assignments unethically neglecting development important skills program design problemsolving computational thinking address propose innovative approach formulating oop tasks using diagrams videos way foster problemsolving deter students copyandprompt approach oop courses introduce novel notation system specifying oop assignments encompassing structural behavioral requirements assess use classroom setting semester student perceptions approach explored survey generally students responded positively diagrams videos videobased projects better received diagrambased exercises notation appears several benefits students investing effort understanding diagrams feeling motivated engage videobased projects furthermore students reported less inclined rely llmbased code generation tools diagram videobased exercises experiments bards vision abilities revealed currently fall short interpreting diagrams generate accurate code solutions,-1,0.0,-1,0.0
rap retrievalaugmented planner adaptive procedure planning instructional videos procedure planning instructional videos entails generating sequence action steps based visual observations initial target states despite rapid progress task remain several critical challenges solved adaptive procedures prior works hold unrealistic assumption number action steps known fixed leading nongeneralizable models realworld scenarios sequence length varies temporal relation understanding step temporal relation knowledge essential producing reasonable executable plans annotation cost annotating instructional videos steplevel labels ie timestamp sequencelevel labels ie action category demanding laborintensive limiting generalizability largescale datasets work propose new practical setting called adaptive procedure planning instructional videos procedure length fixed predetermined address challenges introduce retrievalaugmented planner rap model specifically adaptive procedures rap adaptively determines conclusion actions using autoregressive model architecture temporal relation rap establishes external memory module explicitly retrieve relevant stateaction pairs training videos revises generated procedures tackle high annotation cost rap utilizes weaklysupervised learning manner expand training dataset taskrelevant unannotated videos generating pseudo labels action steps experiments crosstask coin benchmarks show superiority rap traditional fixedlength models establishing strong baseline solution adaptive procedure planning,-1,0.0,-1,0.0
video meetings change expression facial expressions change speak video calls given two unpaired sets videos people seek automatically find spatiotemporal patterns distinctive set existing methods use discriminative approaches perform posthoc explainability analysis methods insufficient unable provide insights beyond obvious dataset biases explanations useful humans good task instead tackle problem lens generative domain translation method generates detailed report learned inputdependent spatiotemporal features extent vary domains demonstrate method discover behavioral differences conversing facetoface videocalls vcs also show applicability method discovering differences presidential communication styles additionally able predict temporal changepoints videos decouple expressions unsupervised way increase interpretability usefulness model finally method generative used transform video call appear recorded setting experiments visualizations show approach able discover range behaviors taking step towards deeper understanding human behaviors,-1,0.0,-1,0.0
popcat propagation particles complex annotation tasks novel dataset creation multiobject tracking crowdcounting industrialbased videos arduous timeconsuming faced unique class densely populates video sequence propose time efficient method called popcat exploits multitarget temporal features video data produce semisupervised pipeline segmentation boxbased video annotation method retains accuracy level associated human level annotation generating large volume semisupervised annotations greater generalization method capitalizes temporal features use particle tracker expand domain humanprovided target points done use particle tracker reassociate initial points set images follow labeled frame yolo model trained generated data rapidly infers target video evaluations conducted animaltrack benchmarks multitarget video trackingdetection sets contain multiple similarlooking targets camera movements features would commonly seen wild situations specifically choose difficult datasets demonstrate efficacy pipeline comparison purposes method applied animaltrack visdrone shows margin improvement best results value metrics collected,-1,0.0,-1,0.0
adapting videolanguage models generalizable robotic rewards via failure prompts generalpurpose robot operate reality executing broad range instructions across various environments imperative central reinforcement learning planning robotic agents generalizable reward function recent advances visionlanguage models clip shown remarkable performance domain deep learning paving way opendomain visual recognition however collecting data robots executing various language instructions across multiple environments remains challenge paper aims transfer videolanguage models robust generalization generalizable languageconditioned reward function utilizing robot video data minimal amount tasks singular environment unlike common robotic datasets used training reward functions human videolanguage datasets rarely contain trivial failure videos enhance models ability distinguish successful failed robot executions cluster failure video features enable model identify patterns within cluster integrate newly trained failure prompt text encoder represent corresponding failure mode languageconditioned reward function shows outstanding generalization new environments new instructions robot planning reinforcement learning,5,0.8814673211035607,5,0.8814673211035607
everything video unifying modalities nextframe prediction multimodal learning involves integrating information various modalities text images audio video pivotal numerous complex tasks like visual question answering crossmodal retrieval caption generation traditional approaches rely modalityspecific encoders late fusion techniques hinder scalability flexibility adapting new tasks modalities address limitations introduce novel framework extends concept task reformulation beyond natural language processing nlp multimodal learning propose reformulate diverse multimodal tasks unified nextframe prediction problem allowing single model handle different modalities without modalityspecific components method treats inputs outputs sequential frames video enabling seamless integration modalities effective knowledge transfer across tasks approach evaluated range tasks including texttotext imagetotext videotovideo videototext audiototext demonstrating models ability generalize across modalities minimal adaptation show task reformulation significantly simplify multimodal model design across various tasks laying groundwork generalized multimodal foundation models,-1,0.0,-1,0.0
bovila bootstrapping videolanguage alignment via llmbased selfquestioning answering development multimodal models rapidly advancing demonstrating remarkable capabilities however annotating videotext pairs remains expensive insufficient take video question answering videoqa tasks example human annotated questions answers often cover part video similar semantics also expressed different text forms leading underutilization video address propose bovila selftraining framework augments question samples training llmbased selfquestioning answering help model exploit video information internal knowledge llms thoroughly improve modality alignment filter bad selfgenerated questions introduce evidential deep learning edl estimate uncertainty assess quality selfgenerated questions evaluating modality alignment within context best knowledge work first explore llmbased selftraining frameworks modality alignment evaluate bovila five strong videoqa benchmarks outperforms several stateoftheart methods demonstrate effectiveness generality additionally provide extensive analyses selftraining framework edlbased uncertainty filtering mechanism code made available httpsgithubcomdunknsabswbovila,0,1.0,0,1.0
followyourclick opendomain regional image animation via short prompts despite recent advances imagetovideo generation better controllability local animation less explored existing imagetovideo methods locally aware tend move entire scene however human artists may need control movement different objects regions additionally current methods require users describe target motion also provide redundant detailed descriptions frame contents two issues hinder practical utilization current tools paper propose practical framework named followyourclick achieve image animation simple user click specifying move short motion prompt specifying move technically propose firstframe masking strategy significantly improves video generation quality motionaugmented module equipped short motion prompt dataset improve short prompt following abilities model control motion speed propose flowbased motion magnitude control control speed target movement precisely framework simpler yet precise user control better generation performance previous methods extensive experiments compared baselines including commercial tools research methods metrics suggest superiority approach project page httpsfollowyourclickgithubio,-1,0.0,-1,0.0
dual conditioned motion diffusion posebased video anomaly detection video anomaly detection vad essential computer vision research existing vad methods utilize either reconstructionbased predictionbased frameworks former excels detecting irregular patterns structures whereas latter capable spotting abnormal deviations trends address posebased video anomaly detection introduce novel framework called dual conditioned motion diffusion dcmd enjoys advantages approaches dcmd integrates conditioned motion conditioned embedding comprehensively utilize pose characteristics latent semantics observed movements respectively reverse diffusion process motion transformer proposed capture potential correlations multilayered characteristics within spectrum space human motion enhance discriminability normal abnormal instances design novel united association discrepancy uad regularization primarily relies gaussian kernelbased time association selfattentionbased global association finally mask completion strategy introduced inference stage reverse diffusion process enhance utilization conditioned motion prediction branch anomaly detection extensive experiments four datasets demonstrate method dramatically outperforms stateoftheart methods exhibits superior generalization performance,-1,0.0,-1,0.0
weaklysupervised pet anomaly detection using implicitlyguided attentionconditional counterfactual diffusion modeling multicenter multicancer multitracer study minimizing need pixellevel annotated data train pet lesion detection segmentation networks highly desired transformative given time cost constraints associated expert annotations current unweaklysupervised anomaly detection methods rely autoencoder generative adversarial networks trained healthy data however ganbased networks challenging train due issues simultaneous optimization two competing networks mode collapse etc paper present weaklysupervised implicitly guided counterfactual diffusion model detecting anomalies pet images igcondapet solution developed validated using pet scans six retrospective cohorts consisting total cases containing local public datasets training conditioned image class labels healthy vs unhealthy via attention modules employ implicit diffusion guidance perform counterfactual generation facilitates unhealthytohealthy domain translation generating synthetic healthy version unhealthy input image enabling detection anomalies calculated differences performance method compared several deep learning based weaklysupervised unsupervised methods well traditional methods like suvmax thresholding also highlight importance incorporating attention modules network detection small anomalies code publicly available httpsgithubcomahxmedsigcondapetgit,-1,0.0,-1,0.0
interactivevideo usercentric controllable video generation synergistic multimodal instructions introduce textitinteractivevideo usercentric framework video generation different traditional generative approaches operate based userprovided images text framework designed dynamic interaction allowing users instruct generative model various intuitive mechanisms whole generation process eg text image prompts painting draganddrop etc propose synergistic multimodal instruction mechanism designed seamlessly integrate users multimodal instructions generative models thus facilitating cooperative responsive interaction user inputs generative process approach enables iterative finegrained refinement generation result precise effective user instructions textitinteractivevideo users given flexibility meticulously tailor key aspects video paint reference image edit semantics adjust video motions requirements fully met code models demo available,15,1.0,15,1.0
generating seamless virtual immunohistochemical whole slide images content color consistency immunohistochemical ihc stains play vital role pathologists analysis medical images providing crucial diagnostic information various diseases virtual staining hematoxylin eosin hestained whole slide images wsis allows automatic production useful ihc stains without expensive physical staining process however current virtual wsi generation methods based tilewise processing often suffer inconsistencies content texture color tile boundaries inconsistencies lead artifacts compromise image quality potentially hinder accurate clinical assessment diagnoses address limitation propose novel consistent wsi synthesis network ccwsinet extends gan models produce seamless synthetic whole slide images ccwsinet integrates content colorconsistency supervisor ensuring consistency across tiles facilitating generation seamless synthetic wsis ensuring immunohistochemistry accuracy melanocyte detection validate method extensive imagequality analyses objective detection assessments subjective survey pathologists generating highquality synthetic wsis method opens doors advanced virtual staining techniques broader applications research clinical care,3,1.0,3,1.0
learning physical properties gaussians via video diffusion recent years rapid development generation models opening new possibilities applications simulating dynamic movements objects customizing behaviors however current generative models tend focus surface features color shape neglecting inherent physical properties govern behavior objects real world accurately simulate physicsaligned dynamics essential predict physical properties materials incorporate behavior prediction process nonetheless predicting diverse materials realworld objects still challenging due complex nature physical attributes paper propose novel method learning various physical properties objects video diffusion model approach involves designing highly generalizable physical simulation system based viscoelastic material model enables us simulate wide range materials highfidelity capabilities moreover distill physical priors video diffusion model contains understanding realistic object materials extensive experiments demonstrate effectiveness method elastic plastic materials shows great potential bridging gap physical world virtual neural space providing better integration application realistic physical principles virtual environments project page,-1,0.0,-1,0.0
magicmirror fast highquality avatar generation constrained search space introduce novel framework human avatar generation personalization leveraging text prompts enhance user engagement customization central approach key innovations aimed overcoming challenges photorealistic avatar synthesis firstly utilize conditional neural radiance fields nerf model trained largescale unannotated multiview dataset create versatile initial solution space accelerates diversifies avatar generation secondly develop geometric prior leveraging capabilities texttoimage diffusion models ensure superior view invariance enable direct optimization avatar geometry foundational ideas complemented optimization pipeline built variational score distillation vsd mitigates texture loss oversaturation issues supported extensive experiments strategies collectively enable creation custom avatars unparalleled visual quality better adherence input text prompts find results videos website httpssyntecresearchgithubiomagicmirror,11,0.9128242234015864,11,0.9128242234015864
temporal spatial super resolution latent diffusion model medical mri images super resolution sr plays critical role computer vision particularly medical imaging hardware acquisition time constraints often result low spatial temporal resolution diffusion models applied spatial temporal sr studies explored use joint spatial temporal sr particularly medical imaging work address gap proposing use latent diffusion model ldm combined vector quantised gan vqganbased encoderdecoder architecture joint super resolution frame sr image denoising problem focusing improving spatial temporal resolution medical images using cardiac mri dataset data science bowl cardiac challenge consisting cine images spatial resolution slices per timestep demonstrate effectiveness approach ldm model achieves peak signal noise ratio psnr structural similarity index ssim learned perceptual image patch similarity lpips outperforming simple baseline method psnr ssim lpips ldm model generates images high fidelity perceptual quality diffusion steps results suggest ldms hold promise advancing super resolution medical imaging potentially enhancing diagnostic accuracy patient outcomes code link also shared,-1,0.0,-1,0.0
ditfastattn attention compression diffusion transformer models diffusion transformers dit excel image video generation face computational challenges due quadratic complexity selfattention operators propose ditfastattn posttraining compression method alleviate computational bottleneck dit identify three key redundancies attention computation dit inference spatial redundancy many attention heads focus local information temporal redundancy high similarity attention outputs neighboring steps conditional redundancy conditional unconditional inferences exhibit significant similarity propose three techniques reduce redundancies window attention residual sharing reduce spatial redundancy attention sharing across timesteps exploit similarity steps attention sharing across cfg skip redundant computations conditional generation apply ditfastattn dit pixartsigma image generation tasks opensora video generation tasks results show image generation method reduces attention flops achieves endtoend speedup highresolution x generation,-1,0.0,-1,0.0
time weaver conditional time series generation model imagine generating citys electricity demand pattern based weather presence electric vehicle location could used capacity planning winter freeze realworld time series often enriched paired heterogeneous contextual metadata weather location etc current approaches time series generation often ignore paired metadata heterogeneity poses several practical challenges adapting existing conditional generation approaches image audio video domains time series domain address gap introduce time weaver novel diffusionbased model leverages heterogeneous metadata form categorical continuous even timevariant variables significantly improve time series generation additionally show naive extensions standard evaluation metrics image time series domain insufficient metrics penalize conditional generation approaches poor specificity reproducing metadataspecific features generated time series thus innovate novel evaluation metric accurately captures specificity conditional generation realism generated time series show time weaver outperforms stateoftheart benchmarks generative adversarial networks gans downstream classification tasks realworld energy medical air quality traffic data sets,-1,0.0,-1,0.0
synthesising handwritten music gans comprehensive evaluation cyclewgan progan dcgan generation handwritten music sheets crucial step toward enhancing optical music recognition omr systems rely large diverse datasets optimal performance however handwritten music sheets often found archives present challenges digitisation due fragility varied handwriting styles image quality paper addresses data scarcity problem applying generative adversarial networks gans synthesise realistic handwritten music sheets provide comprehensive evaluation three gan models dcgan progan cyclewgan comparing ability generate diverse highquality handwritten music images proposed cyclewgan model enhances style transfer training stability significantly outperforms dcgan progan qualitative quantitative evaluations cyclewgan achieves superior performance fid score kid making promising solution improving omr systems,-1,0.0,-1,0.0
diffsal joint audio video learning diffusion saliency prediction audiovisual saliency prediction draw support diverse modality complements performance enhancement still challenged customized architectures well taskspecific loss functions recent studies denoising diffusion models shown promising unifying task frameworks owing inherent ability generalization following motivation novel diffusion architecture generalized audiovisual saliency prediction diffsal proposed work formulates prediction problem conditional generative task saliency map utilizing input audio video conditions based spatiotemporal audiovisual features extra network saliencyunet designed perform multimodal attention modulation progressive refinement groundtruth saliency map noisy map extensive experiments demonstrate proposed diffsal achieve excellent performance across six challenging audiovisual benchmarks average relative improvement previous stateoftheart results six metrics,-1,0.0,-1,0.0
blazebvd make scaletime equalization great blind video deflickering developing blind video deflickering bvd algorithms enhance video temporal consistency gaining importance amid flourish image processing video generation however intricate nature video data complicates training deep learning methods leading high resource consumption instability notably severe lighting flicker underscores critical need compact representation beyond pixel values advance bvd research applications inspired classic scaletime equalization ste work introduces histogramassisted solution called blazebvd highfidelity rapid bvd compared ste directly corrects pixel values temporally smoothing color histograms blazebvd leverages smoothed illumination histograms within ste filtering ease challenge learning temporal data using neural networks technique blazebvd begins condensing pixel values illumination histograms precisely capture flickering local exposure variations histograms smoothed produce singular frames set filtered illumination maps exposure maps resorting deflickering priors blazebvd utilizes network restore faithful consistent texture impacted lighting changes localized exposure issues blazebvd also incorporates lightweight network amend slight temporal inconsistencies avoiding resource consumption issue comprehensive experiments synthetic realworld generated videos showcase superior qualitative quantitative results blazebvd achieving inference speeds faster stateofthearts,-1,0.0,-1,0.0
focusmae gallbladder cancer detection ultrasound videos focused masked autoencoders recent years automated gallbladder cancer gbc detection gained attention researchers current stateoftheart sota methodologies relying ultrasound sonography us images exhibit limited generalization emphasizing need transformative approaches observe individual us frames may lack sufficient information capture disease manifestation study advocates paradigm shift towards videobased gbc detection leveraging inherent advantages spatiotemporal representations employing masked autoencoder mae representation learning address shortcomings conventional imagebased methods propose novel design called focusmae systematically bias selection masking tokens highinformation regions fostering refined representation malignancy additionally contribute extensive us video dataset gbc detection also note first study us videobased gbc detection validate proposed methods curated dataset report new stateoftheart sota accuracy gbc detection problem accuracy current imagebased sota gbcnet radformer videobased sota adamae demonstrate generality proposed focusmae public ctbased covid detection dataset reporting improvement accuracy current baselines source code pretrained models available httpsgbciitdgithubiofocusmae,19,1.0,19,1.0
autotvg new visionlanguage pretraining paradigm temporal video grounding temporal video grounding tvg aims localize moment untrimmed video given language description since annotation tvg laborintensive tvg limited supervision accepted attention recent years great success visionlanguage pretraining guides tvg follow traditional pretraining finetuning paradigm however pretraining process would suffer lack temporal modeling finegrained alignment due difference data nature pretrain test besides large gap pretext downstream tasks makes zeroshot testing impossible pretrained model avoid drawbacks traditional paradigm propose autotvg new visionlanguage pretraining paradigm tvg enables model learn semantic alignment boundary regression automatically annotated untrimmed videos specific autotvg consists novel captioned moment generation cmg module generate captioned moments untrimmed videos tvgnet regression head predict localization results experimental results charadessta activitynet captions show regarding zeroshot temporal video grounding autotvg achieves highly competitive performance indistribution methods outofdistribution testing superior existing pretraining frameworks much less training data,7,0.9378523165984416,7,0.9378523165984416
secure video quality assessment resisting adversarial attacks exponential surge video traffic intensified imperative video quality assessment vqa leveraging cuttingedge architectures current vqa models achieved humancomparable accuracy however recent studies revealed vulnerability existing vqa models adversarial attacks establish reliable practical assessment system secure vqa model capable resisting malicious attacks urgently demanded unfortunately attempt made explore issue paper first attempts investigate general adversarial defense principles aiming endowing existing vqa models security specifically first introduce random spatial grid sampling video frame intraframe defense design pixelwise randomization guardian map globally neutralizing adversarial perturbations meanwhile extract temporal information video sequence compensation interframe defense building upon principles present novel vqa framework securityoriented perspective termed securevqa extensive experiments indicate securevqa sets new benchmark security achieving competitive vqa performance compared stateoftheart models ablation studies delve deeper analyzing principles securevqa demonstrating generalization contributions security leading vqa models,4,1.0,4,1.0
exploring efficient foundational multimodal models video summarization foundational models able generate text outputs given prompt instructions text audio image inputs recently models combined perform tasks video video summarization video foundation models perform pretraining aligning outputs modalityspecific model embedding space embeddings model used within language model finetuned desired instruction set aligning modality pretraining computationally expensive prevents rapid testing different base modality models finetuning evaluation carried within indomain videos hard understand generalizability data efficiency methods alleviate issues propose plugandplay video language model directly uses texts generated input modality language model avoiding pretraining alignment overhead instead finetuning leverage fewshot instruction adaptation strategies compare performance versus computational costs plugandplay style method baseline tuning methods finally explore generalizability method domain shift present insights data useful training data limited analysis present practical insights leverage multimodal foundational models effective results given realistic compute data limitations,-1,0.0,-1,0.0
aligning neuronal coding dynamic visual scenes foundation vision models brains represent everchanging environment neurons highly dynamic fashion temporal features visual pixels dynamic natural scenes entrapped neuronal responses retina crucial establish intrinsic temporal relationship visual pixels neuronal responses recent foundation vision models paved advanced way understanding image pixels yet neuronal coding brain largely lacks deep understanding alignment pixels previous studies employ static images artificial videos derived static images emulating real complicated stimuli despite simple scenarios effectively help separate key factors influencing visual coding complex temporal relationships receive consideration decompose temporal features visual coding natural scenes propose vist spatiotemporal convolutional neural network fed selfsupervised vision transformer vit prior aimed unraveling temporalbased encoding patterns retinal neuronal populations model demonstrates robust predictive performance generalization tests furthermore detailed ablation experiments demonstrate significance temporal module furthermore introduce visual coding evaluation metric designed integrate temporal considerations compare impact different numbers neuronal populations complementary coding conclusion proposed vist demonstrates novel modeling framework neuronal coding dynamic visual scenes brain effectively aligning brain representation video neuronal activity code available httpsgithubcomwuriningvist,-1,0.0,-1,0.0
sora see survey texttovideo generation impressive achievements made artificial intelligence path forward artificial general intelligence sora developed openai capable minutelevel worldsimulative abilities considered milestone developmental path however despite notable successes sora still encounters various obstacles need resolved survey embark perspective disassembling sora texttovideo generation conducting comprehensive review literature trying answer question textitfrom sora see specifically basic preliminaries regarding general algorithms introduced literature categorized three mutually perpendicular dimensions evolutionary generators excellent pursuit realistic panorama subsequently widely used datasets metrics organized detail last importantly identify several challenges open problems domain propose potential future directions research development,10,0.847484149907445,10,0.847484149907445
augmenting efficient realtime surgical instrument segmentation video point tracking segment anything segment anything model sam powerful vision foundation model revolutionizing traditional paradigm segmentation despite reliance prompting frame large computational cost limit usage robotically assisted surgery applications augmented reality guidance require little user intervention along efficient inference usable clinically study address limitations adopting lightweight sam variants meet efficiency requirement employing finetuning techniques enhance generalization surgical scenes recent advancements tracking point tap shown promising results accuracy efficiency particularly points occluded leave field view inspired progress present novel framework combines online point tracker lightweight sam model finetuned surgical instrument segmentation sparse points within region interest tracked used prompt sam throughout video sequence providing temporal consistency quantitative results surpass stateoftheart semisupervised video object segmentation method xmem endovis dataset iou dice method achieves promising performance comparable xmem transformerbased fully supervised segmentation methods ex vivo ucl dvrk vivo datasets addition proposed method shows promising zeroshot generalization ability labelfree stir dataset terms efficiency tested method single geforce rtx gpu respectively achieving fps inference speed code available,-1,0.0,-1,0.0
learning multiple object states actions via large language models recognizing states objects video crucial understanding scene beyond actions objects instance egg raw cracked whisked cooking omelet states coexist simultaneously egg raw whisked however existing research assumes single object state change eg uncracked cracked overlooking coexisting nature multiple object states influence past states current state formulate object state recognition multilabel classification task explicitly handles multiple states propose learn multiple object states narrated videos leveraging large language models llms generate pseudolabels transcribed narrations capturing influence past states challenge narrations mostly describe human actions video rarely explain object states therefore use llms knowledge relationship actions states derive missing object states accumulate derived object states consider past state contexts infer current object state pseudolabels newly collect dataset called multiple object states transition dataset includes manual multilabel annotation evaluation purposes covering object states across six object categories experimental results show model trained llmgenerated pseudolabels significantly outperforms strong visionlanguage models demonstrating effectiveness pseudolabeling framework considers past context via llms,-1,0.0,-1,0.0
dreamwaltzg expressive gaussian avatars skeletonguided diffusion leveraging pretrained diffusion models score distillation sampling sds recent methods shown promising results avatar generation however generating highquality avatars capable expressive animation remains challenging work present dreamwaltzg novel learning framework animatable avatar generation text core framework lies skeletonguided score distillation hybrid gaussian avatar representation specifically proposed skeletonguided score distillation integrates skeleton controls human templates diffusion models enhancing consistency sds supervision terms view human pose facilitates generation highquality avatars mitigating issues multiple faces extra limbs blurring proposed hybrid gaussian avatar representation builds efficient gaussians combining neural implicit fields parameterized meshes enable realtime rendering stable sds optimization expressive animation extensive experiments demonstrate dreamwaltzg highly effective generating animating avatars outperforming existing methods visual quality animation expressiveness framework supports diverse applications including human video reenactment multisubject scene composition,-1,0.0,-1,0.0
dreamstory opendomain story visualization llmguided multisubject consistent diffusion story visualization aims create visually compelling images videos corresponding textual narratives despite recent advances diffusion models yielding promising results existing methods still struggle create coherent sequence subjectconsistent frames based solely story end propose dreamstory automatic opendomain story visualization framework leveraging llms novel multisubject consistent diffusion model dreamstory consists llm acting story director innovative multisubject consistent diffusion model msd generating consistent multisubject across images first dreamstory employs llm generate descriptive prompts subjects scenes aligned story annotating scenes subjects subsequent subjectconsistent generation second dreamstory utilizes detailed subject descriptions create portraits subjects portraits corresponding textual information serving multimodal anchors guidance finally msd uses multimodal anchors generate story scenes consistent multisubject specifically msd includes masked mutual selfattention mmsa masked mutual crossattention mmca modules mmsa mmca modules ensure appearance semantic consistency reference images text respectively modules employ masking mechanisms prevent subject blending validate approach promote progress story visualization established benchmark assess overall performance story visualization framework subjectidentification accuracy consistency generation model extensive experiments validate effectiveness dreamstory subjective objective evaluations please visit project homepage httpsdreamxyzgithubiodreamstory,-1,0.0,-1,0.0
frame interpolation consecutive brownian bridge diffusion recent work video frame interpolation vfi tries formulate vfi diffusionbased conditional image generation problem synthesizing intermediate frame given random noise neighboring frames due relatively high resolution videos latent diffusion models ldms employed conditional generation model autoencoder compresses images latent representations diffusion reconstructs images latent representations formulation poses crucial challenge vfi expects output deterministically equal ground truth intermediate frame ldms randomly generate diverse set different images model runs multiple times reason diverse generation cumulative variance variance accumulated step generation generated latent representations ldms large makes sampling trajectory random resulting diverse rather deterministic generations address problem propose unique solution frame interpolation consecutive brownian bridge diffusion specifically propose consecutive brownian bridge diffusion takes deterministic initial value input resulting much smaller cumulative variance generated latent representations experiments suggest method improve together improvement autoencoder achieve stateoftheart performance vfi leaving strong potential enhancement,2,0.6962709329283329,2,0.6962709329283329
motiondreamer exploring semantic video diffusion features zeroshot mesh animation animation techniques bring digital worlds characters life however manual animation tedious automated techniques often specialized narrow shape classes work propose technique automatic reanimation various shapes based motion prior extracted video diffusion model unlike existing generation methods focus solely motion leverage explicit meshbased representation compatible existing computergraphics pipelines furthermore utilization diffusion features enhances accuracy motion fitting analyze efficacy features animation fitting experimentally validate approach two different diffusion models four animation models finally demonstrate timeefficient zeroshot method achieves superior performance reanimating diverse set shapes compared existing techniques user study project website located httpslukasuzolascommotiondreamer,-1,0.0,-1,0.0
lmmvqa advancing video quality assessment large multimodal models explosive growth videos streaming media platforms underscored urgent need effective video quality assessment vqa algorithms monitor perceptually optimize quality streaming videos however vqa remains extremely challenging task due diverse video content complex spatial temporal distortions thus necessitating advanced methods address issues nowadays large multimodal models lmms exhibited strong capabilities various visual understanding tasks motivating us leverage powerful multimodal representation ability lmms solve vqa task therefore propose first large multimodal video quality assessment lmmvqa model introduces novel spatiotemporal visual modeling strategy qualityaware feature extraction specifically first reformulate quality regression problem question answering qa task construct qa prompts vqa instruction tuning design spatiotemporal vision encoder extract spatial temporal features represent quality characteristics videos subsequently mapped language space spatiotemporal projector modality alignment finally aligned visual tokens qualityinquired text tokens aggregated inputs large language model llm generate quality score level extensive experiments demonstrate lmmvqa achieves stateoftheart performance across five vqa benchmarks exhibiting average improvement generalization ability existing methods furthermore due advanced design spatiotemporal encoder projector lmmvqa also performs exceptionally well general video understanding tasks validating effectiveness code released httpsgithubcomsueqklmmvqa,12,0.6901864339484236,12,0.6901864339484236
improving dynamic object interactions texttovideo generation ai feedback large texttovideo models hold immense potential wide range downstream applications however models struggle accurately depict dynamic object interactions often resulting unrealistic movements frequent violations realworld physics one solution inspired large language models align generated outputs desired outcomes using external feedback enables model refine responses autonomously eliminating extensive manual data collection work investigate use feedback enhance object dynamics texttovideo models aim answer critical question types feedback paired specific selfimprovement algorithms effectively improve textvideo alignment realistic object interactions begin deriving unified probabilistic objective offline rl finetuning texttovideo models perspective highlights design elements existing algorithms like kl regularization policy projection emerge specific choices within unified framework use derived methods optimize set textvideo alignment metrics eg clip scores optical flow notice often fail align human perceptions generation quality address limitation propose leveraging visionlanguage models provide nuanced feedback specifically tailored object dynamics videos experiments demonstrate method effectively optimize wide variety rewards binary ai feedback driving significant improvements video quality dynamic interactions confirmed ai human evaluations notably observe substantial gains using reward signals derived ai feedback particularly scenarios involving complex interactions multiple objects realistic depictions objects falling,-1,0.0,-1,0.0
memoryefficient highresolution oct volume synthesis cascaded amortized latent diffusion models optical coherence tomography oct image analysis plays important role field ophthalmology current successful analysis models rely available large datasets challenging obtained certain tasks use deep generative models create realistic data emerges promising approach however due limitations hardware resources still difficulty synthesize highresolution oct volumes paper introduce cascaded amortized latent diffusion model caldm synthesis highresolution oct volumes memoryefficient way first propose nonholistic autoencoders efficiently build bidirectional mapping highresolution volume space lowresolution latent space tandem autoencoders propose cascaded diffusion processes synthesize highresolution oct volumes globaltolocal refinement process amortizing memory computational demands experiments public highresolution oct dataset show synthetic data realistic highresolution global features surpassing capabilities existing methods moreover performance gains two downstream finegrained segmentation tasks demonstrate benefit proposed method training deep learning models medical imaging tasks code public available,-1,0.0,-1,0.0
noninvasive invasive enhancing ffa synthesis cfp benchmark dataset novel network fundus imaging pivotal tool ophthalmology different imaging modalities characterized specific advantages example fundus fluorescein angiography ffa uniquely provides detailed insights retinal vascular dynamics pathology surpassing color fundus photographs cfp detecting microvascular abnormalities perfusion status however conventional invasive ffa involves discomfort risks due fluorescein dye injection meaningful challenging synthesize ffa images noninvasive cfp previous studies primarily focused ffa synthesis single disease category work explore ffa synthesis multiple diseases devising diffusionguided generative adversarial network introduces adaptive dynamic diffusion forward process discriminator adds categoryaware representation enhancer moreover facilitate research collect first multidisease cfp ffa paired dataset named multidisease paired ocular synthesis mpos dataset four different fundus diseases experimental results show ffa synthesis network generate better ffa images compared stateoftheart methods furthermore introduce pairedmodal diagnostic network validate effectiveness synthetic ffa images diagnosis multiple fundus diseases results show synthesized ffa images real cfp images higher diagnosis accuracy compared ffa synthesizing methods research bridges gap noninvasive imaging ffa thereby offering promising prospects enhance ophthalmic diagnosis patient care focus reducing harm patients noninvasive procedures dataset code released support research field httpsgithubcomwhqxxhffasynthesis,19,1.0,19,1.0
diffusion foundation model bimanual manipulation bimanual manipulation essential robotics yet developing foundation models extremely challenging due inherent complexity coordinating two robot arms leading multimodal action distributions scarcity training data paper present robotics diffusion transformer rdt pioneering diffusion foundation model bimanual manipulation rdt builds diffusion models effectively represent multimodality innovative designs scalable transformer deal heterogeneity multimodal inputs capture nonlinearity high frequency robotic data address data scarcity introduce physically interpretable unified action space unify action representations various robots preserving physical meanings original actions facilitating learning transferrable physical knowledge designs managed pretrain rdt largest collection multirobot datasets date scaled parameters largest diffusionbased foundation model robotic manipulation finally finetuned rdt selfcreated multitask bimanual dataset episodes refine manipulation capabilities experiments real robots demonstrate rdt significantly outperforms existing methods exhibits zeroshot generalization unseen objects scenes understands follows language instructions learns new skills demonstrations effectively handles complex dexterous tasks refer httpsrdtroboticsgithubiordtrobotics code videos,5,1.0,5,1.0
reviewing fid sid metrics generative adversarial networks growth generative adversarial network gan models increased ability image processing provides numerous industries technology produce realistic image transformations however field recently established new evaluation metrics research previous research shown frechet inception distance fid effective metric testing imagetoimage gans realworld applications signed inception distance sid founded metric expands fid allowing unsigned distances paper uses public datasets consist faccades cityscapes maps within cyclegan models training models evaluated inception distance metrics measure generating performance trained models findings indicate usage metric sid incorporates efficient effective metric complement even exceed ability shown using fid imagetoimage gans,17,1.0,17,1.0
generative adversarial network motionblur image restoration everyday life photographs taken camera often suffer motion blur due hand vibrations sudden movements phenomenon significantly detract quality images captured making interesting challenge develop deep learning model utilizes principles adversarial networks restore clarity blurred pixels project focus leveraging generative adversarial networks gans effectively deblur images affected motion blur ganbased tensorflow model defined training evaluating gopro dataset comprises paired street view images featuring clear blurred versions adversarial training process discriminator generator helps produce increasingly realistic images time peak signaltonoise ratio psnr structural similarity index measure ssim two evaluation metrics used provide quantitative measures image quality allowing us evaluate effectiveness deblurring process mean psnr mean ssim average seconds deblurring time achieved project blurry pixels sharper output gan model shows good image restoration effect real world applications,-1,0.0,-1,0.0
echonetsynthetic privacypreserving video generation safe medical data sharing make medical datasets accessible without sharing sensitive patient information introduce novel endtoend approach generative deidentification dynamic medical imaging data generative methods faced constraints terms fidelity spatiotemporal coherence length generation failing capture complete details dataset distributions present model designed produce highfidelity long complete data samples nearrealtime efficiency explore approach challenging task generating echocardiogram videos develop generation method based diffusion models introduce protocol medical video dataset anonymization exemplar present echonetsynthetic fully synthetic privacycompliant echocardiogram dataset paired ejection fraction labels part deidentification protocol evaluate quality generated dataset propose use clinical downstream tasks measurement top widely used potentially biased image quality metrics experimental outcomes demonstrate echonetsynthetic achieves comparable dataset fidelity actual dataset effectively supporting ejection fraction regression task code weights dataset available httpsgithubcomhreynaudechonetsynthetic,-1,0.0,-1,0.0
zerotohero enhancing zeroshot novel view synthesis via attention map filtering generating realistic images arbitrary views based single source image remains significant challenge computer vision broad applications ranging ecommerce immersive virtual experiences recent advancements diffusion models particularly model widely adopted generating plausible views videos models however models still struggle inconsistencies implausibility new views generation especially challenging changes viewpoint work propose zerotohero novel testtime approach enhances view synthesis manipulating attention maps denoising process drawing analogy denoising process stochastic gradient descent sgd implement filtering mechanism aggregates attention maps enhancing generation reliability authenticity process improves geometric consistency without requiring retraining significant computational resources additionally modify selfattention mechanism integrate information source view reducing shape distortions processes supported specialized sampling schedule experimental results demonstrate substantial improvements fidelity consistency validated diverse set outofdistribution objects additionally demonstrate general applicability effectiveness zerotohero multiview image generation conditioned semantic maps pose,-1,0.0,-1,0.0
hicast highly customized arbitrary style transfer adapter enhanced diffusion models goal arbitrary style transfer ast injecting artistic features style reference given imagevideo existing methods usually focus pursuing balance style content whereas ignoring significant demand flexible customized stylization results thereby limiting practical application address critical issue novel ast approach namely hicast proposed capable explicitly customizing stylization results according various source semantic clues specific model constructed based latent diffusion model ldm elaborately designed absorb content style instance conditions ldm characterized introducing textitstyle adapter allows user flexibly manipulate output results aligning multilevel style information intrinsic knowledge ldm lastly extend model perform video ast novel learning objective leveraged video diffusion model training significantly improve crossframe temporal consistency premise maintaining stylization strength qualitative quantitative comparisons well comprehensive user studies demonstrate hicast outperforms existing sota methods generating visually plausible stylization results,-1,0.0,-1,0.0
motion diffusionguided global hmr dynamic camera motion capture technologies transformed numerous fields film gaming industries sports science healthcare providing tool capture analyze human movement great detail holy grail topic monocular global human mesh motion reconstruction ghmr achieve accuracy par traditional multiview capture monocular videos captured dynamic camera inthewild challenging task monocular input inherent depth ambiguity moving camera adds additional complexity rendered human motion product human camera movement accounting confusion existing ghmr methods often output motions unrealistic eg unaccounted root translation human causes foot sliding present diffopt novel global hmr method using diffusion optimization key insight recent advances human motion generation motion diffusion model mdm contain strong prior coherent human motion core method optimize initial motion reconstruction using mdm prior step lead globally coherent human motion optimization jointly optimizes motion prior loss reprojection loss correctly disentangle human camera motions validate diffopt video sequences electromagnetic database global human pose shape wild emdb egobody demonstrate superior global human motion recovery capability stateoftheart global hmr methods prominently long video settings,-1,0.0,-1,0.0
transforming text modality resolution duration via flowbased large diffusion transformers sora unveils potential scaling diffusion transformer generating photorealistic images videos arbitrary resolutions aspect ratios durations yet still lacks sufficient implementation details technical report introduce family series flowbased large diffusion transformers flagdit equipped zeroinitialized attention unified framework designed transform noise images videos multiview objects audio clips conditioned text instructions tokenizing latent spatialtemporal space incorporating learnable placeholders nextline nextframe tokens seamlessly unifies representations different modalities across various spatialtemporal resolutions unified approach enables training within single framework different modalities allows flexible generation multimodal data resolution aspect ratio length inference advanced techniques like rope rmsnorm flow matching enhance stability flexibility scalability flagdit enabling models scale billion parameters extend context window tokens particularly beneficial creating ultrahighdefinition images model long videos model remarkably powered flagdit requires training computational costs naive dit comprehensive analysis underscores preliminary capability resolution extrapolation highresolution editing generating consistent views synthesizing videos seamless transitions expect opensourcing foster creativity transparency diversity generative ai community,-1,0.0,-1,0.0
urcdm ultraresolution image synthesis histopathology diagnosing medical conditions histopathology data requires thorough analysis across various resolutions whole slide images wsi however existing generative methods fail consistently represent hierarchical structure wsis due focus highfidelity patches tackle propose ultraresolution cascaded diffusion models urcdms capable synthesising entire histopathology images high resolutions whilst authentically capturing details underlying anatomy pathology magnification levels evaluate method three separate datasets consisting brain breast kidney tissue surpass existing stateoftheart multiresolution models furthermore expert evaluation study conducted demonstrating urcdms consistently generate outputs across various resolutions trained evaluators distinguish real images code additional examples found github,-1,0.0,-1,0.0
conditional generative models contrastenhanced synthesis maps brain mri contrast enhancement gadoliniumbased contrast agents gbcas vital tool tumor diagnosis neuroradiology based brain mri scans glioblastoma gadolinium administration address enhancement prediction neural networks two new contributions firstly study potential generative models precisely conditional diffusion flow matching uncertainty quantification virtual enhancement secondly examine performance scans quantitive mri versus scans contrast scans scans advantage physically meaningful thereby comparable voxel range compare network prediction performance two modalities incompatible grayvalue scales propose evaluate segmentations contrastenhanced regions interest using dice jaccard scores across models observe better segmentations scans scans,3,0.7169738480114042,3,0.7169738480114042
dialogue director bridging gap dialogue visualization multimodal storytelling recent advances aidriven storytelling enhanced video generation story visualization however translating dialoguecentric scripts coherent storyboards remains significant challenge due limited script detail inadequate physical context understanding complexity integrating cinematic principles address challenges propose dialogue visualization novel task transforms dialogue scripts dynamic multiview storyboards introduce dialogue director trainingfree multimodal framework comprising script director cinematographer storyboard maker framework leverages large multimodal models diffusionbased architectures employing techniques chainofthought reasoning retrievalaugmented generation multiview synthesis improve script understanding physical context comprehension cinematic knowledge integration experimental results demonstrate dialogue director outperforms stateoftheart methods script interpretation physical world understanding cinematic principle application significantly advancing quality controllability dialoguebased story visualization,15,1.0,15,1.0
multimodal latent language modeling nexttoken diffusion multimodal generative models require unified approach handle discrete data eg text code continuous data eg image audio video work propose latent language modeling latentlm seamlessly integrates continuous discrete data using causal transformers specifically employ variational autoencoder vae represent continuous data latent vectors introduce nexttoken diffusion autoregressive generation vectors additionally develop sigmavae address challenges variance collapse crucial autoregressive modeling extensive experiments demonstrate effectiveness latentlm across various modalities image generation latentlm surpasses diffusion transformers performance scalability integrated multimodal large language models latentlm provides generalpurpose interface unifies multimodal generation understanding experimental results show latentlm achieves favorable performance compared transfusion vector quantized models setting scaling training tokens texttospeech synthesis latentlm outperforms stateoftheart valle model speaker similarity robustness requiring fewer decoding steps results establish latentlm highly effective scalable approach advance large multimodal models,-1,0.0,-1,0.0
gomatching simple baseline video text spotting via long short term matching beyond text detection recognition tasks image text spotting video text spotting presents augmented challenge inclusion tracking advanced endtoend trainable methods shown commendable performance pursuit multitask optimization may pose risk producing suboptimal outcomes individual tasks paper identify main bottleneck stateoftheart video text spotter limited recognition capability response issue propose efficiently turn offtheshelf querybased image text spotter specialist video present simple baseline termed gomatching focuses training efforts tracking maintaining strong recognition performance adapt image text spotter video datasets add rescoring head rescore detected instances confidence via efficient tuning leading better tracking candidate pool additionally design longshort term matching module termed lstmatcher enhance spotters tracking capability integrating long shortterm matching results via transformer based simple designs gomatching delivers new records dstext bovtext proposed novel test arbitraryshaped text termed artvideo demonstrates gomatchings capability accommodate general dense small arbitraryshaped chinese english text scenarios saving considerable training budgets,-1,0.0,-1,0.0
uniav unified audiovisual perception multitask video event localization video localization tasks aim temporally locate specific instances videos including temporal action localization tal sound event detection sed audiovisual event localization avel existing methods overspecialize task overlooking fact instances often occur video form complete video content work present uniav unified audiovisual perception network achieve joint learning tal sed avel tasks first time uniav leverage diverse data available taskspecific datasets allowing model learn share mutually beneficial knowledge across tasks modalities tackle challenges posed substantial variations datasets sizedomainduration distinct task characteristics propose uniformly encode visual audio modalities videos derive generic representations also designing taskspecific experts capture unique knowledge task besides develop unified languageaware classifier utilizing pretrained text encoder enabling model flexibly detect various types instances previously unseen ones simply changing prompts inference uniav outperforms singletask counterparts large margin fewer parameters achieving onpar superior performances compared stateoftheart taskspecific methods across activitynet desed benchmarks,-1,0.0,-1,0.0
unmasking illusions understanding human perception audiovisual deepfakes emergence contemporary deepfakes attracted significant attention machine learning research artificial intelligence ai generated synthetic media increases incidence misinterpretation difficult distinguish genuine content currently machine learning techniques extensively studied automatically detecting deepfakes however human perception less explored malicious deepfakes could ultimately cause public social problems humans correctly perceive authenticity content videos watch answer obviously uncertain therefore paper aims evaluate human ability discern deepfake videos subjective study present findings comparing human observers five stateoftheart audiovisual deepfake detection models end used gamification concepts provide participants native english speakers nonnative english speakers webbased platform could access series videos real fake determine authenticity participant performed experiment twice videos different random orders videos manually selected fakeavceleb dataset found ai models performed better humans evaluated videos study also reveals deception impossible humans tend overestimate detection capabilities experimental results may help benchmark human versus machine performance advance forensics analysis enable adaptive countermeasures,4,1.0,4,1.0
madrlbased rate adaptation video streaming multiviewpoint prediction last years video traffic network grown significantly key challenge video playback ensuring high quality experience qoe limited network bandwidth currently studies focus tilebased adaptive bitrate abr streaming based single viewport prediction reduce bandwidth consumption however performance models singleviewpoint prediction severely limited inherent uncertainty head movement cope sudden movement users well paper first presents multimodal spatialtemporal attention transformer generate multiple viewpoint trajectories probabilities given historical trajectory proposed method models viewpoint prediction classification problem uses attention mechanisms capture spatial temporal characteristics input video frames viewpoint trajectories multiviewpoint prediction multiagent deep reinforcement learning madrlbased abr algorithm utilizing multiviewpoint prediction video streaming proposed maximizing different qoe objectives various network conditions formulate abr problem decentralized partially observable markov decision process decpomdp problem present mappo algorithm based centralized training decentralized execution ctde framework solve problem experimental results show proposed method improves defined qoe metric compared existing abr methods,2,1.0,2,1.0
oneshot training video object segmentation video object segmentation vos aims track objects across frames video segment based initial annotated frame target objects previous vos works typically rely fully annotated videos training however acquiring fully annotated training videos vos laborintensive timeconsuming meanwhile selfsupervised vos methods attempted build vos systems correspondence learning label propagation still absence mask priors harms robustness complex scenarios label propagation paradigm makes impractical terms efficiency address issues propose first time general oneshot training framework vos requiring single labeled frame per training video applicable majority stateoftheart vos networks specifically algorithm consists inferring object masks timeforward based initial labeled frame ii reconstructing initial object mask timebackward using masks step bidirectional training satisfactory vos network obtained notably approach extremely simple employed endtoend finally approach uses single labeled frame youtubevos davis datasets achieve comparable results trained fully labeled datasets code released,7,0.9201342459570006,7,0.9201342459570006
many frames useful efficient strategies longform video qa longform videos span across wide temporal intervals highly information redundant contain multiple distinct events entities often loosely related therefore performing longform video question answering lvqa information necessary generate correct response often contained within small subset frames recent literature explore use large language models llms lvqa benchmarks achieving exceptional performance relying vision language models vlms convert visual content within videos natural language vlms often independently caption large number frames uniformly sampled long videos efficient mostly redundant questioning decision choices explore optimal strategies keyframe selection significantly reduce redundancies namely hierarchical keyframe selector proposed framework lvnet achieves stateoftheart performance comparable caption scale across three benchmark lvqa datasets egoschema nextqa intentqa also demonstrating strong performance videos hour long videomme code released publicly code found,-1,0.0,-1,0.0
dabit depth blur informed transformer video focal deblurring many realworld scenarios recorded videos suffer accidental focus blur video deblurring methods exist specifically target motion blur spatialinvariant blur paper introduces framework optimized yet unattempted task video focal deblurring refocusing proposed method employs novel mapguided transformers addition image propagation effectively leverage continuous spatial variance focal blur restore footage also introduce flow refocusing module designed efficiently align relevant features blurry sharp domains additionally propose novel technique generating synthetic focal blur data broadening models learning capabilities robustness include wider array content made new benchmark dataset davisblur available dataset modified extension popular davis video segmentation set provides realistic focal blur degradations well corresponding blur maps comprehensive experiments demonstrate superiority approach achieve stateoftheart results average psnr performance greater comparable existing video restoration methods source code developed databases made available httpsgithubcomcrispianmdabit,-1,0.0,-1,0.0
data overfitting ondevice superresolution dynamic algorithm compiler codesign deep neural networks dnns frequently employed variety computer vision applications nowadays emerging trend current video distribution system take advantage dnns overfitting properties perform video resolution upscaling splitting videos chunks applying superresolution sr model overfit chunk scheme sr models plus video chunks able replace traditional video transmission enhance video quality transmission efficiency however many models chunks needed guarantee high performance leads tremendous overhead model switching memory footprints user end resolve problems propose dynamic deep neural network assisted contentaware data processing pipeline reduce model number one dydca helps promote performance conserving computational resources additionally achieve real acceleration user end designed framework optimizes dynamic features eg dynamic shapes sizes control flow dydca enable series compilation optimizations including fused code generation static execution planning etc employing techniques method achieves better psnr realtime performance fps offtheshelf mobile phone meanwhile assisted compilation optimization achieve speedup saving memory consumption code available,2,1.0,2,1.0
relaxvqa residual fragment layer stack extraction enhancing video quality assessment rapid growth usergenerated content ugc exchanged users sharing platforms need video quality assessment wild increasingly evident ugc typically acquired using consumer devices undergoes multiple rounds compression transcoding reaching end user therefore traditional quality metrics employ original content reference suitable paper propose relaxvqa novel noreference video quality assessment nrvqa model aims address challenges evaluating quality diverse video content without reference original uncompressed videos relaxvqa uses frame differences select spatiotemporal fragments intelligently together different expressions spatial features associated sampled frames used better capture spatial temporal variabilities quality neighbouring frames furthermore model enhances abstraction employing layerstacking techniques deep neural network features residual networks vision transformers extensive testing across four ugc datasets demonstrates relaxvqa consistently outperforms existing nrvqa methods achieving average srcc plcc opensource code trained models facilitate research applications nrvqa found,12,0.9253837143157319,12,0.9253837143157319
vidlpro underlinevideounderlinelanguage underlinepretraining framework underlinerobotic laparoscopic surgery introduce vidlpro novel videolanguage vl pretraining framework designed specifically robotic laparoscopic surgery existing surgical vl models primarily rely contrastive learning propose comprehensive approach capture intricate temporal dynamics align video language vidlpro integrates videotext contrastive learning videotext matching masked language modeling objectives learn rich vl representations support framework present gensurg carefully curated dataset derived gensurgery comprising surgical video clips paired captions generated using transcripts extracted whisper model dataset addresses need largescale highquality vl data surgical domain extensive experiments benchmark datasets including autolaparo demonstrate efficacy approach vidlpro achieves stateoftheart performance zeroshot surgical phase recognition significantly outperforming existing surgical vl models surgvlp hecvl model demonstrates improvements accuracy score setting new benchmark field notably vidlpro exhibits robust performance even singleframe inference effectively scaling increased temporal context ablation studies reveal impact frame sampling strategies model performance computational efficiency results underscore vidlpros potential foundation model surgical video understanding,-1,0.0,-1,0.0
vmid multimodal fusion llm framework detecting identifying misinformation short videos short video platforms become important channels news dissemination offering highly engaging immediate way users access current events share information however platforms also emerged significant conduits rapid spread misinformation fake news rumors leverage visual appeal wide reach short videos circulate extensively among audiences existing fake news detection methods mainly rely singlemodal information text images apply basic fusion techniques limiting ability handle complex multilayered information inherent short videos address limitations paper presents novel fake news detection method based multimodal information designed identify misinformation multilevel analysis video content approach effectively utilizes different modal representations generate unified textual description fed large language model comprehensive evaluation proposed framework successfully integrates multimodal features within videos significantly enhancing accuracy reliability fake news detection experimental results demonstrate proposed approach outperforms existing models terms accuracy robustness utilization multimodal information achieving accuracy significantly higher best baseline model svfend furthermore case studies provide additional evidence effectiveness approach accurately distinguishing fake news debunking content real incidents highlighting reliability robustness realworld applications,4,1.0,4,1.0
videoespresso largescale chainofthought dataset finegrained video reasoning via core frame selection advancement large vision language models lvlms significantly improved multimodal understanding yet challenges remain video reasoning tasks due scarcity highquality largescale datasets existing video questionanswering videoqa datasets often rely costly manual annotations insufficient granularity automatic construction methods redundant framebyframe analysis limiting scalability effectiveness complex reasoning address challenges introduce videoespresso novel dataset features videoqa pairs preserving essential spatial details temporal coherence along multimodal annotations intermediate reasoning steps construction pipeline employs semanticaware method reduce redundancy followed generating qa pairs using develop video chainofthought cot annotations enrich reasoning processes guiding extracting logical relationships qa pairs video content exploit potential highquality videoqa pairs propose hybrid lvlms collaboration framework featuring frame selector twostage instruction finetuned reasoning lvlm framework adaptively selects core frames performs cot reasoning using multimodal evidence evaluated proposed benchmark tasks popular lvlms method outperforms existing baselines tasks demonstrating superior video reasoning capabilities code dataset released httpsgithubcomhshjerryvideoespresso,0,0.9510090888354403,0,0.9510090888354403
realtime anomaly detection video streams thesis part cifre agreement company othello liasd laboratory objective develop artificial intelligence system detect realtime dangers video stream achieve novel approach combining temporal spatial analysis proposed several avenues explored improve anomaly detection integrating object detection human pose detection motion analysis result interpretability techniques commonly used image analysis activation saliency maps extended videos original method proposed proposed architecture performs binary multiclass classification depending whether alert cause needs identified numerous neural networkmodels tested three selected looks yolo used spatial analysis convolutional recurrent neuronal network crnn composed gated recurrent unit gru temporal analysis multilayer perceptron classification models handle different types data combined parallel series although parallel mode faster serial mode generally reliable training models supervised learning chosen two proprietary datasets created first dataset focuses objects may play potential role anomalies second consists videos containing anomalies nonanomalies approach allows processing continuous video streams finite videos providing greater flexibility detection,4,0.7333918486879403,4,0.7333918486879403
videolights feature refinement crosstask alignment transformer joint video highlight detection moment retrieval video highlight detection moment retrieval hdmr essential video analysis recent joint prediction transformer models often overlook crosstask dynamics videotext alignment refinement moreover models typically use limited unidirectional attention mechanisms resulting weakly integrated representations suboptimal performance capturing interdependence video text modalities although largelanguage visionlanguage models llmlvlms gained prominence across various domains application field remains relatively underexplored propose videolights novel hdmr framework addressing limitations convolutional projection feature refinement modules alignment loss better videotext feature alignment ii bidirectional crossmodal fusion network strongly coupled queryaware clip representations iii unidirectional jointtask feedback mechanism enhancing tasks correlation addition iv introduce hard positivenegative losses adaptive error penalization improved learning v leverage lvlms like enhanced multimodal feature integration intelligent pretraining using synthetic data generated lvlms comprehensive experiments qvhighlights tvsum charadessta benchmarks demonstrate stateoftheart performance codes models available,0,0.9033417525022088,0,0.9033417525022088
sweettok semanticaware spatialtemporal tokenizer compact video discretization paper presents textbfsemanticatextbfwartextbfe spatialttextbfemporal textbftokenizer sweettok novel video tokenizer overcome limitations current video tokenization methods compacted yet effective discretization unlike previous approaches process flattened local visual patches via direct discretization adaptive query tokenization sweettok proposes decoupling framework compressing visual inputs distinct spatial temporal queries via textbfdecoupled textbfquery textbfautotextbfencoder dqae design allows sweettok efficiently compress video token count achieving superior fidelity capturing essential information across spatial temporal dimensions furthermore design textbfmotionenhanced textbflanguage textbfcodebook mlc tailored spatial temporal compression address differences semantic representation appearance motion information sweettok significantly improves video reconstruction results wrt rfvd dataset better token compression strategy also boosts downstream video generation results wrt gfvd additionally compressed decoupled tokens imbued semantic information enabling fewshot recognition capabilities powered llms downstream applications,-1,0.0,-1,0.0
blendscape enabling enduser customization videoconferencing environments generative ai todays videoconferencing tools support rich range professional social activities generic meeting environments dynamically adapted align distributed collaborators needs enable enduser customization developed blendscape rendering composition system videoconferencing participants tailor environments meeting context leveraging ai image generation techniques blendscape supports flexible representations task spaces blending users physical digital backgrounds unified environments implements multimodal interaction techniques steer generation exploratory study endusers investigated whether would find value using generative ai customize videoconferencing environments participants envisioned using system like blendscape facilitate collaborative activities future required controls mitigate distracting unrealistic visual elements implemented scenarios demonstrate blendscapes expressiveness supporting environment design strategies prior work propose composition techniques improve quality environments,15,0.9862965903737549,15,0.9862965903737549
actionconditioned video data improves predictability longterm video generation prediction remain challenging tasks computer vision particularly partially observable scenarios cameras mounted moving platforms interaction observed image frames motion recording agent introduces additional complexities address issues introduce actionconditioned video generation acvg framework novel approach investigates relationship actions generated image frames deep dual generatoractor architecture acvg generates video sequences conditioned actions robots enabling exploration analysis vision action mutually influence one another dynamic environments evaluate frameworks effectiveness indoor robot motion dataset consists sequences image frames along sequences actions taken robotic agent conducting comprehensive empirical study comparing acvg stateoftheart frameworks along detailed ablation study,5,0.2540162096164645,5,0.2540162096164645
motioncharacter identitypreserving motion controllable human video generation recent advancements personalized texttovideo generation highlight importance integrating characterspecific identities actions however previous models struggle identity consistency controllable motion dynamics mainly due limited finegrained facial actionbased textual prompts datasets overlook key human attributes actions address challenges propose motioncharacter efficient highfidelity human video generation framework designed identity preservation finegrained motion control introduce idpreserving module maintain identity fidelity allowing flexible attribute modifications integrate idconsistency regionaware loss mechanisms significantly enhancing identity consistency detail fidelity additionally approach incorporates motion control module prioritizes actionrelated text maintaining subject consistency along dataset humanmotion utilizes large language models generate detailed motion descriptions simplify user control inference parameterize motion intensity single coefficient allowing easy adjustments extensive experiments highlight effectiveness motioncharacter demonstrating significant improvements idpreserving highquality video generation,-1,0.0,-1,0.0
unireal universal image generation editing via learning realworld dynamics introduce unireal unified framework designed address various image generation editing tasks existing solutions often vary tasks yet share fundamental principles preserving consistency inputs outputs capturing visual variations inspired recent video generation models effectively balance consistency variation across frames propose unifying approach treats imagelevel tasks discontinuous video generation specifically treat varying numbers input output images frames enabling seamless support tasks image generation editing customization composition etc although designed imagelevel tasks leverage videos scalable source universal supervision unireal learns world dynamics largescale videos demonstrating advanced capability handling shadows reflections pose variation object interaction also exhibiting emergent capability novel applications,9,0.8871375664667852,9,0.8871375664667852
xray sequential representation generation introduce xray novel sequential representation inspired penetrability xray scans xray transforms object series surface frames different layers making suitable generating models images method utilizes ray casting camera center capture geometric textured details including depth normal color across intersected surfaces process efficiently condenses whole object multiframe video format motivating utilize network architecture similar video diffusion models design ensures efficient representation focusing solely surface information also propose twostage pipeline generate objects xray diffusion model upsampler demonstrate practicality adaptability xray representation synthesizing complete visible hidden surfaces object single input image experimental results reveal stateoftheart superiority representation enhancing accuracy generation paving way new representation research practical applications,-1,0.0,-1,0.0
multitask learning minimally invasive surgical vision review minimally invasive surgery mis revolutionized many procedures led reduced recovery time risk patient injury however mis poses additional complexity burden surgical teams datadriven surgical vision algorithms thought key building blocks development future mis systems improved autonomy recent advancements machine learning computer vision led successful applications analyzing videos obtained mis promise alleviating challenges mis videos surgical scene action understanding encompasses multiple related tasks solved individually memoryintensive inefficient fail capture task relationships multitask learning mtl learning paradigm leverages information multiple related tasks improve performance aid generalization well suited finegrained highlevel understanding mis data review provides narrative overview current stateoftheart mtl systems leverage videos obtained mis beyond listing published approaches discuss benefits limitations mtl systems moreover manuscript presents analysis literature various application fields mtl mis including large models highlighting notable trends new directions research developments,-1,0.0,-1,0.0
bounding boxes probabilistic graphical models video anomaly detection simplified study formulate task video anomaly detection probabilistic analysis object bounding boxes hypothesize representation objects via bounding boxes sufficient successfully identify anomalous events scene implied value approach increased object anonymization faster model training fewer computational resources particularly benefit applications within video surveillance running edge devices cameras design model based human reasoning lends explaining model output humanunderstandable terms meanwhile slowest model trains within less seconds generation intel core processor approach constitutes drastic reduction problem feature space comparison prior art show result reduction performance results report highly competitive benchmark datasets cuhk avenue shanghaitech significantly exceed latest stateoftheart results streetscene far proven challenging vad dataset,-1,0.0,-1,0.0
segment videos zeroshot promptable manners introduce preliminary exploration adapting segment anything model sam zeroshot promptable segmentation interprets data series multidirectional videos leverages sam segmentation without training projection framework supports various prompt types including points boxes masks generalize across diverse scenarios objects indoor scenes outdoor environments raw sparse lidar demonstrations multiple datasets eg objaverse scannet kitti highlight robust generalization capabilities best knowledge present faithful implementation sam may serve starting point future research promptable segmentation online demo code,-1,0.0,-1,0.0
actbench towards action controllable world models autonomous driving world models emerged promising neural simulators autonomous driving potential supplement scarce realworld data enable closedloop evaluations however current research primarily evaluates models based visual realism downstream task performance limited focus fidelity specific action instructions crucial property generating targeted simulation scenes although studies address action fidelity evaluations rely closedsource mechanisms limiting reproducibility address gap develop openaccess evaluation framework actbench quantifying action fidelity along baseline world model terra benchmarking framework includes largescale dataset pairing short context videos nuscenes corresponding future trajectory data provides conditional input generating future video frames enables evaluation action fidelity executed motions furthermore terra trained multiple largescale trajectoryannotated datasets enhance action fidelity leveraging framework demonstrate stateoftheart model fully adhere given instructions terra achieves improved action fidelity components benchmark framework made publicly available support future research,-1,0.0,-1,0.0
freestyle free lunch textguided style transfer using diffusion models rapid development generative diffusion models significantly advanced field style transfer however current style transfer methods based diffusion models typically involve slow iterative optimization process eg model finetuning textual inversion style concept paper introduce freestyle innovative style transfer method built upon pretrained large diffusion model requiring optimization besides method enables style transfer text description desired style eliminating necessity style images specifically propose dualstream encoder singlestream decoder architecture replacing conventional unet diffusion models dualstream encoder two distinct branches take content image style text prompt inputs achieving content style decoupling decoder modulate features dual streams based given content image corresponding style text prompt precise style transfer experimental results demonstrate highquality synthesis fidelity method across various content images style text prompts compared stateoftheart methods require training freestyle approach notably reduces computational burden thousands iterations achieving comparable superior performance across multiple evaluation metrics including clip aesthetic score clip score preference released code httpsgithubcomfreestylefreelunchfreestyle,-1,0.0,-1,0.0
rediffinet modeling discrepancies tumor segmentation using diffusion models identification tumor margins essential surgical decisionmaking glioblastoma patients provides reliable assistance neurosurgeons despite improvements deep learning architectures tumor segmentation years creating fully autonomous system suitable clinical floors remains formidable challenge model predictions yet reached desired level accuracy generalizability clinical applications generative modeling techniques seen significant improvements recent times specifically generative adversarial networks gans denoisingdiffusionbased models ddpms used generate higherquality images fewer artifacts finer attributes work introduce framework called rediffinet modeling discrepancy outputs segmentation model like unet ground truth using ddpms explicitly modeling discrepancy results show average improvement dice score crossvalidation compared stateoftheart unet segmentation model,-1,0.0,-1,0.0
tkplanes tiered kplanes high dimensional feature vectors dynamic uavbased scenes paper present new approach bridge domain gap synthetic realworld data unmanned aerial vehicle uavbased perception formulation designed dynamic scenes consisting small moving objects human actions propose extension kplanes neural radiance field nerf wherein algorithm stores set tiered feature vectors tiered feature vectors generated effectively model conceptual information scene well image decoder transforms output feature maps rgb images technique leverages information amongst static dynamic objects within scene able capture salient scene attributes high altitude videos evaluate performance challenging datasets including okutama action observe considerable improvement accuracy state art neural rendering methods,-1,0.0,-1,0.0
taskoriented hierarchical object decomposition visuomotor control good pretrained visual representations could enable robots learn visuomotor policy efficiently still existing representations take onesizefitsalltasks approach comes two important drawbacks completely taskagnostic representations effectively ignore taskirrelevant information scene often lack representational capacity handle unconstrainedcomplex realworld scenes instead propose train large combinatorial family representations organized scene entities objects object parts hierarchical object decomposition taskoriented representations hodor permits selectively assembling different representations specific task scaling representational capacity complexity scene task experiments find hodor outperforms prior pretrained representations scene vector representations objectcentric representations sampleefficient imitation learning across simulated realworld manipulation tasks find invariances captured hodor inherited downstream policies robustly generalize outofdistribution test conditions permitting zeroshot skill chaining appendix code videos,5,1.0,5,1.0
leveraging scene geometry depth information robust image deraining image deraining holds great potential enhancing vision autonomous vehicles rainy conditions contributing safer driving previous works primarily focused employing single network architecture generate derained images however often fail fully exploit rich prior knowledge embedded scenes particularly methods overlook depth information provide valuable context scene geometry guide robust deraining work introduce novel learning framework integrates multiple networks autoencoder deraining auxiliary network incorporate depth information two supervision networks enforce feature consistency rainy clear scenes multinetwork design enables model effectively capture underlying scene structure producing clearer accurately derained images leading improved object detection autonomous vehicles extensive experiments three widelyused datasets demonstrated effectiveness proposed method,1,0.9158901585570781,1,0.9158901585570781
xsvid extremely small video object detection dataset small video object detection svod crucial subfield modern computer vision essential early object discovery detection however existing svod datasets scarce suffer issues insufficiently small objects limited object categories lack scene diversity leading unitary application scenarios corresponding methods address gap develop xsvid dataset comprises aerial data various periods scenes annotates eight major object categories evaluate existing methods detecting extremely small objects xsvid extensively collects three types objects smaller pixel areas extremely small textites relatively small textitrs generally small textitgs xsvid offers unprecedented breadth depth covering quantifying minuscule objects significantly enriching scene object diversity dataset extensive validations xsvid publicly available dataset show existing methods struggle small object detection significantly underperform compared general object detectors leveraging strengths previous methods addressing weaknesses propose yoloft enhances local feature associations integrates temporal motion features significantly improving accuracy stability svod datasets benchmarks available urlhttpsgjhhustgithubioxsvid,-1,0.0,-1,0.0
evaluating texttovisual generation imagetotext generation despite significant progress generative ai comprehensive evaluation remains challenging lack effective metrics standardized benchmarks instance widelyused clipscore measures alignment generated image text prompt fails produce reliable scores complex prompts involving compositions objects attributes relations one reason text encoders clip notoriously act bag words conflating prompts horse eating grass grass eating horse address introduce vqascore uses visualquestionanswering vqa model produce alignment score computing probability yes answer simple figure show text question though simpler prior art vqascore computed offtheshelf models produces stateoftheart results across many imagetext alignment benchmarks also compute vqascore inhouse model follows best practices literature example use bidirectional imagequestion encoder allows image embeddings depend question asked vice versa inhouse model outperforms even strongest baselines make use proprietary interestingly although train images vqascore also align text video models vqascore allows researchers benchmark texttovisual generation using complex texts capture compositional structure realworld prompts introduce genaibench challenging benchmark compositional text prompts require parsing scenes objects attributes relationships highorder reasoning like comparison logic genaibench also offers human ratings leading image video generation models stable diffusion dalle,0,0.8590228702741333,0,0.8590228702741333
multiscale latent diffusion model enhanced feature extraction medical images various imaging modalities used patient diagnosis offering unique advantages valuable insights anatomy pathology computed tomography ct crucial diagnostics providing highresolution images precise internal organ visualization cts ability detect subtle tissue variations vital diagnosing diseases like lung cancer enabling early detection accurate tumor assessment however variations ct scanner models acquisition protocols introduce significant variability extracted radiomic features even imaging patient variability poses considerable challenges downstream research clinical analysis depend consistent reliable feature extraction current methods medical image feature extraction often based supervised learning approaches including ganbased models face limitations generalizing across different imaging environments response challenges propose ltdiff multiscale latent diffusion model designed enhance feature extraction medical imaging model addresses variability standardizing nonuniform distributions latent space improving feature consistency ltdiff utilizes unet encoderdecoder architecture coupled conditional denoising diffusion probabilistic model ddpm latent bottleneck achieve robust feature extraction standardization extensive empirical evaluations patient phantom ct datasets demonstrate significant improvements image standardization higher concordance correlation coefficients ccc across multiple radiomic feature categories advancements ltdiff represents promising solution overcoming inherent variability medical imaging data offering improved reliability accuracy feature extraction processes,3,0.7222833352783923,3,0.7222833352783923
cameracontrolled imagetovideo diffusion model recent advancements integrated camera pose userfriendly physicsinformed condition video diffusion models enabling precise camera control paper identify one key challenges effectively modeling noisy crossframe interactions enhance geometry consistency camera controllability innovatively associate quality condition ability reduce uncertainty interpret noisy crossframe features form noisy condition recognizing noisy conditions provide deterministic information also introducing randomness potential misguidance due added noise propose applying epipolar attention aggregate features along corresponding epipolar lines thereby accessing optimal amount noisy conditions additionally address scenarios epipolar lines disappear commonly caused rapid camera movements dynamic objects occlusions ensuring robust performance diverse environments furthermore develop robust reproducible evaluation pipeline address inaccuracies instabilities existing camera control metrics method achieves improvement camera controllability dataset without compromising dynamics generation quality demonstrates strong generalization outofdomain images training inference require memory respectively sequences resolution release checkpoints along training evaluation code dynamic videos best viewed,-1,0.0,-1,0.0
estimating egobody pose doubly sparse egocentric video data study problem estimating body movements camera wearer egocentric videos current methods egobody pose estimation rely temporally dense sensor data imu measurements spatially sparse body parts like head hands however propose even temporally sparse observations hand poses captured intermittently egocentric videos natural periodic hand movements effectively constrain overall body motion naively applying diffusion models generate fullbody pose head pose sparse hand pose leads suboptimal results overcome develop twostage approach decomposes problem temporal completion spatial completion first method employs masked autoencoders impute hand trajectories leveraging spatiotemporal correlations head pose sequence intermittent hand poses providing uncertainty estimates subsequently employ conditional diffusion models generate plausible fullbody motions based temporally dense trajectories head hands guided uncertainty estimates imputation effectiveness method rigorously tested validated comprehensive experiments conducted various hmd setup amass datasets,1,1.0,1,1.0
frequencyaware guidance blind image restoration via diffusion models blind image restoration remains significant challenge lowlevel vision tasks recently denoising diffusion models shown remarkable performance image synthesis guided diffusion models leveraging potent generative priors pretrained models along differential guidance loss achieved promising results blind image restoration however models typically consider data consistency solely spatial domain often resulting distorted image content paper propose novel frequencyaware guidance loss integrated various diffusion models plugandplay manner proposed guidance loss based discrete wavelet transform simultaneously enforces content consistency spatial frequency domains experimental results demonstrate effectiveness method three blind restoration tasks blind image deblurring imaging turbulence blind restoration multiple degradations notably method achieves significant improvement psnr score remarkable enhancement image deblurring moreover method exhibits superior capability generating images rich details reduced distortion leading best visual quality,-1,0.0,-1,0.0
guys want dance zeroshot compositional human dance generation multiple persons human dance generation hdg aims synthesize realistic videos images sequences driving poses despite great success existing methods limited generating videos single person specific backgrounds generalizability realworld scenarios multiple persons complex backgrounds remains unclear systematically measure generalizability hdg models introduce new task dataset evaluation protocol compositional human dance generation chdg evaluating stateoftheart methods chdg empirically find fail generalize realworld scenarios tackle issue propose novel zeroshot framework dubbed multidancezero synthesize videos consistent arbitrary multiple persons background precisely following driving poses specifically contrast straightforward ddim nulltext inversion first present poseaware inversion method obtain noisy latent code initialization text embeddings accurately reconstruct composed reference image since directly generating videos lead severe appearance inconsistency propose compositional augmentation strategy generate augmented images utilize optimize set generalizable text embeddings addition consistencyguided sampling elaborated encourage background keypoints estimated clean image reverse step close reference image improving temporal consistency generated videos extensive qualitative quantitative results demonstrate effectiveness superiority approach,-1,0.0,-1,0.0
vexpress conditional dropout progressive training portrait video generation field portrait video generation use single images generate portrait videos become increasingly prevalent common approach involves leveraging generative models enhance adapters controlled generation however control signals eg text audio reference image pose depth map etc vary strength among weaker conditions often struggle effective due interference stronger conditions posing challenge balancing conditions work portrait video generation identified audio signals particularly weak often overshadowed stronger signals facial pose reference image however direct training weak signals often leads difficulties convergence address propose vexpress simple method balances different control signals progressive training conditional dropout operation method gradually enables effective control weak conditions thereby achieving generation capabilities simultaneously take account facial pose reference image audio experimental results demonstrate method effectively generate portrait videos controlled audio furthermore potential solution provided simultaneous effective use conditions varying strengths,9,0.6845118564374378,9,0.6845118564374378
image inpainting corrupted images using semisuper resolution gan image inpainting valuable technique enhancing images corrupted primary challenge research revolves around extent corruption input image deep learning model must restore address challenge introduce generative adversarial network gan learning replicating missing pixels additionally developed distinct variant superresolution gan srgan refer semisrgan ssrgan furthermore leveraged three diverse datasets assess robustness accuracy proposed model training process involves varying levels pixel corruption attain optimal accuracy generate highquality images,-1,0.0,-1,0.0
multiresolution guided gans medical image translation medical image translation process converting one imaging modality another order reduce need multiple image acquisitions patient enhance efficiency treatment reducing time equipment labor needed paper introduce multiresolution guided generative adversarial network ganbased framework medical image translation framework uses multiresolution denseattention unet generator multiresolution unet discriminator optimized unique combination loss functions including voxelwise gan loss perception loss approach yields promising results volumetric image quality assessment iqa across variety imaging modalities body regions age groups demonstrating robustness furthermore propose synthetictoreal applicability assessment additional evaluation assess effectiveness synthetic data downstream applications segmentation comprehensive evaluation shows method produces synthetic medical images highquality also potentially useful clinical applications code available,3,0.9013540667001076,3,0.9013540667001076
sora incredible scary emerging governance challenges texttovideo generative ai models texttovideo generative ai models sora openai potential disrupt multiple industries paper report qualitative social media analysis aiming uncover peoples perceived impact concerns soras integration collected analyzed comments popular posts soragenerated videos comparison sora videos midjourney images artists complaints copyright infringement generative ai found people concerned soras impact content creationrelated industries emerging governance challenges included forprofit nature openai blurred boundaries real fake content human autonomy data privacy copyright issues environmental impact potential regulatory solutions proposed people included lawenforced labeling ai content ai literacy education public based findings discuss importance gauging peoples tech perceptions early propose policy recommendations regulate sora public release,10,0.8286298891490861,10,0.8286298891490861
improving unsupervised video object segmentation via fake flow generation unsupervised video object segmentation vos also known video salient object detection aims detect prominent object video pixel level recently twostream approaches leverage rgb images optical flow maps gained significant attention however limited amount training data remains substantial challenge study propose novel data generation method simulates fake optical flows single images thereby creating largescale training data stable network learning inspired observation optical flow maps highly dependent depth maps generate fake optical flows refining augmenting estimated depth maps image incorporating simulated imageflow pairs achieve new stateoftheart performance public benchmark datasets without relying complex modules believe data generation method represents potential breakthrough future vos research,14,0.8737856974900235,14,0.8737856974900235
alignment need trainingfree augmentation strategy poseguided video generation character animation transformative field computer graphics vision enabling dynamic realistic video animations static images despite advancements maintaining appearance consistency animations remains challenge approach addresses introducing trainingfree framework ensures generated video sequence preserves reference images subtleties physique proportions dual alignment strategy decouple skeletal motion priors pose information enabling precise control animation generation method also improves pixellevel alignment conditional control reference character enhancing temporal consistency visual cohesion animations method significantly enhances quality video generation without need large datasets expensive computational resources,1,1.0,1,1.0
generative videolanguageaction model webscale knowledge robot manipulation present stateoftheart generalist robot agent versatile generalizable robot manipulation first pretrained vast number internet videos capture dynamics world largescale pretraining involving million video clips billion tokens equips ability generalize across wide range robotic tasks environments subsequent policy learning following finetuned video generation action prediction using robot trajectories exhibits impressive multitask learning capabilities achieving average success rate across tasks moreover demonstrates exceptional generalization new previously unseen scenarios including novel backgrounds environments objects tasks notably scales effectively model size underscoring potential continued growth application project page,5,0.8100377752228362,5,0.8100377752228362
allegro open black box commerciallevel video generation model significant advancements made field video generation opensource community contributing wealth research papers tools training highquality models however despite efforts available information resources remain insufficient achieving commerciallevel performance report open black box introduce textbfallegro advanced video generation model excels quality temporal consistency also highlight current limitations field present comprehensive methodology training highperformance commerciallevel video generation models addressing key aspects data model architecture training pipeline evaluation user study shows allegro surpasses existing opensource models commercial models ranking behind hailuo kling code httpsgithubcomrhymesaiallegro model httpshuggingfacecorhymesaiallegro gallery,-1,0.0,-1,0.0
nerfad neural radiance field attentionbased disentanglement talking face synthesis talking face synthesis driven audio one current research hotspots fields multidimensional signal processing multimedia neural radiance field nerf recently brought research field order enhance realism effect generated faces however existing nerfbased methods either burden nerf complex learning tasks lacking methods supervised multimodal feature fusion precisely map audio facial region related speech movements reasons ultimately result existing methods generating inaccurate lip shapes paper moves portion nerf learning tasks ahead proposes talking face synthesis method via nerf attentionbased disentanglement nerfad particular attentionbased disentanglement module introduced disentangle face audioface identityface using speechrelated facial action unit au information precisely regulate audio affects talking face fuse audioface audio feature addition au information also utilized supervise fusion two modalities extensive qualitative quantitative experiments demonstrate nerfad outperforms stateoftheart methods generating realistic talking face videos including image quality lip synchronization view video results please refer,6,1.0,6,1.0
highresolution talking head generation emotion style art style although automatically animating audiodriven talking heads recently received growing interest previous efforts mainly concentrated achieving lip synchronization audio neglecting two crucial elements generating expressive videos emotion style art style paper present innovative audiodriven talking face generation method called involves two stylized stages namely stylee stylea integrate textcontrolled emotion style picturecontrolled art style final output order prepare scarce emotional text descriptions corresponding videos propose laborfree paradigm employs largescale pretrained models automatically annotate emotional text labels existing audiovisual datasets incorporating synthetic emotion texts stylee stage utilizes largescale clip model extract emotion representations combined audio serving condition efficient latent diffusion model designed produce emotional motion coefficients model moving stylea stage develop coefficientdriven motion generator artspecific style path embedded wellknown stylegan allows us synthesize highresolution artistically stylized talking head videos using generated emotional motion coefficients art style source picture moreover better preserve image details avoid artifacts provide stylegan multiscale content features extracted identity image refine intermediate feature maps designed content encoder refinement network respectively extensive experimental results demonstrate method outperforms existing stateoftheart methods terms audiolip synchronization performance emotion style art style,6,0.9350883106845811,6,0.9350883106845811
mustan multiscale temporal context attention robust video foreground segmentation video foreground segmentation vfs important computer vision task wherein one aims segment objects motion background current methods imagebased ie rely spatial cues ignoring motion cues therefore tend overfit training data dont generalize well outofdomain ood distribution solve problem prior works exploited several cues optical flow background subtraction mask etc however video data annotations like optical flow challenging task paper utilize temporal information spatial cues video data improve ood performance however challenge lies model temporal information given video data interpretable way creates noticeable difference therefore devise strategy integrates temporal context video development vfs approach give rise deep learning architectures namely based idea multiscale temporal context attention ie aids models learn better representations beneficial vfs introduce new video dataset namely indoor surveillance dataset isd vfs multiple annotations frame level foreground binary mask depth map instance semantic annotations therefore isd benefit computer vision tasks validate efficacy architectures compare performance baselines demonstrate proposed methods significantly outperform benchmark methods ood addition performance significantly improved certain video categories ood data due isd,-1,0.0,-1,0.0
mvad multiple visual artifact detector video streaming visual artifacts often introduced streamed video content due prevailing conditions content production delivery since degrade quality users experience important automatically accurately detect order enable effective quality measurement enhancement existing detection methods often focus single type artifact andor determine presence artifact thresholding objective quality indices approaches reported offer inconsistent prediction performance also impractical realworld applications multiple artifacts coexist interact paper propose multiple visual artifact detector mvad video streaming first time able detect multiple artifacts using single framework reliant video quality assessment models approach employs new artifactaware dynamic feature extractor adfe obtain artifactrelevant spatial features within frame multiple artifact types extracted features processed recurrent memory vision transformer rmvit module captures shortterm longterm temporal information within input video proposed network architecture optimized endtoend manner based new large diverse training database generated simulating video streaming pipeline based adversarial data augmentation model evaluated two video artifact databases maxwell bviartifact achieves consistent improved prediction results ten target visual artifacts compared seven existing single multiple artifact detectors source code training database available httpschenfengbristolgithubiomvad,12,0.3662250808494549,12,0.3662250808494549
scenecraft llm agent synthesizing scene blender code paper introduces scenecraft large language model llm agent converting text descriptions blenderexecutable python scripts render complex scenes hundred assets process requires complex spatial planning arrangement tackle challenges combination advanced abstraction strategic planning library learning scenecraft first models scene graph blueprint detailing spatial relationships among assets scene scenecraft writes python scripts based graph translating relationships numerical constraints asset layout next scenecraft leverages perceptual strengths visionlanguage foundation models like gptv analyze rendered images iteratively refine scene top process scenecraft features library learning mechanism compiles common script functions reusable library facilitating continuous selfimprovement without expensive llm parameter tuning evaluation demonstrates scenecraft surpasses existing llmbased agents rendering complex scenes shown adherence constraints favorable human assessments also showcase broader application potential scenecraft reconstructing detailed scenes sintel movie guiding video generative model generated scenes intermediary control signal,-1,0.0,-1,0.0
spatialfrequency dualdomain feature fusion network lowlight remote sensing image enhancement lowlight remote sensing images generally feature high resolution high spatial complexity continuously distributed surface features space continuity scenes leads extensive longrange correlations spatial domains within remote sensing images convolutional neural networks rely local correlations longdistance modeling struggle establish longrange correlations images hand transformerbased methods focus global information face high computational complexities processing highresolution remote sensing images another perspective fourier transform compute global information without introducing large number parameters enabling network efficiently capture overall image structure establish longrange correlations therefore propose dualdomain feature fusion network dffn lowlight remote sensing image enhancement specifically challenging task lowlight enhancement divided two manageable subtasks first phase learns amplitude information restore image brightness second phase learns phase information refine details facilitate information exchange two phases designed information fusion affine block combines data different phases scales additionally constructed two dark light remote sensing datasets address current lack datasets dark light remote sensing image enhancement extensive evaluations show method outperforms existing stateoftheart methods code available httpsgithubcomiijjlkdffn,-1,0.0,-1,0.0
phased consistency models consistency models cms made significant progress accelerating generation diffusion models however application highresolution textconditioned image generation latent space remains unsatisfactory paper identify three key flaws current design latent consistency models lcms investigate reasons behind limitations propose phased consistency models pcms generalize design space address identified limitations evaluations demonstrate pcms outperform lcms across step generation settings pcms specifically designed multistep refinement achieve comparable generation results previously stateoftheart specifically designed methods furthermore show methodology pcms versatile applicable video generation enabling us train stateoftheart fewstep texttovideo generator code available httpsgithubcomgunphasedconsistencymodel,-1,0.0,-1,0.0
harmonizing pixels melodies maestroguided film score generation composition style transfer introduce film score generation framework harmonize visual pixels music melodies utilizing latent diffusion model framework processes film clips input generates music aligns general theme offering capability tailor outputs specific composition style model directly produces music video utilizing streamlined efficient tuning mechanism controlnet also integrates film encoder adept understanding films semantic depth emotional impact aesthetic appeal additionally introduce novel effective yet straightforward evaluation metric evaluate originality recognizability music within film scores fill gap film scores curate comprehensive dataset film videos legendary original scores injecting domainspecific knowledge datadriven generation model model outperforms existing methodologies creating film scores capable generating music reflects guidance maestros style thereby redefining benchmark automated film scores laying robust groundwork future research domain code generated samples available,8,0.5058523326771341,8,0.5058523326771341
gait sequence upsampling using diffusion models single lidar sensors recently lidar emerged promising technique field gaitbased person identification serving alternative traditional rgb cameras due robustness varying lighting conditions ability capture geometric information however long capture distances use lowcost lidar sensors often result sparse human point clouds leading decline identification performance address challenges propose sparsetodense upsampling model pedestrian point clouds lidarbased gait recognition named lidargsu designed improve generalization capability existing identification models method utilizes diffusion probabilistic models dpms shown high fidelity generative tasks image completion work leverage dpms sparse sequential pedestrian point clouds conditional masks videotovideo translation approach applied inpainting manner conducted extensive experiments dataset evaluate generative quality recognition performance proposed method furthermore demonstrate applicability upsampling model using realworld dataset captured lowresolution sensor across varying measurement distances,-1,0.0,-1,0.0
soar selfoccluded avatar recovery single video wild selfocclusion common capturing people wild performer follow predefined motion scripts challenges existing monocular human reconstruction systems assume full body visibility introduce selfoccluded avatar recovery soar method complete human reconstruction partial observations parts body entirely unobserved soar leverages structural normal prior generative diffusion prior address illposed reconstruction problem structural normal prior model human reposable surfel model welldefined easily readable shapes generative diffusion prior perform initial reconstruction refine using score distillation various benchmarks show soar performs favorably stateoftheart reconstruction generation methods onpar comparing concurrent works additional video results code available httpssoaravatargithubio,-1,0.0,-1,0.0
coherent temporal synthesis incremental action segmentation data replay successful incremental learning technique images prevents catastrophic forgetting keeping reservoir previous data original synthesized ensure model retains past knowledge adapting novel concepts however application video domain rudimentary simply stores frame exemplars action recognition paper presents first exploration video data replay techniques incremental action segmentation focusing action temporal modeling propose temporally coherent action tca model represents actions using generative model instead storing individual frames integration conditioning variable captures temporal coherence allows model understand evolution action features time therefore action segments generated tca replay diverse temporally coherent incremental setup breakfast dataset approach achieves significant increases accuracy compared baselines,-1,0.0,-1,0.0
inclusion global multimedia deepfake detection towards multidimensional facial forgery detection paper present global multimedia deepfake detection held concurrently inclusion multimedia deepfake detection aims detect automatic image audiovideo manipulations including limited editing synthesis generation photoshopetc challenge attracted teams world valid result submission counts invite top teams present solutions challenge top teams awarded prizes grand finale paper present solutions top teams two tracks boost research work field image audiovideo forgery detection methodologies developed challenge contribute development nextgeneration deepfake detection systems encourage participants open source methods,4,0.9983903891486327,4,0.9983903891486327
vision audio beyond unified model audiovisual representation generation video encompasses visual auditory data creating perceptually rich experience two modalities complement videos valuable type media investigation interplay audio visual elements previous studies audiovisual modalities primarily focused either audiovisual representation learning generative modeling modality conditioned creating disconnect two branches unified framework learns representation generates modalities developed yet work introduce novel framework called vision audio beyond vab bridge gap audiovisual representation learning visiontoaudio generation key approach vab rather working raw video frames audio data vab performs representation learning generative modeling within latent spaces particular vab uses pretrained audio tokenizer image encoder obtain audio tokens visual features respectively performs pretraining task visualconditioned masked audio token prediction training strategy enables model engage contextual learning simultaneous videotoaudio generation pretraining phase vab employs iterativedecoding approach rapidly generate audio tokens conditioned visual features since vab unified model backbone finetuned various audiovisual downstream tasks experiments showcase efficiency vab producing highquality audio video capability acquire semantic audiovisual features leading competitive results audiovisual retrieval classification,8,0.6103882579040872,8,0.6103882579040872
faster diffusion action segmentation temporal action segmentation tas essential task video analysis aiming segment classify continuous frames distinct action segments however ambiguous boundaries actions pose significant challenge highprecision segmentation recent advances diffusion models demonstrated substantial success tas tasks due stable training process highquality generation capabilities however heavy sampling steps required diffusion models pose substantial computational burden limiting practicality realtime applications additionally related works utilize transformerbased encoder architectures although architectures excel capturing longrange dependencies incur high computational costs face featuresmoothing issues processing long video sequences address challenges propose effidiffact efficient highperformance tas algorithm specifically develop lightweight temporal feature encoder reduces computational overhead mitigates rank collapse phenomenon associated traditional selfattention mechanisms furthermore introduce adaptive skip strategy allows dynamic adjustment timestep lengths based computed similarity metrics inference thereby enhancing computational efficiency comprehensive experiments breakfast gtea datasets demonstrated effectiveness proposed algorithm,-1,0.0,-1,0.0
taqdit timeaware quantization diffusion transformers transformerbased diffusion models dubbed diffusion transformers dits achieved stateoftheart performance image video generation tasks however large model size slow inference speed limit practical applications calling model compression methods quantization unfortunately existing dit quantization methods overlook impact reconstruction varying quantization sensitivities across different layers hinder achievable performance tackle issues propose innovative timeaware quantization dits taqdit specifically observe nonconvergence issue reconstructing weights activations separately quantization introduce joint reconstruction method resolve problem discover postgelu activations particularly sensitive quantization due significant variability across different denoising steps well extreme asymmetries variations within step address propose timevarianceaware transformations facilitate effective quantization experimental results show quantizing dits weights activations method significantly surpasses previous quantization methods,-1,0.0,-1,0.0
shapepreserving generation food images automatic dietary assessment traditional dietary assessment methods heavily rely selfreporting timeconsuming prone bias recent advancements artificial intelligence ai revealed new possibilities dietary assessment particularly analysis food images recognizing foods estimating food volumes images known key procedures automatic dietary assessment however procedures required large amounts training images labeled food names volumes currently unavailable alternatively recent studies indicated training images artificially generated using generative adversarial networks gans nonetheless convenient generation large amounts food images known volumes remain challenge existing techniques work present simple ganbased neural network architecture conditional food image generation shapes food container generated images closely resemble reference input image experiments demonstrate realism generated images shapepreserving capabilities proposed framework,-1,0.0,-1,0.0
cospeech facial animation generation multimodality guidance synthesis facial animations speech garnered considerable attention due scarcity highquality facial data wellannotated abundant multimodality labels previous methods often suffer limited realism lack lexible conditioning address challenge trilogy first introduce generalized neural parametric facial asset gnpfa efficient variational autoencoder mapping facial geometry images highly generalized expression latent space decoupling expressions identities utilize gnpfa extract highquality expressions accurate head poses large array videos presents dataset large diverse scanlevel cospeech facial animation dataset wellannotated emotional style labels finally propose diffusion model gnpfa latent space cospeech facial animation generation accepting rich multimodality guidances audio text image extensive experiments demonstrate model achieves high fidelity facial animation synthesis also broadens scope expressiveness style adaptability facial animation,6,0.8144182647668694,6,0.8144182647668694
videomv consistent multiview generation based large video generative model generating multiview images based text singleimage prompts critical capability creation content two fundamental questions topic data use training ensure multiview consistency paper introduces novel framework makes fundamental contributions questions unlike leveraging images diffusion models training propose dense consistent multiview generation model finetuned offtheshelf video generative models images video generative models suitable multiview generation underlying network architecture generates employs temporal module enforce frame consistency moreover video data sets used train models abundant diverse leading reduced trainfinetuning domain gap enhance multiview consistency introduce denoising sampling first employs feedforward reconstruction module get explicit global model adopts sampling strategy effectively involves images rendered global model denoising sampling loop improve multiview consistency final images byproduct module also provides fast way create assets represented gaussians within seconds approach generate dense views converges much faster training stateoftheart approaches gpu hours versus many thousand gpu hours comparable visual quality consistency finetuning approach outperforms existing stateoftheart methods quantitative metrics visual effects project page,-1,0.0,-1,0.0
humanactivity agv quality assessment benchmark dataset objective evaluation metric aidriven video generation techniques made significant progress recent years however aigenerated videos agvs involving human activities often exhibit substantial visual semantic distortions hindering practical application video generation technologies realworld scenarios address challenge conduct pioneering study human activity agv quality assessment focusing visual quality evaluation identification semantic distortions first construct aigenerated human activity video quality assessment humanagvqa dataset consisting agvs derived popular texttovideo models using text prompts describe diverse human activities conduct subjective study evaluate human appearance quality action continuity quality overall video quality agvs identify semantic issues human body parts based humanagvqa benchmark performance models analyze strengths weaknesses generating different categories human activities second develop objective evaluation metric named aigenerated human activity video quality metric ghvq automatically analyze quality human activity agvs ghvq systematically extracts humanfocused quality features aigenerated contentaware quality features temporal continuity features making comprehensive explainable quality metric human activity agvs extensive experimental results show ghvq outperforms existing quality metrics humanagvqa dataset large margin demonstrating efficacy assessing quality human activity agvs humanagvqa dataset ghvq metric released public httpsgithubcomzczhangsjtughvqgit,12,1.0,12,1.0
ddpm based xray image synthesizer access highquality datasets medical industry limits machine learning model performance address issue propose denoising diffusion probabilistic model ddpm combined unet architecture xray image synthesis focused pneumonia medical condition methodology employs pneumonia xray images obtained kaggle training results demonstrate effectiveness approach model successfully generated realistic images low mean squared error mse synthesized images showed distinct differences nonpneumonia images highlighting models ability capture key features positive cases beyond pneumonia applications synthesizer extend various medical conditions provided ample dataset available capability produce highquality images potentially enhance machine learning models performance aiding accurate efficient medical diagnoses innovative ddpmbased xray photo synthesizer presents promising avenue addressing scarcity positive medical image datasets paving way improved medical image analysis diagnosis healthcare industry,3,0.5797911238413899,3,0.5797911238413899
aniportrait audiodriven synthesis photorealistic portrait animation study propose aniportrait novel framework generating highquality animation driven audio reference portrait image methodology divided two stages initially extract intermediate representations audio project sequence facial landmarks subsequently employ robust diffusion model coupled motion module convert landmark sequence photorealistic temporally consistent portrait animation experimental results demonstrate superiority aniportrait terms facial naturalness pose diversity visual quality thereby offering enhanced perceptual experience moreover methodology exhibits considerable potential terms flexibility controllability effectively applied areas facial motion editing face reenactment release code model weights httpsgithubcomscutzzjaniportrait,-1,0.0,-1,0.0
high quality human image animation using regional supervision motion blur condition recent advances video diffusion models enabled realistic controllable human image animation temporal coherence although generating reasonable results existing methods often overlook need regional supervision crucial areas face hands neglect explicit modeling motion blur leading unrealistic lowquality synthesis address limitations first leverage regional supervision detailed regions enhance face hand faithfulness second model motion blur explicitly improve appearance quality third explore novel training strategies highresolution human animation improve overall fidelity experimental results demonstrate proposed method outperforms stateoftheart approaches achieving significant improvements upon strongest baseline terms reconstruction precision perceptual quality fvd humandance dataset code model made available,6,0.47011675178577844,6,0.47011675178577844
diffusion models diffusion models emerged powerful generative tools rivaling gans sample quality mirroring likelihood scores autoregressive models subset models exemplified ddims exhibit inherent asymmetry trained steps sample subset generation selective sampling approach though optimized speed inadvertently misses vital information unsampled steps leading potential compromises sample quality address issue present new training method using innovative meticulously designed reintegrate information omitted selective sampling phase benefits approach manifold notably enhances sample quality exceptionally simple implement requires minimal code modifications flexible enough compatible various sampling algorithms dataset models trained using algorithm showed improvement models trained traditional methods across various sampling algorithms ddims pndms deis different numbers sampling steps celeba dataset improvement ranged access code additional resources provided github,-1,0.0,-1,0.0
mobius high efficient spatialtemporal parallel training paradigm texttovideo generation task inspired success texttoimage generation task many researchers devoting texttovideo generation task frameworks usually inherit model add extratemporal layers training generate dynamic videos viewed finetuning task however traditional serial mode temporal layers follow spatial layers result high gpu memory training time consumption according serial feature flow believe serial mode bring training costs large diffusion model massive datasets environmentally friendly suitable development therefore propose highly efficient spatialtemporal parallel training paradigm tasks named mobius temporal layers spatial layers parallel optimizes feature flow backpropagation mobius save gpu memory training time greatly improve finetuning task provide novel insight aigc community release codes future,-1,0.0,-1,0.0
uwafagan ultrawideangle fluorescein angiography transformation via multiscale generation registration enhancement fundus photography combination ultrawideangle fundus uwf techniques becomes indispensable diagnostic tool clinical settings offering comprehensive view retina nonetheless uwf fluorescein angiography uwffa necessitates administration fluorescent dye via injection patients hand elbow unlike uwf scanning laser ophthalmoscopy uwfslo mitigate potential adverse effects associated injections researchers proposed development crossmodality medical image generation algorithms capable converting uwfslo images uwffa counterparts current image generation techniques applied fundus photography encounter difficulties producing highresolution retinal images particularly capturing minute vascular lesions address issues introduce novel conditional generative adversarial network uwafagan synthesize uwffa uwfslo approach employs multiscale generators attention transmit module efficiently extract global structures local lesions additionally counteract image blurriness issue arises training misaligned data registration module integrated within framework method performs nontrivially inception scores details generation clinical user studies indicate uwffa images generated uwafagan clinically comparable authentic images terms diagnostic reliability empirical evaluations proprietary uwf image datasets elucidate uwafagan outperforms extant methodologies code accessible httpsgithubcomtinysquauwafagan,19,0.9852864721246097,19,0.9852864721246097
tvtrees multimodal entailment trees neurosymbolic video reasoning challenging models understand complex multimodal content television clips part videolanguage models often rely singlemodality reasoning lack interpretability combat issues propose tvtrees first multimodal entailment tree generator tvtrees serves approach video understanding promotes interpretable jointmodality reasoning searching trees entailment relationships simple textvideo evidence higherlevel conclusions prove questionanswer pairs also introduce task multimodal entailment tree generation evaluate reasoning quality methods performance challenging tvqa benchmark demonstrates interpretable stateoftheart zeroshot performance full clips illustrating multimodal entailment tree generation bestofbothworlds alternative blackbox systems,0,0.9773612873389849,0,0.9773612873389849
adversarially masked video consistency unsupervised domain adaptation study problem unsupervised domain adaptation egocentric videos propose transformerbased model learn classdiscriminative domaininvariant feature representations consists two novel designs first module called generative adversarial domain alignment network aim learning domaininvariant representations simultaneously learns mask generator domaininvariant encoder adversarial way domaininvariant encoder trained minimize distance source target domain masking generator conversely aims producing challenging masks maximizing domain distance second masked consistency learning module learn classdiscriminative representations enforces prediction consistency masked target videos full forms better evaluate effectiveness domain adaptation methods construct challenging benchmark egocentric videos method achieves stateoftheart performance epickitchen proposed benchmark,7,0.8130008875182181,7,0.8130008875182181
joyhallo digital human model mandarin audiodriven video generation creating mandarin videos presents significant challenges collecting comprehensive mandarin datasets difficult complex lip movements mandarin complicate model training compared english study collected hours mandarin speech video jd health international inc employees resulting jdhhallo dataset dataset includes diverse range ages speaking styles encompassing conversational specialized medical topics adapt joyhallo model mandarin employed chinese model audio feature embedding semidecoupled structure proposed capture interfeature relationships among lip expression pose features integration improves information utilization efficiency also accelerates inference speed notably joyhallo maintains strong ability generate english videos demonstrating excellent crosslanguage generation capabilities code models available httpsjdhalgogithubiojoyhallo,-1,0.0,-1,0.0
beyond fvd enhanced evaluation metrics video generation quality frechet video distance fvd widely adopted metric evaluating video generation distribution quality however effectiveness relies critical assumptions analysis reveals three significant limitations nongaussianity inflated convnet feature space insensitivity features temporal distortions impractical sample sizes required reliable estimation findings undermine fvds reliability show fvd falls short standalone metric video generation evaluation extensive analysis wide range metrics backbone architectures propose jedi jepa embedding distance based features derived joint embedding predictive architecture measured using maximum mean discrepancy polynomial kernel experiments multiple opensource datasets show clear evidence superior alternative widely used fvd metric requiring samples reach steady value increasing alignment human evaluation average,-1,0.0,-1,0.0
vlm see robot human demo video robot action plan via vision language model vision language models vlms recently adopted robotics capability common sense reasoning generalizability existing work applied vlms generate task motion planning natural language instructions simulate training data robot learning work explore using vlm interpret human demonstration videos generate robot task planning method integrates keyframe selection visual perception vlm reasoning pipeline named seedo enables vlm see human demonstrations explain corresponding plans robot validate approach collected set longhorizon human videos demonstrating pickandplace tasks three diverse categories designed set metrics comprehensively benchmark seedo several baselines including stateoftheart videoinput vlms experiments demonstrate seedos superior performance deployed generated task plans simulation environment real robot arm,5,0.43243128640578166,5,0.43243128640578166
another day unique video captioning discriminative prompting long videos contain many repeating actions events shots repetitions frequently given identical captions makes difficult retrieve exact desired clip using text search paper formulate problem unique captioning given multiple clips caption generate new caption clip uniquely identifies propose captioning discriminative prompting cdp predicts property separate identically captioned clips use generate unique captions introduce two benchmarks unique captioning based egocentric footage timeloop movies repeating actions common demonstrate captions generated cdp improve texttovideo egocentric videos timeloop movies,0,0.9602990941444991,0,0.9602990941444991
empowering deaf hard hearing community enhancing video captions using large language models todays digital age video content prevalent serving primary source information education entertainment however deaf hard hearing dhh community often faces significant challenges accessing video content due inadequacy automatic speech recognition asr systems providing accurate reliable captions paper addresses urgent need improve video caption quality leveraging large language models llms present comprehensive study explores integration llms enhance accuracy contextawareness captions generated asr systems methodology involves novel pipeline corrects asrgenerated captions using advanced llms explicitly focuses models like due robust performance language comprehension generation tasks introduce dataset representative realworld challenges dhh community faces evaluate proposed pipeline results indicate llmenhanced captions significantly improve accuracy evidenced notably lower word error rate wer achieved wer compared original asr captions wer shows approximate improvement wer compared original asr captions,-1,0.0,-1,0.0
towards learning contrast kinetics multicondition latent diffusion models contrast agents dynamic contrast enhanced magnetic resonance imaging allow localize tumors observe contrast kinetics essential cancer characterization respective treatment decisionmaking however contrast agent administration associated adverse health risks also restricted patients pregnancy kidney malfunction adverse reactions contrast uptake key biomarker lesion malignancy cancer recurrence risk treatment response becomes pivotal reduce dependency intravenous contrast agent administration end propose multiconditional latent diffusion model capable acquisition timeconditioned image synthesis dcemri temporal sequences evaluate medical image synthesis additionally propose validate frechet radiomics distance image quality measure based biomarker variability synthetic real imaging data results demonstrate methods ability generate realistic multisequence fatsaturated breast dcemri uncover emerging potential deep learning based contrast kinetics simulation publicly share accessible codebase httpsgithubcomrichardobiccnet provide userfriendly library frechet radiomics distance calculation httpspypiorgprojectfrdscore,3,0.888770177161319,3,0.888770177161319
prediction action visual policy learning via joint denoising process diffusion models demonstrated remarkable capabilities image generation tasks including image editing video creation representing good understanding physical world line diffusion models also shown promise robotic control tasks denoising actions known diffusion policy although diffusion generative model diffusion policy exhibit distinct capabilitiesimage prediction robotic action respectivelythey technically follow similar denoising process robotic tasks ability predict future images generate actions highly correlated since share underlying dynamics physical world building insight introduce pad novel visual policy learning framework unifies image prediction robot action within joint denoising process specifically pad utilizes diffusion transformers dit seamlessly integrate images robot states enabling simultaneous prediction future images robot actions additionally pad supports cotraining robotic demonstrations largescale video datasets easily extended robotic modalities depth images pad outperforms previous methods achieving significant relative improvement full metaworld benchmark utilizing single textconditioned visual policy within dataefficient imitation learning setting furthermore pad demonstrates superior generalization unseen tasks realworld robot manipulation settings success rate increase compared strongest baseline project page httpssitesgooglecomviewpadpaper,5,0.7282093675734889,5,0.7282093675734889
navid videobased vlm plans next step visionandlanguage navigation visionandlanguage navigation vln stands key research problem embodied ai aiming enabling agents navigate unseen environments following linguistic instructions field generalization longstanding challenge either outofdistribution scenes sim real paper propose navid videobased large vision language model vlm mitigate generalization gap navid makes first endeavor showcase capability vlms achieve stateoftheart level navigation performance without maps odometers depth inputs following human instruction navid requires onthefly video stream monocular rgb camera equipped robot output nextstep action formulation mimics humans navigate naturally gets rid problems introduced odometer noises gaps map depth inputs moreover videobased approach effectively encode historical observations robots spatiotemporal contexts decision making instruction following train navid navigation samples collected continuous environments including actionplanning instructionreasoning samples along largescale web data extensive experiments show navid achieves stateoftheart performance simulation environments real world demonstrating superior crossdataset transfer thus believe proposed vlm approach plans next step navigation agents also research field,5,0.33867697645905953,5,0.33867697645905953
visionreward finegrained multidimensional human preference learning image video generation visual generative models achieved remarkable progress synthesizing photorealistic images videos yet aligning outputs human preferences across critical dimensions remains persistent challenge though reinforcement learning human feedback offers promise preference alignment existing reward models visual generation face limitations including blackbox scoring without interpretability potentially resultant unexpected biases present visionreward general framework learning human visual preferences image video generation specifically employ hierarchical visual assessment framework capture finegrained human preferences leverages linear weighting enable interpretable preference learning furthermore propose multidimensional consistent strategy using visionreward reward model preference optimization visual generation experiments show visionreward significantly outperform existing image video reward models machine metrics human evaluation notably visionreward surpasses videoscore preference prediction accuracy texttovideo models visionreward achieve higher pairwise win rate compared models using videoscore code datasets provided httpsgithubcomthudmvisionreward,7,0.7938676766692533,7,0.7938676766692533
articulated object manipulation using online axis estimation tracking articulated object manipulation requires precise object interaction objects axis must carefully considered previous research employed interactive perception manipulating articulated objects typically openloop approaches often suffer overlooking interaction dynamics address limitation present closedloop pipeline integrating interactive perception online axis estimation segmented point clouds method leverages interactive perception technique foundation interactive perception inducing slight object movement generate point cloud frames evolving dynamic scene point clouds segmented using segment anything model moving part object masked accurate motion online axis estimation guiding subsequent robotic actions approach significantly enhances precision efficiency manipulation tasks involving articulated objects experiments simulated environments demonstrate method outperforms baseline approaches especially tasks demand precise axisbased control project page httpshytidelgithubiovideotrackingforaxisestimation,-1,0.0,-1,0.0
learning correlation structures vision transformers introduce new attention mechanism dubbed structural selfattention structsa leverages rich correlation patterns naturally emerging keyquery interactions attention structsa generates attention maps recognizing spacetime structures keyquery correlations via convolution uses dynamically aggregate local contexts value features effectively leverages rich structural patterns images videos scene layouts object motion interobject relations using structsa main building block develop structural vision transformer structvit evaluate effectiveness image video classification tasks achieving stateoftheart results somethingsomething finegym,20,1.0,20,1.0
mlstrack multilevel semantic interaction rmot new trend multiobject tracking task track objects interest using natural language however scarcity paired promptinstance data hinders progress address challenge propose highquality yet lowcost data generation method base unreal engine construct brandnew benchmark dataset named referuecity primarily includes scenes intersection surveillance videos detailing appearance actions people vehicles specifically provides videos total expressions comparable scale referkitti dataset additionally propose multilevel semanticguided multiobject framework called mlstrack interaction model text enhanced layer layer introduction semantic guidance module sgm semantic correlation branch scb extensive experiments referuecity referkitti datasets demonstrate effectiveness proposed framework achieves stateoftheart performance code datatsets available,-1,0.0,-1,0.0
grouped discrete representation guides objectcentric learning similar humans perceiving visual scenes objects objectcentric learning ocl abstract dense images videos sparse objectlevel features transformerbased ocl handles complex textures well due decoding guidance discrete representation obtained discretizing noisy features image video feature maps using template features codebook however treating features minimal units overlooks composing attributes thus impeding model generalization indexing features natural numbers loses attributelevel commonalities characteristics thus diminishing heuristics model convergence propose textitgrouped discrete representation gdr address issues grouping features attributes indexing tuple numbers extensive experiments across different query initializations dataset modalities model architectures gdr consistently improves convergence generalizability visualizations show method effectively captures attributelevel information features source code available upon acceptance,-1,0.0,-1,0.0
hybrid localglobal context learning neural video compression neural video codecs current stateoftheart methods typically adopt multiscale motion compensation handle diverse motions methods estimate compress either optical flow deformable offsets reduce interframe redundancy however flowbased methods often suffer inaccurate motion estimation complicated scenes deformable convolutionbased methods robust higher bit cost motion coding paper propose hybrid context generation module combines advantages methods optimal way achieves accurate compensation low bit cost specifically considering characteristics features different scales adopt flowguided deformable compensation largestscale produce accurate alignment detailed regions smallerscale features perform flowbased warping save bit cost motion coding furthermore design localglobal context enhancement module fully explore localglobal information previous reconstructed signals experimental results demonstrate proposed hybrid localglobal context learning hlgc method significantly enhance stateoftheart methods standard test datasets,-1,0.0,-1,0.0
egopoints advancing point tracking egocentric videos introduce egopoints benchmark point tracking egocentric videos annotate challenging tracks egocentric sequences compared popular tapviddavis evaluation benchmark include points go outofview points require reidentification reid returning view measure performance models challenging points introduce evaluation metrics specifically monitor tracking performance points inview outofview points require reidentification propose pipeline create semireal sequences automatic ground truth generate sequences combining dynamic kubric objects scene points epic fields finetuning point tracking methods sequences evaluating annotated egopoints sequences improve cotracker across metrics including tracking accuracy percentage points accuracy reid sequences points also improve pips respectively,-1,0.0,-1,0.0
prism semisupervised multiview stereo monocular structure priors promise unsupervised multiviewstereo mvs leverage large unlabeled datasets yet current methods underperform training difficult data handheld smartphone videos indoor scenes meanwhile highquality synthetic datasets available mvs networks trained datasets fail generalize realworld examples bridge gap propose semisupervised learning framework allows us train real rendered images jointly capturing structural priors synthetic data ensuring parity realworld domain central framework novel set losses leverages powerful existing monocular relativedepth estimators trained synthetic dataset transferring rich structure relative depth mvs predictions unlabeled data inspired perceptual image metrics compare mvs monocular predictions via deep feature loss multiscale statistical loss full framework call prism achieves large quantitative qualitative improvements current unsupervised syntheticsupervised mvs networks bestcasescenario result opening door using unlabeled smartphone videos photorealistic synthetic datasets training mvs networks,1,1.0,1,1.0
texttoimage gan pretrained representations generating desired images conditioned given text descriptions received lots attention recently diffusion models autoregressive models demonstrated outstanding expressivity gradually replaced gan favored architectures texttoimage synthesis however still face obstacles slow inference speed expensive training costs achieve powerful faster texttoimage synthesis complex scenes propose tiger texttoimage gan pretrained representations specific propose visionempowered discriminator highcapacity generator visionempowered discriminator absorbs complex scene understanding ability domain generalization ability pretrained vision models enhance model performance unlike previous works explore stacking multiple pretrained models discriminator collect multiple different representations ii highcapacity generator aims achieve effective textimage fusion increasing model capacity highcapacity generator consists multiple novel highcapacity fusion blocks hfblock hfblock contains several deep fusion modules global fusion module play different roles benefit model extensive experiments demonstrate outstanding performance proposed tiger standard zeroshot texttoimage synthesis tasks standard texttoimage synthesis task tiger achieves stateoftheart performance two challenging datasets obtain new fid coco cub zeroshot texttoimage synthesis task achieve comparable performance fewer model parameters smaller training data size faster inference speed additionally experiments analyses conducted supplementary material,-1,0.0,-1,0.0
enhancing humancentered dynamic scene understanding via multiple llms collaborated reasoning humancentered dynamic scene understanding plays pivotal role enhancing capability robotic autonomous systems videobased humanobject interaction vhoi detection crucial task semantic scene understanding aimed comprehensively understanding hoi relationships within video benefit behavioral decisions mobile robots autonomous driving systems although previous vhoi detection models made significant strides accurate detection specific datasets still lack general reasoning ability like human beings effectively induce hoi relationships study propose vhoi multillms collaborated reasoning vhoi mlcr novel framework consisting series plugandplay modules could facilitate performance current vhoi detection models leveraging strong reasoning ability different offtheshelf pretrained large language models llms design twostage collaboration system different llms vhoi task specifically first stage design crossagents reasoning scheme leverage llm conduct reasoning different aspects second stage perform multillms debate get final reasoning answer based different knowledge different llms additionally devise auxiliary training strategy utilizes clip large visionlanguage model enhance base vhoi models discriminative ability better cooperate llms validate superiority design demonstrating effectiveness improving prediction accuracy base vhoi model via reasoning multiple perspectives,0,0.8431957461875648,0,0.8431957461875648
hierarchical openvocabulary scene graphs languagegrounded robot navigation recent openvocabulary robot mapping methods enrich dense geometric maps pretrained visuallanguage features maps allow prediction pointwise saliency maps queried certain language concept largescale environments abstract queries beyond object level still pose considerable hurdle ultimately limiting languagegrounded robotic navigation work present hovsg hierarchical openvocabulary scene graph mapping approach languagegrounded robot navigation leveraging openvocabulary vision foundation models first obtain stateoftheart openvocabulary segmentlevel maps subsequently construct scene graph hierarchy consisting floor room object concepts enriched openvocabulary features approach able represent multistory buildings allows robotic traversal using crossfloor voronoi graph hovsg evaluated three distinct datasets surpasses previous baselines openvocabulary semantic accuracy object room floor level producing reduction representation size compared dense openvocabulary maps order prove efficacy generalization capabilities hovsg showcase successful longhorizon languageconditioned robot navigation within realworld multistorage environments provide code trial video data httphovsggithubio,5,0.31998859247632794,5,0.31998859247632794
eliteevgs learning eventbased gaussian splatting distilling eventtovideo priors event cameras bioinspired sensors output asynchronous sparse event streams instead fixed frames benefiting distinct advantages high dynamic range high temporal resolution event cameras applied address reconstruction important robotic mapping recently neural rendering techniques gaussian splatting shown successful reconstruction however still remains underexplored develop effective eventbased pipeline particular typically depends highquality initialization dense multiview constraints potential problem appears optimization events given inherent sparse property end propose novel eventbased framework named eliteevgs key idea distill prior knowledge offtheshelf eventtovideo models effectively reconstruct scenes events coarsetofine optimization manner specifically address complexity initialization events introduce novel warmup initialization strategy optimizes coarse frames generated models incorporates events refine details propose progressive event supervision strategy employs windowslicing operation progressively reduce number events used supervision subtly relives temporal randomness event frames benefiting optimization local textural global structural details experiments benchmark datasets demonstrate eliteevgs reconstruct scenes better textural structural details meanwhile method yields plausible performance captured realworld data including diverse challenging conditions fast motion low light scenes,1,1.0,1,1.0
posetalk textandaudiobased pose control motion refinement oneshot talking head generation previous audiodriven talking head generation thg methods generate head poses driving audio generated poses lips match audio well editable study propose textbfposetalk thg system freely generate lipsynchronized talking head videos free head poses conditioned text prompts audio core insight method using head pose connect visual linguistic audio signals first propose generate poses audio text prompts audio offers shortterm variations rhythm correspondence head movements text prompts describe longterm semantics head motions achieve goal devise pose latent diffusion pld model generate motion latent text prompts audio cues pose latent space second observe lossimbalance problem loss lip region contributes less total reconstruction loss caused pose lip making optimization lean towards head movements rather lip shapes address issue propose refinementbased learning strategy synthesize natural talking videos using two cascaded networks ie coarsenet refinenet coarsenet estimates coarse motions produce animated images novel poses refinenet focuses learning finer lip motions progressively estimating lip motions lowtohigh resolutions yielding improved lipsynchronization performance experiments demonstrate pose prediction strategy achieves better pose diversity realness compared textonly audioonly video generator model outperforms stateoftheart methods synthesizing talking videos natural head motions project httpsjunleengithubioprojectsposetalk,6,0.6505399458596948,6,0.6505399458596948
mora enabling generalist video generation via multiagent framework texttovideo generation made significant strides replicating capabilities advanced systems like openai sora remains challenging due closedsource nature existing opensource methods struggle achieve comparable performance often hindered ineffective agent collaboration inadequate training data quality paper introduce mora novel multiagent framework leverages existing opensource modules replicate sora functionalities address fundamental limitations proposing three key techniques multiagent finetuning selfmodulation factor enhance interagent coordination datafree training strategy uses large models synthesize training data humanintheloop mechanism combined multimodal large language models data filtering ensure highquality training datasets comprehensive experiments six video generation tasks demonstrate mora achieves performance comparable sora vbench outperforming existing opensource methods across various tasks specifically texttovideo generation task mora achieved video quality score surpassing sora outperforming baseline models across six key metrics additionally imagetovideo generation task mora achieved perfect dynamic degree score demonstrating exceptional capability enhancing motion realism achieving higher imaging quality sora results highlight potential collaborative multiagent systems humanintheloop mechanisms advancing texttovideo generation code available urlhttpsgithubcomlichaosunmora,-1,0.0,-1,0.0
ganmut generating modifying facial expressions realm emotion synthesis ability create authentic nuanced facial expressions continues gain importance ganmut study discusses recently introduced advanced gan framework instead relying predefined labels learns dynamic interpretable emotion space methodology maps discrete emotion vectors starting neutral state magnitude reflecting emotions intensity current project aims extend study framework benchmarking across various datasets image resolutions facial detection methodologies involve conducting series experiments using two emotional datasets affnet contains videos captured uncontrolled environments include diverse camera angles head positions lighting conditions providing realworld challenge affnet offers images labelled emotions improving diversity emotional expressions available training first two experiments focus training ganmut using dataset processed either retinaface mtcnn highperformance deep learning face detectors setup help determine well ganmut learn synthesise emotions challenging conditions assess comparative effectiveness face detection technologies subsequent two experiments merge affnet datasets combining real world variability diverse emotional labels affnet face detectors retinaface mtcnn employed evaluate whether enhanced diversity combined datasets improves ganmuts performance compare impact face detection method hybrid setup,7,0.9201342459570006,7,0.9201342459570006
microplastic identification using aidriven image segmentation gangenerated ecological context current methods microplastic identification water samples costly require expert analysis propose deep learning segmentation model automatically identify microplastics microscopic images labeled images microplastic moore institute plastic pollution research employ generative adversarial network gan supplement generate diverse training data verify validity generated data conducted reader study expert able discern generated microplastic real microplastic rate percent segmentation model trained combined data achieved diverse dataset compared model without generated datas findings aim enhance ability experts citizens detect microplastic across diverse ecological contexts thereby improving cost accessibility microplastic analysis,-1,0.0,-1,0.0
crossmodality image synthesis tofmra cta using diffusionbased models cerebrovascular disease often requires multiple imaging modalities accurate diagnosis treatment monitoring computed tomography angiography cta timeofflight magnetic resonance angiography tofmra two common noninvasive angiography techniques distinct strengths accessibility safety diagnostic accuracy cta widely used acute stroke due faster acquisition times higher diagnostic accuracy tofmra preferred safety avoids radiation exposure contrast agentrelated health risks despite predominant role cta clinical workflows scarcity opensource cta data limiting research development ai models tasks large vessel occlusion detection aneurysm segmentation study explores diffusionbased imagetoimage translation models generate synthetic cta images tofmra input demonstrate modality conversion tofmra cta show diffusion models outperform traditional unetbased approach work compares different stateoftheart diffusion architectures samplers offering recommendations optimal model performance crossmodality translation task,3,0.9878080077418531,3,0.9878080077418531
poco policy composition heterogeneous robot learning training general robotic policies heterogeneous data different tasks significant challenge existing robotic datasets vary different modalities color depth tactile proprioceptive information collected different domains simulation real robots human videos current methods usually collect pool data one domain train single policy handle heterogeneity tasks domains prohibitively expensive difficult work present flexible approach dubbed policy composition combine information across diverse modalities domains learning scenelevel tasklevel generalized manipulation skills composing different data distributions represented diffusion models method use tasklevel composition multitask manipulation composed analytic cost functions adapt policy behaviors inference time train method simulation human real robot data evaluate tooluse tasks composed policy achieves robust dexterous performance varying scenes tasks outperforms baselines single data source simulation realworld experiments see httpsliruiwgithubiopolicycomp details,5,0.7040625364444743,5,0.7040625364444743
one shot gans long tail problem skin lesion dataset using novel content space assessment metric long tail problems frequently arise medical field particularly due scarcity medical data rare conditions scarcity often leads models overfitting limited samples consequently training models datasets heavily skewed classes number samples varies significantly problem emerges training imbalanced datasets result selective detection model accurately identifies images belonging majority classes disregards minority classes causes model lack generalizability preventing use newer data poses significant challenge developing image detection diagnosis models medical image datasets address challenge one shot gans model employed augment tail class dataset generating additional samples furthermore enhance accuracy novel metric tailored suit one shot gans utilized,3,0.5260576955629406,3,0.5260576955629406
interactive scene authoring specialized generative primitives generating highquality digital assets often requires expert knowledge complex design tools introduce specialized generative primitives generative framework allows nonexpert users author highquality scenes seamless lightweight controllable manner primitive efficient generative model captures distribution single exemplar real world framework users capture video environment turn highquality explicit appearance model thanks gaussian splatting users select regions interest guided semanticallyaware features create generative primitive adapt generative cellular automata singleexemplar training controllable generation decouple generative task appearance model operating sparse voxels recover highquality output subsequent sparse patch consistency step primitive trained within minutes used author new scenes interactively fully compositional manner showcase interactive sessions various primitives extracted realworld scenes controlled create assets scenes minutes also demonstrate additional capabilities primitives handling various representations control generation transferring appearances editing geometries,-1,0.0,-1,0.0
mgancrcm novel multiple generative adversarial network coarserefinement based cognizant method image inpainting image inpainting widely used technique computer vision reconstructing missing damaged pixels images recent advancements generative adversarial networks gans demonstrated superior performance traditional methods due deep learning capabilities adaptability across diverse image domains residual networks resnet also gained prominence ability enhance feature representation compatibility architectures paper introduces novel architecture combining gan resnet models improve image inpainting outcomes framework integrates three components transpose convolutionbased gan guided blind inpainting fast resnetconvolutional neural network frcnn object removal comodulation gan comod gan refinement models performance evaluated benchmark datasets achieving accuracies imagenet celeba comparative analyses demonstrate proposed architecture outperforms existing methods highlighting effectiveness qualitative quantitative evaluations,-1,0.0,-1,0.0
instant policy incontext imitation learning via graph diffusion following impressive capabilities incontext learning large transformers incontext imitation learning icil promising opportunity robotics introduce instant policy learns new tasks instantly without training one two demonstrations achieving icil two key components first introduce inductive biases graph representation model icil graph generation problem learned diffusion process enabling structured reasoning demonstrations observations actions second show model trained using pseudodemonstrations arbitrary trajectories generated simulation virtually infinite pool training data simulated real experiments show instant policy enables rapid learning various everyday robot tasks also show serve foundation crossembodiment zeroshot transfer languagedefined tasks code videos available httpswwwrobotlearningukinstantpolicy,5,0.8196321112629671,5,0.8196321112629671
facelift single image head view generation gslrm present facelift feedforward approach rapid highquality head reconstruction single image pipeline begins employing multiview latent diffusion model generates consistent side back views head single facial input generated views serve input gslrm reconstructor produces comprehensive representation using gaussian splats train system develop dataset multiview renderings using synthetic human head assets diffusionbased multiview generator trained exclusively synthetic head images gslrm reconstructor undergoes initial training objaverse followed finetuning synthetic head data facelift excels preserving identity maintaining view consistency across views despite trained solely synthetic data facelift demonstrates remarkable generalization realworld images extensive qualitative quantitative evaluations show facelift outperforms stateoftheart methods head reconstruction highlighting practical applicability robust performance realworld images addition single image reconstruction facelift supports video inputs novel view synthesis seamlessly integrates reanimation techniques enable facial animation project page httpsweijielyugithubiofacelift,6,0.563602360310174,6,0.563602360310174
smart scenemotionaware human action recognition framework mental disorder group patients mental disorders often exhibit risky abnormal actions climbing walls hitting windows necessitating intelligent video behavior monitoring smart healthcare rising internet things iot technology however development visionbased human action recognition har actions hindered lack specialized algorithms datasets paper innovatively propose build visionbased har dataset including abnormal actions often occurring mental disorder group introduce novel scenemotionaware action recognition technology framework named smart consisting two technical modules first propose scene perception module extract human motion trajectory humanscene interaction features introduces additional scene information supplementary semantic representation actions second multistage fusion module fuses skeleton motion motion trajectory humanscene interaction features enhancing semantic association skeleton motion supplementary representation thus generating comprehensive representation human motion scene information effectiveness proposed method validated selfcollected har dataset mentalhad achieving accuracy unseen subjects scenes outperforming stateoftheart approaches respectively demonstrated subject scene generalizability makes possible smarts migration practical deployment smart healthcare systems mental disorder patients medical settings code dataset released publicly research httpsgithubcominowlzysmartgit,-1,0.0,-1,0.0
swaptalk audiodriven talking face generation oneshot customization latent space combining face swapping lip synchronization technology offers costeffective solution customized talking face generation however directly cascading existing models together tends introduce significant interference tasks reduce video clarity interaction space limited lowlevel semantic rgb space address issue propose innovative unified framework swaptalk accomplishes face swapping lip synchronization tasks latent space referring recent work face generation choose vqembedding space due excellent editability fidelity performance enhance frameworks generalization capabilities unseen identities incorporate identity loss training face swapping module additionally introduce expert discriminator supervision within latent space training lip synchronization module elevate synchronization quality evaluation phase previous studies primarily focused selfreconstruction lip movements synchronous audiovisual videos better approximate realworld applications expand evaluation scope asynchronous audiovideo scenarios furthermore introduce novel identity consistency metric comprehensively assess identity consistency time series generated facial videos experimental results hdtf demonstrate method significantly surpasses existing techniques video quality lip synchronization accuracy face swapping fidelity identity consistency demo available httpswaptalkcc,6,1.0,6,1.0
acdc autoregressive coherent multimodal generation using diffusion correction autoregressive models arms diffusion models dms represent two leading paradigms generative modeling excelling distinct areas arms global context modeling longsequence generation dms generating highquality local contexts especially continuous data images short videos however arms often suffer exponential error accumulation long sequences leading physically implausible results dms limited local context generation capabilities work introduce autoregressive coherent multimodal generation diffusion correction acdc zeroshot approach combines strengths arms dms inference stage without need additional finetuning acdc leverages arms global context generation memoryconditioned dms local correction ensuring highquality outputs correcting artifacts generated multimodal tokens particular propose memory module based large language models llms dynamically adjusts conditioning texts dms preserving crucial global context information experiments multimodal tasks including coherent multiframe story generation autoregressive video generation demonstrate acdc effectively mitigates accumulation errors significantly enhances quality generated outputs achieving superior performance remaining agnostic specific arm dm architectures project page,-1,0.0,-1,0.0
learning spatiotemporal inconsistency via thumbnail layout face deepfake detection deepfake threats society cybersecurity provoked significant public apprehension driving intensified efforts within realm deepfake video detection current videolevel methods mostly based cnns resulting high computational demands although achieved good performance paper introduces elegantly simple yet effective strategy named thumbnail layout tall transforms video clip predefined layout realize preservation spatial temporal dependencies transformation process involves sequentially masking frames positions within frame frames resized subframes reorganized predetermined layout forming thumbnails tall modelagnostic remarkable simplicity necessitating minimal code modifications furthermore introduce graph reasoning block grb semantic consistency sc loss strengthen tall culminating tall grb enhances interactions different semantic regions capture semanticlevel inconsistency clues semantic consistency loss imposes consistency constraints semantic features improve model generalization ability extensive experiments intradataset crossdataset diffusiongenerated image detection deepfake generation method recognition show tall achieves results surpassing comparable stateoftheart methods demonstrating effectiveness approaches various deepfake detection problems code available,4,0.6897877664883997,4,0.6897877664883997
pgcs physical law embedded generative cloud synthesis remote sensing images data quantity quality critical information extraction analyzation remote sensing however current remote sensing datasets often fail meet two requirements cloud primary factor degrading data quantity quality limitation affects precision results remote sensing application particularly derived datadriven techniques paper physical law embedded generative cloud synthesis method pgcs proposed generate diverse realistic cloud images enhance real data promote development algorithms subsequent tasks cloud correction cloud detection data augmentation classification recognition segmentation pgcs method involves two key phases spatial synthesis spectral synthesis spatial synthesis phase stylebased generative adversarial network utilized simulate spatial characteristics generating infinite number singlechannel clouds spectral synthesis phase atmospheric scattering law embedded local statistics global fitting method converting singlechannel clouds multispectral clouds experimental results demonstrate pgcs achieves high accuracy phases performs better three existing cloud synthesis methods two cloud correction methods developed pgcs exhibits superior performance compared stateoftheart methods cloud correction task furthermore application pgcs data various sensors investigated successfully extended code provided httpsgithubcomliyingxupgcs,14,0.8891853327587088,14,0.8891853327587088
nonasymptotic bounds forward processes denoising diffusions ornsteinuhlenbeck hard beat denoising diffusion probabilistic models ddpms represent recent advance generative modelling delivered stateoftheart results across many domains applications despite success rigorous theoretical understanding error within ddpms particularly nonasymptotic bounds required comparison efficiency remain scarce making minimal assumptions initial data distribution allowing example manifold hypothesis paper presents explicit nonasymptotic bounds forward diffusion error total variation tv expressed function terminal time parametrise multimodal data distributions terms distance r furthest modes consider forward diffusions additive multiplicative noise analysis rigorously proves mild assumptions canonical choice ornsteinuhlenbeck ou process significantly improved terms reducing terminal time function r error tolerance motivated data distributions arising generative modelling also establish cutoff like phenomenon rtoinfty convergence invariant measure tv ou process initialized multimodal distribution maximal mode distance r,13,1.0,13,1.0
maggie masked guided gradual human instance matting human matting foundation task image video processing human foreground pixels extracted input prior works either improve accuracy additional guidance improve temporal consistency single instance across frames propose new framework maggie masked guided gradual human instance matting predicts alpha mattes progressively human instances maintaining computational cost precision consistency method leverages modern architectures including transformer attention sparse convolution output instance mattes simultaneously without exploding memory latency although keeping constant inference costs multipleinstance scenario framework achieves robust versatile performance proposed synthesized benchmarks higher quality image video matting benchmarks novel multiinstance synthesis approach publicly available sources introduced increase generalization models realworld scenarios,-1,0.0,-1,0.0
automatically generating mixed reality instructions augmenting extracted motion videos paper introduces mixed reality system automatically generates sports exercise instructions videos mixed reality instructions great potential physical training existing works require substantial time cost create experiences overcomes limitation transforming arbitrary instructional videos available online mr avatars aienabled motion capture deepmotion automatically enhances avatar motion following augmentation techniques contrasting highlighting differences user avatar postures visualizing key trajectories movements specific body parts manipulation time speed using body motion spatially repositioning avatars different perspectives developed hololens azure kinect showcase various use cases including yoga dancing soccer tennis physical exercises study results confirm provides engaging playful learning experiences compared existing video instructions,-1,0.0,-1,0.0
exploring aibased anonymization industrial image video data context feature preservation rising technologies protection privacysensitive information becoming increasingly important industry production facilities image video recordings beneficial documentation tracing production errors coordinating workflows individuals images videos need anonymized however anonymized data reusable applications work apply deep learningbased fullbody anonymization framework generates artificial identities industrial image video data compare performance conventional anonymization techniques therefore consider quality identity generation temporal consistency applicability pose estimation action recognition,4,0.8677395881051077,4,0.8677395881051077
motionbooth motionaware customized texttovideo generation work present motionbooth innovative framework designed animating customized subjects precise control object camera movements leveraging images specific object efficiently finetune texttovideo model capture objects shape attributes accurately approach presents subject region loss video preservation loss enhance subjects learning performance along subject token crossattention loss integrate customized subject motion control signals additionally propose trainingfree techniques managing subject camera motions inference particular utilize crossattention map manipulation govern subject motion introduce novel latent shift module camera movement control well motionbooth excels preserving appearance subjects simultaneously controlling motions generated videos extensive quantitative qualitative evaluations demonstrate superiority effectiveness method project page httpsjianzongwugithubioprojectsmotionbooth,-1,0.0,-1,0.0
rextime benchmark suite reasoningacrosstime videos introduce rextime benchmark designed rigorously test ai models ability perform temporal reasoning within video events specifically rextime focuses reasoning across time ie humanlike understanding question corresponding answer occur different video segments form reasoning requiring advanced understanding causeandeffect relationships across video segments poses significant challenges even frontier multimodal large language models facilitate evaluation develop automated pipeline generating temporal reasoning questionanswer pairs significantly reducing need laborintensive manual annotations benchmark includes carefully vetted validation samples test samples manually curated accuracy relevance evaluation results show frontier large language models outperform academic models still lag behind human performance significant accuracy gap additionally pipeline creates training dataset machine generated samples without manual effort empirical studies suggest enhance acrosstime reasoning via finetuning,0,0.9704390934414208,0,0.9704390934414208
unified embedding alignment openvocabulary video instance segmentation openvocabulary video instance segmentation vis attracting increasing attention due ability segment track arbitrary objects however recent openvocabulary vis attempts obtained unsatisfactory results especially terms generalization ability novel categories discover domain gap vlm features eg clip instance queries underutilization temporal consistency two central causes mitigate issues design train novel openvocabulary vis baseline called ovformer ovformer utilizes lightweight module unified embedding alignment query embeddings clip image embeddings remedy domain gap unlike previous imagebased training methods conduct videobased model training deploy semionline inference scheme fully mine temporal consistency video without bells whistles ovformer achieves map backbone lvvis exceeding previous stateoftheart performance extensive experiments closevocabulary vis datasets also demonstrate strong zeroshot generalization ability ovformer map youtubevis map ovis code available httpsgithubcomfanghaookovformer,7,0.90963292485251,7,0.90963292485251
ecvoice audio text extraction optimization video based idioms similarity replacement text extraction audio video plays important role multimedia editing processing popular open source toolkit whisper performs fast human voice recognition however recognition performance dependent computing resource makes low computing memory running whisper become difficult paper presents available solution extract human voice video gain high quality text generation voice generated voice used video language translation translated voice simulation improve extraction transform quality human voice present ecvoice method using idioms similarity computation analysis improve quality audio text extraction relative experiments held verify ecvoice improve idiom grammar correction rate average method simple fast means method cause less bad influence consuming computing resources improving voice recognition rate method solution significantly enhance whisper recognition low computing memory,-1,0.0,-1,0.0
real face video animation platform recent years facial video generation models gained popularity however models often lack expressive power dealing exaggerated animestyle faces due absence highquality animestyle face training sets propose facial animation platform enables realtime conversion real human faces cartoonstyle faces supporting multiple models built gradio framework platform ensures excellent interactivity userfriendliness users input real face video image select desired cartoon style system automatically analyze facial features execute necessary preprocessing invoke appropriate models generate expressive animestyle faces employ variety models within system process hdtf dataset thereby creating animated facial video dataset,6,0.6033279836254866,6,0.6033279836254866
oneshot video imitation via parameterized symbolic abstraction graphs learning manipulate dynamic deformable objects single demonstration video holds great promise terms scalability previous approaches predominantly focused either replaying object relationships actor trajectories former often struggles generalize across diverse tasks latter suffers data inefficiency moreover methodologies encounter challenges capturing invisible physical attributes forces paper propose interpret video demonstrations parameterized symbolic abstraction graphs psag nodes represent objects edges denote relationships objects ground geometric constraints simulation estimate nongeometric visually imperceptible attributes augmented psag applied real robot experiments approach validated across range tasks cutting avocado cutting vegetable pouring liquid rolling dough slicing pizza demonstrate successful generalization novel objects distinct visual physical properties,5,0.24250738546764128,5,0.24250738546764128
less concatenating videos sign language translation small set signs limited amount labeled data training brazilian sign language libras portuguese translation models challenging problem due video collection annotation costs paper proposes generating sign language content concatenating short clips containing isolated signals training sign language translation models employ vlibrasil dataset composed sign videos signs interpreted least three persons create hundreds thousands sentences respective libras translation feed model specifically propose several experiments varying vocabulary size sentence structure generating datasets approximately videos results achieve meaningful scores meteor respectively technique enables creation extension existing datasets much lower cost collection annotation thousands sentences providing clear directions future works,-1,0.0,-1,0.0
multicounter multiple action agnostic repetition counting untrimmed videos multiinstance repetitive action counting mrac aims estimate number repetitive actions performed multiple instances untrimmed videos commonly found humancentric domains like sports exercise paper propose multicounter fully endtoend deep learning framework enables simultaneous detection tracking counting repetitive actions multiple human instances specifically multicounter incorporates two novel modules mixed spatiotemporal interaction efficient context correlation across consecutive frames taskspecific heads accurate perception periodic boundaries generalization actionagnostic human instances train multicounter synthetic dataset called multirep generated annotated realworld videos experiments multirep dataset validate fundamental challenge mrac tasks showcase superiority proposed model compared bytetrackrepnet solution combines advanced tracker single repetition counter multicounter substantially improves periodmap reduces avgmae increases avgobo times sets new benchmark field mrac moreover multicounter runs realtime commodity gpu server insensitive number human instances video,-1,0.0,-1,0.0
languagebased video colorization creative consistent color automatic video colorization inherently illposed problem monochrome frame multiple optional color candidates previous exemplarbased video colorization methods restrict users imagination due elaborate retrieval process alternatively conditional image colorization methods combined postprocessing algorithms still struggle maintain temporal consistency address issues present languagebased video colorization creative consistent colors guide colorization process using userprovided language descriptions model built upon pretrained crossmodality generative model leveraging comprehensive language understanding robust color representation abilities introduce crossmodality prefusion module generate instanceaware text embeddings enabling application creative colors additionally propose temporally deformable attention prevent flickering color shifts crossclip fusion maintain longterm color consistency extensive experimental results demonstrate outperforms relevant methods achieving semantically accurate colors unrestricted creative correspondence temporally robust consistency,-1,0.0,-1,0.0
designminds enhancing videobased design ideation visionlanguage model contextinjected large language model ideation critical component videobased design vbd videos serve primary medium design exploration inspiration emergence generative ai offers considerable potential enhance process streamlining video analysis facilitating idea generation paper present designminds prototype integrates stateoftheart visionlanguage model vlm contextenhanced large language model llm support ideation vbd evaluate designminds conducted betweensubject study design practitioners comparing performance baseline condition results demonstrate designminds significantly enhances flexibility originality ideation also increasing task engagement importantly introduction technology negatively impact user experience technology acceptance usability,15,1.0,15,1.0
identitypreserving poseguided character animation via facial landmarks transformation creating realistic poseguided imagetovideo character animations preserving facial identity remains challenging especially complex dynamic scenarios dancing precise identity consistency crucial existing methods frequently encounter difficulties maintaining facial coherence due misalignments facial landmarks extracted driving videos provide head pose expression cues facial geometry reference images address limitation introduce facial landmarks transformation flt method leverages morphable model address limitation flt converts landmarks face model adjusts face model align reference identity transforms back landmarks guide imagetovideo generation process approach ensures accurate alignment reference facial geometry enhancing consistency generated videos reference images experimental results demonstrate flt effectively preserves facial identity significantly improving poseguided character animation models,-1,0.0,-1,0.0
polysmart trecvid video captioning vtt paper present methods results videototext vtt task trecvid exploring capabilities visionlanguage models vlms like llava llavanextvideo generating natural language descriptions video content investigate impact finetuning vlms vtt datasets enhance description accuracy contextual relevance linguistic consistency analysis reveals finetuning substantially improves models ability produce detailed domainaligned text bridging gap generic vlm tasks specialized needs vtt experimental results demonstrate finetuned model outperforms baseline vlms across various evaluation metrics underscoring importance domainspecific tuning complex vtt tasks,-1,0.0,-1,0.0
multimodal generative ai multimodal llm diffusion beyond multimodal generative ai received increasing attention academia industry particularly two dominant families techniques multimodal large language model mllm shows impressive ability multimodal understanding ii diffusion model sora exhibits remarkable multimodal powers especially respect visual generation one natural question arises possible unified model understanding generation answer question paper first provide detailed review mllm diffusion models including probabilistic modeling procedure multimodal architecture design advanced applications imagevideo large language models well texttoimagevideo generation discuss two important questions unified model whether unified model adopt autoregressive diffusion probabilistic modeling ii whether model utilize dense architecture mixture expertsmoe architectures better support generation understanding two objectives provide several possible strategies building unified model analyze potential advantages disadvantages also summarize existing largescale multimodal datasets better model pretraining future conclude paper present several challenging future directions believe contribute ongoing advancement multimodal generative ai,-1,0.0,-1,0.0
hybrid conditional latent diffusion high frequency enhancement cbcttoct synthesis background conebeam computed tomography cbct plays crucial role imageguided radiotherapy artifacts noise make unsuitable accurate dose calculation artificial intelligence methods shown promise enhancing cbct quality produce synthetic ct sct images however existing methods either produce images suboptimal quality incur excessive time costs failing satisfy clinical practice standards methods materials propose novel hybrid conditional latent diffusion model efficient accurate cbcttoct synthesis named employ unified feature encoder ufe compress images lowdimensional latent space thereby optimizing computational efficiency beyond use cbct images propose integrating highfrequency knowledge hybrid condition guide diffusion model generating sct images preserved structural details highfrequency information captured using designed highfrequency extractor hfe inference utilize denoising diffusion implicit model facilitate rapid sampling construct new inhouse prostate dataset paired cbct ct validate effectiveness method result extensive experimental results demonstrate approach outperforms stateoftheart methods terms sct quality generation efficiency moreover medical physicist conducts dosimetric evaluations validate benefit method practical dose calculation achieving remarkable gamma passing rate criterion superior methods conclusion proposed efficiently achieve highquality cbcttoct synthesis mins per patient promising performance dose calculation shows great potential enhancing realworld adaptive radiotherapy,3,0.5788775889954405,3,0.5788775889954405
diffusion models enhance resolution microscopy images tutorial diffusion models emerged prominent technique generative modeling neural networks making mark tasks like texttoimage translation superresolution tutorial provide comprehensive guide build denoising diffusion probabilistic models ddpms scratch specific focus transforming lowresolution microscopy images corresponding highresolution versions provide theoretical background mathematical derivations detailed python code implementation using pytorch along techniques enhance model performance,-1,0.0,-1,0.0
radrotator rotation radiographs diffusion models transforming twodimensional images threedimensional volumes wellknown yet challenging problem computer vision community medical domain previous studies attempted convert two input radiographs computed tomography ct volumes following effort introduce diffusion modelbased technology rotate anatomical content input radiograph space potentially enabling visualization entire anatomical content radiograph viewpoint similar previous studies used ct volumes create digitally reconstructed radiographs drrs training data model however addressed two significant limitations encountered previous studies utilized conditional diffusion models classifierfree guidance instead generative adversarial networks gans achieve higher mode coverage improved output image quality tradeoff slower inference time often less critical medical applications demonstrated unreliable output style transfer deep learning dl models cyclegan transfer style actual radiographs drrs could replaced simple yet effective training transformation randomly changes pixel intensity histograms input groundtruth imaging data training transformation makes diffusion model agnostic distribution variations input data pixel intensity enabling reliable training dl model input drrs applying exact model conventional radiographs drrs inference,-1,0.0,-1,0.0
data augmentation pipeline generate synthetic labeled datasets echocardiography images using gan due privacy issues limited amount publicly available labeled datasets domain medical imaging propose image generation pipeline synthesize echocardiographic images corresponding ground truth labels alleviate need data collection laborious errorprone human labeling images subsequent deep learning dl tasks proposed method utilizes detailed anatomical segmentations heart ground truth label sources initial dataset combined second dataset made real echocardiographic images train generative adversarial network gan synthesize realistic cardiovascular ultrasound images paired ground truth labels generate synthetic dataset trained gan uses high resolution anatomical models computed tomography ct input qualitative analysis synthesized images showed main structures heart well delineated closely follow labels obtained anatomical models assess usability synthetic images dl tasks segmentation algorithms trained delineate left ventricle left atrium myocardium quantitative analysis segmentations given models trained synthetic images indicated potential use gan approach generate synthetic data use data train dl models different clinical tasks therefore tackle problem scarcity labeled echocardiography datasets,3,0.8145238542715941,3,0.8145238542715941
early stopping criteria training generative adversarial networks biomedical imaging generative adversarial networks gans high computational costs train complex architectures throughout training process gans output analyzed qualitatively based loss synthetic images diversity quality based qualitative analysis training manually halted desired synthetic images generated utilizing early stopping criterion computational cost dependence manual oversight reduced yet impacted training problems mode collapse nonconvergence instability particularly prevalent biomedical imagery training problems degrade diversity quality synthetic images high computational cost associated training makes complex architectures increasingly inaccessible work proposes novel early stopping criteria quantitatively detect training problems halt training reduce computational costs associated synthesizing biomedical images firstly range generator discriminator loss values investigated assess whether mode collapse nonconvergence instability occur sequentially concurrently interchangeably throughout training gans secondly utilizing occurrences conjunction mean structural similarity index msssim frechet inception distance fid scores synthetic images forms basis proposed early stopping criteria work helps identify occurrence training problems gans using lowresource computational cost reduces training time generate diversified highquality synthetic images,17,1.0,17,1.0
glance focus memory prompting multievent video question answering video question answering videoqa emerged vital tool evaluate agents ability understand human daily behaviors despite recent success large vision language models many multimodal tasks complex situation reasoning videos involving multiple humanobject interaction events still remains challenging contrast humans easily tackle using series episode memories anchors quickly locate questionrelated key moments reasoning mimic effective reasoning strategy propose glancefocus model one simple way apply action detection model predict set actions key memories however actions within closed set vocabulary hard generalize various video domains instead train encoderdecoder generate set dynamic event memories glancing stage apart using supervised bipartite matching obtain event memories design unsupervised memory generation method get rid dependence event annotations next focusing stage event memories act bridge establish correlation questions highlevel event concepts lowlevel lengthy video content given question model first focuses generated key event memory focuses relevant moment reasoning designed multilevel crossattention mechanism conduct extensive experiments four multievent videoqa benchmarks including star egotaskqa agqa nextqa proposed model achieves stateoftheart results surpassing current large models various challenging reasoning tasks code models available,0,0.9471716031402818,0,0.9471716031402818
videototext pedestrian monitoring vtpm leveraging computer vision large language models privacypreserve pedestrian activity monitoring intersections computer vision advanced research methodologies enhancing system services across various fields core component traffic monitoring systems improving road safety however monitoring systems dont preserve privacy pedestrians appear videos potentially revealing identities addressing issue paper introduces videototext pedestrian monitoring vtpm monitors pedestrian movements intersections generates realtime textual reports including traffic signal weather information vtpm uses computer vision models pedestrian detection tracking achieving latency seconds per video frame additionally detects crossing violations accuracy incorporating traffic signal data proposed framework equipped generate realtime textual reports pedestrian activity stating safety concerns like crossing violations conflicts impact weather behavior latency seconds enhance comprehensive analysis generated textual reports medium finetuned historical analysis generated textual reports finetuning enables reliable analysis pedestrian safety intersections effectively detecting patterns safety critical events proposed vtpm offers efficient alternative video footage using textual reports reducing memory usage saving million percent eliminating privacy issues enabling comprehensive interactive historical analysis,4,0.7791289004365893,4,0.7791289004365893
image synthesis graph conditioning clipguided diffusion models scene graphs advancements generative models sparked significant interest generating images adhering specific structural guidelines scene graph image generation one task generating images consistent given scene graph however complexity visual scenes poses challenge accurately aligning objects based specified relations within scene graph existing methods approach task first predicting scene layout generating images layouts using adversarial training work introduce novel approach generate images scene graphs eliminates need predicting intermediate layouts leverage pretrained texttoimage diffusion models clip guidance translate graph knowledge images towards first pretrain graph encoder align graph features clip features corresponding images using gan based training fuse graph features clip embedding object labels present given scene graph create graph consistent clip guided conditioning signal conditioning input object embeddings provide coarse structure image graph features provide structural alignment based relationships among objects finally fine tune pretrained diffusion model graph consistent conditioning signal reconstruction clip alignment loss elaborate experiments reveal method outperforms existing methods standard benchmarks cocostuff visual genome dataset,-1,0.0,-1,0.0
find finetuning initial noise distribution policy optimization diffusion models recent years largescale pretrained diffusion models demonstrated outstanding capabilities image video generation tasks however existing models tend produce visual objects commonly found training dataset diverges user input prompts underlying reason behind inaccurate generated results lies models difficulty sampling specific intervals initial noise distribution corresponding prompt moreover challenging directly optimize initial distribution given diffusion process involves multiple denoising steps paper introduce finetuning initial noise distribution find framework policy optimization unleashes powerful potential pretrained diffusion networks directly optimizing initial distribution align generated contents userinput prompts end first reformulate diffusion denoising procedure onestep markov decision process employ policy optimization directly optimize initial distribution addition dynamic reward calibration module proposed ensure training stability optimization furthermore introduce ratio clipping algorithm utilize historical data network training prevent optimized distribution deviating far original policy restrain excessive optimization magnitudes extensive experiments demonstrate effectiveness method texttoimage texttovideo tasks surpassing sota methods achieving consistency prompts generated content method achieves times faster sota approach homepage available urlhttpsgithubcomvpxecnufindwebsite,-1,0.0,-1,0.0
additional look ganbased augmentation deep learning image classification availability training data one main limitations deep learning applications medical imaging data augmentation popular approach overcome problem new approach machine learning based augmentation particular usage generative adversarial networks gan case gans generate images similar original dataset overall training data amount bigger leads better performance trained networks gan model consists two networks generator discriminator interconnected feedback loop creates competitive environment work continuation previous research trained nvidia limited chest xray image dataset paper study dependence ganbased augmentation performance dataset size focus small samples two datasets considered one images per class images total second images per class images total train sets validating quality generated images use trained gans one augmentations approaches multiclass classification problems compare quality ganbased augmentation approach two different approaches classical augmentation augmentation employing transfer learningbased classification chest xray images results quantified using different classification quality metrics compared results literature ganbased augmentation approach found comparable classical augmentation case medium large datasets underperforms case smaller datasets correlation size original dataset quality classification visible independently augmentation approach,3,0.520277827219063,3,0.520277827219063
add augmenting nerfbased pseudolidar point cloud resolving classimbalance problem typical lidarbased object detection models trained supervised manner realworld data collection often imbalanced classes longtailed deal augmenting minorityclass examples sampling ground truth gt lidar points database pasting scene interest often used challenges still remain inflexibility locating gt samples limited sample diversity work propose leverage pseudolidar point clouds generated low cost videos capturing surround view miniatures realworld objects minor classes method called pseudo ground truth augmentation pgtaug consists three main steps volumetric instance reconstruction using view synthesis model ii objectlevel domain alignment lidar intensity estimation iii hybrid contextaware placement method ground map information demonstrate superiority generality method performance improvements extensive experiments conducted three popular benchmarks ie nuscenes kitti lyft especially datasets large domain gaps captured different lidar configurations code data publicly available upon publication,14,1.0,14,1.0
pcqa strong baseline aigc quality assessment based prompt condition development large language models llm diffusion models brings boom artificial intelligence generated content aigc essential build effective quality assessment framework provide quantifiable evaluation different images videos based aigc technologies content generated aigc methods driven crafted prompts therefore intuitive prompts also serve foundation aigc quality assessment study proposes effective aigc quality assessment qa framework first propose hybrid prompt encoding method based dualsource clip contrastive languageimage pretraining text encoder understand respond prompt conditions second propose ensemblebased feature mixer module effectively blend adapted prompt vision features empirical study practices two datasets aigenerated image quality assessment database texttovideo quality assessment database validates effectiveness proposed method prompt condition quality assessment pcqa proposed simple feasible framework may promote research development multimodal generation field,-1,0.0,-1,0.0
paindiffusion learning express pain accurate pain expression synthesis essential improving clinical training humanrobot interaction current robotic patient simulators rpss lack realistic pain facial expressions limiting effectiveness medical training work introduce paindiffusion generative model synthesizes naturalistic facial pain expressions unlike traditional heuristic autoregressive methods paindiffusion operates continuous latent space ensuring smoother natural facial motion supporting indefinitelength generation via diffusion forcing approach incorporates intrinsic characteristics pain expressiveness emotion allowing personalized controllable pain expression synthesis train evaluate model using biovid heatpain database additionally integrate paindiffusion robotic system assess applicability realtime rehabilitation exercises qualitative studies clinicians reveal paindiffusion produces realistic pain expressions std preference rate groundtruth recordings results suggest paindiffusion serve viable alternative real patients clinical training simulation bridging gap synthetic naturalistic pain expression code videos available,-1,0.0,-1,0.0
latent diffusion models histopathology pretrained embeddings unpaired frozen section ffpe translation frozen section fs technique rapid efficient method taking minutes prepare slides pathologists evaluation surgery enabling immediate decisions surgical interventions however fs process often introduces artifacts distortions like folds icecrystal effects contrast artifacts distortions absent higherquality formalinfixed paraffinembedded ffpe slides require days prepare generative adversarial network ganbased methods used translate fs ffpe images may leave morphological inaccuracies remaining fs artifacts introduce new artifacts reducing quality translations clinical assessments study benchmark recent generative models focusing gans latent diffusion models ldms overcome limitations introduce novel approach combines ldms histopathology pretrained embeddings enhance restoration fs images framework leverages ldms conditioned text pretrained embeddings learn meaningful features fs ffpe histopathology images diffusion denoising techniques approach preserves essential diagnostic attributes like color staining tissue morphology also proposes embedding translation mechanism better predict targeted ffpe representation input fs images result work achieves significant improvement classification performance area curve rising accompanied advantageous casefd work establishes new benchmark fs ffpe image translation quality promising enhanced reliability accuracy histopathology fs image analysis work available,3,1.0,3,1.0
magic fixup streamlining photo editing watching dynamic videos propose generative model given coarsely edited image synthesizes photorealistic output follows prescribed layout method transfers fine details original image preserves identity parts yet adapts lighting context defined new layout key insight videos powerful source supervision task objects camera motions provide many observations world changes viewpoint lighting physical interactions construct image dataset sample pair source target frames extracted video randomly chosen time intervals warp source frame toward target using two motion models mimic expected testtime user edits supervise model translate warped image ground truth starting pretrained diffusion model model design explicitly enables fine detail transfer source frame generated image closely following userspecified layout show using simple segmentations coarse manipulations synthesize photorealistic edit faithful users input addressing secondorder effects like harmonizing lighting physical interactions edited objects,-1,0.0,-1,0.0
humanvid demystifying training data cameracontrollable human image animation human image animation involves generating videos character photo allowing user control unlocking potential video movie production recent approaches yield impressive results using highquality training data inaccessibility datasets hampers fair transparent benchmarking moreover approaches prioritize human motion overlook significance camera motions videos leading limited control unstable video generation demystify training data present humanvid first largescale highquality dataset tailored human image animation combines crafted realworld synthetic data realworld data compile vast collection realworld videos internet developed applied careful filtering rules ensure video quality resulting curated collection highresolution humancentric videos human camera motion annotation accomplished using pose estimator slambased method expand synthetic dataset collected avatar assets leveraged existing assets body shapes skin textures clothings notably introduce rulebased camera trajectory generation method enabling synthetic pipeline incorporate diverse precise camera motion annotation rarely found realworld data verify effectiveness humanvid establish baseline model named camanimate short cameracontrollable human animation considers human camera motions conditions extensive experimentation demonstrate simple baseline training humanvid achieves stateoftheart performance controlling human pose camera motions setting new benchmark demo data code could found project website httpshumanvidgithubio,-1,0.0,-1,0.0
depth anything medical images comparative study monocular depth estimation mde critical component many medical tracking mapping algorithms particularly endoscopic laparoscopic video however ground truth depth maps acquired real patient data supervised learning viable approach predict depth maps medical scenes although selfsupervised learning mde recently gained attention outputs difficult evaluate reliably mdes generalizability patients anatomies limited work evaluates zeroshot performance newly released depth anything model medical endoscopic laparoscopic scenes compare accuracy inference speeds depth anything mde models trained general scenes well indomain models trained endoscopic data findings show although zeroshot capability depth anything quite impressive necessarily better models speed performance hope study spark research employing foundation models mde medical scenes,-1,0.0,-1,0.0
dgd dynamic gaussians distillation tackle task learning dynamic semantic radiance fields given single monocular video input learned semantic radiance field captures perpoint semantics well color geometric properties dynamic scene enabling generation novel views corresponding semantics enables segmentation tracking diverse set semantic entities specified using simple intuitive interface includes user click text prompt end present dgd unified representation appearance semantics dynamic scene building upon recently proposed dynamic gaussians representation representation optimized time color semantic information key method joint optimization appearance semantic attributes jointly affect geometric properties scene evaluate approach ability enable dense semantic object tracking demonstrate highquality results fast render diverse set scenes project webpage available httpsisaaclabegithubiodgdwebsite,-1,0.0,-1,0.0
mllmsul multimodal large language model semantic scene understanding localization traffic scenarios multimodal large language models mllms shown satisfactory effects many autonomous driving tasks paper mllms utilized solve joint semantic scene understanding risk localization tasks relying frontview images proposed mllmsul framework dualbranch visual encoder first designed extract features two resolutions rich visual information conducive language model describing risk objects different sizes accurately language generation llama model finetuned predict scene descriptions containing type driving scenario actions risk objects driving intentions suggestions egovehicle ultimately transformerbased network incorporating regression token trained locate risk objects extensive experiments existing dramarolisp dataset extended dramasris dataset demonstrate method efficient surpassing many stateoftheart imagebased videobased methods specifically method achieves score cider score scene understanding task accuracy localization task codes datasets available httpsgithubcomfjqtongjimllmsul,-1,0.0,-1,0.0
evolving singlemodal multimodal facial deepfake detection survey survey addresses critical challenge deepfake detection amidst rapid advancements artificial intelligence aigenerated media including video audio text become realistic risk misuse spread misinformation commit identity fraud increases focused facecentric deepfakes work traces evolution traditional singlemodality methods sophisticated multimodal approaches handle audiovisual textvisual scenarios provide comprehensive taxonomies detection techniques discuss evolution generative methods autoencoders gans diffusion models categorize technologies unique attributes knowledge first survey kind also explore challenges adapting detection methods new generative models enhancing reliability robustness deepfake detectors proposing directions future research survey offers detailed roadmap researchers supporting development technologies counter deceptive use ai media creation particularly facial forgery curated list related papers found,4,0.7705433929320715,4,0.7705433929320715
vasttrack vast category visual object tracking paper introduce novel benchmark dubbed vasttrack towards facilitating development general visual tracking via encompassing abundant classes videos vasttrack possesses several attractive properties vast object category particular covers target objects classes largely surpassing object categories existing popular benchmarks eg classes lasot categories vast object classes expect learn general object tracking larger scale compared current benchmarks vasttrack offers sequences million frames makes date largest benchmark regarding number videos thus could benefit training even powerful visual trackers deep learning era rich annotation besides conventional bounding box annotations vasttrack also provides linguistic descriptions videos rich annotations vasttrack enables development visiononly visionlanguage tracking ensure precise annotation videos manually labeled multiple rounds careful inspection refinement understand performance existing trackers provide baselines future comparison extensively assess representative trackers results surprisingly show significant drops compared current datasets due lack abundant categories videos diverse scenarios training efforts required improve general tracking vasttrack evaluation results made publicly available httpsgithubcomhenglanvasttrack,-1,0.0,-1,0.0
clipvis adapting clip openvocabulary video instance segmentation openvocabulary video instance segmentation strives segment track instances belonging open set categories videos visionlanguage model contrastive languageimage pretraining clip shown robust zeroshot classification ability imagelevel openvocabulary tasks paper propose simple encoderdecoder network called clipvis adapt clip openvocabulary video instance segmentation clipvis adopts frozen clip introduces three modules including classagnostic mask generation temporal topkenhanced matching weighted openvocabulary classification given set initial queries classagnostic mask generation introduces pixel decoder transformer decoder clip pretrained image encoder predict query masks corresponding object scores mask iou scores temporal topkenhanced matching performs query matching across frames using k mostly matched frames finally weighted openvocabulary classification first employs mask pooling generate query visual features clip pretrained image encoder second performs weighted classification using object scores mask iou scores clipvis require annotations instance categories identities experiments performed various video instance segmentation datasets demonstrate effectiveness proposed method especially novel categories using convnextb backbone clipvis achieves ap apn scores validation set lvvis dataset outperforms respectively release source code models,7,1.0,7,1.0
image video compression using generative sparse representation fidelity controls propose framework learned image video compression using generative sparse visual representation svr guided fidelitypreserving controls embedding inputs discrete latent space spanned learned visual codebooks svrbased compression transmits integer codeword indices efficient crossplatform robust however highquality hq reconstruction decoder relies intermediate feature inputs encoder via direct connections due prohibitively high transmission costs previous svrbased compression methods remove feature links resulting largely degraded reconstruction quality work treat intermediate features fidelitypreserving control signals guide conditioned generative reconstruction decoder instead discarding directly transferring signals draw lowquality lq fidelitypreserving alternative input sent decoder low bitrate control signals provide complementary fidelity cues improve reconstruction quality determined compression rate lq alternative tuned trade bitrate fidelity perceptual quality framework conveniently used learned image compression lic learned video compression lvc since svr robust input perturbations large portion codeword indices adjacent frames transferring different indices svrbased lic lvc share similar processing pipeline experiments standard image video compression benchmarks demonstrate effectiveness approach,2,1.0,2,1.0
improving interpretable embeddings adhoc video search generative captions multiword concept bank aligning user query video clips crossmodal latent space semantic concepts two mainstream approaches adhoc video search avs however effectiveness existing approaches bottlenecked small sizes available videotext datasets low quality concept banks results failures unseen queries outofvocabulary problem paper addresses two problems constructing new dataset developing multiword concept bank specifically capitalizing generative model construct new dataset consisting million generated text video pairs pretraining tackle outofvocabulary problem develop multiword concept bank based syntax analysis enhance capability stateoftheart interpretable avs method modeling relationships query words also study impact current advanced features method experimental results show integration aboveproposed elements doubles performance avs method msrvtt dataset improves xinfap trecvid avs query sets eight years margin average,-1,0.0,-1,0.0
dataset scaling cartoon research handdrawn cartoon animation employs sketches flatcolor segments create illusion motion recent advancements like clip svd sora show impressive results understanding generating natural video scaling large models extensive datasets effective cartoons empirical experiments argue ineffectiveness stems notable bias handdrawn cartoons diverges distribution natural videos harness success scaling paradigm benefit cartoon research unfortunately sizable cartoon dataset available exploration research propose dataset first largescale cartoon animation dataset comprises million keyframes covering various artistic styles regions years comprehensive semantic annotations including videotext description pairs anime tags content taxonomies etc pioneer benefits largescale cartoon dataset comprehension generation tasks finetuning contemporary foundation models like video clip video mamba svd achieving outstanding performance cartoonrelated tasks motivation introduce largescaling cartoon research foster generalization robustness future cartoon applications dataset code pretrained models publicly available,-1,0.0,-1,0.0
styletalk unified framework controlling speaking styles talking heads individuals unique facial expression head pose styles reflect personalized speaking styles existing oneshot talking head methods capture personalized characteristics therefore fail produce diverse speaking styles final videos address challenge propose oneshot stylecontrollable talking face generation method obtain speaking styles reference speaking videos drive oneshot portrait speak reference speaking styles another piece audio method aims synthesize stylecontrollable coefficients morphable model including facial expressions head movements unified framework specifically proposed framework first leverages style encoder extract desired speaking styles reference videos transform style codes framework uses styleaware decoder synthesize coefficients audio input style codes decoding framework adopts twobranch architecture generates stylized facial expression coefficients stylized head movement coefficients respectively obtaining coefficients image renderer renders expression coefficients specific persons talkinghead video extensive experiments demonstrate method generates visually authentic talking head videos diverse speaking styles one portrait image audio clip,6,1.0,6,1.0
sustechgan image generation object detection adverse conditions autonomous driving autonomous driving significantly benefits datadriven deep neural networks however data autonomous driving typically fits longtailed distribution critical driving data adverse conditions hard collect although generative adversarial networks gans applied augment data autonomous driving generating driving images adverse conditions still challenging work propose novel framework sustechgan customized dual attention modules multiscale generators novel loss function generate driving images improving object detection autonomous driving adverse conditions test sustechgan wellknown gans generate driving images adverse conditions rain night apply generated images retrain object detection networks specifically add generated images training datasets retrain wellknown evaluate improvement retrained object detection adverse conditions experimental results show generated driving images sustechgan significantly improved performance retrained rain night conditions outperforms wellknown gans opensource code video description datasets available page facilitate image generation development autonomous driving adverse conditions,14,1.0,14,1.0
place solution lsvos challenge referring video object segmentation recent transformerbased models dominated referring video object segmentation rvos task due superior performance prior works adopt unified detr framework generate segmentation masks querytoinstance manner work integrate strengths leading rvos models build effective paradigm first obtain binary mask sequences rvos models improve consistency quality masks propose twostage multimodel fusion strategy stage rationally ensembles rvos models based framework design well training strategy leverages different video object segmentation vos models enhance mask coherence object propagation mechanism method achieves jf refyoutubevos validation set jf test set ranks place largescale video object segmentation challenge iccv track code available,-1,0.0,-1,0.0
large model based sequential keyframe extraction video summarization keyframe extraction aims sum videos semantics minimum number frames paper puts forward large model based sequential keyframe extraction video summarization dubbed lmske contains three stages first use large model cut video consecutive shots employ large model generate frames visual feature within shot second develop adaptive clustering algorithm yield candidate keyframes shot candidate keyframe locating nearest cluster center third reduce candidate keyframes via redundancy elimination within shot finally concatenate accordance sequence shots final sequential keyframes evaluate lmske curate benchmark dataset conduct rich experiments whose results exhibit lmske performs much better quite sota competitors average average fidelity average compression ratio,-1,0.0,-1,0.0
enhancing surveillance camera fov quality via semantic line detection classification deep hough transform quality recorded videos images significantly influenced cameras field view fov critical applications like surveillance systems selfdriving cars inadequate fov give rise severe safety security concerns including car accidents thefts due failure detect individuals objects conventional methods establishing correct fov heavily rely human judgment lack automated mechanisms assess video image quality based fov paper introduce innovative approach harnesses semantic line detection classification alongside deep hough transform identify semantic lines thus ensuring suitable fov understanding view parallel lines approach yields effective score public egocart dataset coupled notably high median score line placement metric illustrate method offers straightforward means assessing quality cameras field view achieving classification accuracy metric serve proxy evaluating potential performance video image quality applications,-1,0.0,-1,0.0
audio hallucinations large audiovideo language models large audiovideo language models generate descriptions video audio however sometimes ignore audio content producing audio descriptions solely reliant visual information paper refers audio hallucinations analyzes large audiovideo language models gather sentences inquiring audio information annotate whether contain hallucinations sentence hallucinated also categorize type hallucination results reveal sentences hallucinated distinct trends observed nouns verbs hallucination type based tackle task audio hallucination classification using pretrained audiotext models zeroshot finetuning settings experimental results reveal zeroshot models achieve higher performance random finetuning models achieve outperforming zeroshot models,-1,0.0,-1,0.0
trainingfree action recognition goal inference dynamic frame selection introduce vidtfs trainingfree openvocabulary video goal action inference framework combines frozen vision foundational model vfm large language model llm novel dynamic frame selection module experiments demonstrate proposed frame selection module improves performance framework significantly validate performance proposed vidtfs four widely used video datasets including crosstask coin activitynet covering goal inference action recognition tasks openvocabulary settings without requiring training finetuning results show vidtfs outperforms pretrained instructiontuned multimodal language models directly stack llm vfm downstream video inference tasks vidtfs adaptability shows future potential generalizing new trainingfree video inference tasks,7,0.8623844637579823,7,0.8623844637579823
world model millionlength video language blockwise ringattention enabling longcontext understanding remains key challenge scaling existing sequence models crucial component developing generally intelligent models process operate long temporal horizons potentially consist millions tokens paper aim address challenges providing comprehensive exploration full development process producing context language models videolanguage models setting new benchmarks language retrieval new capabilities long video understanding detail long context data curation process progressive context extension tokens present efficient opensource implementation scalable training long sequences additionally opensource family parameter models capable processing long text documents videos exceeding tokens,-1,0.0,-1,0.0
multimodality transrectal ultrasound video classification identification clinically significant prostate cancer prostate cancer common noncutaneous cancer world recently multimodality transrectal ultrasound trus increasingly become effective tool guidance prostate biopsies aim effectively identifying prostate cancer propose framework classification clinically significant prostate cancer cspca multimodality trus videos framework utilizes two models extract features bmode images shear wave elastography images respectively adaptive spatial fusion module introduced aggregate two modalities features orthogonal regularized loss used mitigate feature redundancy proposed framework evaluated inhouse dataset containing trus videos achieves favorable performance identifying cspca area curve auc furthermore visualized class activation mapping cam images generated proposed framework may provide valuable guidance localization cspca thus facilitating trusguided targeted biopsy code publicly available,3,0.5926701246438673,3,0.5926701246438673
digital video manipulation detection technique based compression algorithms digital images videos play important role everyday life nowadays people access affordable mobile devices equipped advanced integrated cameras powerful image processing applications technological development facilitates generation multimedia content also intentional modification either recreational malicious purposes forensic techniques detect manipulation images videos become essential paper proposes forensic technique analysing compression algorithms used coding presence recompression uses information macroblocks characteristic standard motion vectors vector support machine used create model allows accurately detect video recompressed,4,0.7707704746467376,4,0.7707704746467376
hri align videobased hri study designs realworld settings hri research using autonomous robots realworld settings produce results highest ecological validity study modality many difficulties limit studies feasibility effectiveness propose hri research framework maximize realworld insights offered videobased studies hri framework used design online study using firstperson videos robots realworld encounter surrogates online study n distinguished withinsubjects effects four robot behavioral conditions perceived social intelligence human willingness help robot enter exterior door realworld betweensubjects replication n using two conditions confirmed validity online studys findings sufficiency participant recruitment target based power analysis online study results hri framework offers hri researchers principled way take advantage efficiency videobased study modalities generating directly transferable knowledge realworld hri code data study provided,-1,0.0,-1,0.0
efficient effective weaklysupervised action segmentation via actiontransitionaware boundary alignment weaklysupervised action segmentation task learning partition long video several action segments training videos accompanied transcripts ordered list actions existing methods need infer pseudo segmentation training serial alignment frames transcript timeconsuming hard parallelized training work aim escape inefficient alignment massive redundant frames instead directly localize action transitions pseudo segmentation generation transition refers change action segment next adjacent one transcript true transitions submerged noisy boundaries due intrasegment visual variation propose novel actiontransitionaware boundary alignment atba framework efficiently effectively filter noisy boundaries detect transitions addition boost semantic learning case noise inevitably present pseudo segmentation also introduce videolevel losses utilize trusted videolevel supervision extensive experiments show effectiveness approach performance training speed,7,0.8997988651208043,7,0.8997988651208043
point supervision worth video instance segmentation video instance segmentation vis challenging vision task aims detect segment track objects videos conventional vis methods rely denselyannotated object masks expensive reduce human annotations one point object video frame training obtain highquality mask predictions close fully supervised models proposed training method consists classagnostic proposal generation module provide rich negative samples spatiotemporal pointbased matcher match object queries provided point annotations comprehensive experiments three vis benchmarks demonstrate competitive performance proposed framework nearly matching fully supervised methods,7,1.0,7,1.0
prota probabilistic token aggregation textvideo retrieval textvideo retrieval aims find relevant crossmodal samples given query recent methods focus modeling whole spatialtemporal relations however since video clips contain diverse content captions model aligning asymmetric videotext pairs high risk retrieving many false positive results paper propose probabilistic token aggregation prota handle crossmodal interaction content asymmetry specifically propose dual partialrelated aggregation disentangle reaggregate token representations lowdimension highdimension spaces propose tokenbased probabilistic alignment generate tokenlevel probabilistic representation maintain feature representation diversity addition adaptive contrastive loss proposed learn compact crossmodal distribution space based extensive experiments prota achieves significant improvements msrvtt lsmdc didemo,-1,0.0,-1,0.0
screwmimic bimanual imitation human videos screw space projection bimanual manipulation longstanding challenge robotics due large number degrees freedom strict spatial temporal synchronization required generate meaningful behavior humans learn bimanual manipulation skills watching humans refining abilities play work aim enable robots learn bimanual manipulation behaviors human video demonstrations finetune interaction inspired seminal work psychology biomechanics propose modeling interaction two hands serial kinematic linkage screw motion particular use define new action space bimanual manipulation screw actions introduce screwmimic framework leverages novel action representation facilitate learning human demonstration selfsupervised policy finetuning experiments demonstrate screwmimic able learn several complex bimanual behaviors single human video demonstration outperforms baselines interpret demonstrations finetune directly original space motion arms information video results httpsrobinlabcsutexaseduscrewmimic,5,1.0,5,1.0
spacetime reinforcement network video object segmentation recently video object segmentation vos networks typically use memorybased methods query frame mask predicted spacetime matching memory frames despite methods superior performance suffer two issues challenging data destroy spacetime coherence adjacent video frames pixellevel matching lead undesired mismatching caused noises distractors address aforementioned issues first propose generate auxiliary frame adjacent frames serving implicit shorttemporal reference query one next learn prototype video object prototypelevel matching implemented query memory experiment demonstrated network outperforms stateoftheart method davis achieving jf score attains competitive result youtube vos addition network exhibits high inference speed fps,-1,0.0,-1,0.0
previously recaps story summarization introduce multimodal story summarization leveraging tv episode recaps short video sequences interweaving key story moments previous episodes bring viewers speed propose plotsnap dataset featuring two crime thriller tv shows rich recaps long episodes minutes story summarization labels unlocked matching recap shots corresponding substories episode propose hierarchical model talesumm processes entire episodes creating compact shot dialog representations predicts importance scores video shot dialog utterance enabling interactions local story groups unlike traditional summarization method extracts multiple plot points long videos present thorough evaluation story summarization including promising crossseries generalization talesumm also shows good results classic video summarization benchmarks,-1,0.0,-1,0.0
place solution mose track cvpr pvuw workshop complex video object segmentation complex video object segmentation serves fundamental task wide range downstream applications video editing automatic data annotation present place solution mose track pvuw mitigate problems caused tiny objects similar objects fast movements mose use instance segmentation generate extra pretraining data valid test set mose segmented instances combined objects extracted coco augment training data enhance semantic representation baseline model besides motion blur added training increase robustness image blur induced motion finally apply test time augmentation tta memory strategy inference stage method ranked mose track pvuw mathcalj mathcalf mathcaljmathcalf,-1,0.0,-1,0.0
multiscale temporal difference transformer videotext retrieval currently field videotext retrieval many transformerbased methods usually stack frame features regrade frames tokens use transformers video temporal modeling however commonly neglect inferior ability transformer modeling local temporal information tackle problem propose transformer variant named multiscale temporal difference transformer mstdt mstdt mainly addresses defects traditional transformer limited ability capture local temporal information besides order better model detailed dynamic information make use difference feature frames practically reflects dynamic movement video extract interframe difference feature integrate difference frame feature multiscale temporal transformer general proposed mstdt consists shortterm multiscale temporal difference transformer longterm temporal transformer former focuses modeling local temporal information latter aims modeling global temporal information last propose new loss narrow distance similar samples extensive experiments show backbone clip mstdt attained new stateoftheart result,-1,0.0,-1,0.0
videohallucer evaluating intrinsic extrinsic hallucinations large videolanguage models recent advancements multimodal large language models mllms extended capabilities video understanding yet models often plagued hallucinations irrelevant nonsensical content generated deviating actual video context work introduces videohallucer first comprehensive benchmark hallucination detection large videolanguage models lvlms videohallucer categorizes hallucinations two main types intrinsic extrinsic offering subcategories detailed analysis including objectrelation temporal semantic detail extrinsic factual extrinsic nonfactual hallucinations adopt adversarial binary videoqa method comprehensive evaluation pairs basic hallucinated questions crafted strategically evaluating eleven lvlms videohallucer reveal majority current models exhibit significant issues hallucinations ii scaling datasets parameters improves models ability detect basic visual cues counterfactuals provides limited benefit detecting extrinsic factual hallucinations iii existing models adept detecting facts identifying hallucinations byproduct analyses instruct development selfpep framework achieving average improvement hallucination resistance across model architectures,10,0.6205721243145245,10,0.6205721243145245
exposure completing temporally consistent neural high dynamic range video rendering high dynamic range hdr video rendering low dynamic range ldr videos frames alternate exposure encounters significant challenges due exposure change absence time stamp exposure change absence make existing methods generate flickering hdr results paper propose novel paradigm render hdr frames via completing absent exposure information hence exposure information complete consistent approach involves interpolating neighbor ldr frames time dimension reconstruct ldr frames absent exposures combining interpolated given ldr frames complete set exposure information available time stamp benefits fusing process hdr results reducing noise ghosting artifacts therefore improving temporal consistency extensive experimental evaluations standard benchmarks demonstrate method achieves stateoftheart performance highlighting importance absent exposure completing hdr video rendering code available,-1,0.0,-1,0.0
endtoend video question answering frame scoring mechanisms adaptive sampling video question answering videoqa emerged challenging frontier field multimedia processing requiring intricate interactions visual textual modalities simply uniformly sampling frames indiscriminately aggregating framelevel visual features often falls short capturing nuanced relevant contexts videos well perform videoqa mitigate issues propose novel videoqa framework equipped tailored frame selection strategy effective efficient videoqa propose three framescoring mechanisms consider question relevance interframe similarity evaluate importance frame given question video furthermore design differentiable adaptive frame sampling mechanism facilitate endtoend training frame selector answer generator experimental results across three widely adopted benchmarks demonstrate model consistently outperforms existing videoqa methods establishing new sota across nextqa star tvqa furthermore quantitative qualitative analyses validate effectiveness design choice,12,0.6374067026277794,12,0.6374067026277794
data story towards automatic animated data video creation llmbased multiagent systems creating data stories raw data challenging due humans limited attention spans need specialized skills recent advancements large language models llms offer great opportunities develop systems autonomous agents streamline data storytelling workflow though multiagent systems benefits fully realizing llm potentials decomposed tasks individual agents designing systems also faces challenges task decomposition performance optimization subtasks workflow design better understand issues develop data director llmbased multiagent system designed automate creation animated data videos representative genre data stories data director interprets raw data breaks tasks designs agent roles make informed decisions automatically seamlessly integrates diverse components data videos case study demonstrates data directors effectiveness generating data videos throughout development derived lessons learned addressing challenges guiding advancements autonomous agents data storytelling also shed light future directions global optimization humanintheloop design application advanced multimodal llms,-1,0.0,-1,0.0
continuous perception benchmark humans continuously perceive process visual signals however current video models typically either sample key frames sparsely divide videos chunks densely sample within chunk approach stems fact existing video benchmarks addressed analyzing key frames aggregating information separate chunks anticipate next generation vision models emulate human perception processing visual input continuously holistically facilitate development models propose continuous perception benchmark video question answering task solved focusing solely frames captioning small chunks summarizing using language models extensive experiments demonstrate existing models whether commercial opensource struggle tasks indicating need new technical advancements direction,0,0.9112308016795848,0,0.9112308016795848
vdpi video deblurring pseudoinverse modeling video deblurring challenging task aims recover sharp sequences blur noisy observations imageformation model plays crucial role traditional modelbased methods constraining possible solutions however case deep learningbased methods despite deeplearning models achieving better results traditional modelbased methods remain widely popular due flexibility increasing number scholars combine two achieve better deblurring performance paper proposes introducing knowledge imageformation model deep learning network using pseudoinverse blur use deep network fit blurring estimate pseudoinverse use estimation combined variational deeplearning network deblur video sequence notably experimental results demonstrate modifications significantly improve performance deep learning models video deblurring furthermore experiments different datasets achieved notable performance improvements proving proposed method generalize different scenarios cameras,-1,0.0,-1,0.0
learning keypoints multiagent behavior analysis using selfsupervision study social interactions collective behaviors multiagent video analysis crucial biology selfsupervised keypoint discovery emerged promising solution reduce need manual keypoint annotations existing methods often struggle videos containing multiple interacting agents especially species color address introduce bkindmulti novel approach leverages pretrained video segmentation models guide keypoint discovery multiagent scenarios eliminates need timeconsuming manual annotations new experimental settings organisms extensive evaluations demonstrate improved keypoint regression downstream behavioral classification videos flies mice rats furthermore method generalizes well species including ants bees humans highlighting potential broad applications automated keypoint annotation multiagent behavior analysis code available httpsdanielpkhalilgithubiobkindmulti,-1,0.0,-1,0.0
selfprompting polyp segmentation colonoscopy using hybrid yolosam model early diagnosis treatment polyps colonoscopy essential reducing incidence mortality colorectal cancer crc however variability polyp characteristics presence artifacts colonoscopy images videos pose significant challenges accurate efficient polyp detection segmentation paper presents novel approach polyp segmentation integrating segment anything model sam model method leverages bounding box predictions autonomously generate input prompts sam thereby reducing need manual annotations conducted exhaustive tests five benchmark colonoscopy image datasets two colonoscopy video datasets demonstrating method exceeds stateoftheart models image video segmentation tasks notably approach achieves high segmentation accuracy using bounding box annotations significantly reducing annotation time effort advancement holds promise enhancing efficiency scalability polyp detection clinical settings,19,1.0,19,1.0
shaking fake detecting deepfake videos real time via active probes realtime deepfake type generative ai capable creating nonexisting contents eg swapping ones face another video unfortunately misused produce deepfake videos web conferences video calls identity authentication malicious purposes including financial scams political misinformation deepfake detection countermeasure deepfake attracted considerable attention academic community yet existing works typically rely learning passive features may perform poorly beyond seen datasets paper propose sfake new realtime deepfake detection method innovatively exploits deepfake models inability adapt physical interference specifically sfake actively sends probes trigger mechanical vibrations smartphone resulting controllable feature footage consequently sfake determines whether face swapped deepfake based consistency facial area probe pattern implement sfake evaluate effectiveness selfbuilt dataset compare six detection methods results show sfake outperforms detection methods higher detection accuracy faster process speed lower memory consumption,4,1.0,4,1.0
dressrecon freeform human reconstruction monocular video present method reconstruct timeconsistent human body models monocular videos focusing extremely loose clothing handheld object interactions prior work human reconstruction either limited tight clothing object interactions requires calibrated multiview captures personalized template scans costly collect scale key insight highquality yet flexible reconstruction careful combination generic human priors articulated body shape learned largescale training data videospecific articulated bagofbones deformation fit single video via testtime optimization accomplish learning neural implicit model disentangles body versus clothing deformations separate motion model layers capture subtle geometry clothing leverage imagebased priors human body pose surface normals optical flow optimization resulting neural fields extracted timeconsistent meshes optimized explicit gaussians highfidelity interactive rendering datasets highly challenging clothing deformations object interactions dressrecon yields higherfidelity reconstructions prior art project page,1,1.0,1,1.0
tikguard deep learning transformerbased solution detecting unsuitable tiktok content kids rise shortform videos platforms like tiktok brought new challenges safeguarding young viewers inappropriate content traditional moderation methods often fall short handling vast rapidly changing landscape usergenerated videos increasing risk children encountering harmful material paper introduces tikguard transformerbased deep learning approach aimed detecting flagging content unsuitable children tiktok using specially curated dataset tikharm leveraging advanced video classification techniques tikguard achieves accuracy showing notable improvement existing methods similar contexts direct comparisons limited uniqueness tikharm dataset tikguards performance highlights potential enhancing content moderation contributing safer online experience minors study underscores effectiveness transformer models video classification sets foundation future research area,-1,0.0,-1,0.0
solution temporal action localisation task perception test challenge report presents method temporal action localisation tal focuses identifying classifying actions within specific time intervals throughout video sequence employ data augmentation technique expanding training dataset using overlapping labels dataset enhancing models ability generalize across various action classes feature extraction utilize stateoftheart models including umt video features beats cavmae audio features approach involves training multimodal video audio unimodal video models followed combining predictions using weighted box fusion wbf method fusion strategy ensures robust action localisation overall approach achieves score securing first place competition,-1,0.0,-1,0.0
simpler better point tracking pseudolabelling real videos stateoftheart point trackers trained synthetic data due difficulty annotating real videos task however result suboptimal performance due statistical gap synthetic real videos order understand issues better introduce comprising new tracking model new semisupervised training recipe allows real videos without annotations used training generating pseudolabels using offtheshelf teachers new model eliminates simplifies components previous trackers resulting simpler often smaller architecture training scheme much simpler prior work achieves better results using times less data study scaling behaviour understand impact using real unsupervised data point tracking model available online offline variants reliably tracks visible occluded points,7,0.8289041679795414,7,0.8289041679795414
pseudo dataset generation outofdomain multicamera view recommendation multicamera systems indispensable movies tv shows media selecting appropriate camera every timestamp decisive impact production quality audience preferences learningbased view recommendation frameworks assist professionals decisionmaking however often struggle outside training domains scarcity labeled multicamera view recommendation datasets exacerbates issue based insight many videos edited original multicamera videos propose transforming regular videos pseudolabeled multicamera view recommendation datasets promisingly training model pseudolabeled datasets stemming videos target domain achieve relative improvement models accuracy target domain bridge accuracy gap indomain neverbeforeseen domains,-1,0.0,-1,0.0
zeroshot action recognition surveillance videos growing demand surveillance public spaces presents significant challenges due shortage human resources current aibased video surveillance systems heavily rely core computer vision models require extensive finetuning particularly difficult surveillance settings due limited datasets difficult setting viewpoint low quality etc work propose leveraging large visionlanguage models lvlms known strong zero fewshot generalization tackle video understanding tasks surveillance specifically explore stateoftheart lvlm improved tokenlevel sampling method selfreflective sampling selfres experiments ucfcrime dataset show represents significant leap zeroshot performance boost baseline selfres additionally increases zeroshot action recognition performance results highlight potential lvlms paired improved sampling techniques advancing surveillance video analysis diverse scenarios,-1,0.0,-1,0.0
dycoke dynamic compression tokens fast video large language models video large language models vllms significantly advanced recently processing complex video content yet inference efficiency remains constrained high computational cost stemming thousands visual tokens generated video inputs empirically observe unlike single image inputs vllms typically attend visual tokens different frames different decoding iterations making oneshot pruning strategy prone removing important tokens mistake motivated present dycoke trainingfree token compression method optimize token representation accelerate vllms dycoke incorporates plugandplay temporal compression module minimize temporal redundancy merging redundant tokens across frames applies dynamic kv cache reduction prune spatially redundant tokens selectively ensures highquality inference dynamically retaining critical tokens decoding step extensive experimental results demonstrate dycoke outperform prior sota counterparts achieving inference speedup memory reduction baseline vllm still improving performance training,-1,0.0,-1,0.0
mufm mambaenhanced feedback model micro video popularity prediction surge microvideos transforming concept popularity researchers delve vast multimodal datasets growing interest understanding origins popularity forces driving rapid expansion recent studies suggest virality short videos tied inherent multimodal content also heavily influenced strength platform recommendations driven audience feedback paper introduce framework capturing longterm dependencies user feedback dynamic event interactions based mamba hawkes process experiments largescale opensource multimodal dataset show model significantly outperforms stateoftheart approaches across various metrics believe models capability map relationships within user feedback behavior sequences contribute evolution nextgeneration recommendation algorithms platform applications also enhance understanding micro video dissemination broader societal impact,-1,0.0,-1,0.0
advanced learningbased inter prediction future video coding fourth generation audio video coding standard inter prediction filter interpf reduces discontinuities prediction adjacent reconstructed pixels inter prediction paper proposes low complexity learningbased inter prediction llip method replace traditional interpf llip enhances filtering process leveraging lightweight neural network model parameters exported efficient inference specifically extract pixels coordinates utilized traditional interpf form training dataset subsequently export weights biases trained neural network model implement inference process without thirdparty dependency enabling seamless integration video codec without relying libtorch thus achieving faster inference speed ultimately replace traditional handcraft filtering parameters interpf learned optimal filtering parameters practical solution makes combination deep learning encoding tools traditional video encoding schemes efficient experimental results show approach achieves coding gain u v components random access ra configuration average,2,1.0,2,1.0
hyperseg towards universal visual segmentation large language model paper aims address universal segmentation image video perception strong reasoning ability empowered visual large language models vllms despite significant progress current unified segmentation methods limitations adaptation image video scenarios well complex reasoning segmentation make difficult handle various challenging instructions achieve accurate understanding finegrained visionlanguage correlations propose hyperseg first vllmbased universal segmentation model pixellevel image video perception encompassing generic segmentation tasks complex reasoning perception tasks requiring powerful reasoning abilities world knowledge besides fully leverage recognition capabilities vllms finegrained visual information hyperseg incorporates hybrid entity recognition finegrained visual perceiver modules various segmentation tasks combined temporal adapter hyperseg achieves comprehensive understanding temporal information experimental results validate effectiveness insights resolving universal image video segmentation tasks including complex reasoning perception tasks code available,-1,0.0,-1,0.0
towards universal soccer video understanding globally celebrated sport soccer attracted widespread interest fans world paper aims develop comprehensive multimodal framework soccer video understanding specifically make following contributions paper introduce largest multimodal soccer dataset date featuring videos detailed annotations complete matches automated annotation pipeline ii present advanced soccerspecific visual encoder matchvision leverages spatiotemporal information across soccer videos excels various downstream tasks iii conduct extensive experiments ablation studies event classification commentary generation multiview foul recognition matchvision demonstrates stateoftheart performance substantially outperforming existing models highlights superiority proposed data model believe work offer standard paradigm sports understanding research,-1,0.0,-1,0.0
lightweight stochastic video prediction via hybrid warping accurate video prediction deep neural networks especially dynamic regions challenging task computer vision critical applications autonomous driving remote working telemedicine due inherent uncertainties existing prediction models often struggle complexity motion dynamics occlusions paper propose novel stochastic longterm video prediction model focuses dynamic regions employing hybrid warping strategy integrating frames generated forward backward warpings approach effectively compensates weaknesses technique improving prediction accuracy realism moving regions videos also addressing uncertainty making stochastic predictions account various motions furthermore considering realtime predictions introduce mobilenetbased lightweight architecture model model called svphw achieves stateoftheart performance two benchmark datasets,-1,0.0,-1,0.0
dynamicvlm simple dynamic visual token compression videollm application large visionlanguage models lvlms analyzing images videos exciting rapidly evolving field recent years weve seen significant growth highquality imagetext datasets finetuning image understanding still lack comparable datasets videos additionally many videollms extensions singleimage vlms may efficiently handle complexities longer videos study introduce largescale synthetic dataset created proprietary models using carefully designed prompts tackle wide range questions also explore dynamic visual token compression architecture strikes balance computational efficiency performance proposed model achieves stateoftheart results across various video tasks shows impressive generalization setting new baselines multiimage understanding notably model delivers absolute improvement llavaonevision videomme muirbench codes available httpsgithubcomhonwongbytevideollm,-1,0.0,-1,0.0
color enhancement vpcc compressed point cloud via attribute map optimization videobased point cloud compression vpcc converts dynamic point cloud data video sequences using traditional video codecs efficient encoding however lossy compression scheme introduces artifacts degrade color attributes data paper introduces framework designed enhance color quality vpcc compressed point clouds propose lightweight decompression unet ldcunet neural network optimize projection maps generated vpcc encoding optimized maps backprojected space enhance corresponding point cloud attributes additionally introduce transfer learning strategy develop customized natural image dataset initial training model finetuned using projection maps compressed point clouds whole strategy effectively addresses scarcity point cloud training data experiments conducted public voxelized full bodies long sequences dataset demonstrate effectiveness proposed method improving color quality,-1,0.0,-1,0.0
hierarchical diffusion policy kinematicsaware multitask robotic manipulation paper introduces hierarchical diffusion policy hdp hierarchical agent multitask robotic manipulation hdp factorises manipulation policy hierarchical structure highlevel taskplanning agent predicts distant nextbest endeffector pose nbp lowlevel goalconditioned diffusion policy generates optimal motion trajectories factorised policy representation allows hdp tackle longhorizon task planning generating finegrained lowlevel actions generate contextaware motion trajectories satisfying robot kinematics constraints present novel kinematicsaware goalconditioned control agent robot kinematics diffuser rkdiffuser specifically rkdiffuser learns generate endeffector pose joint position trajectories distill accurate kinematicsunaware endeffector pose diffuser kinematicsaware less accurate joint position diffuser via differentiable kinematics empirically show hdp achieves significantly higher success rate stateoftheart methods simulation realworld,5,0.8356199034387872,5,0.8356199034387872
denoising diffusion probabilistic models six simple steps denoising diffusion probabilistic models ddpms popular class deep generative model successfully applied diverse range problems including image video generation protein material synthesis weather forecasting neural surrogates partial differential equations despite ubiquity hard find introduction ddpms simple comprehensive clean clear compact explanations necessary research papers able elucidate different design steps taken formulate ddpm rationale steps presented often omitted save space moreover expositions typically presented variational lower bound perspective unnecessary arguably harmful obfuscates method working suggests generalisations perform well practice hand perspectives take continuous timelimit beautiful general high barriertoentry require background knowledge stochastic differential equations probability flow note distill formulation ddpm six simple steps comes clear rationale assume reader familiar fundamental topics machine learning including basic probabilistic modelling gaussian distributions maximum likelihood estimation deep learning,13,1.0,13,1.0
differentially private medical image synthesis controllable latent diffusion models generally small size public medical imaging datasets coupled stringent privacy concerns hampers advancement datahungry deep learning models medical imaging study addresses challenges cardiac mri images shortaxis view propose latent diffusion models generate synthetic images conditioned medical attributes ensuring patient privacy differentially private model training knowledge first work apply quantify differential privacy medical image generation pretrain models public data finetune differential privacy uk biobank dataset experiments reveal pretraining significantly improves model performance achieving frechet inception distance fid compared models without pretraining additionally explore tradeoff privacy constraints image quality investigating tighter privacy budgets affect output controllability may lead degraded performance results demonstrate proper consideration training differential privacy substantially improve quality synthetic cardiac mri images still notable challenges achieving consistent medical realism,-1,0.0,-1,0.0
unconditional latent diffusion models memorize patient imaging data implications openly sharing synthetic data ai models present wide range applications field medicine however achieving optimal performance requires access extensive healthcare data often readily available furthermore imperative preserve patient privacy restricts patient data sharing third parties even within institutes recently generative ai models gaining traction facilitating opendata sharing proposing synthetic data surrogates real patient data despite promise models susceptible patient data memorization models generate patient data copies instead novel synthetic samples considering importance problem surprisingly received relatively little attention medical imaging community end assess memorization unconditional latent diffusion models train latent diffusion models ct mr xray datasets synthetic data generation detect amount training data memorized utilizing novel selfsupervised copy detection approach investigate various factors influence memorization findings show surprisingly high degree patient data memorization across datasets comparison nondiffusion generative models autoencoders generative adversarial networks indicates latent diffusion models susceptible memorization overall outperform nondiffusion models synthesis quality analyses reveal using augmentation strategies small architecture increasing dataset reduce memorization overtraining models enhance collectively results emphasize importance carefully training generative models private medical imaging datasets examining synthetic data ensure patient privacy sharing medical research applications,-1,0.0,-1,0.0
tracking virtual meetings wild reidentification multiparticipant virtual meetings recent years workplaces educational institutes widely adopted virtual meeting platforms led growing interest analyzing extracting insights meetings requires effective detection tracking unique individuals practice standardization video meetings recording layout captured across different platforms services turn creates challenge acquiring data stream analyzing uniform fashion approach provides solution general form video recording usually consisting grid participants creffigvideomeeting single video source metadata participant locations using least amount constraints assumptions data acquired conventional approaches often use yolo models coupled tracking algorithms assuming linear motion trajectories akin observed cctv footage however assumptions fall short virtual meetings participant video feed window abruptly change location across grid organic video meeting setting participants frequently join leave leading sudden nonlinear movements video grid disrupts optical flowbased tracking methods depend linear motion consequently standard object detection tracking methods might mistakenly assign multiple participants tracker paper introduce novel approach track reidentify participants remote video meetings utilizing spatiotemporal priors arising data domain turn increases tracking capabilities compared use general object tracking approach reduces error rate average compared yolobased tracking methods baseline,-1,0.0,-1,0.0
physmotion physicsgrounded dynamics single image introduce physmotion novel framework leverages principled physicsbased simulations guide intermediate representations generated single image input conditions eg applied force torque producing highquality physically plausible video generation utilizing continuum mechanicsbased simulations prior knowledge approach addresses limitations traditional datadriven generative models result consistent physically plausible motions framework begins reconstructing feedforward gaussian single image geometry optimization representation timestepped using differentiable material point method mpm continuum mechanicsbased elastoplasticity models provides strong foundation realistic dynamics albeit coarse level detail enhance geometry appearance ensure spatiotemporal consistency refine initial simulation using texttoimage diffusion model crossframe attention resulting physically plausible video retains intricate details comparable input image conduct comprehensive qualitative quantitative evaluations validate efficacy method project page available,-1,0.0,-1,0.0
federated voxel scene graph intracranial hemorrhage intracranial hemorrhage potentially lethal condition whose manifestation vastly diverse shifts across clinical centers worldwide deeplearningbased solutions starting model complex relations brain structures still struggle generalize gathering diverse data natural approach privacy regulations often limit sharing medical data propose first application federated scene graph generation show models leverage increased training data diversity scene graph generation recall clinically relevant relations across datasets compared models trained single centralized dataset learning structured data representation federated setting open way development new methods leverage finer information regularize across clients effectively,-1,0.0,-1,0.0
generating synthetic articulated posecontrollable cyclist data computer vision applications autonomous driving ad perception cyclists considered safetycritical scene objects commonly used publiclyavailable ad datasets typically contain large amounts car vehicle object instances low number cyclist instances usually limited appearance pose diversity cyclist training data scarcity problem limits generalization deeplearning perception models cyclist semantic segmentation pose estimation cyclist crossing intention prediction also limits research new cyclistrelated tasks finegrained cyclist pose estimation spatiotemporal analysis complex interactions humans articulated objects address data scarcity problem paper propose framework generate synthetic dynamic cyclist data assets used generate training data different tasks framework designed methodology creating new partbased multiview articulated synthetic bicycle dataset call use train gaussian splatting reconstruction image rendering method propose parametric bicycle composition model assemble posecontrollable bicycles finally using dynamic information cyclist videos build complete synthetic dynamic cyclist rider pedaling bicycle reposing selectable synthetic person automatically placing rider onto one new articulated bicycles using proposed keypoint optimizationbased inverse kinematics pose refinement present qualitative quantitative results compare generated cyclists recent stable diffusionbased method,-1,0.0,-1,0.0
empirical study excitation aggregation design adaptions videotext retrieval model transferred clip defactor standard solve video clip retrieval task framelevel input triggering surge models videotext retrieval domain work rethink inherent limitation widelyused mean pooling operation frame features aggregation investigate adaptions excitation aggregation design discriminative video representation generation present novel excitationandaggregation design including excitation module available capturing nonmutuallyexclusive relationships among frame features achieving framewise features recalibration aggregation module applied learn exclusiveness used frame representations aggregation similarly employ cascade sequential module aggregation design generate discriminative video representation sequential type besides adopt excitation design tight type obtain representative frame features multimodal interaction proposed modules evaluated three benchmark datasets msrvtt activitynet didemo achieving msrvtt activitynet didemo outperform results relative absolute improvements demonstrating superiority proposed excitation aggregation designs hope work serve alternative frame representations aggregation facilitate future research,-1,0.0,-1,0.0
video coding crosscomponent sample offset beyond exploration traditional spatial temporal subjective visual signal redundancy image video compression recent research focused leveraging crosscolor component redundancy enhance coding efficiency crosscomponent coding approaches motivated statistical correlations among different color components ycbcr color space luma color component typically exhibits finer details chroma cbcr color components inspired previous crosscomponent coding algorithms paper introduces novel inloop filtering approach named crosscomponent sample offset ccso ccso utilizes colocated neighboring luma samples generate correction signals luma chroma reconstructed samples multiplicationfree nonlinear mapping process implemented using lookuptable input mapping group reconstructed luma samples output offset value applied center luma colocated chroma sample experimental results demonstrate proposed ccso applied image video coding resulting improved coding efficiency visual quality method adopted experimental nextgeneration video codec beyond developed alliance open media aomedia achieving significant objective coding gains psnr vmaf quality metrics respectively random access configuration additionally ccso notably improves subjective visual quality,2,0.7222858061223736,2,0.7222858061223736
hallucination mitigation prompts longterm video understanding recently multimodal large language models made significant advancements video understanding tasks however ability understand unprocessed long videos limited primarily due difficulty supporting enormous memory overhead although existing methods achieve balance memory information aggregating frames inevitably introduce severe hallucination issue address issue paper constructs comprehensive hallucination mitigation pipeline based existing mllms specifically use clip score guide frame sampling process questions selecting key frames relevant question inject question information queries image qformer obtain important visual features finally answer generation stage utilize chainofthought incontext learning techniques explicitly control generation answers worth mentioning breakpoint mode found image understanding models achieved better results video understanding models therefore aggregated answers types models using comparison mechanism ultimately achieved global breakpoint modes respectively moviechat dataset surpassing official baseline model moreover proposed method third place cvpr loveu longterm video question answering challenge code avaiable,-1,0.0,-1,0.0
graph unfolding sampling transitory video summarization via gershgorin disc alignment usergenerated videos ugvs uploaded mobile phones social media sites like youtube tiktok short nonrepetitive summarize transitory ugv several keyframes linear time via fast graph sampling based gershgorin disc alignment gda specifically first model sequence n frames ugv mhop path graph mathcalgo n similarity two frames within time instants encoded positive edge based feature similarity towards efficient sampling unfold mathcalgo path graph mathcalg specified generalized graph laplacian matrix mathcall via one two graph unfolding procedures provable performance bounds show maximizing smallest eigenvalue coefficient matrix mathbfb textitdiagleftmathbfhright mu mathcall mathbfh binary keyframe selection vector equivalent minimizing worstcase signal reconstruction error maximize instead gershgorin circle theorem gct lower bound choosing mathbfh via new fast graph sampling algorithm iteratively aligns leftends gershgorin discs graph nodes frames extensive experiments multiple short video datasets show algorithm achieves comparable better video summarization performance compared stateoftheart methods substantially reduced complexity,-1,0.0,-1,0.0
videgothink assessing egocentric video understanding capabilities embodied ai recent advancements multimodal large language models mllms opened new avenues applications embodied ai building previous work egothink introduce videgothink comprehensive benchmark evaluating egocentric video understanding capabilities bridge gap mllms lowlevel control embodied ai design four key interrelated tasks video questionanswering hierarchy planning visual grounding reward modeling minimize manual annotation costs develop automatic data generation pipeline based dataset leveraging prior knowledge multimodal capabilities three human annotators filter generated data ensure diversity quality resulting videgothink benchmark conduct extensive experiments three types models apibased mllms opensource imagebased mllms opensource videobased mllms experimental results indicate mllms including perform poorly across tasks related egocentric video understanding findings suggest foundation models still require significant advancements effectively applied firstperson scenarios embodied ai conclusion videgothink reflects research trend towards employing mllms egocentric vision akin human capabilities enabling active observation interaction complex realworld environments,0,0.9654151521335638,0,0.9654151521335638
interpretable representation learning videos using nonlinear priors learning interpretable representations visual data important challenge make machines decisions understandable humans improve generalisation outside training distribution end propose deep learning framework one specify nonlinear priors videos eg newtonian physics allow model learn interpretable latent variables use generate videos hypothetical scenarios observed training time extending variational autoencoder vae prior simple isotropic gaussian arbitrary nonlinear temporal additive noise model anm describe large number processes eg newtonian physics propose novel linearization method constructs gaussian mixture model gmm approximating prior derive numerically stable monte carlo estimate kl divergence posterior prior gmms validate method different realworld physics videos including pendulum mass spring falling object pulsar rotating neutron star specify physical prior experiment show correct variables learned model trained intervene change different physical variables oscillation amplitude adding air drag generate physically correct videos hypothetical scenarios observed previously,5,0.2542313003037879,5,0.2542313003037879
towards unified method network dynamic via adversarial weighted link prediction network dynamic eg traffic burst data center networks channel fading cellular wifi networks great impact performance communication networks eg throughput capacity delay jitter article proposes unified predictionbased method handle dynamic various network systems view graph deep learning generally formulate dynamic prediction networks temporal link prediction task analyze possible challenges prediction weighted networks link weights widevaluerange sparsity issues inspired highresolution video frame prediction generative adversarial network gan try adopt adversarial learning generate highquality predicted snapshots network dynamic expected support precise finegrained network control novel highquality temporal link prediction hqtlp model gan developed illustrate potential basic idea extensive experiments various application scenarios demonstrate powerful capability hqtlp,2,1.0,2,1.0
stegogan leveraging steganography nonbijective imagetoimage translation imagetoimage translation models postulate unique correspondence exists semantic classes source target domains however assumption always hold realworld scenarios due divergent distributions different class sets asymmetrical information representation conventional gans attempt generate images match distribution target domain may hallucinate spurious instances classes absent source domain thereby diminishing usefulness reliability translated images cycleganbased methods also known hide mismatched information generated images bypass cycle consistency objectives process known steganography response challenge nonbijective image translation introduce stegogan novel model leverages steganography prevent spurious features generated images approach enhances semantic consistency translated images without requiring additional postprocessing supervision experimental evaluations demonstrate stegogan outperforms existing ganbased models across various nonbijective imagetoimage translation tasks qualitatively quantitatively code pretrained models accessible httpsgithubcomsianwusidistegogan,-1,0.0,-1,0.0
endtoend inceptionunet based generative adversarial networks snow rain removals superior performance introduced deep learning approaches removing atmospheric particles snow rain single image favors usage classical ones however deep learningbased approaches still suffer challenges related particle appearance characteristics size type transparency furthermore due unique characteristics rain snow particles single network based deep learning approaches struggle handling degradation scenarios simultaneously paper global framework consists two generative adversarial networks gans proposed handles removal particle individually architectures desnowing deraining gans introduce integration feature extraction phase classical unet generator network turn enhances removal performance presence severe variations size appearance furthermore realistic dataset contains pairs snowy images next groundtruth images estimated using lowrank approximation approach presented experiments show proposed desnowing deraining approaches achieve significant improvements comparison stateoftheart approaches tested synthetic realistic datasets,-1,0.0,-1,0.0
drivingsphere building highfidelity world closedloop simulation autonomous driving evaluation requires simulation environments closely replicate actual road conditions including realworld sensory data responsive feedback loops however many existing simulations need predict waypoints along fixed routes public datasets synthetic photorealistic data ie openloop simulation usually lacks ability assess dynamic decisionmaking recent efforts closedloop simulation offer feedbackdriven environments process visual sensor inputs produce outputs differ realworld data address challenges propose drivingsphere realistic closedloop simulation framework core idea build world representation generate reallife controllable driving scenarios specific framework includes dynamic environment composition module constructs detailed driving world format occupancy equipping static backgrounds dynamic objects visual scene synthesis module transforms data highfidelity multiview video outputs ensuring spatial temporal consistency providing dynamic realistic simulation environment drivingsphere enables comprehensive testing validation autonomous driving algorithms ultimately advancing development reliable autonomous cars benchmark publicly released,16,1.0,16,1.0
deep generative data assimilation multimodal setting robust integration physical knowledge data key improve computational simulations earth system models data assimilation crucial achieving goal provides systematic framework calibrate model outputs observations include remote sensing imagery ground station measurements uncertainty quantification conventional methods including kalman filters variational approaches inherently rely simplifying linear gaussian assumptions computationally expensive nevertheless rapid adoption datadriven methods many areas computational sciences see potential emulating traditional data assimilation deep learning especially generative models particular diffusionbased probabilistic framework large overlaps data assimilation principles allows conditional generation samples bayesian inverse framework models shown remarkable success textconditioned image generation imagecontrolled video synthesis likewise one frame data assimilation observationconditioned state calibration work propose slams scorebased latent assimilation multimodal setting specifically assimilate insitu weather station data exsitu satellite imagery calibrate vertical temperature profiles globally extensive ablation demonstrate slams robust even lowresolution noisy sparse data settings knowledge work first apply deep generative framework multimodal data assimilation using realworld datasets important step building robust computational simulators including nextgeneration earth system models code available httpsgithubcomyongquanquslams,-1,0.0,-1,0.0
csg contextsemantic guided diffusion approach de novo musculoskeletal ultrasound image generation use synthetic images medical imaging artificial intelligence ai solutions shown beneficial addressing limited availability diverse unbiased representative data despite extensive use synthetic image generation methods controlling semantics variability context details remains challenging limiting effectiveness producing diverse representative medical image datasets work introduce scalable semantic contextconditioned generative model coined csg contextsemantic guidance dual conditioning approach allows comprehensive control structure appearance advancing synthesis realistic diverse ultrasound images demonstrate ability csg generate findings pathological anomalies musculoskeletal msk ultrasound images moreover test quality synthetic images using threefold validation protocol results show synthetic images generated csg improve performance semantic segmentation models exhibit enhanced similarity real images compared baseline methods undistinguishable real images according turing test furthermore demonstrate extension csg allows enhancing variability space images synthetically generating augmentations anatomical geometries textures,3,0.9239669390512187,3,0.9239669390512187
videogui benchmark gui automation instructional videos graphical user interface gui automation holds significant promise enhancing human productivity assisting computer tasks existing task formulations primarily focus simple tasks specified single languageonly instruction insert new slide work introduce videogui novel multimodal benchmark designed evaluate gui assistants visualcentric gui tasks sourced highquality web instructional videos benchmark focuses tasks involving professional novel software eg adobe photoshop stable diffusion webui complex activities eg video editing videogui evaluates gui assistants hierarchical process allowing identification specific levels may fail highlevel planning reconstruct procedural subtasks visual conditions without language descriptions ii middlelevel planning generate sequences precise action narrations based visual state ie screenshot goals iii atomic action execution perform specific actions accurately clicking designated elements level design evaluation metrics across individual dimensions provide clear signals individual performance clicking dragging typing scrolling atomic action execution evaluation videogui reveals even sota large multimodal model performs poorly visualcentric gui tasks especially highlevel planning,-1,0.0,-1,0.0
exploration improvement nerfbased scene editing techniques nerfs highquality scene synthesis capability quickly accepted scholars years proposed significant progress made scene representation synthesis however high computational cost limits intuitive efficient editing scenes making nerfs development scene editing field facing many challenges paper reviews preliminary explorations scholars nerf scene object editing field recent years mainly changing shape texture scenes objects new synthesized scenes combination residual models gan transformer nerf generalization ability nerf scene editing expanded including realizing realtime new perspective editing feedback multimodal editing text synthesized scenes synthesis performance indepth exploration light shadow editing initially achieving optimization indirect touch editing detail representation complex scenes currently nerf editing methods focus touch points materials indirect points dealing complex larger scenes difficult balance accuracy breadth efficiency quality overcoming challenges may become direction future nerf scene editing technology,-1,0.0,-1,0.0
generative detail compensation via gan diffusion oneshot generalizable neural radiance fields paper focus oneshot novel view synthesis onvs task targets synthesizing photorealistic novel views given one reference image per scene previous oneshot generalizable neural radiance fields ognerf methods solve task inferencetime finetuningfree manner yet suffer blurry issue due encoderonly architecture highly relies limited reference image hand recent diffusionbased methods show vivid plausible results via distilling pretrained diffusion models representation yet require tedious perscene optimization targeting issues propose generative detail compensation framework via gan diffusion inferencetime finetuningfree vivid plausible details detail following coarsetofine strategy mainly composed onestage parallel pipeline opp detail enhancer coarse stage opp first efficiently inserts gan model existing ognerf pipeline primarily relieving blurry issue indistribution priors captured training dataset achieving good balance sharpness lpips fid fidelity psnr ssim fine stage leverages pretrained image diffusion models complement rich outdistribution details maintaining decent consistency extensive experiments synthetic realworld datasets show noticeably improves details without perscene finetuning,-1,0.0,-1,0.0
diffusion policy generalizable visuomotor policy learning via simple representations imitation learning provides efficient way teach robots dexterous skills however learning complex skills robustly generalizablely usually consumes large amounts human demonstrations tackle challenging problem present diffusion policy novel visual imitation learning approach incorporates power visual representations diffusion policies class conditional action generative models core design utilization compact visual representation extracted sparse point clouds efficient point encoder experiments involving simulation tasks successfully handles tasks demonstrations surpasses baselines relative improvement real robot tasks demonstrates precise control high success rate given demonstrations task shows excellent generalization abilities diverse aspects including space viewpoint appearance instance interestingly real robot experiments rarely violates safety requirements contrast baseline methods frequently necessitating human intervention extensive evaluation highlights critical importance representations realworld robot learning videos code data available,5,0.7692860798238623,5,0.7692860798238623
laplacianguided entropy model neural codec blurdissipated synthesis replacing gaussian decoders conditional diffusion model enhances perceptual quality reconstructions neural image compression lack inductive bias image data restricts ability achieve stateoftheart perceptual levels address limitation adopt nonisotropic diffusion model decoder side model imposes inductive bias aimed distinguishing frequency contents thereby facilitating generation highquality images moreover framework equipped novel entropy model accurately models probability distribution latent representation exploiting spatiochannel correlations latent space accelerating entropy decoding step channelwise entropy model leverages local global spatial contexts within channel chunk global spatial context built upon transformer specifically designed image compression tasks designed transformer employs laplacianshaped positional encoding learnable parameters adaptively adjusted channel cluster experiments demonstrate proposed framework yields better perceptual quality compared cuttingedge generativebased codecs proposed entropy model contributes notable bitrate savings,2,1.0,2,1.0
fetaldiffusion posecontrollable fetal mri synthesis conditional diffusion model quality fetal mri significantly affected unpredictable substantial fetal motion leading introduction artifacts even fast acquisition sequences employed development realtime fetal pose estimation approaches volumetric epi fetal mri opens promising avenue fetal motion monitoring prediction challenges arise fetal pose estimation due limited number real scanned fetal mr training images hindering model generalization acquired fetal mri lacks adequate pose study introduce fetaldiffusion novel approach utilizing conditional diffusion model generate synthetic fetal mri controllable pose additionally auxiliary poselevel loss adopted enhance model performance work demonstrates success proposed model producing highquality synthetic fetal mri images accurate recognizable fetal poses comparing favorably invivo real fetal mri furthermore show integration synthetic fetal mr images enhances fetal pose estimation models performance particularly number available real scanned data limited resulting increase pck reduced mean error experiments done single gpu method holds promise improving realtime tracking models thereby addressing fetal motion issues effectively,-1,0.0,-1,0.0
enhanced segmentation femoral bone metastasis ct scans patients using synthetic data generation diffusion models purpose bone metastasis major impact quality life patients diverse terms size location making segmentation complex manual segmentation timeconsuming expert segmentations subject operator variability makes obtaining accurate reproducible segmentations bone metastasis ctscans challenging yet important task achieve materials methods deep learning methods tackle segmentation tasks efficiently require large datasets along expert manual segmentations generalize new images propose automated data synthesis pipeline using denoising diffusion probabilistic models ddpm enchance segmentation femoral metastasis ctscan volumes patients used existing lesions along healthy femurs create new realistic synthetic metastatic images trained ddpm improve diversity realism simulated volumes also investigated operator variability manual segmentation results created new volumes trained unet segmentation models real synthetic data compare segmentation performance evaluated performance models depending amount synthetic data used training conclusion results showed segmentation models trained synthetic data outperformed trained real volumes models perform especially well considering operator variability,3,0.7510905142308243,3,0.7510905142308243
structureaware stylized image synthesis robust medical image segmentation accurate medical image segmentation essential effective diagnosis treatment planning often challenged domain shifts caused variations imaging devices acquisition conditions patientspecific attributes traditional domain generalization methods typically require inclusion parts test domain within training set always feasible clinical settings limited diverse data additionally although diffusion models demonstrated strong capabilities image generation style transfer often fail preserve critical structural information necessary precise medical analysis address issues propose novel medical image segmentation method combines diffusion models structurepreserving network structureaware oneshot image stylization approach effectively mitigates domain shifts transforming images various sources consistent style maintaining location size shape lesions ensures robust accurate segmentation even target domain absent training data experimental evaluations colonoscopy polyp segmentation skin lesion segmentation datasets show method enhances robustness accuracy segmentation models achieving superior performance metrics compared baseline models without style transfer structureaware stylization framework offers practical solution improving medical image segmentation across diverse domains facilitating reliable clinical diagnoses,3,0.9239669390512187,3,0.9239669390512187
adversarial augmentation training makes action recognition models robust realistic video distribution shifts despite recent advances video action recognition achieving strong performance existing benchmarks models often lack robustness faced natural distribution shifts training test data propose two novel evaluation methods assess model resilience distribution disparity one method uses two different datasets collected different sources uses one training validation testing precisely created dataset splits training testing using subset classes overlapping train test datasets proposed method extracts feature mean class target evaluation datasets training data ie class prototype estimates test video prediction cosine similarity score sample class prototypes target class procedure alter model weights using target dataset require aligning overlapping classes two different datasets thus efficient method test model robustness distribution shifts without prior knowledge target distribution address robustness problem adversarial augmentation training generating augmented views videos hard classification model applying gradient ascent augmentation parameters well curriculum scheduling strength video augmentations experimentally demonstrate superior performance proposed adversarial augmentation approach baselines across three stateoftheart action recognition models tsm video swin transformer uniformer presented work provides critical insight model robustness distribution shifts presents effective techniques enhance video action recognition performance realworld deployment,7,0.8170518315267903,7,0.8170518315267903
motion meets attention video motion prompts videos contain rich spatiotemporal information traditional methods extracting motion used tasks action recognition often rely visual contents rather precise motion features phenomenon referred blind motion extraction behavior proves inefficient capturing motions interest due lack motionguided cues recently attention mechanisms enhanced many computer vision tasks effectively highlighting salient visual areas inspired propose modified sigmoid function learnable slope shift parameters attention mechanism modulate motion signals frame differencing maps approach generates sequence attention maps enhance processing motionrelated video content ensure temporal continuity smoothness attention maps apply pairwise temporal attention variation regularization remove unwanted motions eg noise preserving important ones perform hadamard product pair attention maps original video frames highlight evolving motions interest time highlighted motions termed video motion prompts subsequently used inputs model instead original video frames formalize process motion prompt layer incorporate regularization term loss function learn better motion prompts layer serves adapter model video data bridging gap traditional blind motion extraction extraction relevant motions interest show lightweight plugandplay motion prompt layer seamlessly integrates models like slowfast timesformer enhancing performance benchmarks finegym mpii cooking,-1,0.0,-1,0.0
videoautoarena automated arena evaluating large multimodal models video analysis user simulation large multimodal models lmms advanced video analysis capabilities recently garnered significant attention however evaluations rely traditional methods like multiplechoice questions benchmarks videomme longvideobench prone lack depth needed capture complex demands realworld users address limitationand due prohibitive cost slow pace human annotation video taskswe introduce videoautoarena arenastyle benchmark inspired lmsys chatbot arenas framework designed automatically assess lmms video analysis abilities videoautoarena utilizes user simulation generate openended adaptive questions rigorously assess model performance video understanding benchmark features automated scalable evaluation framework incorporating modified elo rating system fair continuous comparisons across multiple lmms validate automated judging system construct gold standard using carefully curated subset human annotations demonstrating arena strongly aligns human judgment maintaining scalability additionally introduce faultdriven evolution strategy progressively increasing question complexity push models toward handling challenging video analysis scenarios experimental results demonstrate videoautoarena effectively differentiates among stateoftheart lmms providing insights model strengths areas improvement streamline evaluation introduce videoautobench auxiliary benchmark human annotators label winners subset videoautoarena battles use judge compare responses humanvalidated answers together videoautoarena videoautobench offer costeffective scalable framework evaluating lmms usercentric video analysis,0,1.0,0,1.0
hindi audiovideodeepfake havdf hindi languagebased audiovideo deepfake dataset deepfakes offer great potential innovation creativity also pose significant risks privacy trust security vast hindispeaking population india particularly vulnerable deepfakedriven misinformation campaigns fake videos speeches hindi enormous impact rural semiurban communities digital literacy tends lower people inclined trust video content development effective frameworks detection tools combat deepfake misuse requires highquality diverse extensive datasets existing popular datasets like ffdf faceforensics dfdc deepfake detection challenge based english language hence paper aims create first novel hindi deep fake dataset named hindi audiovideodeepfake havdf dataset generated using faceswap lipsyn voice cloning methods multistep process allows us create rich varied dataset captures nuances hindi speech facial expressions providing robust foundation training evaluating deepfake detection models hindi language context unique kind previous datasets contain either deepfake videos synthesized audio type deepfake dataset used training detector deepfake video audio datasets notably newly introduced havdf dataset demonstrates lower detection accuracys across existing detection methods like headpose etc compared wellknown datasets ffdf dfdc trend suggests havdf dataset presents deeper challenges detect possibly due focus hindi language content diverse manipulation techniques havdf dataset fills gap hindispecific deepfake datasets aiding multilingual deepfake detection development,4,1.0,4,1.0
instrugen automatic instruction generation visionandlanguage navigation via large multimodal models recent research visionandlanguage navigation vln indicates agents suffer poor generalization unseen environments due lack realistic training environments highquality pathinstruction pairs existing methods constructing realistic navigation scenes high costs extension instructions mainly relies predefined templates rules lacking adaptability alleviate issue propose instrugen vln pathinstruction pairs generation paradigm specifically use youtube house tour videos realistic navigation scenes leverage powerful visual understanding generation abilities large multimodal models lmms automatically generate diverse highquality vln pathinstruction pairs method generates navigation instructions different granularities achieves finegrained alignment instructions visual observations difficult achieve previous methods additionally design multistage verification mechanism reduce hallucinations inconsistency lmms experimental results demonstrate agents trained pathinstruction pairs generated instrugen achieves stateoftheart performance rxr benchmarks particularly unseen environments code available,5,0.27463957143403184,5,0.27463957143403184
cross group attention groupwise rolling multimodal medical image synthesis multimodal mr image synthesis aims generate missing modality image fusing mapping available mri data existing approaches typically adopt imagetoimage translation scheme however methods often suffer suboptimal performance due spatial misalignment different modalities typically treated input channels therefore paper propose adaptive groupwise interaction network aginet explores intermodality intramodality relationships multimodal mr image synthesis specifically groups first predefined along channel dimension perform adaptive rolling standard convolutional kernel capture intermodality spatial correspondences time crossgroup attention module introduced fuse information across different channel groups leading better feature representation evaluated effectiveness model publicly available ixi datasets aginet achieved stateoftheart performance multimodal mr image synthesis code released,3,0.8828735341537767,3,0.8828735341537767
modeling drivers risk perception via attention improve driving assistance advanced driver assistance systems adas alert drivers safetycritical scenarios often provide superfluous alerts due lack consideration drivers knowledge scene awareness modeling aspects together datadriven way challenging due scarcity critical scenario data incabin driver state world state recorded together explore benefits driver modeling context forward collision warning fcw systems working realworld video dataset onroad fcw deployments collect observers subjective validity rating deployed alerts also annotate participants gazetoobjects extract trajectories ego vehicle vehicles semiautomatically generate risk estimate scene drivers perception two step process first model movement vehicles given scenario joint trajectory forecasting problem reason drivers risk perception scene counterfactually modifying input forecasting model represent drivers actual observations vehicles scene difference behaviours gives us estimate driver behaviour accounts actual inattentive observations downstream effect overall scene risk compare learned scene representation well traditional worsecase deceleration model achieve future trajectory forecast experiments show using risk formulation generate fcw alerts may lead improved false positive rate fcws improved fcw timing,4,0.7791289004365893,4,0.7791289004365893
paired conditional generative adversarial network highly accelerated liver mri purpose mri high spatiotemporal resolution desired imageguided liver radiotherapy acquiring densely sampling kspace data timeconsuming accelerated acquisition sparse samples desirable often causes degraded image quality long reconstruction time propose reconstruct paired conditional generative adversarial network recongan shorten mri reconstruction time maintaining reconstruction quality methods patients underwent freebreathing liver mri included study fully retrospectively undersampled data times first reconstructed using nufft algorithm recongan trained input output pairs three types networks unet reconstruction swin transformer explored generators patchgan selected discriminator recongan processed data temporal slices total patients temporal slices split training patients slices test patients slices results recongan consistently achieved comparablebetter psnr ssim rmse scores compared csunet models inference time recongan unet cs gtv detection task showed recongan cs compared unet better improved dice score recongan cs unet unprocessed undersampled images conclusion generative network adversarial training proposed promising efficient reconstruction results demonstrated inhouse dataset rapid qualitative reconstruction liver mr potential facilitate online adaptive mrguided radiotherapy liver cancer,3,0.7674984799402238,3,0.7674984799402238
gghead fast generalizable gaussian heads learning head priors large image collections important step towards highquality human modeling core requirement efficient architecture scales well largescale datasets large image resolutions unfortunately existing gans struggle scale generate samples high resolutions due relatively slow train render speeds typically rely superresolution networks expense global consistency address challenges propose generative gaussian heads gghead adopts recent gaussian splatting representation within gan framework generate representation employ powerful cnn generator predict gaussian attributes uv space template head mesh way gghead exploits regularity templates uv layout substantially facilitating challenging task predicting unstructured set gaussians improve geometric fidelity generated representations novel total variation loss rendered uv coordinates intuitively regularization encourages neighboring rendered pixels stem neighboring gaussians templates uv space taken together pipeline efficiently generate heads trained singleview image observations proposed framework matches quality existing head gans ffhq substantially faster fully consistent result demonstrate realtime generation rendering highquality heads resolution first time project website httpstobiaskirschsteingithubiogghead,-1,0.0,-1,0.0
roboabc affordance generalization beyond categories via semantic correspondence robot manipulation enabling robotic manipulation generalizes outofdistribution scenes crucial step toward openworld embodied intelligence human beings ability rooted understanding semantic correspondence among objects naturally transfers interaction experience familiar objects novel ones although robots lack reservoir interaction experience vast availability human videos internet may serve valuable resource extract affordance memory including contact points inspired natural way humans think propose roboabc confronted unfamiliar objects require generalization robot acquire affordance retrieving objects share visual semantic similarities affordance memory next step map contact points retrieved objects new object establishing correspondence may present formidable challenges first glance recent research finds naturally arises pretrained diffusion models enabling affordance mapping even across disparate object categories roboabc framework robots may generalize manipulate outofcategory objects zeroshot manner without manual annotation additional training part segmentation precoded knowledge viewpoint restrictions quantitatively roboabc significantly enhances accuracy visual affordance retrieval large margin compared stateoftheart sota endtoend affordance models also conduct realworld experiments crosscategory objectgrasping tasks roboabc achieved success rate proving capacity realworld tasks,5,1.0,5,1.0
neural rendering hardware acceleration review neural rendering new image video generation method based deep learning combines deep learning model physical knowledge computer graphics obtain controllable realistic scene model realize control scene attributes lighting camera parameters posture one hand neural rendering make full use advantages deep learning accelerate traditional forward rendering process also provide new solutions specific tasks inverse rendering reconstruction hand design innovative hardware structures adapt neural rendering pipeline breaks parallel computing power consumption bottleneck existing graphics processors expected provide important support future key areas virtual augmented reality film television creation digital entertainment artificial intelligence metaverse paper review technical connotation main challenges research progress neural rendering basis analyze common requirements neural rendering pipeline hardware acceleration characteristics current hardware acceleration architecture discuss design challenges neural rendering processor architecture finally future development trend neural rendering processor architecture prospected,2,1.0,2,1.0
selfdrsc selfsupervised learning dual reversed rolling shutter correction modern consumer cameras commonly employ rolling shutter rs imaging mechanism via images captured scanning scenes rowbyrow resulting rs distortion dynamic scenes correct rs distortion existing methods adopt fully supervised learning manner requires high framerate global shutter gs images groundtruth supervision paper propose enhanced selfsupervised learning framework dual reversed rs distortion correction selfdrsc firstly introduce lightweight drsc network incorporates bidirectional correlation matching block refine joint optimization optical flows corrected rs features thereby improving correction performance reducing network parameters subsequently effectively train drsc network propose selfsupervised learning strategy ensures cycle consistency input reconstructed dual reversed rs images rs reconstruction selfdrsc interestingly formulated specialized instance video frame interpolation row reconstructed rs images interpolated predicted gs images utilizing rs distortion time maps achieving superior performance simplifying training process selfdrsc enables feasible onestage selfsupervised training additionally besides start end rs scanning time selfdrsc allows supervision gs images arbitrary intermediate scanning times thus enabling learned drsc network generate high framerate gs videos code trained models available,-1,0.0,-1,0.0
navigation world models navigation fundamental skill agents visualmotor capabilities introduce navigation world model nwm controllable video generation model predicts future visual observations based past observations navigation actions capture complex environment dynamics nwm employs conditional diffusion transformer cdit trained diverse collection egocentric videos human robotic agents scaled billion parameters familiar environments nwm plan navigation trajectories simulating evaluating whether achieve desired goal unlike supervised navigation policies fixed behavior nwm dynamically incorporate constraints planning experiments demonstrate effectiveness planning trajectories scratch ranking trajectories sampled external policy furthermore nwm leverages learned visual priors imagine trajectories unfamiliar environments single input image making flexible powerful tool nextgeneration navigation systems,5,0.3736349712479308,5,0.3736349712479308
drivinggpt unifying driving world modeling planning multimodal autoregressive transformers world modelbased searching planning widely recognized promising path toward humanlevel physical intelligence however current driving world models primarily rely video diffusion models specialize visual generation lack flexibility incorporate modalities like action contrast autoregressive transformers demonstrated exceptional capability modeling multimodal data work aims unify driving model simulation trajectory planning single sequence modeling problem introduce multimodal driving language based interleaved image action tokens develop drivinggpt learn joint world modeling planning standard nexttoken prediction drivinggpt demonstrates strong performance actionconditioned video generation endtoend planning outperforming strong baselines largescale nuplan navsim benchmarks,5,0.2509638446289208,5,0.2509638446289208
generating printready personalized ai art products minimal user inputs present novel framework advance generative artificial intelligence ai applications realm printed art products specifically addressing largeformat products require highresolution artworks framework consists pipeline addresses two major challenges domain high complexity generating effective prompts low native resolution images produced diffusion models integrating aienhanced prompt generations aipowered upscaling techniques framework efficiently produce highquality diverse artistic images suitable many new commercial use cases work represents significant step towards democratizing highquality ai art opening new avenues consumers artists designers businesses,10,1.0,10,1.0
multiscale texture loss ct denoising gans generative adversarial networks gans proved powerful framework denoising applications medical imaging however ganbased denoising algorithms still suffer limitations capturing complex relationships within images regard loss function plays crucial role guiding image generation process encompassing much synthetic image differs real image grasp highly complex nonlinear textural relationships training process work presents novel approach capture embed multiscale texture information loss function method introduces differentiable multiscale texture representation images dynamically aggregated selfattention layer thus exploiting endtoend gradientbased optimization validate approach carrying extensive experiments context lowdose ct denoising challenging application aims enhance quality noisy ct scans utilize three publicly available datasets including one simulated two real datasets results promising compared wellestablished loss functions also consistent across three different gan architectures code available httpsgithubcomtrainlaboratorymultiscaletexturelossmstlf,-1,0.0,-1,0.0
face cartoon incremental superresolution using knowledge distillation facial superresolutionhallucination important area research seeks enhance lowresolution facial images variety applications generative adversarial networks gans shown promise area ability adapt new unseen data remains challenge paper addresses problem proposing incremental superresolution using gans knowledge distillation isrkd face cartoon previous research area investigated incremental learning critical realworld applications new data continually generated proposed isrkd aims develop novel unified framework facial superresolution handle different settings including different types faces cartoon face various levels detail achieve ganbased superresolution network pretrained celeba dataset incrementally trained icartoonface dataset using knowledge distillation retain performance celeba test set improving performance icartoonface test set experiments demonstrate effectiveness knowledge distillation incrementally adding capability model cartoon face superresolution retaining learned knowledge facial hallucination tasks gans,-1,0.0,-1,0.0
oed towards onestage endtoend dynamic scene graph generation dynamic scene graph generation dsgg focuses identifying visual relationships within spatialtemporal domain videos conventional approaches often employ multistage pipelines typically consist object detection temporal association multirelation classification however methods exhibit inherent limitations due separation multiple stages independent optimization subproblems may yield suboptimal solutions remedy limitations propose onestage endtoend framework termed oed streamlines dsgg pipeline framework reformulates task set prediction problem leverages pairwise features represent subjectobject pair within scene graph moreover another challenge dsgg capturing temporal dependencies introduce progressively refined module prm aggregating temporal context without constraints additional trackers handcrafted trajectories enabling endtoend optimization network extensive experiments conducted action genome benchmark demonstrate effectiveness design code models available urlhttpsgithubcomguanwpkuoed,-1,0.0,-1,0.0
diffusion mri meets diffusion model novel deep generative model diffusion mri generation diffusion mri dmri advanced imaging technique characterizing tissue microstructure white matter structural connectivity human brain demand highquality dmri data growing driven need better resolution improved tissue contrast however acquiring highquality dmri data expensive timeconsuming context deep generative modeling emerges promising solution enhance image quality minimizing acquisition costs scanning time study propose novel generative approach perform dmri generation using deep diffusion models generate high dimension high resolution data preserving gradients information brain structure demonstrated method image mapping task aimed enhancing quality dmri images approach demonstrates highly enhanced performance generating dmri images compared current stateoftheart sota methods achievement underscores substantial progression enhancing dmri quality highlighting potential novel generative approach revolutionize dmri imaging standards,3,1.0,3,1.0
openvocabulary spatiotemporal action detection spatiotemporal action detection stad important finegrained video understanding task current methods require box label supervision action classes advance however realworld applications likely come across new action classes seen training action category space large hard enumerate also cost data annotation model training new classes extremely high traditional methods need perform detailed box annotations retrain whole network scratch paper propose new challenging setting performing openvocabulary stad better mimic situation action detection open world openvocabulary spatiotemporal action detection ovstad requires training model limited set base classes box label supervision expected yield good generalization performance novel action classes ovstad build two benchmarks based existing stad datasets propose simple effective method based pretrained videolanguage models vlm better adapt holistic vlm finegrained action detection task carefully finetune localized video regiontext pairs customized finetuning endows vlm better motion understanding thus contributing accurate alignment video regions texts local region feature global video feature fusion alignment adopted improve action detection performance providing global context method achieves promising performance novel classes,7,1.0,7,1.0
llavidal large language vision model daily activities living current large language vision models llvms trained web videos perform well general video understanding struggle finegrained details complex humanobject interactions hoi viewinvariant representation learning essential activities daily living adl limitation stems lack specialized adl video instructiontuning datasets insufficient modality integration capture discriminative action representations address propose semiautomated framework curating adl datasets creating adlx multiview multimodal rgbs instructiontuning dataset additionally introduce llavidal llvm integrating videos skeletons hois model adls complex spatiotemporal relationships training llavidal simple joint alignment modalities yields suboptimal results thus propose multimodal progressive mmpro training strategy incorporating modalities stages following curriculum also establish adl mcq video description benchmarks assess llvm performance adl tasks trained adlx llavidal achieves stateoftheart performance across adl benchmarks code data made publicly available httpsadlxgithubio,-1,0.0,-1,0.0
hpc hierarchical progressive coding framework volumetric video volumetric video based neural radiance field nerf holds vast potential various applications substantial data volume poses significant challenges compression transmission current nerf compression lacks flexibility adjust video quality bitrate within single model various network device capacities address issues propose hpc novel hierarchical progressive volumetric video coding framework achieving variable bitrate using single model specifically hpc introduces hierarchical representation multiresolution residual radiance field reduce temporal redundancy longduration sequences simultaneously generating various levels detail propose endtoend progressive learning approach multiratedistortion loss function jointly optimize hierarchical representation compression hpc trained realize multiple compression levels current methods need train multiple fixedbitrate models different ratedistortion rd tradeoffs extensive experiments demonstrate hpc achieves flexible quality levels variable bitrate single model exhibits competitive rd performance even outperforming fixedbitrate models across various datasets,2,1.0,2,1.0
zeroshot action localization via confidence large visionlanguage models precise action localization untrimmed video vital fields professional sports minimally invasive surgery delineation particular motions recordings dramatically enhance analysis many cases large scale datasets videolabel pairs localization unavailable limiting opportunity finetune videounderstanding models recent developments large visionlanguage models lvlm address need impressive zeroshot capabilities variety video understanding tasks however adaptation lvlms powerful visual question answering capabilities zeroshot localization longform video still relatively unexplored end introduce true zeroshot action localization method zeal specifically leverage builtin action knowledge large language model llm inflate actions detailed descriptions archetypal start end action descriptions serve queries lvlm generating framelevel confidence scores aggregated produce localization outputs simplicity flexibility method lends amenable capable lvlms developed demonstrate remarkable results zeroshot action localization challenging benchmark without training code publicly available hrefhttpsgithubcomjosaklilaizealgithubcomjosaklilaizeal,0,0.8865815669138393,0,0.8865815669138393
identitydriven multimedia forgery detection via reference assistance recent advancements deepfake techniques paved way generating various media forgeries response potential hazards media forgeries many researchers engage exploring detection methods increasing demand highquality media forgery datasets despite existing datasets certain limitations firstly datasets focus manipulating visual modality usually lack diversity forgery approaches considered secondly quality media often inadequate clarity naturalness meanwhile size dataset also limited thirdly commonly observed realworld forgeries motivated identity yet identity information individuals portrayed forgeries within existing datasets remains underexplored detection identity information could essential clue boost performance moreover official media concerning relevant identities internet serve prior knowledge aiding audience forgery detectors determining true identity therefore propose identitydriven multimedia forgery dataset idforge contains video shots sourced wild videos celebrities collected internet fake video shots involve types manipulation across visual audio textual modalities additionally idforge provides extra real video shots reference set celebrities correspondingly propose referenceassisted multimodal forgery detection network rmfdn aiming detection deepfake videos extensive experiments proposed dataset demonstrate effectiveness rmfdn multimedia detection task,4,1.0,4,1.0
digital twinbased network management better qoe multicast short video streaming multicast short video streaming enhance bandwidth utilization enabling simultaneous video transmission multiple users shared wireless channels existing network management schemes mainly rely sequential buffering principle general quality experience qoe model may deteriorate qoe users swipe behaviors exhibit distinct spatiotemporal variation paper propose digital twin dtbased network management scheme enhance qoe firstly user status emulated dt utilized estimate transmission capabilities watching probability distributions submulticast groups smgs adaptive segment buffering smgs buffers aligned unique virtual buffers managed dt finegrained buffer update multicast qoe model consisting rebuffering time video quality quality variation developed considering mutual influence segment buffering among smgs finally joint optimization problem segment version selection slot division formulated maximize qoe efficiently solve problem datamodeldriven algorithm proposed integrating convex optimization method deep reinforcement learning algorithm simulation results based realworld dataset demonstrate proposed dtbased network management scheme outperforms benchmark schemes terms qoe improvement,2,1.0,2,1.0
crema generalizable efficient videolanguage reasoning via multimodal modular fusion despite impressive advancements recent multimodal reasoning approaches still limited flexibility efficiency models typically process fixed modality inputs require updates numerous parameters paper tackles critical challenges proposes crema generalizable highly efficient modular modalityfusion framework incorporate new modality enhance video reasoning first augment multiple informative modalities optical flow point cloud audio thermal heatmap touch map given videos without extra human annotation leveraging sensors existing pretrained models next introduce query transformer multiple parameterefficient modules associated accessible modality projects diverse modality features llm token embedding space allowing model integrate different data types response generation furthermore propose novel progressive multimodal fusion design supported lightweight fusion module modalitysequential training strategy helps compress information across various assisting modalities maintaining computational efficiency llm improving performance validate method videolanguage reasoning tasks assisted diverse modalities including conventional videoqa qa achieve betterequivalent performance strong multimodal llms including onellm sevila reducing trainable parameters provide extensive analyses crema including impact modality reasoning domains design fusion module example visualizations,-1,0.0,-1,0.0
lave llmpowered agent assistance language augmentation video editing video creation become increasingly popular yet expertise effort required editing often pose barriers beginners paper explore integration large language models llms video editing workflow reduce barriers design vision embodied lave novel system provides llmpowered agent assistance languageaugmented editing features lave automatically generates language descriptions users footage serving foundation enabling llm process videos assist editing tasks user provides editing objectives agent plans executes relevant actions fulfill moreover lave allows users edit videos either agent direct ui manipulation providing flexibility enabling manual refinement agent actions user study included eight participants ranging novices proficient editors demonstrated laves effectiveness results also shed light user perceptions proposed llmassisted editing paradigm impact users creativity sense cocreation based findings propose design implications inform future development agentassisted content editing,15,1.0,15,1.0
learn suspected anomalies event prompts video anomaly detection models weakly supervised video anomaly detection wsvad rely multiple instance learning aiming distinguish normal abnormal snippets without specifying type anomaly however ambiguous nature anomaly definitions across contexts may introduce inaccuracy discriminating abnormal normal events show model anomalous novel framework proposed guide learning suspected anomalies event prompts given textual prompt dictionary potential anomaly events captions generated anomaly videos semantic anomaly similarity could calculated identify suspected events video snippet enables new multiprompt learning process constrain visualsemantic features across videos well provides new way label pseudo anomalies selftraining demonstrate effectiveness comprehensive experiments detailed ablation studies conducted four datasets namely xdviolence ucfcrime tad shanghaitech proposed model outperforms stateoftheart methods terms ap auc furthermore shows promising performance openset crossdataset cases data code models found urlhttpsgithubcomshiwoazlap,-1,0.0,-1,0.0
hdrflow realtime hdr video reconstruction large motions reconstructing high dynamic range hdr video image sequences captured alternating exposures challenging especially presence large camera object motion existing methods typically align low dynamic range sequences using optical flow attention mechanism deghosting however often struggle handle large complex motions computationally expensive address challenges propose robust efficient flow estimator tailored realtime hdr video reconstruction named hdrflow hdrflow three novel designs hdrdomain alignment loss haloss efficient flow network multisize large kernel mlk new hdr flow training scheme haloss supervises flow network learn hdroriented flow accurate alignment saturated dark regions mlk effectively model large motions negligible cost addition incorporate synthetic data sintel training dataset utilizing provided forward flow backward flow generated us supervise flow network enhancing performance large motion regions extensive experiments demonstrate hdrflow outperforms previous methods standard benchmarks best knowledge hdrflow first realtime hdr video reconstruction method video sequences captured alternating exposures capable processing resolution inputs,-1,0.0,-1,0.0
unsupervised modalitytransferable video highlight detection representation activation sequence learning identifying highlight moments raw video materials crucial improving efficiency editing videos pervasive internet platforms however extensive work manually labeling footage created obstacles applying supervised methods videos unseen categories absence audio modality contains valuable cues highlight detection many videos also makes difficult use multimodal strategies paper propose novel model crossmodal perception unsupervised highlight detection proposed model learns representations visualaudio level semantics imageaudio pair data via selfreconstruction task achieve unsupervised highlight detection investigate latent representations network propose representation activation sequence learning rasl module kpoint contrastive learning learn significant representation activations connect visual modality audio modality use symmetric contrastive learning scl module learn paired visual audio representations furthermore auxiliary task masked feature vector sequence fvs reconstruction simultaneously conducted pretraining representation enhancement inference crossmodal pretrained model generate representations paired visualaudio semantics given visual modality rasl module used output highlight scores experimental results show proposed framework achieves superior performance compared stateoftheart approaches,-1,0.0,-1,0.0
towards multimodal video paragraph captioning models robust missing modality video paragraph captioning vpc involves generating detailed narratives long videos utilizing supportive modalities speech event boundaries however existing models constrained assumption constant availability single auxiliary modality impractical given diversity unpredictable nature realworld scenarios end propose missingresistant framework mrvpc effectively harnesses available auxiliary inputs maintains resilience even absence certain modalities framework propose multimodal vpc mvpc architecture integrating video speech event boundary inputs unified manner process various auxiliary inputs moreover fortify model incomplete data introduce dropam data augmentation strategy randomly omits auxiliary inputs paired distillam regularization target distills knowledge teacher models trained modalitycomplete data enabling efficient learning modalitydeficient environments exhaustive experimentation activitynet captions mrvpc proven deliver superior performance modalitycomplete modalitymissing test data work highlights significance developing resilient vpc models paves way adaptive robust multimodal video understanding,-1,0.0,-1,0.0
zeroshot promptbased video encoder surgical gesture recognition purpose order produce surgical gesture recognition system support wide variety procedures either large annotated dataset must acquired fitted models must generalize new labels called zeroshot capability paper investigate feasibility latter option methods leveraging bridgeprompt framework prompttune pretrained visiontext model clip gesture recognition surgical videos utilize extensive outside video data text also make use label metadata weakly supervised contrastive losses results experiments show promptbased video encoder outperforms standard encoders surgical gesture recognition tasks notably displays strong performance zeroshot scenarios gesturestasks provided encoder training phase included prediction phase additionally measure benefit inclusion text descriptions feature extractor training schema conclusion bridgeprompt similar pretrainedprompttuned video encoder models present significant visual representation surgical robotics especially gesture recognition tasks given diverse range surgical tasks gestures ability models zeroshot transfer without need task gesture specific retraining makes invaluable,-1,0.0,-1,0.0
simultaneous detection interaction reasoning objectcentric action recognition interactions human objects important recognizing objectcentric actions existing methods usually adopt twostage pipeline object proposals first detected using pretrained detector fed action recognition model extracting video features learning object relations action recognition however since action prior unknown object detection stage important objects could easily overlooked leading inferior action recognition performance paper propose endtoend objectcentric action recognition framework simultaneously performs detection interaction reasoning one stage particularly extracting video features base network create three modules concurrent object detection interaction reasoning first patchbased object decoder generates proposals video patch tokens interactive object refining aggregation identifies important objects action recognition adjusts proposal scores based position appearance aggregates objectlevel info global video representation lastly object relation modeling module encodes object relations three modules together video feature extractor trained jointly endtoend fashion thus avoiding heavy reliance offtheshelf object detector reducing multistage training burden conduct experiments two datasets somethingelse ikeaassembly evaluate performance proposed approach conventional compositional fewshot action recognition tasks indepth experimental analysis show crucial role interactive objects learning action recognition outperform stateoftheart methods datasets,-1,0.0,-1,0.0
sinc adaptive camerabased vitals unsupervised learning periodic signals subtle periodic signals blood volume pulse respiration extracted rgb video enabling noncontact health monitoring low cost advancements remote pulse estimation remote photoplethysmography rppg currently driven deep learning solutions however modern approaches trained evaluated benchmark datasets ground truth contactppg sensors present first noncontrastive unsupervised learning framework signal regression mitigate need labelled video data minimal assumptions periodicity finite bandwidth approach discovers blood volume pulse directly unlabelled videos find encouraging sparse power spectra within normal physiological bandlimits variance batches power spectra sufficient learning visual features periodic signals perform first experiments utilizing unlabelled video data specifically created rppg train robust pulse rate estimators given limited inductive biases successfully applied approach camerabased respiration changing bandlimits target signal shows approach general enough unsupervised learning bandlimited quasiperiodic signals different domains furthermore show framework effective finetuning models unlabelled video single subject allowing personalized adaptive signal regressors,-1,0.0,-1,0.0
onestage openvocabulary temporal action detection leveraging temporal multiscale action label features openvocabulary temporal action detection openvocab tad advanced video analysis approach expands closedvocabulary temporal action detection closedvocab tad capabilities closedvocab tad typically confined localizing classifying actions based predefined set categories contrast openvocab tad goes limited predefined categories particularly useful realworld scenarios variety actions videos vast always predictable prevalent methods openvocab tad typically employ approach involves generating action proposals identifying actions however errors made first stage adversely affect subsequent action identification accuracy additionally existing studies face challenges handling actions different durations owing use fixed temporal processing methods therefore propose approach consisting two primary modules multiscale video analysis mva videotext alignment vta mva module captures actions varying temporal resolutions overcoming challenge detecting actions diverse durations vta module leverages synergy visual textual modalities precisely align video segments corresponding action labels critical step accurate action identification openvocab scenarios evaluations widely recognized datasets showed proposed method achieved superior results compared methods openvocab closedvocab settings serves strong demonstration effectiveness proposed method tad task,7,1.0,7,1.0
anticipation perfect deepfake identityanchored artifactagnostic detection rebalanced deepfake detection protocol deep generative models advance anticipate deepfakes achieving perfectiongenerating discernible artifacts noise however current deepfake detectors intentionally inadvertently rely artifacts detection exclusive deepfakes absent genuine examples bridge gap introduce rebalanced deepfake detection protocol rddp stresstest detectors balanced scenarios genuine forged examples bear similar artifacts offer two rddp variants rddpwhitehat uses whitehat deepfake algorithms create selfdeepfakes genuine portrait videos resemblance underlying identity yet carry similar artifacts deepfake videos rddpsurrogate employs surrogate functions eg gaussian noise process genuine forged examples introducing equivalent noise thereby sidestepping need deepfake algorithms towards detecting perfect deepfake videos aligns genuine ones present idminer detector identifies puppeteer behind disguise focusing motion artifacts appearances identitybased detector authenticates videos comparing reference footage equipped artifactagnostic loss framelevel identityanchored loss videolevel idminer effectively singles identity signals amidst distracting variations extensive experiments comparing idminer baseline detectors conventional rddp evaluations two deepfake datasets along additional qualitative studies affirm superiority method necessity detectors designed counter perfect deepfakes,4,0.8942521781377731,4,0.8942521781377731
egocentric videolanguage models truly understand handobject interactions egocentric videolanguage pretraining crucial step advancing understanding handobject interactions firstperson scenarios despite successes existing testbeds find current egovlms easily misled simple modifications changing verbs nouns interaction descriptions models struggling distinguish changes raises question egovlms truly understand handobject interactions address question introduce benchmark called egohoibench revealing performance limitation current egocentric models confronted challenges attribute performance gap insufficient finegrained supervision greater difficulty egovlms experience recognizing verbs compared nouns tackle issues propose novel asymmetric contrastive objective named egonce videototext objective enhance text supervision generating negative captions using large language models leveraging pretrained vocabulary hoirelated word substitutions texttovideo objective focus preserving objectcentric feature space clusters video representations based shared nouns extensive experiments demonstrate egonce significantly enhances egohoi understanding leading improved performance across various egovlms tasks multiinstance retrieval action recognition temporal understanding code available httpsgithubcomxuboshenegoncepp,-1,0.0,-1,0.0
ptmvqa efficient video quality assessment leveraging diverse pretrained models wild video quality assessment vqa challenging problem due numerous factors affect perceptual quality video eg content attractiveness distortion type motion pattern level however annotating mean opinion score mos videos expensive timeconsuming limits scale vqa datasets poses significant obstacle deep learningbased methods paper propose vqa method named ptmvqa leverages pretrained models transfer knowledge models pretrained various pretasks enabling benefits vqa different aspects specifically extract features videos different pretrained models frozen weights integrate generate representation since models possess various fields knowledge often trained labels irrelevant quality propose intraconsistency interdivisibility icid loss impose constraints features extracted multiple pretrained models intraconsistency constraint ensures features extracted different pretrained models unified qualityaware latent space interdivisibility introduces pseudo clusters based annotation samples tries separate features samples different clusters furthermore constantly growing number pretrained models crucial determine models use use address problem propose efficient scheme select suitable candidates models better clustering performance vqa datasets chosen candidates extensive experiments demonstrate effectiveness proposed method,-1,0.0,-1,0.0
differentiable task graph learning procedural activity representation online mistake detection egocentric videos procedural activities sequences keysteps aimed achieving specific goals crucial build intelligent agents able assist users effectively context task graphs emerged humanunderstandable representation procedural activities encoding partial ordering keysteps previous works generally relied handcrafted procedures extract task graphs videos paper propose approach based direct maximum likelihood optimization edges weights allows gradientbased learning task graphs naturally plugged neural network architectures experiments dataset demonstrate ability approach predict accurate task graphs observation action sequences improvement previous approaches owing differentiability proposed framework also introduce featurebased approach aiming predict task graphs keystep textual video embeddings observe emerging video understanding abilities task graphs learned approach also shown significantly enhance online mistake detection procedural egocentric videos achieving notable gains epictento datasets code replicating experiments available httpsgithubcomfpviplabdifferentiabletaskgraphlearning,7,0.9126867752823099,7,0.9126867752823099
large gaussian reconstruction model present first large reconstruction model produces animated objects singleview video input single feedforward pass takes second key success novel dataset multiview videos containing curated rendered animated objects objaverse dataset depicts diverse objects animations rendered viewpoints resulting videos total frames keep simple scalability build directly top lgm pretrained large reconstruction model outputs gaussian ellipsoids multiview image input outputs perframe gaussian splatting representation video frames sampled low fps upsamples representation higher fps achieve temporal smoothness add temporal selfattention layers base lgm help learn consistency across time utilize pertimestep multiview rendering loss train model representation upsampled higher framerate training interpolation model produces intermediate gaussian representations showcase trained synthetic data generalizes extremely well inthewild videos producing high quality animated assets,1,1.0,1,1.0
investigating video reasoning capability large language models tropes movies large language models llms demonstrated effectiveness language tasks also video reasoning paper introduces novel dataset tropes movies tim designed testbed exploring two critical yet previously overlooked video reasoning skills abstract perception understanding tokenizing abstract concepts videos longrange compositional reasoning planning integrating intermediate reasoning steps understanding longrange videos numerous frames utilizing tropes movie storytelling tim evaluates reasoning capabilities stateoftheart llmbased approaches experiments show current methods including captionerreasoner large multimodal model instruction finetuning visual programming marginally outperform random baseline tackling challenges abstract perception longrange compositional reasoning address deficiencies propose faceenhanced viper role interactions fevori context query reduction conquer enhance visual programming fostering role interaction awareness progressively refining movie contexts trope queries reasoning processes significantly improving performance points however performance still lags behind human levels vs additionally introduce new protocol evaluate necessity abstract perception longrange compositional reasoning task resolution done analyzing code generated visual programming using abstract syntax tree ast thereby confirming increased complexity tim dataset code available,0,0.9816742749656794,0,0.9816742749656794
pravic probabilistic adaptation framework realtime video classification video processing generally divided two main categories processing entire video typically yields optimal classification outcomes realtime processing objective make decision promptly possible latter often driven need identify rapidly potential critical dangerous situations could include machine failure traffic accidents heart problems dangerous behavior although models dedicated processing entire videos typically welldefined clearly presented literature case online processing plethora handdevised methods exist address present novel unified theoreticallybased adaptation framework dealing online classification problem video data initial phase study establish robust mathematical foundation theory classification sequential data potential make decision early stage allows us construct natural function encourages model return outcome much faster subsequent phase demonstrate straightforward readily implementable method adapting offline models online recurrent operations finally comparing proposed approach nononline stateoftheart baseline demonstrated use encourages network make earlier classification decisions without compromising accuracy,4,0.774140257437633,4,0.774140257437633
groprompt efficient grounded prompting adaptation referring video object segmentation referring video object segmentation rvos aims segment object referred query sentence throughout entire video existing methods require endtoend training dense mask annotations could computationconsuming less scalable work aim efficiently adapt foundation segmentation models addressing rvos weak supervision proposed grounded prompting groprompt framework specifically propose textaware prompt contrastive learning tapcl enhance association position prompts referring sentences box supervisions including textcontrastive prompt learning textcon modalitycontrastive prompt learning modalcon frame level video level respectively proposed tapcl groprompt framework generate temporalconsistent yet textaware position prompts describing locations movements referred object video experimental results standard rvos benchmarks refyoutubevos jhmdbsentences demonstrate competitive performance proposed groprompt framework given bounding box weak supervisions,7,0.9862345609730642,7,0.9862345609730642
unleashing potential tracklets unsupervised video person reidentification rich temporalspatial information videobased person reidentification methods shown broad prospects although tracklets easily obtained readymade tracking models annotating identities still expensive impractical therefore videobased methods propose using identity annotations camera labels facilitate feature learning also simply average frame features tracklet overlooking unexpected variations inherent identity consistency within tracklets paper propose selfsupervised refined clustering ssrc framework without relying annotation auxiliary information promote unsupervised video person reidentification specifically first propose noisefiltered tracklet partition nftp module reduce feature bias tracklets caused noisy tracking results sequentially partition noisefiltered tracklets subtracklets cluster merge subtracklets using selfsupervised signal tracklet partition enhanced progressive strategy generate reliable pseudo labels facilitating intraclass crosstracklet aggregation moreover propose class smoothing classification csc loss efficiently promote model learning extensive experiments mars dukemtmcvideoreid datasets demonstrate proposed ssrc unsupervised video person reidentification achieves stateoftheart results comparable advanced supervised methods,7,0.8139374706007187,7,0.8139374706007187
towards timely video analytics services network edge realtime video analytics services aim provide users accurate recognition results timely however existing studies usually fall dilemma reducing delay improving accuracy edge computing scenario imposes strict transmission computation resource constraints making balancing conflicting metrics dynamic network conditions difficult regard introduce age processed information aopi concept quantifies time elapsed since generation latest accurately recognized frame aopi depicts integrated impact recognition accuracy transmission computation efficiency derive closedform expressions aopi preemptive nonpreemptive computation scheduling policies wrt transmissioncomputation rate recognition accuracy video frames investigate joint problem edge server selection video configuration adaptation bandwidthcomputation resource allocation minimize longterm average aopi cameras propose online method ie lyapunovbased block coordinate descent lbcd solve problem decouples original problem two subproblems optimize video configurationresource allocation edge server selection strategy separately prove lbcd achieves asymptotically optimal performance according testbed experiments simulation results lbcd reduces average aopi compared stateoftheart baselines,2,1.0,2,1.0
long context transfer language vision video sequences offer valuable temporal information existing large multimodal models lmms fall short understanding extremely long videos many works address reducing number visual tokens using visual resamplers alternatively paper approach problem perspective language model simply extrapolating context length language backbone enable lmms comprehend orders magnitude visual tokens without video training call phenomenon long context transfer carefully ablate properties effectively measure lmms ability generalize long contexts vision modality develop vniah visual needleinahaystack purely synthetic long vision benchmark inspired language models niah test proposed long video assistant longva process frames visual tokens without additional complexities extended context length longva achieves stateoftheart performance videomme among models densely sampling input frames work opensourced httpsgithubcomevolvinglmmslablongva,0,0.9509972860623914,0,0.9509972860623914
bviaom new training dataset deep video compression optimization deep learning playing important role enhancing performance conventional hybrid video codecs learningbased methods typically require diverse representative training material optimization order achieve model generalization optimal coding performance however existing datasets either offer limited content variability come restricted licensing terms constraining use research purposes address issues propose new training dataset named bviaom contains uncompressed sequences various resolutions covering wide range content texture types dataset comes flexible licensing terms offers competitive performance used training set optimizing deep video coding tools experimental results demonstrate used training set optimize two popular network architectures two different coding tools proposed dataset leads additional bitrate savings percentage points terms psnry vmaf respectively compared existing training dataset bvidvc widely used deep video coding bviaom dataset available httpsgithubcomfanaaronzhangbviaom,-1,0.0,-1,0.0
sam robotic surgery empirical evaluation robustness generalization surgical video segmentation recent segment anything model sam demonstrated remarkable foundational competence semantic segmentation memory mechanism mask decoder addressing challenges video tracking object occlusion thereby achieving superior results interactive segmentation images videos building upon previous empirical studies explore zeroshot segmentation performance sam robotassisted surgery based prompts alongside robustness realworld corruption static images employ two forms prompts bounding box video sequences prompt applied initial frame extensive experimentation miccai endovis endovis benchmarks sam utilizing bounding box prompts outperforms stateoftheart sota methods comparative evaluations results point prompts also exhibit substantial enhancement sams capabilities nearing even surpassing existing unprompted sota methodologies besides sam demonstrates improved inference speed less performance degradation various image corruption although slightly unsatisfactory results remain specific edges regions sam robust adaptability prompts underscores potential downstream surgical tasks limited prompt requirements,-1,0.0,-1,0.0
breast tumor classification based selfsupervised contrastive learning ultrasound videos background breast ultrasound prominently used diagnosing breast tumors present many automatic systems based deep learning developed help radiologists diagnosis however training systems remains challenging usually datahungry demand amounts labeled data need professional knowledge expensive methods adopted triplet network selfsupervised contrastive learning technique learn representations unlabeled breast ultrasound video clips designed new hard triplet loss learn representations particularly discriminate positive negative image pairs hard recognize also constructed pretraining dataset breast ultrasound videos videos patients includes anchor sample dataset images positive sample dataset images negative sample dataset dynamically generated video clips constructed finetuning dataset including images patients transferred pretrained network downstream benignmalignant classification task compared performance stateoftheart models including three models pretrained imagenet previous contrastive learning model retrained datasets results conclusion experiments revealed model achieved area receiver operating characteristic curve auc significantly higher others assessed dependence pretrained model number labeled data revealed samples required achieve auc proposed framework greatly reduces demand labeled data holds potential use automatic breast ultrasound image diagnosis,3,0.5185714218244637,3,0.5185714218244637
visual language models image video understanding beginning visualglm cogvlm continuously exploring vlms pursuit enhanced visionlanguage fusion efficient higherresolution architecture broader modalities applications propose family new generation visual language models image video understanding including image understanding model inherits visual expert architecture improved training recipes pretraining posttraining stages supporting input resolution times pixels video understanding model integrates multiframe input timestamps proposes automated temporal grounding data construction notably family achieved stateoftheart results benchmarks like mmbench mmvet textvqa mvbench vcgbench models opensourced contributing advancement field,0,1.0,0,1.0
ladtalk latent denoising synthesizing talking head videos high frequency details audiodriven talking head generation pivotal area within filmmaking virtual reality although existing methods made significant strides following endtoend paradigm still encounter challenges producing videos highfrequency details due limited expressivity domain limitation prompted us explore effective postprocessing approach synthesize photorealistic talking head videos specifically employ pretrained model foundation model leveraging robust audiolip alignment capabilities drawing theory lipschitz continuity theoretically established noise robustness vector quantised auto encoders vqaes experiments demonstrate highfrequency texture deficiency foundation model temporally consistently recovered spaceoptimised vector quantised auto encoder sovqae introduced thereby facilitating creation realistic talking head videos conduct experiments conventional dataset highfrequency talking head hftk dataset curated results indicate method ladtalk achieves new stateoftheart video quality outofdomain lip synchronization performance,6,1.0,6,1.0
vimguard novel multimodal system video misinformation guarding rise social media shortform video sfv facilitated breeding ground misinformation emergence large language models significant research gone curbing misinformation problem automatic false claim detection text unfortunately automatic detection misinformation sfv complex problem remains largely unstudied text samples monomodal containing words sfvs comprise three different modalities words visuals nonlinguistic audio work introduce video masked autoencoders misinformation guarding vimguard first deeplearning architecture capable factchecking sfv analysis three constituent modalities vimguard leverages dualcomponent system first video audio masked autoencoders analyze visual nonlinguistic audio elements video discern intention specifically whether intends make informative claim deemed sfv informative intent passed second component retrieval augmented generation system validates factual accuracy spoken words evaluation vimguard outperformed three cuttingedge factcheckers thus setting new standard sfv factchecking marking significant stride toward trustworthy news social platforms promote testing iteration vimguard deployed chrome extension code opensourced github,0,0.8443274919910433,0,0.8443274919910433
tomato assessing visual temporal reasoning capabilities multimodal foundation models existing benchmarks often highlight remarkable performance achieved stateoftheart multimodal foundation models mfms leveraging temporal context video understanding however well models truly perform visual temporal reasoning study existing benchmarks shows capability mfms likely overestimated many questions solved using single outoforder frames systematically examine current visual temporal reasoning tasks propose three principles corresponding metrics multiframe gain frame order sensitivity frame information disparity following principles introduce tomato temporal reasoning multimodal evaluation novel benchmark crafted rigorously assess mfms temporal reasoning capabilities video understanding tomato comprises carefully curated humanannotated questions spanning six tasks ie action count direction rotation shape trend velocity frequency visual cues applied videos including selfrecorded generated videos encompass humancentric realworld simulated scenarios comprehensive evaluation reveals humanmodel performance gap bestperforming model moreover indepth analysis uncovers fundamental limitations beyond gap current mfms accurately recognize events isolated frames fail interpret frames continuous sequence believe tomato serve crucial testbed evaluating nextgeneration mfms call community develop ai systems capable comprehending human world dynamics video modality,0,1.0,0,1.0
loyalty creators dilute loyalty promoted products examining heterogeneous effects livestreamed content video game usage social media platforms led online consumption communities fandoms involve complex networks ancillary creators consumers focused core product intellectual property example video game communities include networks players content creators centered around specific video game networks complex video game publishers often sponsor creators creators publishers may divergent incentives specifically creators potentially benefit content builds following expense core game research investigates relationship consuming livestreamed content engagement specific video game examine causal effect viewing livestreamed content subsequent gameplay specific game using unexpected service interruption livestreaming platform time zone differences among users find livestreamed content significantly increases gameplay increase livestreamed viewing minutes results increase gameplay minutes also explore effect varies user loyalty different types streamer channels firmowned mega micro positive effects livestreamed content greatest microstreamers smallest megastreamers findings salient firms allocating sponsorship resources,-1,0.0,-1,0.0
multicast scheme live streaming courses largescale geographically dense campus networks video courses become significant component modern education however increasing demand live streaming video courses places considerable strain service capabilities campus networks challenges associated live streaming course videos campus network environments exhibit distinct spatial distribution characteristics audience specific video courses may highly concentrated certain areas leading large number users attempting access live stream simultaneously utilizing content delivery network cdn distribute videos campus scenarios creates substantial unicast pressure edge cdn servers paper proposes twolayer dynamic partitioning recursive bit string rbs virtual domain network layer multicast architecture specifically designed largescale geographically dense multicast scenarios within campus networks approach reduces redundant multicast messages approximately compared twolayer fixed partitioning method additionally establishes multicast source authentication capabilities based source address validation improvement savi facilitates secure multicast group key exchange using concise exchange protocol within webrtc framework nextgeneration data plane programmable softwaredefined networks rbs stateless multicast technology integrated unique characteristics largescale geographically dense campus network scenarios dynamically efficiently extend multicast coverage every dormitory,2,1.0,2,1.0
implementing optimized secured multimedia streaming protocol participatory sensing scenario multimedia streaming protocols becoming increasingly popular crowdsensing due ability deliver highquality video content internet realtime streaming multimedia content context live video streaming requires high bandwidth large storage capacity ensure sufficient throughput crowdsensing distribute information shared video contents among multiple users network reducing storage capacity computational bandwidth requirements however crowdsensing introduces several security constraints must taken account ensure confidentiality integrity availability data specific case video streaming commonly named visual crowdsensing vcs within context data transmitted wireless networks making vulnerable security breaches susceptible eavesdropping interception attackers multimedias often contains sensitive user data may subject various privacy laws including data protection laws laws related photography video recording based local gdpr general data protection regulation reason realization secure protocol optimized distributed data streaming realtime becomes increasingly important crowdsensing smartenviroment context article discuss use symmetric aesctr encryption based protocol securing data streaming crowdsensed network,2,1.0,2,1.0
rewind understanding long videos instructed learnable memory visionlanguage models vlms crucial applications requiring integrated understanding textual visual information however existing vlms struggle long videos due computational inefficiency memory limitations difficulties maintaining coherent understanding across extended sequences address challenges introduce rewind novel memorybased vlm designed efficient long video understanding preserving temporal fidelity rewind operates twostage framework first stage rewind maintains dynamic learnable memory module novel textbfreadperceivewrite cycle stores updates instructionrelevant visual information video unfolds module utilizes learnable queries crossattentions memory contents input stream ensuring low memory requirements scaling linearly number tokens second stage propose adaptive frame selection mechanism guided memory content identify instructionrelevant key moments enriches memory representations detailed spatial information selecting highresolution frames combined memory contents fed large language model llm generate final answer empirically demonstrate rewinds superior performance visual question answering vqa temporal grounding tasks surpassing previous methods long video benchmarks notably rewind achieves score gain accuracy improvement vqa dataset miou increase charadessta temporal grounding,0,1.0,0,1.0
stable mean teacher semisupervised video action detection work focus semisupervised learning video action detection video action detection requires spatiotemporal localization addition classification limited amount labels makes model prone unreliable predictions present stable mean teacher simple endtoend teacherbased framework benefits improved temporally consistent pseudo labels relies novel error recovery eor module learns students mistakes labeled samples transfers knowledge teacher improve pseudo labels unlabeled samples moreover existing spatiotemporal losses take temporal coherency account prone temporal inconsistencies address present difference pixels dop simple novel constraint focused temporal consistency leading coherent temporal detections evaluate approach four different spatiotemporal detection benchmarks ava youtubevos approach outperforms supervised baselines action detection average margin ava using merely data provides competitive performance compared supervised baseline trained annotations respectively evaluate effectiveness ava scaling largescale datasets youtubevos video object segmentation demonstrating generalization capability tasks video domain code models publicly available,7,1.0,7,1.0
multimodal sentiment analysis based video audio inputs despite abundance current researches working sentiment analysis videos audios finding best model gives highest accuracy rate still considered challenge researchers field main objective paper prove usability emotion recognition models take video audio inputs datasets used train models cremad dataset audio ravdess dataset video finetuned models used audio video avarage probabilities emotion generated two previous models utilized decision making framework disparity results one models gets much higher accuracy another test framework created methods used weighted average method confidence level threshold method dynamic weighting based confidence method rulebased logic method limited approach gives encouraging results make future research methods viable,4,0.7791289004365893,4,0.7791289004365893
querycentric audiovisual cognition network moment retrieval segmentation stepcaptioning video emerged favored multimedia format internet better gain video contents new topic hirest presented including video retrieval moment retrieval moment segmentation stepcaptioning pioneering work chooses pretrained clipbased model video retrieval leverages feature extractor three challenging tasks solved multitask learning paradigm nevertheless work struggles learn comprehensive cognition userpreferred content due disregarding hierarchies association relations across modalities paper guided shallowtodeep principle propose querycentric audiovisual cognition quag network construct reliable multimodal representation moment retrieval segmentation stepcaptioning specifically first design modalitysynergistic perception obtain rich audiovisual content modeling global contrastive alignment local finegrained interaction visual audio modalities devise querycentric cognition uses deeplevel query perform temporalchannel filtration shallowlevel audiovisual representation cognize userpreferred content thus attain querycentric audiovisual representation three tasks extensive experiments show quag achieves sota results hirest test quag querybased video summarization task verify good generalization,-1,0.0,-1,0.0
cimgen controlled image manipulation finetuning pretrained generative models limited data content creation image editing benefit flexible user controls common intermediate representation conditional image generation semantic map information objects present image compared raw rgb pixels modification semantic map much easier one take semantic map easily modify map selectively insert remove replace objects map method proposed paper takes modified semantic map alter original image accordance modified map method leverages traditional pretrained imagetoimage translation gans cyclegan gan finetuned limited dataset reference images associated semantic maps discuss qualitative quantitative performance technique illustrate capacity possible applications fields image forgery image editing also demonstrate effectiveness proposed image forgery technique thwarting numerous deep learningbased image forensic techniques highlighting urgent need develop robust generalizable image forensic tools fight spread fake media,-1,0.0,-1,0.0
pcacgan sparsetensorbased generative adversarial network point cloud attribute compression learningbased methods proven successful compressing geometric information point clouds attribute compression however still lag behind nonlearningbased methods mpeg gpcc standard bridge gap propose novel deep learningbased point cloud attribute compression method uses generative adversarial network gan sparse convolution layers method also includes module adaptively selects resolution voxels used voxelize input point cloud sparse vectors used represent voxelized point cloud sparse convolutions process sparse tensors ensuring computational efficiency best knowledge first application gans compress point cloud attributes experimental results show method outperforms existing learningbased techniques rivals latest gpcc test model terms visual quality,-1,0.0,-1,0.0
lightweight ganbased image fusion algorithm visible infrared images paper presents lightweight image fusion algorithm specifically designed merging visible light infrared images emphasis balancing performance efficiency proposed method enhances generator generative adversarial network gan integrating convolutional block attention module cbam improve feature focus utilizing depthwise separable convolution dsconv efficient computations innovations significantly reduce models computational cost including number parameters inference latency maintaining even enhancing quality fused images comparative experiments using dataset demonstrate proposed algorithm outperforms similar image fusion methods terms fusion quality also offers resourceefficient solution suitable deployment embedded devices effectiveness lightweight design validated extensive ablation studies confirming potential realtime applications complex environments,-1,0.0,-1,0.0
enhancing image resolution evaluating advanced techniques based convolutional generative neural networks paper investigates enhancement spatial resolution bands contain spectral information using advanced superresolution techniques factor stateoftheart cnn models compared enhanced gan approaches terms quality feasibility therefore representative dataset comprising lowresolution images corresponding highresolution aerial orthophotos required literature study reveals feasible dataset land type interest forests reason adequate dataset generated addition accounting accurate alignment image source optimization results reveal cnnbased approaches produce satisfactory outcomes tend yield blurry images contrast ganbased models provide clear detailed images also demonstrate superior performance terms quantitative assessment underlying potential framework beyond specific land type investigated,14,0.9991369886015271,14,0.9991369886015271
duoliftganreconstructing ct singleview biplanar xrays generative adversarial networks computed tomography ct provides highly detailed threedimensional medical images costly timeconsuming often inaccessible intraoperative settings organization et al recent advancements explored reconstructing chest volumes sparse xrays singleview orthogonal doubleview images however current models tend process images planar manner prioritizing visual realism structural accuracy work introduce duolift generative adversarial networks duoliftgan novel architecture dual branches independently elevate images features representations outputs merged unified feature map decoded complete chest volume enabling richer information capture also present masked loss function directs reconstruction towards critical anatomical regions improving structural accuracy visual quality paper demonstrates duoliftgan significantly enhances reconstruction accuracy achieving superior visual realism compared existing methods,-1,0.0,-1,0.0
leveraging motion data boost motion generation textdriven human motion synthesis capturing significant attention ability effortlessly generate intricate movements abstract text cues showcasing potential revolutionizing motion design film narratives also virtual reality experiences computer game development existing methods often rely motion capture data require special setups resulting higher costs data acquisition ultimately limiting diversity scope human motion contrast human videos offer vast accessible source motion data covering wider range styles activities paper explore leveraging human motion extracted videos alternative data source improve textdriven motion generation approach introduces novel framework disentangles local joint motion global movements enabling efficient learning local motion priors data first train singleview local motion generator large dataset textmotion pairs enhance model synthesize motion finetune generator data transforming multiview generator predicts viewconsistent local joint motion root dynamics experiments dataset novel text prompts demonstrate method efficiently utilizes data supporting realistic human motion generation broadening range motion types supports code made publicly available,18,1.0,18,1.0
phocolens photorealistic consistent reconstruction lensless imaging lensless cameras offer significant advantages size weight cost compared traditional lensbased systems without focusing lens lensless cameras rely computational algorithms recover scenes multiplexed measurements however current algorithms struggle inaccurate forward imaging models insufficient priors reconstruct highquality images overcome limitations introduce novel twostage approach consistent photorealistic lensless image reconstruction first stage approach ensures data consistency focusing accurately reconstructing lowfrequency content spatially varying deconvolution method adjusts changes point spread function psf across cameras field view second stage enhances photorealism incorporating generative prior pretrained diffusion models conditioning lowfrequency content retrieved first stage diffusion model effectively reconstructs highfrequency details typically lost lensless imaging process also maintaining image fidelity method achieves superior balance data fidelity visual quality compared existing methods demonstrated two popular lensless systems phlatcam diffusercam project website httpsphocolensgithubio,-1,0.0,-1,0.0
hunyuanvideo systematic framework large video generative models recent advancements video generation significantly impacted daily life individuals industries however leading video generation models remain closedsource resulting notable performance gap industry capabilities available public report introduce hunyuanvideo innovative opensource video foundation model demonstrates performance video generation comparable even surpassing leading closedsource models hunyuanvideo encompasses comprehensive framework integrates several key elements including data curation advanced architectural design progressive model scaling training efficient infrastructure tailored largescale model training inference result successfully trained video generative model billion parameters making largest among opensource models conducted extensive experiments implemented series targeted designs ensure high visual quality motion dynamics textvideo alignment advanced filming techniques according evaluations professionals hunyuanvideo outperforms previous stateoftheart models including runway luma three topperforming chinese video generative models releasing code foundation model applications aim bridge gap closedsource opensource communities initiative empower individuals within community experiment ideas fostering dynamic vibrant video generation ecosystem code publicly available httpsgithubcomtencenthunyuanvideo,-1,0.0,-1,0.0
enhancing ratedistortionperception flexibility learned image codecs conditional diffusion decoders learned image compression codecs recently achieved impressive compression performances surpassing efficient image coding architectures however approaches trained minimize rate distortion often leads unsatisfactory visual results low bitrates since perceptual metrics taken account paper show conditional diffusion models lead promising results generative compression task used decoder given compressed representation allow creating new tradeoff points distortion perception decoder side based sampling method,2,1.0,2,1.0
fortifying fully convolutional generative adversarial networks image superresolution using divergence measures superresolution sr timehallowed image processing problem aims improve quality lowresolution lr sample standard highresolution hr counterpart aim address introducing superresolution generator surge fullyconvolutional generative adversarial network ganbased architecture sr show distinct convolutional features obtained increasing depths gan generator optimally combined set learnable convex weights improve quality generated sr samples process employ jensenshannon gromovwasserstein losses respectively srhr lrsr pairs distributions aid generator surge better exploit available information attempt improve sr moreover train discriminator surge wasserstein loss gradient penalty primarily prevent mode collapse proposed surge endtoend gan workflow tailormade superresolution offers improved performance maintaining low inference time efficacy surge substantiated superior performance compared stateoftheart contenders benchmark datasets,-1,0.0,-1,0.0
adversarial attack images classification based generative adversarial networks adversarial attacks image classification systems always important problem field machine learning generative adversarial networks gans popular models field image generation widely used various novel scenarios due powerful generative capabilities however popularity generative adversarial networks misuse fake image technology raised series security problems malicious tampering peoples photos videos invasion personal privacy inspired generative adversarial networks work proposes novel adversarial attack method aiming gain insight weaknesses image classification system improve antiattack ability specifically generative adversarial networks used generate adversarial samples small perturbations enough affect decisionmaking classifier adversarial samples generated adversarial learning training generator classifier extensive experiment analysis evaluate effectiveness method classical image classification dataset results show model successfully deceives variety advanced classifiers maintaining naturalness adversarial samples,-1,0.0,-1,0.0
dctdiff intriguing properties image generative modeling dct space paper explores image modeling frequency space introduces dctdiff endtoend diffusion generative paradigm efficiently models images discrete cosine transform dct space investigate design space dctdiff reveal key design factors experiments different frameworks uvit dit generation tasks various diffusion samplers demonstrate dctdiff outperforms pixelbased diffusion models regarding generative quality training efficiency remarkably dctdiff seamlessly scale highresolution generation without using latent diffusion paradigm finally illustrate several intriguing properties dct image modeling example provide theoretical proof image diffusion seen spectral autoregression bridging gap diffusion autoregressive models effectiveness dctdiff introduced properties suggest promising direction image modeling frequency space code,13,1.0,13,1.0
karma augmenting embodied ai agents longandshort term memory systems embodied ai agents responsible executing interconnected longsequence household tasks often face difficulties incontext memory leading inefficiencies errors task execution address issue introduce karma innovative memory system integrates longterm shortterm memory modules enhancing large language models llms planning embodied agents memoryaugmented prompting karma distinguishes longterm shortterm memory longterm memory capturing comprehensive scene graphs representations environment shortterm memory dynamically records changes objects positions states dualmemory structure allows agents retrieve relevant past scene experiences thereby improving accuracy efficiency task planning shortterm memory employs strategies effective adaptive memory replacement ensuring retention critical information discarding less pertinent data compared stateoftheart embodied agents enhanced memory memoryaugmented embodied ai agent improves success rates composite tasks complex tasks within simulator respectively enhances task execution efficiency furthermore demonstrate karmas plugandplay capability allows seamless deployment realworld robotic systems mobile manipulation platformsthrough plugandplay memory system karma significantly enhances ability embodied agents generate coherent contextually appropriate plans making execution complex household tasks efficient experimental videos work found code available,-1,0.0,-1,0.0
interactive visual learning stable diffusion diffusionbased generative models impressive ability create convincing images garnered global attention however complex internal structures operations often pose challenges nonexperts grasp introduce diffusion explainer first interactive visualization tool designed elucidate stable diffusion transforms text prompts images tightly integrates visual overview stable diffusions complex components detailed explanations underlying operations integration enables users fluidly transition multiple levels abstraction animations interactive elements offering realtime handson experience diffusion explainer allows users adjust stable diffusions hyperparameters prompts without need installation specialized hardware accessible via users web browsers diffusion explainer making significant strides democratizing ai education fostering broader public access users spanning countries used opensourced tool httpspoloclubgithubiodiffusionexplainer video demo available httpsyoutubembkiadzjpna,-1,0.0,-1,0.0
genziqa generalized image quality assessment using promptguided latent diffusion models design noreference nr image quality assessment iqa algorithms extremely important benchmark calibrate user experiences modern visual systems major drawback stateoftheart nriqa methods limited ability generalize across diverse iqa settings reasonable distribution shifts recent texttoimage generative models latent diffusion models generate meaningful visual concepts fine details related text concepts work leverage denoising process diffusion models generalized iqa understanding degree alignment learnable qualityaware text prompts images particular learn crossattention maps intermediate layers denoiser latent diffusion models capture qualityaware representations images addition also introduce learnable qualityaware text prompts enable crossattention features better qualityaware extensive cross database experiments across various usergenerated synthetic lowlight contentbased benchmarking databases show latent diffusion models achieve superior generalization iqa compared methods literature,-1,0.0,-1,0.0
lowresource data generation textual control natural language serves common straightforward signal humans interact seamlessly machines recognizing importance interface machine learning community investing considerable effort generating data semantically coherent textual instructions strides made texttodata generation spanning image editing audio synthesis video creation beyond lowresource areas characterized expensive annotations complex data structures molecules motion dynamics time series often lack textual labels deficiency impedes supervised learning thereby constraining application advanced generative models texttodata tasks response challenges lowresource scenario propose novel approach utilizes unlabeled data understand underlying data distribution unsupervised diffusion model subsequently undergoes controllable finetuning via novel constraint optimizationbased learning objective ensures controllability effectively counteracts catastrophic forgetting comprehensive experiments demonstrate able achieve enhanced performance regarding controllability across various modalities including molecules motions time series compared existing baselines,-1,0.0,-1,0.0
tpc testtime procrustes calibration diffusionbased human image animation human image animation aims generate human motion video inputs reference human image target motion video current diffusionbased image animation systems exhibit high precision transferring human identity targeted motion yet still exhibit irregular quality outputs optimal precision achieved physical compositions ie scale rotation human shapes reference image target pose frame aligned absence alignment noticeable decline fidelity consistency especially realworld environments compositional misalignment commonly occurs posing significant challenges practical usage current systems end propose testtime procrustes calibration tpc enhances robustness diffusionbased image animation systems maintaining optimal performance even faced compositional misalignment effectively addressing realworld scenarios tpc provides calibrated reference image diffusion model enhancing capability understand correspondence human shapes reference target images method simple applied diffusionbased image animation system modelagnostic manner improving effectiveness test time without additional training,-1,0.0,-1,0.0
multirobot motion planning diffusion models diffusion models recently successfully applied wide range robotics applications learning complex multimodal behaviors data however prior works mostly confined singlerobot smallscale environments due high sample complexity learning multirobot diffusion models paper propose method generating collisionfree multirobot trajectories conform underlying data distributions using singlerobot data algorithm multirobot multimodel planning diffusion mmd combining learned diffusion models classical searchbased techniques generating datadriven motions collision constraints scaling show compose multiple diffusion models plan large environments single diffusion model fails generalize well demonstrate effectiveness approach planning dozens robots variety simulated scenarios motivated logistics environments view video demonstrations supplementary material code httpsgithubcomyoraishmmd,5,0.516975352180103,5,0.516975352180103
mozarts touch lightweight multimodal music generation framework based pretrained large models recent years aigenerated content aigc witnessed rapid advancements facilitating creation music images artistic forms across wide range industries however current models image videotomusic synthesis struggle capture nuanced emotions atmosphere conveyed visual content fill gap propose mozarts touch multimodal music generation framework capable generating music aligned crossmodal inputs images videos text framework consists three key components multimodal captioning module large language model llm understanding bridging module music generation module unlike traditional endtoend methods mozarts touch uses llms accurately interpret visual elements without requiring training finetuning music generation models providing efficiency transparency clear interpretable prompts also introduce llmbridge method resolve heterogeneous representation challenges descriptive texts different modalities series objective subjective evaluations demonstrate mozarts touch outperforms current stateoftheart models code examples available httpsgithubcomtiffanyblewsmozartstouch,8,0.4362581394694872,8,0.4362581394694872
revisiting registrationbased synthesis focus unsupervised mr image synthesis deep learning dl led significant improvements medical image synthesis enabling advanced imagetoimage translation generate synthetic images however dl methods face challenges domain shift high demands training data limiting generalizability applicability historically image synthesis also carried using deformable image registration dir method warps moving images desired modality match anatomy fixed image however concerns speed accuracy led decline popularity recent advances dlbased dir revisit reinvigorate line research paper propose fast accurate synthesis method based dir use task synthesizing rare magnetic resonance mr sequence white matter nulled wmn images demonstrate potential approach training method learns dir model based widely available mprage sequence cerebrospinal fluid nulled csfn inversion recovery gradient echo pulse sequence testing trained dir model first applied estimate deformation moving fixed csfn images subsequently estimated deformation applied align paired wmn counterpart moving csfn image yielding synthetic wmn image fixed csfn image experiments demonstrate promising results unsupervised image synthesis using dir findings highlight potential technique contexts supervised synthesis methods constrained limited training data,3,0.9178618569303,3,0.9178618569303
combo compositional world models embodied multiagent cooperation paper investigate problem embodied multiagent cooperation decentralized agents must cooperate given egocentric views world effectively plan setting contrast learning world dynamics singleagent scenario must simulate world dynamics conditioned arbitrary number agents actions given partial egocentric visual observations world address issue partial observability first train generative models estimate overall world state given partial egocentric observations enable accurate simulation multiple sets actions world state propose learn compositional world model multiagent cooperation factorizing naturally composable joint actions multiple agents compositionally generating video conditioned world state leveraging compositional world model combination vision language models infer actions agents use tree search procedure integrate modules facilitate online cooperative planning evaluate methods three challenging benchmarks agents results show compositional world model effective framework enables embodied agents cooperate efficiently different agents across various tasks arbitrary number agents showing promising future proposed methods videos found httpsembodiedagicsumasseducombo,5,0.37094868929513,5,0.37094868929513
monkey see monkey harnessing selfattention motion diffusion zeroshot motion transfer given remarkable results motion synthesis diffusion models natural question arises effectively leverage models motion editing existing diffusionbased motion editing methods overlook profound potential prior embedded within weights pretrained models enables manipulating latent feature space hence primarily center handling motion space work explore attention mechanism pretrained motion diffusion models uncover roles interactions attention elements capturing representing intricate human motion patterns carefully integrate elements transfer leader motion follower one maintaining nuanced characteristics follower resulting zeroshot motion transfer editing features associated selected motions allows us confront challenge observed prior motion diffusion approaches use general directives eg text music editing ultimately failing convey subtle nuances effectively work inspired monkey closely imitates sees maintaining unique motion patterns hence call monkey see monkey dub momo employing technique enables accomplishing tasks synthesizing outofdistribution motions style transfer spatial editing furthermore diffusion inversion seldom employed motions result editing efforts focus generated motions limiting editability real ones momo harnesses motion inversion extending application real generated motions experimental results show advantage approach current art particular unlike methods tailored specific applications training approach applied inference time requiring training webpage httpsmonkeyseedocggithubio,9,0.7013490236443066,9,0.7013490236443066
learning dynamic tetrahedra highquality talking head synthesis recent works implicit representations neural radiance fields nerf advanced generation realistic animatable head avatars video sequences implicit methods still confronted visual artifacts jitters since lack explicit geometric constraints poses fundamental challenge accurately modeling complex facial deformations paper introduce dynamic tetrahedra dyntet novel hybrid representation encodes explicit dynamic meshes neural networks ensure geometric consistency across various motions viewpoints dyntet parameterized coordinatebased networks learn signed distance deformation material texture anchoring training data predefined tetrahedra grid leveraging marching tetrahedra dyntet efficiently decodes textured meshes consistent topology enabling fast rendering differentiable rasterizer supervision via pixel loss enhance training efficiency incorporate classical morphable models facilitate geometry learning define canonical space simplifying texture learning advantages readily achievable owing effective geometric representation employed dyntet compared prior works dyntet demonstrates significant improvements fidelity lip synchronization realtime performance according various metrics beyond producing stable visually appealing synthesis videos method also outputs dynamic meshes promising enable many emerging applications,1,0.9838894797985818,1,0.9838894797985818
data augmentation diffusion models colon polyp localization low data regime much real data enough scarcity data medical domains hinders performance deep learning models data augmentation techniques alleviate problem usually rely functional transformations data guarantee preserve original tasks approximate distribution data using generative models way reducing problem also obtain new samples resemble original data denoising diffusion models promising deep learning technique learn good approximations different kinds data like images time series tabular data automatic colonoscopy analysis specifically polyp localization colonoscopy videos task assist clinical diagnosis treatment annotation video frames training deep learning model time consuming task usually small datasets obtained fine tuning application models using large dataset generated data could alternative improve performance conduct set experiments training different diffusion models generate jointly colonoscopy images localization annotations using combination existing open datasets generated data used various transfer learning experiments task polyp localization model based yolo low data regime,3,0.5260576955629406,3,0.5260576955629406
attack scene flow using point clouds deep neural networks made significant advancements accurately estimating scene flow using point clouds vital many applications like video analysis action recognition navigation robustness techniques however remains concern particularly face adversarial attacks proven deceive stateoftheart deep neural networks many domains surprisingly robustness scene flow networks attacks thoroughly investigated address problem proposed approach aims bridge gap introducing adversarial whitebox attacks specifically tailored scene flow networks experimental results show generated adversarial examples obtain relative degradation average endpoint error kitti datasets study also reveals significant impact attacks targeting point clouds one dimension color channel average endpoint error analyzing success failure attacks scene flow networks optical flow network variants shows higher vulnerability optical flow networks code available httpsgithubcomaheldisattackonsceneflowusingpointcloudsgit,-1,0.0,-1,0.0
qdit accurate posttraining quantization diffusion transformers recent advancements diffusion models particularly architectural transformation unetbased models diffusion transformers dits significantly improve quality scalability image video generation however despite impressive capabilities substantial computational costs largescale models pose significant challenges realworld deployment posttraining quantization ptq emerges promising solution enabling model compression accelerated inference pretrained models without costly retraining however research dit quantization remains sparse existing ptq frameworks primarily designed traditional diffusion models tend suffer biased quantization leading notable performance degradation work identify dits typically exhibit significant spatial variance weights activations along temporal variance activations address issues propose qdit novel approach seamlessly integrates two key techniques automatic quantization granularity allocation handle significant variance weights activations across input channels samplewise dynamic activation quantization adaptively capture activation changes across timesteps samples extensive experiments conducted imagenet vbench demonstrate effectiveness proposed qdit specifically quantizing imagenet times qdit achieves remarkable reduction fid compared baseline challenging setting maintains high fidelity image video generation establishing new benchmark efficient highquality quantization dits code available hrefhttpsgithubcomjuanerxqdithttpsgithubcomjuanerxqdit,-1,0.0,-1,0.0
generalized event cameras event cameras capture world high time resolution minimal bandwidth requirements however event streams encode changes brightness contain sufficient scene information support wide variety downstream tasks work design generalized event cameras inherently preserve scene intensity bandwidthefficient manner generalize event cameras terms event generated information transmitted implement designs turn singlephoton sensors provide digital access individual photon detections modality gives us flexibility realize rich space generalized event cameras singlephoton event cameras capable highspeed highfidelity imaging low readout rates consequently event cameras support plugandplay downstream inference without capturing new event datasets designing specialized eventvision models practical implication designs involve lightweight nearsensorcompatible computations provide way use singlephoton sensors without exorbitant bandwidth costs,1,1.0,1,1.0
diffusionbased unsupervised audiovisual speech enhancement paper proposes new unsupervised audiovisual speech enhancement avse approach combines diffusionbased audiovisual speech generative model nonnegative matrix factorization nmf noise model first diffusion model pretrained clean speech conditioned corresponding video data simulate speech generative distribution pretrained model paired nmfbased noise model estimate clean speech iteratively specifically diffusionbased posterior sampling approach implemented within reverse diffusion process iteration speech estimate obtained used update noise parameters experimental results confirm proposed avse approach outperforms audioonly counterpart also generalizes better recent supervisedgenerative avse method additionally new inference algorithm offers better balance inference speed performance compared previous diffusionbased method code demo available,-1,0.0,-1,0.0
generating reward function videos legged robot behavior learning learning behavior legged robots presents significant challenge due inherent instability complex constraints recent research proposed use large language model llm generate reward functions reinforcement learning thereby replacing need manually designed rewards experts however approach relies textual descriptions define learning objectives fails achieve controllable precise behavior learning clear directionality paper introduce new method directly generates reward functions videos depicting behaviors mimicked learned specifically first process videos containing target behaviors converting motion information individuals videos keypoint trajectories represented coordinates transforming module trajectories fed llm generate reward function turn used train policy enhance quality reward function develop videoassisted iterative reward refinement scheme visually assesses learned behaviors provides textual feedback llm feedback guides llm continually refine reward function ultimately facilitating efficient behavior learning experimental results tasks involving bipedal quadrupedal robot motion control demonstrate method surpasses performance stateoftheart llmbased reward generation methods terms human normalized score importantly switching video inputs find method rapidly learn diverse motion behaviors walking running,5,1.0,5,1.0
anidoc animation creation made easier production animation follows industrystandard workflow encompassing four essential stages character design keyframe animation inbetweening coloring research focuses reducing labor costs process harnessing potential increasingly powerful generative ai using video diffusion models foundation anidoc emerges video line art colorization tool automatically converts sketch sequences colored animations following reference character specification model exploits correspondence matching explicit guidance yielding strong robustness variations eg posture reference character line art frame addition model could even automate inbetweening process users easily create temporally consistent animation simply providing character image well start end sketches code available,-1,0.0,-1,0.0
flow crossdomain manipulation interface present scalable learning framework enables robots acquire realworld manipulation skills without need realworld robot training data key idea behind use object flow manipulation interface bridging domain gaps different embodiments ie human robot training environments ie realworld simulated comprises two components flow generation network flowconditioned policy flow generation network trained human demonstration videos generates object flow initial scene image conditioned task description flowconditioned policy trained simulated robot play data maps generated object flow robot actions realize desired object movements using flow input policy directly deployed real world minimal simtoreal gap leveraging realworld human videos simulated robot play data bypass challenges teleoperating physical robots real world resulting scalable system diverse tasks demonstrate capabilities variety realworld tasks including manipulation rigid articulated deformable objects,5,0.6867238948789461,5,0.6867238948789461
pairs scripts shooting scripts short drama generating highquality shooting scripts containing information scene shot language essential short drama script generation collect popular short drama episodes internet average short episodes total number short episodes total duration hours totaling terabytes tb perform keyframe extraction annotation episode obtain shooting scripts perform script restorations extracted shooting scripts based selfdeveloped large short drama generation model skyreels leads dataset containing pairs scripts shooting scripts short dramas called compare existing dataset detail demonstrate deeper insights achieved based based researchers achieve several deeper farreaching script optimization goals may drive paradigm shift entire field texttovideo significantly advance field short drama video generation data code available,-1,0.0,-1,0.0
plamo plan move rich physical environments controlling humanoids complex physically simulated worlds longstanding challenge numerous applications gaming simulation visual content creation setup given rich complex scene user provides list instructions composed target locations locomotion types solve task present plamo sceneaware path planner robust physicsbased controller path planner produces sequence motion paths considering various limitations scene imposes motion location height speed complementing planner control policy generates rich realistic physical motion adhering plan demonstrate combination modules enables traversing complex landscapes diverse forms responding realtime changes environment video,1,0.8797522169030966,1,0.8797522169030966
neural mp generalist neural motion planner current paradigm motion planning generates solutions scratch every new problem consumes significant amounts time computational resources complex cluttered scenes motion planning approaches often take minutes produce solution humans able accurately safely reach goal seconds leveraging prior experience seek applying datadriven learning scale problem motion planning approach builds large number complex scenes simulation collects expert data motion planner distills reactive generalist policy combine lightweight optimization obtain safe path real world deployment perform thorough evaluation method motion planning tasks across four diverse environments randomized poses scenes obstacles real world demonstrating improvement motion planning success rate state art sampling optimization learning based planning methods video results available mihdalalgithubioneuralmotionplanner,5,0.6291536128535694,5,0.6291536128535694
intrinsic singleimage hdr reconstruction low dynamic range ldr common cameras fails capture rich contrast natural scenes resulting loss color details saturated pixels reconstructing high dynamic range hdr luminance present scene single ldr photographs important task many applications computational photography realistic display images hdr reconstruction task aims infer lost details using context present scene requiring neural networks understand highlevel geometric illumination cues makes challenging datadriven algorithms generate accurate highresolution results work introduce physicallyinspired remodeling hdr reconstruction problem intrinsic domain intrinsic model allows us train separate networks extend dynamic range shading domain recover lost color details albedo domain show dividing problem two simpler subtasks improves performance wide variety photographs,1,1.0,1,1.0
actionbased image editing guided human instructions textbased image editing typically approached static task involves operations inserting deleting modifying elements input image based human instructions given static nature task paper aim make task dynamic incorporating actions intend modify positions postures objects image depict different actions maintaining visual properties objects implement challenging task propose new model sensitive action text instructions learning recognize contrastive action discrepancies model training done new datasets defined extracting frames videos show visual scenes action show substantial improvements image editing using actionbased text instructions high reasoning capabilities allow model use input image starting scene action generating new image shows final scene action,-1,0.0,-1,0.0
creative portraiture exploring creative adversarial networks conditional creative adversarial networks convolutional neural networks cnns combined generative adversarial networks gans create deep convolutional generative adversarial networks dcgans great success dcgans used generating images videos creative domains fashion design painting common critique use dcgans creative applications limited ability generate creative products generator simply learns copy training distribution explore extension dcgans creative adversarial networks cans using cans generate novel creative portraits using wikiart dataset train network moreover introduce extension cans conditional creative adversarial networks ccans demonstrate potential generate creative portraits conditioned style label argue generating products conditioned inspired style label closely emulates real creative processes humans produce imaginative work still rooted previous styles,17,1.0,17,1.0
mambamir arbitrarymasked mamba joint medical image reconstruction uncertainty estimation recent mamba model shown remarkable adaptability visual representation learning including medical imaging tasks study introduces mambamir mambabased model medical image reconstruction well generative adversarial networkbased variant mambamirgan proposed mambamir inherits several advantages linear complexity global receptive fields dynamic weights original mamba model innovated arbitrarymask mechanism effectively adapt mamba image reconstruction task providing randomness subsequent monte carlobased uncertainty estimation experiments conducted various medical image reconstruction tasks including fast mri svct cover anatomical regions knee chest abdomen demonstrated mambamir mambamirgan achieve comparable superior reconstruction results relative stateoftheart methods additionally estimated uncertainty maps offer insights reliability reconstruction quality code publicly available httpsgithubcomayanglabmambamir,-1,0.0,-1,0.0
onboard processing hyperspectral imagery deep learning advancements methodologies challenges emerging trends recent advancements deep learning techniques spurred considerable interest application hyperspectral imagery processing paper provides comprehensive review latest developments field focusing methodologies challenges emerging trends deep learning architectures convolutional neural networks cnns autoencoders deep belief networks dbns generative adversarial networks gans recurrent neural networks rnns examined suitability processing hyperspectral data key challenges including limited training data computational constraints identified along strategies data augmentation noise reduction using gans paper discusses efficacy different network architectures highlighting advantages lightweight cnn models cnns onboard processing moreover potential hardware accelerators particularly field programmable gate arrays fpgas enhancing processing efficiency explored review concludes insights ongoing research trends including integration deep learning techniques earth observation missions chime mission emphasizes need exploration refinement deep learning methodologies address evolving demands hyperspectral image processing,-1,0.0,-1,0.0
generalizable diffusion framework lowdose fewview cardiac spect myocardial perfusion imaging using spect widely utilized diagnose coronary artery diseases image quality negatively affected lowdose fewview acquisition settings although various deep learning methods introduced improve image quality lowdose fewview spect data previous approaches often fail generalize across different acquisition settings limiting applicability reality work introduced diffusion framework cardiac spect imaging effectively adapts different acquisition settings without requiring network retraining finetuning using image projection data consistency strategy proposed ensure diffusion sampling step aligns lowdosefewview projection measurements image data scanner geometry thus enabling generalization different lowdosefewview settings incorporating anatomical spatial information ct total variation constraint proposed conditional strategy allow observe contextual information entire image volume addressing memory issues diffusion model extensively evaluated proposed method clinical tetrofosmin stressrest studies patients study reconstructed different lowcount different fewview levels model evaluations ranging view view respectively validated cardiac catheterization results diagnostic comments nuclear cardiologists presented results show potential achieve lowdose fewview spect imaging without compromising clinical performance additionally could directly applied fulldose spect images improve image quality especially lowdose stressfirst cardiac spect imaging protocol,3,0.9818733983711829,3,0.9818733983711829
physicsdriven framework controllable efficient content generation single image task content generation involves creating dynamic models evolve time response specific input conditions images existing methods rely heavily pretrained video diffusion models guide content dynamics approaches often fail capture essential physical principles video diffusion models lack robust understanding realworld physics moreover models face challenges providing finegrained control dynamics exhibit high computational costs work propose novel highefficiency framework generates physicscompliant content single image enhanced control capabilities approach uniquely integrates physical simulations generation pipeline ensuring adherence fundamental physical laws inspired human ability infer physical properties visually introduce physical perception module ppm discerns material properties structural components object input image facilitating accurate downstream simulations significantly accelerates generation process eliminating iterative optimization steps dynamics modeling phase allows users intuitively control movement speed direction generated content adjusting external forces achieving finely tunable physically plausible animations extensive evaluations show outperforms existing methods inference speed physical realism producing highquality controllable content project page available link,-1,0.0,-1,0.0
freescale unleashing resolution diffusion models via tuningfree scale fusion visual diffusion models achieve remarkable progress yet typically trained limited resolutions due lack highresolution data constrained computation resources hampering ability generate highfidelity images videos higher resolutions recent efforts explored tuningfree strategies exhibit untapped potential higherresolution visual generation pretrained models however methods still prone producing lowquality visual content repetitive patterns key obstacle lies inevitable increase highfrequency information model generates visual content exceeding training resolution leading undesirable repetitive patterns deriving accumulated errors tackle challenge propose freescale tuningfree inference paradigm enable higherresolution visual generation via scale fusion specifically freescale processes information different receptive scales fuses extracting desired frequency components extensive experiments validate superiority paradigm extending capabilities higherresolution visual generation image video models notably compared previous bestperforming method freescale unlocks generation images first time,-1,0.0,-1,0.0
belief scene graphs expanding partial scenes objects computation expectation article propose novel concept belief scene graphs utilitydriven extensions partial scene graphs enable efficient highlevel task planning partial information propose graphbased learning methodology computation belief also referred expectation given scene graph used strategically add new nodes referred blind nodes relevant robotic mission propose method computation expectation based correlation information ceci reasonably approximate real beliefexpectation learning histograms available training data novel graph convolutional neural network gcn model developed learn ceci repository scene graphs database scene graphs exists training novel ceci model present novel methodology generating scene graph dataset based semantically annotated reallife spaces generated dataset utilized train proposed ceci model extensive validation proposed method establish novel concept textitbelief scene graphs bsg core component integrate expectations abstract representations new concept evolution classical scene graph concept aims enable highlevel reasoning task planning optimization variety robotics missions efficacy overall framework evaluated object search scenario also tested reallife experiment emulate human common sense unseenobjects video article showcasing experimental demonstration please refer following link,-1,0.0,-1,0.0
optimized twostage aibased neural decoding enhanced visual stimulus reconstruction fmri data aibased neural decoding reconstructs visual perception leveraging generative models map brain activity measured functional mri fmri latent hierarchical representations traditionally ridge linear models transform fmri latent space decoded using latent diffusion models ldm via pretrained variational autoencoder vae due complexity noisiness fmri data newer approaches split reconstruction two sequential steps first one providing rough visual approximation second improving stimulus prediction via ldm endowed clip embeddings work proposes nonlinear deep network improve fmri latent space representation optimizing dimensionality alike experiments natural scenes dataset showed proposed architecture improved structural similarity reconstructed image respect stateoftheart model based ridge linear transform reconstructed images semantics improved measured perceptual similarity respect stateoftheart noise sensitivity analysis ldm showed role first stage fundamental predict stimulus featuring high structural similarity conversely providing large noise stimulus affected less semantics predicted stimulus structural similarity ground truth predicted stimulus poor findings underscore importance leveraging nonlinear relationships bold signal latent representation twostage generative ai optimizing fidelity reconstructed visual stimuli noisy fmri data,-1,0.0,-1,0.0
analyzing neural networkbased generative diffusion models convex optimization diffusion models gaining widespread use cuttingedge image video audio generation scorebased diffusion models stand among methods necessitating estimation score function input data distribution study present theoretical framework analyze twolayer neural networkbased diffusion models reframing score matching denoising score matching convex optimization prove training shallow neural networks score prediction done solving single convex program although analyses diffusion models operate asymptotic setting rely approximations characterize exact predicted score function establish convergence results neural networkbased diffusion models finite data results provide precise characterization neural networkbased diffusion models learn nonasymptotic settings,13,1.0,13,1.0
text diffusion reinforced conditioning diffusion models demonstrated exceptional capability generating highquality images videos audio due adaptiveness iterative refinement provide strong potential achieving better nonautoregressive sequence generation however existing text diffusion models still fall short performance due challenge handling discreteness language paper thoroughly analyzes text diffusion models uncovers two significant limitations degradation selfconditioning training misalignment training sampling motivated findings propose novel text diffusion model called trec mitigates degradation reinforced conditioning misalignment timeaware variance scaling extensive experiments demonstrate competitiveness trec autoregressive nonautoregressive diffusion baselines moreover qualitative analysis shows advanced ability fully utilize diffusion process refining samples,13,0.975147690219401,13,0.975147690219401
stale diffusion hyperrealistic movie generation using oldschool methods two years ago stable diffusion achieved superhuman performance generating images superhuman numbers fingers following steady decline technical novelty propose stale diffusion method solidifies ossifies stable diffusion maximumentropy state stable diffusion works analogously barn stable infinite set horses escaped diffusion horses long left barn proposal may seen antiquated irrelevant nevertheless vigorously defend claim novelty identifying early adopters slow science movement produce extremely important pearls wisdom future speed contributions also seen quasistatic implementation recent call pause ai experiments wholeheartedly support result careful archaeological expedition git commit histories found naturallyaccumulating errors produced novel entropymaximising stale diffusion method produce sleepinducing hyperrealistic video good ones imagination,-1,0.0,-1,0.0
simple effective masked diffusion language models diffusion models excel generating highquality images prior work reports significant performance gap diffusion autoregressive ar methods language modeling work show simple masked discrete diffusion performant previously thought apply effective training recipe improves performance masked diffusion models derive simplified raoblackwellized objective results additional improvements objective simple form mixture classical masked language modeling losses used train encoderonly language models admit efficient samplers including ones generate arbitrary lengths text semiautoregressively like traditional language model language modeling benchmarks range masked diffusion models trained modern engineering practices achieves new stateoftheart among diffusion models approaches ar perplexity provide code along blog post video tutorial project page httpsssahoocommdlm,13,0.9096327397886144,13,0.9096327397886144
ais challenge video quality assessment usergenerated content methods results paper reviews ais video quality assessment vqa challenge focused usergenerated content ugc aim challenge gather deep learningbased methods capable estimating perceptual quality ugc videos usergenerated videos youtube ugc dataset include diverse content sports games lyrics anime etc quality resolutions proposed methods must process fhd frames second challenge total participants registered submitted code models performance submissions reviewed provided survey diverse deep models efficient video quality assessment usergenerated content,12,0.6947954728701426,12,0.6947954728701426
high frequency matters uncertainty guided image compression wavelet diffusion diffusion probabilistic models recently achieved remarkable success generating highquality images however balancing high perceptual quality low distortion remains challenging image compression applications address issue propose efficient uncertaintyguided image compression approach wavelet diffusion ugdiff approach focuses high frequency compression via wavelet transform since high frequency components crucial reconstructing image details introduce wavelet conditional diffusion model high frequency prediction followed residual codec compresses transmits prediction residuals decoder diffusion predictionthenresidual compression paradigm effectively addresses low fidelity issue common direct reconstructions existing diffusion models considering uncertainty random sampling diffusion model design uncertaintyweighted ratedistortion rd loss tailored residual compression providing rational tradeoff rate distortion comprehensive experiments two benchmark datasets validate effectiveness ugdiff surpassing stateoftheart image compression methods rd performance perceptual quality subjective quality inference time code available,2,1.0,2,1.0
recasting generic pretrained vision transformers objectcentric scene encoders manipulation policies generic reusable pretrained image representation encoders become standard component methods many computer vision tasks visual representations robots however utility limited leading recent wave efforts pretrain roboticsspecific image encoders better suited robotic tasks generic counterparts propose scene objects transformers abbreviated soft wrapper around pretrained vision transformer pvt models bridges gap without training rather construct representations final layer activations soft individuates locates objectlike entities pvt attentions describes pvt activations producing objectcentric embedding across standard choices generic pretrained vision transformers pvt demonstrate case policies trained softpvt far outstrip standard pvt representations manipulation tasks simulated real settings approaching stateoftheart roboticsaware representations code appendix videos httpssitesgooglecomviewrobotsoft,5,0.674122388343842,5,0.674122388343842
towards general textguided image synthesis customized multimodal brain mri generation multimodal brain magnetic resonance mr imaging indispensable neuroscience neurology however due accessibility mri scanners lengthy acquisition time multimodal mr images commonly available current mr image synthesis approaches typically trained independent datasets specific tasks leading suboptimal performance applied novel datasets tasks present tumsyn textguided universal mr image synthesis generalist model flexibly generate brain mr images demanded imaging metadata routinely acquired scans guided text prompts ensure tumsyns image synthesis precision versatility generalizability first construct brain mr database comprising images mri modalities centers pretrain mrispecific text encoder using contrastive learning effectively control mr image synthesis based text prompts extensive experiments diverse datasets physician assessments indicate tumsyn generate clinically meaningful mr images specified imaging metadata supervised zeroshot scenarios therefore tumsyn utilized along acquired mr scans facilitate largescale mribased screening diagnosis brain diseases,3,0.9560551471929694,3,0.9560551471929694
texsensegan userguided system optimizing texturerelated vibrotactile feedback using generative adversarial network vibration rendering essential creating realistic tactile experiences humanvirtual object interactions video game controllers vr devices dynamically adjusting vibration parameters based user actions systems convey spatial features contribute texture representation however generating arbitrary vibrations replicate realworld material textures challenging due large parameter space study proposes humanintheloop vibration generation model based user preferences enable users easily control generation vibration samples large parameter spaces introduced optimization model based differential subspace search dss generative adversarial network gan dss users employ onedimensional slider easily modify highdimensional latent space ensure gan generate desired vibrations trained generative model using open dataset tactile vibration data selected five types vibrations target samples generation experiment extensive user experiments conducted using generated real samples results indicated system could generate distinguishable samples matched target characteristics moreover established correlation subjects ability distinguish real samples ability distinguish generated samples,-1,0.0,-1,0.0
eventaided freetrajectory gaussian splatting scene reconstruction casually captured videos wide applications realworld scenarios recent advancements differentiable rendering techniques several methods attempted simultaneously optimize scene representations nerf camera poses despite recent progress existing methods relying traditional camera input tend fail highspeed equivalently lowframerate scenarios event cameras inspired biological vision record pixelwise intensity changes asynchronously high temporal resolution providing valuable scene motion information blind interframe intervals paper introduce event camera aid scene construction casually captured video first time propose eventaided freetrajectory called seamlessly integrates advantages event cameras three key components first leverage event generation model egm fuse events frames supervising rendered views observed event stream second adopt contrast maximization cmax framework piecewise manner extract motion information maximizing contrast image warped events iwe thereby calibrating estimated poses besides based linear event generation model legm brightness information encoded iwe also utilized constrain gradient domain third mitigate absence color information events introduce photometric bundle adjustment pba ensure view consistency across events frames evaluate method public tanks temples benchmark newly collected realworld dataset realevdavis project page,1,1.0,1,1.0
segtalker segmentationbased talking face generation maskguided local editing audiodriven talking face generation aims synthesize video lip movements synchronized input audio however current generative techniques face challenges preserving intricate regional textures skin teeth address aforementioned challenges propose novel framework called segtalker decouple lip movements image textures introducing segmentation intermediate representation specifically given mask image employed parsing network first leverage speech drive mask generate talking segmentation disentangle semantic regions image style codes using maskguided encoder ultimately inject previously generated talking segmentation style codes maskguided stylegan synthesize video frame way textures fully preserved moreover approach inherently achieve background separation facilitate maskguided facial local editing particular editing mask swapping region textures given reference image eg hair lip eyebrows approach enables facial editing seamlessly generating talking face video experiments demonstrate proposed approach effectively preserve texture details generate temporally consistent video remaining competitive lip synchronization quantitative qualitative results hdtf mead datasets illustrate superior performance method existing methods,6,0.8000391858872461,6,0.8000391858872461
realisdance equip controllable character animation realistic hands controllable character animation emerging task generates character videos controlled pose sequences given character images although character consistency made significant progress via reference unet another crucial factor pose control well studied existing methods yet resulting several issues generation may fail input pose sequence corrupted hands generated using dwpose sequence blurry unrealistic generated video shaky pose sequence smooth enough paper present realisdance handle issues realisdance adaptively leverages three types poses avoiding failed generation caused corrupted pose sequences among pose types hamer provides accurate depth information hands enabling realisdance generate realistic hands even complex gestures besides using temporal attention main unet realisdance also inserts temporal attention pose guidance network smoothing video pose condition aspect moreover introduce pose shuffle augmentation training improve generation robustness video smoothness qualitative experiments demonstrate superiority realisdance existing methods especially hand quality,-1,0.0,-1,0.0
panoptic scene graph generation living threedimensional space moving forward fourth dimension time allow artificial intelligence develop comprehensive understanding environment introduce panoptic scene graph new representation bridges raw visual data perceived dynamic world highlevel visual understanding specifically abstracts rich sensory data nodes represent entities precise location status information edges capture temporal relations facilitate research new area build richly annotated dataset consisting rgbd videos total frames labeled panoptic segmentation masks well finegrained dynamic scene graphs solve propose transformerbased model predict panoptic segmentation masks track masks along time axis generate corresponding scene graphs via relation component extensive experiments new dataset show method serve strong baseline future research end provide realworld application example demonstrate achieve dynamic scene understanding integrating large language model system,-1,0.0,-1,0.0
diffusion posterior proximal sampling image restoration diffusion models demonstrated remarkable efficacy generating highquality samples existing diffusionbased image restoration algorithms exploit pretrained diffusion models leverage data priors yet still preserve elements inherited unconditional generation paradigm strategies initiate denoising process pure white noise incorporate random noise generative step leading oversmoothed results paper present refined paradigm diffusionbased image restoration specifically opt sample consistent measurement identity generative step exploiting sampling selection avenue output stability enhancement number candidate samples used selection adaptively determined based signaltonoise ratio timestep additionally start restoration process initialization combined measurement signal providing supplementary information better align generative process extensive experimental results analyses validate proposed method significantly enhances image restoration performance consuming negligible additional computational resources,13,1.0,13,1.0
texttomodel textconditioned neural network diffusion trainonceforall personalization generative artificial intelligence genai made significant progress understanding world knowledge generating content human languages across various modalities like texttotext large language models texttoimage stable diffusion texttovideo sora paper investigate capability genai texttomodel generation see whether genai comprehend hyperlevel knowledge embedded within ai parameters specifically study practical scenario termed trainonceforall personalization aiming generate personalized models diverse endusers tasks using text prompts inspired recent emergence neural network diffusion present tina textconditioned neural network diffusion trainonceforall personalization tina leverages diffusion transformer model conditioned task descriptions embedded using clip model despite astronomical number potential personalized tasks eg design tina demonstrates remarkable indistribution outofdistribution generalization even trained small datasets sim verify whether tina understands world knowledge analyzing capabilities zeroshotfewshot image prompts different numbers personalized classes prompts natural language descriptions predicting unseen entities,-1,0.0,-1,0.0
using spatial diffusions optoacoustic tomography image reconstruction optoacoustic tomography image reconstruction problem interest recent years exploiting exceptional generative power recently proposed diffusion models consider scheme based conditional diffusion process using simple initial image reconstruction method delay sum consider specially designed autoencoder architecture generates latent representation used conditional information generative diffusion process numerical results show merits proposal terms quality metrics psnr ssim showing conditional information generated terms initial reconstructed image able bias generative process diffusion model order enhance image correct artifacts even recover finer details initial reconstruction method able obtain,-1,0.0,-1,0.0
analysing diffusion segmentation medical images denoising diffusion probabilistic models become increasingly popular due ability offer probabilistic modeling generate diverse outputs versatility inspired adaptation image segmentation multiple predictions model produce segmentation results achieve high quality also capture uncertainty inherent model powerful architectures proposed improving diffusion segmentation performance however notable lack analysis discussions differences diffusion segmentation image generation thorough evaluations missing distinguish improvements architectures provide segmentation general benefit diffusion segmentation specifically work critically analyse discuss diffusion segmentation medical images differs diffusion image generation particular focus training behavior furthermore conduct assessment proposed diffusion segmentation architectures perform trained directly segmentation lastly explore different medical segmentation tasks influence diffusion segmentation behavior diffusion process could adapted accordingly analyses aim provide indepth insights behavior diffusion segmentation allow better design evaluation diffusion segmentation methods future,-1,0.0,-1,0.0
infinitydrive breaking time limits driving world models autonomous driving systems struggle complex scenarios due limited access diverse extensive outofdistribution driving data critical safe navigation world models offer promising solution challenge however current driving world models constrained short time windows limited scenario diversity bridge gap introduce infinitydrive first driving world model exceptional generalization capabilities delivering stateoftheart performance high fidelity consistency diversity minutescale video generation infinitydrive introduces efficient spatiotemporal comodeling module paired extended temporal training strategy enabling highresolution video generation consistent spatial temporal coherence incorporating memory injection retention mechanisms alongside adaptive memory curve loss minimize cumulative errors achieving consistent video generation lasting frames minutes comprehensive experiments multiple datasets validate infinitydrives ability generate complex varied scenarios highlighting potential nextgeneration driving world model built evolving demands autonomous driving project homepage,16,1.0,16,1.0
general flow foundation affordance scalable robot learning address challenge acquiring realworld manipulation skills scalable framework hold belief identifying appropriate prediction target capable leveraging largescale datasets crucial achieving efficient universal learning therefore propose utilize flow represents future trajectories points objects interest ideal prediction target exploit scalable data resources turn attention human videos develop first time languageconditioned flow prediction model directly largescale rgbd human video datasets predicted flow offers actionable guidance thus facilitating zeroshot skill transfer realworld scenarios deploy method policy based closedloop flow prediction remarkably without indomain finetuning method achieves impressive success rate zeroshot humantorobot skill transfer covering tasks scenes framework features following benefits scalability leveraging crossembodiment data resources wide application multiple object categories including rigid articulated soft bodies stable skill transfer providing actionable guidance small inference domaingap code data supplementary materials available httpsgeneralflowgithubio,5,0.7680221383176689,5,0.7680221383176689
diffdti fast diffusion tensor imaging using featureenhanced joint diffusion model magnetic resonance diffusion tensor imaging dti critical tool neural disease diagnosis however long scan time greatly hinders widespread clinical use dti accelerate image acquisition featureenhanced joint diffusion model diffdti proposed obtain accurate dti parameter maps limited number diffusionweighted images dwis diffdti introduces joint diffusion model directly learns joint probability distribution dwis dti parametric maps conditional generation additionally feature enhancement fusion mechanism fefm designed incorporated generative process diffdti preserve fine structures generated dti maps comprehensive evaluation performance diffdti conducted human connectome project dataset results demonstrate diffdti outperforms existing stateoftheart fast dti imaging methods terms visual quality quantitative metrics furthermore diffdti shown ability produce highfidelity dti maps three dwis thus overcoming requirement minimum six dwis dti,3,1.0,3,1.0
seppo semipolicy preference optimization diffusion alignment reinforcement learning human feedback rlhf methods emerging way finetune diffusion models dms visual generation however commonly used onpolicy strategies limited generalization capability reward model offpolicy approaches require large amounts difficulttoobtain paired humanannotated data particularly visual generation tasks address limitations offpolicy rlhf propose preference optimization method aligns dms preferences without relying reward models paired humanannotated data specifically introduce semipolicy preference optimization seppo method seppo leverages previous checkpoints reference models using generate onpolicy reference samples replace losing images preference pairs approach allows us optimize using offpolicy winning images furthermore design strategy reference model selection expands exploration policy space notably simply treat reference samples negative examples learning instead design anchorbased criterion assess whether reference samples likely winning losing images allowing model selectively learn generated reference samples approach mitigates performance degradation caused uncertainty reference sample quality validate seppo across texttoimage texttovideo benchmarks seppo surpasses previous approaches texttoimage benchmarks also demonstrates outstanding performance texttovideo benchmarks code released httpsgithubcomdwanzhangaiseppo,7,0.9126867752823099,7,0.9126867752823099
evaluating mitigating ip infringement visual generative ai popularity visual generative ai models like dalle stable diffusion xl stable video diffusion sora increasing extensive evaluation discovered stateoftheart visual generative models generate content bears striking resemblance characters protected intellectual property rights held major entertainment companies sony marvel nintendo raises potential legal concerns happens input prompt contains characters name even descriptive details characteristics mitigate ip infringement problems also propose defense method detail develop revised generation paradigm identify potentially infringing generated content prevent ip infringement utilizing guidance techniques diffusion process capability recognize generated content may infringing intellectual property rights mitigate infringement employing guidance methods throughout diffusion process without retrain finetune pretrained models experiments wellknown character ips like spiderman iron man superman demonstrate effectiveness proposed defense method data code found,4,0.8669530962144073,4,0.8669530962144073
dtvlt multimodal diverse text benchmark visual language tracking based llm visual language tracking vlt emerged cuttingedge research area harnessing linguistic data enhance algorithms multimodal inputs broadening scope traditional single object tracking sot encompass video understanding applications despite vlt benchmarks still depend succinct humanannotated text descriptions video descriptions often fall short capturing nuances video content dynamics lack stylistic variety language constrained uniform level detail fixed annotation frequency result algorithms tend default memorize answer strategy diverging core objective achieving deeper understanding video content fortunately emergence large language models llms enabled generation diverse text work utilizes llms generate varied semantic annotations terms text lengths granularities representative sot benchmarks thereby establishing novel multimodal benchmark specifically propose new visual language tracking benchmark diverse texts named dtvlt based five prominent vlt sot benchmarks including three subtasks shortterm tracking longterm tracking global instance tracking offer four granularity texts benchmark considering extent density semantic information expect multigranular generation strategy foster favorable environment vlt video understanding research conduct comprehensive experimental analyses dtvlt evaluating impact diverse text tracking performance hope identified performance bottlenecks existing algorithms support research vlt video understanding proposed benchmark experimental results toolkit released gradually httpvideocubeaitestunioncom,0,1.0,0,1.0
generic framework high fidelity talking face generation finegrained intramodal alignment despite numerous completed studies achieving high fidelity talking face generation highly synchronized lip movements corresponding arbitrary audio remains significant challenge field shortcomings published studies continue confuse many researchers paper introduces generic framework high fidelity talking face generation finegrained intramodal alignment reenact high fidelity original video producing highly synchronized lip movements regardless given audio tones volumes key success use diagonal matrix enhance ordinary alignment audioimage intramodal features significantly increases comparative learning positive negative samples additionally multiscaled supervision module introduced comprehensively reenact perceptional fidelity original video across facial region emphasizing synchronization lip movements input audio fusion network used fuse facial region rest experimental results demonstrate significant achievements reenactment original video quality well highly synchronized talking lips outperforming generic framework produce talking videos competitively closer ground truth level current stateoftheart methods,6,1.0,6,1.0
single generated video highfidelity reconstruction dynamic gaussian surfels video generative models receiving particular attention given ability generate realistic imaginative frames besides models also observed exhibit strong consistency significantly enhancing potential act world simulators work present novel reconstruction model excels accurately reconstructing ie sequential representations single generated videos addressing challenges associated nonrigidity frame distortion capability pivotal creating highfidelity virtual contents maintain spatial temporal coherence core proposed dynamic gaussian surfels dgs technique dgs optimizes timevarying warping functions transform gaussian surfels surface elements static state dynamically warped state transformation enables precise depiction motion deformation time preserve structural integrity surfacealigned gaussian surfels design warpedstate geometric regularization based continuous warping fields estimating normals additionally learn refinements rotation scaling parameters gaussian surfels greatly alleviates texture flickering warping process enhances capture finegrained appearance details also contains novel initialization state provides proper start warping fields dgs equipping existing video generative model overall framework demonstrates highfidelity generation appearance geometry,-1,0.0,-1,0.0
multigranularity contrastive crossmodal collaborative generation endtoend longterm video question answering longterm video question answering videoqa challenging visionandlanguage bridging task focusing semantic understanding untrimmed longterm videos diverse freeform questions simultaneously emphasizing comprehensive crossmodal reasoning yield precise answers canonical approaches often rely offtheshelf feature extractors detour expensive computation overhead often result domainindependent modalityunrelated representations furthermore inherent gradient blocking unimodal comprehension crossmodal interaction hinders reliable answer generation contrast recent emerging successful videolanguage pretraining models enable costeffective endtoend modeling fall short domainspecific ratiocination exhibit disparities task formulation toward end present entirely endtoend solution longterm videoqa multigranularity contrastive crossmodal collaborative generation mcg model derive discriminative representations possessing high visual concepts introduce joint unimodal modeling jum clipbone architecture leverage multigranularity contrastive learning mcl harness intrinsically explicitly exhibited semantic correspondences alleviate task formulation discrepancy problem propose crossmodal collaborative generation ccg module reformulate videoqa generative task instead conventional classification scheme empowering model capability crossmodal highsemantic fusion generation rationalize answer extensive experiments conducted six publicly available videoqa datasets underscore superiority proposed method,0,0.8459798921779021,0,0.8459798921779021
motif making text count image animation motion focal loss textimagetovideo generation aims generate video image following text description also referred textguided image animation existing methods struggle generate videos align well text prompts particularly motion specified overcome limitation introduce motif simple yet effective approach directs models learning regions motion thereby improving text alignment motion generation use optical flow generate motion heatmap weight loss according intensity motion modified objective leads noticeable improvements complements existing methods utilize motion priors model inputs additionally due lack diverse benchmark evaluating generation propose bench dataset consists imagetext pairs robust evaluation present human evaluation protocol asks annotators select overall preference two videos followed justifications comprehensive evaluation bench motif outperforms nine opensourced models achieving average preference bench additional results released,-1,0.0,-1,0.0
dreamdance animating human images enriching geometry cues poses work present dreamdance novel method animating human images using skeleton pose sequences conditional inputs existing approaches struggle generating coherent highquality content efficient userfriendly manner concretely baseline methods relying pose guidance lack cues information leading suboptimal results methods using representation guidance achieve higher quality involve cumbersome timeintensive process address limitations dreamdance enriches geometry cues poses introducing efficient diffusion model enabling highquality human image animation various guidance key insight human images naturally exhibit multiple levels correlation progressing coarse skeleton poses finegrained geometry cues geometry cues explicit appearance details capturing correlations could enrich guidance signals facilitating intraframe coherency interframe consistency specifically construct dataset comprising highquality dance videos detailed frame annotations including human pose depth normal maps next introduce mutually aligned geometry diffusion model generate finegrained depth normal maps enriched guidance finally crossdomain controller incorporates multilevel guidance animate human images effectively video diffusion model extensive experiments demonstrate method achieves stateoftheart performance animating human images,-1,0.0,-1,0.0
approximately invertible neural network learned image compression learned image compression attracted considerable interests recent years typically comprises analysis transform synthesis transform quantization entropy coding model analysis transform synthesis transform used encode image latent feature decode quantized feature reconstruct image regarded coupled transforms however analysis transform synthesis transform designed independently existing methods making unreliable highquality image compression inspired invertible neural networks generative modeling invertible modules used construct coupled analysis synthesis transforms considering noise introduced feature quantization invalidates invertible process paper proposes approximately invertible neural network ainn framework learned image compression formulates ratedistortion optimization lossy image compression using inn quantization differentiates using inn generative modelling generally speaking ainn used theoretical foundation inn based lossy compression method based formulation ainn progressive denoising module pdm developed effectively reduce quantization noise decoding moreover cascaded feature recovery module cfrm designed learn highdimensional feature recovery lowdimensional ones reduce noise feature channel compression addition frequencyenhanced decomposition synthesis module fdsm developed explicitly enhancing highfrequency components image address loss highfrequency information inherent neural network based image compression extensive experiments demonstrate proposed ainn outperforms existing learned image compression methods,2,1.0,2,1.0
textdriven tumor synthesis tumor synthesis generate examples ai often misses overdetects improving ai performance training challenging cases however existing synthesis methods typically unconditional generating images random variables conditioned tumor shapes lack controllability specific tumor characteristics texture heterogeneity boundaries pathology type result generated tumors may overly similar duplicates existing training data failing effectively address ais weaknesses propose new textdriven tumor synthesis approach termed textomorph provides textual control tumor characteristics particularly beneficial examples confuse ai early tumor detection increasing sensitivity tumor segmentation precise radiotherapy increasing dsc classification benign malignant tumors improving sensitivity incorporating text mined radiology reports synthesis process increase variability controllability synthetic tumors target ais failure cases precisely moreover textomorph uses contrastive learning across different texts ct scans significantly reducing dependence scarce imagereport pairs pairs used study leveraging large corpus radiology reports finally developed rigorous tests evaluate synthetic tumors including textdriven visual turing test radiomics pattern analysis showing synthetic tumors realistic diverse texture heterogeneity boundaries pathology,3,0.705205860145368,3,0.705205860145368
finegrained dynamic network generic event boundary detection generic event boundary detection gebd aims pinpointing event boundaries naturally perceived humans playing crucial role understanding longform videos given diverse nature generic boundaries spanning different video appearances objects actions task remains challenging existing methods usually detect various boundaries protocol regardless distinctive characteristics detection difficulties resulting suboptimal performance intuitively intelligent reasonable way adaptively detect boundaries considering special properties light propose novel dynamic pipeline generic event boundaries named dybdet introducing multiexit network architecture dybdet automatically learns subnet allocation different video snippets enabling finegrained detection various boundaries besides multiorder difference detector also proposed ensure generic boundaries effectively identified adaptively processed extensive experiments challenging kineticsgebd tapos datasets demonstrate adopting dynamic strategy significantly benefits gebd tasks leading obvious improvements performance efficiency compared current stateoftheart,-1,0.0,-1,0.0
large generative modelassisted talkingface semantic communication system rapid development generative artificial intelligence ai continually unveils potential semantic communication semcom however current talkingface semcom systems still encounter challenges low bandwidth utilization semantic ambiguity diminished quality experience qoe study introduces large generative modelassisted talkingface semantic communication lgmtsc system tailored talkingface video communication firstly introduce generative semantic extractor gse transmitter based funasr model convert semantically sparse talkingface videos texts high information density secondly establish private knowledge base kb based large language model llm semantic disambiguation correction complemented joint knowledge basesemanticchannel coding scheme finally receiver propose generative semantic reconstructor gsr utilizes sadtalker models transform text back highqoe talkingface video matching users timbre simulation results demonstrate feasibility effectiveness proposed lgmtsc system,-1,0.0,-1,0.0
mumullama multimodal music understanding generation via large language models research large language models advanced significantly across text speech images videos however multimodal music understanding generation remain underexplored due lack wellannotated datasets address introduce dataset hours multimodal data including text images videos music annotations based dataset propose mumullama model leverages pretrained encoders music images videos music generation integrate audioldm musicgen evaluation across four tasksmusic understanding texttomusic generation promptbased music editing multimodal music generationdemonstrates mumullama outperforms stateoftheart models showing potential multimodal music applications,0,0.9529255615718795,0,0.9529255615718795
attention normalization impacts cardinality generalization slot attention objectcentric scene decompositions important representations downstream tasks fields computer vision robotics recently proposed slot attention module already leveraged several derivative works image segmentation object tracking videos deep learning component performs unsupervised objectcentric scene decomposition input images based attention architecture latent slot vectors hold compressed information objects attend localized perceptual features input image paper demonstrate design decisions normalizing aggregated values attention architecture considerable impact capabilities slot attention generalize higher number slots objects seen training propose investigate alternatives original normalization scheme increase generalization capabilities slot attention varying slot object counts resulting performance gains task unsupervised image segmentation newly proposed normalizations represent minimal easy implement modifications usual slot attention module changing value aggregation mechanism weighted mean operation scaled weighted sum operation,-1,0.0,-1,0.0
physdreamer physicsbased interaction objects via video generation realistic object interactions crucial creating immersive virtual experiences yet synthesizing realistic object dynamics response novel interactions remains significant challenge unlike unconditional textconditioned dynamics generation actionconditioned dynamics requires perceiving physical material properties objects grounding motion prediction properties object stiffness however estimating physical material properties open problem due lack material groundtruth data measuring properties real objects highly difficult present physdreamer physicsbased approach endows static objects interactive dynamics leveraging object dynamics priors learned video generation models distilling priors physdreamer enables synthesis realistic object responses novel interactions external forces agent manipulations demonstrate approach diverse examples elastic objects evaluate realism synthesized interactions user study physdreamer takes step towards engaging realistic virtual experiences enabling static objects dynamically respond interactive stimuli physically plausible manner see project page httpsphysdreamergithubio,18,0.9317039842484443,18,0.9317039842484443
highfidelity freeview synthesis emotional talking head present novel approach synthesizing talking heads controllable emotion featuring enhanced lip synchronization rendering quality despite significant progress field prior methods still suffer multiview consistency lack emotional expressiveness address issues collect dataset calibrated multiview videos emotional annotations perframe geometry training dataset propose textitspeechtogeometrytoappearance mapping framework first predicts faithful geometry sequence audio features appearance talking head represented gaussians synthesized predicted geometry appearance disentangled canonical dynamic gaussians learned multiview videos fused render freeview talking head animation moreover model enables controllable emotion generated talking heads rendered widerange views method exhibits improved rendering quality stability lip motion generation capturing dynamic facial details wrinkles subtle expressions experiments demonstrate effectiveness approach generating highfidelity emotioncontrollable talking heads code dataset released,6,0.9310587210808372,6,0.9310587210808372
champ controllable consistent human image animation parametric guidance study introduce methodology human image animation leveraging human parametric model within latent diffusion framework enhance shape alignment motion guidance curernt human generative techniques methodology utilizes smplskinned multiperson linear model human parametric model establish unified representation body shape pose facilitates accurate capture intricate human geometry motion characteristics source videos specifically incorporate rendered depth images normal maps semantic maps obtained smpl sequences alongside skeletonbased motion guidance enrich conditions latent diffusion model comprehensive shape detailed pose attributes multilayer motion fusion module integrating selfattention mechanisms employed fuse shape motion latent representations spatial domain representing human parametric model motion guidance perform parametric shape alignment human body reference image source video motion experimental evaluations conducted benchmark datasets demonstrate methodologys superior ability generate highquality human animations accurately capture pose shape variations furthermore approach also exhibits superior generalization capabilities proposed inthewild dataset project page httpsfudangenerativevisiongithubiochamp,-1,0.0,-1,0.0
contextual ad narration interleaved multimodal sequence audio description ad task aims generate descriptions visual elements visually impaired individuals help access longform video contents like movie video feature text character bank context information inputs generated ads able correspond characters name provide reasonable contextual descriptions help audience understand storyline movie achieve goal propose leverage pretrained foundation models simple unified framework generate ads interleaved multimodal sequence input termed uniad enhance alignment features across various modalities finer granularity introduce simple lightweight module maps video features textual feature space moreover also propose characterrefinement module provide precise information identifying main characters play significant role video context unique designs incorporate contextual information contrastive loss architecture generate smooth contextual ads experiments madeval dataset show uniad achieve stateoftheart performance ad generation demonstrates effectiveness approach code available httpsgithubcommcgnjuuniad,0,1.0,0,1.0
uniforensics face forgery detection via general facial representation previous deepfake detection methods mostly depend lowlevel textural features vulnerable perturbations fall short detecting unseen forgery methods contrast highlevel semantic features less susceptible perturbations limited forgeryspecific artifacts thus stronger generalization motivated propose detection method utilizes highlevel semantic features faces identify inconsistencies temporal domain introduce uniforensics novel deepfake detection framework leverages transformerbased video classification network initialized metafunctional face encoder enriched facial representation way take advantage powerful spatiotemporal model highlevel semantic information faces furthermore leverage easily accessible real face data guide model focusing spatiotemporal features design dynamic video selfblending dvsb method efficiently generate training samples diverse spatiotemporal forgery traces using real facial videos based advance framework twostage training approach first stage employs novel selfsupervised contrastive learning encourage network focus forgery traces impelling videos generated forgery process similar representations basis representation learned first stage second stage involves finetuning face forgery detection dataset build deepfake detector extensive experiments validates uniforensics outperforms existing face forgery methods generalization ability robustness particular method achieves cross dataset auc challenging dfdc respectively,4,0.9284947904172915,4,0.9284947904172915
tkgdm trainingfree chroma key content generation diffusion model diffusion models enabled generation highquality images strong focus realism textual fidelity yet largescale texttoimage models stable diffusion struggle generate images foreground objects placed chroma key background limiting ability separate foreground background elements without finetuning address limitation present novel trainingfree chroma key content generation diffusion model tkgdm optimizes initial random noise produce images foreground objects specifiable color background proposed method first explore manipulation color aspects initial noise controlled background generation enabling precise separation foreground background without finetuning extensive experiments demonstrate trainingfree method outperforms existing methods qualitative quantitative evaluations matching surpassing finetuned models finally successfully extend tasks eg consistency models texttovideo highlighting transformative potential across various generative applications independent control foreground background crucial,9,0.8514282890742526,9,0.8514282890742526
domain generalization pose estimation nerfbased image synthesis work introduces novel augmentation method increases diversity train set improve generalization abilities pose estimation network purpose neural radiance field trained synthetic images exploited generate augmented set method enriches initial set enabling synthesis images unseen viewpoints ii rich illumination conditions appearance extrapolation iii randomized textures validate augmentation method challenging usecase spacecraft pose estimation show significantly improves pose estimation generalization capabilities speed dataset method reduces error pose target domains,14,1.0,14,1.0
fusecaps investigating feature fusion based framework capsule endoscopy image classification order improve model accuracy generalization class imbalance issues work offers strong methodology classifying endoscopic images suggest hybrid feature extraction method combines convolutional neural networks cnns multilayer perceptrons mlps radiomics rich multiscale feature extraction made possible combination captures deep handmade representations features used classification head classify diseases producing model higher generalization accuracy framework achieved validation accuracy capsule endoscopy video frame classification task,-1,0.0,-1,0.0
explicitnerfqa quality assessment database explicit nerf model compression recent years neural radiance fields nerf demonstrated significant advantages representing synthesizing scenes explicit nerf models facilitate practical nerf applications faster rendering speed also attract considerable attention nerf compression due huge storage cost address challenge nerf compression study paper construct new dataset called explicitnerfqa use objects diverse geometries textures material complexities train four typical explicit nerf models across five parameter levels lossy compression introduced model generation pivoting selection key parameters hash table size instantngp voxel grid resolution plenoxels rendering nerf samples processed video sequences pvs large scale subjective experiment lab environment conducted collect subjective scores viewers diversity content accuracy mean opinion scores mos characteristics nerf distortion comprehensively presented establishing heterogeneity proposed dataset stateoftheart objective metrics tested new dataset best person correlation around collected fullreference objective metric tested noreference metrics report poor results correlations demonstrating need development robust noreference metrics dataset including nerf samples source objects multiview images nerf generation pvss mos made publicly available following location httpsgithubcomyukexingexplicitnerfqa,12,0.36425205153424134,12,0.36425205153424134
unirs unifying multitemporal remote sensing tasks vision language models domain gap remote sensing imagery natural images recently received widespread attention visionlanguage models vlms demonstrated excellent generalization performance remote sensing multimodal tasks however current research still limited exploring remote sensing vlms handle different types visual inputs bridge gap introduce textbfunirs first visionlanguage model textbfunifying multitemporal textbfremote textbfsensing tasks across various types visual input unirs supports single images dualtime image pairs videos input enabling comprehensive remote sensing temporal analysis within unified framework adopt unified visual representation approach enabling model accept various visual inputs dualtime image pair tasks customize change extraction module enhance extraction spatiotemporal features additionally design prompt augmentation mechanism tailored models reasoning process utilizing prior knowledge generalpurpose vlm provide clues unirs promote multitask knowledge sharing model jointly finetuned mixed dataset experimental results show unirs achieves stateoftheart performance across diverse tasks including visual question answering change captioning video scene classification highlighting versatility effectiveness unifying multitemporal remote sensing tasks code dataset released soon,-1,0.0,-1,0.0
arges spatiotemporal transformer ulcerative colitis severity assessment endoscopy videos accurate assessment disease severity endoscopy videos ulcerative colitis uc crucial evaluating drug efficacy clinical trials severity often measured mayo endoscopic subscore mes ulcerative colitis endoscopic index severity uceis score however expert mesuceis annotation timeconsuming susceptible interrater variability factors addressable automation automation attempts framelevel labels face challenges fullysupervised solutions due prevalence videolevel labels clinical trials cnnbased weaklysupervised models wsl endtoend training lack generalization new disease scores ignore spatiotemporal information crucial accurate scoring address limitations propose arges deep learning framework utilizes transformer positional encoding incorporate spatiotemporal information frame features estimate disease severity scores endoscopy video extracted features derived foundation model argesfm pretrained large diverse dataset multiple clinical trials frames videos evaluate four uc disease severity scores including mes three uceis component scores test set evaluation indicates significant improvements scores increasing mes three uceis component scores compared stateoftheart methods prospective validation previously unseen clinical trial data demonstrates models successful generalization,19,1.0,19,1.0
towards nationwide analytical healthcare infrastructures privacypreserving augmented knee rehabilitation case study purpose paper contribute towards nearfuture privacypreserving big data analytical healthcare platforms capable processing streamed uploaded timeseries data videos patients experimental work includes reallife knee rehabilitation video dataset capturing set exercises simple personalised general challenging movements aimed returning sport convert video mobile privacypreserving diagnostic timeseries data employed google mediapipe pose estimation developed proofofconcept algorithms augment knee exercise videos overlaying patient stick figure elements updating generated timeseries plot knee angle estimation streamed csv file format patients physiotherapists video sidetoside timeseries visually indicating potential issues excessive knee flexion unstable knee movements stick figure overlay errors possible setting apriori kneeangle parameters address adherence rehabilitation programme quantify exercise sets repetitions adaptive algorithm correctly identify exercises side frontview videos transparent algorithm design adaptive visual analysis various knee exercise patterns contributes towards interpretable ai inform nearfuture privacypreserving nonvendor locking opensource developments enduser computing devices onpremises nonproprietary cloud platforms deployed within national healthcare system,-1,0.0,-1,0.0
tuningfree visual customization via view iterative selfattention control finetuning diffusion models enable wide range personalized generation editing applications diverse visual modalities lowrank adaptation lora accelerates finetuning process still requires multiple reference images timeconsuming training constrains scalability largescale realtime applications paper propose textitview iterative selfattention control visctrl tackle challenge specifically visctrl trainingfree method injects appearance structure userspecified subject another subject target image unlike previous approaches require finetuning model initially obtain initial noise reference target images ddim inversion denoising phase features reference image injected target image via selfattention mechanism notably iteratively performing feature injection process ensure reference image features gradually integrated target image approach results consistent harmonious editing one reference image denoising steps moreover benefiting plugandplay architecture design proposed feature gradual sampling strategy multiview editing method easily extended edit complex visual domains extensive experiments show efficacy visctrl across spectrum tasks including personalized editing images videos scenes,-1,0.0,-1,0.0
binary noise binary tasks masked bernoulli diffusion unsupervised anomaly detection high performance denoising diffusion models image generation paved way application unsupervised medical anomaly detection diffusionbased methods require lot gpu memory long sampling times present novel fast unsupervised anomaly detection approach based latent bernoulli diffusion models first apply autoencoder compress input images binary latent representation next diffusion model follows bernoulli noise schedule employed latent space trained restore binary latent representations perturbed ones binary nature diffusion model allows us identify entries latent space high probability flipping binary code denoising process indicates outofdistribution data propose masking algorithm based probabilities improves anomaly detection scores achieve stateoftheart performance compared diffusionbased unsupervised anomaly detection algorithms significantly reducing sampling time memory consumption code available,13,0.9750569461574474,13,0.9750569461574474
gaussian need unified framework solving inverse problems via diffusion posterior sampling diffusion models generate variety highquality images modeling complex data distributions trained diffusion models also effective image priors solving inverse problems existing diffusionbased methods integrate data consistency steps within diffusion reverse sampling process data consistency steps rely approximate likelihood function paper show existing approximations either insufficient computationally inefficient address issues propose unified likelihood approximation method incorporates covariance correction term enhance performance avoids propagating gradients diffusion model correction term integrated reverse diffusion sampling process achieves better convergence towards true data posterior selected distributions improves performance realworld natural image datasets furthermore present efficient way factorize invert covariance matrix likelihood function several inverse problems present comprehensive experiments demonstrate effectiveness method several existing approaches,13,1.0,13,1.0
stable diffusion segmentation biomedical images singlestep reverse process diffusion models demonstrated effectiveness across various generative tasks however applied medical image segmentation models encounter several challenges including significant resource time requirements also necessitate multistep reverse process multiple samples produce reliable predictions address challenges introduce first latent diffusion segmentation model named sdseg built upon stable diffusion sd sdseg incorporates straightforward latent estimation strategy facilitate singlestep reverse process utilizes latent fusion concatenation remove necessity multiple samples extensive experiments indicate sdseg surpasses existing stateoftheart methods five benchmark datasets featuring diverse imaging modalities remarkably sdseg capable generating stable predictions solitary reverse step sample epitomizing models stability implied name code available httpsgithubcomlintianyustablediffusionseg,-1,0.0,-1,0.0
splatmover multistage openvocabulary robotic manipulation via editable gaussian splatting present splatmover modular robotics stack openvocabulary robotic manipulation leverages editability gaussian splatting gsplat scene representations enable multistage manipulation tasks splatmover consists asksplat gsplat representation distills semantic grasp affordance features scene asksplat enables geometric semantic affordance understanding scenes critical many robotics tasks ii seesplat realtime sceneediting module using semantic masking infilling visualize motions objects result robot interactions realworld seesplat creates digital twin evolving environment throughout manipulation task iii graspsplat grasp generation module uses asksplat seesplat propose affordancealigned candidate grasps openworld objects asksplat trained realtime rgb images brief scanning phase prior operation seesplat graspsplat run realtime operation demonstrate superior performance splatmover hardware experiments kinova robot compared two recent baselines four singlestage openvocabulary manipulation tasks four multistage manipulation tasks using edited scene reflect changes due prior manipulation stages possible existing baselines video demonstrations code project available httpssplatmovergithubio,5,0.5687027558843686,5,0.5687027558843686
crowdsourced nerf collecting data production vehicles street view reconstruction recently neural radiance fields nerf achieved impressive results novel view synthesis blocknerf showed capability leveraging nerf build large cityscale models largescale modeling mass image data necessary collecting images specially designed datacollection vehicles support largescale applications acquire massive highquality data remains opening problem noting automotive industry huge amount image data crowdsourcing convenient way largescale data collection paper present crowdsourced framework utilizes substantial data captured production vehicles reconstruct scene nerf model approach solves key problem largescale reconstruction data comes use firstly crowdsourced massive data filtered remove redundancy keep balanced distribution terms time space structurefrommotion module performed refine camera poses finally images well poses used train nerf model certain block highlight present comprehensive framework integrates multiple modules including data selection sparse reconstruction sequence appearance embedding depth supervision ground surface occlusion completion complete system capable effectively processing reconstructing highquality scenes crowdsourced data extensive quantitative qualitative experiments conducted validate performance system moreover proposed application named firstview navigation leveraged nerf model generate street view guide driver synthesized video,1,1.0,1,1.0
agentstudio toolkit building general virtual agents general virtual agents need handle multimodal observations master complex action spaces selfimprove dynamic opendomain environments however existing environments often domainspecific require complex setups limits agent development evaluation realworld settings result current evaluations lack indepth analyses decompose fundamental agent capabilities introduce agentstudio trinity environments tools benchmarks address issues agentstudio provides lightweight interactive environment highly generic observation action spaces eg video observations guiapi actions integrates tools creating online benchmark tasks annotating gui elements labeling actions videos based environment tools curate online task suite benchmarks gui interactions function calling efficient autoevaluation also reorganize existing datasets collect new ones using tools establish three datasets groundui idmbench criticbench datasets evaluate fundamental agent abilities including gui grounding learning videos success detection pointing desiderata robust general openended virtual agents,-1,0.0,-1,0.0
ssnvc single stream neural video compression implicit temporal information recently neural video compression nvc techniques achieved remarkable performance even surpassing best traditional lossy video codec however existing nvc methods heavily rely transmitting motion vector mv generate accurate contextual features following drawbacks compressing transmitting mv requires specialized mv encoder decoder makes modules redundant due existence mv encoderdecoder training strategy complex paper present noval single stream nvc framework ssnvc removes complex mv encoderdecoder structure uses onestage training strategy ssnvc implicitly use temporal information adding previous entropy model feature current entropy model using previous two frame generate predicted motion information decoder side besides enhance frame generator generate higher quality reconstructed frame experiments demonstrate ssnvc achieve stateoftheart performance multiple benchmarks greatly simplify compression process well training process,2,1.0,2,1.0
multitalk enhancing talking head generation across languages multilingual video dataset recent studies speechdriven talking head generation achieved convincing results verbal articulations however generating accurate lipsyncs degrades applied input speech languages possibly due lack datasets covering broad spectrum facial movements across languages work introduce novel task generate talking heads speeches diverse languages collect new multilingual video dataset comprising hours talking videos languages proposed dataset present multilingually enhanced model incorporates languagespecific style embeddings enabling capture unique mouth movements associated language additionally present metric assessing lipsync accuracy multilingual settings demonstrate training talking head model proposed dataset significantly enhances multilingual performance codes datasets available httpsmultitalkgithubio,-1,0.0,-1,0.0
robot absence video foundation models enhance intermittent supervision paper investigates application video foundation models vifms generating robot data summaries enhance intermittent human supervision robot teams propose novel framework produces generic querydriven summaries longduration robot vision data three modalities storyboards short videos text user study involving participants evaluate efficacy summary methods allowing operators accurately retrieve observations actions occurred robot operating without supervision extended duration min findings reveal querydriven summaries significantly improve retrieval accuracy compared generic summaries raw data albeit increased task duration storyboards found effective presentation modality especially objectrelated queries work represents knowledge first zeroshot application vifms generating multimodal robottohuman communication intermittent supervision contexts demonstrating promise limitations models humanrobot interaction hri scenarios,0,0.8633212312004127,0,0.8633212312004127
passive deepfake detection across multimodalities comprehensive survey recent years deepfakes dfs utilized malicious purposes individual impersonation misinformation spreading artists style imitation raising questions ethical security concerns however existing surveys focused accuracy performance passive df detection approaches single modalities image video audio comprehensive survey explores passive approaches across multiple modalities including image video audio multimodal domains extend discussion beyond detection accuracy including generalization robustness attribution interpretability additionally discuss threat models passive approaches including potential adversarial strategies different levels adversary knowledge capabilities also highlights current challenges df detection including lack generalization across different generative models need comprehensive trustworthiness evaluation limitations existing multimodal approaches finally propose future research directions address unexplored emerging issues field passive df detection adaptive learning dynamic benchmark holistic trustworthiness evaluation multimodal detectors talkingface video generation,4,0.8860863713207672,4,0.8860863713207672
study data augmentation techniques overcome data scarcity wound classification using deep learning chronic wounds significant burden individuals healthcare system affecting millions people incurring high costs wound classification using deep learning techniques promising approach faster diagnosis treatment initiation however lack high quality data train ml models major challenge realize potential ml wound care fact data limitations biggest challenge studies using medical forensic imaging today study data augmentation techniques used overcome data scarcity limitations unlock potential deep learning based solutions study explore range data augmentation techniques geometric transformations wound images advanced gans enrich expand datasets using keras tensorflow pandas libraries implemented data augmentation techniques generate realistic wound images show geometric data augmentation improve classification performance scores top stateoftheart models across several key classes wounds experiments gan based augmentation prove viability using degans generate wound images richer variations study results show data augmentation valuable privacypreserving tool huge potential overcome data scarcity limitations believe part realworld mlbased wound care system,-1,0.0,-1,0.0
hybrid approach detection combining wasserstein gan transfer learning extremely contagious rapid growth drawn attention towards early diagnosis early diagnosis enables healthcare professionals government authorities break chain transition flatten epidemic curve number cases accelerating across developed world induced viral pneumonia cases big challenge overlapping cases viral pneumonia lung infections limited dataset long training hours serious problem cater limited amount data often results overfitting models due reason model predict generalized results fill gap proposed ganbased approach synthesize images later fed deep learning models classify images normal viral pneumonia specifically customized wasserstein gan proposed generate chest xray images compare real images expanded dataset used train four proposed deep learning models googlenet mnast result showed expanded dataset utilized deep learning models deliver high classification accuracies particular achieved highest accuracy among four proposed schemes rest models like googlenet mnast delivered testing accuracies respectively later efficiency models compared state art models basis accuracy proposed models applied address issue scant datasets problem image analysis,-1,0.0,-1,0.0
generative iris prior embedded transformer iris restoration iris restoration complexly degraded iris images aiming improve iris recognition performance challenging problem due complex degradation directly training convolutional neural network cnn without prior yield satisfactory results work propose generative iris prior embedded transformer model gformer build hierarchical encoderdecoder network employing transformer block generative iris prior first tame transformer blocks model longrange dependencies target images second pretrain iris generative adversarial network gan obtain rich iris prior incorporate iris restoration process iris feature modulator experiments demonstrate proposed gformer outperforms stateoftheart methods besides iris recognition performance significantly improved applying gformer,-1,0.0,-1,0.0
hpix generating vector maps satellite images vector maps find widespread utility across diverse domains due capacity store also represent discrete data boundaries building footprints disaster impact analysis digitization urban planning location points transport links although extensive research exists identifying building footprints road types satellite imagery generation vector maps imagery remains area limited exploration furthermore conventional map generation techniques rely laborintensive manual feature extraction rulebased approaches impose inherent limitations surmount limitations propose novel method called hpix utilizes modified generative adversarial networks gans generate vector tile map satellite images hpix incorporates two hierarchical frameworks one operating global level local level resulting comprehensive model empirical evaluations proposed approach showcases effectiveness producing highly accurate visually captivating vector tile maps derived satellite images extend studys application include mapping road intersections building footprints cluster based area,-1,0.0,-1,0.0
trimodal confluence temporal dynamics scene graph generation operating rooms comprehensive understanding surgical scenes allows monitoring surgical process reducing occurrence accidents enhancing efficiency medical professionals semantic modeling within operating rooms scene graph generation sgg task challenging since involves consecutive recognition subtle surgical actions prolonged periods address challenge propose trimodal ie images point clouds language confluence temporal dynamics framework termed tritempor diverging previous approaches integrated temporal information via memory graphs method embraces two advantages directly exploit bimodal temporal information video streaming hierarchical feature interaction prior knowledge large language models llms embedded alleviate classimbalance problem operating theatre specifically model performs temporal interactions across frames point clouds including scaleadaptive multiview temporal interaction viewtemp geometrictemporal point aggregation pointtemp furthermore transfer knowledge biomedical llm llavamed deepen comprehension intraoperative relations proposed tritempor enables aggregation trimodal features relationaware unification predict relations generate scene graphs experimental results benchmark demonstrate superior performance model longterm streaming,-1,0.0,-1,0.0
videodistill languageaware vision distillation video question answering significant advancements video question answering videoqa made thanks thriving large imagelanguage pretraining frameworks although imagelanguage models efficiently represent video language branches typically employ goalfree vision perception process interact vision language well answer generation thus omitting crucial visual cues paper inspired human recognition learning pattern propose videodistill framework languageaware ie goaldriven behavior vision perception answer generation process videodistill generates answers questionrelated visual embeddings follows thinkingobservinganswering approach closely resembles human behavior distinguishing previous research specifically develop languageaware gating mechanism replace standard crossattention avoiding languages direct fusion visual representations incorporate mechanism two key components entire framework first component differentiable sparse sampling module selects frames containing necessary dynamics semantics relevant questions second component vision refinement module merges existing spatialtemporal attention layers ensure extraction multigrained visual semantics associated questions conduct experimental evaluations various challenging video questionanswering benchmarks videodistill achieves stateoftheart performance general longform videoqa datasets addition verify videodistill effectively alleviate utilization language shortcut solutions egotaskqa dataset,0,0.8823715695525427,0,0.8823715695525427
tooncrafter generative cartoon interpolation introduce tooncrafter novel approach transcends traditional correspondencebased cartoon video interpolation paving way generative interpolation traditional methods implicitly assume linear motion absence complicated phenomena like disocclusion often struggle exaggerated nonlinear large motions occlusion commonly found cartoons resulting implausible even failed interpolation results overcome limitations explore potential adapting liveaction video priors better suit cartoon interpolation within generative framework tooncrafter effectively addresses challenges faced applying liveaction video motion priors generative cartoon interpolation first design toon rectification learning strategy seamlessly adapts liveaction video priors cartoon domain resolving domain gap content leakage issues next introduce dualreferencebased decoder compensate lost details due highly compressed latent prior spaces ensuring preservation fine details interpolation results finally design flexible sketch encoder empowers users interactive control interpolation results experimental results demonstrate proposed method produces visually convincing natural dynamics also effectively handles disocclusion comparative evaluation demonstrates notable superiority approach existing competitors,-1,0.0,-1,0.0
planllm video procedure planning refinable large language models video procedure planning ie planning sequence action steps given video frames start goal states essential ability embodied ai recent works utilize large language models llms generate enriched action step description texts guide action step decoding although llms introduced methods decode action steps closedset onehot vectors limiting models capability generalizing new steps tasks additionally fixed action step descriptions based worldlevel commonsense may contain noise specific instances visual states paper propose planllm crossmodal joint learning framework llms video procedure planning propose llmenhanced planning module fully uses generalization ability llms produce freeform planning output enhance action step decoding also propose mutual information maximization module connect worldlevel commonsense step descriptions samplespecific information visual states enabling llms employ reasoning ability generate step sequences assistance llms method closedset open vocabulary procedure planning tasks planllm achieves superior performance three benchmarks demonstrating effectiveness designs,-1,0.0,-1,0.0
denoising jointembedding predictive architecture jointembedding predictive architectures jepas shown substantial promise selfsupervised representation learning yet application generative modeling remains underexplored conversely diffusion models demonstrated significant efficacy modeling arbitrary probability distributions paper introduce denoising jointembedding predictive architecture djepa pioneering integration jepa within generative modeling recognizing jepa form masked image modeling reinterpret generalized nexttoken prediction strategy facilitating data generation autoregressive manner furthermore incorporate diffusion loss model pertoken probability distribution enabling data generation continuous space also adapt flow matching loss alternative diffusion loss thereby enhancing flexibility djepa empirically increased gflops djepa consistently achieves lower fid scores fewer training epochs indicating good scalability base large huge models outperform previous generative models across scales imagenet conditional generation benchmarks beyond image generation djepa wellsuited continuous data modeling including video audio,-1,0.0,-1,0.0
diffusion assisted image reconstruction optoacoustic tomography paper consider problem acoustic inversion context optoacoustic tomography image reconstruction problem leveraging ability recently proposed diffusion models image generative tasks among others devise image reconstruction architecture based conditional diffusion process scheme makes use initial image reconstruction preprocessed autoencoder generate adequate representation representation used conditional information generative diffusion process although computational requirements training implementing architecture low several design choices discussed work made keep manageable numerical results show conditional information allows properly bias parameters diffusion model improve quality initial reconstructed image eliminating artifacts even reconstructing finer details groundtruth image recoverable initial image reconstruction method also tested proposal experimental conditions obtained results line corresponding numerical simulations improvements image quality terms peak signaltonoise ratio observed,-1,0.0,-1,0.0
frequencydomain refinement multiscale diffusion super resolution performance single image superresolution depends heavily generate complement highfrequency details lowresolution images recently diffusionbased models exhibit great potential generating highquality images superresolution tasks however existing models encounter difficulties directly predicting highfrequency information wide bandwidth solely utilizing highresolution ground truth target sampling timesteps tackle problem achieve higherquality superresolution propose novel frequency domainguided multiscale diffusion model fddiff decomposes highfrequency information complementing process finergrained steps particular wavelet packetbased frequency complement chain developed provide multiscale intermediate targets increasing bandwidth reverse diffusion process fddiff guides reverse diffusion process progressively complement missing highfrequency details timesteps moreover design multiscale frequency refinement network predict required highfrequency components multiple scales within one unified network comprehensive evaluations popular benchmarks conducted demonstrate fddiff outperforms prior generative methods higherfidelity superresolution results,-1,0.0,-1,0.0
diffusion transformer captures spatialtemporal dependencies theory gaussian process data diffusion transformer backbone sora video generation successfully scales capacity diffusion models pioneering new avenues highfidelity sequential data generation unlike static data images sequential data consists consecutive data frames indexed time exhibiting rich spatial temporal dependencies dependencies represent underlying dynamic model critical validate generated data paper make first theoretical step towards bridging diffusion transformers capturing spatialtemporal dependencies specifically establish score approximation distribution estimation guarantees diffusion transformers learning gaussian process data covariance functions various decay patterns highlight spatialtemporal dependencies captured affect learning efficiency study proposes novel transformer approximation theory transformer acts unroll algorithm support theoretical results numerical experiments providing strong evidence spatialtemporal dependencies captured within attention layers aligning approximation theory,13,0.9378102057831864,13,0.9378102057831864
presto fast motion planning using diffusion models based keyconfiguration environment representation introduce learningguided motion planning framework generates seed trajectories using diffusion model trajectory optimization given workspace method approximates configuration space cspace obstacles environment representation consisting sparse set taskrelated key configurations used conditioning input diffusion model diffusion model integrates regularization terms encourage smooth collisionfree trajectories training trajectory optimization refines generated seed trajectories correct colliding segments experimental results demonstrate highquality trajectory priors learned cspacegrounded diffusion model enable efficient generation collisionfree trajectories narrowpassage environments outperforming previous learning planningbased baselines videos additional materials found project page httpskiwisherbetgithubiopresto,5,0.444259845335824,5,0.444259845335824
casc conditionaware semantic communication latent diffusion models diffusionbased semantic communication methods shown significant advantages image transmission harnessing generative power diffusion models however still face challenges including generation randomness leads distorted reconstructions high computational costs address issues propose casc conditionaware semantic communication framework incorporates latent diffusion model ldmbased denoiser ldm denoiser receiver utilizes received noisy latent codes conditioning signal reconstruct latent codes enabling decoder accurately recover source image operating latent space ldm reduces computational complexity compared traditional diffusion models dms additionally introduce conditionaware neural network dynamically adjusts weights hidden layers ldm based conditioning signal enables finer control generation process significantly improving perceptual quality reconstructed images experimental results show casc significantly outperforms deepjscc perceptual quality visual effect moreover casc reduces inference time compared existing dmbased semantic communication systems maintaining comparable perceptual performance ablation studies also validate effectiveness module improving image reconstruction quality,13,0.8647848699374058,13,0.8647848699374058
bracket diffusion hdr image generation consistent ldr denoising demonstrate generating hdr images using concerted action multiple blackbox pretrained ldr image diffusion models relying pretrained ldr generative diffusion models vital first sufficiently large hdr image dataset available retrain second even retraining models impossible compute budgets instead seek inspiration hdr image capture literature traditionally fuses sets ldr images called exposure brackets produce single hdr image operate multiple denoising processes generate multiple ldr brackets together form valid hdr result key making work introduce consistency term diffusion process couple brackets agree across exposure range share accounting possible differences due quantization error demonstrate stateoftheart unconditional conditional restorationtype generative modeling results yet hdr,-1,0.0,-1,0.0
dynamic duos building blocks dimensional mechanics mechanics studies relationships space time matter expressed terms dimensions length mathcall time mathcalt mass mathcalm dimension broadens scope mechanics geometric quantities dimensions form mathcallx like lengths areas kinematic quantities form mathcallxmathcalty like speeds accelerations eventually masscarrying quantities mass force momentum energy action power viscosity etc standard mechanical quantities dimensions form mathcalmmathcallxmathcalty x integers contribution use dimensional structure arrange masscarrying quantities table indexed x ratios quantities rows provide characteristic lengths columns characteristic times encompassing great variety physical phenomena atomic astronomical scales generally show picking duos mechanical quantities neither row column yields dynamics one mechanical quantity understood impelling motion impeding force mass prototypes impelling impeding factors many duos possible review provides novel synthesis revealing power dimensional analysis understand processes governed interplay two mechanical quantities elementary decomposition space time motion pairs mechanical factors foundation dimensional mechanics method review wishes promote advance review complemented online video lectures initiate discussion elaborate interplay two mechanical quantities,-1,0.0,-1,0.0
postmastoidectomy surface multiview synthesis single microscopy image cochlear implant ci procedures involve performing invasive mastoidectomy insert electrode array cochlea paper introduce novel pipeline capable generating synthetic multiview videos single ci microscope image approach use patients preoperative ct scan predict postmastoidectomy surface using method designed purpose manually align surface selected microscope frame obtain accurate initial pose reconstructed ct mesh relative microscope perform uv projection transfer colors frame surface textures novel views textured surface used generate large dataset synthetic frames ground truth poses evaluated quality synthetic views rendered using pyvista found rendering engines lead similarly highquality synthetic novelview frames compared ground truth structural similarity index methods averaging large dataset novel views known poses critical ongoing training method automatically estimate microscope pose registration preoperative ct facilitate augmented reality surgery dataset empower various downstream tasks integrating augmented reality ar tracking surgical tools supporting video analysis studies,-1,0.0,-1,0.0
prototypical transformer unified motion learners work introduce prototypical transformer protoformer general unified framework approaches various motion tasks prototype perspective protoformer seamlessly integrates prototype learning transformer thoughtfully considering motion dynamics introducing two innovative designs first crossattention prototyping discovers prototypes based signature motion patterns providing transparency understanding motion scenes second latent synchronization guides feature representation learning via prototypes effectively mitigating problem motion uncertainty empirical results demonstrate approach achieves competitive performance popular motion tasks optical flow scene depth furthermore exhibits generality across various downstream tasks including object tracking video stabilization,-1,0.0,-1,0.0
diffusionbased handobject interaction prediction egocentric videos understanding humans would behave handobject interaction vital applications service robot manipulation extended reality achieve recent works proposed simultaneously forecast hand trajectories object affordances human egocentric videos joint prediction serves comprehensive representation future handobject interactions space indicating potential human motion motivation however existing approaches mostly adopt autoregressive paradigm unidirectional prediction lacks mutual constraints within holistic future sequence accumulates errors along time axis meanwhile works basically overlook effect camera egomotion firstperson view predictions address limitations propose novel diffusionbased interaction prediction method namely forecast future hand trajectories object affordances concurrently iterative nonautoregressive manner transform sequential images latent feature space design denoising diffusion model predict future latent interaction features conditioned past ones motion features integrated conditional denoising process enable aware camera wearers dynamics accurate interaction prediction extensive experiments demonstrate method significantly outperforms stateoftheart baselines offtheshelf metrics newly proposed evaluation protocol highlights efficacy leveraging generative paradigm handobject interaction prediction code released open source,-1,0.0,-1,0.0
lamod latent motion diffusion model myocardial strain generation motion deformation analysis cardiac magnetic resonance cmr imaging videos crucial assessing myocardial strain patients abnormal heart functions recent advances deep learningbased image registration algorithms shown promising results predicting motion fields routinely acquired cmr sequences however accuracy often diminishes regions subtle appearance changes errors propagating time advanced imaging techniques displacement encoding stimulated echoes dense cmr offer highly accurate reproducible motion data require additional image acquisition poses challenges busy clinical flows paper introduce novel latent motion diffusion model lamod predict highly accurate dense motions standard cmr videos specifically method first employs encoder pretrained registration network learns latent motion features also considered deformationbased shape features image sequences supervised groundtruth motion provided dense lamod leverages probabilistic latent diffusion model reconstruct accurate motion extracted features experimental results demonstrate proposed method lamod significantly improves accuracy motion analysis standard cmr images hence improving myocardial strain analysis clinical settings cardiac patients code publicly available httpsgithubcomjrxinglamod,3,0.6305087491858323,3,0.6305087491858323
crossmodality translation generative adversarial networks unveil alzheimers disease biomarkers generative approaches crossmodality transformation recently gained significant attention neuroimaging previous work focused casecontrol data application generative models disorderspecific datasets ability preserve diagnostic patterns remain relatively unexplored hence study investigated use generative adversarial network gan context alzheimers disease ad generate functional network connectivity fnc structural magnetic resonance imaging data employed cyclegan synthesize data unpaired data transition enhanced transition integrating weak supervision cases paired data available findings revealed model could offer remarkable capability achieving structural similarity index measure ssim pm correlation pm fncs moreover qualitative analysis revealed similar patterns generated actual data comparing ad cognitively normal cn individuals particular observed significantly increased functional connectivity cerebellarsensory motor cerebellarvisual networks reduced connectivity cerebellarsubcortical auditorysensory motor sensory motorvisual cerebellarcognitive control networks additionally images generated model showed similar pattern atrophy hippocampal temporal regions alzheimers patients,3,0.8505330425781458,3,0.8505330425781458
synthesizing betaamyloid pet images structural mri preliminary study betaamyloid positron emission tomography abetapet imaging become critical tool alzheimers disease ad research diagnosis providing insights pathological accumulation amyloid plaques one hallmarks ad however high cost limited availability exposure radioactivity restrict widespread use abetapet imaging leading scarcity comprehensive datasets previous studies suggested structural magnetic resonance imaging mri readily available may serve viable alternative synthesizing abetapet images study propose approach utilize diffusion models synthesize abetapet images mri scans aiming overcome limitations associated direct pet imaging method generates highquality abetapet images cognitive normal cases although less effective mild cognitive impairment mci patients due variability abeta deposition patterns among subjects preliminary results suggest incorporating additional data larger sample mci cases multimodality information including clinical demographic details cognitive functional assessments longitudinal data may necessary improve abetapet image synthesis mci patients,3,0.7082057628860917,3,0.7082057628860917
neural differential appearance equations propose method reproduce dynamic appearance textures spacestationary timevarying visual statistics previous work decomposes dynamic textures static appearance motion focus dynamic appearance results motion variations fundamental properties rusting decaying melting weathering end adopt neural ordinary differential equation ode learn underlying dynamics appearance target exemplar simulate ode two phases warmup phase ode diffuses random noise initial state constrain evolution ode replicate evolution visual feature statistics exemplar generation phase particular innovation work neural ode achieving denoising evolution dynamics synthesis proposed temporal training scheme study relightable brdf nonrelightable rgb appearance models introduce new pilot datasets allowing first time study phenomena rgb provide dynamic textures acquired free online sources brdfs acquire dataset flashlit videos timevarying materials enabled simpletoconstruct setup experiments show method consistently yields realistic coherent results whereas prior works falter pronounced temporal appearance variations user study confirms approach preferred previous work exemplars,-1,0.0,-1,0.0
textvideo retrieval globallocal semantic consistent learning adapting largescale imagetext pretraining models eg clip video domain represents current stateoftheart textvideo retrieval primary approaches involve transferring textvideo pairs common embedding space leveraging crossmodal interactions specific entities semantic alignment though effective paradigms entail prohibitive computational costs leading inefficient retrieval address propose simple yet effective method globallocal semantic consistent learning glscl capitalizes latent shared semantics across modalities textvideo retrieval specifically introduce parameterfree global interaction module explore coarsegrained alignment devise shared local interaction module employs several learnable queries capture latent semantic concepts learning finegrained alignment furthermore interconsistency loss icl devised accomplish concept alignment visual query corresponding textual query intradiversity loss idl developed repulse distribution within visual textual queries generate discriminative concepts extensive experiments five widely used benchmarks ie msrvtt msvd didemo lsmdc activitynet substantiate superior effectiveness efficiency proposed method remarkably method achieves comparable performance sota well nearly times faster terms computational cost code available httpsgithubcomzchoiglscl,7,0.9126867752823099,7,0.9126867752823099
guiworld video benchmark dataset multimodal guioriented understanding recently multimodal large language models mllms used agents control keyboard mouse inputs directly perceiving graphical user interface gui generating corresponding commands however current agents primarily demonstrate strong understanding capabilities static environments mainly applied relatively simple domains web mobile interfaces argue robust gui agent capable perceiving temporal information gui including dynamic web content multistep tasks additionally possess comprehensive understanding various gui scenarios including desktop software multiwindow interactions end paper introduces new dataset termed guiworld features meticulously crafted humanmllm annotations extensively covering six gui scenarios eight types guioriented questions three formats evaluate capabilities current stateoftheart mllms including image llms video llms understanding various types gui content especially dynamic sequential content findings reveal current models struggle dynamic gui content without manually annotated keyframes operation history hand video llms fall short guioriented tasks given sparse gui video dataset therefore take initial step leveraging finetuned video llm guivid guioriented assistant demonstrating improved understanding various gui tasks however due limitations performance base llms conclude using video llms gui agents remains significant challenge believe work provides valuable insights future research dynamic gui content understanding dataset code publicly available httpsguiworldgithubio,0,0.9278286536901079,0,0.9278286536901079
dajc direct analog mjpeg converter video sensor node using lownoise switched capacitor macquantizer autocalibration sparsityaware adc advancement field internet thingsiot internet bodiesiob video camera applications using video sensor nodesvsns gained importance field autonomous driving health monitoring robot control security camera applications however applications typically involve high data rates due transmission highresolution video signals resulting high data volume generated analogtodigital converters adcs significant data deluge poses processing storage overheads exacerbating problem address challenge propose lowpower solution aimed reducing power consumption video sensor nodes vsns shifting computation digital domain inherently energyefficient analog domain unlike standard architectures computation processing typically performed digital signal processing dsp blocks adcs approach eliminates need blocks instead leverage switched capacitorbased computation unit analog domain resulting reduction power consumption achieve reduction power consumption compared digital implementations furthermore employ sparsityaware adc enabled significant compressed samples contribute small fraction total captured analog samples achieve lower adc conversion energy without considerable degradation contributing overall energy savings system,2,1.0,2,1.0
moto latent motion token bridging language learning robot manipulation videos recent developments large language models pretrained extensive corpora shown significant success various natural language processing tasks minimal finetuning success offers new promise robotics long constrained high cost actionlabeled data ask given abundant video data containing interactionrelated knowledge available rich corpus similar generative pretraining approach effectively applied enhance robot learning key challenge identify effective representation autoregressive pretraining benefits robot manipulation tasks inspired way humans learn new skills observing dynamic environments propose effective robotic learning emphasize motionrelated knowledge closely tied lowlevel actions hardwareagnostic facilitating transfer learned motions actual robot actions end introduce moto converts video content latent motion token sequences latent motion tokenizer learning bridging language motion videos unsupervised manner pretrain motogpt motion token autoregression enabling capture diverse visual motion knowledge pretraining motogpt demonstrates promising ability produce semantically interpretable motion tokens predict plausible motion trajectories assess trajectory rationality output likelihood transfer learned motion priors real robot actions implement cofinetuning strategy seamlessly bridges latent motion token prediction real robot control extensive experiments show finetuned motogpt exhibits superior robustness efficiency robot manipulation benchmarks underscoring effectiveness transferring knowledge video data downstream visual manipulation tasks,5,0.674122388343842,5,0.674122388343842
uniadafocus spatialtemporal dynamic computation video recognition paper presents comprehensive exploration phenomenon data redundancy video understanding aim improve computational efficiency investigation commences examination spatial redundancy refers observation informative region video frame usually corresponds small image patch whose shape size location shift smoothly across frames motivated phenomenon formulate patch localization problem dynamic decision task introduce spatially adaptive video recognition approach termed adafocus specific lightweight encoder first employed quickly process full video sequence whose features utilized policy network identify taskrelevant regions subsequently selected patches inferred highcapacity deep network final prediction full model trained endtoend conveniently furthermore adafocus extended considering temporal samplewise redundancies ie allocating majority computation taskrelevant frames minimizing computation spent relatively easier videos resulting approach uniadafocus establishes comprehensive framework seamlessly integrates spatial temporal samplewise dynamic computation preserves merits adafocus terms efficient endtoend training hardware friendliness addition uniadafocus general flexible compatible offtheshelf efficient backbones eg tsm readily deployed feature extractor yielding significantly improved computational efficiency empirically extensive experiments based seven benchmark datasets three application scenarios substantiate uniadafocus considerably efficient competitive baselines,4,0.7791289004365893,4,0.7791289004365893
prime protect videos malicious editing development generative models quality generated content keeps increasing recently opensource models made surprisingly easy manipulate edit photos videos simple prompts cuttingedge technologies gained popularity also given rise concerns regarding privacy portrait rights individuals malicious users exploit tools deceptive illegal purposes although previous works focus protecting photos generative models find still gaps protecting videos images aspects efficiency effectiveness therefore introduce protection method prime significantly reduce time cost improve protection performance moreover evaluate proposed protection method consider objective metrics human subjective metrics evaluation results indicate prime costs gpu hours cost previous stateoftheart method achieves better protection results human evaluation objective metrics code found httpsgithubcomguanlinleeprime,4,1.0,4,1.0
learning expressive generalizable motion features face forgery detection previous face forgery detection methods mainly focus appearance features may easily attacked sophisticated manipulation considering majority current face manipulation methods generate fake faces based single frame take frame consistency coordination consideration artifacts frame sequences effective face forgery detection however current sequencebased face forgery detection methods use general video classification networks directly discard special discriminative motion information face manipulation detection end propose effective sequencebased forgery detection framework based existing video classification method make motion features expressive manipulation detection propose alternative motion consistency block instead original motion features module make learned features generalizable propose auxiliary anomaly detection block two specially designed improvements make general video classification network achieve promising results three popular face forgery datasets,4,0.9171929695866534,4,0.9171929695866534
reacto reconstructing articulated objects single video paper address challenge reconstructing general articulated objects single video existing works employing dynamic neural radiance fields advanced modeling articulated objects like humans animals videos face challenges piecewise rigid general articulated objects due limitations deformation models tackle propose quasirigid blend skinning novel deformation model enhances rigidity part maintaining flexible deformation joints primary insight combines three distinct approaches enhanced bone rigging system improved component modeling use quasisparse skinning weights boost part rigidity reconstruction fidelity application geodesic point assignment precise motion seamless deformation method outperforms previous works producing higherfidelity reconstructions general articulated objects demonstrated real synthetic datasets project page httpschaoyuesonggithubioreacto,-1,0.0,-1,0.0
multistream fusion approach oneclass learning audiovisual deepfake detection paper addresses challenge developing robust audiovisual deepfake detection model practical use cases new generation algorithms continually emerging algorithms encountered development detection methods calls generalization ability method additionally ensure credibility detection methods beneficial model interpret cues video indicate fake motivated considerations propose multistream fusion approach oneclass learning representationlevel regularization technique study generalization problem audiovisual deepfake detection creating new benchmark extending resplitting existing fakeavceleb dataset benchmark contains four categories fake videos real audiofake visual fake audiofake visual fake audioreal visual unsynchronized videos experimental results demonstrate approach surpasses previous models large margin furthermore proposed framework offers interpretability indicating modality model identifies likely fake source code released httpsgithubcombokbokmsoc,-1,0.0,-1,0.0
mmsummary multimodal summary generation fetal ultrasound video present first automated multimodal summary generation system mmsummary medical imaging video particularly focus fetal ultrasound analysis imitating examination process performed human sonographer mmsummary designed threestage pipeline progressing keyframe detection keyframe captioning finally anatomy segmentation measurement keyframe detection stage innovative automated workflow proposed progressively select concise set keyframes preserving sufficient video information without redundancy subsequently adapt large language model generate meaningful captions fetal ultrasound keyframes keyframe captioning stage keyframe captioned fetal biometry segmentation measurement stage estimates biometric parameters segmenting region interest according textual prior mmsummary system provides comprehensive summaries fetal ultrasound examinations based reported experiments estimated reduce scanning time approximately thereby suggesting potential enhance clinical workflow efficiency,-1,0.0,-1,0.0
towards realtime generation delaycompensated video feeds outdoor mobile robot teleoperation teleoperation important technology enable supervisors control agricultural robots remotely however environmental factors dense crop rows limitations network infrastructure hinder reliability data streamed teleoperators issues result delayed variable frame rate video feeds often deviate significantly robots actual viewpoint propose modular learningbased vision pipeline generate delaycompensated images realtime supervisors extensive offline evaluations demonstrate method generates accurate images compared stateoftheart approaches setting additionally one works evaluate delaycompensation method outdoor field environments complex terrain data real robot realtime resulting videos code provided httpssitesgooglecomillinoiseducompteleop,-1,0.0,-1,0.0
robust audiovisual speech recognition models mixtureofexperts visual signals enhance audiovisual speech recognition accuracy providing additional contextual information given complexity visual signals audiovisual speech recognition model requires robust generalization capabilities across diverse video scenarios presenting significant challenge paper introduce eva leveraging mixtureofexperts audiovisual asr perform robust speech recognition inthewild videos specifically first encode visual information visual tokens sequence map speech space lightweight projection build eva upon robust pretrained speech recognition model ensuring generalization ability moreover incorporate visual information effectively inject visual information asr model mixtureofexperts module experiments show model achieves stateoftheart results three benchmarks demonstrates generalization ability eva across diverse video domains,-1,0.0,-1,0.0
enhancing motion variation texttomotion models via pose video conditioned editing texttomotion models generate sequences human poses textual descriptions garnering significant attention however due data scarcity range motions models produce still limited instance current texttomotion models generate motion kicking football instep foot since training data includes martial arts kicks propose novel method uses short video clips images conditions modify existing basic motions approach models understanding kick serves prior video image football kick acts posterior enabling generation desired motion incorporating additional modalities conditions method create motions present training set overcoming limitations textmotion datasets user study participants demonstrated approach produces unseen motions realism comparable commonly represented motions textmotion datasets eg walking running squatting kicking,18,0.9240013763444539,18,0.9240013763444539
mote reconciling generalization specialization visuallanguage video knowledge transfer transferring visuallanguage knowledge largescale foundation models video recognition proved effective bridge domain gap additional parametric modules added capture temporal information however zeroshot generalization diminishes increase number specialized parameters making existing works tradeoff zeroshot closeset performance paper present mote novel framework enables generalization specialization balanced one unified model approach tunes mixture temporal experts learn multiple task views various degrees data fitting maximally preserve knowledge expert propose emphweight merging regularization regularizes merging process experts weight space additionally temporal feature modulation regularize contribution temporal feature test achieve sound balance zeroshot closeset video recognition tasks obtain stateoftheart competitive results various datasets including ucf hmdb code available urlhttpsgithubcomzmhhhmote,-1,0.0,-1,0.0
videotospeech generation speech decomposition rectified flow paper introduce novel videotospeech framework designed generate natural intelligible speech directly silent talking face videos recent systems shown promising results constrained datasets limited speakers vocabularies performance often degrades realworld unconstrained datasets due inherent variability complexity speech signals address challenges decompose speech signal manageable subspaces content pitch speaker information representing distinct speech attributes predict directly visual input generate coherent realistic speech predicted attributes employ rectified flow matching decoder built transformer architecture models efficient probabilistic pathways random noise target speech distribution extensive experiments demonstrate significantly outperforms stateoftheart methods even surpassing naturalness ground truth utterances,-1,0.0,-1,0.0
perse personalized generative avatars single portrait present perse method building animatable personalized generative avatar reference portrait avatar model enables facial attribute editing continuous disentangled latent space control facial attribute preserving individuals identity achieve method begins synthesizing largescale synthetic video datasets video contains consistent changes facial expression viewpoint combined variation specific facial attribute original input propose novel pipeline produce highquality photorealistic videos facial attribute editing leveraging synthetic attribute dataset present personalized avatar creation method based gaussian splatting learning continuous disentangled latent space intuitive facial attribute manipulation enforce smooth transitions latent space introduce latent space regularization technique using interpolated faces supervision compared previous approaches demonstrate perse generates highquality avatars interpolated attributes preserving identity reference person,-1,0.0,-1,0.0
analyzing tumors synthesis computeraided tumor detection shown great potential enhancing interpretation million ct scans performed annually united states however challenges arise due rarity ct scans tumors especially earlystage tumors developing ai real tumor data faces issues scarcity annotation difficulty low prevalence tumor synthesis addresses challenges generating numerous tumor examples medical images aiding ai training tumor detection segmentation successful synthesis requires realistic generalizable synthetic tumors across various organs chapter reviews ai development real synthetic data summarizes two key trends synthetic data cancer imaging research modelingbased learningbased approaches modelingbased methods like simulate tumor development time using generic rules learningbased methods like difftumor learn annotated examples one organ generate synthetic tumors others reader studies expert radiologists show synthetic tumors convincingly realistic also present case studies liver pancreas kidneys reveal ai trained synthetic tumors achieve performance comparable better ai trained real data tumor synthesis holds significant promise expanding datasets enhancing ai reliability improving tumor detection performance preserving patient privacy,3,0.674251634040254,3,0.674251634040254
figclip finegrained clip adaptation via densely annotated videos contrastive language image pretraining clip exhibited impressive performance learning highly semantic generalized representations recent works exposed fundamental drawback syntactic properties includes interpreting finegrained attributes actions spatial relations states details require compositional reasoning one reason natural captions often capture visual details scene leads unaddressed visual concepts misattributed wrong words pooled image text features ends acting bag words hence losing syntactic information work ask possible enhance clips finegrained syntactic abilities without compromising semantic properties show possible adapting clip efficiently highquality comprehensive relatively small dataset demonstrate adaptation strategy vidsitu video situation recognition dataset annotated verbs rich semantic role labels srl use srl verb information create rulebased detailed captions making sure capture visual concepts combined hard negatives hierarchical losses annotations allow us learn powerful visual representation dubbed finegrained clip figclip preserves semantic understanding detailoriented evaluate five diverse visionlanguage tasks finetuning zeroshot settings achieving consistent improvements base clip model,0,0.8472263705533353,0,0.8472263705533353
mags reconstructing simulating dynamic objects meshadsorbed gaussian splatting reconstruction simulation although interrelated distinct objectives reconstruction requires flexible representation adapt diverse scenes simulation needs structured representation model motion principles effectively paper introduces meshadsorbed gaussian splatting mags method address challenge mags constrains gaussians roam near mesh creating mutually adsorbed meshgaussian representation representation harnesses rendering flexibility gaussians structured property meshes achieve introduce rmdnet network learns motion priors video data refine mesh deformations alongside rgdnet models relative displacement mesh gaussians enhance rendering fidelity mesh constraints generalize novel userdefined deformations beyond input video without reliance temporal data propose mpenet leverages inherent mesh information bootstrap rmdnet rgdnet due universality meshes mags compatible various deformation priors arap smpl soft physics simulation extensive experiments dnerf dgmesh peoplesnapshot datasets demonstrate mags achieves stateoftheart performance reconstruction simulation,-1,0.0,-1,0.0
learning precise affordances egocentric videos robotic manipulation affordance defined potential actions object offers crucial robotic manipulation tasks deep understanding affordance lead intelligent ai systems example knowledge directs agent grasp knife handle cutting blade passing someone paper present streamlined affordance learning system encompasses data collection effective model training robot deployment first collect training data egocentric videos automatic manner different previous methods focus object graspable affordance represent coarse heatmaps cover graspable eg object handles functional affordances eg knife blades hammer heads extract data precise segmentation masks propose effective model termed geometryguided affordance transformer gkt train collected data gkt integrates innovative depth feature injector dfi incorporate shape geometric priors enhancing models understanding affordances enable affordanceoriented manipulation introduce affgrasp framework combines gkt grasp generation model comprehensive evaluation create affordance evaluation dataset pixelwise annotations design realworld tasks robot experiments results show gkt surpasses stateoftheart miou affgrasp achieves high success rates affordance prediction successful grasping among trials including evaluations seen unseen objects cluttered scenes,5,0.7876100720652667,5,0.7876100720652667
vera explainable video anomaly detection via verbalized learning visionlanguage models rapid advancement visionlanguage models vlms established new paradigm video anomaly detection vad leveraging vlms simultaneously detect anomalies provide comprehendible explanations decisions existing work direction often assumes complex reasoning required vad exceeds capabilities pretrained vlms consequently approaches either incorporate specialized reasoning modules inference rely instruction tuning datasets additional training adapt vlms vad however strategies often incur substantial computational costs data annotation overhead address challenges explainable vad introduce verbalized learning framework named vera enables vlms perform vad without model parameter modifications specifically vera automatically decomposes complex reasoning required vad reflections simpler focused guiding questions capturing distinct abnormal patterns treats reflective questions learnable parameters optimizes datadriven verbal interactions learner optimizer vlms using coarsely labeled training data inference vera embeds learned questions model prompts guide vlms generating segmentlevel anomaly scores refined framelevel scores via fusion scene temporal contexts experimental results challenging benchmarks demonstrate learned questions vera highly adaptable significantly improving detection performance explainability vlms vad,-1,0.0,-1,0.0
sora openais prelude social media perspectives sora openai future ai video generation rapid advancement generative ai genai transforming humancomputer interaction hci significant implications across various sectors study investigates publics perception sora openai pioneering genai video generation tool via social media discussions reddit release centers two main questions envisioned applications concerns related soras integration analysis forecasts positive shifts content creation predicting sora democratize video marketing innovate game development making video production accessible economical conversely concerns deepfakes potential disinformation underscoring need strategies address disinformation bias paper contributes genai discourse fostering discussion current future capabilities enriching understanding public expectations establishing temporal benchmark user anticipation research underscores necessity informed ethical approaches ai development integration ensuring technological advancements align societal values user needs,10,1.0,10,1.0
flow snapshot neurons action deep neural networks generalize biological motion perception biological motion perception bmp refers humans ability perceive recognize actions living beings solely motion patterns sometimes minimal depicted pointlight displays humans excel tasks without prior training current ai models struggle poor generalization performance close research gap propose motion perceiver mp mp solely relies patchlevel optical flows video clips inputs training learns prototypical flow snapshots competitive binding mechanism integrates invariant motion representations predict action labels given video inference evaluate generalization ability ai models humans video stimuli spanning bmp conditions using pointlight displays neuroscience remarkably mp outperforms existing ai models maximum improvement action recognition accuracy conditions moreover benchmark ai models pointlight displays two standard video datasets computer vision mp also demonstrates superior performance cases interestingly via psychophysics experiments found mp recognizes biological movements way aligns human behaviors data code available httpsgithubcomzhanglabdeepneurocoglabmotionperceiver,5,0.2523497724840113,5,0.2523497724840113
may dance dance generation framework nonhumanoids hypothesize dance motion forms visual rhythm music visual rhythm perceived optical flow agent recognize relationship visual rhythm music able dance generating motion create visual rhythm matches music based propose framework kind nonhumanoid agents learn dance human videos framework works two processes training reward model perceives relationship optical flow visual rhythm music human dance videos training nonhumanoid dancer based reward model reinforcement learning reward model consists two feature encoders optical flow music trained based contrastive learning makes higher similarity concurrent optical flow music features reward model agent learns dancing getting higher reward action creates optical flow whose feature higher similarity given music feature experiment results show generated dance motion align music beat properly user study result indicates framework preferred humans compared baselines best knowledge work nonhumanoid agents learn dance human videos unprecedented example video found,8,0.4761583266156947,8,0.4761583266156947
shine saliencyaware hierarchical negative ranking compositional temporal grounding temporal grounding also known video moment retrieval aims locating video segments corresponding given query sentence compositional nature natural language enables localization beyond predefined events posing certain challenge compositional generalizability existing methods recent studies establish correspondence videos queries decomposereconstruct manner achieve compositional generalization however consider dominant primitives build negative queries random sampling recombination resulting semantically implausible negatives hinder models learning rational compositions addition recent detrbased methods still underperform compositional temporal grounding showing irrational saliency responses given negative queries subtle differences positive queries address limitations first propose large language modeldriven method negative query construction utilizing generate semantically plausible hard negative queries subsequently introduce coarsetofine saliency ranking strategy encourages model learn multigranularity semantic relationships videos hierarchical negative queries boost compositional generalization extensive experiments two challenging benchmarks validate effectiveness generalizability proposed method code available httpsgithubcomzxccadeshine,-1,0.0,-1,0.0
effectively leveraging clip generating situational summaries images videos situation recognition refers ability agent identify understand various situations contexts based available information sensory inputs involves cognitive process interpreting data environment determine happening factors involved actions caused situations interpretation situations formulated semantic role labeling problem computer visionbased situation recognition situations depicted images videos hold pivotal information essential various applications like image video captioning multimedia retrieval autonomous systems event monitoring however existing methods often struggle ambiguity lack context generating meaningful accurate predictions leveraging multimodal models clip propose clipsitu sidesteps need full finetuning achieves stateoftheart results situation recognition localization tasks clipsitu harnesses clipbased image verb role embeddings predict nouns fulfilling roles associated verb providing comprehensive understanding depicted scenarios crossattention transformer clipsitu xtf enhances connection semantic role queries visual token representations leading superior performance situation recognition also propose verbwise role prediction model nearperfect accuracy create endtoend framework producing situational summaries outofdomain images show situational summaries empower clipsitu models produce structured descriptions reduced ambiguity compared generic captions finally extend clipsitu video situation recognition showcase versatility produce comparable performance stateoftheart methods,0,0.9775017631521535,0,0.9775017631521535
resyncer rewiring stylebased generator unified audiovisually synced facial performer lipsyncing videos given audio foundation various applications including creation virtual presenters performers recent studies explore highfidelity lipsync different techniques taskorientated models either require longterm videos clipspecific training retain visible artifacts paper propose unified effective framework resyncer synchronizes generalized audiovisual facial information key design revisiting rewiring stylebased generator efficiently adopt facial dynamics predicted principled styleinjected transformer simply reconfiguring information insertion mechanisms within noise style space framework fuses motion appearance unified training extensive experiments demonstrate resyncer produces highfidelity lipsynced videos according audio also supports multiple appealing properties suitable creating virtual presenters performers including fast personalized finetuning videodriven lipsyncing transfer speaking styles even face swapping resources found,6,0.9204841226690536,6,0.9204841226690536
multireference generative face video compression contrastive learning generative face video coding gfvc demonstrated potential approach lowlatency low bitrate video conferencing gfvc frameworks achieve extreme gain coding efficiency bitrate savings compared conventional codecs bitrates recent mpegjvet standardization efforts information required reconstruct video sequences using gfvc frameworks adopted part supplemental enhancement information sei existing compression pipelines light development aim address challenge weakly addressed prior gfvc frameworks ie reconstruction drift distance reference target frames increases challenge creates need update reference buffer frequently transmitting intrarefresh frames expensive element gfvc bitstream overcome problem propose instead multiple reference animation robust approach minimizing reconstruction drift especially used bidirectional prediction mode propose contrastive learning formulation multireference animation observe using contrastive learning framework enhances representation capabilities animation generator resulting framework mrdac multireference deep animation codec therefore used compress longer sequences fewer reference frames achieve significant gain reconstruction accuracy comparable bitrates previous frameworks quantitative qualitative results show significant coding reconstruction quality gains compared previous gfvc methods accurate animation quality presence large pose facial expression changes,2,1.0,2,1.0
towards synthetic data generation improved pain recognition videos patient constraints recognizing pain video crucial improving patientcomputer interaction systems yet traditional data collection domain raises significant ethical logistical challenges study introduces novel approach leverages synthetic data enhance videobased pain recognition models providing ethical scalable alternative present pipeline synthesizes realistic facial models capturing nuanced facial movements small participant pool mapping onto diverse synthetic avatars process generates synthetic faces accurately reflecting genuine pain expressions varied angles perspectives utilizing advanced facial capture techniques leveraging public datasets like celebvhq ffhquv demographic diversity new synthetic dataset significantly enhances model training ensuring privacy anonymizing identities facial replacements experimental results demonstrate models trained combinations synthetic data paired small amount real participants achieve superior performance pain recognition effectively bridging gap synthetic simulations realworld applications approach addresses data scarcity ethical concerns offering new solution pain detection opening new avenues research privacypreserving dataset generation resources publicly available encourage innovation field,-1,0.0,-1,0.0
texttoon realtime text toonify head avatar single video propose texttoon method generate drivable toonified avatar given short monocular video sequence written instruction avatar style model generate highfidelity toonified avatar driven realtime another video arbitrary identities existing related works heavily rely multiview modeling recover geometry via texture embeddings presented static manner leading control limitations multiview video input also makes difficult deploy models realworld applications address issues adopt conditional embedding triplane learn realistic stylized facial representations gaussian deformation field additionally expand stylization capabilities gaussian splatting introducing adaptive pixeltranslation neural network leveraging patchaware contrastive learning achieve highquality images push work consumer applications develop realtime system operate fps gpu machine fps mobile machine extensive experiments demonstrate efficacy approach generating textual avatars existing methods terms quality realtime animation please refer project page details httpssongluchuangithubiotexttoon,6,0.4045946016362095,6,0.4045946016362095
animatex universal character image animation enhanced motion representation character image animation generates highquality videos reference image target pose sequence seen significant progress recent years however existing methods apply human figures usually generalize well anthropomorphic characters commonly used industries like gaming entertainment indepth analysis suggests attribute limitation insufficient modeling motion unable comprehend movement pattern driving video thus imposing pose sequence rigidly onto target character end paper proposes animatex universal animation framework based ldm various character types collectively named x including anthropomorphic characters enhance motion representation introduce pose indicator captures comprehensive motion pattern driving video implicit explicit manner former leverages clip visual features driving video extract gist motion like overall movement pattern temporal relations among motions latter strengthens generalization ldm simulating possible inputs advance may arise inference moreover introduce new animated anthropomorphic benchmark evaluate performance animatex universal widely applicable animation images extensive experiments demonstrate superiority effectiveness animatex compared stateoftheart methods,-1,0.0,-1,0.0
latent action pretraining videos introduce latent action pretraining general action models lapa unsupervised method pretraining visionlanguageaction vla models without groundtruth robot action labels existing visionlanguageaction models require action labels typically collected human teleoperators pretraining significantly limits possible data sources scale work propose method learn internetscale videos robot action labels first train action quantization model leveraging vqvaebased objective learn discrete latent actions image frames pretrain latent vla model predict latent actions observations task descriptions finally finetune vla smallscale robot manipulation data map latent robot actions experimental results demonstrate method significantly outperforms existing techniques train robot manipulation policies largescale videos furthermore outperforms stateoftheart vla model trained robotic action labels realworld manipulation tasks require language conditioning generalization unseen objects semantic generalization unseen instructions training human manipulation videos also shows positive transfer opening potential leveraging webscale data robotics foundation model,5,0.41710134894811635,5,0.41710134894811635
mmds multimodal medical diagnosis system integrating image analysis knowledgebased departmental consultation present mmds system capable recognizing medical images patient facial details providing professional medical diagnoses system consists two core componentsthe first component analysis medical images videos trained specialized multimodal medical model capable interpreting medical images accurately analyzing patients facial emotions facial paralysis conditions model achieved accuracy facial emotion recognition dataset accuracy recognizing happy emotion facial paralysis recognition model reached accuracy higher based model developed parser analyzing facial movement videos patients facial paralysis achieving precise grading paralysis severity tests videos facial paralysis patients system demonstrated grading accuracy second component generation professional medical responses employed large language model integrated medical knowledge base generate professional diagnoses based analysis medical images videos core innovation lies development departmentspecific knowledge base routing management mechanism large language model categorizes data medical departments retrieval process determines appropriate knowledge base query significantly improves retrieval accuracy rag retrievalaugmented generation process,-1,0.0,-1,0.0
glcf globallocal multimodal coherence analysis framework talking face generation detection talking face generation tfg allows producing lifelike talking videos character using facial images accompanying text abuse technology could pose significant risks society creating urgent need research corresponding detection methods however research field hindered lack public datasets paper construct first largescale multiscenario talking face dataset mstf contains audio video forgery techniques filling gap datasets field dataset covers generation scenarios semantic scenarios closer practical application scenario tfg besides also propose tfg detection framework leverages analysis global local coherence multimodal content tfg videos therefore regionfocused smoothness detection module rsfdm discrepancy capturetime frame aggregation module dctam introduced evaluate global temporal coherence tfg videos aggregating multigrained spatial information additionally visualaudio fusion module vafm designed evaluate audiovisual coherence within localized temporal perspective comprehensive experiments demonstrate reasonableness challenges datasets also indicating superiority proposed method compared stateoftheart deepfake detection approaches,4,0.6958101527585586,4,0.6958101527585586
densepanet improved generative adversarial network photoacoustic tomography image reconstruction sparse data image reconstruction essential step every medical imaging method including photoacoustic tomography pat promising modality imaging unites benefits ultrasound optical imaging methods reconstruction pat images using conventional methods results rough artifacts especially applied directly sparse pat data recent years generative adversarial networks gans shown powerful performance image generation well translation rendering smart choice applied reconstruction tasks study proposed endtoend method called densepanet solve problem pat image reconstruction sparse data proposed model employs novel modification unet generator called fdunet considerably improves reconstruction performance evaluated method various invivo simulated datasets quantitative qualitative results show better performance model prevalent deep learning techniques,-1,0.0,-1,0.0
exploring variational autoencoders medical image generation comprehensive study variational autoencoder vae one common techniques field medical image generation architecture shown advanced researchers recent years developed various architectures vae advantages including improving datasets adding samples smaller datasets datasets imbalanced classes data augmentation works paper provides comprehensive review studies vae medical imaging special focus ability create synthetic images close real data used data augmentation study reviews important architectures methods used develop vaes medical images provides comparison generative models gans issues image quality low diversity generated samples discuss recent developments applications several medical fields highlighting ability vaes improve segmentation classification accuracy,-1,0.0,-1,0.0
aasgan adversarially augmented social gan synthetic data accurately predicting pedestrian trajectories crucial applications autonomous driving service robotics name deep generative models achieve top performance task assuming enough labelled trajectories available training end large amounts synthetically generated labelled trajectories exist eg generated video games however trajectories meant represent pedestrian motion realistically ineffective training predictive model propose method architecture augment synthetic trajectories training time adversarial approach show trajectory augmentation training time unleashes significant gains stateoftheart generative model evaluated realworld trajectories,-1,0.0,-1,0.0
metadiffub contextualized sequencetosequence text diffusion model metaexploration diffusion model new generative modeling paradigm achieved significant success generating images audio video text adapted sequencetosequence text generation diffuseq termed diffusion existing models predominantly rely fixed handcrafted rules schedule noise diffusion denoising processes however models limited noncontextualized noise fails fully consider characteristics tasks paper propose metadiffub framework novel schedulerexploiter paradigm designed overcome limitations existing models employ metaexploration train additional scheduler model dedicated scheduling contextualized noise sentence exploiter model model leverages noise scheduled scheduler model updating generation metadiffub achieves stateoftheart performance compared previous models finetuned pretrained language models plms across four benchmark datasets investigate visualize impact metadiffubs noise scheduling generation sentences varying difficulties additionally scheduler model function plugandplay model enhance diffuseq without need finetuning inference stage,-1,0.0,-1,0.0
physcene physically interactable scene synthesis embodied ai recent developments embodied artificial intelligence eai research growing demand highquality largescale interactive scene generation prior methods scene synthesis prioritized naturalness realism generated scenes physical plausibility interactivity scenes largely left unexplored address disparity introduce physcene novel method dedicated generating interactive scenes characterized realistic layouts articulated objects rich physical interactivity tailored embodied agents based conditional diffusion model capturing scene layouts devise novel physics interactivitybased guidance mechanisms integrate constraints object collision room layout object reachability extensive experiments demonstrate physcene effectively leverages guidance functions physically interactable scene synthesis outperforming existing stateoftheart scene synthesis methods large margin findings suggest scenes generated physcene hold considerable potential facilitating diverse skill acquisition among agents within interactive environments thereby catalyzing advancements embodied ai research project website httpphyscenegithubio,18,0.9416765408641343,18,0.9416765408641343
physicsinspired generative models medical imaging review physicsinspired generative models gms particular diffusion models dms poisson flow models pfms enhance bayesian methods promise great utility medical imaging review examines transformative role generative methods first variety physicsinspired gms including denoising diffusion probabilistic models ddpms scorebased diffusion models sdms poisson flow generative models pfgms pfgm revisited emphasis accuracy robustness well acceleration major applications physicsinspired gms medical imaging presented comprising image reconstruction image generation image analysis finally future research directions brainstormed including unification physicsinspired gms integration visionlanguage models vlms potential novel applications gms since development generative methods rapid review hopefully give peers learners timely snapshot new family physicsdriven generative models help capitalize enormous potential medical imaging,-1,0.0,-1,0.0
diffusion meets options hierarchical generative skill composition temporallyextended tasks safe successful deployment robots requires ability generate complex plans also capacity frequently replan correct execution errors paper addresses challenge longhorizon trajectory planning temporally extended objectives receding horizon manner end propose doppler datadriven hierarchical framework generates updates plans based instruction specified linear temporal logic ltl method decomposes temporal tasks chain options hierarchical reinforcement learning offline nonexpert datasets leverages diffusion models generate options lowlevel actions devise determinantalguided posterior sampling technique batch generation improves speed diversity diffusion generated options leading efficient querying experiments robot navigation manipulation tasks demonstrate doppler generate sequences trajectories progressively satisfy specified formulae obstacle avoidance sequential visitation demonstration videos available online httpsphiliptheothergithubiodoppler,5,0.45990297116530615,5,0.45990297116530615
meddiffusion medical diffusion model controllable highquality medical image generation generation medical images presents significant challenges due highresolution threedimensional nature existing methods often yield suboptimal performance generating highquality medical images currently universal generative framework medical imaging paper introduce medical diffusion meddiffusion model controllable highquality medical image generation meddiffusion incorporates novel highly efficient patchvolume autoencoder compresses medical images latent space patchwise encoding recovers back image space volumewise decoding additionally design new noise estimator capture local details global structure information diffusion denoising process meddiffusion generate finedetailed highresolution images effectively adapt various downstream tasks trained largescale datasets covering ct mri modalities different anatomical regions head leg experimental results demonstrate meddiffusion surpasses stateoftheart methods generative quality exhibits strong generalizability across tasks sparseview ct reconstruction fast mri reconstruction data augmentation,-1,0.0,-1,0.0
ganha generative adversarial network novel heterogeneous dualdiscriminator network new attentionbased fusion strategy infrared visible image fusion infrared visible image fusion ivif aims preserve thermal radiation information infrared images integrating texture details visible images thermal radiation information mainly expressed image intensities texture details typically expressed image gradients however existing dualdiscriminator generative adversarial networks gans often rely two structurally identical discriminators learning fully account distinct learning needs infrared visible image information end paper proposes novel gan heterogeneous dualdiscriminator network attentionbased fusion strategy ganha specifically recognizing intrinsic differences infrared visible images propose first time novel heterogeneous dualdiscriminator network simultaneously capture thermal radiation information texture details two discriminators network structurally different including salient discriminator infrared images detailed discriminator visible images able learn rich image intensity information image gradient information respectively addition new attentionbased fusion strategy designed generator appropriately emphasize learned information different source images thereby improving information representation ability fusion result way fused images generated ganha effectively maintain salience thermal targets sharpness textures extensive experiments various public datasets demonstrate superiority ganha stateoftheart sota algorithms showcasing higher potential practical applications,-1,0.0,-1,0.0
tfsnerf templatefree nerf semantic reconstruction dynamic scene despite advancements neural implicit models surface reconstruction handling dynamic environments interactions arbitrary rigid nonrigid deformable entities remains challenging generic reconstruction methods adaptable dynamic scenes often require additional inputs like depth optical flow rely pretrained image features reasonable outcomes methods typically use latent codes capture framebyframe deformations another set dynamic scene reconstruction methods entityspecific mostly focusing humans relies template models contrast templatefree methods bypass requirements adopt traditional lbs linear blend skinning weights detailed representation deformable object motions although involve complex optimizations leading lengthy training times end remedy paper introduces tfsnerf templatefree semantic nerf dynamic scenes captured sparse singleview rgb videos featuring interactions among two entities timeefficient lbsbased approaches framework uses invertible neural network inn lbs prediction simplifying training process disentangling motions interacting entities optimizing perentity skinning weights method efficiently generates accurate semantically separable geometries extensive experiments demonstrate approach produces highquality reconstructions deformable nondeformable objects complex interactions improved training efficiency compared existing methods,1,1.0,1,1.0
dancefusion spatiotemporal skeleton diffusion transformer audiodriven dance motion reconstruction paper introduces dancefusion novel framework reconstructing generating dance movements synchronized music utilizing spatiotemporal skeleton diffusion transformer framework adeptly handles incomplete noisy skeletal data common shortform dance videos social media platforms like tiktok dancefusion incorporates hierarchical transformerbased variational autoencoder vae integrated diffusion model significantly enhancing motion realism accuracy approach introduces sophisticated masking techniques unique iterative diffusion process refines motion sequences ensuring high fidelity motion generation synchronization accompanying audio cues comprehensive evaluations demonstrate dancefusion surpasses existing methods providing stateoftheart performance generating dynamic realistic stylistically diverse dance motions potential applications framework extend content creation virtual reality interactive entertainment promising substantial advancements automated dance generation visit project page httpsthmlabgithubiodancefusion,-1,0.0,-1,0.0
deepfake detection impact limited computing capabilities rapid development technologies artificial intelligence makes deepfakes increasingly sophisticated challengingtoidentify technique ensure accuracy information control misinformation mass manipulation paramount importance discover develop artificial intelligence models enable generic detection forged videos work aims address detection deepfakes across various existing datasets scenario limited computing resources goal analyze applicability different deep learning techniques restrictions explore possible approaches enhance efficiency,4,0.9578154181924083,4,0.9578154181924083
singular value decompositions thirdorder reduced biquaternion tensors paper introduce applications thirdorder reduced biquaternion tensors color video processing first develop algorithms computing singular value decomposition svd thirdorder reduced biquaternion tensor via new htproduct theoretical applications define moorepenrose inverse thirdorder reduced biquaternion tensor develop characterizations addition discuss general hermitian solutions reduced biquaternion tensor equation mathcalxmathcalb well leastsquare solution finally compress color video svd experimental data shows method faster compared scheme,-1,0.0,-1,0.0
deepspeak dataset describe largescale datasetdeepspeakof real deepfake footage people talking gesturing front webcams real videos first version dataset consist hours footage diverse individuals constituting hours footage fake videos consist range different stateoftheart faceswap lipsync deepfakes natural aigenerated voices expect release future versions dataset different updated deepfake technologies dataset made freely available research noncommercial uses requests commercial use considered,4,0.6489742412963518,4,0.6489742412963518
nimbled enhancing selfsupervised monocular depth estimation pseudolabels largescale video pretraining introduce nimbled efficient selfsupervised monocular depth estimation learning framework incorporates supervision pseudolabels generated large vision model framework require camera intrinsics enabling largescale pretraining publicly available videos straightforward yet effective learning strategy significantly enhances performance fast lightweight models without introducing overhead allowing achieve performance comparable stateoftheart selfsupervised monocular depth estimation models advancement particularly beneficial virtual augmented reality applications requiring low latency inference source code model weights acknowledgments available httpsgithubcomxapaxcanimbled,-1,0.0,-1,0.0
full transformerbased framework automatic pain estimation using videos automatic estimation pain essential designing optimal pain management system offering reliable assessment reducing suffering patients study present novel full transformerbased framework consisting transformer transformer tnt model transformer leveraging crossattention selfattention blocks elaborating videos biovid database demonstrate stateoftheart performances showing efficacy efficiency generalization capability across primary pain estimation tasks,-1,0.0,-1,0.0
computational complexity game boy games analyze computational complexity several popular video games released nintendo game boy video game console analyze complexity generalized versions four popular game boy games donkey kong wario land harvest moon gb mole mania provide original proofs showing games textbfnphard proofs rely karp reductions four karps original textbfnpcomplete problems textscsat textschamiltonian cycle textscknapsack also discuss proofs easily derived known results demonstrating textbfnphardness lock n chase lion king,-1,0.0,-1,0.0
renoise real image inversion iterative noising recent advancements textguided diffusion models unlocked powerful image manipulation capabilities however applying methods real images necessitates inversion images domain pretrained diffusion model achieving faithful inversion remains challenge particularly recent models trained generate images small number denoising steps work introduce inversion method high qualitytooperation ratio enhancing reconstruction accuracy without increasing number operations building reversing diffusion sampling process method employs iterative renoising mechanism inversion sampling step mechanism refines approximation predicted point along forward diffusion trajectory iteratively applying pretrained diffusion model averaging predictions evaluate performance renoise technique using various sampling algorithms models including recent accelerated diffusion models comprehensive evaluations comparisons show effectiveness terms accuracy speed furthermore confirm method preserves editability demonstrating textdriven image editing real images,13,1.0,13,1.0
constrained diffusion implicit models paper describes efficient algorithm solving noisy linear inverse problems using pretrained diffusion models extending paradigm denoising diffusion implicit models ddim propose constrained diffusion implicit models cdim modify diffusion updates enforce constraint upon final output noiseless inverse problems cdim exactly satisfies constraints noisy case generalize cdim satisfy exact constraint residual distribution noise experiments across variety tasks metrics show strong performance cdim analogous inference acceleration unconstrained ddim times faster previous conditional diffusion methods demonstrate versatility approach many problems including superresolution denoising inpainting deblurring point cloud reconstruction,13,0.8895451840829508,13,0.8895451840829508
fora fastforward caching diffusion transformer acceleration diffusion transformers dit become de facto choice generating highquality images videos largely due scalability enables construction larger models enhanced performance however increased size models leads higher inference costs making less attractive realtime applications present fastforward caching fora simple yet effective approach designed accelerate dit exploiting repetitive nature diffusion process fora implements caching mechanism stores reuses intermediate outputs attention mlp layers across denoising steps thereby reducing computational overhead approach require model retraining seamlessly integrates existing transformerbased diffusion models experiments show fora speed diffusion transformers several times minimally affecting performance metrics score fid enabling faster processing minimal tradeoffs quality fora represents significant advancement deploying diffusion transformers realtime applications code made publicly available httpsgithubcomprathebaselvafora,20,1.0,20,1.0
igcfat improved ganbased framework effectively exploiting transformers realworld image superresolution field single image superresolution sisr transformerbased models demonstrated significant advancements however potential efficiency models applied fields realworld image superresolution less noticed substantial opportunities improvement recently composite fusion attention transformer cfat outperformed previous stateoftheart sota models classic image superresolution paper propose novel ganbased framework incorporating cfat model effectively exploit performance transformers realworld image superresolution proposed approach integrate semanticaware discriminator reconstruct fine details accurately employ adaptive degradation model better simulate realworld degradations moreover introduce new combination loss functions adding wavelet loss loss functions ganbased models better recover highfrequency details empirical results demonstrate igcfat significantly outperforms existing sota models quantitative qualitative metrics proposed model revolutionizes field realworld image superresolution demonstrates substantially better performance recovering fine details generating realistic textures introduction igcfat offers robust adaptable solution realworld image superresolution tasks,13,0.9149390547725647,13,0.9149390547725647
latentsync taming audioconditioned latent diffusion models lip sync syncnet supervision endtoend audioconditioned latent diffusion models ldms widely adopted audiodriven portrait animation demonstrating effectiveness generating lifelike highresolution talking videos however direct application audioconditioned ldms lipsynchronization lipsync tasks results suboptimal lipsync accuracy indepth analysis identified underlying cause shortcut learning problem wherein model predominantly learns visualvisual shortcuts neglecting critical audiovisual correlations address issue explored different approaches integrating syncnet supervision audioconditioned ldms explicitly enforce learning audiovisual correlations since performance syncnet directly influences lipsync accuracy supervised model training wellconverged syncnet becomes crucial conducted first comprehensive empirical studies identify key factors affecting syncnet convergence based analysis introduce stablesyncnet architecture designed stable convergence stablesyncnet achieved significant improvement accuracy increasing hdtf test set additionally introduce novel temporal representation alignment trepa mechanism enhance temporal consistency generated videos experimental results show method surpasses stateoftheart lipsync approaches across various evaluation metrics hdtf datasets,6,0.7115534779726284,6,0.7115534779726284
diffusion forcing nexttoken prediction meets fullsequence diffusion paper presents diffusion forcing new training paradigm diffusion model trained denoise set tokens independent pertoken noise levels apply diffusion forcing sequence generative modeling training causal nexttoken prediction model generate one several future tokens without fully diffusing past ones approach shown combine strengths nexttoken prediction models variablelength generation strengths fullsequence diffusion models ability guide sampling desirable trajectories method offers range additional capabilities rollingout sequences continuous tokens video lengths past training horizon baselines diverge new sampling guiding schemes uniquely profit diffusion forcings variablehorizon causal architecture lead marked performance gains decisionmaking planning tasks addition empirical success method proven optimize variational lower bound likelihoods subsequences tokens drawn true joint distribution project website httpsboyuanspacediffusionforcing,-1,0.0,-1,0.0
solution authenticity identification typical target remote sensing images paper propose basic rgb singlemode model based weakly supervised training pseudo labels performs highprecision authenticity identification multiscene typical target remote sensing images due imprecision mask generation divide task two subtasks generating pseudomask finetuning model based generated masks generating pseudo masks use mmfusion base model generate masks large objects planes ships manually calibrating mask small object car highly accurate pseudomask obtained task finetuning models based generating masks use wscl model base model worth noting due difference generated pseudomasks real masks discard image feature extractors srm noiseprint wscl select unscaled original image training alone greatly ensures match image original label final trained model achieved score test set,-1,0.0,-1,0.0
diffpcc diffusionbased neural compression point clouds stable diffusion networks emerged groundbreaking development ability produce realistic detailed visual content characteristic renders ideal decoders capable producing highquality aesthetically pleasing reconstructions paper introduce first diffusionbased point cloud compression method dubbed diffpcc leverage expressive power diffusion model generative aesthetically superior decoding different conventional autoencoder fashion dualspace latent representation devised paper compressor composed two independent encoding backbones considered extract expressive shape latents distinct latent spaces decoding side diffusionbased generator devised produce highquality reconstructions considering shape latents guidance stochastically denoise noisy point clouds experiments demonstrate proposed diffpcc achieves stateoftheart compression performance eg db bdpsnr gains latest gpcc standard ultralow bitrate attaining superior subjective quality source code made publicly available,2,1.0,2,1.0
heuristically adaptive diffusionmodel evolutionary strategy diffusion models represent significant advancement generative modeling employing dualphase process first degrades domainspecific information via gaussian noise restores trainable model framework enables pure noisetodata generation modular reconstruction images videos concurrently evolutionary algorithms employ optimization methods inspired biological principles refine sets numerical parameters encoding potential solutions rugged objective functions research reveals fundamental connection diffusion models evolutionary algorithms shared underlying generative mechanisms methods generate highquality samples via iterative refinement random initial distributions employing deep learningbased diffusion models generative models across diverse evolutionary tasks iteratively refining diffusion models heuristically acquired databases iteratively sample potentially betteradapted offspring parameters integrating successive generations diffusion model approach achieves efficient convergence toward highfitness parameters maintaining explorative diversity diffusion models introduce enhanced memory capabilities evolutionary algorithms retaining historical information across generations leveraging subtle data correlations generate refined samples elevate evolutionary algorithms procedures shallow heuristics frameworks deep memory deploying classifierfree guidance conditional sampling parameter level achieve precise control evolutionary search dynamics specific genotypical phenotypical populationwide traits framework marks major heuristic algorithmic transition offering increased flexibility precision control evolutionary optimization processes,-1,0.0,-1,0.0
longitudinal causal image synthesis clinical decisionmaking relies heavily causal reasoning longitudinal analysis example patient alzheimers disease ad brain grey matter atrophy year intervened abeta level cerebrospinal fluid answer fundamental diagnosis followup treatment however kind inquiry involves counterfactual medical images acquired instrumental correlationbased image synthesis models yet queries require counterfactual medical images obtainable standard image synthesis models hence causal longitudinal image synthesis clis method enabling synthesis images highly valuable however building clis model confronts three primary yet unmet challenges mismatched dimensionality highdimensional images lowdimensional tabular variables inconsistent collection intervals followup data inadequate causal modeling capability existing causal graph methods image data paper established tabularvisual causal graph tvcg clis overcoming challenges novel integration generative imaging continuoustime modeling structural causal models combined neural network train clis based adni dataset evaluate two ad datasets illustrate outstanding yet controllable quality synthesized images contributions synthesized mri characterization ad progression substantiating reliability utility clinics,3,0.5929919460262207,3,0.5929919460262207
progressive boundary guided anomaly synthesis industrial anomaly detection unsupervised anomaly detection methods identify surface defects industrial images leveraging normal samples training due risk overfitting learning single class anomaly synthesis strategies introduced enhance detection capability generating artificial anomalies however existing strategies heavily rely anomalous textures auxiliary datasets moreover limitations coverage directionality anomaly synthesis may result failure capture useful information lead significant redundancy address issues propose novel progressive boundaryguided anomaly synthesis pbas strategy directionally synthesize crucial featurelevel anomalies without auxiliary textures consists three core components approximate boundary learning abl anomaly feature synthesis afs refined boundary optimization rbo make distribution normal samples compact abl first learns approximate decision boundary center constraint improves center initialization feature alignment afs directionally synthesizes anomalies flexible scales guided hypersphere distribution normal features since boundary loose may contain real anomalies rbo refines decision boundary binary classification artificial anomalies normal features experimental results show method achieves stateoftheart performance fastest detection speed three widely used industrial datasets including mvtec ad visa mpdd code available httpsgithubcomcqylunlunpbas,14,0.8803560903671016,14,0.8803560903671016
generating realistic xray scattering images using stable diffusion humanintheloop annotations finetuned foundational stable diffusion model using xray scattering images corresponding descriptions generate new scientific images given prompts however generated images exhibit significant unrealistic artifacts commonly known hallucinations address issue trained various computer vision models dataset composed humanapproved generated images experimental images detect unrealistic images classified images reviewed corrected human experts subsequently used refine classifiers next rounds training inference evaluations demonstrate feasibility generating highfidelity domainspecific images using finetuned diffusion model anticipate generative ai play crucial role enhancing data augmentation driving development digital twins scientific research facilities,-1,0.0,-1,0.0
bridging gap learning inference diffusionbased molecule generation efficacy diffusion models generating spectrum data modalities including images text videos spurred inquiries utility molecular generation yielding significant advancements field however molecular generation process diffusion models involves multiple autoregressive steps finite time horizon leading exposure bias issues inherently address exposure bias issue propose training framework named gapdiff core idea gapdiff utilize modelpredicted conformations ground truth probabilistically training aiming mitigate data distributional disparity training inference thereby enhancing affinity generated molecules conduct experiments using molecular generation model dataset vina energy diversity demonstrate potency framework superior affinity gapdiff available urlhttpsgithubcomhughnewgapdiff,-1,0.0,-1,0.0
explaining implicit neural canvas connecting pixels neurons tracing contributions many variations implicit neural representations inrs neural network trained continuous representation signal tremendous practical utility downstream tasks including novel view synthesis video compression image superresolution unfortunately inner workings networks seriously understudied work explaining implicit neural canvas xinc unified framework explaining properties inrs examining strength neurons contribution output pixel call aggregate contribution maps implicit neural canvas use concept demonstrate inrs study learn see frames represent surprising ways example inrs tend highly distributed representations lacking highlevel object semantics significant bias color edges almost entirely spaceagnostic arrive conclusions examining objects represented across time video inrs using clustering visualize similar neurons across layers architectures show dominated motion insights demonstrate general usefulness analysis framework project page available,-1,0.0,-1,0.0
understanding generalizability diffusion models requires rethinking hidden gaussian structure work study generalizability diffusion models looking hidden properties learned score functions essentially series deep denoisers trained various noise levels observe diffusion models transition memorization generalization corresponding nonlinear diffusion denoisers exhibit increasing linearity discovery leads us investigate linear counterparts nonlinear diffusion models series linear models trained match function mappings nonlinear diffusion denoisers surprisingly linear denoisers approximately optimal denoisers multivariate gaussian distribution characterized empirical mean covariance training dataset finding implies diffusion models inductive bias towards capturing utilizing gaussian structure covariance information training dataset data generation empirically demonstrate inductive bias unique property diffusion models generalization regime becomes increasingly evident models capacity relatively small compared training dataset size case model highly overparameterized inductive bias emerges initial training phases model fully memorizes training data study provides crucial insights understanding notable strong generalization phenomenon recently observed realworld diffusion models,-1,0.0,-1,0.0
acdit interpolating autoregressive conditional modeling diffusion transformer present acdit novel autoregressive blockwise conditional diffusion transformer innovatively combines autoregressive diffusion paradigms modeling continuous visual information introducing blockwise autoregressive unit acdit offers flexible interpolation tokenwise autoregression fullsequence diffusion bypassing limitations discrete tokenization generation block formulated conditional diffusion process conditioned prior blocks acdit easy implement simple creating skipcausal attention mask scam standard diffusion transformer training inference process iterates diffusion denoising autoregressive decoding make full use kvcache show acdit performs best among autoregressive baselines similar model scales image video generation tasks also demonstrate benefiting autoregressive modeling pretrained acdit transferred visual understanding tasks despite trained diffusion objective analysis tradeoff autoregressive modeling diffusion demonstrates potential acdit used longhorizon visual generation tasks hope acdit offers novel perspective visual autoregressive generation unlocks new avenues unified models,-1,0.0,-1,0.0
pixel cancer cellular automata computed tomography ai cancer detection encounters bottleneck data scarcity annotation difficulty low prevalence early tumors tumor synthesis seeks create artificial tumors medical images greatly diversify data annotations ai training however current tumor synthesis approaches applicable across different organs due need specific expertise design paper establishes set generic rules simulate tumor development cell pixel initially assigned state zero ten represent tumor population tumor developed based three rules describe process growth invasion death apply three generic rules simulate tumor developmentfrom pixel cancerusing cellular automata integrate tumor state original computed tomography ct images generate synthetic tumors across different organs tumor synthesis approach allows sampling tumors multiple stages analyzing tumororgan interaction clinically reader study involving three expert radiologists reveals synthetic tumors developing trajectories convincingly realistic technically analyze simulate tumor development various stages using raw unlabeled ct images sourced hospitals worldwide performance segmenting tumors liver pancreas kidneys exceeds prevailing literature benchmarks underlining immense potential tumor synthesis especially earlier cancer detection code models available,3,0.5875662283928498,3,0.5875662283928498
hsigene foundation model hyperspectral image generation hyperspectral image hsi plays vital role various fields agriculture environmental monitoring however due expensive acquisition cost number hyperspectral images limited degenerating performance downstream tasks although recent studies attempted employ diffusion models synthesize hsis still struggle scarcity hsis affecting reliability diversity generated images studies propose incorporate multimodal data enhance spatial diversity spectral fidelity ensured addition existing hsi synthesis models typically uncontrollable support singlecondition control limiting ability generate accurate reliable hsis alleviate issues propose hsigene novel hsi generation foundation model based latent diffusion supports multicondition control allowing precise reliable hsi generation enhance spatial diversity training data preserving spectral fidelity propose new data augmentation method based spatial superresolution hsis upscaled first thus abundant training patches could obtained cropping highresolution hsis addition improve perceptual quality augmented data introduce novel twostage hsi superresolution framework first applies rgb bands superresolution utilizes proposed rectangular guided attention network rgan guided hsi superresolution experiments demonstrate proposed model capable generating vast quantity realistic hsis downstream tasks denoising superresolution code models available httpsgithubcomlipanghsigene,-1,0.0,-1,0.0
contextguided spatiotemporal video grounding spatiotemporal video grounding stvg task aims locating spatiotemporal tube specific instance given text query despite advancements current methods easily suffer distractors heavy object appearance variations videos due insufficient object information text leading degradation addressing propose novel framework contextguided stvg cgstvg mines discriminative instance context object videos applies supplementary guidance target localization key cgstvg lies two specially designed modules including instance context generation icg focuses discovering visual context information appearance motion instance instance context refinement icr aims improve instance context icg eliminating irrelevant even harmful information context grounding icg together icr deployed decoding stage transformer architecture instance context learning particularly instance context learned one decoding stage fed next stage leveraged guidance containing rich discriminative object feature enhance targetawareness decoding feature conversely benefits generating better new instance context improving localization finally compared existing methods cgstvg enjoys object information text query guidance mined instance visual context accurate target localization experiments three benchmarks including vidstg cgstvg sets new stateofthearts showing efficacy code released httpsgithubcomhenglancgstvg,7,0.9126867752823099,7,0.9126867752823099
image video reshaping receptive field via imagetovideo differentiable autoaugmentation fusion landscape deep learning research moving towards innovative strategies harness true potential data traditionally emphasis scaling model architectures resulting large complex neural networks difficult train limited computational resources however independently model size data quality ie amount variability still major factor affects model generalization work propose novel technique exploit available data use automatic data augmentation tasks image classification semantic segmentation introduce first differentiable augmentation search method das generate variations images processed videos compared previous approaches das extremely fast flexible allowing search large search spaces less gpu day intuition increased receptive field temporal dimension provided das could lead benefits also spatial receptive field specifically leverage das guide reshaping spatial receptive field selecting taskdependant transformations result compared standard augmentation alternatives improve terms accuracy imagenet tinyimagenet cityscapes datasets pluggingin das different lightweight video backbones,-1,0.0,-1,0.0
passive screentocamera communication recent technology known transparent screens transforming windows displays smart windows present buses airports offices remain transparent normal window display relevant information overlays panoramic views paper propose transforming windows screens also wireless transmitters achieve goal build upon research area screentocamera communication area videos modified way smartphone cameras decode data data remains invisible viewers person sees normal video camera sees video plus additional information communication method one biggest disadvantages traditional screens power consumption used generate light solve employ novel transparent screens relying ambient light display pictures hence eliminating power source however comes cost lower image quality since use variable outofcontrol environment light instead generating constant strong light led panels work dubbed passivecam overcomes challenge creating first screentocamera communication link using passive displays paper presents two main contributions first analyze modify existing screens encoding methods embed information reliably ambient light second develop android app optimizes decoding process obtaining realtime performance evaluation considers musical application shows packet success rate psr close addition realtime application achieves response times ms ms camera static handheld respectively,-1,0.0,-1,0.0
trafficvlm controllable visual language model traffic video captioning traffic video description analysis received much attention recently due growing demand efficient reliable urban surveillance systems existing methods focus locating traffic event segments severely lack descriptive details related behaviour context subjects interest events paper present trafficvlm novel multimodal dense video captioning model vehicle ego camera view trafficvlm models traffic video events different levels analysis spatially temporally generates long finegrained descriptions vehicle pedestrian different phases event also propose conditional component trafficvlm control generation outputs multitask finetuning paradigm enhance trafficvlms learning capability experiments show trafficvlm performs well vehicle overhead camera views solution achieved outstanding results track ai city challenge ranking us third challenge standings code publicly available httpsgithubcomquangminhdinhtrafficvlm,-1,0.0,-1,0.0
ladder efficient framework video frame interpolation video frame interpolation vfi crucial technique various applications slowmotion generation frame rate conversion video frame restoration etc paper introduces efficient video frame interpolation framework aims strike favorable balance efficiency quality framework follows general paradigm consisting flow estimator refinement module incorporating carefully designed components first adopt depthwise convolution large kernels flow estimator simultaneously reduces parameters enhances receptive field encoding rich context handling complex motion secondly diverging common design refinement module unetstructure encoderdecoder structure find redundant decoderonly refinement module directly enhances result coarse fine features offering efficient process addition address challenge handling highdefinition frames also introduce innovative hdaware augmentation strategy training leading consistent enhancement hd images extensive experiments conducted diverse datasets xiph snufilm results demonstrate approach achieves stateoftheart performance clear improvement requiring much less flops parameters reaching better spot balancing efficiency quality,-1,0.0,-1,0.0
multiframe fusion video stabilization paper present rstab novel framework video stabilization integrates multiframe fusion volume rendering departing conventional methods introduce multiframe perspective generate stabilized images addressing challenge fullframe generation preserving structure core approach lies stabilized rendering sr volume rendering module extends beyond image fusion incorporating feature fusion core rstab framework lies stabilized rendering sr volume rendering module fusing multiframe information space specifically sr involves warping features colors multiple frames projection fusing descriptors render stabilized image however precision warped information depends projection accuracy factor significantly influenced dynamic regions response introduce adaptive ray range arr module integrate depth priors adaptively defining sampling range projection process additionally propose color correction cc assisting geometric constraints optical flow accurate color aggregation thanks three modules rstab demonstrates superior performance compared previous stabilizers field view fov image quality video stability across various datasets,1,1.0,1,1.0
uncertaintyboosted robust video activity anticipation video activity anticipation aims predict happen future embracing broad application prospect ranging robot vision autonomous driving despite recent progress data uncertainty issue reflected content evolution process dynamic correlation event labels somehow ignored reduces model generalization ability deep understanding video content leading serious error accumulation degraded performance paper address uncertainty learning problem propose uncertaintyboosted robust video activity anticipation framework generates uncertainty values indicate credibility anticipation results uncertainty value used derive temperature parameter softmax function modulate predicted target activity distribution guarantee distribution adjustment construct reasonable target activity label representation incorporating activity evolution temporal class correlation semantic relationship moreover quantify uncertainty relative values comparing uncertainty among sample pairs temporallengths relative strategy provides accessible way uncertainty modeling quantifying absolute uncertainty values whole dataset experiments multiple backbones benchmarks show framework achieves promising performance better robustnessinterpretability source codes available,-1,0.0,-1,0.0
behavior imitation manipulator control grasping deep reinforcement learning existing motion imitation models typically require expert data obtained mocap devices vast amount training data needed difficult acquire necessitating substantial investments financial resources manpower time project combines human pose estimation reinforcement learning proposing novel model simplifies motion imitation prediction problem joint angle values reinforcement learning significantly reduces reliance vast amounts training data enabling agent learn imitation policy seconds video exhibit strong generalization capabilities quickly apply learned policy imitate human arm motions unfamiliar videos model first extracts skeletal motions human arms given video using human pose estimation extracted arm motions morphologically retargeted onto robotic manipulator subsequently retargeted motions used generate reference motions finally reference motions used formulate reinforcement learning problem enabling agent learn policy imitating human arm motions project excels imitation tasks demonstrates robust transferability accurately imitating human arm motions unfamiliar videos project provides lightweight convenient efficient accurate motion imitation model simplifying complex process motion imitation achieves notably outstanding performance,5,1.0,5,1.0
supergaussian repurposing video models super resolution present simple modular generic method upsamples coarse models adding geometric appearance details generative models exist yet match quality counterparts image video domains demonstrate possible directly repurpose existing pretrained video models superresolution thus sidestep problem shortage large repositories highquality training models describe repurpose video upsampling models consistent combine consolidation produce results output produce high quality gaussian splat models object centric effective method category agnostic easily incorporated existing workflows evaluate proposed supergaussian variety inputs diverse terms complexity representation eg gaussian splats nerfs demonstrate simple method significantly improves fidelity final models check project website details supergaussiangithubio,1,1.0,1,1.0
advancing compressed video action recognition progressive knowledge distillation compressed video action recognition classifies video samples leveraging different modalities compressed videos namely motion vectors residuals intraframes purpose three neural networks deployed dedicated processing one modality observations indicate network processing intraframes tend converge flatter minimum network processing residuals turn converges flatter minimum motion vector network hierarchy convergence motivates strategy knowledge transfer among modalities achieve flatter minima generally associated better generalization insight propose progressive knowledge distillation pkd technique incrementally transfers knowledge across modalities method involves attaching early exits internal classifiers ics three networks pkd distills knowledge starting motion vector network followed residual finally intraframe network sequentially improving ic accuracy propose weighted inference scaled ensemble wise combines outputs ics using learned weights boosting accuracy inference experiments demonstrate effectiveness training ics pkd compared standard crossentropybased training showing ic accuracy improvements datasets respectively additionally wise improves accuracy respectively,-1,0.0,-1,0.0
bootstrapping visionlanguage models selfsupervised remote physiological measurement facial videobased remote physiological measurement promising research area detecting human vital signs eg heart rate respiration frequency noncontact way conventional approaches mostly supervised learning requiring extensive collections facial videos synchronously recorded photoplethysmography ppg signals tackle selfsupervised learning recently gained attentions due lack ground truth ppg signals performance however limited paper propose novel selfsupervised framework successfully integrates popular visionlanguage models vlms remote physiological measurement task given facial video first augment positive negative video samples varying rppg signal frequencies next introduce frequencyoriented visiontext pair generation method carefully creating contrastive spatiotemporal maps positive negative samples designing proper text prompts describe relative ratios signal frequencies pretrained vlm employed extract features formed visiontext pairs estimate rppg signals thereafter develop series generative contrastive learning mechanisms optimize vlm including textguided visual map reconstruction task visiontext contrastive learning task frequency contrastive ranking task overall method first time adapts vlms digest align frequencyrelated knowledge vision text modalities extensive experiments four benchmark datasets demonstrate significantly outperforms state art selfsupervised methods,7,0.9076764146027259,7,0.9076764146027259
personal action recommendation suggestions egocentric videos intelligent assistance involves understanding also action existing egocentric video datasets contain rich annotations videos actions intelligent assistant could perform moment address gap release new set personal action recommendation annotations dataset take multistage approach generating evaluating annotations first used promptengineered large language model llm generate contextaware action suggestions identified action suggestions synthetic action suggestions valuable inherent limitations llms necessitate human evaluation ensure highquality usercentered recommendations conducted largescale human annotation study provides grounding human preferences analyze interrater agreement evaluate subjective preferences participants based synthetic dataset complete human annotations propose several new tasks action suggestions based egocentric videos encourage novel solutions improve latency energy requirements annotations support researchers developers working building action recommendation systems augmented virtual reality systems,-1,0.0,-1,0.0
endtoend openvocabulary video visual relationship detection using multimodal prompting openvocabulary video visual relationship detection aims expand video visual relationship detection beyond annotated categories detecting unseen relationships seen unseen objects videos existing methods usually use trajectory detectors trained closed datasets detect object trajectories feed trajectories largescale pretrained visionlanguage models achieve openvocabulary classification heavy dependence pretrained trajectory detectors limits ability generalize novel object categories leading performance degradation address challenge propose unify object trajectory detection relationship classification endtoend openvocabulary framework framework propose relationshipaware openvocabulary trajectory detector primarily consists querybased transformer decoder visual encoder clip distilled framewise openvocabulary object detection trajectory associator exploit relationship context trajectory detection relationship query embedded transformer decoder accordingly auxiliary relationship loss designed enable decoder perceive relationships objects explicitly moreover propose openvocabulary relationship classifier leverages rich semantic knowledge clip discover novel relationships adapt clip well relationship classification design multimodal prompting method employs spatiotemporal visual prompting visual representation visionguided language prompting language input extensive experiments two public datasets vidvrd vidor demonstrate effectiveness framework framework also applied difficult crossdataset scenario demonstrate generalization ability,7,0.9980808251913164,7,0.9980808251913164
skills made order efficient acquisition robot cooking skills guided multiple forms internet data study explores utility various internet data sources select among set template robot behaviors perform skills learning contactrich skills involving tool use internet data sources typically challenging due lack physical information contact existence location areas force data prior works generally used internet data foundation models trained data generate lowlevel robot behavior hypothesize data models may better suited selecting among set basic robot behaviors perform contactrich skills explore three methods template selection querying large language models comparing video robot execution retrieved human video using features pretrained video encoder common prior work performing comparison using features optic flow encoder trained internet data results show llms surprisingly capable template selectors despite lack visual information optical flow encoding significantly outperforms video encoders trained order magnitude data important synergies exist various forms internet data template selection exploiting synergies create template selector using multiple forms internet data achieves success rate set different cooking skills involving tooluse,-1,0.0,-1,0.0
rexplain translating radiology patientfriendly video reports radiology reports designed efficient communication medical experts often remain incomprehensible patients inaccessibility could potentially lead anxiety decreased engagement treatment decisions poorer health outcomes undermining patientcentered care present rexplain radiology explanation innovative aidriven system translates radiology findings patientfriendly video reports rexplain uniquely integrates large language model medical text simplification textanatomy association image segmentation model anatomical region identification avatar generation tool engaging interface visualization rexplain enables producing comprehensive explanations plain language highlighted imagery organ renderings form video reports evaluate utility rexplaingenerated explanations conducted two rounds user feedback collection six boardcertified radiologists results proofofconcept study indicate rexplain could accurately deliver radiological information effectively simulate oneonone consultation shedding light enhancing patientcentered radiology potential clinical usage work demonstrates new paradigm aiassisted medical communication potentially improving patient engagement satisfaction radiology care opens new avenues research multimodal medical communication,-1,0.0,-1,0.0
efficient transfer learning videolanguage foundation models pretrained visionlanguage models provide robust foundation efficient transfer learning across various downstream tasks field video action recognition mainstream approaches often introduce additional modules capture temporal information although additional modules increase capacity model enabling better capture videospecific inductive biases existing methods typically introduce substantial number new parameters prone catastrophic forgetting previously acquired generalizable knowledge paper propose parameterefficient multimodal spatiotemporal adapter msta enhance alignment textual visual representations achieving balance generalizable knowledge taskspecific adaptation furthermore mitigate overfitting enhance generalizability introduce spatiotemporal descriptionguided consistency constraintthis constraint involves providing template inputs eg video textbfcls trainable language branch llmgenerated spatiotemporal descriptions pretrained language branch enforcing output consistency branches approach reduces overfitting downstream tasks enhances distinguishability trainable branch within spatiotemporal semantic space evaluate effectiveness approach across four tasks zeroshot transfer fewshot learning basetonovel generalization fullysupervised learning compared many stateoftheart methods msta achieves outstanding performance across evaluations using trainable parameters original model,-1,0.0,-1,0.0
videotext dataset construction multiai feedback promoting weaktostrong preference learning video large language models highquality videotext preference data crucial multimodal large language models mllms alignment however existing preference data scarce obtaining vqa preference data preference training costly manually annotating responses highly unreliable could result lowquality pairs meanwhile aigenerated responses controlled temperature adjustment lack diversity address issues propose highquality vqa preference dataset called textittextbfmultiple textbfmultimodal textbfartificial textbfintelligence textbfpreference datasets textbfvqa textbfmmaipv constructed sampling response distribution set using external scoring function response evaluation furthermore fully leverage preference knowledge mmaipv ensure sufficient optimization propose textittextbfiterative textbfweaktotextbfstrong textbfreinforcement textbflearning textbfai textbffeedback video mllms framework gradually enhances mllms alignment capabilities iteratively updating reference model performing parameter extrapolation finally propose unbiased informationcomplete evaluation scheme vqa evaluation experiments demonstrate mmaipv beneficial mllms preference learning fully exploits alignment information mmaipv believe proposed automatic vqa preference data generation pipeline based ai feedback greatly promote future work mllms alignment textbfcode dataset available,0,0.8898202405051708,0,0.8898202405051708
instit boosting multimodal instance understanding via explicit visual prompt instruction tuning large multimodal models lmms made significant breakthroughs advancement instruction tuning however existing models understand images videos holistic level still struggle instancelevel understanding requires nuanced comprehension alignment instancelevel understanding crucial focuses specific elements interested excitingly existing works find stateoftheart lmms exhibit strong instance understanding capabilities provided explicit visual cues motivated introduce automated annotation pipeline assisted extract instancelevel information images videos explicit visual prompting instance guidance building upon pipeline proposed instit solution enhance lmms instance understanding via explicit visual prompt instruction tuning instit consists benchmark diagnose multimodal instancelevel understanding largescale instructiontuning dataset continuous instructiontuning training paradigm effectively enhance spatialtemporal instance understanding capabilities existing lmms experimental results show boost instit models achieve outstanding performance instit bench also demonstrate significant improvements across various generic image video understanding benchmarks highlights dataset boosts instancelevel understanding also strengthens overall capabilities generic image video comprehension,0,0.8959420199966783,0,0.8959420199966783
shotvl humancentric highlight frame retrieval via language queries existing works humancentric video understanding typically focus analyzing specific moment entire videos however many applications require higher precision frame level work propose novel task bestshot aims locate highlight frames within humancentric videos via language queries task demands deep semantic comprehension human actions also precise temporal localization support task introduce bestshot benchmark benchmark meticulously constructed combining human detection tracking potential frame selection based human judgment detailed textual descriptions crafted human input ensure precision benchmark meticulously constructed combining humanannotated highlight frames detailed textual descriptions duration labeling descriptions encompass three critical elements visual content finegrained action human pose description together elements provide necessary precision identify exact highlight frames videos tackle problem collected two distinct datasets dataset algorithmically generated ii imagesmpltext dataset dataset largescale accurate perframe pose description leveraging posescript existing pose estimation datasets based datasets present strong baseline model shotvl finetuned internvl specifically bestshot highlight impressive zeroshot capabilities model offer comparative analyses existing sota models shotvl demonstrates significant improvement internvl bestshot benchmark notable improvement benchmark maintaining sota performance general image classification retrieval,-1,0.0,-1,0.0
exploiting multimodal spatialtemporal patterns video object tracking multimodal tracking garnered widespread attention result ability effectively address inherent limitations traditional rgb tracking however existing multimodal trackers mainly focus fusion enhancement spatial features merely leverage sparse temporal relationships video frames approaches fully exploit temporal correlations multimodal videos making difficult capture dynamic changes motion information targets complex scenarios alleviate problem propose unified multimodal spatialtemporal tracking approach named sttrack contrast previous paradigms solely relied updating reference information introduced temporal state generator tsg continuously generates sequence tokens containing multimodal temporal information temporal information tokens used guide localization target next time state establish longrange contextual relationships video frames capture temporal trajectory target furthermore spatial level introduced mamba fusion background suppression interactive bsi modules modules establish dualstage mechanism coordinating information interaction fusion modalities extensive comparisons five benchmark datasets illustrate sttrack achieves stateoftheart performance across various multimodal tracking scenarios code available httpsgithubcomnjupcalabsttrack,-1,0.0,-1,0.0
wemgan wavelet transform based facial expression manipulation facial expression manipulation aims change human facial expressions without affecting face recognition order transform facial expressions target expressions previous methods relied expression labels guide manipulation process however methods failed preserve details facial features causes weakening loss identity information output image work propose wemgan short waveletbased expression manipulation gan puts efforts preserving details original image editing process firstly take advantage wavelet transform technique combine generator unet autoencoder backbone order improve generators ability preserve details facial features secondly also implement highfrequency component discriminator use highfrequency domain adversarial loss constrain optimization model providing generated face image abundant details additionally order narrow gap generated facial expressions target expressions use residual connections encoder decoder also using relative action units aus several times extensive qualitative quantitative experiments demonstrated model performs better preserving identity features editing capability image generation quality affectnet dataset also shows superior performance metrics average content distance acd expression distance ed,-1,0.0,-1,0.0
generative ai machine learning approach robust efficient lung segmentation chest radiography climacteric identifying different pulmonary diseases yet radiologist workload inefficiency lead misdiagnoses automatic accurate efficient segmentation lung xray images chest paramount early disease detection study develops deep learning framework using generative adversarial network gan segment pulmonary abnormalities cxr images frameworks image preprocessing augmentation techniques properly incorporated unetinspired generatordiscriminator architecture initially loaded cxr images manual masks montgomery shenzhen datasets preprocessing resizing performed unet generator applied processed cxr images yield segmented masks discriminator network differentiates generated real masks montgomery dataset served models training set study shenzhen dataset used test robustness used first time adversarial loss distance used optimize model training metrics assess precision recall score dice coefficient prove effectiveness framework pulmonary abnormality segmentation therefore sets basis future studies performed shortly using diverse datasets could confirm clinical applicability medical imaging,-1,0.0,-1,0.0
lcbnet longcontext biasing audiovisual speech recognition growing prevalence online conferences courses presents new challenge improving automatic speech recognition asr enriched textual information video slides contrast rare phrase lists slides within videos synchronized realtime speech enabling extraction long contextual bias therefore propose novel longcontext biasing network lcbnet audiovisual speech recognition avsr leverage longcontext information available videos effectively specifically adopt biencoder architecture simultaneously model audio longcontext biasing besides also propose biasing prediction module utilizes binary cross entropy bce loss explicitly determine biased phrases longcontext biasing furthermore introduce dynamic contextual phrases simulation enhance generalization robustness lcbnet experiments slidespeech largescale audiovisual corpus enriched slides reveal proposed lcbnet outperforms general asr model relative weruwerbwer reduction test set enjoys high unbiased biased performance moreover also evaluate model librispeech corpus leading relative weruwerbwer reduction asr model,-1,0.0,-1,0.0
highquality mesh blendshape generation face videos via neural inverse rendering readily editable mesh blendshapes widely used animation pipelines recent advancements neural geometry appearance representations enabled highquality inverse rendering building upon observations introduce novel technique reconstructs meshbased blendshape rigs single sparse multiview videos leveraging stateoftheart neural inverse rendering begin constructing deformation representation parameterizes vertex displacements differential coordinates tetrahedral connections allowing highquality vertex deformation highresolution meshes constructing set semantic regulations representation achieve joint optimization blendshapes expression coefficients furthermore enable userfriendly multiview setup unsynchronized cameras propose neural regressor model timevarying motion parameters approach implicitly considers time difference across multiple cameras enhancing accuracy motion modeling experiments demonstrate flexible input single sparse multiview videos reconstruct personalized highfidelity blendshapes blendshapes geometrically semantically accurate compatible industrial animation pipelines code data available httpsgithubcomgrignarderhighqualityblendshapegeneration,-1,0.0,-1,0.0
gptsee enhancing moment retrieval highlight detection via descriptionbased similarity features moment retrieval mr highlight detection hd aim identify relevant moments highlights video corresponding natural language query large language models llms demonstrated proficiency various computer vision tasks however existing methods mrhd yet integrated llms letter propose novel twostage model takes output llms input secondstage transformer encoderdecoder first employed generate detailed description video frame rewrite query statement fed encoder new features semantic similarity computed generated description rewritten queries finally continuous highsimilarity video frames converted span anchors serving prior position information decoder experiments demonstrate approach achieves stateoftheart result using span anchors similarity scores outputs positioning accuracy outperforms traditional methods like momentdetr,0,1.0,0,1.0
ridtwin endtoend pipeline automatic face deidentification videos face deidentification videos challenging task domain computer vision primarily used privacypreserving applications despite considerable progress achieved generative vision models remain multiple challenges latest approaches lack comprehensive discussion evaluation aspects realism temporal coherence preservation nonidentifiable features work propose ridtwin novel pipeline leverages stateoftheart generative models decouples identity motion perform automatic face deidentification videos investigate task holistic point view discuss approach addresses pertinent existing challenges domain evaluate performance methodology widely employed dataset also custom dataset designed accommodate limitations certain behavioral variations absent dataset discuss implications advantages work suggest directions future research,4,0.676661381258312,4,0.676661381258312
plottal prompt learning optimal transport fewshot temporal action localization paper introduces novel approach temporal action localization tal fewshot learning work addresses inherent limitations conventional singleprompt learning methods often lead overfitting due inability generalize across varying contexts realworld videos recognizing diversity camera views backgrounds objects videos propose multiprompt learning framework enhanced optimal transport design allows model learn set diverse prompts action capturing general characteristics effectively distributing representation mitigate risk overfitting furthermore employing optimal transport theory efficiently align prompts action features optimizing comprehensive representation adapts multifaceted nature video data experiments demonstrate significant improvements action localization accuracy robustness fewshot settings standard challenging datasets highlighting efficacy multiprompt optimal transport approach overcoming challenges conventional fewshot tal methods,5,0.41163999642157534,5,0.41163999642157534
unified framework humancentric point cloud video understanding humancentric point cloud video understanding pvu emerging field focused extracting interpreting humanrelated features sequences human point clouds advancing downstream humancentric tasks applications previous works usually focus tackling one specific task rely huge labeled data poor generalization capability considering human specific characteristics including structural semantics human body dynamics human motions propose unified framework make full use prior knowledge explore inherent features data generalized humancentric point cloud video understanding extensive experiments demonstrate method achieves stateoftheart performance various humanrelated tasks including action recognition pose estimation datasets code released soon,-1,0.0,-1,0.0
perceptionoriented video frame interpolation via asymmetric blending previous methods video frame interpolation vfi encountered challenges notably manifestation blur ghosting effects issues traced back two pivotal factors unavoidable motion errors misalignment supervision practice motion estimates often prove errorprone resulting misaligned features furthermore reconstruction loss tends bring blurry results particularly misaligned regions mitigate challenges propose new paradigm called pervfi perceptionoriented video frame interpolation approach incorporates asymmetric synergistic blending module asb utilizes features sides synergistically blend intermediate features one reference frame emphasizes primary content contributes complementary information impose stringent constraint blending process introduce selflearned sparse quasibinary mask effectively mitigates ghosting blur artifacts output additionally employ normalizing flowbased generator utilize negative loglikelihood loss learn conditional distribution output facilitates generation clear fine details experimental results validate superiority pervfi demonstrating significant improvements perceptual quality compared existing methods codes available urlhttpsgithubcommulnspervfi,-1,0.0,-1,0.0
watching popular musicians learn ear hypothesisgenerating study humanrecording interactions youtube videos popular musicians often learn music ear unclear role technology plays experience task search opportunities development novel humanrecording interactions analyze youtube videos depicting realworld examples byear learning discuss preliminary phase research online videos appropriate data observations generate hypotheses inform future work example musicians scope learning may influence technological interactions would help could benefit tools accommodate working memory transcription appear play key role ear learning based findings pose number research questions discuss methodological considerations guide future study,-1,0.0,-1,0.0
using deep convolutional neural networks detect rendered glitches video games paper present method using deep convolutional neural networks dcnns detect common glitches video games problem setting consists image rgb input classified one five defined classes normal image one four different kinds glitches stretched low resolution missing placeholder textures using supervised approach train using generated data work focuses detecting texture graphical anomalies achieving arguably good performance accuracy detecting glitches false positive rate models able generalize detect glitches even unseen objects apply confidence measure well tackle issue false positives well effective way aggregating images achieve better detection production main use work partial automatization graphical testing final stages video game development,4,0.7155044419945432,4,0.7155044419945432
fullstage pseudo label quality enhancement weaklysupervised temporal action localization weaklysupervised temporal action localization wstal aims localize actions untrimmed videos using videolevel supervision latest wstal methods introduce pseudo label learning framework bridge gap classificationbased training inferencing targets localization achieve cuttingedge results frameworks classificationbased model used generate pseudo labels regressionbased student model learn however quality pseudo labels framework key factor final result carefully studied paper propose set simple yet efficient pseudo label quality enhancement mechanisms build fustal framework fustal enhances pseudo label quality three stages crossvideo contrastive learning proposal generationstage priorbased filtering proposal selectionstage emabased distillation trainingstage designs enhance pseudo label quality different stages framework help produce informative less false smoother action proposals help comprehensive designs stages fustal achieves average map outperforming previous best method becomes first method reach milestone,7,0.8212143188911067,7,0.8212143188911067
arbitraryscale video superresolution structural textural priors arbitraryscale video superresolution avsr aims enhance resolution video frames potentially various scaling factors presents several challenges regarding spatial detail reproduction temporal consistency computational complexity paper first describe strong baseline avsr putting together three variants elementary building blocks flowguided recurrent unit aggregates spatiotemporal information previous frames flowrefined crossattention unit selects spatiotemporal information future frames hyperupsampling unit generates scaleaware contentindependent upsampling kernels introduce stavsr equipping baseline multiscale structural textural prior computed pretrained vgg network prior proven effective discriminating structure texture across different locations scales beneficial avsr comprehensive experiments show stavsr significantly improves superresolution quality generalization ability inference speed stateoftheart code available,-1,0.0,-1,0.0
ophthalmic biomarker detection highlights ieee video image processing cup student competition vip cup offers unique experience undergraduates allowing students work together solve challenging realworld problems video image processing techniques iteration vip cup challenged students balance personalization generalization performing biomarker detection optical coherence tomography oct images balancing personalization generalization important challenge tackle variation within oct scans patients visits minimal difference manifestation disease across different patients may substantial domain difference oct scans arise due pathology manifestation across patients clinical labels visit along treatment process scan taken hence provided multimodal oct dataset allow teams effectively target challenge overall competition gave undergraduates opportunity learn artificial intelligence powerful tool medical field well unique challenges one faces applying machine learning biomedical data,3,0.5260576955629406,3,0.5260576955629406
sam sam slicer segmentwithsam extension annotating medical images creating annotations medical data timeconsuming often requires highly specialized expertise various tools implemented aid process segment anything model sam offers generalpurpose promptbased segmentation algorithm designed annotate videos paper adapt model annotation medical images offer implementation form extension popular annotation software slicer extension allows users place point prompts slices generate annotation masks propagate annotations across entire volumes either singledirectional bidirectional manners code publicly available httpsgithubcommazurowskilabslicersegmentwithsam easily installed directly extension manager slicer well,-1,0.0,-1,0.0
taptotab videobased guitar tabs generation using ai audio analysis automation guitar tablature generation video inputs holds significant promise enhancing music education transcription accuracy performance analysis existing methods face challenges consistency completeness particularly detecting fretboards accurately identifying notes address issues paper introduces advanced approach leveraging deep learning specifically yolo models realtime fretboard detection fourier transformbased audio analysis precise note identification experimental results demonstrate substantial improvements detection accuracy robustness compared traditional techniques paper outlines development implementation evaluation methodologies aiming revolutionize guitar instruction automating creation guitar tabs video recordings,8,1.0,8,1.0
talkinnerf animatable neural fields fullbody talking humans introduce novel framework learns dynamic neural radiance field nerf fullbody talking humans monocular videos prior work represents body pose face however humans communicate full body combining body pose hand gestures well facial expressions work propose talkinnerf unified nerfbased network represents holistic human motion given monocular video subject learn corresponding modules body face hands combined together generate final result capture complex finger articulation learn additional deformation field hands multiidentity representation enables simultaneous training multiple subjects well robust animation completely unseen poses also generalize novel identities given short video input demonstrate stateoftheart performance animating fullbody talking humans finegrained hand articulation facial expressions,-1,0.0,-1,0.0
show guide instructionalplan grounded vision language model guiding users complex procedural plans inherently multimodal task visually illustrated plan steps crucial deliver effective plan guidance however existing works planfollowing language models lms often capable multimodal input output work present mmplanllm first multimodal llm designed assist users executing instructional tasks leveraging textual plans visual information specifically bring crossmodality two key tasks conversational video moment retrieval model retrieves relevant stepvideo segments based user queries visuallyinformed step generation model generates next step plan conditioned image users current progress mmplanllm trained using novel multitaskmultistage approach designed gradually expose model multimodal instructionalplans semantic layers achieving strong performance multimodal textual dialogue plangrounded setting furthermore show model delivers crossmodal temporal planstructure representations aligned textual plan steps instructional video moments,-1,0.0,-1,0.0
experts public governing multimodal language models politically sensitive video analysis paper examines governance multimodal large language models mmllms individual collective deliberation focusing analyses politically sensitive videos conducted twostep study first interviews journalists established baseline understanding expert video interpretation second individuals general public engaged deliberation using inclusiveai platform facilitates democratic decisionmaking decentralized autonomous organization dao mechanisms findings show experts emphasized emotion narrative general public prioritized factual clarity objectivity situation emotional neutrality additionally explored impact different governance mechanisms quadratic vs weighted ranking voting equal vs power distributions users decisionmaking ai behave specifically quadratic voting enhanced perceptions liberal democracy political equality participants optimistic ai perceived voting process higher level participatory democracy results suggest potential applying dao mechanisms help democratize ai governance,10,0.8525042267438383,10,0.8525042267438383
multi class activity classification videos using motion history image generation human action recognition topic interest across multiple fields ranging security entertainment systems tracking motion identifying action performed real time basis necessary critical security systems entertainment especially gaming need immediate responses actions gestures paramount success system show motion history image well established framework capture temporal activity information multi dimensional detail enabling various usecases including classification utilize mhi produce sample data train classifier demonstrate effectiveness action classification across six different activities single multiaction video analyze classifier performance identify usecases mhi struggles generate appropriate activity image discuss mechanisms future work overcome limitations,-1,0.0,-1,0.0
describe userdriven audio description blind low vision individuals audio descriptions ad make videos accessible blind low vision blv users describing visual elements understood main audio track ad created professionals novice describers timeconsuming lacks scalability offering little control blv viewers description length content receive address gap explore userdriven aigenerated descriptions blv viewer controls receive descriptions study blv participants activated audio descriptions seven different video genres two levels detail concise detailed results show differences ad frequency level detail blv users wanted different videos sense control style ad delivery limitations variations among blv users ad needs perception aigenerated descriptions discuss implications findings future aibased ad tools,-1,0.0,-1,0.0
enhancing videollm reasoning via agentofthoughts distillation paper tackles problem video question answering videoqa task often requires multistep reasoning profound understanding spatialtemporal dynamics large videolanguage models perform well benchmarks often lack explainability spatialtemporal grounding paper propose agentofthoughts distillation aotd method enhances models incorporating automatically generated chainofthoughts cots instructiontuning process specifically leverage agentbased system decompose complex questions subtasks address specialized vision models intermediate results treated reasoning chains also introduce verification mechanism using large language model llm ensure reliability generated cots extensive experiments demonstrate aotd improves performance multiplechoice openended benchmarks,0,1.0,0,1.0
scenefactor factored latent diffusion controllable scene generation present scenefactor diffusionbased approach largescale scene generation enables controllable generation effortless editing scenefactor enables textguided scene synthesis factored diffusion formulation leveraging latent semantic geometric manifolds generation arbitrarysized scenes text input enables easy controllable generation text guidance remains imprecise intuitive localized editing manipulation generated scenes factored semantic diffusion generates proxy semantic space composed semantic boxes enables controllable editing generated scenes adding removing changing size semantic proxy boxes guides highfidelity consistent geometric editing extensive experiments demonstrate approach enables highfidelity scene synthesis effective controllable editing factored diffusion approach,-1,0.0,-1,0.0
domaintransferred synthetic data generation improving monocular depth estimation major obstacle development effective monocular depth estimation algorithms difficulty obtaining highquality depth data corresponds collected rgb images collecting data timeconsuming costly even data collected modern sensors limited range resolution subject inconsistencies noise combat propose method data generation simulation using synthetic environments cyclegan domain transfer compare method data generation popular nyudepth dataset training depth estimation model based densedepth structure using different training sets real simulated data evaluate performance models newly collected images lidar depth data husky robot verify generalizability approach show gantransformed data serve effective alternative realworld data particularly depth estimation,14,1.0,14,1.0
towards physicsinformed cyclic adversarial multipsf lensless imaging lensless imaging emerged promising field within inverse imaging offering compact costeffective solutions potential revolutionize computational camera market circumventing traditional optical components like lenses mirrors novel approaches like maskbased lensless imaging eliminate need conventional hardware however advancements lensless image reconstruction particularly leveraging generative adversarial networks gans hindered reliance datadriven training processes resulting network specificity point spread function psf imaging system necessitates complete retraining minor psf changes limiting adaptability generalizability across diverse imaging scenarios paper introduce novel approach multipsf lensless imaging employing dual discriminator cyclic adversarial framework propose unique generator architecture sparse convolutional psfaware auxiliary branch coupled forward model integrated training loop facilitate physicsinformed learning handle substantial domain gap lensless lensed images comprehensive performance evaluation ablation studies underscore effectiveness model offering robust adaptable lensless image reconstruction capabilities method achieves comparable performance existing psfagnostic generative methods single psf cases demonstrates resilience psf changes without need retraining,-1,0.0,-1,0.0
moving object proposals deep learned optical flow video object segmentation dynamic scene understanding one conspicuous field interest among computer vision community order enhance dynamic scene understanding pixelwise segmentation neural networks widely accepted latest researches pixelwise segmentation combined semantic motion information produced good performance work propose state art architecture neural networks accurately efficiently get moving object proposals mop first train unsupervised convolutional neural network unflow generate optical flow estimation render output optical flow net fully convolutional segnet model main contribution work finetuning pretrained optical flow model brand new davis dataset leveraging fully convolutional neural networks encoderdecoder architecture segment objects developed codes tensorflow executed training evaluation processes aws instance,-1,0.0,-1,0.0
gaussian shadow casting neural characters neural character models reconstruct detailed geometry texture video lack explicit shadows shading leading artifacts generating novel views poses relighting particularly difficult include shadows global effect required casting secondary rays costly propose new shadow model using gaussian density proxy replaces sampling simple analytic formula supports dynamic motion tailored shadow computation thereby avoiding affine projection approximation sorting required closely related gaussian splatting combined deferred neural rendering model gaussian shadows enable lambertian shading shadow casting minimal overhead demonstrate improved reconstructions better separation albedo shading shadows challenging outdoor scenes direct sun light hard shadows method able optimize light direction without input user result novel poses fewer shadow artifacts relighting novel scenes realistic compared stateoftheart methods providing new ways pose neural characters novel environments increasing applicability,1,1.0,1,1.0
pixelwise color constancy via smoothness techniques multiilluminant scenes scenes illuminated several light sources traditional assumption uniform illumination invalid issue ignored color constancy methods primarily due complex spatial impact multiple light sources image moreover existing multiilluminant methods fail preserve smooth change illumination stems spatial dependencies natural images motivated propose novel multiilluminant color constancy method learning pixelwise illumination maps caused multiple light sources proposed method enforces smoothness within neighboring pixels regularizing training total variation loss moreover bilateral filter provisioned enhance natural appearance estimated images preserving edges additionally propose labelsmoothing technique enables model generalize well despite uncertainties ground truth quantitative qualitative experiments demonstrate proposed method outperforms stateoftheart,-1,0.0,-1,0.0
eventbased asynchronous hdr imaging temporal incident light modulation dynamic range dr pivotal characteristic imaging systems current framebased cameras struggle achieve high dynamic range imaging due conflict globally uniform exposure spatially variant scene illumination paper propose asynhdr pixelasynchronous hdr imaging system based key insights challenges hdr imaging unique eventgenerating mechanism dynamic vision sensors dvs proposed asynhdr system integrates dvs set lcd panels lcd panels modulate irradiance incident upon dvs altering transparency thereby triggering pixelindependent event streams hdr image subsequently decoded event streams temporalweighted algorithm experiments standard test platform several challenging scenes verified feasibility system hdr imaging task,1,1.0,1,1.0
visiongpt llmassisted realtime anomaly detection safe visual navigation paper explores potential large language modelsllms zeroshot anomaly detection safe visual navigation assistance stateoftheart realtime openworld object detection model yoloworld specialized prompts proposed framework identify anomalies within cameracaptured frames include possible obstacles generate concise audiodelivered descriptions emphasizing abnormalities assist safe visual navigation complex circumstances moreover proposed framework leverages advantages llms openvocabulary object detection model achieve dynamic scenario switch allows users transition smoothly scene scene addresses limitation traditional visual navigation furthermore paper explored performance contribution different prompt components provided vision future improvement visual accessibility paved way llms video anomaly detection visionlanguage understanding,0,0.8287917623265034,0,0.8287917623265034
pointsoup highperformance extremely lowdecodinglatency learned geometry codec largescale point cloud scenes despite considerable progress achieved point cloud geometry compression still remains challenge effectively compressing largescale scenes sparse surfaces another key challenge lies reducing decoding latency crucial requirement realworld application paper propose pointsoup efficient learningbased geometry codec attains highperformance extremely lowdecodinglatency simultaneously inspired conventional trisoup codec point modelbased strategy devised characterize local surfaces specifically skin features embedded local windows via attentionbased encoder dilated windows introduced crossscale priors infer distribution quantized features parallel decoding features undergo fast refinement followed foldingbased point generator reconstructs point coordinates fairly fast speed experiments show pointsoup achieves stateoftheart performance multiple benchmarks significantly lower decoding complexity ie faster trisoup decoder comparatively lowend platform eg one rtx furthermore offers variablerate control single neural model attractive industrial practitioners,2,0.9485684994110857,2,0.9485684994110857
investigation unsupervised supervised hyperspectral anomaly detection hyperspectral sensing valuable tool detecting anomalies distinguishing materials scene hyperspectral anomaly detection hsad helps characterize captured scenes separates anomaly background classes vital agriculture environment military applications rsta reconnaissance surveillance target acquisition missions previously designed equal voting ensemble hyperspectral unmixing three unsupervised hsad algorithms later utilized supervised classifier determine weights voting ensemble creating hybrid heterogeneous unsupervised hsad algorithms supervised classifier model stacking improved detection accuracy however supervised classification methods usually fail detect novel unknown patterns substantially deviate seen previously work evaluate technique supervised unsupervised methods using general hyperspectral data provide new insights,14,0.9072562657693726,14,0.9072562657693726
towards infusing auxiliary knowledge distracted driver detection distracted driving leading cause road accidents globally identification distracted driving involves reliably detecting classifying various forms driver distraction eg texting eating using incar devices invehicle camera feeds enhance road safety task challenging due need robust models generalize diverse set driver behaviors without requiring extensive annotated datasets paper propose novel method distracted driver detection ddd infusing auxiliary knowledge semantic relations entities scene structural configuration drivers pose specifically construct unified framework integrates scene graphs driver pose information visual cues video frames create holistic representation drivers actionsour results indicate achieves accuracy improvement visiononly baseline incorporating auxiliary knowledge visual information,4,0.7322130247376135,4,0.7322130247376135
eventbased mosaicing bundle adjustment tackle problem mosaicing bundle adjustment ie simultaneous refinement camera orientations scene map purely rotating event camera formulate problem regularized nonlinear least squares optimization objective function defined using linearized event generation model camera orientations panoramic gradient map scene show ba optimization exploitable blockdiagonal sparsity structure problem solved efficiently best knowledge first work leverage sparsity speed optimization context eventbased cameras without need convert events imagelike representations evaluate method called emba synthetic realworld datasets show effectiveness photometric error decrease yielding results unprecedented quality addition demonstrate emba using high spatial resolution event cameras yielding delicate panoramas wild even without initial map project page httpsgithubcomtubripemba,1,1.0,1,1.0
local policies enable zeroshot longhorizon manipulation robotic manipulation difficult due challenges simulating complex contacts generating realistic task distributions tackle latter problem introduce manipgen leverages new class policies transfer local policies locality enables variety appealing properties including invariances absolute robot object pose skill ordering global scene configuration combine policies foundation models vision language motion planning demonstrate sota zeroshot performance method robosuite benchmark tasks simulation transfer local policies simulation reality observe solve unseen longhorizon manipulation tasks stages significant pose object scene configuration variation manipgen outperforms sota approaches saycan openvla llmtrajgen voxposer across realworld manipulation tasks respectively video results httpsmihdalalgithubiomanipgen,5,0.48998791526502466,5,0.48998791526502466
splats splats embedding invisible watermark within gaussian splatting gaussian splatting demonstrated impressive reconstruction performance explicit scene representations given widespread application reconstruction generation tasks urgent need protect copyright assets however existing copyright protection techniques overlook usability assets posing challenges practical deployment describe watergs first watermarking framework embeds content without modifying attributes vanilla achieve take deep insight spherical harmonics sh devise importancegraded sh coefficient encryption strategy embed hidden sh coefficients furthermore employ convolutional autoencoder establish mapping original gaussian primitives opacity hidden gaussian primitives opacity extensive experiments indicate watergs significantly outperforms existing steganography techniques higher scene fidelity faster rendering speed ensuring security robustness user experience codes data released httpswatergsgithubio,4,1.0,4,1.0
towards physicallybased skymodeling accurate environment maps key component rendering photorealistic outdoor scenes coherent illumination enable captivating visual arts immersive virtual reality wide range engineering scientific applications recent works extended skymodels comprehensive inclusive cloud formations existing approaches fall short faithfully recreating keycharacteristics physically captured hdri demonstrate environment maps produced skymodels relight scenes tones shadows illumination coherence physically captured hdr imagery though visual quality dnngenerated ldr hdr imagery greatly progressed recent years demonstrate progress tangential skymodelling due extended dynamic range edr required outdoor environment maps inclusive sun skymodelling extends beyond conventional paradigm high dynamic range imagery hdri work propose allweather skymodel learning weatheredskies directly physically captured hdr imagery per usercontrolled positioning sun cloud formations model allsky allows emulation physically captured environment maps improved retention extended dynamic range edr sky,-1,0.0,-1,0.0
enhancing autonomous vehicle safety rain datacentric approach clear vision autonomous vehicles face significant challenges navigating adverse weather particularly rain due visual impairment camerabased systems study leveraged contemporary deep learning techniques mitigate challenges aiming develop vision model processes live vehicle camera feeds eliminate raininduced visual hindrances yielding visuals closely resembling clear rainfree scenes using car learning act carla simulation environment generated comprehensive dataset clear rainy images model training testing model employed classic encoderdecoder architecture skip connections concatenation operations trained using novel batching schemes designed effectively distinguish highfrequency rain patterns lowfrequency scene features across successive image frames evaluate model performance integrated steering module processes frontview images input results demonstrated notable improvements steering accuracy underscoring models potential enhance navigation safety reliability rainy weather conditions,14,1.0,14,1.0
neural network diffusion diffusion models achieved remarkable success image video generation work demonstrate diffusion models also textitgenerate highperforming neural network parameters approach simple utilizing autoencoder diffusion model autoencoder extracts latent representations subset trained neural network parameters next diffusion model trained synthesize latent representations random noise model generates new representations passed autoencoders decoder produce new subsets highperforming network parameters across various architectures datasets approach consistently generates models comparable improved performance trained networks minimal additional cost notably empirically find generated models memorizing trained ones results encourage exploration versatile use diffusion models code available hrefhttpsgithubcomnushpcailabneuralnetworkdiffusionhere,-1,0.0,-1,0.0
cosign fewstep guidance consistency model solve general inverse problems diffusion models demonstrated strong priors solving general inverse problems existing diffusion modelbased inverse problem solvers dis employ plugandplay approach guide sampling trajectory either projections gradients though effective methods generally necessitate hundreds sampling steps posing dilemma inference time reconstruction quality work try push boundary inference steps nfes still maintaining high reconstruction quality achieve propose leverage pretrained distillation diffusion model namely consistency model data prior key achieving fewstep guidance enforce two types constraints sampling process consistency model soft measurement constraint controlnet hard measurement constraint via optimization supporting singlestep reconstruction multistep refinement proposed framework provides way trade image quality additional computational cost within comparable nfes method achieves new stateoftheart diffusionbased inverse problem solving showcasing significant potential employing priorbased inverse problem solvers realworld applications code available httpsgithubcombiomedailabumichgancosign,-1,0.0,-1,0.0
grin zeroshot metric depth pixellevel diffusion reconstruction single image longstanding problem computer vision learningbased methods address inherent scale ambiguity leveraging increasingly large labeled unlabeled datasets produce geometric priors capable generating accurate predictions across domains result state art approaches show impressive performance zeroshot relative metric depth estimation recently diffusion models exhibited remarkable scalability generalizable properties learned representations however models repurpose tools originally designed image generation operate dense groundtruth available depth labels especially realworld settings paper present grin efficient diffusion model designed ingest sparse unstructured training data use image features geometric positional encodings condition diffusion process globally locally generating depth predictions pixellevel comprehensive experiments across eight indoor outdoor datasets show grin establishes new state art zeroshot metric monocular depth estimation even trained scratch,1,1.0,1,1.0
zeroshot dynamic mri reconstruction globaltolocal diffusion model diffusion models recently demonstrated considerable advancement generation reconstruction magnetic resonance imaging mri data models exhibit great potential handling unsampled data reducing noise highlighting promise generative models however application dynamic mri remains relatively underexplored primarily due substantial amount fullysampled data typically required training difficult obtain dynamic mri due spatiotemporal complexity high acquisition costs address challenge propose dynamic mri reconstruction method based timeinterleaved acquisition scheme termed globaltolocal diffusion model specifically fully encoded fullresolution reference data constructed merging undersampled kspace data adjacent time frames generating two distinct bulk training datasets global local models globaltolocal diffusion framework alternately optimizes global information local image details enabling zeroshot reconstruction extensive experiments demonstrate proposed method performs well terms noise reduction detail preservation achieving reconstruction quality comparable supervised approaches,3,0.861286002840848,3,0.861286002840848
evaluation agent efficient promptable evaluation framework visual generative models recent advancements visual generative models enabled highquality image video generation opening diverse applications however evaluating models often demands sampling hundreds thousands images videos making process computationally expensive especially diffusionbased models inherently slow sampling moreover existing evaluation methods rely rigid pipelines overlook specific user needs provide numerical results without clear explanations contrast humans quickly form impressions models capabilities observing samples mimic propose evaluation agent framework employs humanlike strategies efficient dynamic multiround evaluations using samples per round offering detailed usertailored analyses offers four key advantages efficiency promptable evaluation tailored diverse user needs explainability beyond single numerical scores scalability across various models tools experiments show evaluation agent reduces evaluation time traditional methods delivering comparable results evaluation agent framework fully opensourced advance research visual generative models efficient evaluation,10,0.6425171534172058,10,0.6425171534172058
cortical surface diffusion generative models cortical surface analysis gained increased prominence given potential implications neurological developmental disorders traditional vision diffusion models effective generating natural images present limitations capturing intricate development patterns neuroimaging due limited datasets particularly true generating cortical surfaces individual variability cortical morphology high leading urgent need better methods model brain development diverse variability inherent across different individuals work proposed novel diffusion model generation cortical surface metrics using modified surface vision transformers principal architecture validate method developing human connectome project dhcp results suggest model demonstrates superior performance capturing intricate details evolving cortical surfaces furthermore model generate highquality realistic samples cortical surfaces conditioned postmenstrual agepma scan,3,0.7756637380528129,3,0.7756637380528129
paired diffusion generation related synthetic petctsegmentation scans using linked denoising diffusion probabilistic models rapid advancement artificial intelligence ai biomedical imaging radiotherapy hindered limited availability large imaging data repositories recent research improvements denoising diffusion probabilistic models ddpm high quality synthetic medical scans possible despite currently way generating multiple related images corresponding ground truth used train models synthetic scans often manually annotated use research introduces novel architecture able generate multiple related petcttumour mask pairs using paired networks conditional encoders approach includes innovative time stepcontrolled mechanisms noiseseeding strategy improve ddpm sampling consistency model requires modified perceptual loss function ensure accurate feature alignment show generation clearly aligned synthetic images improvement segmentation accuracy generated images,3,0.644926792316581,3,0.644926792316581
diffusion sound propagation physicsinspired model ultrasound image generation deep learning dl methods typically require large datasets effectively learn data distributions however medical field data often limited quantity acquiring labeled data costly mitigate data scarcity data augmentation techniques commonly employed among techniques generative models play pivotal role expanding datasets however comes ultrasound us imaging authenticity generated data often diminishes due oversight ultrasound physics propose novel approach improve quality generated us images introducing physicsbased diffusion model specifically designed image modality proposed model incorporates usspecific scheduler scheme mimics natural behavior sound wave propagation ultrasound imaging analysis demonstrates proposed method aids modeling attenuation dynamics us imaging present qualitative quantitative results based standard generative model metrics showing proposed method results overall plausible images code available httpsgithubcommarinadominguezdiffusionforusimages,-1,0.0,-1,0.0
smgdiff soccer motion generation using diffusion probabilistic models soccer globally renowned sport significant applications video games vrar however generating realistic soccer motions remains challenging due intricate interactions human player ball paper introduce smgdiff novel twostage framework generating realtime usercontrollable soccer motions key idea integrate realtime character control powerful diffusionbased generative model ensuring highquality diverse output motion first stage instantly transform coarse user controls diverse global trajectories character second stage employ transformerbased autoregressive diffusion model generate soccer motions based trajectory conditioning incorporate contact guidance module inference optimize contact details realistic ballfoot interactions moreover contribute largescale soccer motion dataset consisting million frames diverse soccer motions extensive experiments demonstrate smgdiff significantly outperforms existing methods terms motion quality condition alignment,-1,0.0,-1,0.0
pixelwise recognition holistic surgical scene understanding paper presents holistic multigranular surgical scene understanding prostatectomies grasp dataset curated benchmark models surgical scene understanding hierarchy complementary tasks varying levels granularity approach encompasses longterm tasks surgical phase step recognition shortterm tasks including surgical instrument segmentation atomic visual actions detection exploit proposed benchmark introduce transformers actions phases steps instrument segmentation tapis model general architecture combines global video feature extractor localized region proposals instrument segmentation model tackle multigranularity benchmark extensive experimentation alternative benchmarks demonstrate tapiss versatility stateoftheart performance across different tasks work represents foundational step forward endoscopic vision offering novel framework future research towards holistic surgical scene understanding,7,0.8382877337344382,7,0.8382877337344382
sgiformer semanticguided geometricenhanced interleaving transformer instance segmentation recent years transformerbased models exhibited considerable potential point cloud instance segmentation despite promising performance achieved existing methods encounter challenges instance query initialization problems excessive reliance stacked layers rendering incompatible largescale scenes paper introduces novel method named sgiformer instance segmentation composed semanticguided mix query smq initialization geometricenhanced interleaving transformer git decoder specifically principle smq initialization scheme leverage predicted voxelwise semantic information implicitly generate sceneaware query yielding adequate scene prior compensating learnable query set subsequently feed formed overall query git decoder alternately refine instance query global scene features capturing finegrained information reducing complex design intricacies simultaneously emphasize geometric property consider bias estimation auxiliary task progressively integrate shifted point coordinates embedding reinforce instance localization sgiformer attains stateoftheart performance scannet datasets challenging highfidelity scannet benchmark striking balance accuracy efficiency code weights demo videos publicly available httpsrayyohgithubiosgiformer,-1,0.0,-1,0.0
iterative approach reconstructing neural disparity fields lightfield data study proposes neural disparity field ndf establishes implicit continuous representation scene disparity based neural field iterative approach address inverse problem ndf reconstruction lightfield data ndf enables seamless precise characterization disparity variations threedimensional scenes discretize disparity arbitrary resolution overcoming limitations traditional disparity maps prone sampling errors interpolation inaccuracies proposed ndf network architecture utilizes hash encoding combined multilayer perceptrons capture detailed disparities texture levels thereby enhancing ability represent geometric information complex scenes leveraging spatialangular consistency inherent lightfield data differentiable forward model generate central view image lightfield data developed based forward model optimization scheme inverse problem ndf reconstruction using differentiable propagation operators established furthermore iterative solution method adopted reconstruct ndf optimization scheme require training datasets applies lightfield data captured various acquisition methods experimental results demonstrate highquality ndf reconstructed lightfield data using proposed method highresolution disparity effectively recovered ndf demonstrating capability implicit continuous representation scene disparities,1,0.9068399472469079,1,0.9068399472469079
dataset benchmark hand motion generation piano performance recently artificial intelligence techniques education received increasing attentions still remains open problem design effective music instrument instructing systems although key presses directly derived sheet music transitional movements among key presses require extensive guidance piano performance work construct pianohand motion generation benchmark guide hand movements fingerings piano playing end collect annotated dataset consisting hours piano playing videos birdseye view million annotated hand poses also introduce powerful baseline model generates hand motions piano audios position predictor positionguided gesture generator furthermore series evaluation metrics designed assess performance baseline model including motion similarity smoothness positional accuracy left right hands overall fidelity movement distribution despite piano key presses respect music scores audios already accessible aims provide guidance piano fingering instruction purposes source code dataset accessed,8,0.42131365473221294,8,0.42131365473221294
timestep embedding tells time cache video diffusion model fundamental backbone video generation diffusion models challenged low inference speed due sequential nature denoising previous methods speed models caching reusing model outputs uniformly selected timesteps however strategy neglects fact differences among model outputs uniform across timesteps hinders selecting appropriate model outputs cache leading poor balance inference efficiency visual quality study introduce timestep embedding aware cache teacache trainingfree caching approach estimates leverages fluctuating differences among model outputs across timesteps rather directly using timeconsuming model outputs teacache focuses model inputs strong correlation modeloutputs incurring negligible computational cost teacache first modulates noisy inputs using timestep embeddings ensure differences better approximating model outputs teacache introduces rescaling strategy refine estimated differences utilizes indicate output caching experiments show teacache achieves acceleration opensoraplan negligible vbench score degradation visual quality,20,1.0,20,1.0
rdeic accelerating diffusionbased extreme image compression relay residual diffusion diffusionbased extreme image compression methods achieved impressive performance extremely low bitrates however constrained iterative denoising process starts pure noise methods limited fidelity efficiency address two issues present relay residual diffusion extreme image compression rdeic leverages compressed feature initialization residual diffusion specifically first use compressed latent features image added noise instead pure noise starting point eliminate unnecessary initial stages denoising process second directly derive novel residual diffusion equation stable diffusions original diffusion equation reconstructs raw image iteratively removing added noise residual compressed target latent features way effectively combine efficiency residual diffusion powerful generative capability stable diffusion third propose fixedstep finetuning strategy eliminate discrepancy training inference phases thereby improving reconstruction quality extensive experiments demonstrate proposed rdeic achieves stateoftheart visual quality outperforms existing diffusionbased extreme image compression methods fidelity efficiency source code provided httpsgithubcomhuaichangrdeic,-1,0.0,-1,0.0
ddmi domainagnostic latent diffusion models synthesizing highquality implicit neural representations recent studies introduced new class generative models synthesizing implicit neural representations inrs capture arbitrary continuous signals various domains models opened door domainagnostic generative models often fail achieve highquality generation observed existing methods generate weights neural networks parameterize inrs evaluate network fixed positional embeddings pes arguably architecture limits expressive power generative models results lowquality inr generation address limitation propose domainagnostic latent diffusion model inrs ddmi generates adaptive positional embeddings instead neural networks weights specifically develop discretetocontinuous space variational autoencoder seamlessly connects discrete data continuous signal functions shared latent space additionally introduce novel conditioning mechanism evaluating inrs hierarchically decomposed pes enhance expressive power extensive experiments across four modalities eg images shapes neural radiance fields videos seven benchmark datasets demonstrate versatility ddmi superior performance compared existing inr generative models,13,0.9149390547725647,13,0.9149390547725647
lediff latent exposure diffusion hdr generation consumer displays increasingly support stops dynamic range image assets internet photographs generative ai content remain limited low dynamic range ldr constraining utility across high dynamic range hdr applications currently generative model produce highbit highdynamic range content generalizable way existing ldrtohdr conversion methods often struggle produce photorealistic details physicallyplausible dynamic range clipped areas introduce lediff method enables generative model hdr content generation latent space fusion inspired imagespace exposure fusion techniques also functions ldrtohdr converter expanding dynamic range existing lowdynamic range images approach uses small hdr dataset enable pretrained diffusion model recover detail dynamic range clipped highlights shadows lediff brings hdr capabilities existing generative models converts ldr image hdr creating photorealistic hdr outputs image generation imagebased lighting hdr environment map generation photographic effects depth field simulation linear hdr data essential realistic quality,-1,0.0,-1,0.0
head neck tumor segmentation petct images based diffusion model head neck hn cancers among prevalent types cancer worldwide petct widely used hn cancer management recently diffusion model demonstrated remarkable performance various imagegeneration tasks work proposed diffusion model accurately perform hn tumor segmentation pet ct volumes diffusion model developed considering nature pet ct images acquired reverse process model utilized unet structure took concatenation pet ct gaussian noise volumes network input generate tumor mask experiments based hecktor challenge dataset conducted evaluate effectiveness proposed diffusion model several stateoftheart techniques based unet transformer structures adopted reference methods benefits employing pet ct network input well extending diffusion model investigated based various quantitative metrics uncertainty maps generated results showed proposed diffusion model could generate accurate segmentation results compared methods compared diffusion model format proposed model yielded superior results experiments also highlighted advantage utilizing dualmodality pet ct data singlemodality data hn tumor segmentation,3,0.6754972750739129,3,0.6754972750739129
latent diffusion implicit amplification efficient continuousscale superresolution remote sensing images recent advancements diffusion models significantly improved performance superresolution sr tasks however previous research often overlooks fundamental differences sr general image generation general image generation involves creating images scratch sr focuses specifically enhancing existing lowresolution lr images adding typically missing highfrequency details oversight increases training difficulty also limits inference efficiency furthermore previous diffusionbased sr methods typically trained inferred fixed integer scale factors lacking flexibility meet needs upsampling noninteger scale factors address issues paper proposes efficient elastic diffusionbased sr model specially designed continuousscale sr remote sensing imagery employs twostage latent diffusion paradigm first stage autoencoder trained capture differential priors highresolution hr lr images encoder intentionally ignores existing lr content alleviate encoding burden decoder introduces sr branch equipped continuous scale upsampling module accomplish reconstruction guidance differential prior second stage conditional diffusion model learned within latent space predict true differential prior encoding experimental results demonstrate achieves superior objective metrics visual quality compared stateoftheart sr methods additionally reduces inference time diffusionbased sr methods level comparable nondiffusion methods,-1,0.0,-1,0.0
neural radiance fieldsbased holography invited study presents novel approach generating holograms based neural radiance fields nerf technique generating threedimensional data difficult hologram computation nerf stateoftheart technique lightfield reconstruction images based volume rendering nerf rapidly predict newview images include training dataset study constructed rendering pipeline directly light field generated images nerf hologram generation using deep neural networks within reasonable time pipeline comprises three main components nerf depth predictor hologram generator constructed using deep neural networks pipeline include physical calculations predicted holograms scene viewed direction computed using proposed pipeline simulation experimental results presented,-1,0.0,-1,0.0
atombot embodied fulfillment unspoken human needs affective theory mind propose atombot novel task generation execution framework proactive robothuman interaction leverages human mental physical state inference capabilities vision language model vlm prompted affective theory mind atom without requiring explicit commands humans atombot proactively generates follows feasible tasks improve general human wellbeing around humans atombot first detects current human needs based inferred human states observations surrounding environment generates tasks fulfill needs taking account embodied constraints designed daily life scenarios spanning common scenes tasked visual stimulus human subjects robot used similarity human openended answers robot output human satisfaction scores metric robot performance atombot received high human evaluations need detection embodied solution task execution show atombot excels generating executing feasible plans fulfill unspoken human needs videos code available httpsaffectivetombotgithubio,5,0.32362207037426394,5,0.32362207037426394
solutions deepfakes camera hardware cryptography deep learning verify real images exponential progress generative ai poses serious implications credibility real images videos exist point future digital content produced generative ai indistinguishable created cameras highquality generative algorithms accessible anyone ratio synthetic real images large imperative establish methods separate real data synthetic data high confidence define real images produced camera hardware capturing realworld scene synthetic generation image alteration real image generative ai computer graphics techniques labeled synthetic image end document aims present known strategies detection cryptography employed verify images real weight strengths weaknesses strategies suggest additional improvements alleviate shortcomings,4,0.682052490014207,4,0.682052490014207
diffusion multidomain adaptation methods eosinophil segmentation eosinophilic esophagitis eoe represents challenging condition medical providers today cause currently unknown impact patients daily life significant increasing prevalence traditional approaches medical image diagnosis standard deep learning algorithms limited relatively small amount data difficulty generalization response two methods arisen seem perform well diffusion multidomain methods current research efforts favoring diffusion methods eoe dataset discovered multidomain adversarial network outperformed diffusion based method fid compared future work diffusion methods include comparison multidomain adaptation methods ensure best performance achieved,3,0.513676080537326,3,0.513676080537326
fddm frequencydecomposed diffusion model rectum cancer dose prediction radiotherapy accurate dose distribution prediction crucial radiotherapy planning although previous methods based convolutional neural network shown promising performance problem oversmoothing leading prediction without important highfrequency details recently diffusion model achieved great success computer vision excels generating images highfrequency details yet suffers timeconsuming extensive computational resource consumption alleviate problems propose frequencydecomposed diffusion model fddm refines highfrequency subbands dose map specific design coarse dose prediction module cdpm first predict coarse dose map utilize discrete wavelet transform decompose coarse dose map lowfrequency subband three highfrequency subbands notable difference coarse predicted results ground truth highfrequency subbands therefore design diffusionbased module called highfrequency refinement module hfrm performs diffusion operation highfrequency components dose map instead original dose map extensive experiments inhouse dataset verify effectiveness approach,-1,0.0,-1,0.0
dynamic point cloud sequences videos dynamic point cloud sequences serve one common practical representation modalities dynamic realworld environments however unstructured nature spatial temporal domains poses significant challenges effective efficient processing existing deep point cloud sequence modeling approaches imitate mature video learning mechanisms developing complex spatiotemporal point neighbor grouping feature aggregation schemes often resulting methods lacking effectiveness efficiency expressive power paper propose novel generic representation called textitstructured point cloud videos spcvs intuitively leveraging fact geometric shapes essentially manifolds spcv reorganizes point cloud sequence video spatial smoothness temporal consistency pixel values correspond coordinates points structured nature spcv representation allows seamless adaptation wellestablished imagevideo techniques enabling efficient effective processing analysis point cloud sequences achieve reorganization design selfsupervised learning pipeline geometrically regularized driven selfreconstructive deformation field learning objectives additionally construct spcvbased frameworks lowlevel highlevel point cloud sequence processing analysis tasks including action recognition temporal interpolation compression extensive experiments demonstrate versatility superiority proposed spcv potential offer new possibilities deep learning unstructured point cloud sequences code released httpsgithubcomzengyimingeamonspcv,-1,0.0,-1,0.0
spatial cognition egocentric video sight mind humans move around performing daily tasks able recall positioned objects environment even objects currently sight paper aim mimic spatial cognition ability thus formulate task sight mind tracking active objects using observations captured egocentric camera introduce simple effective approach address challenging problem called lift match keep lmk lmk lifts partial observations world coordinates matches time using visual appearance location interactions form object tracks keeps object tracks even go outofview camera benchmark lmk long videos epickitchens results demonstrate spatial cognition critical correctly locating objects short long time scales eg one long egocentric video estimate location active objects seconds objects correctly localised lmk compared recent method egocentric videos general tracking method,-1,0.0,-1,0.0
turbulence strength estimation video using physicsbased deep learning images captured long distance suffer dynamic image distortion due turbulent flow air cells random temperatures thus refractive indices phenomenon known image dancing commonly characterized refractiveindex structure constant measure turbulence strength many applications atmospheric forecast model longrangeastronomy imaging aviation safety optical communication technology estimation critical accurately sensing turbulent environment previous methods estimation include estimation meteorological data temperature relative humidity wind shear etc singlepoint measurements twoended pathlength measurements optical scintillometer pathaveraged recently estimating passive video cameras low cost hardware complexity paper present comparative analysis classical image gradient methods estimation modern deep learningbased methods leveraging convolutional neural networks enable collect dataset video capture along reference scintillometer measurements ground truth release unique dataset scientific community observe deep learning methods achieve higher accuracy trained similar data suffer generalization errors unseen imagery compared classical methods overcome tradeoff present novel physicsbased network architecture combines learned convolutional layers differentiable image gradient method maintains high accuracy generalizable across image datasets,14,1.0,14,1.0
echofm foundation model generalizable echocardiogram analysis foundation models recently gained significant attention generalizability adaptability across multiple tasks data distributions although medical foundation models emerged solutions cardiac imaging especially echocardiography videos still unexplored paper introduce echofm foundation model specifically designed represent analyze echocardiography videos echofm propose selfsupervised learning framework captures spatial temporal variability patterns spatiotemporal consistent masking strategy periodicdriven contrastive learning framework effectively capture spatiotemporal dynamics echocardiography learn representative video features without labels pretrain model extensive dataset comprising echocardiography videos covering scan views across different imaging modes million frames images pretrained echofm easily adapted finetuned variety downstream tasks serving robust backbone model evaluation systemically designed four downstream tasks echocardiography examination routine experiment results show echofm surpasses stateoftheart methods including specialized echocardiography methods selfsupervised pretraining models generalpurposed pretrained foundation models across downstream tasks,-1,0.0,-1,0.0
exploiting vlm localizability semantics open vocabulary action detection action detection aims detect recognize localize human actions spatially temporally videos existing approaches focus closedset setting action detector trained tested videos fixed set action categories however constrained setting viable open world test videos inevitably come beyond trained action categories paper address practical yet challenging openvocabulary action detection ovad problem aims detect action test videos training model fixed set action categories achieve openvocabulary capability propose novel method openmixer exploits inherent semantics localizability large visionlanguage models vlm within family querybased detection transformers detr specifically openmixer developed spatial temporal openmixer blocks somb tomb dynamically fused alignment dfa module three components collectively enjoy merits strong generalization pretrained vlms endtoend learning detr design moreover established ovad benchmarks various settings experimental results show openmixer performs best baselines detecting seen unseen actions release codes models dataset splits,7,1.0,7,1.0
compactflownet efficient realtime optical flow estimation mobile devices present compactflownet first realtime mobile neural network optical flow prediction involves determining displacement pixel initial frame relative corresponding pixel subsequent frame optical flow serves fundamental building block various videorelated tasks video restoration motion estimation video stabilization object tracking action recognition video generation current stateoftheart methods prioritize accuracy often overlook constraints regarding speed memory usage existing light models typically focus reducing size still exhibit high latency compromise significantly quality optimized highperformance gpus resulting suboptimal performance mobile devices study aims develop mobileoptimized optical flow model proposing novel mobile devicecompatible architecture well enhancements training pipeline optimize model reduced weight low memory utilization increased speed maintaining minimal error approach demonstrates superior comparable performance stateoftheart lightweight models challenging kitti sintel benchmarks furthermore attains significantly accelerated inference speed thereby yielding realtime operational efficiency iphone surpassing realtime performance levels advanced mobile devices,2,1.0,2,1.0
sportsqa largescale video question answering benchmark complex professional sports reasoning sports videos question answering important task numerous applications player training information retrieval however task explored due lack relevant datasets challenging nature presents datasets video question answering videoqa focus mainly general coarsegrained understanding dailylife videos applicable sports scenarios requiring professional action understanding finegrained motion analysis paper introduce first dataset named sportsqa specifically designed sports videoqa task sportsqa dataset includes various types questions descriptions chronologies causalities counterfactual conditions covering multiple sports furthermore address characteristics sports videoqa task propose new autofocus transformer aft capable automatically focusing particular scales temporal information question answering conduct extensive experiments sportsqa including baseline studies evaluation different methods results demonstrate aft achieves stateoftheart performance,0,0.866120600633356,0,0.866120600633356
efficient uavs deployment resource allocation uavrelay assisted public safety networks video transmission wireless communication highly depends cellular ground base station gbs failure cellular gbs fully partially natural manmade disasters creates communication gap disasteraffected areas situations public safety communication psc significantly save national infrastructure property lives throughout emergencies psc provide missioncritical communication video transmission services affected area unmanned aerial vehicles uavs flying base stations uavbss particularly suitable psc services flexible mobile easily deployable manuscript considers multiuavassisted psc network observational uav receiving videos affected areas ground users agus transmitting nearby gbs via relay uav objective proposed study maximize average utility video streams generated agus upon reaching gbs achieved optimizing positions observational relay uavs well distribution communication resources bandwidth transmit power satisfying systemdesigned constraints transmission rate rate outage probability transmit power budget available bandwidth end joint uavs placement resource allocation problem mathematically formulated proposed problem poses significant challenge solution considering block coordinate descent successive convex approximation techniques efficient iterative algorithm proposed finally simulation results provided show proposed approach outperforms existing methods,2,1.0,2,1.0
scaling masking new paradigm data sampling image video quality assessment quality assessment images videos emphasizes local details global semantics whereas general data sampling methods eg resizing cropping gridbased fragment fail catch simultaneously address deficiency current approaches adopt multibranch models take input multiresolution data burdens model complexity work instead stacking models elegant data sampling method named sama scaling masking explored compacts local global content regular input size basic idea scale data pyramid first reduce pyramid regular data dimension masking strategy benefiting spatial temporal redundancy images videos processed data maintains multiscale characteristics regular input size thus processed singlebranch model verify sampling method image video quality assessment experiments show sampling method improve performance current singlebranch models significantly achieves competitive performance multibranch models without extra model complexity source code available httpsgithubcomsissuiresama,-1,0.0,-1,0.0
experimental evaluation interactive edgecloud virtual reality gaming wifi using unity render streaming virtual reality vr streaming enables endusers seamlessly immerse interactive virtual environments using even lowend devices however quality vr experience heavily relies wireless fidelity wifi performance since serves last hop network chain study delves intricate interplay wifi vr traffic drawing upon empirical data leveraging wifi simulator work evaluate wifis suitability vr streaming terms quality service qos provides particular employ unity render streaming remotely stream realtime vr gaming content wifi using web realtime communication webrtc considering server physically located networks edge near end user findings demonstrate systems sustained network performance showcasing minimal roundtrip time rtt jitter frames per second fps addition uncover characteristics patterns generated traffic streams unveiling distinctive video transmission approach inherent webrtcbased services systematic packetization video frames vfs transmission discrete batches regular intervals regardless targeted frame rate intervalbased transmission strategy maintains consistent video packet delays across video frame rates leads increased wifi airtime consumption results demonstrate shortening interval batches advantageous enhances wifi efficiency reduces delays delivering complete frames,2,1.0,2,1.0
univs unified universal video segmentation prompts queries despite recent advances unified image segmentation developing unified video segmentation vs model remains challenge mainly generic categoryspecified vs tasks need detect objects track across consecutive frames promptguided vs tasks require reidentifying target visualtext prompts throughout entire video making hard handle different tasks architecture make attempt address issues present novel unified vs architecture namely univs using prompts queries univs averages prompt features target previous frames initial query explicitly decode masks introduces targetwise prompt crossattention layer mask decoder integrate prompt features memory pool taking predicted masks entities previous frames visual prompts univs converts different vs tasks promptguided target segmentation eliminating heuristic interframe matching process framework unifies different vs tasks also naturally achieves universal training testing ensuring robust performance across different scenarios univs shows commendable balance performance universality challenging vs benchmarks covering video instance semantic panoptic object referring segmentation tasks code found urlhttpsgithubcomminghanliunivs,-1,0.0,-1,0.0
diverse dataset youtube video comment stances data programming model public opinion military organizations significantly influences ability recruit talented individuals recruitment efforts increasingly extend digital spaces like social media becomes essential assess stance social media users toward online military content however notable lack data analyzing opinions military recruiting efforts online compounded challenges stance labeling crucial understanding public perceptions despite importance stance analysis successful online military recruitment creating humanannotated indomain stance labels resourceintensive paper address challenges stance labeling scarcity data public opinions online military recruitment introducing releasing diverse dataset dataset comprises comments us armys official youtube channel videos employed stateoftheart weak supervision approach leveraging large language models label stance comment toward respective video us army findings indicate us armys videos began attracting significant number comments stance distribution generally balanced among supportive oppositional neutral comments slight skew towards oppositional versus supportive comments,-1,0.0,-1,0.0
detection object throwing behavior surveillance videos anomalous behavior detection challenging research area within computer vision progress area enables automated detection dangerous behavior using surveillance camera feeds dangerous behavior often overlooked research throwing action traffic flow one unique requirements smart city project enhance public safety paper proposes solution throwing action detection surveillance videos using deep learning present datasets throwing actions publicly available address usecase smart city project first generate novel public throwing action dataset consisting videos throwing actions performed traffic participants pedestrians bicyclists car drivers normal videos without throwing actions second compare performance different feature extractors anomaly detection method ucfcrime throwingaction datasets explored feature extractors convolutional network inflated convnet network multifiber network mfnet finally performance anomaly detection algorithm improved applying adam optimizer instead adadelta proposing mean normal loss function covers multitude normal situations traffic aspects yield better anomaly detection performance besides proposed mean normal loss function lowers false alarm rate combined dataset experimental results reach area roc curve throwingaction dataset combined dataset respectively,-1,0.0,-1,0.0
weaksurg weakly supervised surgical instrument segmentation using temporal equivariance semantic continuity robotic surgical videos instrument presence annotations typically recorded video streams offering potential reduce manually annotated costs segmentation however weakly supervised surgical instrument segmentation instrument presence labels rarely explored surgical domain due highly underconstrained challenges temporal properties enhance representation learning capturing sequential dependencies patterns time even incomplete supervision situations take inherent temporal attributes surgical video account extend twostage weakly supervised segmentation paradigm different perspectives firstly make temporal equivariance constraint enhance pixelwise temporal consistency adjacent features secondly constrain classaware semantic continuity global local regions across temporal dimension finally generate temporalenhanced pseudo masks consecutive frames suppress irrelevant regions extensive experiments validated two surgical video datasets including one cholecystectomy surgery benchmark one real robotic left lateral segment liver surgery dataset annotate instancewise instrument labels fixed timesteps double checked clinician experience evaluate segmentation results experimental results demonstrate promising performances method consistently achieves comparable favorable results previous stateoftheart approaches,7,0.887472843554077,7,0.887472843554077
testtime zeroshot temporal action localization zeroshot temporal action localization zstal seeks identify locate actions untrimmed videos unseen training existing zstal methods involve finetuning model large amount annotated training data effective trainingbased zstal approaches assume availability labeled data supervised learning impractical applications furthermore training process naturally induces domain bias learned model may adversely affect models generalization ability arbitrary videos considerations prompt us approach zstal problem radically novel perspective relaxing requirement training data aim introduce novel method performs testtime adaptation temporal action localization nutshell adapts pretrained vision language model vlm operates three steps first videolevel pseudolabel action category computed aggregating information entire video action localization performed adopting novel procedure inspired selfsupervised learning finally framelevel textual descriptions extracted stateoftheart captioning model employed refining action region proposals validate effectiveness conducting experiments datasets results demonstrate significantly outperforms zeroshot baselines based stateoftheart vlms confirming benefit testtime adaptation approach,7,0.9862345609730642,7,0.9862345609730642
reconstructing handheld objects images videos objects manipulated hand ie manipulanda particularly challenging reconstruct internet videos hand occlude much object also object often visible small number image pixels time two strong anchors emerge setting estimated hands help disambiguate location scale object set manipulanda small relative possible objects insights mind present scalable paradigm handheld object reconstruction builds recent breakthroughs large languagevision models object datasets given monocular rgb video aim reconstruct handheld object geometry time order obtain best performing single frame model first present mcchandobject mccho jointly reconstructs hand object geometry given single rgb image inferred hand inputs subsequently prompt generative model using retrieve object model matches object images call alignment retrievalaugmented reconstruction rar rar provides unified object geometry across frames result rigidly aligned input images mccho observations temporally consistent manner experiments demonstrate approach achieves stateoftheart performance lab internet imagevideo datasets make code models available project website httpsjanehwugithubiomccho,5,0.32586420591407567,5,0.32586420591407567
animationbased augmentation approach action recognition discontinuous video action recognition essential component computer vision plays pivotal role multiple applications despite significant improvements brought convolutional neural networks cnns models suffer performance declines trained discontinuous video frames frequent scenario realworld settings decline primarily results loss temporal continuity crucial understanding semantics human actions overcome issue introduce action animationbased augmentation approach pipeline employs series sophisticated techniques starting human pose estimation rgb videos followed quaternionbased graph convolution network joint orientation trajectory prediction dynamic skeletal interpolation creating smoother diversified actions using game engine technology innovative approach generates realistic animations varied game environments viewed multiple viewpoints way method effectively bridges domain gap virtual realworld data experimental evaluations pipeline achieves comparable even superior performance traditional training approaches using realworld data requiring original data volume additionally approach demonstrates enhanced performance inthewild videos marking significant advancement field action recognition,-1,0.0,-1,0.0
mumpy multilateral temporalview pyramid transformer video inpainting detection task video inpainting detection expose pixellevel inpainted regions within video sequence existing methods usually focus leveraging spatial temporal inconsistencies however methods typically employ fixed operations combine spatial temporal clues limiting applicability different scenarios paper introduce novel multilateral temporalview pyramid transformer em mumpy collaborates spatialtemporal clues flexibly method utilizes newly designed multilateral temporalview encoder extract various collaborations spatialtemporal clues introduces deformable windowbased temporalview interaction module enhance diversity collaborations subsequently develop multipyramid decoder aggregate various types features generate detection maps adjusting contribution strength spatial temporal clues method effectively identify inpainted regions validate method existing datasets also introduce new challenging largescale video inpainting dataset based youtubevos dataset employs several recent inpainting methods results demonstrate superiority method indomain crossdomain evaluation scenarios,-1,0.0,-1,0.0
onthefly point annotation fast medical video labeling purpose medical research deep learning models rely highquality annotated data process often laborious timeconsuming particularly true detection tasks bounding box annotations required need adjust two corners makes process inherently framebyframe given scarcity experts time efficient annotation methods suitable clinicians needed methods propose onthefly method live video annotation enhance annotation efficiency approach continuous singlepoint annotation maintained keeping cursor object live video mitigating need tedious pausing repetitive navigation inherent traditional annotation methods novel annotation paradigm inherits point annotations ability generate pseudolabels using pointtobox teacher model empirically evaluate approach developing dataset comparing onthefly annotation time traditional annotation method results using method annotation speed faster traditional annotation technique achieved mean improvement conventional method equivalent annotation budgets developed dataset conclusion without bells whistles approach offers significant speedup annotation tasks easily implemented annotation platform accelerate integration deep learning videobased medical research,-1,0.0,-1,0.0
skim focus integrating contextual finegrained views repetitive action counting key action counting accurately locating videos repetitive actions instead estimating probability frame belonging action directly propose dualbranch network ie skimfocusnet working twostep manner model draws inspiration empirical observations indicating humans typically engage coarse skimming entire sequences grasp general action pattern initially followed finer framebyframe focus determine aligns target action specifically skimfocusnet incorporates skim branch focus branch skim branch scans global contextual information throughout sequence identify potential target action guidance subsequently focus branch utilizes guidance diligently identify repetitive actions using longshort adaptive guidance lsag block additionally observed videos existing datasets often feature one type repetitive action inadequately represents realworld scenarios accurately describe reallife situations establish multirepcount dataset includes videos containing multiple repetitive motions multirepcount skimfoucsnet perform specified action counting enable counting particular action type referencing exemplary video capability substantially exhibits robustness method extensive experiments demonstrate skimfocusnet achieves stateoftheart performances significant improvements also conduct thorough ablation study evaluate network components source code published upon acceptance,-1,0.0,-1,0.0
unsupervised learning categorylevel pose objectcentric videos categorylevel pose estimation fundamentally important problem computer vision robotics eg embodied agents train generative models however far methods estimate categorylevel object pose require either large amounts human annotations cad models input rgbd sensors contrast tackle problem learning estimate categorylevel pose casually taken objectcentric videos without human supervision propose twostep pipeline first introduce multiview alignment procedure determines canonical camera poses across videos novel robust cyclic distance formulation geometric appearance matching using reconstructed coarse meshes features second step canonical poses reconstructed meshes enable us train model pose estimation single image particular model learns estimate dense correspondences images prototypical template predicting pixel image feature vector corresponding vertex template mesh demonstrate method outperforms baselines unsupervised alignment objectcentric videos large margin provides faithful robust predictions inthewild code data available,1,1.0,1,1.0
benchmark tracking point introduce new benchmark evaluating task longrange tracking point point tracking two dimensions tap many benchmarks measuring performance realworld videos tapviddavis threedimensional point tracking none end leveraging existing footage build new benchmark point tracking featuring realworld videos composed three different data sources spanning variety object types motion patterns indoor outdoor environments measure performance task formulate collection metrics extend jaccardbased metric used tap handle complexities ambiguous depth scales across models occlusions multitrack spatiotemporal smoothness manually verify large sample trajectories ensure correct video annotations assess current state task constructing competitive baselines using existing tracking models anticipate benchmark serve guidepost improve ability understand precise motion surface deformation monocular video code dataset download generation model evaluation available,-1,0.0,-1,0.0
comprehensive review fewshot action recognition fewshot action recognition aims address high cost impracticality manually labeling complex variable video data action recognition requires accurately classifying human actions videos using labeled examples per class compared fewshot learning image scenarios fewshot action recognition challenging due intrinsic complexity video data recognizing actions involves modeling intricate temporal sequences extracting rich semantic information goes beyond mere human object identification frame furthermore issue intraclass variance becomes particularly pronounced limited video samples complicating learning representative features novel action categories overcome challenges numerous approaches driven significant advancements fewshot action recognition underscores need comprehensive survey unlike early surveys focus fewshot image text classification deeply consider unique challenges fewshot action recognition survey review wide variety recent methods summarize general framework additionally survey presents commonly used benchmarks discusses relevant advanced topics promising future directions hope survey serve valuable resource researchers offering essential guidance newcomers stimulating seasoned researchers fresh insights,-1,0.0,-1,0.0
unqa unified noreference quality assessment audio image video audiovisual content multimedia data flourishes internet quality assessment qa multimedia data becomes paramount digital media applications since multimedia data includes multiple modalities including audio image video audiovisual av content researchers developed range qa methods evaluate quality different modality data exclusively focus addressing single modality qa issues unified qa model handle diverse media across multiple modalities still missing whereas latter better resemble human perception behaviour also wider range applications paper propose unified noreference quality assessment model unqa audio image video av content tries train single qa model across different media modalities tackle issue inconsistent quality scales among different qa databases develop multimodality strategy jointly train unqa multiple qa databases based input modality unqa selectively extracts spatial features motion features audio features calculates final quality score via four corresponding modality regression modules compared existing qa methods unqa two advantages multimodality training strategy makes qa model learn general robust qualityaware feature representation evidenced superior performance unqa compared stateoftheart qa methods unqa reduces number models required assess multimedia data across different modalities friendly deploy practical applications,12,0.3751161122066533,12,0.3751161122066533
dynamic compressive adaptation transformers images videos recently remarkable success pretrained vision transformers vits imagetext matching sparked interest imagetovideo adaptation however current approaches retain full forward pass frame leading high computation overhead processing entire videos paper present inti novel approach compressive imagetovideo adaptation using dynamic interframe token interpolation inti aims softly preserve informative tokens without disrupting coherent spatiotemporal structure specifically token pair identical positions within neighbor frames linearly aggregated new token aggregation weights generated multiscale contextaware network way information neighbor frames adaptively compressed pointbypoint manner thereby effectively reducing number processed frames half time importantly inti seamlessly integrated existing adaptation methods achieving strong performance without extracomplex design inti reaches accuracy remarkable reduction gflops compared naive adaptation combined additional temporal modules inti achieves accuracy reduction gflops similar conclusions verified common datasets,-1,0.0,-1,0.0
classification endoscopy video capsule images using cnntransformer model gastrointestinal cancer leading cause cancerrelated incidence death making crucial develop novel computeraided diagnosis systems early detection enhanced treatment traditional approaches rely expertise gastroenterologists identify diseases however process subjective interpretation vary even among expert clinicians considering recent advancements classifying gastrointestinal anomalies landmarks endoscopic video capsule endoscopy images study proposes hybrid model combines advantages transformers convolutional neural networks cnns enhance classification performance model utilizes cnn branch extract local features integrates swin transformer branch global feature understanding combining perform classification task gastrovision dataset proposed model demonstrates excellent performance precision recall score accuracy matthews correlation coefficient mcc respectively showcasing robustness class imbalance surpassing cnns well swin transformer model similarly kvasircapsule large video capsule endoscopy dataset model outperforms others achieving overall precision recall score accuracy mcc moreover generated saliency maps explain models focus areas demonstrating reliable decisionmaking process results underscore potential hybrid cnntransformer model aiding early accurate detection gastrointestinal gi anomalies,19,1.0,19,1.0
cascaded temporal updating network efficient video superresolution existing video superresolution vsr methods generally adopt recurrent propagation network extract spatiotemporal information entire video sequences exhibiting impressive performance however key components recurrentbased vsr networks significantly impact model efficiency eg alignment module occupies substantial portion model parameters bidirectional propagation mechanism significantly amplifies inference time consequently developing compact efficient vsr method deployed resourceconstrained devices eg smartphones remains challenging end propose cascaded temporal updating network ctun efficient vsr first develop implicit cascaded alignment module explore spatiotemporal correspondences adjacent frames moreover propose unidirectional propagation updating network efficiently explore longrange temporal information crucial highquality video reconstruction specifically develop simple yet effective hidden updater leverage future information update hidden features forward propagation significantly reducing inference time maintaining performance finally formulate components endtoend trainable vsr network extensive experimental results show ctun achieves favorable tradeoff efficiency performance compared existing methods notably compared basicvsr method obtains better results employing parameters running time source code pretrained models available httpsgithubcomhouseleoctun,-1,0.0,-1,0.0
learning play video games intuitive physics priors video game playing extremely structured domain algorithmic decisionmaking tested without adverse realworld consequences prevailing methods rely image inputs avoid problem handcrafting state space representations approach systematically diverges way humans actually learn play games paper design objectbased input representations generalize well across number video games using representations evaluate agents ability learn games similar infant limited world experience employing simple inductive biases derived intuitive representations physics real world using biases construct object category representation used qlearning algorithm assess well learns play multiple games based observed object affordances results suggest humanlike object interaction setup capably learns play several video games demonstrates superior generalizability particularly unfamiliar objects exploring methods allow machines learn humancentric way thus incorporating humanlike learning benefits,5,0.5463613926647951,5,0.5463613926647951
xprompt multimodal visual prompt video object segmentation multimodal video object segmentation vos including rgbthermal rgbdepth rgbevent garnered attention due capability address challenging scenarios traditional vos methods struggle extreme illumination rapid motion background distraction existing approaches often involve designing specific additional branches performing fullparameter finetuning fusion task however paradigm duplicates research efforts hardware costs also risks model collapse limited multimodal annotated data paper propose universal framework named xprompt multimodal video object segmentation tasks designated rgbx xprompt framework first pretrains video object segmentation foundation model using rgb data utilize additional modality prompt adapt downstream multimodal tasks limited data within xprompt framework introduce multimodal visual prompter mvp allows prompting foundation model various modalities segment objects precisely propose multimodal adaptation experts maes adapt foundation model pluggable modalityspecific knowledge without compromising generalization capacity evaluate effectiveness xprompt framework conduct extensive experiments tasks across benchmarks proposed universal xprompt framework consistently outperforms full finetuning paradigm achieves stateoftheart performance code httpsgithubcompinxueguoxpromptgit,-1,0.0,-1,0.0
recording dynamic facial microexpressions multifocus camera array present approach utilizing multicamera array system capturing dynamic highresolution videos human face improved imaging performance compared traditional singlecamera configurations employing array individual highresolution cameras megapixel sensor megapixels total uniquely focus camera different plane across curved surface human face order capture dynamic facial expressions postprocessing methods stitch together synchronized set images composite video frame multifocus strategy overcomes resolution depthoffield dof limitations capturing macroscopically curved surfaces human face maintaining high lateral resolution specifically demonstrate setup achieves generally uniform lateral resolution micrometer across composite dof covers entire face fov compared singlefocus configuration almost increase effective dof believe new approach multifocus camera array video sets stage future video capture variety dynamic macroscopically curved surfaces microscopic resolution,1,0.9699944904453465,1,0.9699944904453465
flaash flowattention adaptive semantic hierarchical fusion multimodal tobacco content analysis proliferation tobaccorelated content social media platforms poses significant challenges public health monitoring intervention paper introduces novel multimodal deep learning framework named flowattention adaptive semantic hierarchical fusion flaash designed analyze tobaccorelated video content comprehensively flaash addresses complexities integrating visual textual information shortform videos leveraging hierarchical fusion mechanism inspired flow network theory approach incorporates three key innovations including flowattention mechanism captures nuanced interactions visual textual modalities adaptive weighting scheme balances contribution different hierarchical levels gating mechanism selectively emphasizes relevant features multifaceted approach enables flaash effectively process analyze diverse tobaccorelated content product showcases usage scenarios evaluate flaash multimodal tobacco content analysis dataset mtcad largescale collection tobaccorelated videos popular social media platforms results demonstrate significant improvements existing methods outperforming stateoftheart approaches classification accuracy score temporal consistency proposed method also shows strong generalization capabilities tested standard video questionanswering datasets surpassing current models work contributes intersection public health artificial intelligence offering effective tool analyzing tobacco promotion digital media,-1,0.0,-1,0.0
msegvcuq multimodal segmentation enhanced vision foundation models convolutional neural networks uncertainty quantification highspeed video phase detection data highspeed video hsv phase detection pd segmentation crucial monitoring vapor liquid microlayer phases industrial processes cnnbased models like unet shown success simplified shadowgraphybased twophase flow tpf analysis application complex hsv pd tasks remains unexplored vision foundation models vfms yet address complexities either shadowgraphybased pd tpf video segmentation existing uncertainty quantification uq methods lack pixellevel reliability critical metrics like contact line density dry area fraction absence largescale multimodal experimental datasets tailored pd segmentation impedes progress address gaps propose msegvcuq hybrid framework integrates unet cnns transformerbased segment anything model sam achieve enhanced segmentation accuracy crossmodality generalization approach incorporates systematic uq robust error assessment introduces first opensource multimodal hsv pd datasets empirical results demonstrate msegvcuq outperforms baseline cnns vfms enabling scalable reliable pd segmentation realworld boiling dynamics,-1,0.0,-1,0.0
principles visual tokens efficient video understanding video understanding made huge strides recent years relying largely power transformers architecture notoriously expensive video data highly redundant research improving efficiency become particularly relevant creative solutions include token selection merging methods succeed reducing cost model maintaining accuracy interesting pattern arises methods outperform baseline randomly discarding tokens paper take closer look phenomenon observe principles nature visual tokens example observe value tokens follows clear paretodistribution tokens remarkably low value carry perceptual information build insights propose lightweight video model lite select small number tokens effectively outperforming stateoftheart existing baselines across datasets challenging tradeoff computation gflops vs accuracy experiments also show lite generalizes across datasets even tasks without need retraining,-1,0.0,-1,0.0
selfsupervised video instance segmentation boost geographic entity alignment historical maps tracking geographic entities historical maps buildings offers valuable insights cultural heritage urbanization patterns environmental changes various historical research endeavors however linking entities across diverse maps remains persistent challenge researchers traditionally addressed twostep process detecting entities within individual maps associating via heuristicbased postprocessing step paper propose novel approach combines segmentation association geographic entities historical maps using video instance segmentation vis method significantly streamlines geographic entity alignment enhances automation however acquiring highquality videoformat training data vis models prohibitively expensive especially historical maps often contain hundreds thousands geographic entities mitigate challenge explore selfsupervised learning ssl techniques enhance vis performance historical maps evaluate performance vis models different pretraining configurations introduce novel method generating synthetic videos unlabeled historical map images pretraining proposed selfsupervised vis method substantially reduces need manual annotation experimental results demonstrate superiority proposed selfsupervised vis approach achieving improvement ap increase score compared model trained scratch,-1,0.0,-1,0.0
