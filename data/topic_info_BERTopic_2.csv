Topic,Count,Name,Representation,Representative_Docs
-1,985,-1_video_generation_models_model,"['video', 'generation', 'models', 'model', 'videos', 'diffusion', 'motion', 'image', 'data', 'video generation']","['sectorshaped diffusion models video generation diffusion models achieved great success image generation however leveraging idea video generation face significant challenges maintaining consistency continuity across video frames mainly caused lack effective framework align frames videos desired temporal features preserving consistent semantic stochastic features work propose novel sectorshaped diffusion model whose sectorshaped diffusion region formed set rayshaped reverse diffusion processes starting noise point generate group intrinsically related data sharing semantic stochastic features varying temporal features appropriate guided conditions apply video generation tasks explore use optical flow temporal conditions experimental results show outperforms many existing methods task video generation without temporalfeature modelling modules texttovideo generation tasks temporal conditions explicitly given propose twostage generation strategy decouple generation temporal features semanticcontent features show without additional training model integrated another temporal conditions generative model still achieve comparable performance existing works results viewd', 'sfv single forward video generation model diffusionbased video generation models demonstrated remarkable success obtaining highfidelity videos iterative denoising process however models require multiple denoising steps sampling resulting high computational costs work propose novel approach obtain singlestep video generation models leveraging adversarial training finetune pretrained video diffusion models show adversarial training multisteps video diffusion model ie stable video diffusion svd trained perform single forward pass synthesize highquality videos capturing temporal spatial dependencies video data extensive experiments demonstrate method achieves competitive generation quality synthesized videos significantly reduced computational overhead denoising process ie around speedup compared svd speedup compared existing works even better generation quality paving way realtime video synthesis editing visualization results made publicly available httpssnapresearchgithubiosfv', 'zerosmooth trainingfree diffuser adaptation high frame rate video generation video generation made remarkable progress recent years especially since advent video diffusion models many video generation models produce plausible synthetic videos eg stable video diffusion svd however video models generate low frame rate videos due limited gpu memory well difficulty modeling large set frames training videos always uniformly sampled specified interval temporal compression previous methods promote frame rate either training video interpolation model pixel space postprocessing stage training interpolation model latent space specific base video model paper propose trainingfree video interpolation method generative video diffusion models generalizable different models plugandplay manner investigate nonlinearity feature space video diffusion models transform video model selfcascaded video diffusion model incorporating designed hidden state correction modules selfcascaded architecture correction module proposed retain temporal consistency key frames interpolated frames extensive evaluations preformed multiple popular video models demonstrate effectiveness propose method especially trainingfree method even comparable trained interpolation models supported huge compute resources largescale datasets']"
0,338,0_video_models_videos_understanding,"['video', 'models', 'videos', 'understanding', 'model', 'temporal', 'visual', 'language', 'large', 'multimodal']","['needle video haystack scalable synthetic evaluator video mllms video understanding crucial next step multimodal large language models mllms various benchmarks introduced better evaluating mllms nevertheless current video benchmarks still inefficient evaluating video models iterative development due high cost constructing datasets difficulty isolating specific skills paper propose videoniah video needle haystack benchmark construction framework synthetic video generation videoniah decouples video content queryresponses inserting unrelated visual needles original videos framework automates generation queryresponse pairs using predefined rules minimizing manual labor queries focus specific aspects video understanding enabling skillspecific evaluations separation video content queries also allow increased video variety evaluations across different lengths utilizing videoniah compile video benchmark vnbench includes tasks retrieval ordering counting evaluate three key aspects video understanding temporal perception chronological ordering spatiotemporal coherence conduct comprehensive evaluation proprietary opensource models uncovering significant differences video understanding capabilities across various tasks additionally perform indepth analysis test results model configurations based findings provide advice improving video mllm training offering valuable insights guide future research model development code data available', 'longvlm efficient long video understanding via large language models empowered large language models llms recent advancements videobased llms videollms driven progress various video understanding tasks models encode video representations pooling query aggregation vast number visual tokens making computational memory costs affordable despite successfully providing overall comprehension video content existing videollms still face challenges achieving detailed understanding due overlooking local information longterm videos tackle challenge introduce longvlm simple yet powerful videollm long video understanding building upon observation long videos often consist sequential key events complex actions camera movements approach proposes decompose long videos multiple shortterm segments encode local features segment via hierarchical token merging module features concatenated temporal order maintain storyline across sequential shortterm segments additionally propose integrate global semantics local feature enhance context understanding way encode video representations incorporate local global information enabling llm generate comprehensive responses longterm videos experimental results videochatgpt benchmark zeroshot video questionanswering datasets demonstrate superior capabilities model previous stateoftheart methods qualitative examples show model produces precise responses long video understanding code available httpsgithubcomziplablongvlm', 'topa extending large language models video understanding via textonly prealignment recent advancements image understanding benefited extensive use web imagetext pairs however video understanding remains challenge despite availability substantial web videotext data difficulty primarily arises inherent complexity videos inefficient language supervision recent webcollected videotext datasets paper introduce textonly prealignment topa novel approach extend large language models llms video understanding without need pretraining real video data specifically first employ advanced llm automatically generate textual videos comprising continuous textual frames along corresponding annotations simulate real videotext data annotated textual videos used prealign languageonly llm video modality bridge gap textual real videos employ clip model feature extractor align image text modalities textonly prealignment continuous textual frames encoded sequence clip text features analogous continuous clip image features thus aligning llm real video representation extensive experiments including zeroshot evaluation finetuning various video understanding tasks demonstrate topa effective efficient framework aligning video content llms particular without training video data model achieves accuracy challenging longform video understanding benchmark egoschema performance surpasses previous videotext pretraining approaches proves competitive recent video agents']"
1,295,1_video_motion_generation_diffusion,"['video', 'motion', 'generation', 'diffusion', 'videos', 'models', 'model', 'scene', 'novel', 'methods']","['dynamic content generation multiframe multiview consistency present stable video latent video diffusion model multiframe multiview consistent dynamic content generation unlike previous methods rely separately trained generative models video generation novel view synthesis design unified diffusion model generate novel view videos dynamic objects specifically given monocular reference video generates novel views video frame temporally consistent use generated novel view videos optimize implicit representation dynamic nerf efficiently without need cumbersome sdsbased optimization used prior works train unified novel view video generation model curate dynamic object dataset existing objaverse dataset extensive experimental results multiple datasets user studies demonstrate stateoftheart performance novelview video synthesis well generation compared prior works', 'sparse input view synthesis representations reliable priors novel view synthesis refers problem synthesizing novel viewpoints scene given images viewpoints fundamental problem computer vision graphics enables vast variety applications metaverse freeview watching events video gaming video stabilization video compression recent representations radiance fields multiplane images significantly improve quality images rendered novel viewpoints however models require dense sampling input views high quality renders performance goes significantly input views available thesis focus sparse input novel view synthesis problem static dynamic scenes first part work mainly focus sparse input novel view synthesis static scenes using neural radiance fields nerf study design reliable dense priors better regularize nerf situations particular propose prior visibility pixels pair input views show visibility prior related relative depth objects dense reliable existing priors absolute depth compute visibility prior using plane sweep volumes without need train neural network large datasets evaluate approach multiple datasets show model outperforms existing approaches sparse input novel view synthesis second part aim improve regularization learning scenespecific prior suffer generalization issues achieve learning prior given scene alone without pretraining large datasets particular design augmented nerfs obtain better depth supervision certain regions scene main nerf extend framework also apply newer faster radiance field models tensorf zipnerf extensive experiments multiple datasets show superiority approach sparse input novel view synthesis design sparse input fast dynamic radiance fields severely constrained lack suitable representations reliable priors motion address first challenge designing explicit motion model based factorized volumes compact optimizes quickly also introduce reliable sparse flow priors constrain motion field since find popularly employed dense optical flow priors unreliable show benefits motion representation reliable priors multiple datasets final part thesis study application view synthesis frame rate upsampling video gaming specifically consider problem temporal view synthesis goal predict future frames given past frames camera motion key challenge predicting future motion objects estimating past motion extrapolating explore use multiplane image representations scene depth reliably estimate object motion particularly occluded regions design new database effectively evaluate approach temporal view synthesis dynamic scenes show achieve stateoftheart performance', 'motionmaster trainingfree camera motion transfer video generation emergence diffusion models greatly propelled progress image video generation recently efforts made controllable video generation including texttovideo generation video motion control among camera motion control important topic however existing camera motion control methods rely training temporal camera module necessitate substantial computation resources due large amount parameters video generation models moreover existing methods predefine camera motion types training limits flexibility camera control therefore reduce training costs achieve flexible camera control propose comd novel trainingfree video motion transfer model disentangles camera motions object motions source videos transfers extracted camera motions new videos first propose oneshot camera motion disentanglement method extract camera motion single source video separates moving objects background estimates camera motion moving objects region based motion background solving poisson equation furthermore propose fewshot camera motion disentanglement method extract common camera motion multiple videos similar camera motions employs windowbased clustering technique extract common features temporal attention maps multiple videos finally propose motion combination method combine different types camera motions together enabling model controllable flexible camera control extensive experiments demonstrate trainingfree approach effectively decouple cameraobject motion apply decoupled camera motion wide range controllable video generation tasks achieving flexible diverse camera motion control']"
2,195,2_diffusion_image_models_images,"['diffusion', 'image', 'models', 'images', 'data', 'generative', 'model', 'medical', 'diffusion models', 'synthesis']","['domain translation framework adversarial denoising diffusion model generate synthetic datasets echocardiography images currently medical image domain translation operations show high demand researchers clinicians amongst capabilities task allows generation new medical images sufficiently high image quality making clinically relevant deep learning dl architectures specifically deep generative models widely used generate translate images one domain another proposed framework relies adversarial denoising diffusion model ddm synthesize echocardiography images perform domain translation contrary generative adversarial networks gans ddms able generate high quality image samples large diversity ddm combined gan ability generate new data completed even faster sampling time work trained adversarial ddm combined gan learn reverse denoising process relying guide image making sure relevant anatomical structures echocardiography image kept represented generated image samples several domain translation operations results verified generative model able synthesize high quality image samples mse psnr db ssim proposed method showed high generalization ability introducing framework create echocardiography images suitable used clinical research purposes', 'crossconditioned diffusion model medical image image translation multimodal magnetic resonance imaging mri provides rich complementary information analyzing diseases however practical challenges acquiring multiple mri modalities cost scan time safety considerations often result incomplete datasets affects quality diagnosis performance deep learning models trained data recent advancements generative adversarial networks gans denoising diffusion models shown promise natural medical imagetoimage translation tasks however complexity training gans computational expense associated diffusion models hinder development application task address issues introduce crossconditioned diffusion model cdm medical imagetoimage translation core idea cdm use distribution target modalities guidance improve synthesis quality achieving higher generation efficiency compared conventional diffusion models first propose modalityspecific representation model mrm model distribution target modalities design modalitydecoupled diffusion network mdn efficiently effectively learn distribution mrm finally crossconditioned unet cunet condition embedding module designed synthesize target modalities source modalities input target distribution guidance extensive experiments conducted upenngbm benchmark datasets demonstrate superiority method', 'wdm wavelet diffusion models highresolution medical image synthesis due threedimensional nature ct mrscans generative modeling medical images particularly challenging task existing approaches mostly apply patchwise slicewise cascaded generation techniques fit highdimensional data limited gpu memory however approaches may introduce artifacts potentially restrict models applicability certain downstream tasks work presents wdm waveletbased medical image synthesis framework applies diffusion model wavelet decomposed images presented approach simple yet effective way scaling diffusion models high resolutions trained single gpu experimental results brats lidcidri unconditional image generation resolution times times demonstrate stateoftheart image fidelity fid sample diversity msssim scores compared recent gans diffusion models latent diffusion models proposed method one capable generating highquality images resolution times times outperforming comparing methods']"
3,71,3_video_compression_coding_video compression,"['video', 'compression', 'coding', 'video compression', 'neural', 'streaming', 'quality', 'model', 'performance', 'network']","['unified framework intra interframe video compression video compression aims reconstruct seamless frames encoding motion residual information existing frames previous neural video compression methods necessitate distinct codecs three types frames iframe pframe bframe hinders unified approach generalization across different video contexts intracodec techniques lack advanced motion estimation motion compensation memc found intercodec leading fragmented frameworks lacking uniformity proposed intra interframe video compression framework employs single spatiotemporal codec guides feature compression rates according content importance unified codec transforms dependence across frames conditional coding scheme thus integrating intra interframe compression one cohesive strategy given absence explicit motion data achieving competent interframe compression conditional codec poses challenge resolve approach includes implicit interframe alignment mechanism pretrained diffusion denoising process utilization diffusioninverted reference feature rather random noise supports initial compression state process allows selective denoising motionrich regions based decoded features facilitating accurate alignment without need memc experimental findings across various compression configurations ai ld ra frame types prove outperforms stateoftheart perceptual learned codecs impressively exhibits enhancement perceptual reconstruction performance benchmarked standard vtm official implementation found', 'parameterefficient instanceadaptive neural video compression learningbased neural video codecs nvcs emerged compelling alternative standard video codecs demonstrating promising performance simple easily maintainable pipelines however nvcs often fall short compression performance occasionally exhibit poor generalization capability due inferenceonly compression scheme dependence training data instanceadaptive video compression techniques recently suggested viable solution finetuning encoder decoder networks particular test instance video however finetuning model parameters incurs high computational costs increases bitrates often leads unstable training work propose parameterefficient instanceadaptive video compression framework inspired remarkable success parameterefficient finetuning largescale neural network models propose use lightweight adapter module easily attached pretrained nvcs finetuned test video sequences resulting algorithm significantly improves compression performance reduces encoding time compared existing instantadaptive video compression algorithms furthermore suggested finetuning method enhances robustness training process allowing proposed method widely used many practical settings conducted extensive experiments various standard benchmark datasets including uvg mcljvc hevc sequences experimental results shown significant improvement ratedistortion rd curves db psnr bd rates compared baselines nvc code available httpsgithubcomohsngjunpevc', 'accelerating learned video compression via lowresolution representation learning recent years field learned video compression witnessed rapid advancement exemplified latest neural video codecs dcvcdc outperformed upcoming nextgeneration codec ecm terms compression ratio despite learned video compression frameworks often exhibit low encoding decoding speeds primarily due increased computational complexity unnecessary highresolution spatial operations hugely hinder applications reality work introduce efficiencyoptimized framework learned video compression focuses lowresolution representation learning aiming significantly enhance encoding decoding speeds firstly diminish computational load reducing resolution interframe propagated features obtained reused features decoded frames including iframes implement joint training strategy iframe pframe models improving compression ratio secondly approach efficiently leverages multiframe priors parameter prediction minimizing computation decoding end thirdly revisit application online encoder update oeu strategy highresolution sequences achieving notable improvements compression ratio without compromising decoding efficiency efficiencyoptimized framework significantly improved balance compression ratio speed learned video compression comparison traditional codecs method achieves performance levels par lowdecay p configuration reference software vtm furthermore contrasted dcvchem approach delivers comparable compression ratio boosting encoding decoding speeds factor respectively rtx method decode frame']"
4,69,4_robot_manipulation_learning_policy,"['robot', 'manipulation', 'learning', 'policy', 'tasks', 'human', 'videos', 'video', 'data', 'world']","['flow crossdomain manipulation interface present scalable learning framework enables robots acquire realworld manipulation skills without need realworld robot training data key idea behind use object flow manipulation interface bridging domain gaps different embodiments ie human robot training environments ie realworld simulated comprises two components flow generation network flowconditioned policy flow generation network trained human demonstration videos generates object flow initial scene image conditioned task description flowconditioned policy trained simulated robot play data maps generated object flow robot actions realize desired object movements using flow input policy directly deployed real world minimal simtoreal gap leveraging realworld human videos simulated robot play data bypass challenges teleoperating physical robots real world resulting scalable system diverse tasks demonstrate capabilities variety realworld tasks including manipulation rigid articulated deformable objects', 'human video generation novel scenarios enables generalizable robot manipulation robot manipulation policies generalize novel tasks involving unseen object types new motions paper provide solution terms predicting motion information web data human video generation conditioning robot policy generated video instead attempting scale robot data collection expensive show leverage video generation models trained easily available web data enabling generalization approach casts languageconditioned manipulation zeroshot human video generation followed execution single policy conditioned generated video train policy use order magnitude less robot interaction data compared video prediction model trained doesnt require finetuning video model directly use pretrained model generating human videos results diverse realworld scenarios show enables manipulating unseen object types performing novel motions tasks present robot data videos', 'moto latent motion token bridging language learning robot manipulation videos recent developments large language models pretrained extensive corpora shown significant success various natural language processing tasks minimal finetuning success offers new promise robotics long constrained high cost actionlabeled data ask given abundant video data containing interactionrelated knowledge available rich corpus similar generative pretraining approach effectively applied enhance robot learning key challenge identify effective representation autoregressive pretraining benefits robot manipulation tasks inspired way humans learn new skills observing dynamic environments propose effective robotic learning emphasize motionrelated knowledge closely tied lowlevel actions hardwareagnostic facilitating transfer learned motions actual robot actions end introduce moto converts video content latent motion token sequences latent motion tokenizer learning bridging language motion videos unsupervised manner pretrain motogpt motion token autoregression enabling capture diverse visual motion knowledge pretraining motogpt demonstrates promising ability produce semantically interpretable motion tokens predict plausible motion trajectories assess trajectory rationality output likelihood transfer learned motion priors real robot actions implement cofinetuning strategy seamlessly bridges latent motion token prediction real robot control extensive experiments show finetuned motogpt exhibits superior robustness efficiency robot manipulation benchmarks underscoring effectiveness transferring knowledge video data downstream visual manipulation tasks']"
5,61,5_detection_videos_deepfake_video,"['detection', 'videos', 'deepfake', 'video', 'dataset', 'models', 'forgery', 'generated', 'methods', 'detectors']","['tugofwar deepfake generation detection multimodal generative models rapidly evolving leading surge generation realistic video audio offers exciting possibilities also serious risks deepfake videos convincingly impersonate individuals particularly garnered attention due potential misuse spreading misinformation creating fraudulent content survey paper examines dual landscape deepfake video generation detection emphasizing need effective countermeasures potential abuses provide comprehensive overview current deepfake generation techniques including face swapping reenactment audiodriven animation leverage cuttingedge technologies like gans diffusion models produce highly realistic fake videos additionally analyze various detection approaches designed differentiate authentic altered videos detecting visual artifacts deploying advanced algorithms pinpoint inconsistencies across video audio signals effectiveness detection methods heavily relies diversity quality datasets used training evaluation discuss evolution deepfake datasets highlighting importance robust diverse frequently updated collections enhance detection accuracy generalizability deepfakes become increasingly indistinguishable authentic content developing advanced detection techniques keep pace generation technologies crucial advocate proactive approach tugofwar deepfake creators detectors emphasizing need continuous research collaboration standardization evaluation metrics creation comprehensive benchmarks', 'hindi audiovideodeepfake havdf hindi languagebased audiovideo deepfake dataset deepfakes offer great potential innovation creativity also pose significant risks privacy trust security vast hindispeaking population india particularly vulnerable deepfakedriven misinformation campaigns fake videos speeches hindi enormous impact rural semiurban communities digital literacy tends lower people inclined trust video content development effective frameworks detection tools combat deepfake misuse requires highquality diverse extensive datasets existing popular datasets like ffdf faceforensics dfdc deepfake detection challenge based english language hence paper aims create first novel hindi deep fake dataset named hindi audiovideodeepfake havdf dataset generated using faceswap lipsyn voice cloning methods multistep process allows us create rich varied dataset captures nuances hindi speech facial expressions providing robust foundation training evaluating deepfake detection models hindi language context unique kind previous datasets contain either deepfake videos synthesized audio type deepfake dataset used training detector deepfake video audio datasets notably newly introduced havdf dataset demonstrates lower detection accuracys across existing detection methods like headpose etc compared wellknown datasets ffdf dfdc trend suggests havdf dataset presents deeper challenges detect possibly due focus hindi language content diverse manipulation techniques havdf dataset fills gap hindispecific deepfake datasets aiding multilingual deepfake detection development', 'deepfake detection videos multiple faces using geometricfakeness features due development facial manipulation techniques recent years deepfake detection video stream became important problem face biometrics brand monitoring online video conferencing solutions case biometric authentication replace real datastream deepfake bypass liveness detection system using deepfake video conference penetrate private meeting deepfakes victims public figures also used fraudsters blackmailing extorsion financial fraud therefore task detecting deepfakes relevant ensuring privacy security existing approaches deepfake detection performance deteriorates multiple faces present video simultaneously objects erroneously classified faces research propose use geometricfakeness features gff characterize dynamic degree face presence video perframe deepfake scores analyze temporal inconsistencies gffs frames train complex deep learning model outputs final deepfake prediction employ approach analyze videos multiple faces simultaneously present video videos often occur practice eg online video conference case real faces appearing frame together deepfake face significantly affect deepfake detection approach allows counter problem extensive experiments demonstrate approach outperforms current stateoftheart methods popular benchmark datasets faceforensics dfdc celebdf wilddeepfake proposed approach remains accurate trained detect multiple different deepfake generation techniques']"
6,29,6_quality_assessment_quality assessment_video quality,"['quality', 'assessment', 'quality assessment', 'video quality', 'video quality assessment', 'video', 'ugc', 'content', 'videos', 'vqa']","['finevq finegrained user generated content video quality assessment rapid growth usergenerated content ugc videos produced urgent need effective video quality assessment vqa algorithms monitor video quality guide optimization recommendation procedures however current vqa models generally give overall rating ugc video lacks finegrained labels serving video processing recommendation applications address challenges promote development ugc videos establish first largescale finegrained video quality assessment database termed finevd comprises ugc videos finegrained quality scores descriptions across multiple dimensions based database propose finegrained video quality assessment finevq model learn finegrained quality ugc videos capabilities quality rating quality scoring quality attribution extensive experimental results demonstrate proposed finevq produce finegrained videoquality results achieve stateoftheart performance finevd commonly used ugcvqa datasets finevd finevq made publicly available', 'benchmarking multidimensional aigc video quality assessment dataset unified model recent years artificial intelligence aidriven video generation gained significant attention consequently growing need accurate video quality assessment vqa metrics evaluate perceptual quality aigenerated content aigc videos optimize video generation models however assessing quality aigc videos remains significant challenge videos often exhibit highly complex distortions unnatural actions irrational objects address challenge systematically investigate aigcvqa problem considering subjective objective quality assessment perspectives subjective perspective construct largescale generated video quality assessment lgvq dataset consisting aigc videos generated video generation models using carefully curated text prompts evaluate perceptual quality aigc videos three critical dimensions spatial quality temporal quality textvideo alignment objective perspective establish benchmark evaluating existing quality assessment metrics lgvq dataset findings show current metrics perform poorly dataset highlighting gap effective evaluation tools bridge gap propose unify generated video quality assessment ugvq model designed accurately evaluate multidimensional quality aigc videos ugvq model integrates visual motion features videos textual features corresponding prompts forming unified qualityaware feature representation tailored aigc videos experimental results demonstrate ugvq achieves stateoftheart performance lgvq dataset across three quality dimensions lgvq dataset ugvq model publicly available httpsgithubcomzczhangsjtuugvqgit', 'perceptual video quality assessment survey perceptual video quality assessment plays vital role field video processing due existence quality degradations introduced various stages video signal acquisition compression transmission display advancement internet communication cloud service technology video content traffic growing exponentially emphasizes requirement accurate rapid assessment video quality therefore numerous subjective objective video quality assessment studies conducted past two decades generic videos specific videos streaming usergenerated content ugc virtual augmented reality vr ar high frame rate hfr audiovisual etc survey provides uptodate comprehensive review video quality assessment studies specifically first review subjective video quality assessment methodologies databases necessary validating performance video quality metrics second objective video quality assessment algorithms general purposes surveyed concluded according methodologies utilized quality measures third overview objective video quality assessment measures specific applications emerging topics finally performances stateoftheart video quality assessment measures compared analyzed survey provides systematic overview classical works recent progresses realm video quality assessment help researchers quickly access field conduct relevant research']"
7,23,7_ai_video_sora_generation,"['ai', 'video', 'sora', 'generation', 'video generation', 'models', 'videos', 'generative ai', 'generative', 'future']","['video worth thousand images exploring latest trends long video generation image may convey thousand words video composed hundreds thousands image frames tells intricate story despite significant progress multimodal large language models mllms generating extended videos remains formidable challenge writing openais sora current stateoftheart system still limited producing videos one minute length limitation stems complexity long video generation requires generative ai techniques approximating density functions essential aspects planning story development maintaining spatial temporal consistency present additional hurdles integrating generative ai divideandconquer approach could improve scalability longer videos offering greater control survey examine current landscape long video generation covering foundational techniques like gans diffusion models video generation strategies largescale training datasets quality metrics evaluating long videos future research areas address limitations existing video generation capabilities believe would serve comprehensive foundation offering extensive information guide future advancements research field long video generation', 'movie gen swot analysis metas generative ai foundation model transforming media generation advertising entertainment industries generative ai reshaping media landscape enabling unprecedented capabilities video creation personalization scalability paper presents comprehensive swot analysis metas movie gen cuttingedge generative ai foundation model designed produce hd videos synchronized audio simple text prompts explore strengths including highresolution video generation precise editing seamless audio integration make transformative tool across industries filmmaking advertising education however analysis also addresses limitations constraints video length potential biases generated content pose challenges broader adoption addition examine evolving regulatory ethical considerations surrounding generative ai focusing issues like content authenticity cultural representation responsible use comparative insights leading models like dalle google imagen paper highlights movie gens unique features video personalization multimodal synthesis identifying opportunities innovation areas requiring research findings provide actionable insights stakeholders emphasizing opportunities challenges deploying generative ai media production work aims guide future advancements generative ai ensuring scalability quality ethical integrity rapidly evolving field', 'comprehensive survey human video generation challenges methods insights human video generation dynamic rapidly evolving task aims synthesize human body video sequences generative models given control conditions text audio pose potential wideranging applications film gaming virtual communication ability generate natural realistic human video critical recent advancements generative models laid solid foundation growing interest area despite significant progress task human video generation remains challenging due consistency characters complexity human motion difficulties relationship environment survey provides comprehensive review current state human video generation marking best knowledge first extensive literature review domain start introduction fundamentals human video generation evolution generative models facilitated fields growth examine main methods employed three key subtasks within human video generation textdriven audiodriven posedriven motion generation areas explored concerning conditions guide generation process furthermore offer collection commonly utilized datasets evaluation metrics crucial assessing quality realism generated videos survey concludes discussion current challenges field suggests possible directions future research goal survey offer research community clear holistic view advancements human video generation highlighting milestones achieved challenges lie ahead']"
8,18,8_data_estimation_images_image,"['data', 'estimation', 'images', 'image', 'segmentation', 'method', 'object', 'ir', 'using', 'conditions']","['turbulence strength estimation video using physicsbased deep learning images captured long distance suffer dynamic image distortion due turbulent flow air cells random temperatures thus refractive indices phenomenon known image dancing commonly characterized refractiveindex structure constant measure turbulence strength many applications atmospheric forecast model longrangeastronomy imaging aviation safety optical communication technology estimation critical accurately sensing turbulent environment previous methods estimation include estimation meteorological data temperature relative humidity wind shear etc singlepoint measurements twoended pathlength measurements optical scintillometer pathaveraged recently estimating passive video cameras low cost hardware complexity paper present comparative analysis classical image gradient methods estimation modern deep learningbased methods leveraging convolutional neural networks enable collect dataset video capture along reference scintillometer measurements ground truth release unique dataset scientific community observe deep learning methods achieve higher accuracy trained similar data suffer generalization errors unseen imagery compared classical methods overcome tradeoff present novel physicsbased network architecture combines learned convolutional layers differentiable image gradient method maintains high accuracy generalizable across image datasets', 'dvos selfsupervised densepattern video object segmentation video object segmentation approaches primarily rely largescale pixelaccurate humanannotated datasets model development dense video object segmentation dvos scenarios video frame encompasses hundreds small dense partially occluded objects accordingly laborintensive manual annotation even single frame often takes hours hinders development dvos many applications furthermore videos dense patterns following large number objects move different directions poses additional challenges address challenges proposed semiselfsupervised spatiotemporal approach dvos utilizing diffusionbased method multitask learning emulating real videos optical flow simulating motion developed methodology synthesize computationally annotated videos used training dvos models model performance improved utilizing weakly labeled computationally generated imprecise data demonstrate utility efficacy proposed approach developed dvos models wheat head segmentation handheld dronecaptured videos capturing wheat crops fields different locations across various growth stages spanning heading maturity despite using manually annotated video frames proposed approach yielded highperforming models achieving dice score tested dronecaptured external test set showed efficacy proposed approach wheat head segmentation application extended crops dvos domains crowd analysis microscopic image analysis', 'sustechgan image generation object detection adverse conditions autonomous driving autonomous driving significantly benefits datadriven deep neural networks however data autonomous driving typically fits longtailed distribution critical driving data adverse conditions hard collect although generative adversarial networks gans applied augment data autonomous driving generating driving images adverse conditions still challenging work propose novel framework sustechgan customized dual attention modules multiscale generators novel loss function generate driving images improving object detection autonomous driving adverse conditions test sustechgan wellknown gans generate driving images adverse conditions rain night apply generated images retrain object detection networks specifically add generated images training datasets retrain wellknown evaluate improvement retrained object detection adverse conditions experimental results show generated driving images sustechgan significantly improved performance retrained rain night conditions outperforms wellknown gans opensource code video description datasets available page facilitate image generation development autonomous driving adverse conditions']"
9,16,9_driving_autonomous_autonomous driving_world,"['driving', 'autonomous', 'autonomous driving', 'world', 'video generation', 'driving videos', 'videos', 'generation', 'simulation', 'video']","['world models effective data machines driving scene representation closedloop simulation essential advancing endtoend autonomous driving systems contemporary sensor simulation methods nerf rely predominantly conditions closely aligned training data distributions largely confined forwarddriving scenarios consequently methods face limitations rendering complex maneuvers eg lane change acceleration deceleration recent advancements autonomousdriving world models demonstrated potential generate diverse driving videos however approaches remain constrained video generation inherently lacking spatiotemporal coherence required capture intricacies dynamic driving environments paper introduce enhances driving scene representation leveraging world model priors specifically utilize world model data machine synthesize novel trajectory videos structured conditions explicitly leveraged control spatialtemporal consistency traffic elements besides cousin data training strategy proposed facilitate merging real synthetic data optimizing knowledge first utilize video generation models improving reconstruction driving scenarios experimental results reveal significantly enhances generation quality novel trajectory views achieving relative improvement fid compared pvg deformablegs moreover markedly enhances spatiotemporal coherence driving agents verified comprehensive user study relative increases ntaiou metric', 'exploring interplay video generation world models autonomous driving survey world models video generation pivotal technologies domain autonomous driving playing critical role enhancing robustness reliability autonomous systems world models simulate dynamics realworld environments video generation models produce realistic video sequences increasingly integrated improve situational awareness decisionmaking capabilities autonomous vehicles paper investigates relationship two technologies focusing structural parallels particularly diffusionbased models contribute accurate coherent simulations driving scenarios examine leading works jepa genie sora exemplify different approaches world model design thereby highlighting lack universally accepted definition world models diverse interpretations underscore fields evolving understanding world models optimized various autonomous driving tasks furthermore paper discusses key evaluation metrics employed domain chamfer distance scene reconstruction frechet inception distance fid assessing quality generated video content analyzing interplay video generation world models survey identifies critical challenges future research directions emphasizing potential technologies jointly advance performance autonomous driving systems findings presented paper aim provide comprehensive understanding integration video generation world models drive innovation development safer reliable autonomous vehicles', 'llmenhanced world models diverse driving video generation world models demonstrated superiority autonomous driving particularly generation multiview driving videos however significant challenges still exist generating customized driving videos paper propose builds upon framework drivedreamer incorporates large language model llm generate userdefined driving videos specifically llm interface initially incorporated convert users query agent trajectories subsequently hdmap adhering traffic regulations generated based trajectories ultimately propose unified multiview model enhance temporal spatial coherence generated driving videos first world model generate customized driving videos generate uncommon driving videos eg vehicles abruptly cut userfriendly manner besides experimental results demonstrate generated videos enhance training driving perception methods eg detection tracking furthermore video generation quality surpasses stateoftheart methods showcasing fid fvd scores representing relative improvements']"
