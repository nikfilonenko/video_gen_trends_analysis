Topic,Count,Name,Representation,Representative_Docs
-1,1097,-1_video_models_generation_model,"['video', 'models', 'generation', 'model', 'videos', 'diffusion', 'image', 'motion', 'data', 'methods']","['sned superposition network architecture search efficient video diffusion model aigenerated content garnered significant attention achieving photorealistic video synthesis remains formidable challenge despite promising advances diffusion models video generation quality complex model architecture substantial computational demands training inference create significant gap models realworld applications paper presents sned superposition network architecture search method efficient video diffusion model method employs supernet training paradigm targets various model cost resolution options using weightsharing method moreover propose supernet training sampling warmup fast training optimization showcase flexibility method conduct experiments involving pixelspace latentspace video diffusion models results demonstrate framework consistently produces comparable results across different model options high efficiency according experiment pixelspace video diffusion model achieve consistent video generation results simultaneously across x x resolutions large range model sizes number parameters pixelspace video diffusion models', 'multisentence video grounding long video generation video generation witnessed great success recently application generating long videos still remains challenging due difficulty maintaining temporal consistency generated videos high memory cost generation tackle problems paper propose brave new idea multisentence video grounding long video generation connecting massive video moment retrieval video generation task first time providing new paradigm long video generation method work summarized three steps design sequential scene text prompts queries video grounding utilizing massive video moment retrieval search video moment segments meet text requirements video database ii based source frames retrieved video moment segments adopt video editing methods create new video content preserving temporal consistency retrieved video since editing conducted segment segment even frame frame largely reduces memory cost iii also attempt video morphing personalized generation methods improve subject consistency long video generation providing ablation experimental results subtasks long video generation approach seamlessly extends development imagevideo editing video morphing personalized generation video grounding long video generation offering effective solutions generating long videos low memory cost', 'zerosmooth trainingfree diffuser adaptation high frame rate video generation video generation made remarkable progress recent years especially since advent video diffusion models many video generation models produce plausible synthetic videos eg stable video diffusion svd however video models generate low frame rate videos due limited gpu memory well difficulty modeling large set frames training videos always uniformly sampled specified interval temporal compression previous methods promote frame rate either training video interpolation model pixel space postprocessing stage training interpolation model latent space specific base video model paper propose trainingfree video interpolation method generative video diffusion models generalizable different models plugandplay manner investigate nonlinearity feature space video diffusion models transform video model selfcascaded video diffusion model incorporating designed hidden state correction modules selfcascaded architecture correction module proposed retain temporal consistency key frames interpolated frames extensive evaluations preformed multiple popular video models demonstrate effectiveness propose method especially trainingfree method even comparable trained interpolation models supported huge compute resources largescale datasets']"
0,159,0_video_understanding_models_videos,"['video', 'understanding', 'models', 'videos', 'reasoning', 'language', 'large', 'multimodal', 'visual', 'video understanding']","['longvlm efficient long video understanding via large language models empowered large language models llms recent advancements videobased llms videollms driven progress various video understanding tasks models encode video representations pooling query aggregation vast number visual tokens making computational memory costs affordable despite successfully providing overall comprehension video content existing videollms still face challenges achieving detailed understanding due overlooking local information longterm videos tackle challenge introduce longvlm simple yet powerful videollm long video understanding building upon observation long videos often consist sequential key events complex actions camera movements approach proposes decompose long videos multiple shortterm segments encode local features segment via hierarchical token merging module features concatenated temporal order maintain storyline across sequential shortterm segments additionally propose integrate global semantics local feature enhance context understanding way encode video representations incorporate local global information enabling llm generate comprehensive responses longterm videos experimental results videochatgpt benchmark zeroshot video questionanswering datasets demonstrate superior capabilities model previous stateoftheart methods qualitative examples show model produces precise responses long video understanding code available httpsgithubcomziplablongvlm', 'cgbench cluegrounded question answering benchmark long video understanding existing video understanding benchmarks multimodal large language models mllms focus short videos limited number benchmarks long video understanding often rely solely multiplechoice questions mcqs however inherent limitation mcqbased evaluation increasing reasoning ability mllms models give current answer purely combining short video understanding elimination without genuinely understanding video content address gap introduce cgbench novel benchmark designed cluegrounded question answering long videos cgbench emphasizes models ability retrieve relevant clues questions enhancing evaluation credibility features manually curated videos categorized granular system primary categories secondary categories tertiary categories making largest benchmark long video analysis benchmark includes qa pairs three major question types perception reasoning hallucination compensating drawbacks pure mcqbased evaluation design two novel cluebased evaluation methods cluegrounded white box black box evaluations assess whether model generates answers based correct understanding video evaluate multiple closedsource opensource mllms cgbench results indicate current models significantly underperform understanding long videos compared short ones significant gap exists opensource commercial models hope cgbench advance development trustworthy capable mllms long video understanding annotations video data released httpscgbenchgithubioleaderboard', 'seconds hours reviewing multimodal large language models comprehensive long video understanding integration large language models llms visual encoders recently shown promising performance visual understanding tasks leveraging inherent capability comprehend generate humanlike text visual reasoning given diverse nature visual data multimodal large language models mmllms exhibit variations model designing training understanding images short videos long videos paper focuses substantial differences unique challenges posed long video understanding compared static image short video understanding unlike static images short videos encompass sequential frames spatial withinevent temporal information long videos consist multiple events betweenevent longterm temporal information survey aim trace summarize advancements mmllms image understanding long video understanding review differences among various visual understanding tasks highlight challenges long video understanding including finegrained spatiotemporal details dynamic events longterm dependencies provide detailed summary advancements mmllms terms model design training methodologies understanding long videos finally compare performance existing mmllms video understanding benchmarks various lengths discuss potential future directions mmllms long video understanding']"
1,122,1_scene_dynamic_scenes_video,"['scene', 'dynamic', 'scenes', 'video', 'novel', 'reconstruction', 'view', 'videos', 'camera', 'generation']","['dynamic content generation multiframe multiview consistency present stable video latent video diffusion model multiframe multiview consistent dynamic content generation unlike previous methods rely separately trained generative models video generation novel view synthesis design unified diffusion model generate novel view videos dynamic objects specifically given monocular reference video generates novel views video frame temporally consistent use generated novel view videos optimize implicit representation dynamic nerf efficiently without need cumbersome sdsbased optimization used prior works train unified novel view video generation model curate dynamic object dataset existing objaverse dataset extensive experimental results multiple datasets user studies demonstrate stateoftheart performance novelview video synthesis well generation compared prior works', 'selfcalibrating novel view synthesis monocular videos using gaussian splatting gaussian splatting gs significantly elevated scene reconstruction efficiency novel view synthesis nvs accuracy compared neural radiance fields nerf particularly dynamic scenes however current nvs methods whether based gs nerf primarily rely camera parameters provided colmap even utilize sparse point clouds generated colmap initialization lack accuracy well timeconsuming sometimes results poor dynamic scene representation especially scenes large object movements extreme camera conditions eg small translations combined large rotations studies simultaneously optimize estimation camera parameters scenes supervised additional information like depth optical flow etc obtained offtheshelf models using unverified information ground truth reduce robustness accuracy frequently occur long monocular videos eg hundreds frames propose novel approach learns highfidelity gs scene representation selfcalibration camera parameters includes extraction point features robustly represent structure use subsequent joint optimization camera parameters structure towards overall scene optimization demonstrate accuracy time efficiency method extensive quantitative qualitative experimental results several standard benchmarks results show significant improvements stateoftheart methods novel view synthesis source code released soon', 'sparse input view synthesis representations reliable priors novel view synthesis refers problem synthesizing novel viewpoints scene given images viewpoints fundamental problem computer vision graphics enables vast variety applications metaverse freeview watching events video gaming video stabilization video compression recent representations radiance fields multiplane images significantly improve quality images rendered novel viewpoints however models require dense sampling input views high quality renders performance goes significantly input views available thesis focus sparse input novel view synthesis problem static dynamic scenes first part work mainly focus sparse input novel view synthesis static scenes using neural radiance fields nerf study design reliable dense priors better regularize nerf situations particular propose prior visibility pixels pair input views show visibility prior related relative depth objects dense reliable existing priors absolute depth compute visibility prior using plane sweep volumes without need train neural network large datasets evaluate approach multiple datasets show model outperforms existing approaches sparse input novel view synthesis second part aim improve regularization learning scenespecific prior suffer generalization issues achieve learning prior given scene alone without pretraining large datasets particular design augmented nerfs obtain better depth supervision certain regions scene main nerf extend framework also apply newer faster radiance field models tensorf zipnerf extensive experiments multiple datasets show superiority approach sparse input novel view synthesis design sparse input fast dynamic radiance fields severely constrained lack suitable representations reliable priors motion address first challenge designing explicit motion model based factorized volumes compact optimizes quickly also introduce reliable sparse flow priors constrain motion field since find popularly employed dense optical flow priors unreliable show benefits motion representation reliable priors multiple datasets final part thesis study application view synthesis frame rate upsampling video gaming specifically consider problem temporal view synthesis goal predict future frames given past frames camera motion key challenge predicting future motion objects estimating past motion extrapolating explore use multiplane image representations scene depth reliably estimate object motion particularly occluded regions design new database effectively evaluate approach temporal view synthesis dynamic scenes show achieve stateoftheart performance']"
2,83,2_video_compression_coding_video compression,"['video', 'compression', 'coding', 'video compression', 'neural', 'diffusion', 'model', 'streaming', 'quality', 'latent']","['unified framework intra interframe video compression video compression aims reconstruct seamless frames encoding motion residual information existing frames previous neural video compression methods necessitate distinct codecs three types frames iframe pframe bframe hinders unified approach generalization across different video contexts intracodec techniques lack advanced motion estimation motion compensation memc found intercodec leading fragmented frameworks lacking uniformity proposed intra interframe video compression framework employs single spatiotemporal codec guides feature compression rates according content importance unified codec transforms dependence across frames conditional coding scheme thus integrating intra interframe compression one cohesive strategy given absence explicit motion data achieving competent interframe compression conditional codec poses challenge resolve approach includes implicit interframe alignment mechanism pretrained diffusion denoising process utilization diffusioninverted reference feature rather random noise supports initial compression state process allows selective denoising motionrich regions based decoded features facilitating accurate alignment without need memc experimental findings across various compression configurations ai ld ra frame types prove outperforms stateoftheart perceptual learned codecs impressively exhibits enhancement perceptual reconstruction performance benchmarked standard vtm official implementation found', 'accelerating learned video compression via lowresolution representation learning recent years field learned video compression witnessed rapid advancement exemplified latest neural video codecs dcvcdc outperformed upcoming nextgeneration codec ecm terms compression ratio despite learned video compression frameworks often exhibit low encoding decoding speeds primarily due increased computational complexity unnecessary highresolution spatial operations hugely hinder applications reality work introduce efficiencyoptimized framework learned video compression focuses lowresolution representation learning aiming significantly enhance encoding decoding speeds firstly diminish computational load reducing resolution interframe propagated features obtained reused features decoded frames including iframes implement joint training strategy iframe pframe models improving compression ratio secondly approach efficiently leverages multiframe priors parameter prediction minimizing computation decoding end thirdly revisit application online encoder update oeu strategy highresolution sequences achieving notable improvements compression ratio without compromising decoding efficiency efficiencyoptimized framework significantly improved balance compression ratio speed learned video compression comparison traditional codecs method achieves performance levels par lowdecay p configuration reference software vtm furthermore contrasted dcvchem approach delivers comparable compression ratio boosting encoding decoding speeds factor respectively rtx method decode frame', 'parameterefficient instanceadaptive neural video compression learningbased neural video codecs nvcs emerged compelling alternative standard video codecs demonstrating promising performance simple easily maintainable pipelines however nvcs often fall short compression performance occasionally exhibit poor generalization capability due inferenceonly compression scheme dependence training data instanceadaptive video compression techniques recently suggested viable solution finetuning encoder decoder networks particular test instance video however finetuning model parameters incurs high computational costs increases bitrates often leads unstable training work propose parameterefficient instanceadaptive video compression framework inspired remarkable success parameterefficient finetuning largescale neural network models propose use lightweight adapter module easily attached pretrained nvcs finetuned test video sequences resulting algorithm significantly improves compression performance reduces encoding time compared existing instantadaptive video compression algorithms furthermore suggested finetuning method enhances robustness training process allowing proposed method widely used many practical settings conducted extensive experiments various standard benchmark datasets including uvg mcljvc hevc sequences experimental results shown significant improvement ratedistortion rd curves db psnr bd rates compared baselines nvc code available httpsgithubcomohsngjunpevc']"
3,70,3_images_image_data_synthesis,"['images', 'image', 'data', 'synthesis', 'diffusion', 'medical', 'tumor', 'segmentation', 'mri', 'imaging']","['generative enhancement medical images limited availability medical image datasets due privacy concerns high collection annotation costs poses significant challenges field medical imaging promising alternative use synthesized medical data solutions realistic medical image synthesis due difficulties backbone design fewer training samples compared counterparts paper propose novel generative approach synthesis medical images enhancement existing datasets using conditional diffusion models method begins slice noted informed slice serve patient prior propagates generation process using segmentation mask decomposing medical images masks patient prior information offers flexible yet effective solution generating versatile images existing datasets enable dataset enhancement combining informed slice selection generation random positions along editable mask volumes introduce large variations diffusion sampling moreover informed slice contains patientwise information also facilitate counterfactual image synthesis datasetlevel deenhancement desired control experiments brain mri abdomen ct images demonstrate capable synthesizing highquality medical images volumetric consistency offering straightforward solution dataset enhancement inference code available', 'analyzing tumors synthesis computeraided tumor detection shown great potential enhancing interpretation million ct scans performed annually united states however challenges arise due rarity ct scans tumors especially earlystage tumors developing ai real tumor data faces issues scarcity annotation difficulty low prevalence tumor synthesis addresses challenges generating numerous tumor examples medical images aiding ai training tumor detection segmentation successful synthesis requires realistic generalizable synthetic tumors across various organs chapter reviews ai development real synthetic data summarizes two key trends synthetic data cancer imaging research modelingbased learningbased approaches modelingbased methods like simulate tumor development time using generic rules learningbased methods like difftumor learn annotated examples one organ generate synthetic tumors others reader studies expert radiologists show synthetic tumors convincingly realistic also present case studies liver pancreas kidneys reveal ai trained synthetic tumors achieve performance comparable better ai trained real data tumor synthesis holds significant promise expanding datasets enhancing ai reliability improving tumor detection performance preserving patient privacy', 'synthetic brain images bridging gap brain mapping generative adversarial model magnetic resonance imaging mri vital modality gaining precise anatomical information plays significant role medical imaging diagnosis therapy planning image synthesis problems seen revolution recent years due introduction deep learning techniques specifically generative adversarial networks gans work investigates use deep convolutional generative adversarial networks dcgan producing highfidelity realistic mri image slices suggested approach uses dataset variety brain mri scans train dcgan architecture discriminator network discerns created real slices generator network learns synthesise realistic mri image slices generator refines capacity generate slices closely mimic real mri data adversarial training approach outcomes demonstrate dcgan promise range uses medical imaging research since show effectively produce mri image slices train consequent number epochs work adds expanding corpus research application deep learning techniques medical image synthesis slices could produced possess capability enhance datasets provide data augmentation training deep learning models well number functions made available make mri data cleaning easier three ready use clean dataset major anatomical plans']"
4,69,4_detection_deepfake_videos_video,"['detection', 'deepfake', 'videos', 'video', 'dataset', 'forgery', 'models', 'attacks', 'fake', 'deepfake detection']","['tugofwar deepfake generation detection multimodal generative models rapidly evolving leading surge generation realistic video audio offers exciting possibilities also serious risks deepfake videos convincingly impersonate individuals particularly garnered attention due potential misuse spreading misinformation creating fraudulent content survey paper examines dual landscape deepfake video generation detection emphasizing need effective countermeasures potential abuses provide comprehensive overview current deepfake generation techniques including face swapping reenactment audiodriven animation leverage cuttingedge technologies like gans diffusion models produce highly realistic fake videos additionally analyze various detection approaches designed differentiate authentic altered videos detecting visual artifacts deploying advanced algorithms pinpoint inconsistencies across video audio signals effectiveness detection methods heavily relies diversity quality datasets used training evaluation discuss evolution deepfake datasets highlighting importance robust diverse frequently updated collections enhance detection accuracy generalizability deepfakes become increasingly indistinguishable authentic content developing advanced detection techniques keep pace generation technologies crucial advocate proactive approach tugofwar deepfake creators detectors emphasizing need continuous research collaboration standardization evaluation metrics creation comprehensive benchmarks', 'hindi audiovideodeepfake havdf hindi languagebased audiovideo deepfake dataset deepfakes offer great potential innovation creativity also pose significant risks privacy trust security vast hindispeaking population india particularly vulnerable deepfakedriven misinformation campaigns fake videos speeches hindi enormous impact rural semiurban communities digital literacy tends lower people inclined trust video content development effective frameworks detection tools combat deepfake misuse requires highquality diverse extensive datasets existing popular datasets like ffdf faceforensics dfdc deepfake detection challenge based english language hence paper aims create first novel hindi deep fake dataset named hindi audiovideodeepfake havdf dataset generated using faceswap lipsyn voice cloning methods multistep process allows us create rich varied dataset captures nuances hindi speech facial expressions providing robust foundation training evaluating deepfake detection models hindi language context unique kind previous datasets contain either deepfake videos synthesized audio type deepfake dataset used training detector deepfake video audio datasets notably newly introduced havdf dataset demonstrates lower detection accuracys across existing detection methods like headpose etc compared wellknown datasets ffdf dfdc trend suggests havdf dataset presents deeper challenges detect possibly due focus hindi language content diverse manipulation techniques havdf dataset fills gap hindispecific deepfake datasets aiding multilingual deepfake detection development', 'deepfake detection videos multiple faces using geometricfakeness features due development facial manipulation techniques recent years deepfake detection video stream became important problem face biometrics brand monitoring online video conferencing solutions case biometric authentication replace real datastream deepfake bypass liveness detection system using deepfake video conference penetrate private meeting deepfakes victims public figures also used fraudsters blackmailing extorsion financial fraud therefore task detecting deepfakes relevant ensuring privacy security existing approaches deepfake detection performance deteriorates multiple faces present video simultaneously objects erroneously classified faces research propose use geometricfakeness features gff characterize dynamic degree face presence video perframe deepfake scores analyze temporal inconsistencies gffs frames train complex deep learning model outputs final deepfake prediction employ approach analyze videos multiple faces simultaneously present video videos often occur practice eg online video conference case real faces appearing frame together deepfake face significantly affect deepfake detection approach allows counter problem extensive experiments demonstrate approach outperforms current stateoftheart methods popular benchmark datasets faceforensics dfdc celebdf wilddeepfake proposed approach remains accurate trained detect multiple different deepfake generation techniques']"
5,69,5_robot_manipulation_learning_policy,"['robot', 'manipulation', 'learning', 'policy', 'tasks', 'human', 'videos', 'data', 'world', 'video']","['flow crossdomain manipulation interface present scalable learning framework enables robots acquire realworld manipulation skills without need realworld robot training data key idea behind use object flow manipulation interface bridging domain gaps different embodiments ie human robot training environments ie realworld simulated comprises two components flow generation network flowconditioned policy flow generation network trained human demonstration videos generates object flow initial scene image conditioned task description flowconditioned policy trained simulated robot play data maps generated object flow robot actions realize desired object movements using flow input policy directly deployed real world minimal simtoreal gap leveraging realworld human videos simulated robot play data bypass challenges teleoperating physical robots real world resulting scalable system diverse tasks demonstrate capabilities variety realworld tasks including manipulation rigid articulated deformable objects', 'human video generation novel scenarios enables generalizable robot manipulation robot manipulation policies generalize novel tasks involving unseen object types new motions paper provide solution terms predicting motion information web data human video generation conditioning robot policy generated video instead attempting scale robot data collection expensive show leverage video generation models trained easily available web data enabling generalization approach casts languageconditioned manipulation zeroshot human video generation followed execution single policy conditioned generated video train policy use order magnitude less robot interaction data compared video prediction model trained doesnt require finetuning video model directly use pretrained model generating human videos results diverse realworld scenarios show enables manipulating unseen object types performing novel motions tasks present robot data videos', 'moto latent motion token bridging language learning robot manipulation videos recent developments large language models pretrained extensive corpora shown significant success various natural language processing tasks minimal finetuning success offers new promise robotics long constrained high cost actionlabeled data ask given abundant video data containing interactionrelated knowledge available rich corpus similar generative pretraining approach effectively applied enhance robot learning key challenge identify effective representation autoregressive pretraining benefits robot manipulation tasks inspired way humans learn new skills observing dynamic environments propose effective robotic learning emphasize motionrelated knowledge closely tied lowlevel actions hardwareagnostic facilitating transfer learned motions actual robot actions end introduce moto converts video content latent motion token sequences latent motion tokenizer learning bridging language motion videos unsupervised manner pretrain motogpt motion token autoregression enabling capture diverse visual motion knowledge pretraining motogpt demonstrates promising ability produce semantically interpretable motion tokens predict plausible motion trajectories assess trajectory rationality output likelihood transfer learned motion priors real robot actions implement cofinetuning strategy seamlessly bridges latent motion token prediction real robot control extensive experiments show finetuned motogpt exhibits superior robustness efficiency robot manipulation benchmarks underscoring effectiveness transferring knowledge video data downstream visual manipulation tasks']"
6,67,6_talking_facial_face_head,"['talking', 'facial', 'face', 'head', 'lip', 'talking head', 'motion', 'generation', 'audiodriven', 'audio']","['posetalk textandaudiobased pose control motion refinement oneshot talking head generation previous audiodriven talking head generation thg methods generate head poses driving audio generated poses lips match audio well editable study propose textbfposetalk thg system freely generate lipsynchronized talking head videos free head poses conditioned text prompts audio core insight method using head pose connect visual linguistic audio signals first propose generate poses audio text prompts audio offers shortterm variations rhythm correspondence head movements text prompts describe longterm semantics head motions achieve goal devise pose latent diffusion pld model generate motion latent text prompts audio cues pose latent space second observe lossimbalance problem loss lip region contributes less total reconstruction loss caused pose lip making optimization lean towards head movements rather lip shapes address issue propose refinementbased learning strategy synthesize natural talking videos using two cascaded networks ie coarsenet refinenet coarsenet estimates coarse motions produce animated images novel poses refinenet focuses learning finer lip motions progressively estimating lip motions lowtohigh resolutions yielding improved lipsynchronization performance experiments demonstrate pose prediction strategy achieves better pose diversity realness compared textonly audioonly video generator model outperforms stateoftheart methods synthesizing talking videos natural head motions project httpsjunleengithubioprojectsposetalk', 'emotivetalk expressive talking head generation audio information decoupling emotional video diffusion diffusion models revolutionized field talking head generation yet still face challenges expressiveness controllability stability longtime generation research propose emotivetalk framework address issues firstly realize better control generation lip movement facial expression visionguided audio information decoupling vaid approach designed generate audiobased decoupled representations aligned lip movements expression specifically achieve alignment audio facial expression representation spaces present diffusionbased cospeech temporal expansion dicte module within vaid generate expressionrelated representations multisource emotion condition constraints propose welldesigned emotional talking head diffusion ethd backbone efficiently generate highly expressive talking head videos contains expression decoupling injection edi module automatically decouple expressions reference portraits integrating target expression information achieving expressive generation performance experimental results show emotivetalk generate expressive talking head videos ensuring promised controllability emotions stability longtime generation yielding stateoftheart performance compared existing methods', 'emodiffhead continuously emotional control talking head generation via diffusion task audiodriven portrait animation involves generating talking head video using identity image audio track speech many existing approaches focus lip synchronization video quality tackle challenge generating emotiondriven talking head videos ability control edit emotions essential producing expressive realistic animations response challenge propose emodiffhead novel method emotional talking head video generation enables finegrained control emotion categories intensities also enables oneshot generation given flame models linearity expression modeling utilize deca method extract expression vectors combined audio guide diffusion model generating videos precise lip synchronization rich emotional expressiveness approach enables learning rich facial information emotionirrelevant data also facilitates generation emotional videos effectively overcomes limitations emotional data lack diversity facial background information addresses absence emotional details emotionirrelevant data extensive experiments user studies demonstrate approach achieves stateoftheart performance compared emotion portrait animation methods']"
7,52,7_action_video_segmentation_instance,"['action', 'video', 'segmentation', 'instance', 'openvocabulary', 'temporal', 'training', 'pseudo', 'detection', 'task']","['advancing weaklysupervised audiovisual video parsing via segmentwise pseudo labeling audiovisual video parsing task aims identify temporally localize events occur either audio visual streams audible videos often performs weaklysupervised manner video event labels provided ie modalities timestamps labels unknown due lack densely annotated labels recent work attempts leverage pseudo labels enrich supervision commonly used strategy generate pseudo labels categorizing known video event labels modality however labels still confined video level temporal boundaries events remain unlabeled paper propose new pseudo label generation strategy explicitly assign labels video segment utilizing prior knowledge learned open world specifically exploit largescale pretrained models namely clip clap estimate events video segment generate segmentlevel visual audio pseudo labels respectively propose new loss function exploit pseudo labels taking account categoryrichness segmentrichness label denoising strategy also adopted improve visual pseudo labels flipping whenever abnormally large forward losses occur perform extensive experiments llp dataset demonstrate effectiveness proposed design achieve stateoftheart video parsing performance types event parsing ie audio event visual event audiovisual event also examine proposed pseudo label generation strategy relevant weaklysupervised audiovisual event localization task experimental results verify benefits generalization method', 'openvocabulary spatiotemporal action detection spatiotemporal action detection stad important finegrained video understanding task current methods require box label supervision action classes advance however realworld applications likely come across new action classes seen training action category space large hard enumerate also cost data annotation model training new classes extremely high traditional methods need perform detailed box annotations retrain whole network scratch paper propose new challenging setting performing openvocabulary stad better mimic situation action detection open world openvocabulary spatiotemporal action detection ovstad requires training model limited set base classes box label supervision expected yield good generalization performance novel action classes ovstad build two benchmarks based existing stad datasets propose simple effective method based pretrained videolanguage models vlm better adapt holistic vlm finegrained action detection task carefully finetune localized video regiontext pairs customized finetuning endows vlm better motion understanding thus contributing accurate alignment video regions texts local region feature global video feature fusion alignment adopted improve action detection performance providing global context method achieves promising performance novel classes', 'clipvis adapting clip openvocabulary video instance segmentation openvocabulary video instance segmentation strives segment track instances belonging open set categories videos visionlanguage model contrastive languageimage pretraining clip shown robust zeroshot classification ability imagelevel openvocabulary tasks paper propose simple encoderdecoder network called clipvis adapt clip openvocabulary video instance segmentation clipvis adopts frozen clip introduces three modules including classagnostic mask generation temporal topkenhanced matching weighted openvocabulary classification given set initial queries classagnostic mask generation introduces pixel decoder transformer decoder clip pretrained image encoder predict query masks corresponding object scores mask iou scores temporal topkenhanced matching performs query matching across frames using k mostly matched frames finally weighted openvocabulary classification first employs mask pooling generate query visual features clip pretrained image encoder second performs weighted classification using object scores mask iou scores clipvis require annotations instance categories identities experiments performed various video instance segmentation datasets demonstrate effectiveness proposed method especially novel categories using convnextb backbone clipvis achieves ap apn scores validation set lvvis dataset outperforms respectively release source code models']"
8,50,8_audio_music_sound_generation,"['audio', 'music', 'sound', 'generation', 'video', 'videotoaudio', 'model', 'audiovisual', 'alignment', 'dance']","['sonicvisionlm playing sound vision language models growing interest task generating sound silent videos primarily practicality streamlining video postproduction however existing methods videosound generation attempt directly create sound visual representations challenging due difficulty aligning visual representations audio representations paper present sonicvisionlm novel framework aimed generating wide range sound effects leveraging visionlanguage modelsvlms instead generating audio directly video use capabilities powerful vlms provided silent video approach first identifies events within video using vlm suggest possible sounds match video content shift approach transforms challenging task aligning image audio wellstudied subproblems aligning imagetotext texttoaudio popular diffusion models improve quality audio recommendations llms collected extensive dataset maps text descriptions specific sound effects developed timecontrolled audio adapter approach surpasses current stateoftheart methods converting video audio enhancing synchronization visuals improving alignment audio video components project page httpsyusiissygithubiosonicvisionlmgithubio', 'tell hear see video audio generation text content visual audio scenes multifaceted video paired various audio viceversa thereby videotoaudio generation task imperative introduce steering approaches controlling generated audio videotoaudio generation wellestablished generative task existing methods lack controllability work propose vatt multimodal generative framework takes video optional text prompt input generates audio optional textual description audio framework two advantages videotoaudio generation process refined controlled via text complements context visual information ii model suggest audio generate video generating audio captions vatt consists two key modules vatt converter llm finetuned instructions includes projection layer maps video features llm vector space vatt audio transformer generates audio tokens visual frames optional text prompt using iterative parallel decoding audio tokens converted waveform pretrained neural codec experiments show vatt compared existing videotoaudio generation methods objective metrics achieves competitive performance audio caption provided audio caption provided prompt vatt achieves even refined performance lowest kld score furthermore subjective studies show vatt audio chosen preferred generated audio audio generated existing methods vatt enables controllable videotoaudio generation text well suggesting text prompts videos audio captions unlocking novel applications textguided videotoaudio generation videotoaudio captioning', 'vintage joint video text conditioning holistic audio generation recent advances audio generation focused texttoaudio videotoaudio tasks however methods generate holistic sounds onscreen offscreen generate sounds aligning onscreen objects generate semantically complete offscreen sounds missing work address task holistic audio generation given video text prompt aim generate onscreen offscreen sounds temporally synchronized video semantically aligned text video previous approaches joint text videotoaudio generation often suffer modality bias favoring one modality overcome limitation introduce vintage flowbased transformer model jointly considers text video guide audio generation framework comprises two key components visualtext encoder joint vtsit model reduce modality bias improve generation quality employ pretrained unimodal texttoaudio videotoaudio generation models additional guidance due lack appropriate benchmarks also introduce vintagebench dataset videotextaudio pairs containing onscreen offscreen sounds comprehensive experiments vintagebench demonstrate joint text visual interaction necessary holistic audio generation furthermore vintage achieves stateoftheart results vggsound benchmark source code pretrained models released demo available httpswwwyoutubecomwatchvqmqwhujpkji']"
9,49,9_editing_motion_video_video editing,"['editing', 'motion', 'video', 'video editing', 'diffusion', 'models', 'control', 'generation', 'diffusion models', 'consistency']","['monkey see monkey harnessing selfattention motion diffusion zeroshot motion transfer given remarkable results motion synthesis diffusion models natural question arises effectively leverage models motion editing existing diffusionbased motion editing methods overlook profound potential prior embedded within weights pretrained models enables manipulating latent feature space hence primarily center handling motion space work explore attention mechanism pretrained motion diffusion models uncover roles interactions attention elements capturing representing intricate human motion patterns carefully integrate elements transfer leader motion follower one maintaining nuanced characteristics follower resulting zeroshot motion transfer editing features associated selected motions allows us confront challenge observed prior motion diffusion approaches use general directives eg text music editing ultimately failing convey subtle nuances effectively work inspired monkey closely imitates sees maintaining unique motion patterns hence call monkey see monkey dub momo employing technique enables accomplishing tasks synthesizing outofdistribution motions style transfer spatial editing furthermore diffusion inversion seldom employed motions result editing efforts focus generated motions limiting editability real ones momo harnesses motion inversion extending application real generated motions experimental results show advantage approach current art particular unlike methods tailored specific applications training approach applied inference time requiring training webpage httpsmonkeyseedocggithubio', 'freemask rethinking importance attention masks zeroshot video editing texttovideo diffusion models made remarkable advancements driven ability generate temporally coherent videos research zeroshot video editing using fundamental models expanded rapidly enhance editing quality structural controls frequently employed video editing among techniques crossattention mask control stands effectiveness efficiency however crossattention masks naively applied video editing introduce artifacts blurring flickering experiments uncover critical factor overlooked previous video editing research crossattention masks consistently clear vary model structure denoising timestep address issue propose metric mask matching cost mmc quantifies variability propose freemask method selecting optimal masks tailored specific video editing tasks using mmcselected masks improve masked fusion mechanism within comprehensive attention features eg temp cross selfattention modules approach seamlessly integrated existing zeroshot video editing frameworks better performance requiring control assistance parameter finetuning enabling adaptive decoupling unedited semantic layouts mask precision control extensive experiments demonstrate freemask achieves superior semantic fidelity temporal consistency editing quality compared stateoftheart methods', 'revideo remake video motion content control despite significant advancements video generation editing using diffusion models achieving accurate localized video editing remains substantial challenge additionally existing video editing methods primarily focus altering visual content limited research dedicated motion editing paper present novel attempt remake video revideo stands existing methods allowing precise video editing specific areas specification content motion content editing facilitated modifying first frame trajectorybased motion control offers intuitive user interaction experience revideo addresses new task involving coupling training imbalance content motion control tackle develop threestage training strategy progressively decouples two aspects coarse fine furthermore propose spatiotemporal adaptive fusion module integrate content motion control across various sampling steps spatial locations extensive experiments demonstrate revideo promising performance several accurate video editing applications ie locally changing video content keeping motion constant keeping content unchanged customizing new motion trajectories modifying content motion trajectories method also seamlessly extend applications multiarea editing without specific training demonstrating flexibility robustness']"
10,31,10_video_evaluation_models_ai,"['video', 'evaluation', 'models', 'ai', 'generation', 'video generation', 'sora', 'comprehensive', 'content', 'future']","['video worth thousand images exploring latest trends long video generation image may convey thousand words video composed hundreds thousands image frames tells intricate story despite significant progress multimodal large language models mllms generating extended videos remains formidable challenge writing openais sora current stateoftheart system still limited producing videos one minute length limitation stems complexity long video generation requires generative ai techniques approximating density functions essential aspects planning story development maintaining spatial temporal consistency present additional hurdles integrating generative ai divideandconquer approach could improve scalability longer videos offering greater control survey examine current landscape long video generation covering foundational techniques like gans diffusion models video generation strategies largescale training datasets quality metrics evaluating long videos future research areas address limitations existing video generation capabilities believe would serve comprehensive foundation offering extensive information guide future advancements research field long video generation', 'comprehensive survey human video generation challenges methods insights human video generation dynamic rapidly evolving task aims synthesize human body video sequences generative models given control conditions text audio pose potential wideranging applications film gaming virtual communication ability generate natural realistic human video critical recent advancements generative models laid solid foundation growing interest area despite significant progress task human video generation remains challenging due consistency characters complexity human motion difficulties relationship environment survey provides comprehensive review current state human video generation marking best knowledge first extensive literature review domain start introduction fundamentals human video generation evolution generative models facilitated fields growth examine main methods employed three key subtasks within human video generation textdriven audiodriven posedriven motion generation areas explored concerning conditions guide generation process furthermore offer collection commonly utilized datasets evaluation metrics crucial assessing quality realism generated videos survey concludes discussion current challenges field suggests possible directions future research goal survey offer research community clear holistic view advancements human video generation highlighting milestones achieved challenges lie ahead', 'vbench comprehensive versatile benchmark suite video generative models video generation witnessed significant advancements yet evaluating models remains challenge comprehensive evaluation benchmark video generation indispensable two reasons existing metrics fully align human perceptions ideal evaluation system provide insights inform future developments video generation end present vbench comprehensive benchmark suite dissects video generation quality specific hierarchical disentangled dimensions tailored prompts evaluation methods vbench several appealing properties comprehensive dimensions vbench comprises dimensions video generation eg subject identity inconsistency motion smoothness temporal flickering spatial relationship etc evaluation metrics finegrained levels reveal individual models strengths weaknesses human alignment also provide dataset human preference annotations validate benchmarks alignment human perception evaluation dimension respectively valuable insights look current models ability across various evaluation dimensions various content types also investigate gaps video image generation models versatile benchmarking vbench supports evaluating texttovideo imagetovideo introduce highquality image suite adaptive aspect ratio enable fair evaluations across different imagetovideo generation settings beyond assessing technical quality vbench evaluates trustworthiness video generative models providing holistic view model performance full opensourcing fully opensource vbench continually add new video generation models leaderboard drive forward field video generation']"
11,30,11_video_generation_diffusion_video generation,"['video', 'generation', 'diffusion', 'video generation', 'videos', 'temporal', 'models', 'model', 'tryon', 'quality']","['fashionvdm video diffusion model virtual tryon present fashionvdm video diffusion model vdm generating virtual tryon videos given input garment image person video method aims generate highquality tryon video person wearing given garment preserving persons identity motion imagebased virtual tryon shown impressive results however existing video virtual tryon vvt methods still lacking garment details temporal consistency address issues propose diffusionbased architecture video virtual tryon split classifierfree guidance increased control conditioning inputs progressive temporal training strategy singlepass video generation also demonstrate effectiveness joint imagevideo training video tryon especially video data limited qualitative quantitative experiments show approach sets new stateoftheart video virtual tryon additional results visit project page httpsjohannakarrasgithubiofashionvdm', 'redefining temporal modeling video diffusion vectorized timestep approach diffusion models revolutionized image generation extension video generation shown promise however current video diffusion modelsvdms rely scalar timestep variable applied clip level limits ability model complex temporal dependencies needed various tasks like imagetovideo generation address limitation propose frameaware video diffusion modelfvdm introduces novel vectorized timestep variablevtv unlike conventional vdms approach allows frame follow independent noise schedule enhancing models capacity capture finegrained temporal dependencies fvdms flexibility demonstrated across multiple tasks including standard video generation imagetovideo generation video interpolation long video synthesis diverse set vtv configurations achieve superior quality generated videos overcoming challenges catastrophic forgetting finetuning limited generalizability zeroshot methodsour empirical evaluations show fvdm outperforms stateoftheart methods video generation quality also excelling extended tasks addressing fundamental shortcomings existing vdms fvdm sets new paradigm video synthesis offering robust framework significant implications generative modeling multimedia applications', 'pemfvto pointenhanced video virtual tryon via maskfree paradigm video virtual tryon aims seamlessly transfer reference garment onto target person video preserving visual fidelity temporal coherence existing methods typically rely inpainting masks define tryon area enabling accurate garment transfer simple scenes eg inshop videos however maskbased approaches struggle complex realworld scenarios overly large inconsistent masks often destroy spatialtemporal information leading distorted results maskfree methods alleviate issue face challenges accurately determining tryon area especially videos dynamic body movements address limitations propose pemfvto novel pointenhanced maskfree video virtual tryon framework leverages sparse point alignments explicitly guide garment transfer key innovation introduction pointenhanced guidance provides flexible reliable control spatiallevel garment transfer temporallevel video coherence specifically design pointenhanced transformer pet two core components pointenhanced spatial attention psa uses framecloth point alignments precisely guide garment transfer pointenhanced temporal attention pta leverages frameframe point correspondences enhance temporal coherence ensure smooth transitions across frames extensive experiments demonstrate pemfvto outperforms stateoftheart methods generating natural coherent visually appealing tryon videos particularly challenging inthewild scenarios link papers homepage httpspemfvtogithubio']"
12,29,12_quality_assessment_quality assessment_video quality,"['quality', 'assessment', 'quality assessment', 'video quality', 'video quality assessment', 'video', 'ugc', 'vqa', 'content', 'videos']","['finevq finegrained user generated content video quality assessment rapid growth usergenerated content ugc videos produced urgent need effective video quality assessment vqa algorithms monitor video quality guide optimization recommendation procedures however current vqa models generally give overall rating ugc video lacks finegrained labels serving video processing recommendation applications address challenges promote development ugc videos establish first largescale finegrained video quality assessment database termed finevd comprises ugc videos finegrained quality scores descriptions across multiple dimensions based database propose finegrained video quality assessment finevq model learn finegrained quality ugc videos capabilities quality rating quality scoring quality attribution extensive experimental results demonstrate proposed finevq produce finegrained videoquality results achieve stateoftheart performance finevd commonly used ugcvqa datasets finevd finevq made publicly available', 'benchmarking multidimensional aigc video quality assessment dataset unified model recent years artificial intelligence aidriven video generation gained significant attention consequently growing need accurate video quality assessment vqa metrics evaluate perceptual quality aigenerated content aigc videos optimize video generation models however assessing quality aigc videos remains significant challenge videos often exhibit highly complex distortions unnatural actions irrational objects address challenge systematically investigate aigcvqa problem considering subjective objective quality assessment perspectives subjective perspective construct largescale generated video quality assessment lgvq dataset consisting aigc videos generated video generation models using carefully curated text prompts evaluate perceptual quality aigc videos three critical dimensions spatial quality temporal quality textvideo alignment objective perspective establish benchmark evaluating existing quality assessment metrics lgvq dataset findings show current metrics perform poorly dataset highlighting gap effective evaluation tools bridge gap propose unify generated video quality assessment ugvq model designed accurately evaluate multidimensional quality aigc videos ugvq model integrates visual motion features videos textual features corresponding prompts forming unified qualityaware feature representation tailored aigc videos experimental results demonstrate ugvq achieves stateoftheart performance lgvq dataset across three quality dimensions lgvq dataset ugvq model publicly available httpsgithubcomzczhangsjtuugvqgit', 'perceptual video quality assessment survey perceptual video quality assessment plays vital role field video processing due existence quality degradations introduced various stages video signal acquisition compression transmission display advancement internet communication cloud service technology video content traffic growing exponentially emphasizes requirement accurate rapid assessment video quality therefore numerous subjective objective video quality assessment studies conducted past two decades generic videos specific videos streaming usergenerated content ugc virtual augmented reality vr ar high frame rate hfr audiovisual etc survey provides uptodate comprehensive review video quality assessment studies specifically first review subjective video quality assessment methodologies databases necessary validating performance video quality metrics second objective video quality assessment algorithms general purposes surveyed concluded according methodologies utilized quality measures third overview objective video quality assessment measures specific applications emerging topics finally performances stateoftheart video quality assessment measures compared analyzed survey provides systematic overview classical works recent progresses realm video quality assessment help researchers quickly access field conduct relevant research']"
13,26,13_diffusion_models_image_diffusion models,"['diffusion', 'models', 'image', 'diffusion models', 'generative', 'model', 'data', 'noise', 'images', 'sampling']","['gaussian need unified framework solving inverse problems via diffusion posterior sampling diffusion models generate variety highquality images modeling complex data distributions trained diffusion models also effective image priors solving inverse problems existing diffusionbased methods integrate data consistency steps within diffusion reverse sampling process data consistency steps rely approximate likelihood function paper show existing approximations either insufficient computationally inefficient address issues propose unified likelihood approximation method incorporates covariance correction term enhance performance avoids propagating gradients diffusion model correction term integrated reverse diffusion sampling process achieves better convergence towards true data posterior selected distributions improves performance realworld natural image datasets furthermore present efficient way factorize invert covariance matrix likelihood function several inverse problems present comprehensive experiments demonstrate effectiveness method several existing approaches', 'sequential posterior sampling diffusion models diffusion models quickly risen popularity ability model complex distributions perform effective posterior sampling unfortunately iterative nature generative models makes computationally expensive unsuitable realtime sequential inverse problems ultrasound imaging considering strong temporal structure across sequences frames propose novel approach models transition dynamics improve efficiency sequential diffusion posterior sampling conditional image synthesis modeling sequence data using video vision transformer vivit transition model based previous diffusion outputs initialize reverse diffusion trajectory lower noise scale greatly reducing number iterations required convergence demonstrate effectiveness approach realworld dataset high frame rate cardiac ultrasound images show achieves performance full diffusion trajectory accelerating inference enabling realtime posterior sampling furthermore show addition transition model improves psnr cases severe motion method opens new possibilities realtime applications diffusion models imaging domains requiring realtime inference', 'advancing diffusion models aliasfree resampling enhanced rotational equivariance recent advances image generation particularly via diffusion models led impressive improvements image synthesis quality despite diffusion models still challenged modelinduced artifacts limited stability image fidelity work hypothesize primary cause issue improper resampling operation introduces aliasing diffusion model careful aliasfree resampling dictated image processing theory improve models performance image synthesis propose integration aliasfree resampling layers unet architecture diffusion models without adding extra trainable parameters thereby maintaining computational efficiency assess whether theorydriven modifications enhance image quality rotational equivariance experimental results benchmark datasets including mnist mnistm reveal consistent gains image quality particularly terms fid kid scores furthermore propose modified diffusion process enables usercontrolled rotation generated images without requiring additional training findings highlight potential theorydriven enhancements aliasfree resampling generative models improve image quality maintaining model efficiency pioneer future research directions incorporate videogenerating diffusion models enabling deeper exploration applications aliasfree resampling generative modeling']"
14,17,14_data_images_estimation_synthesis,"['data', 'images', 'estimation', 'synthesis', 'image', 'ir', 'method', 'sensing', 'cloud', 'conditions']","['electrooptical image synthesis sar imagery using generative adversarial networks utility synthetic aperture radar sar imagery remote sensing satellite image analysis well established offering robustness various weather lighting conditions however sar images characterized unique structural texture characteristics often pose interpretability challenges analysts accustomed electrooptical eo imagery application compares stateoftheart generative adversarial networks gans including cyclegan scyclegan novel dualgenerator gan utilizing partial convolutions novel dualgenerator architecture utilizing transformers models designed progressively refine realism translated optical images thereby enhancing visual interpretability sar data demonstrate efficacy approach qualitative quantitative evaluations comparing synthesized eo images actual eo images terms visual fidelity feature preservation results show significant improvements interpretability making sar data accessible analysts familiar eo imagery furthermore explore potential technology various applications including environmental monitoring urban planning military reconnaissance rapid accurate interpretation sar data crucial research contributes field remote sensing bridging gap sar eo imagery offering novel tool enhanced data interpretation broader application sar technology various domains', 'sustechgan image generation object detection adverse conditions autonomous driving autonomous driving significantly benefits datadriven deep neural networks however data autonomous driving typically fits longtailed distribution critical driving data adverse conditions hard collect although generative adversarial networks gans applied augment data autonomous driving generating driving images adverse conditions still challenging work propose novel framework sustechgan customized dual attention modules multiscale generators novel loss function generate driving images improving object detection autonomous driving adverse conditions test sustechgan wellknown gans generate driving images adverse conditions rain night apply generated images retrain object detection networks specifically add generated images training datasets retrain wellknown evaluate improvement retrained object detection adverse conditions experimental results show generated driving images sustechgan significantly improved performance retrained rain night conditions outperforms wellknown gans opensource code video description datasets available page facilitate image generation development autonomous driving adverse conditions', 'pgcs physical law embedded generative cloud synthesis remote sensing images data quantity quality critical information extraction analyzation remote sensing however current remote sensing datasets often fail meet two requirements cloud primary factor degrading data quantity quality limitation affects precision results remote sensing application particularly derived datadriven techniques paper physical law embedded generative cloud synthesis method pgcs proposed generate diverse realistic cloud images enhance real data promote development algorithms subsequent tasks cloud correction cloud detection data augmentation classification recognition segmentation pgcs method involves two key phases spatial synthesis spectral synthesis spatial synthesis phase stylebased generative adversarial network utilized simulate spatial characteristics generating infinite number singlechannel clouds spectral synthesis phase atmospheric scattering law embedded local statistics global fitting method converting singlechannel clouds multispectral clouds experimental results demonstrate pgcs achieves high accuracy phases performs better three existing cloud synthesis methods two cloud correction methods developed pgcs exhibits superior performance compared stateoftheart methods cloud correction task furthermore application pgcs data various sensors investigated successfully extended code provided httpsgithubcomliyingxupgcs']"
15,16,15_editing_video_generation_agent,"['editing', 'video', 'generation', 'agent', 'users', 'multimodal', 'user', 'videos', 'video generation', 'language']","['raccoon versatile instructional video editing framework autogenerated narratives recent video generative models primarily rely carefully written text prompts specific tasks like inpainting style editing require laborintensive textual descriptions input videos hindering flexibility adapt personalraw videos user specifications paper proposes raccoon versatile userfriendly videotoparagraphtovideo generative framework supports multiple video editing capabilities removal addition modification unified pipeline raccoon consists two principal stages videotoparagraph paragraphtovideo stage automatically describe video scenes wellstructured natural language capturing holistic context focused object details subsequently stage users optionally refine descriptions guide video diffusion model enabling various modifications input video removing changing subjects andor adding new objects proposed approach stands methods several significant contributions raccoon suggests multigranular spatiotemporal pooling strategy generate wellstructured video descriptions capturing broad context object details without requiring complex human annotations simplifying precise video content editing based text users video generative model incorporates autogenerated narratives instructions enhance quality accuracy generated content raccoon also plans imagine new objects given video users simply prompt model receive detailed video editing plan complex video editing proposed framework demonstrates impressive versatile capabilities videotoparagraph generation video content editing incorporated sota video generative models enhancement', 'expressedit video editing natural language sketching informational videos serve crucial source explaining conceptual procedural knowledge novices experts alike producing informational videos editors edit videos overlaying textimages trimming footage enhance video quality make engaging however video editing difficult timeconsuming especially novice video editors often struggle expressing implementing editing ideas address challenge first explored multimodalitynatural language nl sketching natural modalities humans use expressioncan utilized support video editors expressing video editing ideas gathered multimodal expressions editing commands video editors revealed patterns use nl sketching describing edit intents based findings present expressedit system enables editing videos via nl text sketching video frame powered llm vision models system interprets temporal spatial operational references nl command spatial references sketching system implements interpreted edits user iterate observational study showed expressedit enhanced ability novice video editors express implement edit ideas system allowed participants perform edits efficiently generate ideas generating edits based users multimodal edit commands supporting iterations editing commands work offers insights design future multimodal interfaces aibased pipelines video editing', 'spagent adaptive task decomposition model selection general video generation editing opensource video generation editing models made significant progress individual models typically limited specific tasks failing meet diverse needs users effectively coordinating models unlock wide range video generation editing capabilities however manual coordination complex timeconsuming requiring users deeply understand task requirements possess comprehensive knowledge models performance applicability limitations thereby increasing barrier entry address challenges propose novel video generation editing system powered semantic planning agent spagent spagent bridges gap diverse user intents effective utilization existing generative models enhancing adaptability efficiency overall quality video generation editing specifically spagent assembles tool library integrating stateoftheart opensource image video generation editing models tools finetuning manually annotated dataset spagent automatically coordinate tools video generation editing novelly designed threestep framework decoupled intent recognition principleguided route planning capabilitybased execution model selection additionally enhance spagents video quality evaluation capability enabling autonomously assess incorporate new video generation editing models tool library without human intervention experimental results demonstrate spagent effectively coordinates models generate edit videos highlighting versatility adaptability across various video tasks']"
16,16,16_driving_autonomous_autonomous driving_world,"['driving', 'autonomous', 'autonomous driving', 'world', 'driving videos', 'simulation', 'generation', 'video generation', 'world models', 'videos']","['world models effective data machines driving scene representation closedloop simulation essential advancing endtoend autonomous driving systems contemporary sensor simulation methods nerf rely predominantly conditions closely aligned training data distributions largely confined forwarddriving scenarios consequently methods face limitations rendering complex maneuvers eg lane change acceleration deceleration recent advancements autonomousdriving world models demonstrated potential generate diverse driving videos however approaches remain constrained video generation inherently lacking spatiotemporal coherence required capture intricacies dynamic driving environments paper introduce enhances driving scene representation leveraging world model priors specifically utilize world model data machine synthesize novel trajectory videos structured conditions explicitly leveraged control spatialtemporal consistency traffic elements besides cousin data training strategy proposed facilitate merging real synthetic data optimizing knowledge first utilize video generation models improving reconstruction driving scenarios experimental results reveal significantly enhances generation quality novel trajectory views achieving relative improvement fid compared pvg deformablegs moreover markedly enhances spatiotemporal coherence driving agents verified comprehensive user study relative increases ntaiou metric', 'exploring interplay video generation world models autonomous driving survey world models video generation pivotal technologies domain autonomous driving playing critical role enhancing robustness reliability autonomous systems world models simulate dynamics realworld environments video generation models produce realistic video sequences increasingly integrated improve situational awareness decisionmaking capabilities autonomous vehicles paper investigates relationship two technologies focusing structural parallels particularly diffusionbased models contribute accurate coherent simulations driving scenarios examine leading works jepa genie sora exemplify different approaches world model design thereby highlighting lack universally accepted definition world models diverse interpretations underscore fields evolving understanding world models optimized various autonomous driving tasks furthermore paper discusses key evaluation metrics employed domain chamfer distance scene reconstruction frechet inception distance fid assessing quality generated video content analyzing interplay video generation world models survey identifies critical challenges future research directions emphasizing potential technologies jointly advance performance autonomous driving systems findings presented paper aim provide comprehensive understanding integration video generation world models drive innovation development safer reliable autonomous vehicles', 'llmenhanced world models diverse driving video generation world models demonstrated superiority autonomous driving particularly generation multiview driving videos however significant challenges still exist generating customized driving videos paper propose builds upon framework drivedreamer incorporates large language model llm generate userdefined driving videos specifically llm interface initially incorporated convert users query agent trajectories subsequently hdmap adhering traffic regulations generated based trajectories ultimately propose unified multiview model enhance temporal spatial coherence generated driving videos first world model generate customized driving videos generate uncommon driving videos eg vehicles abruptly cut userfriendly manner besides experimental results demonstrate generated videos enhance training driving perception methods eg detection tracking furthermore video generation quality surpasses stateoftheart methods showcasing fid fvd scores representing relative improvements']"
17,13,17_gans_training_adversarial networks_adversarial,"['gans', 'training', 'adversarial networks', 'adversarial', 'gan', 'networks', 'images', 'generative adversarial', 'generative adversarial networks', 'generative']","['creative portraiture exploring creative adversarial networks conditional creative adversarial networks convolutional neural networks cnns combined generative adversarial networks gans create deep convolutional generative adversarial networks dcgans great success dcgans used generating images videos creative domains fashion design painting common critique use dcgans creative applications limited ability generate creative products generator simply learns copy training distribution explore extension dcgans creative adversarial networks cans using cans generate novel creative portraits using wikiart dataset train network moreover introduce extension cans conditional creative adversarial networks ccans demonstrate potential generate creative portraits conditioned style label argue generating products conditioned inspired style label closely emulates real creative processes humans produce imaginative work still rooted previous styles', 'mcgan enhancing gan training regressionbased generator loss generative adversarial networks gans emerged powerful tool generating highfidelity data however main bottleneck existing approaches lack supervision generator training often results undamped oscillation unsatisfactory performance address issue propose algorithm called monte carlo gan mcgan approach utilizing innovative generative loss function termly regression loss reformulates generator training regression task enables generator training minimizing mean squared error discriminators output real data expected discriminator fake data demonstrate desirable analytic properties regression loss including discriminability optimality show method requires weaker condition discriminator effective generator training properties justify strength approach improve training stability retaining optimality gan leveraging strong supervision regression loss extensive experiments diverse datasets including image data imagenet lsun bedroom time series data var stock data video data conducted demonstrate flexibility effectiveness proposed mcgan numerical results show proposed mcgan versatile enhancing variety backbone gan models achieves consistent significant improvement terms quality accuracy training stability learned latent space', 'early stopping criteria training generative adversarial networks biomedical imaging generative adversarial networks gans high computational costs train complex architectures throughout training process gans output analyzed qualitatively based loss synthetic images diversity quality based qualitative analysis training manually halted desired synthetic images generated utilizing early stopping criterion computational cost dependence manual oversight reduced yet impacted training problems mode collapse nonconvergence instability particularly prevalent biomedical imagery training problems degrade diversity quality synthetic images high computational cost associated training makes complex architectures increasingly inaccessible work proposes novel early stopping criteria quantitatively detect training problems halt training reduce computational costs associated synthesizing biomedical images firstly range generator discriminator loss values investigated assess whether mode collapse nonconvergence instability occur sequentially concurrently interchangeably throughout training gans secondly utilizing occurrences conjunction mean structural similarity index msssim frechet inception distance fid scores synthetic images forms basis proposed early stopping criteria work helps identify occurrence training problems gans using lowresource computational cost reduces training time generate diversified highquality synthetic images']"
18,13,18_motion_motions_scene_objects,"['motion', 'motions', 'scene', 'objects', 'human', 'human motion', 'object', 'generation', 'realistic', 'dynamics']","['physicsbased scene layout generation human motion creating scenes captured motions achieve realistic humanscene interaction crucial animation movies video games character motion often captured bluescreened studio without real furniture objects place may discrepancy planned motion captured one gives rise need automatic scene layout generation relieve burdens selecting positioning furniture objects previous approaches avoid artifacts like penetration floating due lack physical constraints furthermore heavily rely specific data learn contact affordances restricting generalization ability different motions work present physicsbased approach simultaneously optimizes scene layout generator simulates moving human physics simulator attain plausible realistic interaction motions method explicitly introduces physical constraints automatically recover generate scene layout minimize motion tracking errors identify objects afford interaction use reinforcement learning perform dualoptimization character motion imitation controller scene layout generator facilitate optimization reshape tracking rewards devise pose prior guidance obtained estimated pseudocontact labels evaluate method using motions samp prox demonstrate physically plausible scene layout reconstruction compared previous kinematicsbased method', 'motion dreamer boundary conditional motion reasoning physically coherent video generation recent advances video generation shown promise generating future scenarios critical planning control autonomous driving embodied intelligence however realworld applications demand visually plausible predictions require reasoning object motions based explicitly defined boundary conditions initial scene image partial object motion term capability boundary conditional motion reasoning current approaches either neglect explicit userdefined motion constraints producing physically inconsistent motions conversely demand complete motion inputs rarely available practice introduce motion dreamer twostage framework explicitly separates motion reasoning visual synthesis addressing limitations approach introduces instance flow sparsetodense motion representation enabling effective integration partial userdefined motions motion inpainting strategy robustly enable reasoning motions objects extensive experiments demonstrate motion dreamer significantly outperforms existing methods achieving superior motion plausibility visual realism thus bridging gap towards practical boundary conditional motion reasoning webpage available httpsenvisionresearchgithubiomotiondreamer', 'leveraging motion data boost motion generation textdriven human motion synthesis capturing significant attention ability effortlessly generate intricate movements abstract text cues showcasing potential revolutionizing motion design film narratives also virtual reality experiences computer game development existing methods often rely motion capture data require special setups resulting higher costs data acquisition ultimately limiting diversity scope human motion contrast human videos offer vast accessible source motion data covering wider range styles activities paper explore leveraging human motion extracted videos alternative data source improve textdriven motion generation approach introduces novel framework disentangles local joint motion global movements enabling efficient learning local motion priors data first train singleview local motion generator large dataset textmotion pairs enhance model synthesize motion finetune generator data transforming multiview generator predicts viewconsistent local joint motion root dynamics experiments dataset novel text prompts demonstrate method efficiently utilizes data supporting realistic human motion generation broadening range motion types supports code made publicly available']"
19,12,19_ffa_fundus_images_fluorescein,"['ffa', 'fundus', 'images', 'fluorescein', 'retinal', 'angiography', 'clinical', 'disease', 'fluorescein angiography', 'polyp']","['fundus fluorescein angiography video generation retinal generative foundation model fundus fluorescein angiography ffa crucial diagnosing monitoring retinal vascular issues limited invasive nature restricted accessibility compared color fundus cf imaging existing methods convert cf images ffa confined static image generation missing dynamic lesional changes introduce autoregressive generative adversarial network gan model generates dynamic ffa videos single cf images excels video generation achieving fvd psnr clinical experts validated fidelity generated videos additionally models generator demonstrates remarkable downstream transferability across ten external public datasets including blood vessel segmentation retinal disease diagnosis systemic disease prediction multimodal retrieval showcasing impressive zeroshot fewshot capabilities findings position powerful noninvasive alternative ffa exams versatile retinal generative foundation model captures static temporal retinal features enabling representation complex intermodality relationships', 'crossmodal angiography video generation static fundus photography clinical knowledge guidance fundus fluorescein angiography ffa critical tool assessing retinal vascular dynamics aiding diagnosis eye diseases however invasive nature less accessibility compared color fundus cf images pose significant challenges current cf ffa translation methods limited static generation work pioneer dynamic ffa video generation static cf images introduce autoregressive gan smooth memorysaving framebyframe ffa synthesis enhance focus dynamic lesion changes ffa regions design knowledge mask based clinical experience leveraging mask approach integrates innovative knowledge maskguided techniques including knowledgeboosted attention knowledgeaware discriminators maskenhanced patchnce loss aimed refining generation critical areas addressing pixel misalignment challenge method achieves best fvd psnr compared common video generation approaches human assessment ophthalmologist confirms high generation quality notably knowledge mask surpasses supervised lesion segmentation masks offering promising noninvasive alternative traditional ffa research clinical applications code available', 'noninvasive invasive enhancing ffa synthesis cfp benchmark dataset novel network fundus imaging pivotal tool ophthalmology different imaging modalities characterized specific advantages example fundus fluorescein angiography ffa uniquely provides detailed insights retinal vascular dynamics pathology surpassing color fundus photographs cfp detecting microvascular abnormalities perfusion status however conventional invasive ffa involves discomfort risks due fluorescein dye injection meaningful challenging synthesize ffa images noninvasive cfp previous studies primarily focused ffa synthesis single disease category work explore ffa synthesis multiple diseases devising diffusionguided generative adversarial network introduces adaptive dynamic diffusion forward process discriminator adds categoryaware representation enhancer moreover facilitate research collect first multidisease cfp ffa paired dataset named multidisease paired ocular synthesis mpos dataset four different fundus diseases experimental results show ffa synthesis network generate better ffa images compared stateoftheart methods furthermore introduce pairedmodal diagnostic network validate effectiveness synthetic ffa images diagnosis multiple fundus diseases results show synthesized ffa images real cfp images higher diagnosis accuracy compared ffa synthesizing methods research bridges gap noninvasive imaging ffa thereby offering promising prospects enhance ophthalmic diagnosis patient care focus reducing harm patients noninvasive procedures dataset code released support research field httpsgithubcomwhqxxhffasynthesis']"
20,10,20_quantization_diffusion_diffusion transformers_transformers,"['quantization', 'diffusion', 'diffusion transformers', 'transformers', 'caching', 'video', 'dit', 'across', 'acceleration', 'model outputs']","['timestep embedding tells time cache video diffusion model fundamental backbone video generation diffusion models challenged low inference speed due sequential nature denoising previous methods speed models caching reusing model outputs uniformly selected timesteps however strategy neglects fact differences among model outputs uniform across timesteps hinders selecting appropriate model outputs cache leading poor balance inference efficiency visual quality study introduce timestep embedding aware cache teacache trainingfree caching approach estimates leverages fluctuating differences among model outputs across timesteps rather directly using timeconsuming model outputs teacache focuses model inputs strong correlation modeloutputs incurring negligible computational cost teacache first modulates noisy inputs using timestep embeddings ensure differences better approximating model outputs teacache introduces rescaling strategy refine estimated differences utilizes indicate output caching experiments show teacache achieves acceleration opensoraplan negligible vbench score degradation visual quality', 'qvd posttraining quantization video diffusion models recently video diffusion models vdms garnered significant attention due notable advancements generating coherent realistic video content however processing multiple frame features concurrently coupled considerable model size results high latency extensive memory consumption hindering broader application posttraining quantization ptq effective technique reduce memory footprint improve computational efficiency unlike image diffusion observe temporal features integrated frame features exhibit pronounced skewness furthermore investigate significant interchannel disparities asymmetries activation video diffusion models resulting low coverage quantization levels individual channels increasing challenge quantization address issues introduce first ptq strategy tailored video diffusion models dubbed qvd specifically propose high temporal discriminability quantization htdq method designed temporal features retains high discriminability quantized features providing precise temporal guidance video frames addition present scattered channel range integration scri method aims improve coverage quantization levels across individual channels experimental validations across various models datasets bitwidth settings demonstrate effectiveness qvd terms diverse metrics particular achieve nearlossless performance degradation outperforming current methods fvd', 'viditq efficient accurate quantization diffusion transformers image video generation diffusion transformers demonstrated remarkable performance visual generation tasks generating realistic images videos based textual instructions however larger model sizes multiframe processing video generation lead increased computational memory costs posing challenges practical deployment edge devices posttraining quantization ptq effective method reducing memory costs computational complexity quantizing diffusion transformers find existing quantization methods face challenges applied texttoimage video tasks address challenges begin systematically analyzing source quantization error conclude unique challenges posed dit quantization accordingly design improved quantization scheme viditq video image diffusion transformer quantization tailored specifically dit models validate effectiveness viditq across variety texttoimage video models achieving negligible degradation visual quality metrics additionally implement efficient gpu kernels achieve practical memory saving endtoend latency speedup']"
