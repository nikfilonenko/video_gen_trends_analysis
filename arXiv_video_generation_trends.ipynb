{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Выполнил: Филоненко Никита Дмитриевич\n",
    "\n",
    "-----"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc4b781cec399d0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "QR-код (ссылка на репозиторий с проектом)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccec38a280fcaa72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Оглавление\n",
    "\n",
    "[Описание тестового задания](#0.1)\n",
    "1. [Сбор данных через arXiv API](#01)\n",
    "2. [EDA](#02)\n",
    "    - [Получение общего представления о данных](#02.1)\n",
    "    - [Предобработка текста для анализа](#02.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e730f6ad4a27b8da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Описание тестового задания <a name=\"0.1\"></a>\n",
    "\n",
    "Задача - выделить основные тренды в видео генерации в 2024 году.\n",
    "\n",
    "1. Получить метаданные статей с arXiv (любым способом) по теме “видео генерация”, выпущенных в 2024 году.\n",
    "    \n",
    "2. Написать код для выделения основных тем/трендов.\n",
    "    \n",
    "- Будет плюсом, если каждая тема будет представлена связным словосочетанием/предложением\n",
    "\n",
    "3. Представить визуализацию результатов.\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89e96e23aa4dd2cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Решение"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0da5da385085dad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Импортируем необходимые модули "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f55a218475f9d1d"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T21:33:08.029670300Z",
     "start_time": "2025-03-27T21:33:08.020210100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arxiv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "для получения метаданных статей из arXiv будем использовать его API, вместо написания парсера\n",
    "\n",
    "поскольку:\n",
    "\n",
    "- у arXiv нет жестких ограничений, платных тарифов\n",
    "\n",
    "- API предоставляет доступ ко всему необходимому для выполнения данного задания\n",
    "\n",
    "- лимит в 10 000 результатов на запрос – этого более чем достаточно\n",
    "\n",
    "- соответственно нет жесткого rate limit, если не спамить слишком часто\n",
    "\n",
    "- использование API избавит от лишней предобработки текста (например html теги)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49865790e1e6574a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Сбор данных через arXiv API <a name=\"01\"></a>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bee0b4f75422dab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "будем использовать библиотеку arxiv, которая предоставляет удобный высокоуровневый интерфейс для работы с API arXiv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "299371dc12cad8d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query = (\n",
    "    'all:video generation OR all:video synthesis OR '\n",
    "    'all:diffusion video OR all:GAN video OR '\n",
    "    'all:text-to-video OR all:image-to-video OR '\n",
    "    'all:scene generation '\n",
    "    'AND submittedDate:[20240101 TO 20241231]'\n",
    ")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=150000,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    "    sort_order=arxiv.SortOrder.Ascending\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T21:58:48.053392200Z",
     "start_time": "2025-03-27T21:58:48.036233600Z"
    }
   },
   "id": "ac1563f1997cf59",
   "execution_count": 174
  },
  {
   "cell_type": "markdown",
   "source": [
    "выполним ленивую загрузку метаданных, создав генератор через `client.results(search)`, благодаря чему с экономим на оперативной памяти и времени начальной загрузки данных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18239277276eee9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "также, учитывая тот факт, что arXiv API не позволяет фильтровать запросы по конкретному году (в нашем случае — 2024), при формировании набора данных дополнительно проверим соответствие года публикации, чтобы гарантировать включение только статей, опубликованных за 2024 год"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "715bc27c216987f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Достигнут предел доступных результатов\n"
     ]
    }
   ],
   "source": [
    "client = arxiv.Client(\n",
    "    page_size=500,\n",
    "    delay_seconds=5,\n",
    "    num_retries=5\n",
    ")\n",
    "\n",
    "papers = []\n",
    "\n",
    "current_year = 2024\n",
    "\n",
    "try:\n",
    "    for result in client.results(search):\n",
    "        if result.published.year == current_year:\n",
    "            papers.append({\n",
    "                \"title\": result.title,\n",
    "                \"abstract\": result.summary,\n",
    "                \"published\": result.published,\n",
    "                \"authors\": [a.name for a in result.authors],\n",
    "                \"url\": result.entry_id\n",
    "            })\n",
    "except arxiv.UnexpectedEmptyPageError:\n",
    "    print(\"Достигнут предел доступных результатов\")\n",
    "\n",
    "df = pd.DataFrame(papers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T21:59:44.328078600Z",
     "start_time": "2025-03-27T21:58:48.918883600Z"
    }
   },
   "id": "e0d687b94bf27c83",
   "execution_count": 175
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сколько всего статей 2024 года получено: \u001B[32m2100\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore, Style\n",
    "\n",
    "print(f\"Сколько всего статей 2024 года получено: {Fore.GREEN}{len(df)}{Style.RESET_ALL}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:53.332764700Z",
     "start_time": "2025-03-27T22:33:53.312985900Z"
    }
   },
   "id": "a4c5fa71331a6cee",
   "execution_count": 203
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  title  \\\n0     Video to Video Generative Adversarial Network ...   \n1     Contrastive Sequential-Diffusion Learning: Non...   \n2     ReCapture: Generative Video Camera Controls fo...   \n3     StreamingT2V: Consistent, Dynamic, and Extenda...   \n4     FreeLong: Training-Free Long Video Generation ...   \n...                                                 ...   \n2095  Recording dynamic facial micro-expressions wit...   \n2096  FLAASH: Flow-Attention Adaptive Semantic Hiera...   \n2097  MSEG-VCUQ: Multimodal SEGmentation with Enhanc...   \n2098  Principles of Visual Tokens for Efficient Vide...   \n2099  Self-supervised Video Instance Segmentation Ca...   \n\n                                               abstract  \\\n0     The development of sophisticated models for vi...   \n1     Generated video scenes for action-centric sequ...   \n2     Recently, breakthroughs in video modeling have...   \n3     Text-to-video diffusion models enable the gene...   \n4     Video diffusion models have made substantial p...   \n...                                                 ...   \n2095  We present an approach of utilizing a multi-ca...   \n2096  The proliferation of tobacco-related content o...   \n2097  High-speed video (HSV) phase detection (PD) se...   \n2098  Video understanding has made huge strides in r...   \n2099  Tracking geographic entities from historical m...   \n\n                     published  \\\n0    2024-10-28 01:35:10+00:00   \n1    2024-07-16 15:03:05+00:00   \n2    2024-11-07 18:59:45+00:00   \n3    2024-03-21 18:27:29+00:00   \n4    2024-07-29 11:52:07+00:00   \n...                        ...   \n2095 2024-10-02 19:30:21+00:00   \n2096 2024-10-25 17:20:22+00:00   \n2097 2024-11-12 00:54:26+00:00   \n2098 2024-11-20 14:09:47+00:00   \n2099 2024-11-26 13:31:51+00:00   \n\n                                                authors  \\\n0                 [Yintai Ma, Diego Klabjan, Jean Utke]   \n1     [Vasco Ramos, Yonatan Bitton, Michal Yarom, Id...   \n2     [David Junhao Zhang, Roni Paiss, Shiran Zada, ...   \n3     [Roberto Henschel, Levon Khachatryan, Daniil H...   \n4          [Yu Lu, Yuanzhi Liang, Linchao Zhu, Yi Yang]   \n...                                                 ...   \n2095  [Lucas Kreiss, Weiheng Tang, Ramana Balla, Xi ...   \n2096  [Naga VS Raviteja Chappa, Page Daniel Dobbs, B...   \n2097  [Chika Maduabuchi, Ericmoore Jossou, Matteo Bu...   \n2098  [Xinyue Hao, Gen Li, Shreyank N Gowda, Robert ...   \n2099  [Xue Xia, Randall Balestriero, Tao Zhang, Lore...   \n\n                                    url  \n0     http://arxiv.org/abs/2410.20657v1  \n1     http://arxiv.org/abs/2407.11814v3  \n2     http://arxiv.org/abs/2411.05003v1  \n3     http://arxiv.org/abs/2403.14773v1  \n4     http://arxiv.org/abs/2407.19918v1  \n...                                 ...  \n2095  http://arxiv.org/abs/2410.01973v1  \n2096  http://arxiv.org/abs/2410.19896v2  \n2097  http://arxiv.org/abs/2411.07463v4  \n2098  http://arxiv.org/abs/2411.13626v2  \n2099  http://arxiv.org/abs/2411.17425v1  \n\n[2100 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>published</th>\n      <th>authors</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Video to Video Generative Adversarial Network ...</td>\n      <td>The development of sophisticated models for vi...</td>\n      <td>2024-10-28 01:35:10+00:00</td>\n      <td>[Yintai Ma, Diego Klabjan, Jean Utke]</td>\n      <td>http://arxiv.org/abs/2410.20657v1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Contrastive Sequential-Diffusion Learning: Non...</td>\n      <td>Generated video scenes for action-centric sequ...</td>\n      <td>2024-07-16 15:03:05+00:00</td>\n      <td>[Vasco Ramos, Yonatan Bitton, Michal Yarom, Id...</td>\n      <td>http://arxiv.org/abs/2407.11814v3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ReCapture: Generative Video Camera Controls fo...</td>\n      <td>Recently, breakthroughs in video modeling have...</td>\n      <td>2024-11-07 18:59:45+00:00</td>\n      <td>[David Junhao Zhang, Roni Paiss, Shiran Zada, ...</td>\n      <td>http://arxiv.org/abs/2411.05003v1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>StreamingT2V: Consistent, Dynamic, and Extenda...</td>\n      <td>Text-to-video diffusion models enable the gene...</td>\n      <td>2024-03-21 18:27:29+00:00</td>\n      <td>[Roberto Henschel, Levon Khachatryan, Daniil H...</td>\n      <td>http://arxiv.org/abs/2403.14773v1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FreeLong: Training-Free Long Video Generation ...</td>\n      <td>Video diffusion models have made substantial p...</td>\n      <td>2024-07-29 11:52:07+00:00</td>\n      <td>[Yu Lu, Yuanzhi Liang, Linchao Zhu, Yi Yang]</td>\n      <td>http://arxiv.org/abs/2407.19918v1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2095</th>\n      <td>Recording dynamic facial micro-expressions wit...</td>\n      <td>We present an approach of utilizing a multi-ca...</td>\n      <td>2024-10-02 19:30:21+00:00</td>\n      <td>[Lucas Kreiss, Weiheng Tang, Ramana Balla, Xi ...</td>\n      <td>http://arxiv.org/abs/2410.01973v1</td>\n    </tr>\n    <tr>\n      <th>2096</th>\n      <td>FLAASH: Flow-Attention Adaptive Semantic Hiera...</td>\n      <td>The proliferation of tobacco-related content o...</td>\n      <td>2024-10-25 17:20:22+00:00</td>\n      <td>[Naga VS Raviteja Chappa, Page Daniel Dobbs, B...</td>\n      <td>http://arxiv.org/abs/2410.19896v2</td>\n    </tr>\n    <tr>\n      <th>2097</th>\n      <td>MSEG-VCUQ: Multimodal SEGmentation with Enhanc...</td>\n      <td>High-speed video (HSV) phase detection (PD) se...</td>\n      <td>2024-11-12 00:54:26+00:00</td>\n      <td>[Chika Maduabuchi, Ericmoore Jossou, Matteo Bu...</td>\n      <td>http://arxiv.org/abs/2411.07463v4</td>\n    </tr>\n    <tr>\n      <th>2098</th>\n      <td>Principles of Visual Tokens for Efficient Vide...</td>\n      <td>Video understanding has made huge strides in r...</td>\n      <td>2024-11-20 14:09:47+00:00</td>\n      <td>[Xinyue Hao, Gen Li, Shreyank N Gowda, Robert ...</td>\n      <td>http://arxiv.org/abs/2411.13626v2</td>\n    </tr>\n    <tr>\n      <th>2099</th>\n      <td>Self-supervised Video Instance Segmentation Ca...</td>\n      <td>Tracking geographic entities from historical m...</td>\n      <td>2024-11-26 13:31:51+00:00</td>\n      <td>[Xue Xia, Randall Balestriero, Tao Zhang, Lore...</td>\n      <td>http://arxiv.org/abs/2411.17425v1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2100 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T18:30:14.765641100Z",
     "start_time": "2025-03-27T18:30:14.738636700Z"
    }
   },
   "id": "b9bab68999aef083",
   "execution_count": 137
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.to_csv(\"data/data_arxiv_video_generation_2024.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T18:31:02.759565300Z",
     "start_time": "2025-03-27T18:31:02.722723500Z"
    }
   },
   "id": "633936c69dd9931f",
   "execution_count": 148
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. EDA <a name=\"02\"></a>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6446110c0919bee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1 общее представление полученных данных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7db3a979cffbbe2c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(2100, 5)"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/data_arxiv_video_generation_2024.csv', \n",
    "    sep=',',\n",
    "    engine='python',\n",
    "    encoding='utf'\n",
    ")\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:36.371076Z",
     "start_time": "2025-03-27T22:33:36.337481300Z"
    }
   },
   "id": "998292ac6913a380",
   "execution_count": 190
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                               title  \\\n0  Video to Video Generative Adversarial Network ...   \n1  Contrastive Sequential-Diffusion Learning: Non...   \n2  ReCapture: Generative Video Camera Controls fo...   \n3  StreamingT2V: Consistent, Dynamic, and Extenda...   \n4  FreeLong: Training-Free Long Video Generation ...   \n\n                                            abstract  \\\n0  The development of sophisticated models for vi...   \n1  Generated video scenes for action-centric sequ...   \n2  Recently, breakthroughs in video modeling have...   \n3  Text-to-video diffusion models enable the gene...   \n4  Video diffusion models have made substantial p...   \n\n                   published  \\\n0  2024-10-28 01:35:10+00:00   \n1  2024-07-16 15:03:05+00:00   \n2  2024-11-07 18:59:45+00:00   \n3  2024-03-21 18:27:29+00:00   \n4  2024-07-29 11:52:07+00:00   \n\n                                             authors  \\\n0        ['Yintai Ma', 'Diego Klabjan', 'Jean Utke']   \n1  ['Vasco Ramos', 'Yonatan Bitton', 'Michal Yaro...   \n2  ['David Junhao Zhang', 'Roni Paiss', 'Shiran Z...   \n3  ['Roberto Henschel', 'Levon Khachatryan', 'Dan...   \n4  ['Yu Lu', 'Yuanzhi Liang', 'Linchao Zhu', 'Yi ...   \n\n                                 url  \n0  http://arxiv.org/abs/2410.20657v1  \n1  http://arxiv.org/abs/2407.11814v3  \n2  http://arxiv.org/abs/2411.05003v1  \n3  http://arxiv.org/abs/2403.14773v1  \n4  http://arxiv.org/abs/2407.19918v1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>published</th>\n      <th>authors</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Video to Video Generative Adversarial Network ...</td>\n      <td>The development of sophisticated models for vi...</td>\n      <td>2024-10-28 01:35:10+00:00</td>\n      <td>['Yintai Ma', 'Diego Klabjan', 'Jean Utke']</td>\n      <td>http://arxiv.org/abs/2410.20657v1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Contrastive Sequential-Diffusion Learning: Non...</td>\n      <td>Generated video scenes for action-centric sequ...</td>\n      <td>2024-07-16 15:03:05+00:00</td>\n      <td>['Vasco Ramos', 'Yonatan Bitton', 'Michal Yaro...</td>\n      <td>http://arxiv.org/abs/2407.11814v3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ReCapture: Generative Video Camera Controls fo...</td>\n      <td>Recently, breakthroughs in video modeling have...</td>\n      <td>2024-11-07 18:59:45+00:00</td>\n      <td>['David Junhao Zhang', 'Roni Paiss', 'Shiran Z...</td>\n      <td>http://arxiv.org/abs/2411.05003v1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>StreamingT2V: Consistent, Dynamic, and Extenda...</td>\n      <td>Text-to-video diffusion models enable the gene...</td>\n      <td>2024-03-21 18:27:29+00:00</td>\n      <td>['Roberto Henschel', 'Levon Khachatryan', 'Dan...</td>\n      <td>http://arxiv.org/abs/2403.14773v1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FreeLong: Training-Free Long Video Generation ...</td>\n      <td>Video diffusion models have made substantial p...</td>\n      <td>2024-07-29 11:52:07+00:00</td>\n      <td>['Yu Lu', 'Yuanzhi Liang', 'Linchao Zhu', 'Yi ...</td>\n      <td>http://arxiv.org/abs/2407.19918v1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:37.347201500Z",
     "start_time": "2025-03-27T22:33:37.326197600Z"
    }
   },
   "id": "f17331d6c631a46b",
   "execution_count": 191
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2100 entries, 0 to 2099\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      2100 non-null   object\n",
      " 1   abstract   2100 non-null   object\n",
      " 2   published  2100 non-null   object\n",
      " 3   authors    2100 non-null   object\n",
      " 4   url        2100 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 82.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:37.535007800Z",
     "start_time": "2025-03-27T22:33:37.523956300Z"
    }
   },
   "id": "1f85c57ff0e249b1",
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "title        object\nabstract     object\npublished    object\nauthors      object\nurl          object\ndtype: object"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:37.716461100Z",
     "start_time": "2025-03-27T22:33:37.712460900Z"
    }
   },
   "id": "2ce11b300180a433",
   "execution_count": 193
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "np.int64(0)"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated(keep='first').sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:37.938617200Z",
     "start_time": "2025-03-27T22:33:37.923617400Z"
    }
   },
   "id": "fb2e70af3974473b",
   "execution_count": 194
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    title  \\\ncount                                                2100   \nunique                                               2100   \ntop     Self-supervised Video Instance Segmentation Ca...   \nfreq                                                    1   \n\n                                                 abstract  \\\ncount                                                2100   \nunique                                               2100   \ntop     Tracking geographic entities from historical m...   \nfreq                                                    1   \n\n                        published  \\\ncount                        2100   \nunique                       2098   \ntop     2024-03-26 17:59:52+00:00   \nfreq                            2   \n\n                                                  authors  \\\ncount                                                2100   \nunique                                               2091   \ntop     ['Dengsheng Chen', 'Jie Hu', 'Xiaoming Wei', '...   \nfreq                                                    2   \n\n                                      url  \ncount                                2100  \nunique                               2100  \ntop     http://arxiv.org/abs/2411.17425v1  \nfreq                                    1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>published</th>\n      <th>authors</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2100</td>\n      <td>2100</td>\n      <td>2100</td>\n      <td>2100</td>\n      <td>2100</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>2100</td>\n      <td>2100</td>\n      <td>2098</td>\n      <td>2091</td>\n      <td>2100</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Self-supervised Video Instance Segmentation Ca...</td>\n      <td>Tracking geographic entities from historical m...</td>\n      <td>2024-03-26 17:59:52+00:00</td>\n      <td>['Dengsheng Chen', 'Jie Hu', 'Xiaoming Wei', '...</td>\n      <td>http://arxiv.org/abs/2411.17425v1</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:38.144613300Z",
     "start_time": "2025-03-27T22:33:38.128607100Z"
    }
   },
   "id": "ef34942c8a6f1c4f",
   "execution_count": 195
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "title        0\nabstract     0\npublished    0\nauthors      0\nurl          0\ndtype: int64"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:38.491804100Z",
     "start_time": "2025-03-27T22:33:38.476799700Z"
    }
   },
   "id": "1fc71c200e00ab59",
   "execution_count": 196
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Video to Video Generative Adversarial Network for Few-shot Learning Based on Policy Gradient'"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:38.693900800Z",
     "start_time": "2025-03-27T22:33:38.679901200Z"
    }
   },
   "id": "a0f288dbcaacde7b",
   "execution_count": 197
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'The development of sophisticated models for video-to-video synthesis has been\\nfacilitated by recent advances in deep reinforcement learning and generative\\nadversarial networks (GANs). In this paper, we propose RL-V2V-GAN, a new deep\\nneural network approach based on reinforcement learning for unsupervised\\nconditional video-to-video synthesis. While preserving the unique style of the\\nsource video domain, our approach aims to learn a mapping from a source video\\ndomain to a target video domain. We train the model using policy gradient and\\nemploy ConvLSTM layers to capture the spatial and temporal information by\\ndesigning a fine-grained GAN architecture and incorporating spatio-temporal\\nadversarial goals. The adversarial losses aid in content translation while\\npreserving style. Unlike traditional video-to-video synthesis methods requiring\\npaired inputs, our proposed approach is more general because it does not\\nrequire paired inputs. Thus, when dealing with limited videos in the target\\ndomain, i.e., few-shot learning, it is particularly effective. Our experiments\\nshow that RL-V2V-GAN can produce temporally coherent video results. These\\nresults highlight the potential of our approach for further advances in\\nvideo-to-video synthesis.'"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:39.205931300Z",
     "start_time": "2025-03-27T22:33:39.190781800Z"
    }
   },
   "id": "5cff98de1b56a4ff",
   "execution_count": 198
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['published'] = pd.to_datetime(df['published'])\n",
    "\n",
    "df['year_publisher'] = df['published'].dt.year\n",
    "df['month_publisher'] = df['published'].dt.month\n",
    "\n",
    "df.drop(columns=['published'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:39.544687600Z",
     "start_time": "2025-03-27T22:33:39.533575100Z"
    }
   },
   "id": "692292d16d33a901",
   "execution_count": 199
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      year_publisher  month_publisher\n0               2024               10\n1               2024                7\n2               2024               11\n3               2024                3\n4               2024                7\n...              ...              ...\n2095            2024               10\n2096            2024               10\n2097            2024               11\n2098            2024               11\n2099            2024               11\n\n[2100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year_publisher</th>\n      <th>month_publisher</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2024</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2024</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2024</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2024</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2024</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2095</th>\n      <td>2024</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2096</th>\n      <td>2024</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2097</th>\n      <td>2024</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2098</th>\n      <td>2024</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2099</th>\n      <td>2024</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n<p>2100 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['year_publisher', 'month_publisher']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:40.097346700Z",
     "start_time": "2025-03-27T22:33:40.076376400Z"
    }
   },
   "id": "b12328df7c33856d",
   "execution_count": 200
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([2024], dtype=int32)"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year_publisher'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:40.271390100Z",
     "start_time": "2025-03-27T22:33:40.254734200Z"
    }
   },
   "id": "177b8428ff8a8710",
   "execution_count": 201
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "month_publisher\n12    309\n11    216\n10    209\n3     204\n6     194\n7     167\n5     163\n9     159\n4     145\n8     134\n1     102\n2      98\nName: count, dtype: int64"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['month_publisher'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:33:42.037043800Z",
     "start_time": "2025-03-27T22:33:42.029624600Z"
    }
   },
   "id": "9071e69620c40835",
   "execution_count": 202
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fb63e84a6b87b0a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
